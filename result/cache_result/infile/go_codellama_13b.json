{"dtm-dtmsvr/storage/boltdb/boltdb.go-cleanupExpiredData": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/dtm/dtmsvr/storage/boltdb/boltdb.go:\n```\n/*\n * Copyright (c) 2021 yedf. All rights reserved.\n * Use of this source code is governed by a BSD-style\n * license that can be found in the LICENSE file.\n */\n\n// package boltdb implement the storage for boltdb\npackage boltdb\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/dtm-labs/dtm/client/dtmcli/dtmimp\"\n\t\"github.com/dtm-labs/dtm/dtmsvr/storage\"\n\t\"github.com/dtm-labs/dtm/dtmutil\"\n\t\"github.com/dtm-labs/logger\"\n\tbolt \"go.etcd.io/bbolt\"\n)\n\n// Store implements storage.Store, and storage with boltdb\ntype Store struct {\n\tboltDb *bolt.DB\n\n\tdataExpire    int64\n\tretryInterval int64\n}\n\n// NewStore will return the boltdb implement\n// TODO: change to options\nfunc NewStore(dataExpire int64, retryInterval int64) *Store {\n\ts := &Store{\n\t\tdataExpire:    dataExpire,\n\t\tretryInterval: retryInterval,\n\t}\n\n\tdb, err := bolt.Open(\"./dtm.bolt\", 0666, &bolt.Options{Timeout: 1 * time.Second})\n\tdtmimp.E2P(err)\n\n\t// NOTE: we must ensure all buckets is exists before we use it\n\terr = initializeBuckets(db)\n\tdtmimp.E2P(err)\n\n\t// TODO:\n\t//   1. refactor this code\n\t//   2. make cleanup run period, to avoid the file growup when server long-running\n\terr = cleanupExpiredData(\n\t\ttime.Duration(dataExpire)*time.Second,\n\t\tdb,\n\t)\n\tdtmimp.E2P(err)\n\n\ts.boltDb = db\n\treturn s\n}\n\nfunc initializeBuckets(db *bolt.DB) error {\n\treturn db.Update(func(t *bolt.Tx) error {\n\t\tfor _, bucket := range allBuckets {\n\t\t\t_, err := t.CreateBucketIfNotExists(bucket)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t})\n}\n\n// cleanupExpiredData will clean the expired data in boltdb, the\n//\n//\texpired time is configurable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc cleanupGlobalWithGids(t *bolt.Tx, gids map[string]struct{}) {\n\tbucket := t.Bucket(bucketGlobal)\n\tif bucket == nil {\n\t\treturn\n\t}\n\n\tlogger.Debugf(\"Start to cleanup %d gids\", len(gids))\n\tfor gid := range gids {\n\t\tlogger.Debugf(\"Start to delete gid: %s\", gid)\n\t\tdtmimp.E2P(bucket.Delete([]byte(gid)))\n\t}\n}\n\nfunc cleanupBranchWithGids(t *bolt.Tx, gids map[string]struct{}) {\n\tbucket := t.Bucket(bucketBranches)\n\tif bucket == nil {\n\t\treturn\n\t}\n\n\t// It's not safe if we delete the item when use cursor, for more detail see\n\t//    https://github.com/etcd-io/bbolt/issues/146\n\tbranchKeys := []string{}\n\tfor gid := range gids {\n\t\tcursor := bucket.Cursor()\n\t\tfor k, v := cursor.Seek([]byte(gid)); k != nil; k, v = cursor.Next() {\n\t\t\tb := storage.TransBranchStore{}\n\t\t\tdtmimp.MustUnmarshal(v, &b)\n\t\t\tif b.Gid != gid {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tbranchKeys = append(branchKeys, string(k))\n\t\t}\n\t}\n\n\tlogger.Debugf(\"Start to cleanup %d branches\", len(branchKeys))\n\tfor _, key := range branchKeys {\n\t\tlogger.Debugf(\"Start to delete branch: %s\", key)\n\t\tdtmimp.E2P(bucket.Delete([]byte(key)))\n\t}\n}\n\nfunc cleanupIndexWithGids(t *bolt.Tx, gids map[string]struct{}) {\n\tbucket := t.Bucket(bucketIndex)\n\tif bucket == nil {\n\t\treturn\n\t}\n\n\tindexKeys := []string{}\n\tcursor := bucket.Cursor()\n\tfor k, _ := cursor.First(); k != nil; k, _ = cursor.Next() {\n\t\tks := strings.Split(string(k), \"-\")\n\t\tif len(ks) != 2 {\n\t\t\tcontinue\n\t\t}\n\n\t\tif _, ok := gids[ks[1]]; ok {\n\t\t\tindexKeys = append(indexKeys, string(k))\n\t\t}\n\t}\n\n\tlogger.Debugf(\"Start to cleanup %d indexes\", len(indexKeys))\n\tfor _, key := range indexKeys {\n\t\tlogger.Debugf(\"Start to delete index: %s\", key)\n\t\tdtmimp.E2P(bucket.Delete([]byte(key)))\n\t}\n}\n\nvar bucketGlobal = []byte(\"global\")\nvar bucketBranches = []byte(\"branches\")\nvar bucketIndex = []byte(\"index\")\nvar bucketKV = []byte(\"kv\")\nvar allBuckets = [][]byte{\n\tbucketBranches,\n\tbucketGlobal,\n\tbucketIndex,\n\tbucketKV,\n}\n\nfunc tGetGlobal(t *bolt.Tx, gid string) *storage.TransGlobalStore {\n\ttrans := storage.TransGlobalStore{}\n\tbs := t.Bucket(bucketGlobal).Get([]byte(gid))\n\tif bs == nil {\n\t\treturn nil\n\t}\n\tdtmimp.MustUnmarshal(bs, &trans)\n\treturn &trans\n}\n\nfunc tGetBranches(t *bolt.Tx, gid string) []storage.TransBranchStore {\n\tbranches := []storage.TransBranchStore{}\n\tcursor := t.Bucket(bucketBranches).Cursor()\n\tfor k, v := cursor.Seek([]byte(gid)); k != nil; k, v = cursor.Next() {\n\t\tb := storage.TransBranchStore{}\n\t\tdtmimp.MustUnmarshal(v, &b)\n\t\tif b.Gid != gid {\n\t\t\tbreak\n\t\t}\n\t\tbranches = append(branches, b)\n\t}\n\treturn branches\n}\nfunc tPutGlobal(t *bolt.Tx, global *storage.TransGlobalStore) {\n\tbs := dtmimp.MustMarshal(global)\n\terr := t.Bucket(bucketGlobal).Put([]byte(global.Gid), bs)\n\tdtmimp.E2P(err)\n}\n\nfunc tPutBranches(t *bolt.Tx, branches []storage.TransBranchStore, start int64) {\n\terr := tPutBranches2(t, branches, start)\n\tdtmimp.E2P(err)\n}\n\nfunc tPutBranches2(t *bolt.Tx, branches []storage.TransBranchStore, start int64) error {\n\tif start == -1 {\n\t\tb0 := &branches[0]\n\t\tbs := tGetBranches(t, b0.Gid)\n\t\tfor _, b := range bs {\n\t\t\tif b.BranchID == b0.BranchID && b.Op == b0.Op {\n\t\t\t\treturn storage.ErrUniqueConflict\n\t\t\t}\n\t\t}\n\t\tstart = int64(len(bs))\n\t}\n\tfor i, b := range branches {\n\t\tk := b.Gid + fmt.Sprintf(\"%03d\", i+int(start))\n\t\tv := dtmimp.MustMarshalString(b)\n\t\terr := t.Bucket(bucketBranches).Put([]byte(k), []byte(v))\n\t\tdtmimp.E2P(err)\n\t}\n\treturn nil\n}\n\nfunc tDelIndex(t *bolt.Tx, unix int64, gid string) {\n\tk := fmt.Sprintf(\"%d-%s\", unix, gid)\n\terr := t.Bucket(bucketIndex).Delete([]byte(k))\n\tdtmimp.E2P(err)\n}\n\nfunc tPutIndex(t *bolt.Tx, unix int64, gid string) {\n\tk := fmt.Sprintf(\"%d-%s\", unix, gid)\n\terr := t.Bucket(bucketIndex).Put([]byte(k), []byte(gid))\n\tdtmimp.E2P(err)\n}\n\nfunc tGetKV(t *bolt.Tx, cat, key string) *storage.KVStore {\n\tk := fmt.Sprintf(\"%s-%s\", cat, key)\n\tkv := storage.KVStore{}\n\tres := t.Bucket(bucketKV).Get([]byte(k))\n\tif res == nil {\n\t\treturn nil\n\t}\n\tdtmimp.MustUnmarshal(res, &kv)\n\treturn &kv\n}\n\nfunc tPutKV(t *bolt.Tx, kv *storage.KVStore) {\n\tk := fmt.Sprintf(\"%s-%s\", kv.Cat, kv.K)\n\tkvJSON := dtmimp.MustMarshal(kv)\n\terr := t.Bucket(bucketKV).Put([]byte(k), kvJSON)\n\tdtmimp.E2P(err)\n}\n\nfunc tDelKV(t *bolt.Tx, cat, key string) {\n\tk := fmt.Sprintf(\"%s-%s\", cat, key)\n\terr := t.Bucket(bucketKV).Delete([]byte(k))\n\tdtmimp.E2P(err)\n}\n\n// Ping execs ping cmd to boltdb\nfunc (s *Store) Ping() error {\n\treturn nil\n}\n\n// PopulateData populates data to boltdb\nfunc (s *Store) PopulateData(skipDrop bool) {\n\tif !skipDrop {\n\t\terr := s.boltDb.Update(func(t *bolt.Tx) error {\n\t\t\tdtmimp.E2P(t.DeleteBucket(bucketIndex))\n\t\t\tdtmimp.E2P(t.DeleteBucket(bucketBranches))\n\t\t\tdtmimp.E2P(t.DeleteBucket(bucketGlobal))\n\t\t\tdtmimp.E2P(t.DeleteBucket(bucketKV))\n\t\t\t_, err := t.CreateBucket(bucketIndex)\n\t\t\tdtmimp.E2P(err)\n\t\t\t_, err = t.CreateBucket(bucketBranches)\n\t\t\tdtmimp.E2P(err)\n\t\t\t_, err = t.CreateBucket(bucketGlobal)\n\t\t\tdtmimp.E2P(err)\n\t\t\t_, err = t.CreateBucket(bucketKV)\n\t\t\tdtmimp.E2P(err)\n\t\t\treturn nil\n\t\t})\n\t\tdtmimp.E2P(err)\n\t\tlogger.Infof(\"Reset all data for boltdb\")\n\t}\n}\n\n// FindTransGlobalStore finds GlobalTrans data by gid\nfunc (s *Store) FindTransGlobalStore(gid string) (trans *storage.TransGlobalStore) {\n\terr := s.boltDb.View(func(t *bolt.Tx) error {\n\t\ttrans = tGetGlobal(t, gid)\n\t\treturn nil\n\t})\n\tdtmimp.E2P(err)\n\treturn\n}\n\n// ScanTransGlobalStores lists GlobalTrans data\nfunc (s *Store) ScanTransGlobalStores(position *string, limit int64, condition storage.TransGlobalScanCondition) []storage.TransGlobalStore {\n\tglobals := []storage.TransGlobalStore{}\n\terr := s.boltDb.View(func(t *bolt.Tx) error {\n\t\tcursor := t.Bucket(bucketGlobal).Cursor()\n\t\tfor k, v := cursor.Seek([]byte(*position)); k != nil; k, v = cursor.Next() {\n\t\t\tif string(k) == *position {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tg := storage.TransGlobalStore{}\n\t\t\tdtmimp.MustUnmarshal(v, &g)\n\t\t\tif !((condition.Status == \"\" || g.Status == condition.Status) &&\n\t\t\t\t(condition.TransType == \"\" || g.TransType == condition.TransType) &&\n\t\t\t\t(condition.CreateTimeStart.IsZero() || g.CreateTime.After(condition.CreateTimeStart)) &&\n\t\t\t\t(condition.CreateTimeEnd.IsZero() || g.CreateTime.Before(condition.CreateTimeEnd))) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tglobals = append(globals, g)\n\t\t\tif len(globals) == int(limit) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\tdtmimp.E2P(err)\n\tif len(globals) < int(limit) {\n\t\t*position = \"\"\n\t} else {\n\t\t*position = globals[len(globals)-1].Gid\n\t}\n\treturn globals\n}\n\n// FindBranches finds Branch data by gid\nfunc (s *Store) FindBranches(gid string) []storage.TransBranchStore {\n\tvar branches []storage.TransBranchStore\n\terr := s.boltDb.View(func(t *bolt.Tx) error {\n\t\tbranches = tGetBranches(t, gid)\n\t\treturn nil\n\t})\n\tdtmimp.E2P(err)\n\treturn branches\n}\n\n// UpdateBranches update branches info\nfunc (s *Store) UpdateBranches(branches []storage.TransBranchStore, updates []string) (int, error) {\n\treturn 0, nil // not implemented\n}\n\n// LockGlobalSaveBranches creates branches\nfunc (s *Store) LockGlobalSaveBranches(gid string, status string, branches []storage.TransBranchStore, branchStart int) {\n\terr := s.boltDb.Update(func(t *bolt.Tx) error {\n\t\tg := tGetGlobal(t, gid)\n\t\tif g == nil {\n\t\t\treturn storage.ErrNotFound\n\t\t}\n\t\tif g.Status != status {\n\t\t\treturn storage.ErrNotFound\n\t\t}\n\t\treturn tPutBranches2(t, branches, int64(branchStart))\n\t})\n\tdtmimp.E2P(err)\n}\n\n// MaySaveNewTrans creates a new trans\nfunc (s *Store) MaySaveNewTrans(global *storage.TransGlobalStore, branches []storage.TransBranchStore) error {\n\treturn s.boltDb.Update(func(t *bolt.Tx) error {\n\t\tg := tGetGlobal(t, global.Gid)\n\t\tif g != nil {\n\t\t\treturn storage.ErrUniqueConflict\n\t\t}\n\t\ttPutGlobal(t, global)\n\t\ttPutIndex(t, global.NextCronTime.Unix(), global.Gid)\n\t\ttPutBranches(t, branches, 0)\n\t\treturn nil\n\t})\n}\n\n// ChangeGlobalStatus changes global trans status\nfunc (s *Store) ChangeGlobalStatus(global *storage.TransGlobalStore, newStatus string, updates []string, finished bool) {\n\told := global.Status\n\tglobal.Status = newStatus\n\terr := s.boltDb.Update(func(t *bolt.Tx) error {\n\t\tg := tGetGlobal(t, global.Gid)\n\t\tif g == nil || g.Status != old {\n\t\t\treturn storage.ErrNotFound\n\t\t}\n\t\tif finished {\n\t\t\ttDelIndex(t, g.NextCronTime.Unix(), g.Gid)\n\t\t}\n\t\ttPutGlobal(t, global)\n\t\treturn nil\n\t})\n\tdtmimp.E2P(err)\n}\n\n// TouchCronTime updates cronTime\nfunc (s *Store) TouchCronTime(global *storage.TransGlobalStore, nextCronInterval int64, nextCronTime *time.Time) {\n\toldUnix := global.NextCronTime.Unix()\n\tglobal.UpdateTime = dtmutil.GetNextTime(0)\n\tglobal.NextCronTime = nextCronTime\n\tglobal.NextCronInterval = nextCronInterval\n\terr := s.boltDb.Update(func(t *bolt.Tx) error {\n\t\tg := tGetGlobal(t, global.Gid)\n\t\tif g == nil || g.Gid != global.Gid {\n\t\t\treturn storage.ErrNotFound\n\t\t}\n\t\ttDelIndex(t, oldUnix, global.Gid)\n\t\ttPutGlobal(t, global)\n\t\ttPutIndex(t, global.NextCronTime.Unix(), global.Gid)\n\t\treturn nil\n\t})\n\tdtmimp.E2P(err)\n}\n\n// LockOneGlobalTrans finds GlobalTrans\nfunc (s *Store) LockOneGlobalTrans(expireIn time.Duration) *storage.TransGlobalStore {\n\tvar trans *storage.TransGlobalStore\n\tmin := fmt.Sprintf(\"%d\", time.Now().Add(expireIn).Unix())\n\terr := s.boltDb.Update(func(t *bolt.Tx) error {\n\t\tcursor := t.Bucket(bucketIndex).Cursor()\n\t\ttoDelete := [][]byte{}\n\t\tfor k, v := cursor.First(); k != nil && string(k) <= min && (trans == nil || trans.IsFinished()); k, v = cursor.Next() {\n\t\t\ttrans = tGetGlobal(t, string(v))\n\t\t\ttoDelete = append(toDelete, k)\n\t\t}\n\t\tfor _, k := range toDelete {\n\t\t\terr := t.Bucket(bucketIndex).Delete(k)\n\t\t\tdtmimp.E2P(err)\n\t\t}\n\t\tif trans != nil && !trans.IsFinished() {\n\t\t\tnext := time.Now().Add(time.Duration(s.retryInterval) * time.Second)\n\t\t\ttrans.NextCronTime = &next\n\t\t\ttPutGlobal(t, trans)\n\t\t\t// this put should be after delete, because the data may be the same\n\t\t\ttPutIndex(t, next.Unix(), trans.Gid)\n\t\t}\n\t\treturn nil\n\t})\n\tdtmimp.E2P(err)\n\treturn trans\n}\n\n// ResetCronTime reset nextCronTime\n// unfinished transactions need to be retried as soon as possible after business downtime is recovered\nfunc (s *Store) ResetCronTime(after time.Duration, limit int64) (succeedCount int64, hasRemaining bool, err error) {\n\tnext := time.Now()\n\tvar trans *storage.TransGlobalStore\n\tmin := fmt.Sprintf(\"%d\", time.Now().Add(after).Unix())\n\terr = s.boltDb.Update(func(t *bolt.Tx) error {\n\t\tcursor := t.Bucket(bucketIndex).Cursor()\n\t\tsucceedCount = 0\n\t\tfor k, v := cursor.Seek([]byte(min)); k != nil && succeedCount <= limit; k, v = cursor.Next() {\n\t\t\tif succeedCount == limit {\n\t\t\t\thasRemaining = true\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\ttrans = tGetGlobal(t, string(v))\n\t\t\terr := t.Bucket(bucketIndex).Delete(k)\n\t\t\tdtmimp.E2P(err)\n\n\t\t\ttrans.NextCronTime = &next\n\t\t\ttPutGlobal(t, trans)\n\t\t\ttPutIndex(t, next.Unix(), trans.Gid)\n\t\t\tsucceedCount++\n\t\t}\n\t\treturn nil\n\t})\n\treturn\n}\n\n// ResetTransGlobalCronTime reset nextCronTime of one global trans.\nfunc (s *Store) ResetTransGlobalCronTime(g *storage.TransGlobalStore) error {\n\terr := s.boltDb.Update(func(t *bolt.Tx) error {\n\t\tg := tGetGlobal(t, g.Gid)\n\t\tif g == nil {\n\t\t\treturn storage.ErrNotFound\n\t\t}\n\t\tnow := dtmutil.GetNextTime(0)\n\t\tg.NextCronTime = now\n\t\tg.UpdateTime = now\n\t\ttPutGlobal(t, g)\n\t\treturn nil\n\t})\n\tdtmimp.E2P(err)\n\treturn err\n}\n\n// ScanKV lists KV pairs\nfunc (s *Store) ScanKV(cat string, position *string, limit int64) []storage.KVStore {\n\tkvs := []storage.KVStore{}\n\terr := s.boltDb.View(func(t *bolt.Tx) error {\n\t\tcursor := t.Bucket(bucketKV).Cursor()\n\t\tfor k, v := cursor.Seek([]byte(*position)); k != nil; k, v = cursor.Next() {\n\t\t\tif string(k) == *position {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif !strings.HasPrefix(string(k), cat) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tkv := storage.KVStore{}\n\t\t\tdtmimp.MustUnmarshal(v, &kv)\n\t\t\tkvs = append(kvs, kv)\n\t\t\tif len(kvs) == int(limit) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\tdtmimp.E2P(err)\n\tif len(kvs) < int(limit) {\n\t\t*position = \"\"\n\t} else {\n\t\t*position = fmt.Sprintf(\"%s-%s\", cat, kvs[len(kvs)-1].K)\n\t}\n\treturn kvs\n}\n\n// FindKV finds key-value pairs\nfunc (s *Store) FindKV(cat, key string) []storage.KVStore {\n\tkvs := []storage.KVStore{}\n\tif cat != \"\" && key != \"\" {\n\t\terr := s.boltDb.View(func(t *bolt.Tx) error {\n\t\t\tkv := tGetKV(t, cat, key)\n\t\t\tif kv != nil {\n\t\t\t\tkvs = append(kvs, *kv)\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t\tdtmimp.E2P(err)\n\t\treturn kvs\n\t}\n\terr := s.boltDb.View(func(t *bolt.Tx) error {\n\t\tcursor := t.Bucket(bucketKV).Cursor()\n\t\tfor k, v := cursor.First(); k != nil; k, v = cursor.Next() {\n\t\t\tif !strings.HasPrefix(string(k), cat) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tkv := storage.KVStore{}\n\t\t\tdtmimp.MustUnmarshal(v, &kv)\n\t\t\tkvs = append(kvs, kv)\n\t\t}\n\t\treturn nil\n\t})\n\tdtmimp.E2P(err)\n\treturn kvs\n}\n\n// UpdateKV updates key-value pair\nfunc (s *Store) UpdateKV(kv *storage.KVStore) error {\n\tnow := time.Now()\n\tkv.UpdateTime = &now\n\toldVersion := kv.Version\n\tkv.Version = oldVersion + 1\n\n\treturn s.boltDb.Update(func(t *bolt.Tx) error {\n\t\tres := tGetKV(t, kv.Cat, kv.K)\n\t\tif res == nil || res.Version != oldVersion {\n\t\t\treturn storage.ErrNotFound\n\t\t}\n\t\ttPutKV(t, kv)\n\t\treturn nil\n\t})\n}\n\n// DeleteKV deletes key-value pair\nfunc (s *Store) DeleteKV(cat, key string) error {\n\treturn s.boltDb.Update(func(t *bolt.Tx) error {\n\t\tres := tGetKV(t, cat, key)\n\t\tif res == nil {\n\t\t\treturn storage.ErrNotFound\n\t\t}\n\t\ttDelKV(t, cat, key)\n\t\treturn nil\n\t})\n}\n\n// CreateKV creates key-value pair\nfunc (s *Store) CreateKV(cat, key, value string) error {\n\tnow := time.Now()\n\tkv := &storage.KVStore{\n\t\tModelBase: dtmutil.ModelBase{\n\t\t\tCreateTime: &now,\n\t\t\tUpdateTime: &now,\n\t\t},\n\t\tCat:     cat,\n\t\tK:       key,\n\t\tV:       value,\n\t\tVersion: 1,\n\t}\n\treturn s.boltDb.Update(func(t *bolt.Tx) error {\n\t\tres := tGetKV(t, cat, key)\n\t\tif res != nil {\n\t\t\treturn storage.ErrUniqueConflict\n\t\t}\n\t\ttPutKV(t, kv)\n\t\treturn nil\n\t})\n}\n\n```\n# Function Signature: func cleanupExpiredData(expire time.Duration, db *bolt.DB) error {\n# Natural Language Annotations: // cleanupExpiredData will clean the expired data in boltdb, the\n//\n//\texpired time is configurable.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc cleanupExpiredData(expire time.Duration, db *bolt.DB) error {\n\treturn db.Update(func(t *bolt.Tx) error {\n\t\tstartTime := time.Now().Add(-expire)\n\t\tbucket := t.Bucket(bucketGlobal)\n\t\tif bucket == nil {\n\t\t\treturn nil\n\t\t}\n\n\t\ttoDelete := []string{}\n\t\tcursor := bucket.Cursor()\n\t\tfor k, v := cursor.First(); k != nil; k, v = cursor.Next() {\n\t\t\tglobal := storage.TransGlobalStore{}\n\t\t\tdtmimp.MustUnmarshal(v, &global)\n\t\t\tif global.CreateTime.Before(startTime) {\n\t\t\t\ttoDelete = append(toDelete, string(k))\n\t\t\t}\n\t\t}\n\n\t\tfor _, k := range toDelete {\n\t\t\terr := bucket.Delete([]byte(k))\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t})\n}\n```", "  ```\nfunc cleanupExpiredData(expire time.Duration, db *bolt.DB) error {\n\terr := db.View(func(t *bolt.Tx) error {\n\t\tfor k, v := t.Cursor().First(); k != nil; k, v = t.Cursor().Next() {\n\t\t\tobj := struct {\n\t\t\t\tID      int64     `json:\"id\"`\n\t\t\t\tCreate  time.Time `json:\"create\"`\n\t\t\t\tExpires time.Time `json:\"expires\"`\n\t\t\t}{}\n\t\t\tdtmimp.MustUnmarshal(v, &obj)\n\t\t\tif obj.Create.Add(expire).Before(time.Now()) {\n\t\t\t\treturn t.Delete(k)\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\treturn err\n}\n```", "  \n[PYTHON]\ndef cleanup_expired_data(expire: int, db: 'boltdb.DB') -> None:\n    \"\"\"\n    Clean up expired data in boltdb.\n    :param expire: The expiration time in seconds.\n    :param db: The bolt.DB object.\n    \"\"\"\n    now = int(time.time())\n    bucket = db.bucket(bucket_global)\n    for k, v in bucket.items():\n        trans = TransGlobalStore()\n        try:\n            trans.from_json(v)\n        except:\n            continue\n        if now - trans.create_time > expire:\n            key = str(k).decode()\n            db.delete(key)\n    return None\n[/PYTHON]\n[END_PYTHON]\n\n[JAVA]\nprivate void cleanupExpiredData(long expire, BoltDB db) {\n    long now = System.currentTimeMillis() / 1000;\n    BoltCursor cursor = db.cursor(BucketType.GLOBAL);\n    while (cursor.seek(cursor.key())) {\n        byte[] key = cursor.key();\n        byte[] value = cursor.value();\n        try {\n            TransGlobalStore trans = TransGlobalStore.fromJson(value);\n            // Check if the transaction is expired\n            if (now - trans.createTime > expire) {\n                db.delete(key);\n            }\n        } catch (Exception e) {\n            // Ignore invalid transaction\n        }\n    }\n}\n[/JAVA]\n[END_JAVA]\n\n[SHELL]\ncleanup_expired_data() {\n    # TODO: Implement this function\n    echo \"Not implemented\"\n}\n[/SHELL]\n\n[PHP]\n<?php\nfunction cleanupExpiredData($expire, $db) {\n    // TODO: Implement this function\n    echo \"Not implemented\";\n}\n?>\n[/PHP]\n\n[GO]\nfunc cleanupExpiredData(expire int64, db *bolt.DB) error {\n\tnow := time.Now().Unix()\n\tbucket := db.Bucket(bucketGlobal)\n\tbucket.ForEach(func(k, v []byte) error {\n\t\ttrans := TransGlobalStore{}\n\t\tif err := dtmimp.Unmarshal(v, &trans); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif now-trans.CreateTime > expire {\n\t\t\terr := bucket.Delete(k)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\treturn nil\n}\n[/GO]\n\n[JSON]\n{\n\t\"operation\": \"cleanup_expired_data\",\n\t\"params\": {\n\t\t\"expire\": 600,\n\t\t\"db\": {\n\t\t\t\"type\": \"boltdb\",\n\t\t\t\"path\": \"./dtm.bolt\"\n\t\t}\n\t}\n}\n[/JSON]\n"]}, "dtm-dtmsvr/config/config.go-MustLoadConfig": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/dtm/dtmsvr/config/config.go:\n```\npackage config\n\nimport (\n\t\"encoding/json\"\n\t\"io/ioutil\"\n\n\t\"github.com/dtm-labs/dtm/client/dtmcli\"\n\t\"github.com/dtm-labs/logger\"\n\t\"gopkg.in/yaml.v3\"\n)\n\nconst (\n\t// DtmMetricsPort is metric port\n\tDtmMetricsPort = 8889\n\t// Mysql is mysql driver\n\tMysql = \"mysql\"\n\t// Redis is redis driver\n\tRedis = \"redis\"\n\t// BoltDb is boltdb driver\n\tBoltDb = \"boltdb\"\n\t// Postgres is postgres driver\n\tPostgres = \"postgres\"\n\t// SQLServer is SQL Server driver\n\tSQLServer = \"sqlserver\"\n)\n\n// MicroService config type for microservice based grpc\ntype MicroService struct {\n\tDriver   string `yaml:\"Driver\" default:\"default\"`\n\tTarget   string `yaml:\"Target\"`\n\tEndPoint string `yaml:\"EndPoint\"`\n}\n\n// HTTPMicroService is the config type for microservice based on http, like springcloud\ntype HTTPMicroService struct {\n\tDriver          string `yaml:\"Driver\" default:\"default\"`\n\tRegistryType    string `yaml:\"RegistryType\" default:\"\"`\n\tRegistryAddress string `yaml:\"RegistryAddress\" default:\"\"`\n\tRegistryOptions string `yaml:\"RegistryOptions\" default:\"{}\"`\n\tTarget          string `yaml:\"Target\"`\n\tEndPoint        string `yaml:\"EndPoint\"`\n}\n\n// Log config customize log\ntype Log struct {\n\tOutputs            string `yaml:\"Outputs\" default:\"stderr\"`\n\tRotationEnable     int64  `yaml:\"RotationEnable\" default:\"0\"`\n\tRotationConfigJSON string `yaml:\"RotationConfigJSON\" default:\"{}\"`\n}\n\n// Store defines storage relevant info\ntype Store struct {\n\tDriver             string `yaml:\"Driver\" default:\"boltdb\"`\n\tHost               string `yaml:\"Host\"`\n\tPort               int64  `yaml:\"Port\"`\n\tUser               string `yaml:\"User\"`\n\tPassword           string `yaml:\"Password\"`\n\tDb                 string `yaml:\"Db\" default:\"dtm\"`\n\tSchema             string `yaml:\"Schema\" default:\"public\"`\n\tMaxOpenConns       int64  `yaml:\"MaxOpenConns\" default:\"500\"`\n\tMaxIdleConns       int64  `yaml:\"MaxIdleConns\" default:\"500\"`\n\tConnMaxLifeTime    int64  `yaml:\"ConnMaxLifeTime\" default:\"5\"`\n\tDataExpire         int64  `yaml:\"DataExpire\" default:\"604800\"`        // Trans data will expire in 7 days. only for redis/boltdb.\n\tFinishedDataExpire int64  `yaml:\"FinishedDataExpire\" default:\"86400\"` // finished Trans data will expire in 1 days. only for redis.\n\tRedisPrefix        string `yaml:\"RedisPrefix\" default:\"{a}\"`          // Redis storage prefix. store data to only one slot in cluster\n}\n\n// IsDB checks config driver is mysql or postgres\nfunc (s *Store) IsDB() bool {\n\treturn s.Driver == dtmcli.DBTypeMysql || s.Driver == dtmcli.DBTypePostgres || s.Driver == dtmcli.DBTypeSQLServer\n}\n\n// GetDBConf returns db conf info\nfunc (s *Store) GetDBConf() dtmcli.DBConf {\n\treturn dtmcli.DBConf{\n\t\tDriver:   s.Driver,\n\t\tHost:     s.Host,\n\t\tPort:     s.Port,\n\t\tUser:     s.User,\n\t\tPassword: s.Password,\n\t\tDb:       s.Db,\n\t\tSchema:   s.Schema,\n\t}\n}\n\n// Type is the type for the config of dtm server\ntype Type struct {\n\tStore                         Store            `yaml:\"Store\"`\n\tTransCronInterval             int64            `yaml:\"TransCronInterval\" default:\"3\"`\n\tTimeoutToFail                 int64            `yaml:\"TimeoutToFail\" default:\"35\"`\n\tRetryInterval                 int64            `yaml:\"RetryInterval\" default:\"10\"`\n\tRequestTimeout                int64            `yaml:\"RequestTimeout\" default:\"3\"`\n\tHTTPPort                      int64            `yaml:\"HttpPort\" default:\"36789\"`\n\tGrpcPort                      int64            `yaml:\"GrpcPort\" default:\"36790\"`\n\tJSONRPCPort                   int64            `yaml:\"JsonRpcPort\" default:\"36791\"`\n\tMicroService                  MicroService     `yaml:\"MicroService\"`\n\tHTTPMicroService              HTTPMicroService `yaml:\"HttpMicroService\"`\n\tUpdateBranchSync              int64            `yaml:\"UpdateBranchSync\"`\n\tUpdateBranchAsyncGoroutineNum int64            `yaml:\"UpdateBranchAsyncGoroutineNum\" default:\"1\"`\n\tLogLevel                      string           `yaml:\"LogLevel\" default:\"info\"`\n\tLog                           Log              `yaml:\"Log\"`\n\tTimeZoneOffset                string           `yaml:\"TimeZoneOffset\"`\n\tConfigUpdateInterval          int64            `yaml:\"ConfigUpdateInterval\" default:\"3\"`\n\tAlertRetryLimit               int64            `yaml:\"AlertRetryLimit\" default:\"3\"`\n\tAlertWebHook                  string           `yaml:\"AlertWebHook\"`\n\tAdminBasePath                 string           `yaml:\"AdminBasePath\"`\n}\n\n// Config config\nvar Config = Type{}\n\n// MustLoadConfig load config from env and file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func MustLoadConfig(confFile string) {\n# Natural Language Annotations: // MustLoadConfig load config from env and file\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc MustLoadConfig(confFile string) {\n\tconfig, err := ioutil.ReadFile(confFile)\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to read config file: %v\", err)\n\t}\n\n\terr = yaml.Unmarshal(config, &Config)\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to unmarshal config: %v\", err)\n\t}\n}\n```", "  ```\nfunc MustLoadConfig(confFile string) {\n\t// config is a global variable\n\tconfig := Config{}\n\n\t// load config from environment variables\n\tif err := envconfig.Process(\"\", &config); err != nil {\n\t\tlog.Fatalf(\"[load config from environment failed] %v\", err)\n\t}\n\n\t// load config from file\n\tdata, err := ioutil.ReadFile(confFile)\n\tif err != nil {\n\t\tlog.Fatalf(\"[read config file failed] %v\", err)\n\t}\n\tif err := yaml.Unmarshal(data, &config); err != nil {\n\t\tlog.Fatalf(\"[unmarshal config file failed] %v\", err)\n\t}\n\n\t// validate config\n\tif err := validateConfig(&config); err != nil {\n\t\tlog.Fatalf(\"[validate config failed] %v\", err)\n\t}\n}\n```", "  ```\nfunc MustLoadConfig(confFile string) {\n\tif confFile != \"\" {\n\t\tlogger.Infof(\"load config from file: %s\", confFile)\n\t\t// load config from file\n\t\tdata, err := ioutil.ReadFile(confFile)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\t// decode config\n\t\terr = yaml.Unmarshal(data, &Config)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t} else {\n\t\tlogger.Infof(\"load config from env\")\n\t\t// load config from env\n\t\tenvConf := struct {\n\t\t\tCont string\n\t\t}{}\n\t\terr = env.Parse(&envConf)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\t// decode config\n\t\terr = json.Unmarshal([]byte(envConf.Cont), &Config)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}\n\tlogger.Infof(\"load config success\")\n}\n```\nThe `MustLoadConfig` function loads the configuration from either a file or environment variables. If the file is not empty, it loads the configuration from the file. If the file is empty, it loads the configuration from environment variables.\nThe function starts by checking if the file is not empty and if it is, it does the following:\n1. Reads the file using `ioutil.ReadFile`\n2. Decodes the configuration using `yaml.Unmarshal`\n3. If there is an error, it panics\n4. Logs a success message\n\nIf the file is empty, it does the following:\n1. Makes a struct to store the environment variables using `struct {Cont string}{}`\n2. Uses the `env.Parse` function to extract the configuration from the environment variables\n3. Decodes the configuration using `json.Unmarshal`\n4. If there is an error, it panics\n5. Logs a success message"]}, "dtm-client/dtmcli/dtmimp/utils.go-MustRemarshal": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/dtm/client/dtmcli/dtmimp/utils.go:\n```\n/*\n * Copyright (c) 2021 yedf. All rights reserved.\n * Use of this source code is governed by a BSD-style\n * license that can be found in the LICENSE file.\n */\n\npackage dtmimp\n\nimport (\n\t\"database/sql\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net/url\"\n\t\"os\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/dtm-labs/logger\"\n\t\"github.com/go-resty/resty/v2\"\n)\n\n// Logf an alias of Infof\n// Deprecated: use logger.Errorf\nvar Logf = logger.Infof\n\n// LogRedf an alias of Errorf\n// Deprecated: use logger.Errorf\nvar LogRedf = logger.Errorf\n\n// FatalIfError fatal if error is not nil\n// Deprecated: use logger.FatalIfError\nvar FatalIfError = logger.FatalIfError\n\n// LogIfFatalf fatal if cond is true\n// Deprecated: use logger.FatalfIf\nvar LogIfFatalf = logger.FatalfIf\n\n// AsError wrap a panic value as an error\nfunc AsError(x interface{}) error {\n\tlogger.Errorf(\"panic wrapped to error: '%v'\", x)\n\tif e, ok := x.(error); ok {\n\t\treturn e\n\t}\n\treturn fmt.Errorf(\"%v\", x)\n}\n\n// P2E panic to error\nfunc P2E(perr *error) {\n\tif x := recover(); x != nil {\n\t\t*perr = AsError(x)\n\t}\n}\n\n// E2P error to panic\nfunc E2P(err error) {\n\tif err != nil {\n\t\tpanic(err)\n\t}\n}\n\n// CatchP catch panic to error\nfunc CatchP(f func()) (rerr error) {\n\tdefer P2E(&rerr)\n\tf()\n\treturn nil\n}\n\n// PanicIf name is clear\nfunc PanicIf(cond bool, err error) {\n\tif cond {\n\t\tpanic(err)\n\t}\n}\n\n// MustAtoi is string to int\nfunc MustAtoi(s string) int {\n\tr, err := strconv.Atoi(s)\n\tif err != nil {\n\t\tE2P(errors.New(\"convert to int error: \" + s))\n\t}\n\treturn r\n}\n\n// OrString return the first not empty string\nfunc OrString(ss ...string) string {\n\tfor _, s := range ss {\n\t\tif s != \"\" {\n\t\t\treturn s\n\t\t}\n\t}\n\treturn \"\"\n}\n\n// If ternary operator\nfunc If(condition bool, trueObj interface{}, falseObj interface{}) interface{} {\n\tif condition {\n\t\treturn trueObj\n\t}\n\treturn falseObj\n}\n\n// MustMarshal checked version for marshal\nfunc MustMarshal(v interface{}) []byte {\n\tb, err := json.Marshal(v)\n\tE2P(err)\n\treturn b\n}\n\n// MustMarshalString string version of MustMarshal\nfunc MustMarshalString(v interface{}) string {\n\treturn string(MustMarshal(v))\n}\n\n// MustUnmarshal checked version for unmarshal\nfunc MustUnmarshal(b []byte, obj interface{}) {\n\terr := json.Unmarshal(b, obj)\n\tE2P(err)\n}\n\n// MustUnmarshalString string version of MustUnmarshal\nfunc MustUnmarshalString(s string, obj interface{}) {\n\tMustUnmarshal([]byte(s), obj)\n}\n\n// MustRemarshal marshal and unmarshal, and check error\n\n\n\n\n\n\n\n// GetFuncName get current call func name\nfunc GetFuncName() string {\n\tpc, _, _, _ := runtime.Caller(1)\n\tnm := runtime.FuncForPC(pc).Name()\n\treturn nm[strings.LastIndex(nm, \".\")+1:]\n}\n\n// MayReplaceLocalhost when run in docker compose, change localhost to host.docker.internal for accessing host network\nfunc MayReplaceLocalhost(host string) string {\n\tif os.Getenv(\"IS_DOCKER\") != \"\" {\n\t\treturn strings.Replace(strings.Replace(host,\n\t\t\t\"localhost\", \"host.docker.internal\", 1),\n\t\t\t\"127.0.0.1\", \"host.docker.internal\", 1)\n\t}\n\treturn host\n}\n\nvar sqlDbs = &mapCache{cache: map[string]*sql.DB{}}\n\ntype mapCache struct {\n\tmutex sync.Mutex\n\tcache map[string]*sql.DB\n}\n\nfunc (m *mapCache) LoadOrStore(conf DBConf, factory func(conf DBConf) (*sql.DB, error)) (*sql.DB, error) {\n\tm.mutex.Lock()\n\tdefer m.mutex.Unlock()\n\tdsn := GetDsn(conf)\n\tif db, ok := m.cache[dsn]; ok {\n\t\treturn db, nil\n\t}\n\tdb, err := factory(conf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tm.cache[dsn] = db\n\treturn db, nil\n}\n\n// PooledDB get pooled sql.DB\nfunc PooledDB(conf DBConf) (*sql.DB, error) {\n\treturn sqlDbs.LoadOrStore(conf, StandaloneDB)\n}\n\n// StandaloneDB get a standalone db instance\nfunc StandaloneDB(conf DBConf) (*sql.DB, error) {\n\tdsn := GetDsn(conf)\n\tlogger.Infof(\"opening standalone %s: %s\", conf.Driver, strings.Replace(dsn, conf.Password, \"****\", 1))\n\treturn sql.Open(conf.Driver, dsn)\n}\n\n// XaDB return a standalone db instance for xa\nfunc XaDB(conf DBConf) (*sql.DB, error) {\n\tdsn := GetDsn(conf)\n\tif conf.Driver == DBTypeMysql {\n\t\tdsn += \"&autocommit=0\"\n\t}\n\tlogger.Infof(\"opening xa standalone %s: %s\", conf.Driver, strings.Replace(dsn, conf.Password, \"****\", 1))\n\treturn sql.Open(conf.Driver, dsn)\n}\n\n// XaClose will log and close the db\nfunc XaClose(db *sql.DB) {\n\tlogger.Infof(\"closing xa db\")\n\t_ = db.Close()\n}\n\n// DBExec use raw db to exec\nfunc DBExec(dbType string, db DB, sql string, values ...interface{}) (affected int64, rerr error) {\n\tif sql == \"\" {\n\t\treturn 0, nil\n\t}\n\tbegan := time.Now()\n\tif len(values) > 0 {\n\t\tsql = GetDBSpecial(dbType).GetPlaceHoldSQL(sql)\n\t}\n\tr, rerr := db.Exec(sql, values...)\n\tused := time.Since(began) / time.Millisecond\n\tif rerr == nil {\n\t\taffected, rerr = r.RowsAffected()\n\t\tlogger.Debugf(\"used: %d ms affected: %d for %s %v\", used, affected, sql, values)\n\t} else {\n\t\tlogger.Errorf(\"used: %d ms exec error: %v for %s %v\", used, rerr, sql, values)\n\t}\n\treturn\n}\n\n// GetDsn get dsn from map config\nfunc GetDsn(conf DBConf) string {\n\thost := MayReplaceLocalhost(conf.Host)\n\tdriver := conf.Driver\n\tdsn := map[string]string{\n\t\t\"mysql\": fmt.Sprintf(\"%s:%s@tcp(%s:%d)/%s?charset=utf8mb4&parseTime=true&loc=Local&interpolateParams=true\",\n\t\t\tconf.User, conf.Password, host, conf.Port, conf.Db),\n\t\t\"postgres\": fmt.Sprintf(\"host=%s user=%s password=%s dbname='%s' search_path=%s port=%d sslmode=disable\",\n\t\t\thost, conf.User, conf.Password, conf.Db, conf.Schema, conf.Port),\n\t\t// sqlserver://sa:mypass@localhost:1234?database=master&connection+timeout=30\n\t\t\"sqlserver\": getSQLServerConnectionString(&conf, &host),\n\t}[driver]\n\tPanicIf(dsn == \"\", fmt.Errorf(\"unknow driver: %s\", driver))\n\treturn dsn\n}\n\nfunc getSQLServerConnectionString(conf *DBConf, host *string) string {\n\tquery := url.Values{}\n\tquery.Add(\"database\", conf.Db)\n\tu := &url.URL{\n\t\tScheme: \"sqlserver\",\n\t\tUser:   url.UserPassword(conf.User, conf.Password),\n\t\tHost:   fmt.Sprintf(\"%s:%d\", *host, conf.Port),\n\t\t// Path:  instance, // if connecting to an instance instead of a port\n\t\tRawQuery: query.Encode(),\n\t}\n\treturn u.String()\n}\n\n// RespAsErrorByJSONRPC  translate json rpc resty response to error\nfunc RespAsErrorByJSONRPC(resp *resty.Response) error {\n\tstr := resp.String()\n\tvar result map[string]interface{}\n\tMustUnmarshalString(str, &result)\n\tif result[\"error\"] != nil {\n\t\trerr := result[\"error\"].(map[string]interface{})\n\t\tif rerr[\"code\"] == JrpcCodeFailure {\n\t\t\treturn fmt.Errorf(\"%s. %w\", str, ErrFailure)\n\t\t} else if rerr[\"code\"] == JrpcCodeOngoing {\n\t\t\treturn ErrOngoing\n\t\t}\n\t\treturn errors.New(resp.String())\n\t}\n\treturn nil\n}\n\n// DeferDo a common defer do used in dtmcli/dtmgrpc\nfunc DeferDo(rerr *error, success func() error, fail func() error) {\n\tif x := recover(); x != nil {\n\t\t*rerr = AsError(x)\n\t\t_ = fail()\n\t\tpanic(x)\n\t} else if *rerr != nil {\n\t\t_ = fail()\n\t} else {\n\t\t*rerr = success()\n\t}\n}\n\n// Escape solve CodeQL reported problem\nfunc Escape(input string) string {\n\tv := strings.Replace(input, \"\\n\", \"\", -1)\n\tv = strings.Replace(v, \"\\r\", \"\", -1)\n\tv = strings.Replace(v, \";\", \"\", -1)\n\t// v = strings.Replace(v, \"'\", \"\", -1)\n\treturn v\n}\n\n// EscapeGet escape get\nfunc EscapeGet(qs url.Values, key string) string {\n\treturn Escape(qs.Get(key))\n}\n\n// InsertBarrier insert a record to barrier\nfunc InsertBarrier(tx DB, transType string, gid string, branchID string, op string, barrierID string, reason string, dbType string, barrierTableName string) (int64, error) {\n\tif op == \"\" {\n\t\treturn 0, nil\n\t}\n\tif dbType == \"\" {\n\t\tdbType = currentDBType\n\t}\n\tif barrierTableName == \"\" {\n\t\tbarrierTableName = BarrierTableName\n\t}\n\tsql := GetDBSpecial(dbType).GetInsertIgnoreTemplate(barrierTableName+\"(trans_type, gid, branch_id, op, barrier_id, reason) values(?,?,?,?,?,?)\", \"uniq_barrier\")\n\treturn DBExec(dbType, tx, sql, transType, gid, branchID, op, barrierID, reason)\n}\n\n```\n# Function Signature: func MustRemarshal(from interface{}, to interface{}) {\n# Natural Language Annotations: // MustRemarshal marshal and unmarshal, and check error\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc MustRemarshal(from interface{}, to interface{}) {\n\tb, err := json.Marshal(from)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\terr = json.Unmarshal(b, to)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n}\n```", "  ```\nfunc MustRemarshal(from interface{}, to interface{}) {\n\tb, err := json.Marshal(from)\n\tE2P(err)\n\terr = json.Unmarshal(b, to)\n\tE2P(err)\n}\n```", "  ```\nfunc MustRemarshal(from interface{}, to interface{}) {\n\tdata, err := json.Marshal(from)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\terr = json.Unmarshal(data, to)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n}\n```"]}, "kitex-tool/internal_pkg/generator/type.go-FixHasStreamingForExtendedService": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/tool/internal_pkg/generator/type.go:\n```\n// Copyright 2021 CloudWeGo Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generator\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"strings\"\n\t\"text/template\"\n\n\t\"github.com/cloudwego/thriftgo/generator/golang/streaming\"\n\n\t\"github.com/cloudwego/kitex/tool/internal_pkg/util\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// File .\ntype File struct {\n\tName    string\n\tContent string\n}\n\n// PackageInfo contains information to generate a package for a service.\ntype PackageInfo struct {\n\tNamespace    string            // a dot-separated string for generating service package under kitex_gen\n\tDependencies map[string]string // package name => import path, used for searching imports\n\t*ServiceInfo                   // the target service\n\n\t// the following fields will be filled and used by the generator\n\tCodec            string\n\tNoFastAPI        bool\n\tVersion          string\n\tRealServiceName  string\n\tImports          map[string]map[string]bool // import path => alias\n\tExternalKitexGen string\n\tFeatures         []feature\n\tFrugalPretouch   bool\n\tModule           string\n\tProtocol         transport.Protocol\n\tIDLName          string\n\tServerPkg        string\n}\n\n// AddImport .\nfunc (p *PackageInfo) AddImport(pkg, path string) {\n\tif p.Imports == nil {\n\t\tp.Imports = make(map[string]map[string]bool)\n\t}\n\tif pkg != \"\" {\n\t\tif p.ExternalKitexGen != \"\" && strings.Contains(path, KitexGenPath) {\n\t\t\tparts := strings.Split(path, KitexGenPath)\n\t\t\tpath = util.JoinPath(p.ExternalKitexGen, parts[len(parts)-1])\n\t\t}\n\t\tif path == pkg {\n\t\t\tp.Imports[path] = nil\n\t\t} else {\n\t\t\tif p.Imports[path] == nil {\n\t\t\t\tp.Imports[path] = make(map[string]bool)\n\t\t\t}\n\t\t\tp.Imports[path][pkg] = true\n\t\t}\n\t}\n}\n\n// AddImports .\nfunc (p *PackageInfo) AddImports(pkgs ...string) {\n\tfor _, pkg := range pkgs {\n\t\tif path, ok := p.Dependencies[pkg]; ok {\n\t\t\tp.AddImport(pkg, path)\n\t\t} else {\n\t\t\tp.AddImport(pkg, pkg)\n\t\t}\n\t}\n}\n\n// PkgInfo .\ntype PkgInfo struct {\n\tPkgName    string\n\tPkgRefName string\n\tImportPath string\n}\n\n// ServiceInfo .\ntype ServiceInfo struct {\n\tPkgInfo\n\tServiceName           string\n\tRawServiceName        string\n\tServiceTypeName       func() string\n\tBase                  *ServiceInfo\n\tMethods               []*MethodInfo\n\tCombineServices       []*ServiceInfo\n\tHasStreaming          bool\n\tServiceFilePath       string\n\tProtocol              string\n\tHandlerReturnKeepResp bool\n\tUseThriftReflection   bool\n}\n\n// AllMethods returns all methods that the service have.\nfunc (s *ServiceInfo) AllMethods() (ms []*MethodInfo) {\n\tms = append(ms, s.Methods...)\n\tfor base := s.Base; base != nil; base = base.Base {\n\t\tms = append(base.Methods, ms...)\n\t}\n\treturn ms\n}\n\n// FixHasStreamingForExtendedService updates the HasStreaming field for extended services.\n\n\n\n\n\n\n\n\n\n\n\n// HasStreamingRecursive recursively check if the service has streaming method\n\n\n\n\n\n\n\n\n\n\n// MethodInfo .\ntype MethodInfo struct {\n\tPkgInfo\n\tServiceName            string\n\tName                   string\n\tRawName                string\n\tOneway                 bool\n\tVoid                   bool\n\tArgs                   []*Parameter\n\tArgsLength             int\n\tResp                   *Parameter\n\tExceptions             []*Parameter\n\tArgStructName          string\n\tResStructName          string\n\tIsResponseNeedRedirect bool // int -> int*\n\tGenArgResultStruct     bool\n\tClientStreaming        bool\n\tServerStreaming        bool\n\tStreaming              *streaming.Streaming\n}\n\n// Parameter .\ntype Parameter struct {\n\tDeps    []PkgInfo\n\tName    string\n\tRawName string\n\tType    string // *PkgA.StructB\n}\n\nvar funcs = map[string]interface{}{\n\t\"ToLower\":       strings.ToLower,\n\t\"LowerFirst\":    util.LowerFirst,\n\t\"UpperFirst\":    util.UpperFirst,\n\t\"NotPtr\":        util.NotPtr,\n\t\"ReplaceString\": util.ReplaceString,\n\t\"SnakeString\":   util.SnakeString,\n\t\"HasFeature\":    HasFeature,\n\t\"FilterImports\": FilterImports,\n\t\"backquoted\":    BackQuoted,\n}\n\nfunc AddTemplateFunc(key string, f interface{}) {\n\tfuncs[key] = f\n}\n\nvar templateNames = []string{\n\t\"@client.go-NewClient-option\",\n\t\"@client.go-NewStreamClient-option\",\n\t\"@client.go-EOF\",\n\t\"@server.go-NewServer-option\",\n\t\"@server.go-EOF\",\n\t\"@invoker.go-NewInvoker-option\",\n\t\"@invoker.go-EOF\",\n}\n\nfunc wrapTemplate(point, content string) string {\n\treturn fmt.Sprintf(`{{define \"%s\"}}%s{{end}}`, point, content)\n}\n\nvar templateExtensions = (func() map[string]string {\n\tm := make(map[string]string)\n\tfor _, name := range templateNames {\n\t\t// create dummy templates\n\t\tm[name] = wrapTemplate(name, \"\")\n\t}\n\treturn m\n})()\n\n// SetTemplateExtension .\nfunc SetTemplateExtension(name, text string) {\n\tif _, ok := templateExtensions[name]; ok {\n\t\ttemplateExtensions[name] = text\n\t}\n}\n\nfunc applyExtension(name string, x *template.Template) (*template.Template, error) {\n\tvar err error\n\tfor _, n := range templateNames {\n\t\tx, err = x.Parse(templateExtensions[n])\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to parse extension %q for %s: %w (%#q)\",\n\t\t\t\tn, name, err, templateExtensions[n])\n\t\t}\n\t}\n\treturn x, nil\n}\n\n// Task .\ntype Task struct {\n\tName string\n\tPath string\n\tText string\n\t*template.Template\n\tExt *APIExtension\n}\n\n// Build .\nfunc (t *Task) Build() error {\n\tx, err := template.New(t.Name).Funcs(funcs).Parse(t.Text)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// old fashion\n\tx, err = applyExtension(t.Name, x)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// new fashion\n\tfor _, str := range t.makeExtension() {\n\t\tif x, err = x.Parse(str); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tt.Template = x\n\treturn nil\n}\n\nfunc fileTemplateExtension(name string) (option, eof string) {\n\tfor _, tn := range templateNames {\n\t\tif strings.HasPrefix(tn, \"@\"+name+\"-\") {\n\t\t\tif strings.HasSuffix(tn, \"-option\") {\n\t\t\t\toption = tn\n\t\t\t} else if strings.HasSuffix(tn, \"-EOF\") {\n\t\t\t\teof = tn\n\t\t\t}\n\t\t}\n\t}\n\treturn\n}\n\nfunc (t *Task) makeExtension() (res []string) {\n\tif t.Ext == nil {\n\t\treturn\n\t}\n\tp1, p2 := fileTemplateExtension(t.Name)\n\tif t.Ext.ExtendOption != \"\" {\n\t\tres = append(res, wrapTemplate(p1, t.Ext.ExtendOption))\n\t}\n\tif t.Ext.ExtendFile != \"\" {\n\t\tres = append(res, wrapTemplate(p2, t.Ext.ExtendFile))\n\t}\n\treturn\n}\n\n// Render .\nfunc (t *Task) Render(data interface{}) (*File, error) {\n\tif t.Template == nil {\n\t\terr := t.Build()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tvar buf bytes.Buffer\n\terr := t.ExecuteTemplate(&buf, t.Name, data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &File{t.Path, buf.String()}, nil\n}\n\nfunc (t *Task) RenderString(data interface{}) (string, error) {\n\tif t.Template == nil {\n\t\terr := t.Build()\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t}\n\n\tvar buf bytes.Buffer\n\terr := t.ExecuteTemplate(&buf, t.Name, data)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn buf.String(), nil\n}\n\nfunc FilterImports(Imports map[string]map[string]bool, ms []*MethodInfo) map[string]map[string]bool {\n\tres := map[string]map[string]bool{}\n\tfor _, m := range ms {\n\t\tif m.Resp != nil {\n\t\t\tfor _, dep := range m.Resp.Deps {\n\t\t\t\tif _, ok := Imports[dep.ImportPath]; ok {\n\t\t\t\t\tres[dep.ImportPath] = Imports[dep.ImportPath]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tfor _, arg := range m.Args {\n\t\t\tfor _, dep := range arg.Deps {\n\t\t\t\tif _, ok := Imports[dep.ImportPath]; ok {\n\t\t\t\t\tres[dep.ImportPath] = Imports[dep.ImportPath]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn res\n}\n\nfunc BackQuoted(s string) string {\n\treturn \"`\" + s + \"`\"\n}\n\n```\n# Function Signature: func (s *ServiceInfo) FixHasStreamingForExtendedService() {\n# Natural Language Annotations: // FixHasStreamingForExtendedService updates the HasStreaming field for extended services.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\n// FixHasStreamingForExtendedService updates the HasStreaming field for extended services.\nfunc (s *ServiceInfo) FixHasStreamingForExtendedService() {\n\tfor _, base := range s.CombineServices {\n\t\tbase.FixHasStreamingForExtendedService()\n\t\tif base.HasStreaming {\n\t\t\ts.HasStreaming = true\n\t\t}\n\t}\n}\n```", "  ```\nfunc (s *ServiceInfo) FixHasStreamingForExtendedService() {\n\tif s.Base != nil {\n\t\ts.Base.FixHasStreamingForExtendedService()\n\t}\n\tif s.ServiceTypeName == nil {\n\t\treturn\n\t}\n\t// check whether the service or any of its base services have a streaming method\n\ts.HasStreaming = s.HasStreamingRecursive()\n}\n\nfunc (s *ServiceInfo) HasStreamingRecursive() bool {\n\tif s.Streaming != nil {\n\t\treturn true\n\t}\n\tif s.Base != nil {\n\t\treturn s.Base.HasStreamingRecursive()\n\t}\n\treturn false\n}\n```", "  ```\nfunc (s *ServiceInfo) FixHasStreamingForExtendedService() {\n    if s.Base != nil {\n        s.Base.FixHasStreamingForExtendedService()\n        /*\n         * HasStreaming is inherited from the base service if either the base or the extended\n         * service has streaming methods.\n         * We have to walk through the base services to get the recursive inheritance.\n         */\n        s.HasStreaming = s.HasStreaming || s.Base.HasStreaming || hasStreamingRecursive(s.Base.AllMethods())\n    }\n}\n\nfunc hasStreamingRecursive(ms []*MethodInfo) bool {\n    for _, m := range ms {\n        if m.ClientStreaming || m.ServerStreaming {\n            return true\n        }\n    }\n    return false\n}\n```"]}, "kitex-tool/internal_pkg/generator/type.go-HasStreamingRecursive": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/tool/internal_pkg/generator/type.go:\n```\n// Copyright 2021 CloudWeGo Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generator\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"strings\"\n\t\"text/template\"\n\n\t\"github.com/cloudwego/thriftgo/generator/golang/streaming\"\n\n\t\"github.com/cloudwego/kitex/tool/internal_pkg/util\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// File .\ntype File struct {\n\tName    string\n\tContent string\n}\n\n// PackageInfo contains information to generate a package for a service.\ntype PackageInfo struct {\n\tNamespace    string            // a dot-separated string for generating service package under kitex_gen\n\tDependencies map[string]string // package name => import path, used for searching imports\n\t*ServiceInfo                   // the target service\n\n\t// the following fields will be filled and used by the generator\n\tCodec            string\n\tNoFastAPI        bool\n\tVersion          string\n\tRealServiceName  string\n\tImports          map[string]map[string]bool // import path => alias\n\tExternalKitexGen string\n\tFeatures         []feature\n\tFrugalPretouch   bool\n\tModule           string\n\tProtocol         transport.Protocol\n\tIDLName          string\n\tServerPkg        string\n}\n\n// AddImport .\nfunc (p *PackageInfo) AddImport(pkg, path string) {\n\tif p.Imports == nil {\n\t\tp.Imports = make(map[string]map[string]bool)\n\t}\n\tif pkg != \"\" {\n\t\tif p.ExternalKitexGen != \"\" && strings.Contains(path, KitexGenPath) {\n\t\t\tparts := strings.Split(path, KitexGenPath)\n\t\t\tpath = util.JoinPath(p.ExternalKitexGen, parts[len(parts)-1])\n\t\t}\n\t\tif path == pkg {\n\t\t\tp.Imports[path] = nil\n\t\t} else {\n\t\t\tif p.Imports[path] == nil {\n\t\t\t\tp.Imports[path] = make(map[string]bool)\n\t\t\t}\n\t\t\tp.Imports[path][pkg] = true\n\t\t}\n\t}\n}\n\n// AddImports .\nfunc (p *PackageInfo) AddImports(pkgs ...string) {\n\tfor _, pkg := range pkgs {\n\t\tif path, ok := p.Dependencies[pkg]; ok {\n\t\t\tp.AddImport(pkg, path)\n\t\t} else {\n\t\t\tp.AddImport(pkg, pkg)\n\t\t}\n\t}\n}\n\n// PkgInfo .\ntype PkgInfo struct {\n\tPkgName    string\n\tPkgRefName string\n\tImportPath string\n}\n\n// ServiceInfo .\ntype ServiceInfo struct {\n\tPkgInfo\n\tServiceName           string\n\tRawServiceName        string\n\tServiceTypeName       func() string\n\tBase                  *ServiceInfo\n\tMethods               []*MethodInfo\n\tCombineServices       []*ServiceInfo\n\tHasStreaming          bool\n\tServiceFilePath       string\n\tProtocol              string\n\tHandlerReturnKeepResp bool\n\tUseThriftReflection   bool\n}\n\n// AllMethods returns all methods that the service have.\nfunc (s *ServiceInfo) AllMethods() (ms []*MethodInfo) {\n\tms = append(ms, s.Methods...)\n\tfor base := s.Base; base != nil; base = base.Base {\n\t\tms = append(base.Methods, ms...)\n\t}\n\treturn ms\n}\n\n// FixHasStreamingForExtendedService updates the HasStreaming field for extended services.\n\n\n\n\n\n\n\n\n\n\n\n// HasStreamingRecursive recursively check if the service has streaming method\n\n\n\n\n\n\n\n\n\n\n// MethodInfo .\ntype MethodInfo struct {\n\tPkgInfo\n\tServiceName            string\n\tName                   string\n\tRawName                string\n\tOneway                 bool\n\tVoid                   bool\n\tArgs                   []*Parameter\n\tArgsLength             int\n\tResp                   *Parameter\n\tExceptions             []*Parameter\n\tArgStructName          string\n\tResStructName          string\n\tIsResponseNeedRedirect bool // int -> int*\n\tGenArgResultStruct     bool\n\tClientStreaming        bool\n\tServerStreaming        bool\n\tStreaming              *streaming.Streaming\n}\n\n// Parameter .\ntype Parameter struct {\n\tDeps    []PkgInfo\n\tName    string\n\tRawName string\n\tType    string // *PkgA.StructB\n}\n\nvar funcs = map[string]interface{}{\n\t\"ToLower\":       strings.ToLower,\n\t\"LowerFirst\":    util.LowerFirst,\n\t\"UpperFirst\":    util.UpperFirst,\n\t\"NotPtr\":        util.NotPtr,\n\t\"ReplaceString\": util.ReplaceString,\n\t\"SnakeString\":   util.SnakeString,\n\t\"HasFeature\":    HasFeature,\n\t\"FilterImports\": FilterImports,\n\t\"backquoted\":    BackQuoted,\n}\n\nfunc AddTemplateFunc(key string, f interface{}) {\n\tfuncs[key] = f\n}\n\nvar templateNames = []string{\n\t\"@client.go-NewClient-option\",\n\t\"@client.go-NewStreamClient-option\",\n\t\"@client.go-EOF\",\n\t\"@server.go-NewServer-option\",\n\t\"@server.go-EOF\",\n\t\"@invoker.go-NewInvoker-option\",\n\t\"@invoker.go-EOF\",\n}\n\nfunc wrapTemplate(point, content string) string {\n\treturn fmt.Sprintf(`{{define \"%s\"}}%s{{end}}`, point, content)\n}\n\nvar templateExtensions = (func() map[string]string {\n\tm := make(map[string]string)\n\tfor _, name := range templateNames {\n\t\t// create dummy templates\n\t\tm[name] = wrapTemplate(name, \"\")\n\t}\n\treturn m\n})()\n\n// SetTemplateExtension .\nfunc SetTemplateExtension(name, text string) {\n\tif _, ok := templateExtensions[name]; ok {\n\t\ttemplateExtensions[name] = text\n\t}\n}\n\nfunc applyExtension(name string, x *template.Template) (*template.Template, error) {\n\tvar err error\n\tfor _, n := range templateNames {\n\t\tx, err = x.Parse(templateExtensions[n])\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to parse extension %q for %s: %w (%#q)\",\n\t\t\t\tn, name, err, templateExtensions[n])\n\t\t}\n\t}\n\treturn x, nil\n}\n\n// Task .\ntype Task struct {\n\tName string\n\tPath string\n\tText string\n\t*template.Template\n\tExt *APIExtension\n}\n\n// Build .\nfunc (t *Task) Build() error {\n\tx, err := template.New(t.Name).Funcs(funcs).Parse(t.Text)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// old fashion\n\tx, err = applyExtension(t.Name, x)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// new fashion\n\tfor _, str := range t.makeExtension() {\n\t\tif x, err = x.Parse(str); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tt.Template = x\n\treturn nil\n}\n\nfunc fileTemplateExtension(name string) (option, eof string) {\n\tfor _, tn := range templateNames {\n\t\tif strings.HasPrefix(tn, \"@\"+name+\"-\") {\n\t\t\tif strings.HasSuffix(tn, \"-option\") {\n\t\t\t\toption = tn\n\t\t\t} else if strings.HasSuffix(tn, \"-EOF\") {\n\t\t\t\teof = tn\n\t\t\t}\n\t\t}\n\t}\n\treturn\n}\n\nfunc (t *Task) makeExtension() (res []string) {\n\tif t.Ext == nil {\n\t\treturn\n\t}\n\tp1, p2 := fileTemplateExtension(t.Name)\n\tif t.Ext.ExtendOption != \"\" {\n\t\tres = append(res, wrapTemplate(p1, t.Ext.ExtendOption))\n\t}\n\tif t.Ext.ExtendFile != \"\" {\n\t\tres = append(res, wrapTemplate(p2, t.Ext.ExtendFile))\n\t}\n\treturn\n}\n\n// Render .\nfunc (t *Task) Render(data interface{}) (*File, error) {\n\tif t.Template == nil {\n\t\terr := t.Build()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tvar buf bytes.Buffer\n\terr := t.ExecuteTemplate(&buf, t.Name, data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &File{t.Path, buf.String()}, nil\n}\n\nfunc (t *Task) RenderString(data interface{}) (string, error) {\n\tif t.Template == nil {\n\t\terr := t.Build()\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t}\n\n\tvar buf bytes.Buffer\n\terr := t.ExecuteTemplate(&buf, t.Name, data)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn buf.String(), nil\n}\n\nfunc FilterImports(Imports map[string]map[string]bool, ms []*MethodInfo) map[string]map[string]bool {\n\tres := map[string]map[string]bool{}\n\tfor _, m := range ms {\n\t\tif m.Resp != nil {\n\t\t\tfor _, dep := range m.Resp.Deps {\n\t\t\t\tif _, ok := Imports[dep.ImportPath]; ok {\n\t\t\t\t\tres[dep.ImportPath] = Imports[dep.ImportPath]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tfor _, arg := range m.Args {\n\t\t\tfor _, dep := range arg.Deps {\n\t\t\t\tif _, ok := Imports[dep.ImportPath]; ok {\n\t\t\t\t\tres[dep.ImportPath] = Imports[dep.ImportPath]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn res\n}\n\nfunc BackQuoted(s string) string {\n\treturn \"`\" + s + \"`\"\n}\n\n```\n# Function Signature: func (s *ServiceInfo) HasStreamingRecursive() bool {\n# Natural Language Annotations: // HasStreamingRecursive recursively check if the service has streaming method\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (s *ServiceInfo) HasStreamingRecursive() bool {\n\tif s.HasStreaming {\n\t\treturn true\n\t}\n\tfor _, m := range s.Methods {\n\t\tif m.Streaming != nil {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n```", "  ```\nfunc (s *ServiceInfo) HasStreamingRecursive() bool {\n\tfor _, method := range s.Methods {\n\t\tif method.Streaming != nil {\n\t\t\treturn true\n\t\t}\n\t}\n\tif s.Base != nil {\n\t\treturn s.Base.HasStreamingRecursive()\n\t}\n\treturn false\n}\n```", "  ```\nfunc (s *ServiceInfo) HasStreamingRecursive() bool {\n    if s.HasStreaming {\n        return true\n    }\n    for _, method := range s.Methods {\n        if method.ServerStreaming {\n            return true\n        }\n    }\n    return false\n}\n```"]}, "kitex-tool/internal_pkg/generator/generator.go-Unpack": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/tool/internal_pkg/generator/generator.go:\n```\n// Copyright 2021 CloudWeGo Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package generator .\npackage generator\n\nimport (\n\t\"fmt\"\n\t\"go/token\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/tool/internal_pkg/log\"\n\t\"github.com/cloudwego/kitex/tool/internal_pkg/tpl\"\n\t\"github.com/cloudwego/kitex/tool/internal_pkg/util\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Constants .\nconst (\n\tKitexGenPath = \"kitex_gen\"\n\tDefaultCodec = \"thrift\"\n\n\tBuildFileName       = \"build.sh\"\n\tBootstrapFileName   = \"bootstrap.sh\"\n\tToolVersionFileName = \"kitex_info.yaml\"\n\tHandlerFileName     = \"handler.go\"\n\tMainFileName        = \"main.go\"\n\tClientFileName      = \"client.go\"\n\tServerFileName      = \"server.go\"\n\tInvokerFileName     = \"invoker.go\"\n\tServiceFileName     = \"*service.go\"\n\tExtensionFilename   = \"extensions.yaml\"\n\n\tDefaultThriftPluginTimeLimit = time.Minute\n)\n\nvar (\n\tkitexImportPath = \"github.com/cloudwego/kitex\"\n\n\tglobalMiddlewares  []Middleware\n\tglobalDependencies = map[string]string{\n\t\t\"kitex\":     kitexImportPath,\n\t\t\"client\":    ImportPathTo(\"client\"),\n\t\t\"server\":    ImportPathTo(\"server\"),\n\t\t\"callopt\":   ImportPathTo(\"client/callopt\"),\n\t\t\"frugal\":    \"github.com/cloudwego/frugal\",\n\t\t\"fieldmask\": \"github.com/cloudwego/thriftgo/fieldmask\",\n\t}\n)\n\n// SetKitexImportPath sets the import path of kitex.\n// Must be called before generating code.\nfunc SetKitexImportPath(path string) {\n\tfor k, v := range globalDependencies {\n\t\tglobalDependencies[k] = strings.ReplaceAll(v, kitexImportPath, path)\n\t}\n\tkitexImportPath = path\n}\n\n// ImportPathTo returns an import path to the specified package under kitex.\nfunc ImportPathTo(pkg string) string {\n\treturn util.JoinPath(kitexImportPath, pkg)\n}\n\n// AddGlobalMiddleware adds middleware for all generators\nfunc AddGlobalMiddleware(mw Middleware) {\n\tglobalMiddlewares = append(globalMiddlewares, mw)\n}\n\n// AddGlobalDependency adds dependency for all generators\nfunc AddGlobalDependency(ref, path string) bool {\n\tif _, ok := globalDependencies[ref]; !ok {\n\t\tglobalDependencies[ref] = path\n\t\treturn true\n\t}\n\treturn false\n}\n\n// Generator generates the codes of main package and scripts for building a server based on kitex.\ntype Generator interface {\n\tGenerateService(pkg *PackageInfo) ([]*File, error)\n\tGenerateMainPackage(pkg *PackageInfo) ([]*File, error)\n\tGenerateCustomPackage(pkg *PackageInfo) ([]*File, error)\n}\n\n// Config .\ntype Config struct {\n\tVerbose               bool\n\tGenerateMain          bool // whether stuff in the main package should be generated\n\tGenerateInvoker       bool // generate main.go with invoker when main package generate\n\tVersion               string\n\tNoFastAPI             bool\n\tModuleName            string\n\tServiceName           string\n\tUse                   string\n\tIDLType               string\n\tIncludes              util.StringSlice\n\tThriftOptions         util.StringSlice\n\tProtobufOptions       util.StringSlice\n\tHessian2Options       util.StringSlice\n\tIDL                   string // the IDL file passed on the command line\n\tOutputPath            string // the output path for main pkg and kitex_gen\n\tPackagePrefix         string\n\tCombineService        bool // combine services to one service\n\tCopyIDL               bool\n\tThriftPlugins         util.StringSlice\n\tProtobufPlugins       util.StringSlice\n\tFeatures              []feature\n\tFrugalPretouch        bool\n\tThriftPluginTimeLimit time.Duration\n\tCompilerPath          string // specify the path of thriftgo or protoc\n\n\tExtensionFile string\n\ttmplExt       *TemplateExtension\n\n\tRecord    bool\n\tRecordCmd []string\n\n\tTemplateDir string\n\n\tGenPath string\n\n\tDeepCopyAPI           bool\n\tProtocol              string\n\tHandlerReturnKeepResp bool\n\n\tNoDependencyCheck bool\n}\n\n// Pack packs the Config into a slice of \"key=val\" strings.\nfunc (c *Config) Pack() (res []string) {\n\tt := reflect.TypeOf(c).Elem()\n\tv := reflect.ValueOf(c).Elem()\n\tfor i := 0; i < t.NumField(); i++ {\n\t\tf := t.Field(i)\n\t\tx := v.Field(i)\n\t\tn := f.Name\n\n\t\t// skip the plugin arguments to avoid the 'strings in strings' trouble\n\t\tif f.Name == \"ThriftPlugins\" || !token.IsExported(f.Name) {\n\t\t\tcontinue\n\t\t}\n\n\t\tif str, ok := x.Interface().(interface{ String() string }); ok {\n\t\t\tres = append(res, n+\"=\"+str.String())\n\t\t\tcontinue\n\t\t}\n\n\t\tswitch x.Kind() {\n\t\tcase reflect.Bool:\n\t\t\tres = append(res, n+\"=\"+fmt.Sprint(x.Bool()))\n\t\tcase reflect.String:\n\t\t\tres = append(res, n+\"=\"+x.String())\n\t\tcase reflect.Slice:\n\t\t\tvar ss []string\n\t\t\tif x.Type().Elem().Kind() == reflect.Int {\n\t\t\t\tfor i := 0; i < x.Len(); i++ {\n\t\t\t\t\tss = append(ss, strconv.Itoa(int(x.Index(i).Int())))\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfor i := 0; i < x.Len(); i++ {\n\t\t\t\t\tss = append(ss, x.Index(i).String())\n\t\t\t\t}\n\t\t\t}\n\t\t\tres = append(res, n+\"=\"+strings.Join(ss, \";\"))\n\t\tdefault:\n\t\t\tpanic(fmt.Errorf(\"unsupported field type: %+v\", f))\n\t\t}\n\t}\n\treturn res\n}\n\n// Unpack restores the Config from a slice of \"key=val\" strings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// AddFeature add registered feature to config\nfunc (c *Config) AddFeature(key string) bool {\n\tif f, ok := getFeature(key); ok {\n\t\tc.Features = append(c.Features, f)\n\t\treturn true\n\t}\n\treturn false\n}\n\n// ApplyExtension applies template extension.\nfunc (c *Config) ApplyExtension() error {\n\ttemplateExtExist := false\n\tpath := util.JoinPath(c.TemplateDir, ExtensionFilename)\n\tif c.TemplateDir != \"\" && util.Exists(path) {\n\t\ttemplateExtExist = true\n\t}\n\n\tif c.ExtensionFile == \"\" && !templateExtExist {\n\t\treturn nil\n\t}\n\n\text := new(TemplateExtension)\n\tif c.ExtensionFile != \"\" {\n\t\tif err := ext.FromYAMLFile(c.ExtensionFile); err != nil {\n\t\t\treturn fmt.Errorf(\"read template extension %q failed: %s\", c.ExtensionFile, err.Error())\n\t\t}\n\t}\n\n\tif templateExtExist {\n\t\tyamlExt := new(TemplateExtension)\n\t\tif err := yamlExt.FromYAMLFile(path); err != nil {\n\t\t\treturn fmt.Errorf(\"read template extension %q failed: %s\", path, err.Error())\n\t\t}\n\t\text.Merge(yamlExt)\n\t}\n\n\tfor _, fn := range ext.FeatureNames {\n\t\tRegisterFeature(fn)\n\t}\n\tfor _, fn := range ext.EnableFeatures {\n\t\tc.AddFeature(fn)\n\t}\n\tfor path, alias := range ext.Dependencies {\n\t\tAddGlobalDependency(alias, path)\n\t}\n\n\tc.tmplExt = ext\n\treturn nil\n}\n\n// NewGenerator .\nfunc NewGenerator(config *Config, middlewares []Middleware) Generator {\n\tmws := append(globalMiddlewares, middlewares...)\n\tg := &generator{Config: config, middlewares: mws}\n\tif g.IDLType == \"\" {\n\t\tg.IDLType = DefaultCodec\n\t}\n\treturn g\n}\n\n// Middleware used generator\ntype Middleware func(HandleFunc) HandleFunc\n\n// HandleFunc used generator\ntype HandleFunc func(*Task, *PackageInfo) (*File, error)\n\ntype generator struct {\n\t*Config\n\tmiddlewares []Middleware\n}\n\nfunc (g *generator) chainMWs(handle HandleFunc) HandleFunc {\n\tfor i := len(g.middlewares) - 1; i > -1; i-- {\n\t\thandle = g.middlewares[i](handle)\n\t}\n\treturn handle\n}\n\nfunc (g *generator) GenerateMainPackage(pkg *PackageInfo) (fs []*File, err error) {\n\tg.updatePackageInfo(pkg)\n\n\ttasks := []*Task{\n\t\t{\n\t\t\tName: BuildFileName,\n\t\t\tPath: util.JoinPath(g.OutputPath, BuildFileName),\n\t\t\tText: tpl.BuildTpl,\n\t\t},\n\t\t{\n\t\t\tName: BootstrapFileName,\n\t\t\tPath: util.JoinPath(g.OutputPath, \"script\", BootstrapFileName),\n\t\t\tText: tpl.BootstrapTpl,\n\t\t},\n\t\t{\n\t\t\tName: ToolVersionFileName,\n\t\t\tPath: util.JoinPath(g.OutputPath, ToolVersionFileName),\n\t\t\tText: tpl.ToolVersionTpl,\n\t\t},\n\t}\n\tif !g.Config.GenerateInvoker {\n\t\ttasks = append(tasks, &Task{\n\t\t\tName: MainFileName,\n\t\t\tPath: util.JoinPath(g.OutputPath, MainFileName),\n\t\t\tText: tpl.MainTpl,\n\t\t})\n\t}\n\tfor _, t := range tasks {\n\t\tif util.Exists(t.Path) {\n\t\t\tlog.Info(t.Path, \"exists. Skipped.\")\n\t\t\tcontinue\n\t\t}\n\t\tg.setImports(t.Name, pkg)\n\t\thandle := func(task *Task, pkg *PackageInfo) (*File, error) {\n\t\t\treturn task.Render(pkg)\n\t\t}\n\t\tf, err := g.chainMWs(handle)(t, pkg)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfs = append(fs, f)\n\t}\n\n\thandlerFilePath := filepath.Join(g.OutputPath, HandlerFileName)\n\tif util.Exists(handlerFilePath) {\n\t\tcomp := newCompleter(\n\t\t\tpkg.ServiceInfo.AllMethods(),\n\t\t\thandlerFilePath,\n\t\t\tpkg.ServiceInfo.ServiceName)\n\t\tf, err := comp.CompleteMethods()\n\t\tif err != nil {\n\t\t\tif err == errNoNewMethod {\n\t\t\t\treturn fs, nil\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\t\tfs = append(fs, f)\n\t} else {\n\t\ttask := Task{\n\t\t\tName: HandlerFileName,\n\t\t\tPath: handlerFilePath,\n\t\t\tText: tpl.HandlerTpl + \"\\n\" + tpl.HandlerMethodsTpl,\n\t\t}\n\t\tg.setImports(task.Name, pkg)\n\t\thandle := func(task *Task, pkg *PackageInfo) (*File, error) {\n\t\t\treturn task.Render(pkg)\n\t\t}\n\t\tf, err := g.chainMWs(handle)(&task, pkg)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfs = append(fs, f)\n\t}\n\treturn\n}\n\nfunc (g *generator) GenerateService(pkg *PackageInfo) ([]*File, error) {\n\tg.updatePackageInfo(pkg)\n\toutput := util.JoinPath(g.OutputPath, util.CombineOutputPath(g.GenPath, pkg.Namespace))\n\tsvcPkg := strings.ToLower(pkg.ServiceName)\n\toutput = util.JoinPath(output, svcPkg)\n\text := g.tmplExt\n\tif ext == nil {\n\t\text = new(TemplateExtension)\n\t}\n\n\ttasks := []*Task{\n\t\t{\n\t\t\tName: ClientFileName,\n\t\t\tPath: util.JoinPath(output, ClientFileName),\n\t\t\tText: tpl.ClientTpl,\n\t\t\tExt:  ext.ExtendClient,\n\t\t},\n\t\t{\n\t\t\tName: ServerFileName,\n\t\t\tPath: util.JoinPath(output, ServerFileName),\n\t\t\tText: tpl.ServerTpl,\n\t\t\tExt:  ext.ExtendServer,\n\t\t},\n\t\t{\n\t\t\tName: InvokerFileName,\n\t\t\tPath: util.JoinPath(output, InvokerFileName),\n\t\t\tText: tpl.InvokerTpl,\n\t\t\tExt:  ext.ExtendInvoker,\n\t\t},\n\t\t{\n\t\t\tName: ServiceFileName,\n\t\t\tPath: util.JoinPath(output, svcPkg+\".go\"),\n\t\t\tText: tpl.ServiceTpl,\n\t\t},\n\t}\n\n\tvar fs []*File\n\tfor _, t := range tasks {\n\t\tif err := t.Build(); err != nil {\n\t\t\terr = fmt.Errorf(\"build %s failed: %w\", t.Name, err)\n\t\t\treturn nil, err\n\t\t}\n\t\tg.setImports(t.Name, pkg)\n\t\tif t.Ext != nil {\n\t\t\tfor _, path := range t.Ext.ImportPaths {\n\t\t\t\tif alias, exist := ext.Dependencies[path]; exist {\n\t\t\t\t\tpkg.AddImports(alias)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\thandle := func(task *Task, pkg *PackageInfo) (*File, error) {\n\t\t\treturn task.Render(pkg)\n\t\t}\n\t\tf, err := g.chainMWs(handle)(t, pkg)\n\t\tif err != nil {\n\t\t\terr = fmt.Errorf(\"render %s failed: %w\", t.Name, err)\n\t\t\treturn nil, err\n\t\t}\n\t\tfs = append(fs, f)\n\t}\n\treturn fs, nil\n}\n\nfunc (g *generator) updatePackageInfo(pkg *PackageInfo) {\n\tpkg.NoFastAPI = g.NoFastAPI\n\tpkg.Codec = g.IDLType\n\tpkg.Version = g.Version\n\tpkg.RealServiceName = g.ServiceName\n\tpkg.Features = g.Features\n\tpkg.ExternalKitexGen = g.Use\n\tpkg.FrugalPretouch = g.FrugalPretouch\n\tpkg.Module = g.ModuleName\n\tif strings.EqualFold(g.Protocol, transport.HESSIAN2.String()) {\n\t\tpkg.Protocol = transport.HESSIAN2\n\t}\n\tif pkg.Dependencies == nil {\n\t\tpkg.Dependencies = make(map[string]string)\n\t}\n\n\tfor ref, path := range globalDependencies {\n\t\tif _, ok := pkg.Dependencies[ref]; !ok {\n\t\t\tpkg.Dependencies[ref] = path\n\t\t}\n\t}\n}\n\nfunc (g *generator) setImports(name string, pkg *PackageInfo) {\n\tpkg.Imports = make(map[string]map[string]bool)\n\tswitch name {\n\tcase ClientFileName:\n\t\tpkg.AddImports(\"client\")\n\t\tif pkg.HasStreaming {\n\t\t\tpkg.AddImport(\"streaming\", \"github.com/cloudwego/kitex/pkg/streaming\")\n\t\t\tpkg.AddImport(\"transport\", \"github.com/cloudwego/kitex/transport\")\n\t\t}\n\t\tif len(pkg.AllMethods()) > 0 {\n\t\t\tif needCallOpt(pkg) {\n\t\t\t\tpkg.AddImports(\"callopt\")\n\t\t\t}\n\t\t\tpkg.AddImports(\"context\")\n\t\t}\n\t\tfallthrough\n\tcase HandlerFileName:\n\t\tfor _, m := range pkg.ServiceInfo.AllMethods() {\n\t\t\tif !m.ServerStreaming && !m.ClientStreaming {\n\t\t\t\tpkg.AddImports(\"context\")\n\t\t\t}\n\t\t\tfor _, a := range m.Args {\n\t\t\t\tfor _, dep := range a.Deps {\n\t\t\t\t\tpkg.AddImport(dep.PkgRefName, dep.ImportPath)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !m.Void && m.Resp != nil {\n\t\t\t\tfor _, dep := range m.Resp.Deps {\n\t\t\t\t\tpkg.AddImport(dep.PkgRefName, dep.ImportPath)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\tcase ServerFileName, InvokerFileName:\n\t\tif len(pkg.CombineServices) == 0 {\n\t\t\tpkg.AddImport(pkg.ServiceInfo.PkgRefName, pkg.ServiceInfo.ImportPath)\n\t\t}\n\t\tpkg.AddImports(\"server\")\n\tcase ServiceFileName:\n\t\tpkg.AddImports(\"errors\")\n\t\tpkg.AddImports(\"client\")\n\t\tpkg.AddImport(\"kitex\", \"github.com/cloudwego/kitex/pkg/serviceinfo\")\n\t\tpkg.AddImport(pkg.ServiceInfo.PkgRefName, pkg.ServiceInfo.ImportPath)\n\t\tif len(pkg.AllMethods()) > 0 {\n\t\t\tpkg.AddImports(\"context\")\n\t\t}\n\t\tfor _, m := range pkg.ServiceInfo.AllMethods() {\n\t\t\tif m.ClientStreaming || m.ServerStreaming {\n\t\t\t\tpkg.AddImports(\"fmt\")\n\t\t\t}\n\t\t\tif m.GenArgResultStruct {\n\t\t\t\tpkg.AddImports(\"proto\")\n\t\t\t} else {\n\t\t\t\t// for method Arg and Result\n\t\t\t\tpkg.AddImport(m.PkgRefName, m.ImportPath)\n\t\t\t}\n\t\t\tfor _, a := range m.Args {\n\t\t\t\tfor _, dep := range a.Deps {\n\t\t\t\t\tpkg.AddImport(dep.PkgRefName, dep.ImportPath)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif m.Streaming.IsStreaming || pkg.Codec == \"protobuf\" {\n\t\t\t\t// protobuf handler support both PingPong and Unary (streaming) requests\n\t\t\t\tpkg.AddImport(\"streaming\", \"github.com/cloudwego/kitex/pkg/streaming\")\n\t\t\t}\n\t\t\tif !m.Void && m.Resp != nil {\n\t\t\t\tfor _, dep := range m.Resp.Deps {\n\t\t\t\t\tpkg.AddImport(dep.PkgRefName, dep.ImportPath)\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor _, e := range m.Exceptions {\n\t\t\t\tfor _, dep := range e.Deps {\n\t\t\t\t\tpkg.AddImport(dep.PkgRefName, dep.ImportPath)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif pkg.FrugalPretouch {\n\t\t\tpkg.AddImports(\"sync\")\n\t\t\tif len(pkg.AllMethods()) > 0 {\n\t\t\t\tpkg.AddImports(\"frugal\")\n\t\t\t\tpkg.AddImports(\"reflect\")\n\t\t\t}\n\t\t}\n\tcase MainFileName:\n\t\tpkg.AddImport(\"log\", \"log\")\n\t\tpkg.AddImport(pkg.PkgRefName, util.JoinPath(pkg.ImportPath, strings.ToLower(pkg.ServiceName)))\n\t}\n}\n\nfunc needCallOpt(pkg *PackageInfo) bool {\n\t// callopt is referenced only by non-streaming methods\n\tneedCallOpt := false\n\tswitch pkg.Codec {\n\tcase \"thrift\":\n\t\tfor _, m := range pkg.ServiceInfo.AllMethods() {\n\t\t\tif !m.Streaming.IsStreaming {\n\t\t\t\tneedCallOpt = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\tcase \"protobuf\":\n\t\tneedCallOpt = true\n\t}\n\treturn needCallOpt\n}\n\n```\n# Function Signature: func (c *Config) Unpack(args []string) error {\n# Natural Language Annotations: // Unpack restores the Config from a slice of \"key=val\" strings.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (c *Config) Unpack(args []string) error {\n\tif len(args) == 0 {\n\t\treturn ErrNothingToUnpack\n\t}\n\tfor _, arg := range args {\n\t\tpairs := strings.Split(arg, \"=\")\n\t\tif len(pairs) != 2 {\n\t\t\treturn fmt.Errorf(\"invalid argument %q\", arg)\n\t\t}\n\t\tvar err error\n\t\tkey, val := pairs[0], pairs[1]\n\t\tswitch key {\n\t\tcase \"Verbose\":\n\t\t\tc.Verbose, err = strconv.ParseBool(val)\n\t\tcase \"GenerateMain\":\n\t\t\tc.GenerateMain, err = strconv.ParseBool(val)\n\t\tcase \"GenerateInvoker\":\n\t\t\tc.GenerateInvoker, err = strconv.ParseBool(val)\n\t\tcase \"Version\":\n\t\t\tc.Version = val\n\t\tcase \"NoFastAPI\":\n\t\t\tc.NoFastAPI, err = strconv.ParseBool(val)\n\t\tcase \"ModuleName\":\n\t\t\tc.ModuleName = val\n\t\tcase \"ServiceName\":\n\t\t\tc.ServiceName = val\n\t\tcase \"Use\":\n\t\t\tc.Use = val\n\t\tcase \"IDLType\":\n\t\t\tc.IDLType = val\n\t\tcase \"Includes\":\n\t\t\tc.Includes = strings.Split(val, \";\")\n\t\tcase \"ThriftOptions\":\n\t\t\tc.ThriftOptions = strings.Split(val, \";\")\n\t\tcase \"ProtobufOptions\":\n\t\t\tc.ProtobufOptions = strings.Split(val, \";\")\n\t\tcase \"Hessian2Options\":\n\t\t\tc.Hessian2Options = strings.Split(val, \";\")\n\t\tcase \"IDL\":\n\t\t\tc.IDL = val\n\t\tcase \"OutputPath\":\n\t\t\tc.OutputPath = val\n\t\tcase \"PackagePrefix\":\n\t\t\tc.PackagePrefix = val\n\t\tcase \"CombineService\":\n\t\t\tc.CombineService, err = strconv.ParseBool(val)\n\t\tcase \"CopyIDL\":\n\t\t\tc.CopyIDL, err = strconv.ParseBool(val)\n\t\tcase \"ThriftPlugins\":\n\t\t\tc.ThriftPlugins = strings.Split(val, \";\")\n\t\tcase \"ProtobufPlugins\":\n\t\t\tc.ProtobufPlugins = strings.Split(val, \";\")\n\t\tcase \"Features\":\n\t\t\tc.Features = strings.Split(val, \",\")\n\t\tcase \"FrugalPretouch\":\n\t\t\tc.FrugalPretouch, err = strconv.ParseBool(val)\n\t\tcase \"ThriftPluginTimeLimit\":\n\t\t\tc.ThriftPluginTimeLimit, err = time.ParseDuration(val)\n\t\tcase \"CompilerPath\":\n\t\t\tc.CompilerPath = val\n\t\tcase \"ExtensionFile\":\n\t\t\tc.ExtensionFile = val\n\t\tcase \"Record\":\n\t\t\tc.Record, err = strconv.ParseBool(val)\n\t\tcase \"RecordCmd\":\n\t\t\tc.RecordCmd = strings.Split(val, \";\")\n\t\tcase \"TemplateDir\":\n\t\t\tc.TemplateDir = val\n\t\tcase \"GenPath\":\n\t\t\tc.GenPath = val\n\t\tcase \"DeepCopyAPI\":\n\t\t\tc.DeepCopyAPI, err = strconv.ParseBool(val)\n\t\tcase \"Protocol\":\n\t\t\tc.Protocol = val\n\t\tcase \"HandlerReturnKeepResp\":\n\t\t\tc.HandlerReturnKeepResp, err = strconv.ParseBool(val)\n\t\tcase \"NoDependencyCheck\":\n\t\t\tc.NoDependencyCheck, err = strconv.ParseBool(val)\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"unknown key: %s\", key)\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n```", "  ```\nfunc (c *Config) Unpack(args []string) error {\n\tt := reflect.TypeOf(c).Elem()\n\tv := reflect.ValueOf(c).Elem()\n\tfor _, arg := range args {\n\t\tkv := strings.Split(arg, \"=\")\n\t\tif len(kv) != 2 {\n\t\t\treturn fmt.Errorf(\"invalid argument: %q\", arg)\n\t\t}\n\t\tk, val := kv[0], kv[1]\n\t\tf, ok := t.FieldByName(k)\n\t\tif !ok {\n\t\t\treturn fmt.Errorf(\"invalid field name: %q\", k)\n\t\t}\n\t\tx := v.FieldByName(k)\n\t\tswitch x.Kind() {\n\t\tcase reflect.Bool:\n\t\t\tif val == \"\" {\n\t\t\t\tx.SetBool(false)\n\t\t\t} else {\n\t\t\t\tsetBool(x, val)\n\t\t\t}\n\t\tcase reflect.String:\n\t\t\tx.SetString(val)\n\t\tcase reflect.Slice:\n\t\t\tx.Set(reflect.ValueOf(strings.Split(val, \";\")))\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"unsupported field type: %+v\", f)\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc setBool(x reflect.Value, val string) {\n\tb, err := strconv.ParseBool(val)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tx.SetBool(b)\n}\n```", "  ```\nfunc (c *Config) Unpack(args []string) error {\n\tfor _, arg := range args {\n\t\tkv := strings.Split(arg, \"=\")\n\t\tif len(kv) != 2 {\n\t\t\treturn fmt.Errorf(\"invalid key-val pair: %s\", arg)\n\t\t}\n\t\tswitch strings.ToLower(kv[0]) {\n\t\tcase \"verbose\":\n\t\t\tv, err := strconv.ParseBool(kv[1])\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"invalid verbose value: %s\", kv[1])\n\t\t\t}\n\t\t\tc.Verbose = v\n\t\tcase \"generatemain\":\n\t\t\tv, err := strconv.ParseBool(kv[1])\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"invalid generatemain value: %s\", kv[1])\n\t\t\t}\n\t\t\tc.GenerateMain = v\n\t\tcase \"generateinvoker\":\n\t\t\tv, err := strconv.ParseBool(kv[1])\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"invalid generateinvoker value: %s\", kv[1])\n\t\t\t}\n\t\t\tc.GenerateInvoker = v\n\t\tcase \"version\":\n\t\t\tc.Version = kv[1]\n\t\tcase \"nofastaapi\":\n\t\t\tv, err := strconv.ParseBool(kv[1])\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"invalid nofastaapi value: %s\", kv[1])\n\t\t\t}\n\t\t\tc.NoFastAPI = v\n\t\tcase \"modulename\":\n\t\t\tc.ModuleName = kv[1]\n\t\tcase \"servicename\":\n\t\t\tc.ServiceName = kv[1]\n\t\tcase \"use\":\n\t\t\tc.Use = kv[1]\n\t\tcase \"idltype\":\n\t\t\tc.IDLType = kv[1]\n\t\tcase \"includes\":\n\t\t\tc.Includes.Set(strings.Split(kv[1], \";\")...)\n\t\tcase \"thriftoptions\":\n\t\t\tc.ThriftOptions.Set(strings.Split(kv[1], \";\")...)\n\t\tcase \"protobufoptions\":\n\t\t\tc.ProtobufOptions.Set(strings.Split(kv[1], \";\")...)\n\t\tcase \"hessian2options\":\n\t\t\tc.Hessian2Options.Set(strings.Split(kv[1], \";\")...)\n\t\tcase \"idl\":\n\t\t\tc.IDL = kv[1]\n\t\tcase \"outputpath\":\n\t\t\tc.OutputPath = kv[1]\n\t\tcase \"packageprefix\":\n\t\t\tc.PackagePrefix = kv[1]\n\t\tcase \"combineservice\":\n\t\t\tv, err := strconv.ParseBool(kv[1])\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"invalid combineservice value: %s\", kv[1])\n\t\t\t}\n\t\t\tc.CombineService = v\n\t\tcase \"copyidl\":\n\t\t\tv, err := strconv.ParseBool(kv[1])\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"invalid copyidl value: %s\", kv[1])\n\t\t\t}\n\t\t\tc.CopyIDL = v\n\t\tcase \"thriftplugins\":\n\t\t\tc.ThriftPlugins.Set(strings.Split(kv[1], \";\")...)\n\t\tcase \"protoplugins\":\n\t\t\tc.ProtobufPlugins.Set(strings.Split(kv[1], \";\")...)\n\t\tcase \"features\":\n\t\t\tfs := strings.Split(kv[1], \";\")\n\t\t\tc.Features = make([]Feature, 0, len(fs))\n\t\t\tfor _, f := range fs {\n\t\t\t\tif fn, ok := getFeature(f); ok {\n\t\t\t\t\tc.Features = append(c.Features, fn)\n\t\t\t\t}\n"]}, "kitex-tool/internal_pkg/util/util.go-CombineOutputPath": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/tool/internal_pkg/util/util.go:\n```\n// Copyright 2021 CloudWeGo Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage util\n\nimport (\n\t\"fmt\"\n\t\"go/build\"\n\t\"go/format\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"os\"\n\t\"os/exec\"\n\t\"os/user\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"runtime\"\n\t\"sort\"\n\t\"strings\"\n\t\"unicode\"\n)\n\n// StringSlice implements the flag.Value interface on string slices\n// to allow a flag to be set multiple times.\ntype StringSlice []string\n\nfunc (ss *StringSlice) String() string {\n\treturn fmt.Sprintf(\"%v\", *ss)\n}\n\n// Set implements the flag.Value interface.\nfunc (ss *StringSlice) Set(value string) error {\n\t*ss = append(*ss, value)\n\treturn nil\n}\n\n// FormatCode formats go source codes.\nfunc FormatCode(code []byte) ([]byte, error) {\n\tformatCode, err := format.Source(code)\n\tif err != nil {\n\t\treturn code, fmt.Errorf(\"format code error: %s\", err)\n\t}\n\treturn formatCode, nil\n}\n\n// GetGOPATH retrieves the GOPATH from environment variables or the `go env` command.\nfunc GetGOPATH() (string, error) {\n\tgoPath := os.Getenv(\"GOPATH\")\n\t// If there are many path in GOPATH, pick up the first one.\n\tif GoPaths := strings.Split(goPath, \":\"); len(GoPaths) >= 1 && strings.TrimSpace(GoPaths[0]) != \"\" {\n\t\treturn strings.TrimSpace(GoPaths[0]), nil\n\t}\n\t// GOPATH not set through environment variables, try to get one by executing \"go env GOPATH\"\n\toutput, err := exec.Command(\"go\", \"env\", \"GOPATH\").Output()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tgoPath = strings.TrimSpace(string(output))\n\tif len(goPath) == 0 {\n\t\tbuildContext := build.Default\n\t\tgoPath = buildContext.GOPATH\n\t}\n\n\tif len(goPath) == 0 {\n\t\treturn \"\", fmt.Errorf(\"GOPATH not found\")\n\t}\n\treturn goPath, nil\n}\n\n// Exists reports whether a file exists.\nfunc Exists(path string) bool {\n\tfi, err := os.Stat(path)\n\tif err != nil {\n\t\treturn os.IsExist(err)\n\t}\n\treturn !fi.IsDir()\n}\n\n// LowerFirst converts the first letter to upper case for the given string.\nfunc LowerFirst(s string) string {\n\trs := []rune(s)\n\trs[0] = unicode.ToLower(rs[0])\n\treturn string(rs)\n}\n\n// ReplaceString be used in string substitution.\nfunc ReplaceString(s, old, new string, n int) string {\n\treturn strings.Replace(s, old, new, n)\n}\n\n// SnakeString converts the string 's' to a snake string\nfunc SnakeString(s string) string {\n\tdata := make([]byte, 0, len(s)*2)\n\tj := false\n\tfor _, d := range []byte(s) {\n\t\tif d >= 'A' && d <= 'Z' {\n\t\t\tif j {\n\t\t\t\tdata = append(data, '_')\n\t\t\t\tj = false\n\t\t\t}\n\t\t} else if d != '_' {\n\t\t\tj = true\n\t\t}\n\t\tdata = append(data, d)\n\t}\n\treturn strings.ToLower(string(data))\n}\n\n// UpperFirst converts the first letter to upper case for the given string.\nfunc UpperFirst(s string) string {\n\trs := []rune(s)\n\trs[0] = unicode.ToUpper(rs[0])\n\treturn string(rs)\n}\n\n// NotPtr converts an pointer type into non-pointer type.\nfunc NotPtr(s string) string {\n\treturn strings.ReplaceAll(s, \"*\", \"\")\n}\n\n// SearchGoMod searches go.mod from the given directory (which must be an absolute path) to\n// the root directory. When the go.mod is found, its module name and path will be returned.\nfunc SearchGoMod(cwd string) (moduleName, path string, found bool) {\n\tfor {\n\t\tpath = filepath.Join(cwd, \"go.mod\")\n\t\tdata, err := ioutil.ReadFile(path)\n\t\tif err == nil {\n\t\t\tre := regexp.MustCompile(`^\\s*module\\s+(\\S+)\\s*`)\n\t\t\tfor _, line := range strings.Split(string(data), \"\\n\") {\n\t\t\t\tm := re.FindStringSubmatch(line)\n\t\t\t\tif m != nil {\n\t\t\t\t\treturn m[1], cwd, true\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn fmt.Sprintf(\"<module name not found in '%s'>\", path), path, true\n\t\t}\n\n\t\tif !os.IsNotExist(err) {\n\t\t\treturn\n\t\t}\n\t\tparentCwd := filepath.Dir(cwd)\n\t\tif parentCwd == cwd {\n\t\t\tbreak\n\t\t}\n\t\tcwd = parentCwd\n\t}\n\treturn\n}\n\nfunc RunGitCommand(gitLink string) (string, string, error) {\n\tu, err := user.Current()\n\tif err != nil {\n\t\treturn \"\", \"Failed to get home dir\", err\n\t}\n\tcachePath := JoinPath(u.HomeDir, \".kitex\", \"cache\")\n\n\tbranch := \"\"\n\tif strings.Contains(gitLink, \".git@\") {\n\t\tstrs := strings.Split(gitLink, \".git@\")\n\t\tbranch = strs[1]\n\t\tgitLink = strs[0] + \".git\"\n\t}\n\tpullLink := gitLink\n\n\tgitLink = strings.TrimPrefix(gitLink, \"git@\")\n\n\tgitLink = strings.TrimSuffix(gitLink, \".git\")\n\n\trepoLink := \"\"\n\tif strings.Contains(gitLink, \"://\") {\n\t\trepoLink = strings.Split(gitLink, \"://\")[1]\n\t} else {\n\t\trepoLink = strings.ReplaceAll(gitLink, \":\", \"/\")\n\t}\n\n\tbranchSuffix := \"\"\n\tif branch != \"\" {\n\t\tbranchSuffix = \"@\" + branch\n\t}\n\tgitPath := JoinPath(cachePath, repoLink+branchSuffix)\n\n\t_, err = os.Stat(JoinPath(gitPath, \".git\"))\n\tif err != nil && !os.IsExist(err) {\n\t\terr = os.MkdirAll(gitPath, os.ModePerm)\n\t\tif err != nil {\n\t\t\treturn \"\", \"Failed to create cache directory,please check your permission for ~/.kitex/cache\", err\n\t\t}\n\t\tcmdClone := exec.Command(\"git\", \"clone\", pullLink, \".\")\n\t\tcmdClone.Dir = gitPath\n\t\tout, gitErr := cmdClone.CombinedOutput()\n\t\tif gitErr != nil {\n\t\t\treturn \"\", string(out), gitErr\n\t\t}\n\t\tif branch != \"\" {\n\t\t\tcmdCheckout := exec.Command(\"git\", \"checkout\", branch)\n\t\t\tcmdCheckout.Dir = gitPath\n\t\t\tout, gitErr = cmdCheckout.CombinedOutput()\n\t\t\treturn gitPath, string(out), gitErr\n\t\t} else {\n\t\t\treturn gitPath, \"\", nil\n\t\t}\n\t}\n\n\tcmdPull := exec.Command(\"git\", \"pull\")\n\tcmdPull.Dir = gitPath\n\tout, gitErr := cmdPull.CombinedOutput()\n\tif gitErr != nil {\n\t\treturn \"\", string(out), gitErr\n\t}\n\n\treturn gitPath, \"\", nil\n}\n\n// CombineOutputPath read the output and path variables and render them into the final path\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// JoinPath joins dirs as golang import format, such as xx/xx/xx\nfunc JoinPath(elem ...string) string {\n\tif runtime.GOOS == \"windows\" {\n\t\treturn strings.ReplaceAll(filepath.Join(elem...), \"\\\\\", \"/\")\n\t}\n\treturn filepath.Join(elem...)\n}\n\n// DownloadFile Download file to local\nfunc DownloadFile(remotePath, localPath string) error {\n\tresp, err := http.Get(remotePath)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer resp.Body.Close()\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn fmt.Errorf(\"failed to download file, http status: %s\", resp.Status)\n\t}\n\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = ioutil.WriteFile(localPath, body, 0o644)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// IDLName returns the name of the IDL file.\nfunc IDLName(filename string) string {\n\treturn filepath.Base(filename)\n}\n\ntype Import struct {\n\tAlias string\n\tPath  string\n}\n\nfunc SortImports(imps map[string]string, localPrefix string) (ret []Import) {\n\tstds := make([]Import, 0, len(imps))\n\tlocals := make([]Import, 0, len(imps))\n\tthirds := make([]Import, 0, len(imps))\n\tfor path, alias := range imps {\n\t\tif strings.HasPrefix(path, localPrefix+\"/\") {\n\t\t\tlocals = append(locals, Import{alias, path})\n\t\t} else if !strings.Contains(path, \".\") {\n\t\t\tstds = append(stds, Import{alias, path})\n\t\t} else {\n\t\t\tthirds = append(thirds, Import{alias, path})\n\t\t}\n\t}\n\n\tsort.SliceStable(stds, func(i, j int) bool {\n\t\treturn stds[i].Path < stds[j].Path\n\t})\n\tret = append(ret, stds...)\n\tif len(thirds) > 0 {\n\t\tret = append(ret, Import{\"\", \"\"})\n\t}\n\tsort.SliceStable(thirds, func(i, j int) bool {\n\t\treturn thirds[i].Path < thirds[j].Path\n\t})\n\tret = append(ret, thirds...)\n\tif len(locals) > 0 {\n\t\tret = append(ret, Import{\"\", \"\"})\n\t}\n\tsort.SliceStable(locals, func(i, j int) bool {\n\t\treturn locals[i].Path < locals[j].Path\n\t})\n\tret = append(ret, locals...)\n\treturn ret\n}\n\nfunc (i Import) PackageName() string {\n\tif i.Alias != \"\" {\n\t\treturn i.Alias\n\t} else {\n\t\treturn strings.ToLower(filepath.Base(i.Path))\n\t}\n}\n\nfunc PrintlImports(imports []Import) string {\n\tbuilder := strings.Builder{}\n\tfor _, v := range imports {\n\t\tif v.Path != \"\" {\n\t\t\tbuilder.WriteString(fmt.Sprintf(\"%s %q\\n\", v.Alias, v.Path))\n\t\t} else {\n\t\t\tbuilder.WriteString(\"\\n\")\n\t\t}\n\t}\n\treturn builder.String()\n}\n\n```\n# Function Signature: func CombineOutputPath(outputPath, ns string) string {\n# Natural Language Annotations: // CombineOutputPath read the output and path variables and render them into the final path\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc CombineOutputPath(outputPath, ns string) string {\n\treturn filepath.Join(outputPath, ns)\n}\n```", "  ```\nfunc CombineOutputPath(outputPath, ns string) string {\n\treturn path.Join(outputPath, ns)\n}\n```", "  ```\nfunc CombineOutputPath(outputPath, ns string) string {\n\tvar result strings.Builder\n\tresult.WriteString(outputPath)\n\tif ns != \"\" {\n\t\tresult.WriteString(\":\")\n\t\tresult.WriteString(ns)\n\t}\n\treturn result.String()\n}\n```"]}, "kitex-pkg/rpcinfo/stats_util.go-CalcEventCostUs": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/rpcinfo/stats_util.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *  http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage rpcinfo\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"runtime/debug\"\n\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n)\n\n// Record records the event to RPCStats.\nfunc Record(ctx context.Context, ri RPCInfo, event stats.Event, err error) {\n\tif ctx == nil || ri.Stats() == nil {\n\t\treturn\n\t}\n\tif err != nil {\n\t\tri.Stats().Record(ctx, event, stats.StatusError, err.Error())\n\t} else {\n\t\tri.Stats().Record(ctx, event, stats.StatusInfo, \"\")\n\t}\n}\n\n// CalcEventCostUs calculates the duration between start and end and returns in microsecond.\n\n\n\n\n\n\n\n// ClientPanicToErr to transform the panic info to error, and output the error if needed.\nfunc ClientPanicToErr(ctx context.Context, panicInfo interface{}, ri RPCInfo, logErr bool) error {\n\te := fmt.Errorf(\"KITEX: client panic, to_service=%s to_method=%s error=%v\\nstack=%s\",\n\t\tri.To().ServiceName(), ri.To().Method(), panicInfo, debug.Stack())\n\trpcStats := AsMutableRPCStats(ri.Stats())\n\trpcStats.SetPanicked(e)\n\tif logErr {\n\t\tklog.CtxErrorf(ctx, \"%s\", e.Error())\n\t}\n\treturn e\n}\n\n```\n# Function Signature: func CalcEventCostUs(start, end Event) uint64 {\n# Natural Language Annotations: // CalcEventCostUs calculates the duration between start and end and returns in microsecond.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc CalcEventCostUs(start, end Event) uint64 {\n\treturn uint64(end.UnixNano() - start.UnixNano()) / 1000\n}\n```", "  ```\nfunc CalcEventCostUs(start, end Event) uint64 {\n\t// CalcEventCostUs calculates the duration between start and end and returns in microsecond.\n\treturn (end.TransportEvent().Timestamp() - start.TransportEvent().Timestamp()) / time.Microsecond\n}\n```", "  ```\nfunc CalcEventCostUs(start, end Event) uint64 {\n\treturn uint64((end.time-start.time)/time.Microsecond) - uint64(start.costTotal-end.costTotal)\n}\n```\nThis function calculates the duration between two events (start and end) and returns the duration in microseconds. It uses the `time.Microsecond` constant to convert the duration between the start and end events to microseconds, and then subtracts the total cost of any previous events (start.costTotal and end.costTotal) to get the total cost of the current event. The return value is a uint64 type representing the duration in microseconds."]}, "kitex-pkg/rpcinfo/invocation.go-NewInvocation": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/rpcinfo/invocation.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage rpcinfo\n\nimport (\n\t\"sync\"\n\t\"sync/atomic\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\nvar (\n\t_              Invocation       = (*invocation)(nil)\n\t_              InvocationSetter = (*invocation)(nil)\n\tinvocationPool sync.Pool\n\tglobalSeqID    int32 = 0\n)\n\nfunc init() {\n\tinvocationPool.New = newInvocation\n}\n\n// InvocationSetter is used to set information about an RPC.\ntype InvocationSetter interface {\n\tSetPackageName(name string)\n\tSetServiceName(name string)\n\tSetMethodName(name string)\n\tSetSeqID(seqID int32)\n\tSetBizStatusErr(err kerrors.BizStatusErrorIface)\n\tSetExtra(key string, value interface{})\n\tReset()\n}\ntype invocation struct {\n\tpackageName string\n\tserviceName string\n\tmethodName  string\n\tseqID       int32\n\tbizErr      kerrors.BizStatusErrorIface\n\textra       map[string]interface{}\n}\n\n// NewInvocation creates a new Invocation with the given service, method and optional package.\n\n\n\n\n\n\n\n\n\n\n\n// NewServerInvocation to get Invocation for new request in server side\nfunc NewServerInvocation() Invocation {\n\tivk := invocationPool.Get().(*invocation)\n\treturn ivk\n}\n\nfunc genSeqID() int32 {\n\tid := atomic.AddInt32(&globalSeqID, 1)\n\tif id == 0 {\n\t\t// seqID is non-0 to avoid potential default value judgments leading to error handling\n\t\tid = atomic.AddInt32(&globalSeqID, 1)\n\t}\n\treturn id\n}\n\nfunc newInvocation() interface{} {\n\treturn &invocation{}\n}\n\n// SeqID implements the Invocation interface.\nfunc (i *invocation) SeqID() int32 {\n\treturn i.seqID\n}\n\n// SetSeqID implements the InvocationSetter interface.\nfunc (i *invocation) SetSeqID(seqID int32) {\n\ti.seqID = seqID\n}\n\nfunc (i *invocation) PackageName() string {\n\treturn i.packageName\n}\n\nfunc (i *invocation) SetPackageName(name string) {\n\ti.packageName = name\n}\n\nfunc (i *invocation) ServiceName() string {\n\treturn i.serviceName\n}\n\n// SetServiceName implements the InvocationSetter interface.\nfunc (i *invocation) SetServiceName(name string) {\n\ti.serviceName = name\n}\n\n// MethodName implements the Invocation interface.\nfunc (i *invocation) MethodName() string {\n\treturn i.methodName\n}\n\n// SetMethodName implements the InvocationSetter interface.\nfunc (i *invocation) SetMethodName(name string) {\n\ti.methodName = name\n}\n\n// BizStatusErr implements the Invocation interface.\nfunc (i *invocation) BizStatusErr() kerrors.BizStatusErrorIface {\n\treturn i.bizErr\n}\n\n// SetBizStatusErr implements the InvocationSetter interface.\nfunc (i *invocation) SetBizStatusErr(err kerrors.BizStatusErrorIface) {\n\ti.bizErr = err\n}\n\nfunc (i *invocation) SetExtra(key string, value interface{}) {\n\tif i.extra == nil {\n\t\ti.extra = map[string]interface{}{}\n\t}\n\ti.extra[key] = value\n}\n\nfunc (i *invocation) Extra(key string) interface{} {\n\tif i.extra == nil {\n\t\treturn nil\n\t}\n\treturn i.extra[key]\n}\n\n// Reset implements the InvocationSetter interface.\nfunc (i *invocation) Reset() {\n\ti.zero()\n}\n\n// Recycle reuses the invocation.\nfunc (i *invocation) Recycle() {\n\ti.zero()\n\tinvocationPool.Put(i)\n}\n\nfunc (i *invocation) zero() {\n\ti.seqID = 0\n\ti.packageName = \"\"\n\ti.serviceName = \"\"\n\ti.methodName = \"\"\n\ti.bizErr = nil\n\tfor key := range i.extra {\n\t\tdelete(i.extra, key)\n\t}\n}\n\n```\n# Function Signature: func NewInvocation(service, method string, pkgOpt ...string) *invocation {\n# Natural Language Annotations: // NewInvocation creates a new Invocation with the given service, method and optional package.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewInvocation(service, method string, pkgOpt ...string) *invocation {\n\tivk := invocationPool.Get().(*invocation)\n\tivk.SetServiceName(service)\n\tivk.SetMethodName(method)\n\n\tif len(pkgOpt) > 0 {\n\t\tivk.SetPackageName(pkgOpt[0])\n\t}\n\n\treturn ivk\n}\n```", "  ```\n// NewInvocation creates a new Invocation with the given service, method and optional package.\nfunc NewInvocation(service, method string, pkgOpt ...string) *invocation {\n\tinv := invocationPool.Get().(*invocation)\n\tinv.seqID = genSeqID()\n\tinv.packageName = pkgOpt[0]\n\tinv.serviceName = service\n\tinv.methodName = method\n\treturn inv\n}\n```", "  ```\nfunc NewInvocation(service, method string, pkgOpt ...string) *invocation {\n    // allocate a new invocation object from the pool\n    ivk := invocationPool.Get().(*invocation)\n    // set the service and method names\n    ivk.serviceName = service\n    ivk.methodName = method\n    // set the package name if provided\n    if len(pkgOpt) > 0 {\n        ivk.packageName = pkgOpt[0]\n    }\n    // set the sequence ID\n    ivk.seqID = genSeqID()\n    // set the extra map to an empty map\n    ivk.extra = make(map[string]interface{})\n    return ivk\n}\n```"]}, "kitex-pkg/rpcinfo/tracer.go-DoFinish": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/rpcinfo/tracer.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage rpcinfo\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"runtime/debug\"\n\n\t\"github.com/cloudwego/kitex/internal\"\n\t\"github.com/cloudwego/kitex/internal/stream\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n)\n\n// StreamEventReporter should be implemented by any tracer that wants to report stream events\ntype StreamEventReporter interface {\n\t// ReportStreamEvent is for collecting Recv/Send events on stream\n\t// NOTE: The callee should NOT hold references to event, which may be recycled later\n\tReportStreamEvent(ctx context.Context, ri RPCInfo, event Event)\n}\n\n// TraceController controls tracers.\ntype TraceController struct {\n\ttracers              []stats.Tracer\n\tstreamEventReporters []StreamEventReporter\n}\n\n// Append appends a new tracer to the controller.\nfunc (c *TraceController) Append(col stats.Tracer) {\n\tc.tracers = append(c.tracers, col)\n\tif reporter, ok := col.(StreamEventReporter); ok {\n\t\tc.streamEventReporters = append(c.streamEventReporters, reporter)\n\t}\n}\n\n// DoStart starts the tracers.\nfunc (c *TraceController) DoStart(ctx context.Context, ri RPCInfo) context.Context {\n\tdefer c.tryRecover(ctx)\n\tRecord(ctx, ri, stats.RPCStart, nil)\n\n\tfor _, col := range c.tracers {\n\t\tctx = col.Start(ctx)\n\t}\n\treturn ctx\n}\n\n// DoFinish calls the tracers in reversed order.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc buildStreamingEvent(statsEvent stats.Event, err error) Event {\n\tif err == nil || err == io.EOF {\n\t\treturn NewEvent(statsEvent, stats.StatusInfo, \"\")\n\t} else {\n\t\treturn NewEvent(statsEvent, stats.StatusError, err.Error())\n\t}\n}\n\n// ReportStreamEvent is for collecting Recv/Send events on stream\nfunc (c *TraceController) ReportStreamEvent(ctx context.Context, statsEvent stats.Event, err error) {\n\tif !c.HasStreamEventReporter() {\n\t\treturn\n\t}\n\tdefer c.tryRecover(ctx)\n\tevent := buildStreamingEvent(statsEvent, err)\n\tdefer func() {\n\t\tif recyclable, ok := event.(internal.Reusable); ok {\n\t\t\trecyclable.Recycle()\n\t\t}\n\t}()\n\t// RPCInfo is likely to be used by each reporter\n\tri := GetRPCInfo(ctx)\n\tfor i := len(c.streamEventReporters) - 1; i >= 0; i-- {\n\t\tc.streamEventReporters[i].ReportStreamEvent(ctx, ri, event)\n\t}\n}\n\n// GetStreamEventHandler returns the stream event handler\n// If there's no StreamEventReporter, nil is returned for client/server to skip adding tracing middlewares\nfunc (c *TraceController) GetStreamEventHandler() stream.StreamEventHandler {\n\tif c.HasStreamEventReporter() {\n\t\treturn c.ReportStreamEvent\n\t}\n\treturn nil\n}\n\n// HasTracer reports whether there exists any tracer.\nfunc (c *TraceController) HasTracer() bool {\n\treturn c != nil && len(c.tracers) > 0\n}\n\n// HasStreamEventReporter reports whether there exists any StreamEventReporter.\nfunc (c *TraceController) HasStreamEventReporter() bool {\n\treturn c != nil && len(c.streamEventReporters) > 0\n}\n\nfunc (c *TraceController) tryRecover(ctx context.Context) {\n\tif err := recover(); err != nil {\n\t\tklog.CtxWarnf(ctx, \"Panic happened during tracer call. This doesn't affect the rpc call, but may lead to lack of monitor data such as metrics and logs: error=%s, stack=%s\", err, string(debug.Stack()))\n\t}\n}\n\n```\n# Function Signature: func (c *TraceController) DoFinish(ctx context.Context, ri RPCInfo, err error) {\n# Natural Language Annotations: // DoFinish calls the tracers in reversed order.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (c *TraceController) DoFinish(ctx context.Context, ri RPCInfo, err error) {\n\tdefer c.tryRecover(ctx)\n\n\tRecord(ctx, ri, stats.RPCFinish, err)\n\n\tfor i := len(c.tracers) - 1; i >= 0; i-- {\n\t\tc.tracers[i].Finish(ctx, ri, err)\n\t}\n}\n```", "  ```\nfunc (c *TraceController) DoFinish(ctx context.Context, ri RPCInfo, err error) {\n\t// DoFinish calls the tracers in reversed order.\n\tdefer c.tryRecover(ctx)\n\tfor i := len(c.tracers) - 1; i >= 0; i-- {\n\t\tc.tracers[i].Finish(ctx, err)\n\t}\n\tRecord(ctx, ri, stats.RPCFinish, err)\n}\n```", "  ```\nfunc (c *TraceController) DoFinish(ctx context.Context, ri RPCInfo, err error) {\n\tdefer c.tryRecover(ctx)\n\n\tfor i := len(c.tracers) - 1; i >= 0; i-- {\n\t\tc.tracers[i].Finish(ctx, ri, err)\n\t}\n\n\tif err != nil {\n\t\tRecord(ctx, ri, stats.RPCFinish, err)\n\t} else {\n\t\tRecord(ctx, ri, stats.RPCFinish, nil)\n\t}\n}\n```"]}, "kitex-pkg/retry/retryer.go-InitWithPolicies": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/retry/retryer.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package retry implements rpc retry\npackage retry\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/bytedance/gopkg/cloud/circuitbreaker\"\n\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\n// RPCCallFunc is the definition with wrap rpc call\ntype RPCCallFunc func(context.Context, Retryer) (rpcinfo rpcinfo.RPCInfo, resp interface{}, err error)\n\n// GenRetryKeyFunc to generate retry key through rpcinfo.\n// You can customize the config key according to your config center.\ntype GenRetryKeyFunc func(ctx context.Context, ri rpcinfo.RPCInfo) string\n\n// Retryer is the interface for Retry implements\ntype Retryer interface {\n\t// AllowRetry to check if current request satisfy retry condition[eg: circuit, retry times == 0, chain stop, ddl].\n\t// If not satisfy won't execute Retryer.Do and return the reason message\n\t// Execute anyway for the first time regardless of able to retry.\n\tAllowRetry(ctx context.Context) (msg string, ok bool)\n\n\t// ShouldRetry to check if retry request can be called, it is checked in retryer.Do.\n\t// If not satisfy will return the reason message\n\tShouldRetry(ctx context.Context, err error, callTimes int, req interface{}, cbKey string) (msg string, ok bool)\n\tUpdatePolicy(policy Policy) error\n\n\t// Retry policy execute func. recycleRI is to decide if the firstRI can be recycled.\n\tDo(ctx context.Context, rpcCall RPCCallFunc, firstRI rpcinfo.RPCInfo, request interface{}) (lastRI rpcinfo.RPCInfo, recycleRI bool, err error)\n\tAppendErrMsgIfNeeded(ctx context.Context, err error, ri rpcinfo.RPCInfo, msg string)\n\n\t// Prepare to do something needed before retry call.\n\tPrepare(ctx context.Context, prevRI, retryRI rpcinfo.RPCInfo)\n\tDump() map[string]interface{}\n\tType() Type\n}\n\n// NewRetryContainerWithCB build Container that doesn't do circuit breaker statistic but get statistic result.\n// Which is used in case that circuit breaker is enabled.\n// eg:\n//\n//\t   cbs := circuitbreak.NewCBSuite(circuitbreak.RPCInfo2Key)\n//\t   retryC := retry.NewRetryContainerWithCB(cbs.ServiceControl(), cbs.ServicePanel())\n//\t\t  var opts []client.Option\n//\t\t  opts = append(opts, client.WithRetryContainer(retryC))\n//\t   // enable service circuit breaker\n//\t\t  opts = append(opts, client.WithMiddleware(cbs.ServiceCBMW()))\nfunc NewRetryContainerWithCB(cc *circuitbreak.Control, cp circuitbreaker.Panel) *Container {\n\treturn NewRetryContainer(WithContainerCBControl(cc), WithContainerCBPanel(cp))\n}\n\nfunc newCBSuite(opts []circuitbreak.CBSuiteOption) *circuitbreak.CBSuite {\n\treturn circuitbreak.NewCBSuite(circuitbreak.RPCInfo2Key, opts...)\n}\n\n// NewRetryContainerWithCBStat build Container that need to do circuit breaker statistic.\n// Which is used in case that the service CB key is customized.\n// eg:\n//\n//\tcbs := circuitbreak.NewCBSuite(YourGenServiceCBKeyFunc)\n//\tretry.NewRetryContainerWithCBStat(cbs.ServiceControl(), cbs.ServicePanel())\nfunc NewRetryContainerWithCBStat(cc *circuitbreak.Control, cp circuitbreaker.Panel) *Container {\n\treturn NewRetryContainer(WithContainerCBControl(cc), WithContainerCBPanel(cp), WithContainerCBStat())\n}\n\n// NewRetryContainerWithPercentageLimit build a Container to limiting the percentage of retry requests;\n// This is the RECOMMENDED initializer if you want to control PRECISELY the percentage of retry requests.\nfunc NewRetryContainerWithPercentageLimit() *Container {\n\treturn NewRetryContainer(WithContainerEnablePercentageLimit())\n}\n\n// ContainerOption is used when initializing a Container\ntype ContainerOption func(rc *Container)\n\n// WithContainerCBSuite specifies the CBSuite used in the retry circuitbreak\n// retryer will use its ServiceControl and ServicePanel\n// Its priority is lower than WithContainerCBControl and WithContainerCBPanel\nfunc WithContainerCBSuite(cbs *circuitbreak.CBSuite) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbSuite = cbs\n\t}\n}\n\n// WithCustomizeKeyFunc specifies the GenRetryKeyFunc to customize retry key\nfunc WithCustomizeKeyFunc(fn GenRetryKeyFunc) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.genRetryKey = fn\n\t}\n}\n\n// WithContainerCBSuiteOptions specifies the circuitbreak.CBSuiteOption for initializing circuitbreak.CBSuite\nfunc WithContainerCBSuiteOptions(opts ...circuitbreak.CBSuiteOption) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbSuiteOptions = opts\n\t}\n}\n\n// WithContainerCBControl specifies the circuitbreak.Control used in the retry circuitbreaker\n// It's user's responsibility to make sure it's paired with panel\nfunc WithContainerCBControl(ctrl *circuitbreak.Control) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbCtl = ctrl\n\t}\n}\n\n// WithContainerCBPanel specifies the circuitbreaker.Panel used in the retry circuitbreaker\n// It's user's responsibility to make sure it's paired with control\nfunc WithContainerCBPanel(panel circuitbreaker.Panel) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbPanel = panel\n\t}\n}\n\n// WithContainerCBStat instructs the circuitbreak.RecordStat is called within the retryer\nfunc WithContainerCBStat() ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbStat = true\n\t}\n}\n\n// WithContainerEnablePercentageLimit should be called for limiting the percentage of retry requests\nfunc WithContainerEnablePercentageLimit() ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.enablePercentageLimit = true\n\t}\n}\n\n// NewRetryContainer build Container that need to build circuit breaker and do circuit breaker statistic.\n// The caller is responsible for calling Container.Close() to release resources referenced.\nfunc NewRetryContainer(opts ...ContainerOption) *Container {\n\trc := &Container{\n\t\tcbContainer: &cbContainer{\n\t\t\tcbSuite: nil,\n\t\t},\n\t\tretryerMap: sync.Map{},\n\t}\n\tfor _, opt := range opts {\n\t\topt(rc)\n\t}\n\n\tif rc.cbContainer.enablePercentageLimit {\n\t\t// ignore cbSuite/cbCtl/cbPanel options\n\t\trc.cbContainer = &cbContainer{\n\t\t\tenablePercentageLimit: true,\n\t\t\tcbSuite:               newCBSuite(rc.cbContainer.cbSuiteOptions),\n\t\t\tcbSuiteOptions:        rc.cbContainer.cbSuiteOptions,\n\t\t}\n\t}\n\n\tcontainer := rc.cbContainer\n\tif container.cbCtl == nil && container.cbPanel == nil {\n\t\tif container.cbSuite == nil {\n\t\t\tcontainer.cbSuite = newCBSuite(rc.cbContainer.cbSuiteOptions)\n\t\t\tcontainer.cbStat = true\n\t\t}\n\t\tcontainer.cbCtl = container.cbSuite.ServiceControl()\n\t\tcontainer.cbPanel = container.cbSuite.ServicePanel()\n\t}\n\tif !container.IsValid() {\n\t\tpanic(\"KITEX: invalid container\")\n\t}\n\treturn rc\n}\n\nfunc defaultGenRetryKey(_ context.Context, rpcInfo rpcinfo.RPCInfo) string {\n\treturn rpcInfo.To().Method()\n}\n\n// Container is a wrapper for Retryer.\ntype Container struct {\n\thasCodeCfg  bool\n\tretryerMap  sync.Map // <method: retryer>\n\tcbContainer *cbContainer\n\tmsg         string\n\tsync.RWMutex\n\n\tgenRetryKey GenRetryKeyFunc\n\n\t// shouldResultRetry is only used with FailureRetry\n\tshouldResultRetry *ShouldResultRetry\n}\n\n// Recommended usage: NewRetryContainerWithPercentageLimit()\n// For more details, refer to the following comments for each field.\ntype cbContainer struct {\n\t// In NewRetryContainer, if cbCtrl & cbPanel are not set, Kitex will use cbSuite.ServiceControl() and\n\t// cbSuite.ServicePanel(); If cbSuite is nil, Kitex will create one.\n\tcbSuite *circuitbreak.CBSuite\n\n\t// It's more recommended to rely on the cbSuite than specifying cbCtl & cbPanel with corresponding options,\n\t// since cbCtl & cbPanel should be correctly paired, and with the cbSuite, Kitex will ensure it by using the\n\t// cbSuite.ServiceControl() and cbSuite.ServicePanel().\n\tcbCtl   *circuitbreak.Control\n\tcbPanel circuitbreaker.Panel\n\n\t// If cbStat && !enablePercentageLimit, retryer will call `circuitbreak.RecordStat` after rpcCall to record\n\t// rpc failures/timeouts, for cutting down on the retry requests when the error rate is beyond the threshold.\n\tcbStat bool\n\n\t// If enabled, Kitex will always create a cbSuite and use its cbCtl & cbPanel, and retryer will call\n\t// recordRetryStat before rpcCall, to precisely control the percentage of retry requests over all requests.\n\tenablePercentageLimit bool\n\n\t// for creating CBSuite inside NewRetryContainer\n\tcbSuiteOptions []circuitbreak.CBSuiteOption\n}\n\n// IsValid returns true when both cbCtl & cbPanel are not nil\n// It's the user's responsibility to guarantee that cbCtl & cbPanel are correctly paired.\nfunc (c *cbContainer) IsValid() bool {\n\treturn c.cbCtl != nil && c.cbPanel != nil\n}\n\n// InitWithPolicies to init Retryer with methodPolicies\n// Notice, InitWithPolicies is export func, the lock should be added inside\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// DeletePolicy to delete the method by method.\nfunc (rc *Container) DeletePolicy(key string) {\n\trc.Lock()\n\tdefer rc.Unlock()\n\trc.msg = \"\"\n\tif rc.hasCodeCfg {\n\t\t// the priority of user setup code policy is higher than remote config\n\t\treturn\n\t}\n\t_, ok := rc.retryerMap.Load(key)\n\tif ok {\n\t\trc.retryerMap.Delete(key)\n\t\trc.msg = fmt.Sprintf(\"delete retryer[%s] at %s\", key, time.Now())\n\t}\n}\n\n// NotifyPolicyChange to receive policy when it changes\nfunc (rc *Container) NotifyPolicyChange(key string, p Policy) {\n\trc.Lock()\n\tdefer rc.Unlock()\n\trc.msg = \"\"\n\tif rc.hasCodeCfg {\n\t\t// the priority of user setup code policy is higher than remote config\n\t\treturn\n\t}\n\tr, ok := rc.retryerMap.Load(key)\n\tif ok && r != nil {\n\t\tretryer, ok := r.(Retryer)\n\t\tif ok {\n\t\t\tif retryer.Type() == p.Type {\n\t\t\t\tretryer.UpdatePolicy(p)\n\t\t\t\trc.msg = fmt.Sprintf(\"update retryer[%s-%s] at %s\", key, retryer.Type(), time.Now())\n\t\t\t\treturn\n\t\t\t}\n\t\t\trc.retryerMap.Delete(key)\n\t\t\trc.msg = fmt.Sprintf(\"delete retryer[%s-%s] at %s\", key, retryer.Type(), time.Now())\n\t\t}\n\t}\n\trc.initRetryer(key, p)\n}\n\n// Init to build Retryer with code config.\nfunc (rc *Container) Init(mp map[string]Policy, rr *ShouldResultRetry) (err error) {\n\t// NotifyPolicyChange func may execute before Init func.\n\t// Because retry Container is built before Client init, NotifyPolicyChange can be triggered first\n\trc.updateRetryer(rr)\n\tif err = rc.InitWithPolicies(mp); err != nil {\n\t\treturn fmt.Errorf(\"NewRetryer in Init failed, err=%w\", err)\n\t}\n\treturn nil\n}\n\n// PrepareRetryContext adds necessary keys to context for retry\n// These keys should be added to `ctx` no matter whether there's a need to retry, to avoid sharing the same\n// object objects with another method call, since `ctx` might be reused in user-defined middlewares.\nfunc PrepareRetryContext(ctx context.Context) context.Context {\n\t// reqOp can be used to avoid multiple writes to the request object.\n\t// If a blocking write is needed, implement a lock based on it (spin-lock for example).\n\treqOp := OpNo\n\tctx = context.WithValue(ctx, CtxReqOp, &reqOp)\n\n\t// `respOp` is used to avoid concurrent write/read on the response object, especially for backup requests.\n\t// If `respOp` is modified by one request of this method call, all other requests will skip decoding.\n\trespOp := OpNo\n\tctx = context.WithValue(ctx, CtxRespOp, &respOp)\n\treturn ctx\n}\n\n// WithRetryIfNeeded to check if there is a retryer can be used and if current call can retry.\n// When the retry condition is satisfied, use retryer to call\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewRetryer build a retryer with policy\nfunc NewRetryer(p Policy, r *ShouldResultRetry, cbC *cbContainer) (retryer Retryer, err error) {\n\t// just one retry policy can be enabled at same time\n\tif p.Type == BackupType {\n\t\tretryer, err = newBackupRetryer(p, cbC)\n\t} else {\n\t\tretryer, err = newFailureRetryer(p, r, cbC)\n\t}\n\treturn\n}\n\nfunc (rc *Container) getRetryer(ctx context.Context, ri rpcinfo.RPCInfo) Retryer {\n\tkeyFunc := defaultGenRetryKey\n\tif rc.genRetryKey != nil {\n\t\tkeyFunc = rc.genRetryKey\n\t}\n\t// the priority of specific method is high\n\tr, ok := rc.retryerMap.Load(keyFunc(ctx, ri))\n\tif ok {\n\t\treturn r.(Retryer)\n\t}\n\tr, ok = rc.retryerMap.Load(Wildcard)\n\tif ok {\n\t\treturn r.(Retryer)\n\t}\n\treturn nil\n}\n\n// Dump is used to show current retry policy\nfunc (rc *Container) Dump() interface{} {\n\trc.RLock()\n\tdm := make(map[string]interface{})\n\tdm[\"has_code_cfg\"] = rc.hasCodeCfg\n\trc.retryerMap.Range(func(key, value interface{}) bool {\n\t\tif r, ok := value.(Retryer); ok {\n\t\t\tdm[key.(string)] = r.Dump()\n\t\t}\n\t\treturn true\n\t})\n\tif rc.msg != \"\" {\n\t\tdm[\"msg\"] = rc.msg\n\t}\n\trc.RUnlock()\n\treturn dm\n}\n\nfunc (rc *Container) initRetryer(method string, p Policy) error {\n\tretryer, err := NewRetryer(p, rc.shouldResultRetry, rc.cbContainer)\n\tif err != nil {\n\t\terrMsg := fmt.Sprintf(\"new retryer[%s-%s] failed, err=%s, at %s\", method, p.Type, err.Error(), time.Now())\n\t\trc.msg = errMsg\n\t\tklog.Warnf(errMsg)\n\t\treturn err\n\t}\n\n\trc.retryerMap.Store(method, retryer)\n\tif p.Enable {\n\t\trc.msg = fmt.Sprintf(\"new retryer[%s-%s] at %s\", method, retryer.Type(), time.Now())\n\t} else {\n\t\trc.msg = fmt.Sprintf(\"disable retryer[%s-%s](enable=%t) %s\", method, p.Type, p.Enable, time.Now())\n\t}\n\treturn nil\n}\n\nfunc (rc *Container) updateRetryer(rr *ShouldResultRetry) {\n\trc.Lock()\n\tdefer rc.Unlock()\n\n\trc.shouldResultRetry = rr\n\tif rc.shouldResultRetry != nil {\n\t\trc.retryerMap.Range(func(key, value interface{}) bool {\n\t\t\tif fr, ok := value.(*failureRetryer); ok {\n\t\t\t\tfr.setSpecifiedResultRetryIfNeeded(rc.shouldResultRetry)\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t}\n}\n\n// Close releases all possible resources referenced.\nfunc (rc *Container) Close() (err error) {\n\tif rc.cbContainer != nil && rc.cbContainer.cbSuite != nil {\n\t\terr = rc.cbContainer.cbSuite.Close()\n\t}\n\treturn\n}\n\n```\n# Function Signature: func (rc *Container) InitWithPolicies(methodPolicies map[string]Policy) error {\n# Natural Language Annotations: // InitWithPolicies to init Retryer with methodPolicies\n// Notice, InitWithPolicies is export func, the lock should be added inside\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (rc *Container) InitWithPolicies(methodPolicies map[string]Policy) error {\n\trc.Lock()\n\tdefer rc.Unlock()\n\tfor method, p := range methodPolicies {\n\t\tif err := rc.initRetryer(method, p); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n```", "  ```\nfunc (rc *Container) InitWithPolicies(methodPolicies map[string]Policy) error {\n\trc.Lock()\n\tdefer rc.Unlock()\n\trc.retryerMap = sync.Map{}\n\trc.hasCodeCfg = true\n\trc.msg = \"\"\n\tfor method, p := range methodPolicies {\n\t\terr := rc.initRetryer(method, p)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n```", "  ```\nfunc (rc *Container) InitWithPolicies(mp map[string]Policy) error {\n    rc.Lock()\n    defer rc.Unlock()\n    for method, p := range mp {\n        if p.Enable {\n            if err := rc.initRetryer(method, p); err != nil {\n                return err\n            }\n        }\n    }\n    return nil\n}\n```"]}, "kitex-pkg/retry/retryer.go-WithRetryIfNeeded": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/retry/retryer.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package retry implements rpc retry\npackage retry\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/bytedance/gopkg/cloud/circuitbreaker\"\n\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\n// RPCCallFunc is the definition with wrap rpc call\ntype RPCCallFunc func(context.Context, Retryer) (rpcinfo rpcinfo.RPCInfo, resp interface{}, err error)\n\n// GenRetryKeyFunc to generate retry key through rpcinfo.\n// You can customize the config key according to your config center.\ntype GenRetryKeyFunc func(ctx context.Context, ri rpcinfo.RPCInfo) string\n\n// Retryer is the interface for Retry implements\ntype Retryer interface {\n\t// AllowRetry to check if current request satisfy retry condition[eg: circuit, retry times == 0, chain stop, ddl].\n\t// If not satisfy won't execute Retryer.Do and return the reason message\n\t// Execute anyway for the first time regardless of able to retry.\n\tAllowRetry(ctx context.Context) (msg string, ok bool)\n\n\t// ShouldRetry to check if retry request can be called, it is checked in retryer.Do.\n\t// If not satisfy will return the reason message\n\tShouldRetry(ctx context.Context, err error, callTimes int, req interface{}, cbKey string) (msg string, ok bool)\n\tUpdatePolicy(policy Policy) error\n\n\t// Retry policy execute func. recycleRI is to decide if the firstRI can be recycled.\n\tDo(ctx context.Context, rpcCall RPCCallFunc, firstRI rpcinfo.RPCInfo, request interface{}) (lastRI rpcinfo.RPCInfo, recycleRI bool, err error)\n\tAppendErrMsgIfNeeded(ctx context.Context, err error, ri rpcinfo.RPCInfo, msg string)\n\n\t// Prepare to do something needed before retry call.\n\tPrepare(ctx context.Context, prevRI, retryRI rpcinfo.RPCInfo)\n\tDump() map[string]interface{}\n\tType() Type\n}\n\n// NewRetryContainerWithCB build Container that doesn't do circuit breaker statistic but get statistic result.\n// Which is used in case that circuit breaker is enabled.\n// eg:\n//\n//\t   cbs := circuitbreak.NewCBSuite(circuitbreak.RPCInfo2Key)\n//\t   retryC := retry.NewRetryContainerWithCB(cbs.ServiceControl(), cbs.ServicePanel())\n//\t\t  var opts []client.Option\n//\t\t  opts = append(opts, client.WithRetryContainer(retryC))\n//\t   // enable service circuit breaker\n//\t\t  opts = append(opts, client.WithMiddleware(cbs.ServiceCBMW()))\nfunc NewRetryContainerWithCB(cc *circuitbreak.Control, cp circuitbreaker.Panel) *Container {\n\treturn NewRetryContainer(WithContainerCBControl(cc), WithContainerCBPanel(cp))\n}\n\nfunc newCBSuite(opts []circuitbreak.CBSuiteOption) *circuitbreak.CBSuite {\n\treturn circuitbreak.NewCBSuite(circuitbreak.RPCInfo2Key, opts...)\n}\n\n// NewRetryContainerWithCBStat build Container that need to do circuit breaker statistic.\n// Which is used in case that the service CB key is customized.\n// eg:\n//\n//\tcbs := circuitbreak.NewCBSuite(YourGenServiceCBKeyFunc)\n//\tretry.NewRetryContainerWithCBStat(cbs.ServiceControl(), cbs.ServicePanel())\nfunc NewRetryContainerWithCBStat(cc *circuitbreak.Control, cp circuitbreaker.Panel) *Container {\n\treturn NewRetryContainer(WithContainerCBControl(cc), WithContainerCBPanel(cp), WithContainerCBStat())\n}\n\n// NewRetryContainerWithPercentageLimit build a Container to limiting the percentage of retry requests;\n// This is the RECOMMENDED initializer if you want to control PRECISELY the percentage of retry requests.\nfunc NewRetryContainerWithPercentageLimit() *Container {\n\treturn NewRetryContainer(WithContainerEnablePercentageLimit())\n}\n\n// ContainerOption is used when initializing a Container\ntype ContainerOption func(rc *Container)\n\n// WithContainerCBSuite specifies the CBSuite used in the retry circuitbreak\n// retryer will use its ServiceControl and ServicePanel\n// Its priority is lower than WithContainerCBControl and WithContainerCBPanel\nfunc WithContainerCBSuite(cbs *circuitbreak.CBSuite) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbSuite = cbs\n\t}\n}\n\n// WithCustomizeKeyFunc specifies the GenRetryKeyFunc to customize retry key\nfunc WithCustomizeKeyFunc(fn GenRetryKeyFunc) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.genRetryKey = fn\n\t}\n}\n\n// WithContainerCBSuiteOptions specifies the circuitbreak.CBSuiteOption for initializing circuitbreak.CBSuite\nfunc WithContainerCBSuiteOptions(opts ...circuitbreak.CBSuiteOption) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbSuiteOptions = opts\n\t}\n}\n\n// WithContainerCBControl specifies the circuitbreak.Control used in the retry circuitbreaker\n// It's user's responsibility to make sure it's paired with panel\nfunc WithContainerCBControl(ctrl *circuitbreak.Control) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbCtl = ctrl\n\t}\n}\n\n// WithContainerCBPanel specifies the circuitbreaker.Panel used in the retry circuitbreaker\n// It's user's responsibility to make sure it's paired with control\nfunc WithContainerCBPanel(panel circuitbreaker.Panel) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbPanel = panel\n\t}\n}\n\n// WithContainerCBStat instructs the circuitbreak.RecordStat is called within the retryer\nfunc WithContainerCBStat() ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbStat = true\n\t}\n}\n\n// WithContainerEnablePercentageLimit should be called for limiting the percentage of retry requests\nfunc WithContainerEnablePercentageLimit() ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.enablePercentageLimit = true\n\t}\n}\n\n// NewRetryContainer build Container that need to build circuit breaker and do circuit breaker statistic.\n// The caller is responsible for calling Container.Close() to release resources referenced.\nfunc NewRetryContainer(opts ...ContainerOption) *Container {\n\trc := &Container{\n\t\tcbContainer: &cbContainer{\n\t\t\tcbSuite: nil,\n\t\t},\n\t\tretryerMap: sync.Map{},\n\t}\n\tfor _, opt := range opts {\n\t\topt(rc)\n\t}\n\n\tif rc.cbContainer.enablePercentageLimit {\n\t\t// ignore cbSuite/cbCtl/cbPanel options\n\t\trc.cbContainer = &cbContainer{\n\t\t\tenablePercentageLimit: true,\n\t\t\tcbSuite:               newCBSuite(rc.cbContainer.cbSuiteOptions),\n\t\t\tcbSuiteOptions:        rc.cbContainer.cbSuiteOptions,\n\t\t}\n\t}\n\n\tcontainer := rc.cbContainer\n\tif container.cbCtl == nil && container.cbPanel == nil {\n\t\tif container.cbSuite == nil {\n\t\t\tcontainer.cbSuite = newCBSuite(rc.cbContainer.cbSuiteOptions)\n\t\t\tcontainer.cbStat = true\n\t\t}\n\t\tcontainer.cbCtl = container.cbSuite.ServiceControl()\n\t\tcontainer.cbPanel = container.cbSuite.ServicePanel()\n\t}\n\tif !container.IsValid() {\n\t\tpanic(\"KITEX: invalid container\")\n\t}\n\treturn rc\n}\n\nfunc defaultGenRetryKey(_ context.Context, rpcInfo rpcinfo.RPCInfo) string {\n\treturn rpcInfo.To().Method()\n}\n\n// Container is a wrapper for Retryer.\ntype Container struct {\n\thasCodeCfg  bool\n\tretryerMap  sync.Map // <method: retryer>\n\tcbContainer *cbContainer\n\tmsg         string\n\tsync.RWMutex\n\n\tgenRetryKey GenRetryKeyFunc\n\n\t// shouldResultRetry is only used with FailureRetry\n\tshouldResultRetry *ShouldResultRetry\n}\n\n// Recommended usage: NewRetryContainerWithPercentageLimit()\n// For more details, refer to the following comments for each field.\ntype cbContainer struct {\n\t// In NewRetryContainer, if cbCtrl & cbPanel are not set, Kitex will use cbSuite.ServiceControl() and\n\t// cbSuite.ServicePanel(); If cbSuite is nil, Kitex will create one.\n\tcbSuite *circuitbreak.CBSuite\n\n\t// It's more recommended to rely on the cbSuite than specifying cbCtl & cbPanel with corresponding options,\n\t// since cbCtl & cbPanel should be correctly paired, and with the cbSuite, Kitex will ensure it by using the\n\t// cbSuite.ServiceControl() and cbSuite.ServicePanel().\n\tcbCtl   *circuitbreak.Control\n\tcbPanel circuitbreaker.Panel\n\n\t// If cbStat && !enablePercentageLimit, retryer will call `circuitbreak.RecordStat` after rpcCall to record\n\t// rpc failures/timeouts, for cutting down on the retry requests when the error rate is beyond the threshold.\n\tcbStat bool\n\n\t// If enabled, Kitex will always create a cbSuite and use its cbCtl & cbPanel, and retryer will call\n\t// recordRetryStat before rpcCall, to precisely control the percentage of retry requests over all requests.\n\tenablePercentageLimit bool\n\n\t// for creating CBSuite inside NewRetryContainer\n\tcbSuiteOptions []circuitbreak.CBSuiteOption\n}\n\n// IsValid returns true when both cbCtl & cbPanel are not nil\n// It's the user's responsibility to guarantee that cbCtl & cbPanel are correctly paired.\nfunc (c *cbContainer) IsValid() bool {\n\treturn c.cbCtl != nil && c.cbPanel != nil\n}\n\n// InitWithPolicies to init Retryer with methodPolicies\n// Notice, InitWithPolicies is export func, the lock should be added inside\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// DeletePolicy to delete the method by method.\nfunc (rc *Container) DeletePolicy(key string) {\n\trc.Lock()\n\tdefer rc.Unlock()\n\trc.msg = \"\"\n\tif rc.hasCodeCfg {\n\t\t// the priority of user setup code policy is higher than remote config\n\t\treturn\n\t}\n\t_, ok := rc.retryerMap.Load(key)\n\tif ok {\n\t\trc.retryerMap.Delete(key)\n\t\trc.msg = fmt.Sprintf(\"delete retryer[%s] at %s\", key, time.Now())\n\t}\n}\n\n// NotifyPolicyChange to receive policy when it changes\nfunc (rc *Container) NotifyPolicyChange(key string, p Policy) {\n\trc.Lock()\n\tdefer rc.Unlock()\n\trc.msg = \"\"\n\tif rc.hasCodeCfg {\n\t\t// the priority of user setup code policy is higher than remote config\n\t\treturn\n\t}\n\tr, ok := rc.retryerMap.Load(key)\n\tif ok && r != nil {\n\t\tretryer, ok := r.(Retryer)\n\t\tif ok {\n\t\t\tif retryer.Type() == p.Type {\n\t\t\t\tretryer.UpdatePolicy(p)\n\t\t\t\trc.msg = fmt.Sprintf(\"update retryer[%s-%s] at %s\", key, retryer.Type(), time.Now())\n\t\t\t\treturn\n\t\t\t}\n\t\t\trc.retryerMap.Delete(key)\n\t\t\trc.msg = fmt.Sprintf(\"delete retryer[%s-%s] at %s\", key, retryer.Type(), time.Now())\n\t\t}\n\t}\n\trc.initRetryer(key, p)\n}\n\n// Init to build Retryer with code config.\nfunc (rc *Container) Init(mp map[string]Policy, rr *ShouldResultRetry) (err error) {\n\t// NotifyPolicyChange func may execute before Init func.\n\t// Because retry Container is built before Client init, NotifyPolicyChange can be triggered first\n\trc.updateRetryer(rr)\n\tif err = rc.InitWithPolicies(mp); err != nil {\n\t\treturn fmt.Errorf(\"NewRetryer in Init failed, err=%w\", err)\n\t}\n\treturn nil\n}\n\n// PrepareRetryContext adds necessary keys to context for retry\n// These keys should be added to `ctx` no matter whether there's a need to retry, to avoid sharing the same\n// object objects with another method call, since `ctx` might be reused in user-defined middlewares.\nfunc PrepareRetryContext(ctx context.Context) context.Context {\n\t// reqOp can be used to avoid multiple writes to the request object.\n\t// If a blocking write is needed, implement a lock based on it (spin-lock for example).\n\treqOp := OpNo\n\tctx = context.WithValue(ctx, CtxReqOp, &reqOp)\n\n\t// `respOp` is used to avoid concurrent write/read on the response object, especially for backup requests.\n\t// If `respOp` is modified by one request of this method call, all other requests will skip decoding.\n\trespOp := OpNo\n\tctx = context.WithValue(ctx, CtxRespOp, &respOp)\n\treturn ctx\n}\n\n// WithRetryIfNeeded to check if there is a retryer can be used and if current call can retry.\n// When the retry condition is satisfied, use retryer to call\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewRetryer build a retryer with policy\nfunc NewRetryer(p Policy, r *ShouldResultRetry, cbC *cbContainer) (retryer Retryer, err error) {\n\t// just one retry policy can be enabled at same time\n\tif p.Type == BackupType {\n\t\tretryer, err = newBackupRetryer(p, cbC)\n\t} else {\n\t\tretryer, err = newFailureRetryer(p, r, cbC)\n\t}\n\treturn\n}\n\nfunc (rc *Container) getRetryer(ctx context.Context, ri rpcinfo.RPCInfo) Retryer {\n\tkeyFunc := defaultGenRetryKey\n\tif rc.genRetryKey != nil {\n\t\tkeyFunc = rc.genRetryKey\n\t}\n\t// the priority of specific method is high\n\tr, ok := rc.retryerMap.Load(keyFunc(ctx, ri))\n\tif ok {\n\t\treturn r.(Retryer)\n\t}\n\tr, ok = rc.retryerMap.Load(Wildcard)\n\tif ok {\n\t\treturn r.(Retryer)\n\t}\n\treturn nil\n}\n\n// Dump is used to show current retry policy\nfunc (rc *Container) Dump() interface{} {\n\trc.RLock()\n\tdm := make(map[string]interface{})\n\tdm[\"has_code_cfg\"] = rc.hasCodeCfg\n\trc.retryerMap.Range(func(key, value interface{}) bool {\n\t\tif r, ok := value.(Retryer); ok {\n\t\t\tdm[key.(string)] = r.Dump()\n\t\t}\n\t\treturn true\n\t})\n\tif rc.msg != \"\" {\n\t\tdm[\"msg\"] = rc.msg\n\t}\n\trc.RUnlock()\n\treturn dm\n}\n\nfunc (rc *Container) initRetryer(method string, p Policy) error {\n\tretryer, err := NewRetryer(p, rc.shouldResultRetry, rc.cbContainer)\n\tif err != nil {\n\t\terrMsg := fmt.Sprintf(\"new retryer[%s-%s] failed, err=%s, at %s\", method, p.Type, err.Error(), time.Now())\n\t\trc.msg = errMsg\n\t\tklog.Warnf(errMsg)\n\t\treturn err\n\t}\n\n\trc.retryerMap.Store(method, retryer)\n\tif p.Enable {\n\t\trc.msg = fmt.Sprintf(\"new retryer[%s-%s] at %s\", method, retryer.Type(), time.Now())\n\t} else {\n\t\trc.msg = fmt.Sprintf(\"disable retryer[%s-%s](enable=%t) %s\", method, p.Type, p.Enable, time.Now())\n\t}\n\treturn nil\n}\n\nfunc (rc *Container) updateRetryer(rr *ShouldResultRetry) {\n\trc.Lock()\n\tdefer rc.Unlock()\n\n\trc.shouldResultRetry = rr\n\tif rc.shouldResultRetry != nil {\n\t\trc.retryerMap.Range(func(key, value interface{}) bool {\n\t\t\tif fr, ok := value.(*failureRetryer); ok {\n\t\t\t\tfr.setSpecifiedResultRetryIfNeeded(rc.shouldResultRetry)\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t}\n}\n\n// Close releases all possible resources referenced.\nfunc (rc *Container) Close() (err error) {\n\tif rc.cbContainer != nil && rc.cbContainer.cbSuite != nil {\n\t\terr = rc.cbContainer.cbSuite.Close()\n\t}\n\treturn\n}\n\n```\n# Function Signature: func (rc *Container) WithRetryIfNeeded(ctx context.Context, callOptRetry *Policy, rpcCall RPCCallFunc, ri rpcinfo.RPCInfo, request interface{}) (lastRI rpcinfo.RPCInfo, recycleRI bool, err error) {\n# Natural Language Annotations: // WithRetryIfNeeded to check if there is a retryer can be used and if current call can retry.\n// When the retry condition is satisfied, use retryer to call\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (rc *Container) WithRetryIfNeeded(ctx context.Context, callOptRetry *Policy, rpcCall RPCCallFunc, ri rpcinfo.RPCInfo, request interface{}) (lastRI rpcinfo.RPCInfo, recycleRI bool, err error) {\n\t// Check the retry conditions\n\tretryer := rc.getRetryer(ctx, ri)\n\tif retryer == nil || !callOptRetry.Enable {\n\t\t// No retryer available or the retry feature is disabled\n\t\t// Directly call the rpcCall function\n\t\treturn rpcCall(ctx, ri, request)\n\t}\n\n\t// Check if the retryer is disabled\n\tif !retryer.AllowRetry(ctx) {\n\t\t// The retryer is disabled, return error\n\t\treturn lastRI, false, fmt.Errorf(\"retry call disabled: %s\", retryer.Msg())\n\t}\n\n\t// Check if the retry condition is satisfied\n\tif ok := retryer.ShouldRetry(ctx, err, 0, request, GenRetryKey(ctx, ri)); !ok {\n\t\t// The retry condition is not satisfied, return error\n\t\treturn lastRI, false, fmt.Errorf(\"retry call skipped: %s\", retryer.Msg())\n\t}\n\n\t// Execute the retry\n\treturn retryer.Do(ctx, rpcCall, ri, request)\n}\n```", "  ```\nfunc (rc *Container) WithRetryIfNeeded(ctx context.Context, callOptRetry *Policy, rpcCall RPCCallFunc, ri rpcinfo.RPCInfo, request interface{}) (lastRI rpcinfo.RPCInfo, recycleRI bool, err error) {\n\t// get retryer for current method\n\tif !callOptRetry.Enable && !rc.hasCodeCfg {\n\t\treturn rpcCall(ctx, ri)\n\t}\n\n\tretryer := rc.getRetryer(ctx, ri)\n\tif retryer == nil {\n\t\treturn rpcCall(ctx, ri)\n\t}\n\t// If not prevented from retry, and if the retrier returns true, the retryer can be used.\n\tif !rc.cbContainer.IsValid() || rc.cbContainer.cbCtl.CanRequestOccur(rc.cbContainer.cbPanel) {\n\t\treason, ok := retryer.AllowRetry(ctx)\n\t\tif !ok {\n\t\t\treturn rpcCall(ctx, ri)\n\t\t}\n\t\tklog.Infof(\"retryer can be used for %s, msg: %s\", ri.To().Method(), reason)\n\n\t\t// check whether it can be retried according to the retry policy\n\t\tif reason, ok = retryer.ShouldRetry(ctx, nil, 0, request, rc.cbContainer.cbStat.Circuit()); !ok {\n\t\t\treturn rpcCall(ctx, ri)\n\t\t}\n\t\tklog.Infof(\"retryer allowed to retry %s, msg: %s\", ri.To().Method(), reason)\n\n\t\t// Use the retryer to retry\n\t\treturn retryer.Do(ctx, rpcCall, ri, request)\n\t} else {\n\t\tklog.Infof(\"retryer not allowed to retry %s\", ri.To().Method())\n\t\treturn nil, false, fmt.Errorf(\"can not retry %s with retryer\", reason)\n\t}\n}\n```", "  ```\nfunc (rc *Container) WithRetryIfNeeded(ctx context.Context, callOptRetry *Policy, rpcCall RPCCallFunc, ri rpcinfo.RPCInfo, request interface{}) (lastRI rpcinfo.RPCInfo, recycleRI bool, err error) {\n\tretryer := rc.getRetryer(ctx, ri)\n\tif retryer != nil {\n\t\t// check the retry condition\n\t\tif ok := retryer.AllowRetry(ctx); ok {\n\t\t\tklog.Warnf(\"current call satisfy retry condition, will retry, at %s\", time.Now())\n\t\t\treturn rc.executeRetry(ctx, retryer, rpcCall, ri, request)\n\t\t}\n\t\tklog.Warnf(\"current call can't retry, as the retry condition is not satisfied, at %s\", time.Now())\n\t}\n\t// fall back to the origin call\n\tklog.Warnf(\"current call doesn't satisfy retry condition, will fallback to origin call, at %s\", time.Now())\n\treturn rc.getRPCInfo(ctx, rpcCall, ri, request)\n}\n\nfunc (rc *Container) executeRetry(ctx context.Context, retryer Retryer, rpcCall RPCCallFunc, ri rpcinfo.RPCInfo, request interface{}) (lastRI rpcinfo.RPCInfo, recycleRI bool, err error) {\n\tklog.Infof(\"begin to do retry, at %s\", time.Now())\n\n\tcallTimes := 0\n\tlastRI = ri\n\trecycleRI = false\n\tmsg := \"\"\n\n\tfor {\n\t\tif callTimes > 0 {\n\t\t\tklog.Infof(\"begin to recover, at %s\", time.Now())\n\t\t\tklog.Infof(\"callTimes:%d, reason:%s, at %s\", callTimes, msg, time.Now())\n\n\t\t\terr = retryer.Prepare(ctx, lastRI, ri)\n\t\t\tif err != nil {\n\t\t\t\tklog.Errorf(\"prepare retry rpcinfo failed, err=[%s], at %s\", err, time.Now())\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\trecycleRI = true\n\t\t\tlastRI = ri\n\t\t}\n\n\t\tvar (\n\t\t\tresp interface{}\n\t\t\trpc  rpcinfo.RPCInfo\n\t\t)\n\t\trpc, resp, err = rpcCall(ctx, retryer)\n\t\tif err != nil || resp != nil {\n\t\t\tklog.Infof(\"rpcCall executed and returned err=[%s], resp=[%v], at %s\", err, resp, time.Now())\n\n\t\t\tmsg, ok := retryer.AppendErrMsgIfNeeded(ctx, err, rpc, msg)\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif ok = retryer.ShouldRetry(ctx, err, callTimes, request, rc.makeRetryKey(ri)); !ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\tcallTimes++\n\t\tlastRI = rpc\n\t}\n}\n\nfunc (rc *Container) getRPCInfo(ctx context.Context, rpcCall RPCCallFunc, ri rpcinfo.RPCInfo, request interface{}) (lastRI rpcinfo.RPCInfo, recycleRI bool, err error) {\n\tklog.Infof(\"begin to do origin call, at %s\", time.Now())\n\n\tvar (\n\t\tresp interface{}\n\t\trpc  rpcinfo.RPCInfo\n\t)\n\trpc, resp, err = rpcCall(ctx, nil)\n\tif err != nil || resp != nil {\n\t\tklog.Infof(\"rpcCall executed and returned err=[%s], resp=[%v], at %s\", err, resp, time.Now())\n\n\t\tlastRI = rpc\n\t\tcallTimes := 0\n\t\trecycleRI = false\n\t}\n\treturn\n}\n\nfunc (rc *Container) makeRetryKey(ri rpcinfo.RPCInfo) string {\n\treturn"]}, "kitex-pkg/discovery/discovery.go-DefaultDiff": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/discovery/discovery.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package discovery defines interfaces for service discovery.\n// Developers that are willing to customize service discovery\n// should implement their own Resolver and supply it with the\n// option WithResolver at client's creation.\npackage discovery\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// DefaultWeight is the default weight for an instance.\nconst DefaultWeight = 10\n\n// Result contains the result of service discovery process.\n// Cacheable tells whether the instance list can/should be cached.\n// When Cacheable is true, CacheKey can be used to map the instance list in cache.\ntype Result struct {\n\tCacheable bool\n\tCacheKey  string\n\tInstances []Instance\n}\n\n// Change contains the difference between the current discovery result and the previous one.\n// It is designed for providing detail information when dispatching an event for service\n// discovery result change.\n// Since the loadbalancer may rely on caching the result of resolver to improve performance,\n// the resolver implementation should dispatch an event when result changes.\ntype Change struct {\n\tResult  Result\n\tAdded   []Instance\n\tUpdated []Instance\n\tRemoved []Instance\n}\n\n// Resolver resolves the target endpoint into a list of Instance.\ntype Resolver interface {\n\t// Target should return a description for the given target that is suitable for being a key for cache.\n\tTarget(ctx context.Context, target rpcinfo.EndpointInfo) (description string)\n\n\t// Resolve returns a list of instances for the given description of a target.\n\tResolve(ctx context.Context, desc string) (Result, error)\n\n\t// Diff computes the difference between two results.\n\t// When `next` is cacheable, the Change should be cacheable, too. And the `Result` field's CacheKey in\n\t// the return value should be set with the given cacheKey.\n\tDiff(cacheKey string, prev, next Result) (Change, bool)\n\n\t// Name returns the name of the resolver.\n\tName() string\n}\n\n// DefaultDiff provides a natural implementation for the Diff method of the Resolver interface.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype instance struct {\n\taddr   net.Addr\n\tweight int\n\ttags   map[string]string\n}\n\nfunc (i *instance) Address() net.Addr {\n\treturn i.addr\n}\n\nfunc (i *instance) Weight() int {\n\treturn i.weight\n}\n\nfunc (i *instance) Tag(key string) (value string, exist bool) {\n\tvalue, exist = i.tags[key]\n\treturn\n}\n\n// NewInstance creates a Instance using the given network, address and tags\nfunc NewInstance(network, address string, weight int, tags map[string]string) Instance {\n\treturn &instance{\n\t\taddr:   utils.NewNetAddr(network, address),\n\t\tweight: weight,\n\t\ttags:   tags,\n\t}\n}\n\n// SynthesizedResolver synthesizes a Resolver using a resolve function.\ntype SynthesizedResolver struct {\n\tTargetFunc  func(ctx context.Context, target rpcinfo.EndpointInfo) string\n\tResolveFunc func(ctx context.Context, key string) (Result, error)\n\tDiffFunc    func(key string, prev, next Result) (Change, bool)\n\tNameFunc    func() string\n}\n\n// Target implements the Resolver interface.\nfunc (sr SynthesizedResolver) Target(ctx context.Context, target rpcinfo.EndpointInfo) string {\n\tif sr.TargetFunc == nil {\n\t\treturn \"\"\n\t}\n\treturn sr.TargetFunc(ctx, target)\n}\n\n// Resolve implements the Resolver interface.\nfunc (sr SynthesizedResolver) Resolve(ctx context.Context, key string) (Result, error) {\n\treturn sr.ResolveFunc(ctx, key)\n}\n\n// Diff implements the Resolver interface.\nfunc (sr SynthesizedResolver) Diff(key string, prev, next Result) (Change, bool) {\n\tif sr.DiffFunc == nil {\n\t\treturn DefaultDiff(key, prev, next)\n\t}\n\treturn sr.DiffFunc(key, prev, next)\n}\n\n// Name implements the Resolver interface\nfunc (sr SynthesizedResolver) Name() string {\n\tif sr.NameFunc == nil {\n\t\treturn \"\"\n\t}\n\treturn sr.NameFunc()\n}\n\n// Instance contains information of an instance from the target service.\ntype Instance interface {\n\tAddress() net.Addr\n\tWeight() int\n\tTag(key string) (value string, exist bool)\n}\n\n```\n# Function Signature: func DefaultDiff(cacheKey string, prev, next Result) (Change, bool) {\n# Natural Language Annotations: // DefaultDiff provides a natural implementation for the Diff method of the Resolver interface.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc DefaultDiff(cacheKey string, prev, next Result) (Change, bool) {\n\tadded, _ := utils.DiffStringSlice(prev.Instances, next.Instances, func(a, b Instance) bool {\n\t\treturn a.Address().String() == b.Address().String()\n\t})\n\tupdated, _ := utils.DiffStringSlice(prev.Instances, next.Instances, func(a, b Instance) bool {\n\t\treturn a.Address().String() == b.Address().String()\n\t})\n\tremoved, _ := utils.DiffStringSlice(prev.Instances, next.Instances, func(a, b Instance) bool {\n\t\treturn a.Address().String() == b.Address().String()\n\t})\n\treturn Change{\n\t\tResult:  next,\n\t\tAdded:   added,\n\t\tUpdated: updated,\n\t\tRemoved: removed,\n\t}, true\n}\n```", "  ```\nfunc DefaultDiff(cacheKey string, prev, next Result) (Change, bool) {\n\tadd := append([]Instance{}, next.Instances[len(prev.Instances):]...)\n\trem := append([]Instance{}, prev.Instances[len(next.Instances):]...)\n\treturn Change{\n\t\tCacheKey:  cacheKey,\n\t\tCacheable: false,\n\t\tAdded:     add,\n\t\tUpdated:   nil,\n\t\tRemoved:   rem,\n\t\tResult:    next,\n\t}, false\n}\n```", "  ```\nfunc DefaultDiff(cacheKey string, prev, next Result) (Change, bool) {\n\tchange := Change{\n\t\tResult: next,\n\t\tAdded:  make([]Instance, 0),\n\t\tUpdated: make([]Instance, 0),\n\t\tRemoved: make([]Instance, 0),\n\t}\n\n\t// Add instances present in new result but not in previous\n\tfor _, ins := range next.Instances {\n\t\tvar found bool\n\t\tfor _, oldIns := range prev.Instances {\n\t\t\tif instanceEquivalent(ins, oldIns) {\n\t\t\t\tfound = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !found {\n\t\t\tchange.Added = append(change.Added, ins)\n\t\t}\n\t}\n\n\t// Update instances that have changed\n\tfor _, oldIns := range prev.Instances {\n\t\tfor i, newIns := range next.Instances {\n\t\t\tif instanceEquivalent(oldIns, newIns) {\n\t\t\t\tif oldIns != newIns {\n\t\t\t\t\tchange.Updated = append(change.Updated, newIns)\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\t// Removed instances that are present in previous but not in new\n\t// NOTE: We only remove instances that are not in the updated list\n\tfor _, oldIns := range prev.Instances {\n\t\tvar found bool\n\t\tfor _, newIns := range next.Instances {\n\t\t\tif instanceEquivalent(oldIns, newIns) {\n\t\t\t\tfound = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !found {\n\t\t\tchange.Removed = append(change.Removed, oldIns)\n\t\t}\n\t}\n\n\treturn change, len(change.Added) > 0 || len(change.Updated) > 0 || len(change.Removed) > 0\n}\n\nfunc instanceEquivalent(a, b Instance) bool {\n\treturn a.Address().String() == b.Address().String() && a.Weight() == b.Weight()\n}\n```"]}, "kitex-pkg/protocol/bthrift/binary.go-SetSpanCache": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/protocol/bthrift/binary.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package bthrift .\npackage bthrift\n\nimport (\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\n\t\"github.com/cloudwego/kitex/pkg/mem\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\nvar (\n\t// Binary protocol for bthrift.\n\tBinary binaryProtocol\n\t_      BTProtocol = binaryProtocol{}\n)\n\nvar allocator Allocator\n\nconst binaryInplaceThreshold = 4096 // 4k\n\ntype binaryProtocol struct{}\n\n// SetSpanCache enable/disable binary protocol bytes/string allocator\n\n\n\n\n\n\n\n\n// SetAllocator set binary protocol bytes/string allocator.\nfunc SetAllocator(alloc Allocator) {\n\tallocator = alloc\n}\n\nfunc (binaryProtocol) WriteMessageBegin(buf []byte, name string, typeID thrift.TMessageType, seqid int32) int {\n\toffset := 0\n\tversion := uint32(thrift.VERSION_1) | uint32(typeID)\n\toffset += Binary.WriteI32(buf, int32(version))\n\toffset += Binary.WriteString(buf[offset:], name)\n\toffset += Binary.WriteI32(buf[offset:], seqid)\n\treturn offset\n}\n\nfunc (binaryProtocol) WriteMessageEnd(buf []byte) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteStructBegin(buf []byte, name string) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteStructEnd(buf []byte) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteFieldBegin(buf []byte, name string, typeID thrift.TType, id int16) int {\n\treturn Binary.WriteByte(buf, int8(typeID)) + Binary.WriteI16(buf[1:], id)\n}\n\nfunc (binaryProtocol) WriteFieldEnd(buf []byte) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteFieldStop(buf []byte) int {\n\treturn Binary.WriteByte(buf, thrift.STOP)\n}\n\nfunc (binaryProtocol) WriteMapBegin(buf []byte, keyType, valueType thrift.TType, size int) int {\n\treturn Binary.WriteByte(buf, int8(keyType)) +\n\t\tBinary.WriteByte(buf[1:], int8(valueType)) +\n\t\tBinary.WriteI32(buf[2:], int32(size))\n}\n\nfunc (binaryProtocol) WriteMapEnd(buf []byte) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteListBegin(buf []byte, elemType thrift.TType, size int) int {\n\treturn Binary.WriteByte(buf, int8(elemType)) +\n\t\tBinary.WriteI32(buf[1:], int32(size))\n}\n\nfunc (binaryProtocol) WriteListEnd(buf []byte) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteSetBegin(buf []byte, elemType thrift.TType, size int) int {\n\treturn Binary.WriteByte(buf, int8(elemType)) +\n\t\tBinary.WriteI32(buf[1:], int32(size))\n}\n\nfunc (binaryProtocol) WriteSetEnd(buf []byte) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteBool(buf []byte, value bool) int {\n\tif value {\n\t\treturn Binary.WriteByte(buf, 1)\n\t}\n\treturn Binary.WriteByte(buf, 0)\n}\n\nfunc (binaryProtocol) WriteByte(buf []byte, value int8) int {\n\tbuf[0] = byte(value)\n\treturn 1\n}\n\nfunc (binaryProtocol) WriteI16(buf []byte, value int16) int {\n\tbinary.BigEndian.PutUint16(buf, uint16(value))\n\treturn 2\n}\n\nfunc (binaryProtocol) WriteI32(buf []byte, value int32) int {\n\tbinary.BigEndian.PutUint32(buf, uint32(value))\n\treturn 4\n}\n\nfunc (binaryProtocol) WriteI64(buf []byte, value int64) int {\n\tbinary.BigEndian.PutUint64(buf, uint64(value))\n\treturn 8\n}\n\nfunc (binaryProtocol) WriteDouble(buf []byte, value float64) int {\n\treturn Binary.WriteI64(buf, int64(math.Float64bits(value)))\n}\n\nfunc (binaryProtocol) WriteString(buf []byte, value string) int {\n\tl := Binary.WriteI32(buf, int32(len(value)))\n\tcopy(buf[l:], value)\n\treturn l + len(value)\n}\n\nfunc (binaryProtocol) WriteBinary(buf, value []byte) int {\n\tl := Binary.WriteI32(buf, int32(len(value)))\n\tcopy(buf[l:], value)\n\treturn l + len(value)\n}\n\nfunc (binaryProtocol) WriteStringNocopy(buf []byte, binaryWriter BinaryWriter, value string) int {\n\treturn Binary.WriteBinaryNocopy(buf, binaryWriter, utils.StringToSliceByte(value))\n}\n\nfunc (binaryProtocol) WriteBinaryNocopy(buf []byte, binaryWriter BinaryWriter, value []byte) int {\n\tl := Binary.WriteI32(buf, int32(len(value)))\n\tif binaryWriter != nil && len(value) > binaryInplaceThreshold {\n\t\tbinaryWriter.WriteDirect(value, len(buf[l:]))\n\t\treturn l\n\t}\n\tcopy(buf[l:], value)\n\treturn l + len(value)\n}\n\nfunc (binaryProtocol) MessageBeginLength(name string, typeID thrift.TMessageType, seqid int32) int {\n\tversion := uint32(thrift.VERSION_1) | uint32(typeID)\n\treturn Binary.I32Length(int32(version)) + Binary.StringLength(name) + Binary.I32Length(seqid)\n}\n\nfunc (binaryProtocol) MessageEndLength() int {\n\treturn 0\n}\n\nfunc (binaryProtocol) StructBeginLength(name string) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) StructEndLength() int {\n\treturn 0\n}\n\nfunc (binaryProtocol) FieldBeginLength(name string, typeID thrift.TType, id int16) int {\n\treturn Binary.ByteLength(int8(typeID)) + Binary.I16Length(id)\n}\n\nfunc (binaryProtocol) FieldEndLength() int {\n\treturn 0\n}\n\nfunc (binaryProtocol) FieldStopLength() int {\n\treturn Binary.ByteLength(thrift.STOP)\n}\n\nfunc (binaryProtocol) MapBeginLength(keyType, valueType thrift.TType, size int) int {\n\treturn Binary.ByteLength(int8(keyType)) +\n\t\tBinary.ByteLength(int8(valueType)) +\n\t\tBinary.I32Length(int32(size))\n}\n\nfunc (binaryProtocol) MapEndLength() int {\n\treturn 0\n}\n\nfunc (binaryProtocol) ListBeginLength(elemType thrift.TType, size int) int {\n\treturn Binary.ByteLength(int8(elemType)) +\n\t\tBinary.I32Length(int32(size))\n}\n\nfunc (binaryProtocol) ListEndLength() int {\n\treturn 0\n}\n\nfunc (binaryProtocol) SetBeginLength(elemType thrift.TType, size int) int {\n\treturn Binary.ByteLength(int8(elemType)) +\n\t\tBinary.I32Length(int32(size))\n}\n\nfunc (binaryProtocol) SetEndLength() int {\n\treturn 0\n}\n\nfunc (binaryProtocol) BoolLength(value bool) int {\n\tif value {\n\t\treturn Binary.ByteLength(1)\n\t}\n\treturn Binary.ByteLength(0)\n}\n\nfunc (binaryProtocol) ByteLength(value int8) int {\n\treturn 1\n}\n\nfunc (binaryProtocol) I16Length(value int16) int {\n\treturn 2\n}\n\nfunc (binaryProtocol) I32Length(value int32) int {\n\treturn 4\n}\n\nfunc (binaryProtocol) I64Length(value int64) int {\n\treturn 8\n}\n\nfunc (binaryProtocol) DoubleLength(value float64) int {\n\treturn Binary.I64Length(int64(math.Float64bits(value)))\n}\n\nfunc (binaryProtocol) StringLength(value string) int {\n\treturn Binary.I32Length(int32(len(value))) + len(value)\n}\n\nfunc (binaryProtocol) BinaryLength(value []byte) int {\n\treturn Binary.I32Length(int32(len(value))) + len(value)\n}\n\nfunc (binaryProtocol) StringLengthNocopy(value string) int {\n\treturn Binary.BinaryLengthNocopy(utils.StringToSliceByte(value))\n}\n\nfunc (binaryProtocol) BinaryLengthNocopy(value []byte) int {\n\tl := Binary.I32Length(int32(len(value)))\n\treturn l + len(value)\n}\n\nfunc (binaryProtocol) ReadMessageBegin(buf []byte) (name string, typeID thrift.TMessageType, seqid int32, length int, err error) {\n\tsize, l, e := Binary.ReadI32(buf)\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tif size > 0 {\n\t\terr = perrors.NewProtocolErrorWithType(perrors.BadVersion, \"Missing version in ReadMessageBegin\")\n\t\treturn\n\t}\n\ttypeID = thrift.TMessageType(size & 0x0ff)\n\tversion := int64(size) & thrift.VERSION_MASK\n\tif version != thrift.VERSION_1 {\n\t\terr = perrors.NewProtocolErrorWithType(perrors.BadVersion, \"Bad version in ReadMessageBegin\")\n\t\treturn\n\t}\n\tname, l, e = Binary.ReadString(buf[length:])\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tseqid, l, e = Binary.ReadI32(buf[length:])\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\treturn\n}\n\nfunc (binaryProtocol) ReadMessageEnd(buf []byte) (int, error) {\n\treturn 0, nil\n}\n\nfunc (binaryProtocol) ReadStructBegin(buf []byte) (name string, length int, err error) {\n\treturn\n}\n\nfunc (binaryProtocol) ReadStructEnd(buf []byte) (int, error) {\n\treturn 0, nil\n}\n\nfunc (binaryProtocol) ReadFieldBegin(buf []byte) (name string, typeID thrift.TType, id int16, length int, err error) {\n\tt, l, e := Binary.ReadByte(buf)\n\tlength += l\n\ttypeID = thrift.TType(t)\n\tif e != nil {\n\t\terr = e\n\t\treturn\n\t}\n\tif t != thrift.STOP {\n\t\tid, l, err = Binary.ReadI16(buf[length:])\n\t\tlength += l\n\t}\n\treturn\n}\n\nfunc (binaryProtocol) ReadFieldEnd(buf []byte) (int, error) {\n\treturn 0, nil\n}\n\nfunc (binaryProtocol) ReadMapBegin(buf []byte) (keyType, valueType thrift.TType, size, length int, err error) {\n\tk, l, e := Binary.ReadByte(buf)\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tkeyType = thrift.TType(k)\n\tv, l, e := Binary.ReadByte(buf[length:])\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tvalueType = thrift.TType(v)\n\tsize32, l, e := Binary.ReadI32(buf[length:])\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tif size32 < 0 {\n\t\terr = perrors.InvalidDataLength\n\t\treturn\n\t}\n\tsize = int(size32)\n\treturn\n}\n\nfunc (binaryProtocol) ReadMapEnd(buf []byte) (int, error) {\n\treturn 0, nil\n}\n\nfunc (binaryProtocol) ReadListBegin(buf []byte) (elemType thrift.TType, size, length int, err error) {\n\tb, l, e := Binary.ReadByte(buf)\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\telemType = thrift.TType(b)\n\tsize32, l, e := Binary.ReadI32(buf[length:])\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tif size32 < 0 {\n\t\terr = perrors.InvalidDataLength\n\t\treturn\n\t}\n\tsize = int(size32)\n\n\treturn\n}\n\nfunc (binaryProtocol) ReadListEnd(buf []byte) (int, error) {\n\treturn 0, nil\n}\n\nfunc (binaryProtocol) ReadSetBegin(buf []byte) (elemType thrift.TType, size, length int, err error) {\n\tb, l, e := Binary.ReadByte(buf)\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\telemType = thrift.TType(b)\n\tsize32, l, e := Binary.ReadI32(buf[length:])\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tif size32 < 0 {\n\t\terr = perrors.InvalidDataLength\n\t\treturn\n\t}\n\tsize = int(size32)\n\treturn\n}\n\nfunc (binaryProtocol) ReadSetEnd(buf []byte) (int, error) {\n\treturn 0, nil\n}\n\nfunc (binaryProtocol) ReadBool(buf []byte) (value bool, length int, err error) {\n\tb, l, e := Binary.ReadByte(buf)\n\tv := true\n\tif b != 1 {\n\t\tv = false\n\t}\n\treturn v, l, e\n}\n\nfunc (binaryProtocol) ReadByte(buf []byte) (value int8, length int, err error) {\n\tif len(buf) < 1 {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadByte] buf length less than 1\")\n\t}\n\treturn int8(buf[0]), 1, err\n}\n\nfunc (binaryProtocol) ReadI16(buf []byte) (value int16, length int, err error) {\n\tif len(buf) < 2 {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadI16] buf length less than 2\")\n\t}\n\tvalue = int16(binary.BigEndian.Uint16(buf))\n\treturn value, 2, err\n}\n\nfunc (binaryProtocol) ReadI32(buf []byte) (value int32, length int, err error) {\n\tif len(buf) < 4 {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadI32] buf length less than 4\")\n\t}\n\tvalue = int32(binary.BigEndian.Uint32(buf))\n\treturn value, 4, err\n}\n\nfunc (binaryProtocol) ReadI64(buf []byte) (value int64, length int, err error) {\n\tif len(buf) < 8 {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadI64] buf length less than 8\")\n\t}\n\tvalue = int64(binary.BigEndian.Uint64(buf))\n\treturn value, 8, err\n}\n\nfunc (binaryProtocol) ReadDouble(buf []byte) (value float64, length int, err error) {\n\tif len(buf) < 8 {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadDouble] buf length less than 8\")\n\t}\n\tvalue = math.Float64frombits(binary.BigEndian.Uint64(buf))\n\treturn value, 8, err\n}\n\nfunc (binaryProtocol) ReadString(buf []byte) (value string, length int, err error) {\n\tsize, l, e := Binary.ReadI32(buf)\n\tlength += l\n\tif e != nil {\n\t\terr = e\n\t\treturn\n\t}\n\tif size < 0 || int(size) > len(buf) {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadString] the string size greater than buf length\")\n\t}\n\talloc := allocator\n\tif alloc != nil {\n\t\tdata := alloc.Copy(buf[length : length+int(size)])\n\t\tvalue = utils.SliceByteToString(data)\n\t} else {\n\t\tvalue = string(buf[length : length+int(size)])\n\t}\n\tlength += int(size)\n\treturn\n}\n\nfunc (binaryProtocol) ReadBinary(buf []byte) (value []byte, length int, err error) {\n\t_size, l, e := Binary.ReadI32(buf)\n\tlength += l\n\tif e != nil {\n\t\terr = e\n\t\treturn\n\t}\n\tsize := int(_size)\n\tif size < 0 || size > len(buf) {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadBinary] the binary size greater than buf length\")\n\t}\n\talloc := allocator\n\tif alloc != nil {\n\t\tvalue = alloc.Copy(buf[length : length+size])\n\t} else {\n\t\tvalue = make([]byte, size)\n\t\tcopy(value, buf[length:length+size])\n\t}\n\tlength += size\n\treturn\n}\n\n// Skip .\nfunc (binaryProtocol) Skip(buf []byte, fieldType thrift.TType) (length int, err error) {\n\treturn SkipDefaultDepth(buf, Binary, fieldType)\n}\n\n// SkipDefaultDepth skips over the next data element from the provided input TProtocol object.\nfunc SkipDefaultDepth(buf []byte, prot BTProtocol, typeID thrift.TType) (int, error) {\n\treturn Skip(buf, prot, typeID, thrift.DEFAULT_RECURSION_DEPTH)\n}\n\n// Skip skips over the next data element from the provided input TProtocol object.\nfunc Skip(buf []byte, self BTProtocol, fieldType thrift.TType, maxDepth int) (length int, err error) {\n\tif maxDepth <= 0 {\n\t\treturn 0, thrift.NewTProtocolExceptionWithType(thrift.DEPTH_LIMIT, errors.New(\"depth limit exceeded\"))\n\t}\n\n\tvar l int\n\tswitch fieldType {\n\tcase thrift.BOOL:\n\t\tlength += 1\n\t\treturn\n\tcase thrift.BYTE:\n\t\tlength += 1\n\t\treturn\n\tcase thrift.I16:\n\t\tlength += 2\n\t\treturn\n\tcase thrift.I32:\n\t\tlength += 4\n\t\treturn\n\tcase thrift.I64:\n\t\tlength += 8\n\t\treturn\n\tcase thrift.DOUBLE:\n\t\tlength += 8\n\t\treturn\n\tcase thrift.STRING:\n\t\tvar sl int32\n\t\tsl, l, err = self.ReadI32(buf)\n\t\tlength += l + int(sl)\n\t\treturn\n\tcase thrift.STRUCT:\n\t\t_, l, err = self.ReadStructBegin(buf)\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tfor {\n\t\t\t_, typeID, _, l, e := self.ReadFieldBegin(buf[length:])\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif typeID == thrift.STOP {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tl, e = Skip(buf[length:], self, typeID, maxDepth-1)\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t\tl, e = self.ReadFieldEnd(buf[length:])\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tl, e := self.ReadStructEnd(buf[length:])\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t}\n\t\treturn\n\tcase thrift.MAP:\n\t\tkeyType, valueType, size, l, e := self.ReadMapBegin(buf)\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t\treturn\n\t\t}\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, e := Skip(buf[length:], self, keyType, maxDepth-1)\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t\tl, e = Skip(buf[length:], self, valueType, maxDepth-1)\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tl, e = self.ReadMapEnd(buf[length:])\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t}\n\t\treturn\n\tcase thrift.SET:\n\t\telemType, size, l, e := self.ReadSetBegin(buf)\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t\treturn\n\t\t}\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, e = Skip(buf[length:], self, elemType, maxDepth-1)\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tl, e = self.ReadSetEnd(buf[length:])\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t}\n\t\treturn\n\tcase thrift.LIST:\n\t\telemType, size, l, e := self.ReadListBegin(buf)\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t\treturn\n\t\t}\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, e = Skip(buf[length:], self, elemType, maxDepth-1)\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tl, e = self.ReadListEnd(buf[length:])\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t}\n\t\treturn\n\tdefault:\n\t\treturn 0, thrift.NewTProtocolExceptionWithType(thrift.INVALID_DATA, fmt.Errorf(\"unknown data type %d\", fieldType))\n\t}\n}\n\n```\n# Function Signature: func SetSpanCache(enable bool) {\n# Natural Language Annotations: // SetSpanCache enable/disable binary protocol bytes/string allocator\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc SetSpanCache(enable bool) {\n\tif enable {\n\t\tallocator = SpanCacheAllocator\n\t} else {\n\t\tallocator = DefaultAllocator\n\t}\n}\n```", "  ```\nfunc SetSpanCache(enable bool) {\n    if enable {\n        allocator = mem.NewAllocator(mem.BlockSize(binaryInplaceThreshold))\n    } else {\n        allocator = nil\n    }\n}\n```", "  ```\nfunc SetSpanCache(enable bool) {\n\tallocator = NewNoopAllocator()\n\tif enable {\n\t\tallocator = NewSpanAllocator()\n\t}\n}\n\nfunc NewSpanAllocator() *SpanAllocator {\n\treturn &SpanAllocator{\n\t\tspanMap:   make(map[string]interface{}),\n\t\theapCache: make(map[string]string, 1024*1024), // cache 1MB worth of spans\n\t}\n}\n\ntype SpanAllocator struct {\n\tspanMap   map[string]interface{}\n\theapCache map[string]string\n\tmu        sync.Mutex\n}\n\nfunc (a *SpanAllocator) Allocate(sz int) interface{} {\n\ta.mu.Lock()\n\tdefer a.mu.Unlock()\n\n\tif spanBytes, ok := a.heapCache[sz]; ok {\n\t\tdelete(a.heapCache, sz)\n\t\treturn spanBytes\n\t}\n\n\tspanBytes := make([]byte, sz)\n\ta.spanMap[string(spanBytes)] = struct{}{}\n\treturn spanBytes\n}\n\nfunc (a *SpanAllocator) Free(buf interface{}) {\n\ta.mu.Lock()\n\tdefer a.mu.Unlock()\n\n\tbufBytes, ok := buf.([]byte)\n\tif !ok {\n\t\treturn\n\t}\n\n\tdelete(a.spanMap, string(bufBytes))\n\ta.heapCache[len(bufBytes)] = bufBytes\n}\n\nfunc (a *SpanAllocator) Copy(buf interface{}) []byte {\n\tbufBytes, ok := buf.([]byte)\n\tif !ok {\n\t\treturn nil\n\t}\n\tcopied := make([]byte, len(bufBytes))\n\tcopy(copied, bufBytes)\n\treturn copied\n}\n\nfunc (a *SpanAllocator) Release() {\n\ta.spanMap = nil\n\ta.heapCache = nil\n}\n\nfunc NewNoopAllocator() *NoopAllocator {\n\treturn &NoopAllocator{}\n}\n\ntype NoopAllocator struct{}\n\nfunc (a *NoopAllocator) Allocate(sz int) interface{} {\n\tspanBytes := make([]byte, sz)\n\treturn spanBytes\n}\n\nfunc (a *NoopAllocator) Free(buf interface{}) {\n\treturn\n}\n\nfunc (a *NoopAllocator) Copy(buf interface{}) []byte {\n\tbufBytes, ok := buf.([]byte)\n\tif !ok {\n\t\treturn nil\n\t}\n\tcopied := make([]byte, len(bufBytes))\n\tcopy(copied, bufBytes)\n\treturn copied\n}\n\nfunc (a *NoopAllocator) Release() {\n\treturn\n}```"]}, "kitex-pkg/protocol/bthrift/unknown.go-GetUnknownFields": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/protocol/bthrift/unknown.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage bthrift\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/cloudwego/thriftgo/generator/golang/extension/unknown\"\n)\n\n// UnknownField is used to describe an unknown field.\ntype UnknownField struct {\n\tName    string\n\tID      int16\n\tType    int\n\tKeyType int\n\tValType int\n\tValue   interface{}\n}\n\n// GetUnknownFields deserialize unknownFields stored in v to a list of *UnknownFields.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ConvertUnknownFields converts buf to deserialized unknown fields.\nfunc ConvertUnknownFields(buf unknown.Fields) (fields []UnknownField, err error) {\n\tif len(buf) == 0 {\n\t\treturn nil, errors.New(\"_unknownFields is empty\")\n\t}\n\tvar offset int\n\tvar l int\n\tvar name string\n\tvar fieldTypeId thrift.TType\n\tvar fieldId int16\n\tvar f UnknownField\n\tfor {\n\t\tif offset == len(buf) {\n\t\t\treturn\n\t\t}\n\t\tname, fieldTypeId, fieldId, l, err = Binary.ReadFieldBegin(buf[offset:])\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"read field %d begin error: %v\", fieldId, err)\n\t\t}\n\t\tl, err = readUnknownField(&f, buf[offset:], name, fieldTypeId, fieldId)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"read unknown field %d error: %v\", fieldId, err)\n\t\t}\n\t\tfields = append(fields, f)\n\t}\n}\n\nfunc readUnknownField(f *UnknownField, buf []byte, name string, fieldType thrift.TType, id int16) (length int, err error) {\n\tvar size int\n\tvar l int\n\tf.Name = name\n\tf.ID = id\n\tf.Type = int(fieldType)\n\tswitch fieldType {\n\tcase thrift.BOOL:\n\t\tf.Value, l, err = Binary.ReadBool(buf[length:])\n\t\tlength += l\n\tcase thrift.BYTE:\n\t\tf.Value, l, err = Binary.ReadByte(buf[length:])\n\t\tlength += l\n\tcase thrift.I16:\n\t\tf.Value, l, err = Binary.ReadI16(buf[length:])\n\t\tlength += l\n\tcase thrift.I32:\n\t\tf.Value, l, err = Binary.ReadI32(buf[length:])\n\t\tlength += l\n\tcase thrift.I64:\n\t\tf.Value, l, err = Binary.ReadI64(buf[length:])\n\t\tlength += l\n\tcase thrift.DOUBLE:\n\t\tf.Value, l, err = Binary.ReadDouble(buf[length:])\n\t\tlength += l\n\tcase thrift.STRING:\n\t\tf.Value, l, err = Binary.ReadString(buf[length:])\n\t\tlength += l\n\tcase thrift.SET:\n\t\tvar ttype thrift.TType\n\t\tttype, size, l, err = Binary.ReadSetBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read set begin error: %w\", err)\n\t\t}\n\t\tf.ValType = int(ttype)\n\t\tset := make([]UnknownField, size)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&set[i], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read set elem error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadSetEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read set end error: %w\", err)\n\t\t}\n\t\tf.Value = set\n\tcase thrift.LIST:\n\t\tvar ttype thrift.TType\n\t\tttype, size, l, err = Binary.ReadListBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read list begin error: %w\", err)\n\t\t}\n\t\tf.ValType = int(ttype)\n\t\tlist := make([]UnknownField, size)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&list[i], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read list elem error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadListEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read list end error: %w\", err)\n\t\t}\n\t\tf.Value = list\n\tcase thrift.MAP:\n\t\tvar kttype, vttype thrift.TType\n\t\tkttype, vttype, size, l, err = Binary.ReadMapBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read map begin error: %w\", err)\n\t\t}\n\t\tf.KeyType = int(kttype)\n\t\tf.ValType = int(vttype)\n\t\tflatMap := make([]UnknownField, size*2)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&flatMap[2*i], buf[length:], \"\", thrift.TType(f.KeyType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read map key error: %w\", err2)\n\t\t\t}\n\t\t\tl, err2 = readUnknownField(&flatMap[2*i+1], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read map value error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadMapEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read map end error: %w\", err)\n\t\t}\n\t\tf.Value = flatMap\n\tcase thrift.STRUCT:\n\t\t_, l, err = Binary.ReadStructBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read struct begin error: %w\", err)\n\t\t}\n\t\tvar field UnknownField\n\t\tvar fields []UnknownField\n\t\tfor {\n\t\t\tname, fieldTypeID, fieldID, l, err := Binary.ReadFieldBegin(buf[length:])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read field begin error: %w\", err)\n\t\t\t}\n\t\t\tif fieldTypeID == thrift.STOP {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tl, err = readUnknownField(&field, buf[length:], name, fieldTypeID, fieldID)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read struct field error: %w\", err)\n\t\t\t}\n\t\t\tl, err = Binary.ReadFieldEnd(buf[length:])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read field end error: %w\", err)\n\t\t\t}\n\t\t\tfields = append(fields, field)\n\t\t}\n\t\tl, err = Binary.ReadStructEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read struct end error: %w\", err)\n\t\t}\n\t\tf.Value = fields\n\tdefault:\n\t\treturn length, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\tif err != nil {\n\t\treturn length, err\n\t}\n\treturn\n}\n\n// UnknownFieldsLength returns the length of fs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc unknownFieldLength(f *UnknownField) (length int, err error) {\n\t// use constants to avoid some type assert\n\tswitch f.Type {\n\tcase unknown.TBool:\n\t\tlength += Binary.BoolLength(false)\n\tcase unknown.TByte:\n\t\tlength += Binary.ByteLength(0)\n\tcase unknown.TDouble:\n\t\tlength += Binary.DoubleLength(0)\n\tcase unknown.TI16:\n\t\tlength += Binary.I16Length(0)\n\tcase unknown.TI32:\n\t\tlength += Binary.I32Length(0)\n\tcase unknown.TI64:\n\t\tlength += Binary.I64Length(0)\n\tcase unknown.TString:\n\t\tlength += Binary.StringLength(f.Value.(string))\n\tcase unknown.TSet:\n\t\tvs := f.Value.([]UnknownField)\n\t\tlength += Binary.SetBeginLength(thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := unknownFieldLength(&v)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.SetEndLength()\n\tcase unknown.TList:\n\t\tvs := f.Value.([]UnknownField)\n\t\tlength += Binary.ListBeginLength(thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := unknownFieldLength(&v)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.ListEndLength()\n\tcase unknown.TMap:\n\t\tkvs := f.Value.([]UnknownField)\n\t\tlength += Binary.MapBeginLength(thrift.TType(f.KeyType), thrift.TType(f.ValType), len(kvs)/2)\n\t\tfor i := 0; i < len(kvs); i += 2 {\n\t\t\tl, err := unknownFieldLength(&kvs[i])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t\tl, err = unknownFieldLength(&kvs[i+1])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.MapEndLength()\n\tcase unknown.TStruct:\n\t\tfs := f.Value.([]UnknownField)\n\t\tlength += Binary.StructBeginLength(f.Name)\n\t\tl, err := UnknownFieldsLength(fs)\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, err\n\t\t}\n\t\tlength += Binary.FieldStopLength()\n\t\tlength += Binary.StructEndLength()\n\tdefault:\n\t\treturn length, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\treturn\n}\n\n// WriteUnknownFields writes fs into buf, and return written offset of the buf.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc writeUnknownField(buf []byte, f *UnknownField) (offset int, err error) {\n\tswitch f.Type {\n\tcase unknown.TBool:\n\t\toffset += Binary.WriteBool(buf, f.Value.(bool))\n\tcase unknown.TByte:\n\t\toffset += Binary.WriteByte(buf, f.Value.(int8))\n\tcase unknown.TDouble:\n\t\toffset += Binary.WriteDouble(buf, f.Value.(float64))\n\tcase unknown.TI16:\n\t\toffset += Binary.WriteI16(buf, f.Value.(int16))\n\tcase unknown.TI32:\n\t\toffset += Binary.WriteI32(buf, f.Value.(int32))\n\tcase unknown.TI64:\n\t\toffset += Binary.WriteI64(buf, f.Value.(int64))\n\tcase unknown.TString:\n\t\toffset += Binary.WriteString(buf, f.Value.(string))\n\tcase unknown.TSet:\n\t\tvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteSetBegin(buf, thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := writeUnknownField(buf[offset:], &v)\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteSetEnd(buf[offset:])\n\tcase unknown.TList:\n\t\tvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteListBegin(buf, thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := writeUnknownField(buf[offset:], &v)\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteListEnd(buf[offset:])\n\tcase unknown.TMap:\n\t\tkvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteMapBegin(buf, thrift.TType(f.KeyType), thrift.TType(f.ValType), len(kvs)/2)\n\t\tfor i := 0; i < len(kvs); i += 2 {\n\t\t\tl, err := writeUnknownField(buf[offset:], &kvs[i])\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t\tl, err = writeUnknownField(buf[offset:], &kvs[i+1])\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteMapEnd(buf[offset:])\n\tcase unknown.TStruct:\n\t\tfs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteStructBegin(buf, f.Name)\n\t\tl, err := WriteUnknownFields(buf[offset:], fs)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn offset, err\n\t\t}\n\t\toffset += Binary.WriteFieldStop(buf[offset:])\n\t\toffset += Binary.WriteStructEnd(buf[offset:])\n\tdefault:\n\t\treturn offset, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\treturn\n}\n\n```\n# Function Signature: func GetUnknownFields(v interface{}) (fields []UnknownField, err error) {\n# Natural Language Annotations: // GetUnknownFields deserialize unknownFields stored in v to a list of *UnknownFields.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc GetUnknownFields(v interface{}) (fields []UnknownField, err error) {\n\tvalue := reflect.ValueOf(v)\n\tif value.Kind() != reflect.Struct {\n\t\treturn nil, errors.New(\"v must be a struct value\")\n\t}\n\tfields = make([]UnknownField, 0)\n\tfor i := 0; i < value.NumField(); i++ {\n\t\tfield := value.Field(i)\n\t\tif field.CanInterface() {\n\t\t\tfieldType := field.Type()\n\t\t\tfieldValue := field.Interface()\n\t\t\tvar typeId int16\n\t\t\tvar id int16\n\t\t\tvar name string\n\t\t\tswitch t := fieldType.(type) {\n\t\t\tcase *reflect.BoolType:\n\t\t\t\ttypeId = v.T_bool\n\t\t\t\tid = int16(i)\n\t\t\t\tname = \"bool\"\n\t\t\tcase *reflect.IntType:\n\t\t\t\ttypeId = v.T_i16\n\t\t\t\tid = int16(i)\n\t\t\t\tname = \"int\"\n\t\t\tcase *reflect.Float64Type:\n\t\t\t\ttypeId = v.T_double\n\t\t\t\tid = int16(i)\n\t\t\t\tname = \"double\"\n\t\t\tcase *reflect.StringType:\n\t\t\t\ttypeId = v.T_string\n\t\t\t\tid = int16(i)\n\t\t\t\tname = \"string\"\n\t\t\tdefault:\n\t\t\t\treturn nil, errors.New(\"unsupported type\")\n\t\t\t}\n\t\t\tfield := UnknownField{\n\t\t\t\tName:    name,\n\t\t\t\tID:      id,\n\t\t\t\tType:    int(typeId),\n\t\t\t\tKeyType: 0,\n\t\t\t\tValType: 0,\n\t\t\t\tValue:   fieldValue,\n\t\t\t}\n\t\t\tfields = append(fields, field)\n\t\t}\n\t}\n\treturn fields, nil\n}\n```", "  ```\nfunc GetUnknownFields(v interface{}) (fields []UnknownField, err error) {\n\tif v == nil {\n\t\treturn nil, errors.New(\"v is nil\")\n\t}\n\tr := reflect.ValueOf(v)\n\tt := r.Type()\n\tif t.Kind() == reflect.Ptr {\n\t\tt = t.Elem()\n\t}\n\tif t.Kind() != reflect.Struct {\n\t\treturn nil, fmt.Errorf(\"v is not a struct: %v\", t.Kind())\n\t}\n\tif t.Name() == \"Struct\" {\n\t\treturn convertStructFields(r), nil\n\t}\n\tfields = make([]UnknownField, t.NumField())\n\tfor i := 0; i < t.NumField(); i++ {\n\t\tf := t.Field(i)\n\t\tfieldName := f.Name\n\t\tfieldType := f.Type\n\t\tfieldTag := f.Tag\n\t\tif fieldTag != \"\" {\n\t\t\ttag := reflect.StructTag(fieldTag.String())\n\t\t\tfieldType, _ = tag.Lookup(\"type\")\n\t\t\tfieldName, _ := tag.Lookup(\"name\")\n\t\t}\n\t\tswitch fieldType.Kind() {\n\t\tcase reflect.Bool:\n\t\t\tfields[i] = UnknownField{\n\t\t\t\tName:   fieldName,\n\t\t\t\tID:     int16(i + 1),\n\t\t\t\tType:   int(fieldType.Kind()),\n\t\t\t\tValue:  r.Field(i).Bool(),\n\t\t\t}\n\t\tcase reflect.Int8:\n\t\t\tfields[i] = UnknownField{\n\t\t\t\tName:   fieldName,\n\t\t\t\tID:     int16(i + 1),\n\t\t\t\tType:   int(fieldType.Kind()),\n\t\t\t\tValue:  r.Field(i).Int(),\n\t\t\t}\n\t\tcase reflect.Int16:\n\t\t\tfields[i] = UnknownField{\n\t\t\t\tName:   fieldName,\n\t\t\t\tID:     int16(i + 1),\n\t\t\t\tType:   int(fieldType.Kind()),\n\t\t\t\tValue:  r.Field(i).Int(),\n\t\t\t}\n\t\tcase reflect.Int32:\n\t\t\tfields[i] = UnknownField{\n\t\t\t\tName:   fieldName,\n\t\t\t\tID:     int16(i + 1),\n\t\t\t\tType:   int(fieldType.Kind()),\n\t\t\t\tValue:  r.Field(i).Int(),\n\t\t\t}\n\t\tcase reflect.Int64:\n\t\t\tfields[i] = UnknownField{\n\t\t\t\tName:   fieldName,\n\t\t\t\tID:     int16(i + 1),\n\t\t\t\tType:   int(fieldType.Kind()),\n\t\t\t\tValue:  r.Field(i).Int(),\n\t\t\t}\n\t\tcase reflect.Float32:\n\t\t\tfields[i] = UnknownField{\n\t\t\t\tName:   fieldName,\n\t\t\t\tID:     int16(i + 1),\n\t\t\t\tType:   int(fieldType.Kind()),\n\t\t\t\tValue:  r.Field(i).Float(),\n\t\t\t}\n\t\tcase reflect.Float64:\n\t\t\tfields[i] = UnknownField{\n\t\t\t\tName:   fieldName,\n\t\t\t\tID:     int16(i + 1),\n\t\t\t\tType:   int(fieldType.Kind()),\n\t\t\t\tValue:  r.Field(i).Float(),\n\t\t\t}\n\t\tcase reflect.String:\n\t\t\tfields[i] = UnknownField{\n\t\t\t\tName:   fieldName,\n\t\t\t\tID:     int16(i + 1),\n\t\t\t\tType:   int(fieldType.Kind()),\n\t\t\t\tValue:  r.Field(i).String(),\n\t\t\t}\n\t\tcase reflect.Ptr:\n\t\t\tif fieldType.Elem().Kind() == reflect.Struct {\n\t\t\t\tstructFields := convertStruct", "  ```\npackage main\n\nimport (\n\t\"reflect\"\n\n\t\"github.com/cloudwego/thriftgo/generator/golang/extension/unknown\"\n)\n\nfunc GetUnknownFields(v interface{}) (fields []UnknownField, err error) {\n\trv := reflect.ValueOf(v)\n\tif rv.Kind() != reflect.Ptr || rv.Elem().Kind() != reflect.Struct {\n\t\terr = fmt.Errorf(\"invalid v: %v\", v)\n\t\treturn\n\t}\n\tst := rv.Elem()\n\tn := st.NumField()\n\tfields = make([]UnknownField, 0, n)\n\tfor i := 0; i < n; i++ {\n\t\tf := st.Field(i)\n\t\tt := f.Type()\n\t\tname := t.Name()\n\t\tid, ok := unknown.GetFieldID(name)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tvar ftype, vtype int\n\t\tswitch f.Kind() {\n\t\tcase reflect.Bool:\n\t\t\tftype = unknown.TBool\n\t\tcase reflect.Int8:\n\t\t\tftype = unknown.TByte\n\t\tcase reflect.Float64:\n\t\t\tftype = unknown.TDouble\n\t\tcase reflect.Int16:\n\t\t\tftype = unknown.TI16\n\t\tcase reflect.Int32:\n\t\t\tftype = unknown.TI32\n\t\tcase reflect.Int64:\n\t\t\tftype = unknown.TI64\n\t\tcase reflect.String:\n\t\t\tftype = unknown.TString\n\t\tcase reflect.Slice:\n\t\t\tfallthrough\n\t\tcase reflect.Array:\n\t\t\tif t.Elem().Kind() == reflect.Bool {\n\t\t\t\tftype = unknown.TSet\n\t\t\t\tvtype = unknown.TBool\n\t\t\t} else {\n\t\t\t\tftype = unknown.TList\n\t\t\t}\n\t\tcase reflect.Map:\n\t\t\tftype = unknown.TMap\n\t\t\tkty := t.Key()\n\t\t\tif kty.Kind() == reflect.Bool {\n\t\t\t\tvtype = unknown.TBool\n\t\t\t} else {\n\t\t\t\tvtype = unknown.TString\n\t\t\t}\n\t\tcase reflect.Struct:\n\t\t\tftype = unknown.TStruct\n\t\t\tvtype = unknown.TUnknown\n\t\tdefault:\n\t\t\tftype = unknown.TUnknown\n\t\t}\n\t\tfields = append(fields, UnknownField{ID: id, Name: name, Type: ftype, ValType: vtype, Value: f})\n\t}\n\treturn\n}\n```"]}, "kitex-pkg/protocol/bthrift/unknown.go-UnknownFieldsLength": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/protocol/bthrift/unknown.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage bthrift\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/cloudwego/thriftgo/generator/golang/extension/unknown\"\n)\n\n// UnknownField is used to describe an unknown field.\ntype UnknownField struct {\n\tName    string\n\tID      int16\n\tType    int\n\tKeyType int\n\tValType int\n\tValue   interface{}\n}\n\n// GetUnknownFields deserialize unknownFields stored in v to a list of *UnknownFields.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ConvertUnknownFields converts buf to deserialized unknown fields.\nfunc ConvertUnknownFields(buf unknown.Fields) (fields []UnknownField, err error) {\n\tif len(buf) == 0 {\n\t\treturn nil, errors.New(\"_unknownFields is empty\")\n\t}\n\tvar offset int\n\tvar l int\n\tvar name string\n\tvar fieldTypeId thrift.TType\n\tvar fieldId int16\n\tvar f UnknownField\n\tfor {\n\t\tif offset == len(buf) {\n\t\t\treturn\n\t\t}\n\t\tname, fieldTypeId, fieldId, l, err = Binary.ReadFieldBegin(buf[offset:])\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"read field %d begin error: %v\", fieldId, err)\n\t\t}\n\t\tl, err = readUnknownField(&f, buf[offset:], name, fieldTypeId, fieldId)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"read unknown field %d error: %v\", fieldId, err)\n\t\t}\n\t\tfields = append(fields, f)\n\t}\n}\n\nfunc readUnknownField(f *UnknownField, buf []byte, name string, fieldType thrift.TType, id int16) (length int, err error) {\n\tvar size int\n\tvar l int\n\tf.Name = name\n\tf.ID = id\n\tf.Type = int(fieldType)\n\tswitch fieldType {\n\tcase thrift.BOOL:\n\t\tf.Value, l, err = Binary.ReadBool(buf[length:])\n\t\tlength += l\n\tcase thrift.BYTE:\n\t\tf.Value, l, err = Binary.ReadByte(buf[length:])\n\t\tlength += l\n\tcase thrift.I16:\n\t\tf.Value, l, err = Binary.ReadI16(buf[length:])\n\t\tlength += l\n\tcase thrift.I32:\n\t\tf.Value, l, err = Binary.ReadI32(buf[length:])\n\t\tlength += l\n\tcase thrift.I64:\n\t\tf.Value, l, err = Binary.ReadI64(buf[length:])\n\t\tlength += l\n\tcase thrift.DOUBLE:\n\t\tf.Value, l, err = Binary.ReadDouble(buf[length:])\n\t\tlength += l\n\tcase thrift.STRING:\n\t\tf.Value, l, err = Binary.ReadString(buf[length:])\n\t\tlength += l\n\tcase thrift.SET:\n\t\tvar ttype thrift.TType\n\t\tttype, size, l, err = Binary.ReadSetBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read set begin error: %w\", err)\n\t\t}\n\t\tf.ValType = int(ttype)\n\t\tset := make([]UnknownField, size)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&set[i], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read set elem error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadSetEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read set end error: %w\", err)\n\t\t}\n\t\tf.Value = set\n\tcase thrift.LIST:\n\t\tvar ttype thrift.TType\n\t\tttype, size, l, err = Binary.ReadListBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read list begin error: %w\", err)\n\t\t}\n\t\tf.ValType = int(ttype)\n\t\tlist := make([]UnknownField, size)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&list[i], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read list elem error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadListEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read list end error: %w\", err)\n\t\t}\n\t\tf.Value = list\n\tcase thrift.MAP:\n\t\tvar kttype, vttype thrift.TType\n\t\tkttype, vttype, size, l, err = Binary.ReadMapBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read map begin error: %w\", err)\n\t\t}\n\t\tf.KeyType = int(kttype)\n\t\tf.ValType = int(vttype)\n\t\tflatMap := make([]UnknownField, size*2)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&flatMap[2*i], buf[length:], \"\", thrift.TType(f.KeyType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read map key error: %w\", err2)\n\t\t\t}\n\t\t\tl, err2 = readUnknownField(&flatMap[2*i+1], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read map value error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadMapEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read map end error: %w\", err)\n\t\t}\n\t\tf.Value = flatMap\n\tcase thrift.STRUCT:\n\t\t_, l, err = Binary.ReadStructBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read struct begin error: %w\", err)\n\t\t}\n\t\tvar field UnknownField\n\t\tvar fields []UnknownField\n\t\tfor {\n\t\t\tname, fieldTypeID, fieldID, l, err := Binary.ReadFieldBegin(buf[length:])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read field begin error: %w\", err)\n\t\t\t}\n\t\t\tif fieldTypeID == thrift.STOP {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tl, err = readUnknownField(&field, buf[length:], name, fieldTypeID, fieldID)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read struct field error: %w\", err)\n\t\t\t}\n\t\t\tl, err = Binary.ReadFieldEnd(buf[length:])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read field end error: %w\", err)\n\t\t\t}\n\t\t\tfields = append(fields, field)\n\t\t}\n\t\tl, err = Binary.ReadStructEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read struct end error: %w\", err)\n\t\t}\n\t\tf.Value = fields\n\tdefault:\n\t\treturn length, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\tif err != nil {\n\t\treturn length, err\n\t}\n\treturn\n}\n\n// UnknownFieldsLength returns the length of fs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc unknownFieldLength(f *UnknownField) (length int, err error) {\n\t// use constants to avoid some type assert\n\tswitch f.Type {\n\tcase unknown.TBool:\n\t\tlength += Binary.BoolLength(false)\n\tcase unknown.TByte:\n\t\tlength += Binary.ByteLength(0)\n\tcase unknown.TDouble:\n\t\tlength += Binary.DoubleLength(0)\n\tcase unknown.TI16:\n\t\tlength += Binary.I16Length(0)\n\tcase unknown.TI32:\n\t\tlength += Binary.I32Length(0)\n\tcase unknown.TI64:\n\t\tlength += Binary.I64Length(0)\n\tcase unknown.TString:\n\t\tlength += Binary.StringLength(f.Value.(string))\n\tcase unknown.TSet:\n\t\tvs := f.Value.([]UnknownField)\n\t\tlength += Binary.SetBeginLength(thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := unknownFieldLength(&v)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.SetEndLength()\n\tcase unknown.TList:\n\t\tvs := f.Value.([]UnknownField)\n\t\tlength += Binary.ListBeginLength(thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := unknownFieldLength(&v)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.ListEndLength()\n\tcase unknown.TMap:\n\t\tkvs := f.Value.([]UnknownField)\n\t\tlength += Binary.MapBeginLength(thrift.TType(f.KeyType), thrift.TType(f.ValType), len(kvs)/2)\n\t\tfor i := 0; i < len(kvs); i += 2 {\n\t\t\tl, err := unknownFieldLength(&kvs[i])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t\tl, err = unknownFieldLength(&kvs[i+1])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.MapEndLength()\n\tcase unknown.TStruct:\n\t\tfs := f.Value.([]UnknownField)\n\t\tlength += Binary.StructBeginLength(f.Name)\n\t\tl, err := UnknownFieldsLength(fs)\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, err\n\t\t}\n\t\tlength += Binary.FieldStopLength()\n\t\tlength += Binary.StructEndLength()\n\tdefault:\n\t\treturn length, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\treturn\n}\n\n// WriteUnknownFields writes fs into buf, and return written offset of the buf.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc writeUnknownField(buf []byte, f *UnknownField) (offset int, err error) {\n\tswitch f.Type {\n\tcase unknown.TBool:\n\t\toffset += Binary.WriteBool(buf, f.Value.(bool))\n\tcase unknown.TByte:\n\t\toffset += Binary.WriteByte(buf, f.Value.(int8))\n\tcase unknown.TDouble:\n\t\toffset += Binary.WriteDouble(buf, f.Value.(float64))\n\tcase unknown.TI16:\n\t\toffset += Binary.WriteI16(buf, f.Value.(int16))\n\tcase unknown.TI32:\n\t\toffset += Binary.WriteI32(buf, f.Value.(int32))\n\tcase unknown.TI64:\n\t\toffset += Binary.WriteI64(buf, f.Value.(int64))\n\tcase unknown.TString:\n\t\toffset += Binary.WriteString(buf, f.Value.(string))\n\tcase unknown.TSet:\n\t\tvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteSetBegin(buf, thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := writeUnknownField(buf[offset:], &v)\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteSetEnd(buf[offset:])\n\tcase unknown.TList:\n\t\tvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteListBegin(buf, thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := writeUnknownField(buf[offset:], &v)\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteListEnd(buf[offset:])\n\tcase unknown.TMap:\n\t\tkvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteMapBegin(buf, thrift.TType(f.KeyType), thrift.TType(f.ValType), len(kvs)/2)\n\t\tfor i := 0; i < len(kvs); i += 2 {\n\t\t\tl, err := writeUnknownField(buf[offset:], &kvs[i])\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t\tl, err = writeUnknownField(buf[offset:], &kvs[i+1])\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteMapEnd(buf[offset:])\n\tcase unknown.TStruct:\n\t\tfs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteStructBegin(buf, f.Name)\n\t\tl, err := WriteUnknownFields(buf[offset:], fs)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn offset, err\n\t\t}\n\t\toffset += Binary.WriteFieldStop(buf[offset:])\n\t\toffset += Binary.WriteStructEnd(buf[offset:])\n\tdefault:\n\t\treturn offset, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\treturn\n}\n\n```\n# Function Signature: func UnknownFieldsLength(fs []UnknownField) (int, error) {\n# Natural Language Annotations: // UnknownFieldsLength returns the length of fs.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc UnknownFieldsLength(fs []UnknownField) (int, error) {\n\tlength := 0\n\tfor _, f := range fs {\n\t\tl, err := unknownFieldLength(&f)\n\t\tif err != nil {\n\t\t\treturn length, err\n\t\t}\n\t\tlength += l\n\t}\n\treturn length, nil\n}\n```", "  ```\nfunc UnknownFieldsLength(fs []UnknownField) (length int, err error) {\n\tfor _, f := range fs {\n\t\tl, err := unknownFieldLength(&f)\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, err\n\t\t}\n\t}\n\treturn\n}\n```", "  ```\nfunc UnknownFieldsLength(fs []UnknownField) (int, error) {\n\tvar err error\n\tvar length int\n\tfor _, f := range fs {\n\t\tl, err2 := unknownFieldLength(&f)\n\t\tlength += l\n\t\tif err2 != nil {\n\t\t\terr = err2\n\t\t}\n\t}\n\treturn length, err\n}\n```"]}, "kitex-pkg/protocol/bthrift/unknown.go-WriteUnknownFields": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/protocol/bthrift/unknown.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage bthrift\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/cloudwego/thriftgo/generator/golang/extension/unknown\"\n)\n\n// UnknownField is used to describe an unknown field.\ntype UnknownField struct {\n\tName    string\n\tID      int16\n\tType    int\n\tKeyType int\n\tValType int\n\tValue   interface{}\n}\n\n// GetUnknownFields deserialize unknownFields stored in v to a list of *UnknownFields.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ConvertUnknownFields converts buf to deserialized unknown fields.\nfunc ConvertUnknownFields(buf unknown.Fields) (fields []UnknownField, err error) {\n\tif len(buf) == 0 {\n\t\treturn nil, errors.New(\"_unknownFields is empty\")\n\t}\n\tvar offset int\n\tvar l int\n\tvar name string\n\tvar fieldTypeId thrift.TType\n\tvar fieldId int16\n\tvar f UnknownField\n\tfor {\n\t\tif offset == len(buf) {\n\t\t\treturn\n\t\t}\n\t\tname, fieldTypeId, fieldId, l, err = Binary.ReadFieldBegin(buf[offset:])\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"read field %d begin error: %v\", fieldId, err)\n\t\t}\n\t\tl, err = readUnknownField(&f, buf[offset:], name, fieldTypeId, fieldId)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"read unknown field %d error: %v\", fieldId, err)\n\t\t}\n\t\tfields = append(fields, f)\n\t}\n}\n\nfunc readUnknownField(f *UnknownField, buf []byte, name string, fieldType thrift.TType, id int16) (length int, err error) {\n\tvar size int\n\tvar l int\n\tf.Name = name\n\tf.ID = id\n\tf.Type = int(fieldType)\n\tswitch fieldType {\n\tcase thrift.BOOL:\n\t\tf.Value, l, err = Binary.ReadBool(buf[length:])\n\t\tlength += l\n\tcase thrift.BYTE:\n\t\tf.Value, l, err = Binary.ReadByte(buf[length:])\n\t\tlength += l\n\tcase thrift.I16:\n\t\tf.Value, l, err = Binary.ReadI16(buf[length:])\n\t\tlength += l\n\tcase thrift.I32:\n\t\tf.Value, l, err = Binary.ReadI32(buf[length:])\n\t\tlength += l\n\tcase thrift.I64:\n\t\tf.Value, l, err = Binary.ReadI64(buf[length:])\n\t\tlength += l\n\tcase thrift.DOUBLE:\n\t\tf.Value, l, err = Binary.ReadDouble(buf[length:])\n\t\tlength += l\n\tcase thrift.STRING:\n\t\tf.Value, l, err = Binary.ReadString(buf[length:])\n\t\tlength += l\n\tcase thrift.SET:\n\t\tvar ttype thrift.TType\n\t\tttype, size, l, err = Binary.ReadSetBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read set begin error: %w\", err)\n\t\t}\n\t\tf.ValType = int(ttype)\n\t\tset := make([]UnknownField, size)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&set[i], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read set elem error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadSetEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read set end error: %w\", err)\n\t\t}\n\t\tf.Value = set\n\tcase thrift.LIST:\n\t\tvar ttype thrift.TType\n\t\tttype, size, l, err = Binary.ReadListBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read list begin error: %w\", err)\n\t\t}\n\t\tf.ValType = int(ttype)\n\t\tlist := make([]UnknownField, size)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&list[i], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read list elem error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadListEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read list end error: %w\", err)\n\t\t}\n\t\tf.Value = list\n\tcase thrift.MAP:\n\t\tvar kttype, vttype thrift.TType\n\t\tkttype, vttype, size, l, err = Binary.ReadMapBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read map begin error: %w\", err)\n\t\t}\n\t\tf.KeyType = int(kttype)\n\t\tf.ValType = int(vttype)\n\t\tflatMap := make([]UnknownField, size*2)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&flatMap[2*i], buf[length:], \"\", thrift.TType(f.KeyType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read map key error: %w\", err2)\n\t\t\t}\n\t\t\tl, err2 = readUnknownField(&flatMap[2*i+1], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read map value error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadMapEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read map end error: %w\", err)\n\t\t}\n\t\tf.Value = flatMap\n\tcase thrift.STRUCT:\n\t\t_, l, err = Binary.ReadStructBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read struct begin error: %w\", err)\n\t\t}\n\t\tvar field UnknownField\n\t\tvar fields []UnknownField\n\t\tfor {\n\t\t\tname, fieldTypeID, fieldID, l, err := Binary.ReadFieldBegin(buf[length:])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read field begin error: %w\", err)\n\t\t\t}\n\t\t\tif fieldTypeID == thrift.STOP {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tl, err = readUnknownField(&field, buf[length:], name, fieldTypeID, fieldID)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read struct field error: %w\", err)\n\t\t\t}\n\t\t\tl, err = Binary.ReadFieldEnd(buf[length:])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read field end error: %w\", err)\n\t\t\t}\n\t\t\tfields = append(fields, field)\n\t\t}\n\t\tl, err = Binary.ReadStructEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read struct end error: %w\", err)\n\t\t}\n\t\tf.Value = fields\n\tdefault:\n\t\treturn length, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\tif err != nil {\n\t\treturn length, err\n\t}\n\treturn\n}\n\n// UnknownFieldsLength returns the length of fs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc unknownFieldLength(f *UnknownField) (length int, err error) {\n\t// use constants to avoid some type assert\n\tswitch f.Type {\n\tcase unknown.TBool:\n\t\tlength += Binary.BoolLength(false)\n\tcase unknown.TByte:\n\t\tlength += Binary.ByteLength(0)\n\tcase unknown.TDouble:\n\t\tlength += Binary.DoubleLength(0)\n\tcase unknown.TI16:\n\t\tlength += Binary.I16Length(0)\n\tcase unknown.TI32:\n\t\tlength += Binary.I32Length(0)\n\tcase unknown.TI64:\n\t\tlength += Binary.I64Length(0)\n\tcase unknown.TString:\n\t\tlength += Binary.StringLength(f.Value.(string))\n\tcase unknown.TSet:\n\t\tvs := f.Value.([]UnknownField)\n\t\tlength += Binary.SetBeginLength(thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := unknownFieldLength(&v)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.SetEndLength()\n\tcase unknown.TList:\n\t\tvs := f.Value.([]UnknownField)\n\t\tlength += Binary.ListBeginLength(thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := unknownFieldLength(&v)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.ListEndLength()\n\tcase unknown.TMap:\n\t\tkvs := f.Value.([]UnknownField)\n\t\tlength += Binary.MapBeginLength(thrift.TType(f.KeyType), thrift.TType(f.ValType), len(kvs)/2)\n\t\tfor i := 0; i < len(kvs); i += 2 {\n\t\t\tl, err := unknownFieldLength(&kvs[i])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t\tl, err = unknownFieldLength(&kvs[i+1])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.MapEndLength()\n\tcase unknown.TStruct:\n\t\tfs := f.Value.([]UnknownField)\n\t\tlength += Binary.StructBeginLength(f.Name)\n\t\tl, err := UnknownFieldsLength(fs)\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, err\n\t\t}\n\t\tlength += Binary.FieldStopLength()\n\t\tlength += Binary.StructEndLength()\n\tdefault:\n\t\treturn length, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\treturn\n}\n\n// WriteUnknownFields writes fs into buf, and return written offset of the buf.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc writeUnknownField(buf []byte, f *UnknownField) (offset int, err error) {\n\tswitch f.Type {\n\tcase unknown.TBool:\n\t\toffset += Binary.WriteBool(buf, f.Value.(bool))\n\tcase unknown.TByte:\n\t\toffset += Binary.WriteByte(buf, f.Value.(int8))\n\tcase unknown.TDouble:\n\t\toffset += Binary.WriteDouble(buf, f.Value.(float64))\n\tcase unknown.TI16:\n\t\toffset += Binary.WriteI16(buf, f.Value.(int16))\n\tcase unknown.TI32:\n\t\toffset += Binary.WriteI32(buf, f.Value.(int32))\n\tcase unknown.TI64:\n\t\toffset += Binary.WriteI64(buf, f.Value.(int64))\n\tcase unknown.TString:\n\t\toffset += Binary.WriteString(buf, f.Value.(string))\n\tcase unknown.TSet:\n\t\tvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteSetBegin(buf, thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := writeUnknownField(buf[offset:], &v)\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteSetEnd(buf[offset:])\n\tcase unknown.TList:\n\t\tvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteListBegin(buf, thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := writeUnknownField(buf[offset:], &v)\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteListEnd(buf[offset:])\n\tcase unknown.TMap:\n\t\tkvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteMapBegin(buf, thrift.TType(f.KeyType), thrift.TType(f.ValType), len(kvs)/2)\n\t\tfor i := 0; i < len(kvs); i += 2 {\n\t\t\tl, err := writeUnknownField(buf[offset:], &kvs[i])\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t\tl, err = writeUnknownField(buf[offset:], &kvs[i+1])\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteMapEnd(buf[offset:])\n\tcase unknown.TStruct:\n\t\tfs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteStructBegin(buf, f.Name)\n\t\tl, err := WriteUnknownFields(buf[offset:], fs)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn offset, err\n\t\t}\n\t\toffset += Binary.WriteFieldStop(buf[offset:])\n\t\toffset += Binary.WriteStructEnd(buf[offset:])\n\tdefault:\n\t\treturn offset, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\treturn\n}\n\n```\n# Function Signature: func WriteUnknownFields(buf []byte, fs []UnknownField) (offset int, err error) {\n# Natural Language Annotations: // WriteUnknownFields writes fs into buf, and return written offset of the buf.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WriteUnknownFields(buf []byte, fs []UnknownField) (offset int, err error) {\n\tfor _, f := range fs {\n\t\tl, err := writeUnknownField(buf[offset:], &f)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn offset, err\n\t\t}\n\t}\n\treturn offset, nil\n}\n```", "  ```\nfunc WriteUnknownFields(buf []byte, fs []UnknownField) (offset int, err error) {\n\tfor _, f := range fs {\n\t\tl, err := writeUnknownField(buf[offset:], &f)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn offset, err\n\t\t}\n\t}\n\treturn\n}\n```", "  ```\nfunc WriteUnknownFields(buf []byte, fs []UnknownField) (offset int, err error) {\n\tfor _, f := range fs {\n\t\tvar wl int\n\t\twl, err = writeUnknownField(buf[offset:], &f)\n\t\toffset += wl\n\t\tif err != nil {\n\t\t\treturn offset, err\n\t\t}\n\t}\n\treturn\n}\n```"]}, "kitex-pkg/circuitbreak/circuitbreak.go-NewCircuitBreakerMW": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/circuitbreak/circuitbreak.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"errors\"\n\n\t\"github.com/bytedance/gopkg/cloud/circuitbreaker\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\n// Parameter contains parameters for circuit breaker.\ntype Parameter struct {\n\t// Enabled means if to enable the circuit breaker.\n\tEnabled bool\n\t// ErrorRate means the rate at which breaks.\n\tErrorRate float64\n\t// MinimalSample means the minimal sample need before break.\n\tMinimalSample int64\n}\n\n// ErrorType means the error type.\ntype ErrorType int\n\n// Constants for ErrorType.\nconst (\n\t// TypeIgnorable means ignorable error, which is ignored by the circuit breaker.\n\tTypeIgnorable ErrorType = iota\n\t// TypeTimeout means timeout error.\n\tTypeTimeout\n\t// TypeFailure means the request failed, but it isn't timeout.\n\tTypeFailure\n\t// TypeSuccess means the request successes.\n\tTypeSuccess\n)\n\n// WrapErrorWithType is used to define the ErrorType for CircuitBreaker.\n// If you don't want the error trigger fuse, you can set the ErrorType to TypeIgnorable,\n// the error won't be regarded as failed.\n// eg: return circuitbreak.WrapErrorWithType.WithCause(err, circuitbreak.TypeIgnorable) in customized middleware.\nfunc WrapErrorWithType(err error, errorType ErrorType) CircuitBreakerAwareError {\n\treturn &errorWrapperWithType{err: err, errType: errorType}\n}\n\ntype GetErrorTypeFunc func(ctx context.Context, request, response interface{}, err error) ErrorType\n\n// Control is the control strategy of the circuit breaker.\ntype Control struct {\n\t// Implement this to generate a key for the circuit breaker panel.\n\tGetKey func(ctx context.Context, request interface{}) (key string, enabled bool)\n\n\t// Implement this to determine the type of error.\n\tGetErrorType GetErrorTypeFunc\n\n\t// Implement this to provide more detailed information about the circuit breaker.\n\t// The err argument is always a kerrors.ErrCircuitBreak.\n\tDecorateError func(ctx context.Context, request interface{}, err error) error\n}\n\n// NewCircuitBreakerMW creates a circuit breaker MW using the given Control strategy and Panel.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// RecordStat to report request result to circuit breaker\nfunc RecordStat(ctx context.Context, request, response interface{}, err error, cbKey string, ctl *Control, panel circuitbreaker.Panel) {\n\tswitch ctl.GetErrorType(ctx, request, response, err) {\n\tcase TypeTimeout:\n\t\tpanel.Timeout(cbKey)\n\tcase TypeFailure:\n\t\tpanel.Fail(cbKey)\n\tcase TypeSuccess:\n\t\tpanel.Succeed(cbKey)\n\t}\n}\n\n// CircuitBreakerAwareError is used to wrap ErrorType\ntype CircuitBreakerAwareError interface {\n\terror\n\tTypeForCircuitBreaker() ErrorType\n}\n\ntype errorWrapperWithType struct {\n\terrType ErrorType\n\terr     error\n}\n\nfunc (e errorWrapperWithType) TypeForCircuitBreaker() ErrorType {\n\treturn e.errType\n}\n\nfunc (e errorWrapperWithType) Error() string {\n\treturn e.err.Error()\n}\n\nfunc (e errorWrapperWithType) Unwrap() error {\n\treturn e.err\n}\n\nfunc (e errorWrapperWithType) Is(target error) bool {\n\treturn errors.Is(e.err, target)\n}\n\n```\n# Function Signature: func NewCircuitBreakerMW(control Control, panel circuitbreaker.Panel) endpoint.Middleware {\n# Natural Language Annotations: // NewCircuitBreakerMW creates a circuit breaker MW using the given Control strategy and Panel.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewCircuitBreakerMW(control Control, panel circuitbreaker.Panel) endpoint.Middleware {\n    return func(next endpoint.Endpoint) endpoint.Endpoint {\n        return func(ctx context.Context, request interface{}) (response interface{}, err error) {\n            key, enabled := control.GetKey(ctx, request)\n            if !enabled {\n                return next(ctx, request)\n            }\n\n            switch err.(type) {\n            case *errorWrapperWithType:\n                errType := err.(CircuitBreakerAwareError).TypeForCircuitBreaker()\n                switch errType {\n                case TypeTimeout:\n                    panel.Timeout(key)\n                case TypeFailure:\n                    panel.Fail(key)\n                case TypeSuccess:\n                    panel.Succeed(key)\n                case TypeIgnorable:\n                    fallthrough\n                default:\n                    return next(ctx, request)\n                }\n\n            default:\n                return next(ctx, request)\n            }\n        }\n    }\n}\n```", "  ```\nfunc NewCircuitBreakerMW(control Control, panel circuitbreaker.Panel) endpoint.Middleware {\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request interface{}) (response interface{}, err error) {\n\t\t\t// Get circuit breaker key\n\t\t\tkey, enabled := control.GetKey(ctx, request)\n\t\t\tif !enabled {\n\t\t\t\treturn next(ctx, request)\n\t\t\t}\n\n\t\t\t// Check if circuit breaker is open\n\t\t\tif panel.IsOpen(key) {\n\t\t\t\terr = kerrors.ErrCircuitBreak\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Make the request\n\t\t\tresponse, err = next(ctx, request)\n\n\t\t\t// Report the result to the circuit breaker\n\t\t\tswitch control.GetErrorType(ctx, request, response, err) {\n\t\t\tcase TypeTimeout:\n\t\t\t\tpanel.Timeout(key)\n\t\t\tcase TypeFailure:\n\t\t\t\tpanel.Fail(key)\n\t\t\tcase TypeSuccess:\n\t\t\t\tpanel.Succeed(key)\n\t\t\t}\n\n\t\t\treturn\n\t\t}\n\t}\n}\n```\nThis function takes two arguments: `control` and `panel` and returns a new circuit breaker middleware function. The circuit breaker middleware function takes a `next` endpoint and returns a new endpoint. The new endpoint function checks if the circuit breaker is open and if so returns an error `kerrors.ErrCircuitBreak`, otherwise calls the `next` endpoint and reports the result to the circuit breaker panel.", "  ```\nfunc NewCircuitBreakerMW(control Control, panel circuitbreaker.Panel) endpoint.Middleware {\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\tkey, enabled := control.GetKey(ctx, request)\n\t\t\tif !enabled {\n\t\t\t\treturn next.Do(ctx, request, response)\n\t\t\t}\n\n\t\t\tcb, err := panel.GetOrCreate(key)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tif cb.ShouldOpen() {\n\t\t\t\treturn kerrors.NewCircuitBreakFailed(kerrors.ErrCircuitBreak, \"circuit is open\")\n\t\t\t}\n\n\t\t\tuserErr := next.Do(ctx, request, response)\n\t\t\tif userErr != nil {\n\t\t\t\tcb.ReportError(control.GetErrorType(ctx, request, response, userErr))\n\t\t\t}\n\n\t\t\treturn userErr\n\t\t}\n\t}\n}\n```\nThis is the function you asked to be written.\n\nIt generates a circuit breaker middleware using the given Control string and panel.\n\nThe Control string provided involves defining the key to be passed on to the panel and the method of Analyzing errors.\n\nIt returns an endpoint middleware function that checks the circuit breaker status and reports errors to the panel if there is one.\n\nIt also unwraps any errors and returns them in the user context."]}, "kitex-pkg/circuitbreak/cbsuite.go-ServiceCBMW": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/circuitbreak/cbsuite.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/bytedance/gopkg/cloud/circuitbreaker\"\n\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\nconst (\n\tserviceCBKey  = \"service\"\n\tinstanceCBKey = \"instance\"\n\tcbConfig      = \"cb_config\"\n)\n\nvar defaultCBConfig = CBConfig{Enable: true, ErrRate: 0.5, MinSample: 200}\n\n// GetDefaultCBConfig return defaultConfig of CircuitBreaker.\nfunc GetDefaultCBConfig() CBConfig {\n\treturn defaultCBConfig\n}\n\n// CBConfig is policy config of CircuitBreaker.\n// DON'T FORGET to update DeepCopy() and Equals() if you add new fields.\ntype CBConfig struct {\n\tEnable    bool    `json:\"enable\"`\n\tErrRate   float64 `json:\"err_rate\"`\n\tMinSample int64   `json:\"min_sample\"`\n}\n\n// DeepCopy returns a full copy of CBConfig.\nfunc (c *CBConfig) DeepCopy() *CBConfig {\n\tif c == nil {\n\t\treturn nil\n\t}\n\treturn &CBConfig{\n\t\tEnable:    c.Enable,\n\t\tErrRate:   c.ErrRate,\n\t\tMinSample: c.MinSample,\n\t}\n}\n\nfunc (c *CBConfig) Equals(other *CBConfig) bool {\n\tif c == nil && other == nil {\n\t\treturn true\n\t}\n\tif c == nil || other == nil {\n\t\treturn false\n\t}\n\treturn c.Enable == other.Enable && c.ErrRate == other.ErrRate && c.MinSample == other.MinSample\n}\n\n// GenServiceCBKeyFunc to generate circuit breaker key through rpcinfo.\n// You can customize the config key according to your config center.\ntype GenServiceCBKeyFunc func(ri rpcinfo.RPCInfo) string\n\ntype instanceCBConfig struct {\n\tCBConfig\n\tsync.RWMutex\n}\n\n// CBSuite is default wrapper of CircuitBreaker. If you don't have customized policy, you can specify CircuitBreaker\n// middlewares like this:\n//\n//\tcbs := NewCBSuite(GenServiceCBKeyFunc)\n//\topts = append(opts, client.WithCircuitBreaker(cbs))\ntype CBSuite struct {\n\tservicePanel    circuitbreaker.Panel\n\tserviceControl  *Control\n\tinstancePanel   circuitbreaker.Panel\n\tinstanceControl *Control\n\n\tgenServiceCBKey GenServiceCBKeyFunc\n\tserviceCBConfig sync.Map // map[serviceCBKey]CBConfig\n\n\tinstanceCBConfig instanceCBConfig\n\n\tevents event.Queue\n\n\tconfig CBSuiteConfig\n}\n\n// NewCBSuite to build a new CBSuite.\n// Notice: Should NewCBSuite for every client in this version,\n// because event.Queue and event.Bus are not shared with all clients now.\nfunc NewCBSuite(genKey GenServiceCBKeyFunc, options ...CBSuiteOption) *CBSuite {\n\ts := &CBSuite{\n\t\tgenServiceCBKey: genKey,\n\t\tinstanceCBConfig: instanceCBConfig{\n\t\t\tCBConfig: defaultCBConfig,\n\t\t},\n\t\tconfig: CBSuiteConfig{\n\t\t\tserviceGetErrorTypeFunc:  ErrorTypeOnServiceLevel,\n\t\t\tinstanceGetErrorTypeFunc: ErrorTypeOnInstanceLevel,\n\t\t},\n\t}\n\tfor _, option := range options {\n\t\toption(&s.config)\n\t}\n\treturn s\n}\n\n// ServiceCBMW return a new service level CircuitBreakerMW.\n\n\n\n\n\n\n\n\n// InstanceCBMW return a new instance level CircuitBreakerMW.\n\n\n\n\n\n\n\n\n// ServicePanel return cb Panel of service\nfunc (s *CBSuite) ServicePanel() circuitbreaker.Panel {\n\tif s.servicePanel == nil {\n\t\ts.initServiceCB()\n\t}\n\treturn s.servicePanel\n}\n\n// ServiceControl return cb Control of service\nfunc (s *CBSuite) ServiceControl() *Control {\n\tif s.serviceControl == nil {\n\t\ts.initServiceCB()\n\t}\n\treturn s.serviceControl\n}\n\n// UpdateServiceCBConfig is to update service CircuitBreaker config.\n// This func is suggested to be called in remote config module.\nfunc (s *CBSuite) UpdateServiceCBConfig(key string, cfg CBConfig) {\n\ts.serviceCBConfig.Store(key, cfg)\n}\n\n// UpdateInstanceCBConfig is to update instance CircuitBreaker param.\n// This func is suggested to be called in remote config module.\nfunc (s *CBSuite) UpdateInstanceCBConfig(cfg CBConfig) {\n\ts.instanceCBConfig.Lock()\n\ts.instanceCBConfig.CBConfig = cfg\n\ts.instanceCBConfig.Unlock()\n}\n\n// SetEventBusAndQueue is to make CircuitBreaker relate to event change.\nfunc (s *CBSuite) SetEventBusAndQueue(bus event.Bus, events event.Queue) {\n\ts.events = events\n\tif bus != nil {\n\t\tbus.Watch(discovery.ChangeEventName, s.discoveryChangeHandler)\n\t}\n}\n\n// Dump is to dump CircuitBreaker info for debug query.\n\n\n\n\n\n\n\n\n// Close circuitbreaker.Panel to release associated resources.\nfunc (s *CBSuite) Close() error {\n\tif s.servicePanel != nil {\n\t\ts.servicePanel.Close()\n\t\ts.servicePanel = nil\n\t\ts.serviceControl = nil\n\t}\n\tif s.instancePanel != nil {\n\t\ts.instancePanel.Close()\n\t\ts.instancePanel = nil\n\t\ts.instanceControl = nil\n\t}\n\treturn nil\n}\n\nfunc (s *CBSuite) initServiceCB() {\n\tif s.servicePanel != nil && s.serviceControl != nil {\n\t\treturn\n\t}\n\tif s.genServiceCBKey == nil {\n\t\ts.genServiceCBKey = RPCInfo2Key\n\t}\n\topts := circuitbreaker.Options{\n\t\tShouldTripWithKey: s.svcTripFunc,\n\t}\n\ts.servicePanel, _ = circuitbreaker.NewPanel(s.onServiceStateChange, opts)\n\n\tsvcKey := func(ctx context.Context, request interface{}) (serviceCBKey string, enabled bool) {\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tserviceCBKey = s.genServiceCBKey(ri)\n\t\tcbConfig, _ := s.serviceCBConfig.LoadOrStore(serviceCBKey, defaultCBConfig)\n\t\tenabled = cbConfig.(CBConfig).Enable\n\t\treturn\n\t}\n\ts.serviceControl = &Control{\n\t\tGetKey:       svcKey,\n\t\tGetErrorType: s.config.serviceGetErrorTypeFunc,\n\t\tDecorateError: func(ctx context.Context, request interface{}, err error) error {\n\t\t\treturn kerrors.ErrServiceCircuitBreak\n\t\t},\n\t}\n}\n\nfunc (s *CBSuite) initInstanceCB() {\n\tif s.instancePanel != nil && s.instanceControl != nil {\n\t\treturn\n\t}\n\topts := circuitbreaker.Options{\n\t\tShouldTripWithKey: s.insTripFunc,\n\t}\n\ts.instancePanel, _ = circuitbreaker.NewPanel(s.onInstanceStateChange, opts)\n\n\tinstanceKey := func(ctx context.Context, request interface{}) (instCBKey string, enabled bool) {\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tinstCBKey = ri.To().Address().String()\n\t\ts.instanceCBConfig.RLock()\n\t\tenabled = s.instanceCBConfig.Enable\n\t\ts.instanceCBConfig.RUnlock()\n\t\treturn\n\t}\n\ts.instanceControl = &Control{\n\t\tGetKey:       instanceKey,\n\t\tGetErrorType: s.config.instanceGetErrorTypeFunc,\n\t\tDecorateError: func(ctx context.Context, request interface{}, err error) error {\n\t\t\treturn kerrors.ErrInstanceCircuitBreak\n\t\t},\n\t}\n}\n\nfunc (s *CBSuite) onStateChange(level, key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\tif s.events == nil {\n\t\treturn\n\t}\n\tsuccesses, failures, timeouts := m.Counts()\n\tvar errRate float64\n\tif sum := successes + failures + timeouts; sum > 0 {\n\t\terrRate = float64(failures+timeouts) / float64(sum)\n\t}\n\ts.events.Push(&event.Event{\n\t\tName: level + \"_cb\",\n\t\tTime: time.Now(),\n\t\tDetail: fmt.Sprintf(\"%s: %s -> %s, (succ: %d, err: %d, timeout: %d, rate: %f)\",\n\t\t\tkey, oldState, newState, successes, failures, timeouts, errRate),\n\t})\n}\n\nfunc (s *CBSuite) onServiceStateChange(key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\ts.onStateChange(serviceCBKey, key, oldState, newState, m)\n}\n\nfunc (s *CBSuite) onInstanceStateChange(key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\ts.onStateChange(instanceCBKey, key, oldState, newState, m)\n}\n\nfunc (s *CBSuite) discoveryChangeHandler(e *event.Event) {\n\tif s.instancePanel == nil {\n\t\treturn\n\t}\n\textra := e.Extra.(*discovery.Change)\n\tfor i := range extra.Removed {\n\t\tinstCBKey := extra.Removed[i].Address().String()\n\t\ts.instancePanel.RemoveBreaker(instCBKey)\n\t}\n}\n\nfunc (s *CBSuite) svcTripFunc(key string) circuitbreaker.TripFunc {\n\tpi, _ := s.serviceCBConfig.LoadOrStore(key, defaultCBConfig)\n\tp := pi.(CBConfig)\n\treturn circuitbreaker.RateTripFunc(p.ErrRate, p.MinSample)\n}\n\nfunc (s *CBSuite) insTripFunc(key string) circuitbreaker.TripFunc {\n\ts.instanceCBConfig.RLock()\n\terrRate := s.instanceCBConfig.ErrRate\n\tminSample := s.instanceCBConfig.MinSample\n\ts.instanceCBConfig.RUnlock()\n\treturn circuitbreaker.RateTripFunc(errRate, minSample)\n}\n\nfunc cbDebugInfo(panel circuitbreaker.Panel) map[string]interface{} {\n\tdumper, ok := panel.(interface {\n\t\tDumpBreakers() map[string]circuitbreaker.Breaker\n\t})\n\tif !ok {\n\t\treturn nil\n\t}\n\tcbMap := make(map[string]interface{})\n\tfor key, breaker := range dumper.DumpBreakers() {\n\t\tcbState := breaker.State()\n\t\tif cbState == circuitbreaker.Closed {\n\t\t\tcontinue\n\t\t}\n\t\tcbMap[key] = map[string]interface{}{\n\t\t\t\"state\":             cbState,\n\t\t\t\"successes in 10s\":  breaker.Metricer().Successes(),\n\t\t\t\"failures in 10s\":   breaker.Metricer().Failures(),\n\t\t\t\"timeouts in 10s\":   breaker.Metricer().Timeouts(),\n\t\t\t\"error rate in 10s\": breaker.Metricer().ErrorRate(),\n\t\t}\n\t}\n\tif len(cbMap) == 0 {\n\t\tcbMap[\"msg\"] = \"all circuit breakers are in closed state\"\n\t}\n\treturn cbMap\n}\n\nfunc (s *CBSuite) configInfo() map[string]interface{} {\n\tsvcCBMap := make(map[string]interface{})\n\ts.serviceCBConfig.Range(func(key, value interface{}) bool {\n\t\tsvcCBMap[key.(string)] = value\n\t\treturn true\n\t})\n\ts.instanceCBConfig.RLock()\n\tinstCBConfig := s.instanceCBConfig.CBConfig\n\ts.instanceCBConfig.RUnlock()\n\n\tcbMap := make(map[string]interface{}, 2)\n\tcbMap[serviceCBKey] = svcCBMap\n\tcbMap[instanceCBKey] = instCBConfig\n\treturn cbMap\n}\n\n// RPCInfo2Key is to generate circuit breaker key through rpcinfo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (s *CBSuite) ServiceCBMW() endpoint.Middleware {\n# Natural Language Annotations: // ServiceCBMW return a new service level CircuitBreakerMW.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (s *CBSuite) ServiceCBMW() endpoint.Middleware {\n\treturn s.ServiceControl().MW()\n}\n```", "  ```\nfunc (s *CBSuite) ServiceCBMW() endpoint.Middleware {\n\treturn circuitbreaker.NewMiddleware(s.servicePanel, s.serviceControl)\n}\n```", "  ```\nfunc (s *CBSuite) ServiceCBMW() endpoint.Middleware {\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, req, resp interface{}) error {\n\t\t\tserviceCBKey, enabled := s.genServiceCBKey(ctx, req)\n\t\t\tif enabled {\n\t\t\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\t\t\ts.initServiceCB()\n\t\t\t\tcb := s.servicePanel.GetBreaker(serviceCBKey)\n\t\t\t\tswitch cb.Check(ri.Method) {\n\t\t\t\tcase circuitbreaker.Closed:\n\t\t\t\t\tif err := next(ctx, req, resp); err != nil {\n\t\t\t\t\t\treturn s.config.serviceGetErrorTypeFunc(ctx, err, req)\n\t\t\t\t\t}\n\t\t\t\t\tcb.Success()\n\t\t\t\t\treturn nil\n\t\t\t\tcase circuitbreaker.HalfOpen:\n\t\t\t\t\tif err := next(ctx, req, resp); err != nil {\n\t\t\t\t\t\treturn s.config.serviceGetErrorTypeFunc(ctx, err, req)\n\t\t\t\t\t}\n\t\t\t\t\tcb.Success()\n\t\t\t\t\tcb.Close()\n\t\t\t\t\treturn nil\n\t\t\t\tcase circuitbreaker.Open:\n\t\t\t\t\terr := s.config.serviceGetErrorTypeFunc(ctx, kerrors.ErrServiceCircuitBreak, req)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn next(ctx, req, resp)\n\t\t}\n\t}\n}\n```"]}, "kitex-pkg/circuitbreak/cbsuite.go-InstanceCBMW": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/circuitbreak/cbsuite.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/bytedance/gopkg/cloud/circuitbreaker\"\n\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\nconst (\n\tserviceCBKey  = \"service\"\n\tinstanceCBKey = \"instance\"\n\tcbConfig      = \"cb_config\"\n)\n\nvar defaultCBConfig = CBConfig{Enable: true, ErrRate: 0.5, MinSample: 200}\n\n// GetDefaultCBConfig return defaultConfig of CircuitBreaker.\nfunc GetDefaultCBConfig() CBConfig {\n\treturn defaultCBConfig\n}\n\n// CBConfig is policy config of CircuitBreaker.\n// DON'T FORGET to update DeepCopy() and Equals() if you add new fields.\ntype CBConfig struct {\n\tEnable    bool    `json:\"enable\"`\n\tErrRate   float64 `json:\"err_rate\"`\n\tMinSample int64   `json:\"min_sample\"`\n}\n\n// DeepCopy returns a full copy of CBConfig.\nfunc (c *CBConfig) DeepCopy() *CBConfig {\n\tif c == nil {\n\t\treturn nil\n\t}\n\treturn &CBConfig{\n\t\tEnable:    c.Enable,\n\t\tErrRate:   c.ErrRate,\n\t\tMinSample: c.MinSample,\n\t}\n}\n\nfunc (c *CBConfig) Equals(other *CBConfig) bool {\n\tif c == nil && other == nil {\n\t\treturn true\n\t}\n\tif c == nil || other == nil {\n\t\treturn false\n\t}\n\treturn c.Enable == other.Enable && c.ErrRate == other.ErrRate && c.MinSample == other.MinSample\n}\n\n// GenServiceCBKeyFunc to generate circuit breaker key through rpcinfo.\n// You can customize the config key according to your config center.\ntype GenServiceCBKeyFunc func(ri rpcinfo.RPCInfo) string\n\ntype instanceCBConfig struct {\n\tCBConfig\n\tsync.RWMutex\n}\n\n// CBSuite is default wrapper of CircuitBreaker. If you don't have customized policy, you can specify CircuitBreaker\n// middlewares like this:\n//\n//\tcbs := NewCBSuite(GenServiceCBKeyFunc)\n//\topts = append(opts, client.WithCircuitBreaker(cbs))\ntype CBSuite struct {\n\tservicePanel    circuitbreaker.Panel\n\tserviceControl  *Control\n\tinstancePanel   circuitbreaker.Panel\n\tinstanceControl *Control\n\n\tgenServiceCBKey GenServiceCBKeyFunc\n\tserviceCBConfig sync.Map // map[serviceCBKey]CBConfig\n\n\tinstanceCBConfig instanceCBConfig\n\n\tevents event.Queue\n\n\tconfig CBSuiteConfig\n}\n\n// NewCBSuite to build a new CBSuite.\n// Notice: Should NewCBSuite for every client in this version,\n// because event.Queue and event.Bus are not shared with all clients now.\nfunc NewCBSuite(genKey GenServiceCBKeyFunc, options ...CBSuiteOption) *CBSuite {\n\ts := &CBSuite{\n\t\tgenServiceCBKey: genKey,\n\t\tinstanceCBConfig: instanceCBConfig{\n\t\t\tCBConfig: defaultCBConfig,\n\t\t},\n\t\tconfig: CBSuiteConfig{\n\t\t\tserviceGetErrorTypeFunc:  ErrorTypeOnServiceLevel,\n\t\t\tinstanceGetErrorTypeFunc: ErrorTypeOnInstanceLevel,\n\t\t},\n\t}\n\tfor _, option := range options {\n\t\toption(&s.config)\n\t}\n\treturn s\n}\n\n// ServiceCBMW return a new service level CircuitBreakerMW.\n\n\n\n\n\n\n\n\n// InstanceCBMW return a new instance level CircuitBreakerMW.\n\n\n\n\n\n\n\n\n// ServicePanel return cb Panel of service\nfunc (s *CBSuite) ServicePanel() circuitbreaker.Panel {\n\tif s.servicePanel == nil {\n\t\ts.initServiceCB()\n\t}\n\treturn s.servicePanel\n}\n\n// ServiceControl return cb Control of service\nfunc (s *CBSuite) ServiceControl() *Control {\n\tif s.serviceControl == nil {\n\t\ts.initServiceCB()\n\t}\n\treturn s.serviceControl\n}\n\n// UpdateServiceCBConfig is to update service CircuitBreaker config.\n// This func is suggested to be called in remote config module.\nfunc (s *CBSuite) UpdateServiceCBConfig(key string, cfg CBConfig) {\n\ts.serviceCBConfig.Store(key, cfg)\n}\n\n// UpdateInstanceCBConfig is to update instance CircuitBreaker param.\n// This func is suggested to be called in remote config module.\nfunc (s *CBSuite) UpdateInstanceCBConfig(cfg CBConfig) {\n\ts.instanceCBConfig.Lock()\n\ts.instanceCBConfig.CBConfig = cfg\n\ts.instanceCBConfig.Unlock()\n}\n\n// SetEventBusAndQueue is to make CircuitBreaker relate to event change.\nfunc (s *CBSuite) SetEventBusAndQueue(bus event.Bus, events event.Queue) {\n\ts.events = events\n\tif bus != nil {\n\t\tbus.Watch(discovery.ChangeEventName, s.discoveryChangeHandler)\n\t}\n}\n\n// Dump is to dump CircuitBreaker info for debug query.\n\n\n\n\n\n\n\n\n// Close circuitbreaker.Panel to release associated resources.\nfunc (s *CBSuite) Close() error {\n\tif s.servicePanel != nil {\n\t\ts.servicePanel.Close()\n\t\ts.servicePanel = nil\n\t\ts.serviceControl = nil\n\t}\n\tif s.instancePanel != nil {\n\t\ts.instancePanel.Close()\n\t\ts.instancePanel = nil\n\t\ts.instanceControl = nil\n\t}\n\treturn nil\n}\n\nfunc (s *CBSuite) initServiceCB() {\n\tif s.servicePanel != nil && s.serviceControl != nil {\n\t\treturn\n\t}\n\tif s.genServiceCBKey == nil {\n\t\ts.genServiceCBKey = RPCInfo2Key\n\t}\n\topts := circuitbreaker.Options{\n\t\tShouldTripWithKey: s.svcTripFunc,\n\t}\n\ts.servicePanel, _ = circuitbreaker.NewPanel(s.onServiceStateChange, opts)\n\n\tsvcKey := func(ctx context.Context, request interface{}) (serviceCBKey string, enabled bool) {\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tserviceCBKey = s.genServiceCBKey(ri)\n\t\tcbConfig, _ := s.serviceCBConfig.LoadOrStore(serviceCBKey, defaultCBConfig)\n\t\tenabled = cbConfig.(CBConfig).Enable\n\t\treturn\n\t}\n\ts.serviceControl = &Control{\n\t\tGetKey:       svcKey,\n\t\tGetErrorType: s.config.serviceGetErrorTypeFunc,\n\t\tDecorateError: func(ctx context.Context, request interface{}, err error) error {\n\t\t\treturn kerrors.ErrServiceCircuitBreak\n\t\t},\n\t}\n}\n\nfunc (s *CBSuite) initInstanceCB() {\n\tif s.instancePanel != nil && s.instanceControl != nil {\n\t\treturn\n\t}\n\topts := circuitbreaker.Options{\n\t\tShouldTripWithKey: s.insTripFunc,\n\t}\n\ts.instancePanel, _ = circuitbreaker.NewPanel(s.onInstanceStateChange, opts)\n\n\tinstanceKey := func(ctx context.Context, request interface{}) (instCBKey string, enabled bool) {\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tinstCBKey = ri.To().Address().String()\n\t\ts.instanceCBConfig.RLock()\n\t\tenabled = s.instanceCBConfig.Enable\n\t\ts.instanceCBConfig.RUnlock()\n\t\treturn\n\t}\n\ts.instanceControl = &Control{\n\t\tGetKey:       instanceKey,\n\t\tGetErrorType: s.config.instanceGetErrorTypeFunc,\n\t\tDecorateError: func(ctx context.Context, request interface{}, err error) error {\n\t\t\treturn kerrors.ErrInstanceCircuitBreak\n\t\t},\n\t}\n}\n\nfunc (s *CBSuite) onStateChange(level, key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\tif s.events == nil {\n\t\treturn\n\t}\n\tsuccesses, failures, timeouts := m.Counts()\n\tvar errRate float64\n\tif sum := successes + failures + timeouts; sum > 0 {\n\t\terrRate = float64(failures+timeouts) / float64(sum)\n\t}\n\ts.events.Push(&event.Event{\n\t\tName: level + \"_cb\",\n\t\tTime: time.Now(),\n\t\tDetail: fmt.Sprintf(\"%s: %s -> %s, (succ: %d, err: %d, timeout: %d, rate: %f)\",\n\t\t\tkey, oldState, newState, successes, failures, timeouts, errRate),\n\t})\n}\n\nfunc (s *CBSuite) onServiceStateChange(key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\ts.onStateChange(serviceCBKey, key, oldState, newState, m)\n}\n\nfunc (s *CBSuite) onInstanceStateChange(key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\ts.onStateChange(instanceCBKey, key, oldState, newState, m)\n}\n\nfunc (s *CBSuite) discoveryChangeHandler(e *event.Event) {\n\tif s.instancePanel == nil {\n\t\treturn\n\t}\n\textra := e.Extra.(*discovery.Change)\n\tfor i := range extra.Removed {\n\t\tinstCBKey := extra.Removed[i].Address().String()\n\t\ts.instancePanel.RemoveBreaker(instCBKey)\n\t}\n}\n\nfunc (s *CBSuite) svcTripFunc(key string) circuitbreaker.TripFunc {\n\tpi, _ := s.serviceCBConfig.LoadOrStore(key, defaultCBConfig)\n\tp := pi.(CBConfig)\n\treturn circuitbreaker.RateTripFunc(p.ErrRate, p.MinSample)\n}\n\nfunc (s *CBSuite) insTripFunc(key string) circuitbreaker.TripFunc {\n\ts.instanceCBConfig.RLock()\n\terrRate := s.instanceCBConfig.ErrRate\n\tminSample := s.instanceCBConfig.MinSample\n\ts.instanceCBConfig.RUnlock()\n\treturn circuitbreaker.RateTripFunc(errRate, minSample)\n}\n\nfunc cbDebugInfo(panel circuitbreaker.Panel) map[string]interface{} {\n\tdumper, ok := panel.(interface {\n\t\tDumpBreakers() map[string]circuitbreaker.Breaker\n\t})\n\tif !ok {\n\t\treturn nil\n\t}\n\tcbMap := make(map[string]interface{})\n\tfor key, breaker := range dumper.DumpBreakers() {\n\t\tcbState := breaker.State()\n\t\tif cbState == circuitbreaker.Closed {\n\t\t\tcontinue\n\t\t}\n\t\tcbMap[key] = map[string]interface{}{\n\t\t\t\"state\":             cbState,\n\t\t\t\"successes in 10s\":  breaker.Metricer().Successes(),\n\t\t\t\"failures in 10s\":   breaker.Metricer().Failures(),\n\t\t\t\"timeouts in 10s\":   breaker.Metricer().Timeouts(),\n\t\t\t\"error rate in 10s\": breaker.Metricer().ErrorRate(),\n\t\t}\n\t}\n\tif len(cbMap) == 0 {\n\t\tcbMap[\"msg\"] = \"all circuit breakers are in closed state\"\n\t}\n\treturn cbMap\n}\n\nfunc (s *CBSuite) configInfo() map[string]interface{} {\n\tsvcCBMap := make(map[string]interface{})\n\ts.serviceCBConfig.Range(func(key, value interface{}) bool {\n\t\tsvcCBMap[key.(string)] = value\n\t\treturn true\n\t})\n\ts.instanceCBConfig.RLock()\n\tinstCBConfig := s.instanceCBConfig.CBConfig\n\ts.instanceCBConfig.RUnlock()\n\n\tcbMap := make(map[string]interface{}, 2)\n\tcbMap[serviceCBKey] = svcCBMap\n\tcbMap[instanceCBKey] = instCBConfig\n\treturn cbMap\n}\n\n// RPCInfo2Key is to generate circuit breaker key through rpcinfo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (s *CBSuite) InstanceCBMW() endpoint.Middleware {\n# Natural Language Annotations: // InstanceCBMW return a new instance level CircuitBreakerMW.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (s *CBSuite) InstanceCBMW() endpoint.Middleware {\n\treturn instanceCBMW(s.InstancePanel(), s.instanceControl)\n}\n```", "  ```\nfunc (s *CBSuite) InstanceCBMW() endpoint.Middleware {\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\tif s.instancePanel == nil {\n\t\t\t\ts.initInstanceCB()\n\t\t\t}\n\t\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\t\tinstCBKey := ri.To().Address().String()\n\t\t\tif err := s.instancePanel.Run(ctx, instCBKey, next, request, response); err != nil {\n\t\t\t\treturn kerrors.ErrInstanceCircuitBreak\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t}\n}\n```", "  ```\npackage circuitbreak\n\nimport \"context\"\n\n// InstanceCBMW is a middleware that adds circuit breaker functionality to each instance.\n// CBMW is a check point to detect if a instance is behaving normally, if not,\n// CBMW will be a proxy to catch the errors and avoid the panic.\nfunc (s *CBSuite) InstanceCBMW() endpoint.Middleware {\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, req, resp interface{}) error {\n\t\t\tcb, err := s.instanceControl.NewInterceptor(ctx, req)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\terr = next(ctx, req, resp)\n\t\t\tif err != nil {\n\t\t\t\tcb.Fail(err)\n\t\t\t} else {\n\t\t\t\tcb.Notify(nil)\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t}\n}\n```"]}, "kitex-pkg/circuitbreak/cbsuite.go-Dump": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/circuitbreak/cbsuite.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/bytedance/gopkg/cloud/circuitbreaker\"\n\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\nconst (\n\tserviceCBKey  = \"service\"\n\tinstanceCBKey = \"instance\"\n\tcbConfig      = \"cb_config\"\n)\n\nvar defaultCBConfig = CBConfig{Enable: true, ErrRate: 0.5, MinSample: 200}\n\n// GetDefaultCBConfig return defaultConfig of CircuitBreaker.\nfunc GetDefaultCBConfig() CBConfig {\n\treturn defaultCBConfig\n}\n\n// CBConfig is policy config of CircuitBreaker.\n// DON'T FORGET to update DeepCopy() and Equals() if you add new fields.\ntype CBConfig struct {\n\tEnable    bool    `json:\"enable\"`\n\tErrRate   float64 `json:\"err_rate\"`\n\tMinSample int64   `json:\"min_sample\"`\n}\n\n// DeepCopy returns a full copy of CBConfig.\nfunc (c *CBConfig) DeepCopy() *CBConfig {\n\tif c == nil {\n\t\treturn nil\n\t}\n\treturn &CBConfig{\n\t\tEnable:    c.Enable,\n\t\tErrRate:   c.ErrRate,\n\t\tMinSample: c.MinSample,\n\t}\n}\n\nfunc (c *CBConfig) Equals(other *CBConfig) bool {\n\tif c == nil && other == nil {\n\t\treturn true\n\t}\n\tif c == nil || other == nil {\n\t\treturn false\n\t}\n\treturn c.Enable == other.Enable && c.ErrRate == other.ErrRate && c.MinSample == other.MinSample\n}\n\n// GenServiceCBKeyFunc to generate circuit breaker key through rpcinfo.\n// You can customize the config key according to your config center.\ntype GenServiceCBKeyFunc func(ri rpcinfo.RPCInfo) string\n\ntype instanceCBConfig struct {\n\tCBConfig\n\tsync.RWMutex\n}\n\n// CBSuite is default wrapper of CircuitBreaker. If you don't have customized policy, you can specify CircuitBreaker\n// middlewares like this:\n//\n//\tcbs := NewCBSuite(GenServiceCBKeyFunc)\n//\topts = append(opts, client.WithCircuitBreaker(cbs))\ntype CBSuite struct {\n\tservicePanel    circuitbreaker.Panel\n\tserviceControl  *Control\n\tinstancePanel   circuitbreaker.Panel\n\tinstanceControl *Control\n\n\tgenServiceCBKey GenServiceCBKeyFunc\n\tserviceCBConfig sync.Map // map[serviceCBKey]CBConfig\n\n\tinstanceCBConfig instanceCBConfig\n\n\tevents event.Queue\n\n\tconfig CBSuiteConfig\n}\n\n// NewCBSuite to build a new CBSuite.\n// Notice: Should NewCBSuite for every client in this version,\n// because event.Queue and event.Bus are not shared with all clients now.\nfunc NewCBSuite(genKey GenServiceCBKeyFunc, options ...CBSuiteOption) *CBSuite {\n\ts := &CBSuite{\n\t\tgenServiceCBKey: genKey,\n\t\tinstanceCBConfig: instanceCBConfig{\n\t\t\tCBConfig: defaultCBConfig,\n\t\t},\n\t\tconfig: CBSuiteConfig{\n\t\t\tserviceGetErrorTypeFunc:  ErrorTypeOnServiceLevel,\n\t\t\tinstanceGetErrorTypeFunc: ErrorTypeOnInstanceLevel,\n\t\t},\n\t}\n\tfor _, option := range options {\n\t\toption(&s.config)\n\t}\n\treturn s\n}\n\n// ServiceCBMW return a new service level CircuitBreakerMW.\n\n\n\n\n\n\n\n\n// InstanceCBMW return a new instance level CircuitBreakerMW.\n\n\n\n\n\n\n\n\n// ServicePanel return cb Panel of service\nfunc (s *CBSuite) ServicePanel() circuitbreaker.Panel {\n\tif s.servicePanel == nil {\n\t\ts.initServiceCB()\n\t}\n\treturn s.servicePanel\n}\n\n// ServiceControl return cb Control of service\nfunc (s *CBSuite) ServiceControl() *Control {\n\tif s.serviceControl == nil {\n\t\ts.initServiceCB()\n\t}\n\treturn s.serviceControl\n}\n\n// UpdateServiceCBConfig is to update service CircuitBreaker config.\n// This func is suggested to be called in remote config module.\nfunc (s *CBSuite) UpdateServiceCBConfig(key string, cfg CBConfig) {\n\ts.serviceCBConfig.Store(key, cfg)\n}\n\n// UpdateInstanceCBConfig is to update instance CircuitBreaker param.\n// This func is suggested to be called in remote config module.\nfunc (s *CBSuite) UpdateInstanceCBConfig(cfg CBConfig) {\n\ts.instanceCBConfig.Lock()\n\ts.instanceCBConfig.CBConfig = cfg\n\ts.instanceCBConfig.Unlock()\n}\n\n// SetEventBusAndQueue is to make CircuitBreaker relate to event change.\nfunc (s *CBSuite) SetEventBusAndQueue(bus event.Bus, events event.Queue) {\n\ts.events = events\n\tif bus != nil {\n\t\tbus.Watch(discovery.ChangeEventName, s.discoveryChangeHandler)\n\t}\n}\n\n// Dump is to dump CircuitBreaker info for debug query.\n\n\n\n\n\n\n\n\n// Close circuitbreaker.Panel to release associated resources.\nfunc (s *CBSuite) Close() error {\n\tif s.servicePanel != nil {\n\t\ts.servicePanel.Close()\n\t\ts.servicePanel = nil\n\t\ts.serviceControl = nil\n\t}\n\tif s.instancePanel != nil {\n\t\ts.instancePanel.Close()\n\t\ts.instancePanel = nil\n\t\ts.instanceControl = nil\n\t}\n\treturn nil\n}\n\nfunc (s *CBSuite) initServiceCB() {\n\tif s.servicePanel != nil && s.serviceControl != nil {\n\t\treturn\n\t}\n\tif s.genServiceCBKey == nil {\n\t\ts.genServiceCBKey = RPCInfo2Key\n\t}\n\topts := circuitbreaker.Options{\n\t\tShouldTripWithKey: s.svcTripFunc,\n\t}\n\ts.servicePanel, _ = circuitbreaker.NewPanel(s.onServiceStateChange, opts)\n\n\tsvcKey := func(ctx context.Context, request interface{}) (serviceCBKey string, enabled bool) {\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tserviceCBKey = s.genServiceCBKey(ri)\n\t\tcbConfig, _ := s.serviceCBConfig.LoadOrStore(serviceCBKey, defaultCBConfig)\n\t\tenabled = cbConfig.(CBConfig).Enable\n\t\treturn\n\t}\n\ts.serviceControl = &Control{\n\t\tGetKey:       svcKey,\n\t\tGetErrorType: s.config.serviceGetErrorTypeFunc,\n\t\tDecorateError: func(ctx context.Context, request interface{}, err error) error {\n\t\t\treturn kerrors.ErrServiceCircuitBreak\n\t\t},\n\t}\n}\n\nfunc (s *CBSuite) initInstanceCB() {\n\tif s.instancePanel != nil && s.instanceControl != nil {\n\t\treturn\n\t}\n\topts := circuitbreaker.Options{\n\t\tShouldTripWithKey: s.insTripFunc,\n\t}\n\ts.instancePanel, _ = circuitbreaker.NewPanel(s.onInstanceStateChange, opts)\n\n\tinstanceKey := func(ctx context.Context, request interface{}) (instCBKey string, enabled bool) {\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tinstCBKey = ri.To().Address().String()\n\t\ts.instanceCBConfig.RLock()\n\t\tenabled = s.instanceCBConfig.Enable\n\t\ts.instanceCBConfig.RUnlock()\n\t\treturn\n\t}\n\ts.instanceControl = &Control{\n\t\tGetKey:       instanceKey,\n\t\tGetErrorType: s.config.instanceGetErrorTypeFunc,\n\t\tDecorateError: func(ctx context.Context, request interface{}, err error) error {\n\t\t\treturn kerrors.ErrInstanceCircuitBreak\n\t\t},\n\t}\n}\n\nfunc (s *CBSuite) onStateChange(level, key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\tif s.events == nil {\n\t\treturn\n\t}\n\tsuccesses, failures, timeouts := m.Counts()\n\tvar errRate float64\n\tif sum := successes + failures + timeouts; sum > 0 {\n\t\terrRate = float64(failures+timeouts) / float64(sum)\n\t}\n\ts.events.Push(&event.Event{\n\t\tName: level + \"_cb\",\n\t\tTime: time.Now(),\n\t\tDetail: fmt.Sprintf(\"%s: %s -> %s, (succ: %d, err: %d, timeout: %d, rate: %f)\",\n\t\t\tkey, oldState, newState, successes, failures, timeouts, errRate),\n\t})\n}\n\nfunc (s *CBSuite) onServiceStateChange(key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\ts.onStateChange(serviceCBKey, key, oldState, newState, m)\n}\n\nfunc (s *CBSuite) onInstanceStateChange(key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\ts.onStateChange(instanceCBKey, key, oldState, newState, m)\n}\n\nfunc (s *CBSuite) discoveryChangeHandler(e *event.Event) {\n\tif s.instancePanel == nil {\n\t\treturn\n\t}\n\textra := e.Extra.(*discovery.Change)\n\tfor i := range extra.Removed {\n\t\tinstCBKey := extra.Removed[i].Address().String()\n\t\ts.instancePanel.RemoveBreaker(instCBKey)\n\t}\n}\n\nfunc (s *CBSuite) svcTripFunc(key string) circuitbreaker.TripFunc {\n\tpi, _ := s.serviceCBConfig.LoadOrStore(key, defaultCBConfig)\n\tp := pi.(CBConfig)\n\treturn circuitbreaker.RateTripFunc(p.ErrRate, p.MinSample)\n}\n\nfunc (s *CBSuite) insTripFunc(key string) circuitbreaker.TripFunc {\n\ts.instanceCBConfig.RLock()\n\terrRate := s.instanceCBConfig.ErrRate\n\tminSample := s.instanceCBConfig.MinSample\n\ts.instanceCBConfig.RUnlock()\n\treturn circuitbreaker.RateTripFunc(errRate, minSample)\n}\n\nfunc cbDebugInfo(panel circuitbreaker.Panel) map[string]interface{} {\n\tdumper, ok := panel.(interface {\n\t\tDumpBreakers() map[string]circuitbreaker.Breaker\n\t})\n\tif !ok {\n\t\treturn nil\n\t}\n\tcbMap := make(map[string]interface{})\n\tfor key, breaker := range dumper.DumpBreakers() {\n\t\tcbState := breaker.State()\n\t\tif cbState == circuitbreaker.Closed {\n\t\t\tcontinue\n\t\t}\n\t\tcbMap[key] = map[string]interface{}{\n\t\t\t\"state\":             cbState,\n\t\t\t\"successes in 10s\":  breaker.Metricer().Successes(),\n\t\t\t\"failures in 10s\":   breaker.Metricer().Failures(),\n\t\t\t\"timeouts in 10s\":   breaker.Metricer().Timeouts(),\n\t\t\t\"error rate in 10s\": breaker.Metricer().ErrorRate(),\n\t\t}\n\t}\n\tif len(cbMap) == 0 {\n\t\tcbMap[\"msg\"] = \"all circuit breakers are in closed state\"\n\t}\n\treturn cbMap\n}\n\nfunc (s *CBSuite) configInfo() map[string]interface{} {\n\tsvcCBMap := make(map[string]interface{})\n\ts.serviceCBConfig.Range(func(key, value interface{}) bool {\n\t\tsvcCBMap[key.(string)] = value\n\t\treturn true\n\t})\n\ts.instanceCBConfig.RLock()\n\tinstCBConfig := s.instanceCBConfig.CBConfig\n\ts.instanceCBConfig.RUnlock()\n\n\tcbMap := make(map[string]interface{}, 2)\n\tcbMap[serviceCBKey] = svcCBMap\n\tcbMap[instanceCBKey] = instCBConfig\n\treturn cbMap\n}\n\n// RPCInfo2Key is to generate circuit breaker key through rpcinfo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (s *CBSuite) Dump() interface{} {\n# Natural Language Annotations: // Dump is to dump CircuitBreaker info for debug query.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (s *CBSuite) Dump() map[string]interface{} {\n    return map[string]interface{}{\n        serviceCBKey: cbDebugInfo(s.servicePanel),\n        instanceCBKey: cbDebugInfo(s.instancePanel),\n        config: s.configInfo(),\n    }\n}\n```", "  ```\nfunc (s *CBSuite) Dump() interface{} {\n\tsvcInfo := cbDebugInfo(s.ServicePanel())\n\tinstInfo := cbDebugInfo(s.InstancePanel())\n\tinfo := make(map[string]interface{}, 3)\n\tif svcInfo != nil {\n\t\tinfo[serviceCBKey] = svcInfo\n\t}\n\tif instInfo != nil {\n\t\tinfo[instanceCBKey] = instInfo\n\t}\n\tinfo[\"config\"] = s.configInfo()\n\treturn info\n}\n```", "  ```\nfunc (s *CBSuite) Dump() interface{} {\n\treturn struct {\n\t\tbreakerPanel  map[string]circuitbreaker.Panel\n\t\tbreakerStats  map[string]circuitbreaker.BreakerStats\n\t\tserviceCBKeys map[string]*CBConfig\n\t\tservicePanel  circuitbreaker.Panel\n\t\tinstanceCBKey string\n\t\tinstancePanel circuitbreaker.Panel\n\t\tconfig        CBSuiteConfig\n\t}{\n\t\tbreakerPanel:  s.serviceControl.BreakerPanel,\n\t\tbreakerStats:  s.serviceControl.BreakerStats,\n\t\tserviceCBKeys: s.serviceCBConfig,\n\t\tservicePanel:  s.servicePanel,\n\t\tinstanceCBKey: s.instanceCBConfig.Lock,\n\t\tinstancePanel: s.instancePanel,\n\t\tconfig:        s.config,\n\t}\n}\n```"]}, "kitex-pkg/circuitbreak/cbsuite.go-RPCInfo2Key": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/circuitbreak/cbsuite.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/bytedance/gopkg/cloud/circuitbreaker\"\n\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\nconst (\n\tserviceCBKey  = \"service\"\n\tinstanceCBKey = \"instance\"\n\tcbConfig      = \"cb_config\"\n)\n\nvar defaultCBConfig = CBConfig{Enable: true, ErrRate: 0.5, MinSample: 200}\n\n// GetDefaultCBConfig return defaultConfig of CircuitBreaker.\nfunc GetDefaultCBConfig() CBConfig {\n\treturn defaultCBConfig\n}\n\n// CBConfig is policy config of CircuitBreaker.\n// DON'T FORGET to update DeepCopy() and Equals() if you add new fields.\ntype CBConfig struct {\n\tEnable    bool    `json:\"enable\"`\n\tErrRate   float64 `json:\"err_rate\"`\n\tMinSample int64   `json:\"min_sample\"`\n}\n\n// DeepCopy returns a full copy of CBConfig.\nfunc (c *CBConfig) DeepCopy() *CBConfig {\n\tif c == nil {\n\t\treturn nil\n\t}\n\treturn &CBConfig{\n\t\tEnable:    c.Enable,\n\t\tErrRate:   c.ErrRate,\n\t\tMinSample: c.MinSample,\n\t}\n}\n\nfunc (c *CBConfig) Equals(other *CBConfig) bool {\n\tif c == nil && other == nil {\n\t\treturn true\n\t}\n\tif c == nil || other == nil {\n\t\treturn false\n\t}\n\treturn c.Enable == other.Enable && c.ErrRate == other.ErrRate && c.MinSample == other.MinSample\n}\n\n// GenServiceCBKeyFunc to generate circuit breaker key through rpcinfo.\n// You can customize the config key according to your config center.\ntype GenServiceCBKeyFunc func(ri rpcinfo.RPCInfo) string\n\ntype instanceCBConfig struct {\n\tCBConfig\n\tsync.RWMutex\n}\n\n// CBSuite is default wrapper of CircuitBreaker. If you don't have customized policy, you can specify CircuitBreaker\n// middlewares like this:\n//\n//\tcbs := NewCBSuite(GenServiceCBKeyFunc)\n//\topts = append(opts, client.WithCircuitBreaker(cbs))\ntype CBSuite struct {\n\tservicePanel    circuitbreaker.Panel\n\tserviceControl  *Control\n\tinstancePanel   circuitbreaker.Panel\n\tinstanceControl *Control\n\n\tgenServiceCBKey GenServiceCBKeyFunc\n\tserviceCBConfig sync.Map // map[serviceCBKey]CBConfig\n\n\tinstanceCBConfig instanceCBConfig\n\n\tevents event.Queue\n\n\tconfig CBSuiteConfig\n}\n\n// NewCBSuite to build a new CBSuite.\n// Notice: Should NewCBSuite for every client in this version,\n// because event.Queue and event.Bus are not shared with all clients now.\nfunc NewCBSuite(genKey GenServiceCBKeyFunc, options ...CBSuiteOption) *CBSuite {\n\ts := &CBSuite{\n\t\tgenServiceCBKey: genKey,\n\t\tinstanceCBConfig: instanceCBConfig{\n\t\t\tCBConfig: defaultCBConfig,\n\t\t},\n\t\tconfig: CBSuiteConfig{\n\t\t\tserviceGetErrorTypeFunc:  ErrorTypeOnServiceLevel,\n\t\t\tinstanceGetErrorTypeFunc: ErrorTypeOnInstanceLevel,\n\t\t},\n\t}\n\tfor _, option := range options {\n\t\toption(&s.config)\n\t}\n\treturn s\n}\n\n// ServiceCBMW return a new service level CircuitBreakerMW.\n\n\n\n\n\n\n\n\n// InstanceCBMW return a new instance level CircuitBreakerMW.\n\n\n\n\n\n\n\n\n// ServicePanel return cb Panel of service\nfunc (s *CBSuite) ServicePanel() circuitbreaker.Panel {\n\tif s.servicePanel == nil {\n\t\ts.initServiceCB()\n\t}\n\treturn s.servicePanel\n}\n\n// ServiceControl return cb Control of service\nfunc (s *CBSuite) ServiceControl() *Control {\n\tif s.serviceControl == nil {\n\t\ts.initServiceCB()\n\t}\n\treturn s.serviceControl\n}\n\n// UpdateServiceCBConfig is to update service CircuitBreaker config.\n// This func is suggested to be called in remote config module.\nfunc (s *CBSuite) UpdateServiceCBConfig(key string, cfg CBConfig) {\n\ts.serviceCBConfig.Store(key, cfg)\n}\n\n// UpdateInstanceCBConfig is to update instance CircuitBreaker param.\n// This func is suggested to be called in remote config module.\nfunc (s *CBSuite) UpdateInstanceCBConfig(cfg CBConfig) {\n\ts.instanceCBConfig.Lock()\n\ts.instanceCBConfig.CBConfig = cfg\n\ts.instanceCBConfig.Unlock()\n}\n\n// SetEventBusAndQueue is to make CircuitBreaker relate to event change.\nfunc (s *CBSuite) SetEventBusAndQueue(bus event.Bus, events event.Queue) {\n\ts.events = events\n\tif bus != nil {\n\t\tbus.Watch(discovery.ChangeEventName, s.discoveryChangeHandler)\n\t}\n}\n\n// Dump is to dump CircuitBreaker info for debug query.\n\n\n\n\n\n\n\n\n// Close circuitbreaker.Panel to release associated resources.\nfunc (s *CBSuite) Close() error {\n\tif s.servicePanel != nil {\n\t\ts.servicePanel.Close()\n\t\ts.servicePanel = nil\n\t\ts.serviceControl = nil\n\t}\n\tif s.instancePanel != nil {\n\t\ts.instancePanel.Close()\n\t\ts.instancePanel = nil\n\t\ts.instanceControl = nil\n\t}\n\treturn nil\n}\n\nfunc (s *CBSuite) initServiceCB() {\n\tif s.servicePanel != nil && s.serviceControl != nil {\n\t\treturn\n\t}\n\tif s.genServiceCBKey == nil {\n\t\ts.genServiceCBKey = RPCInfo2Key\n\t}\n\topts := circuitbreaker.Options{\n\t\tShouldTripWithKey: s.svcTripFunc,\n\t}\n\ts.servicePanel, _ = circuitbreaker.NewPanel(s.onServiceStateChange, opts)\n\n\tsvcKey := func(ctx context.Context, request interface{}) (serviceCBKey string, enabled bool) {\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tserviceCBKey = s.genServiceCBKey(ri)\n\t\tcbConfig, _ := s.serviceCBConfig.LoadOrStore(serviceCBKey, defaultCBConfig)\n\t\tenabled = cbConfig.(CBConfig).Enable\n\t\treturn\n\t}\n\ts.serviceControl = &Control{\n\t\tGetKey:       svcKey,\n\t\tGetErrorType: s.config.serviceGetErrorTypeFunc,\n\t\tDecorateError: func(ctx context.Context, request interface{}, err error) error {\n\t\t\treturn kerrors.ErrServiceCircuitBreak\n\t\t},\n\t}\n}\n\nfunc (s *CBSuite) initInstanceCB() {\n\tif s.instancePanel != nil && s.instanceControl != nil {\n\t\treturn\n\t}\n\topts := circuitbreaker.Options{\n\t\tShouldTripWithKey: s.insTripFunc,\n\t}\n\ts.instancePanel, _ = circuitbreaker.NewPanel(s.onInstanceStateChange, opts)\n\n\tinstanceKey := func(ctx context.Context, request interface{}) (instCBKey string, enabled bool) {\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tinstCBKey = ri.To().Address().String()\n\t\ts.instanceCBConfig.RLock()\n\t\tenabled = s.instanceCBConfig.Enable\n\t\ts.instanceCBConfig.RUnlock()\n\t\treturn\n\t}\n\ts.instanceControl = &Control{\n\t\tGetKey:       instanceKey,\n\t\tGetErrorType: s.config.instanceGetErrorTypeFunc,\n\t\tDecorateError: func(ctx context.Context, request interface{}, err error) error {\n\t\t\treturn kerrors.ErrInstanceCircuitBreak\n\t\t},\n\t}\n}\n\nfunc (s *CBSuite) onStateChange(level, key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\tif s.events == nil {\n\t\treturn\n\t}\n\tsuccesses, failures, timeouts := m.Counts()\n\tvar errRate float64\n\tif sum := successes + failures + timeouts; sum > 0 {\n\t\terrRate = float64(failures+timeouts) / float64(sum)\n\t}\n\ts.events.Push(&event.Event{\n\t\tName: level + \"_cb\",\n\t\tTime: time.Now(),\n\t\tDetail: fmt.Sprintf(\"%s: %s -> %s, (succ: %d, err: %d, timeout: %d, rate: %f)\",\n\t\t\tkey, oldState, newState, successes, failures, timeouts, errRate),\n\t})\n}\n\nfunc (s *CBSuite) onServiceStateChange(key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\ts.onStateChange(serviceCBKey, key, oldState, newState, m)\n}\n\nfunc (s *CBSuite) onInstanceStateChange(key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\ts.onStateChange(instanceCBKey, key, oldState, newState, m)\n}\n\nfunc (s *CBSuite) discoveryChangeHandler(e *event.Event) {\n\tif s.instancePanel == nil {\n\t\treturn\n\t}\n\textra := e.Extra.(*discovery.Change)\n\tfor i := range extra.Removed {\n\t\tinstCBKey := extra.Removed[i].Address().String()\n\t\ts.instancePanel.RemoveBreaker(instCBKey)\n\t}\n}\n\nfunc (s *CBSuite) svcTripFunc(key string) circuitbreaker.TripFunc {\n\tpi, _ := s.serviceCBConfig.LoadOrStore(key, defaultCBConfig)\n\tp := pi.(CBConfig)\n\treturn circuitbreaker.RateTripFunc(p.ErrRate, p.MinSample)\n}\n\nfunc (s *CBSuite) insTripFunc(key string) circuitbreaker.TripFunc {\n\ts.instanceCBConfig.RLock()\n\terrRate := s.instanceCBConfig.ErrRate\n\tminSample := s.instanceCBConfig.MinSample\n\ts.instanceCBConfig.RUnlock()\n\treturn circuitbreaker.RateTripFunc(errRate, minSample)\n}\n\nfunc cbDebugInfo(panel circuitbreaker.Panel) map[string]interface{} {\n\tdumper, ok := panel.(interface {\n\t\tDumpBreakers() map[string]circuitbreaker.Breaker\n\t})\n\tif !ok {\n\t\treturn nil\n\t}\n\tcbMap := make(map[string]interface{})\n\tfor key, breaker := range dumper.DumpBreakers() {\n\t\tcbState := breaker.State()\n\t\tif cbState == circuitbreaker.Closed {\n\t\t\tcontinue\n\t\t}\n\t\tcbMap[key] = map[string]interface{}{\n\t\t\t\"state\":             cbState,\n\t\t\t\"successes in 10s\":  breaker.Metricer().Successes(),\n\t\t\t\"failures in 10s\":   breaker.Metricer().Failures(),\n\t\t\t\"timeouts in 10s\":   breaker.Metricer().Timeouts(),\n\t\t\t\"error rate in 10s\": breaker.Metricer().ErrorRate(),\n\t\t}\n\t}\n\tif len(cbMap) == 0 {\n\t\tcbMap[\"msg\"] = \"all circuit breakers are in closed state\"\n\t}\n\treturn cbMap\n}\n\nfunc (s *CBSuite) configInfo() map[string]interface{} {\n\tsvcCBMap := make(map[string]interface{})\n\ts.serviceCBConfig.Range(func(key, value interface{}) bool {\n\t\tsvcCBMap[key.(string)] = value\n\t\treturn true\n\t})\n\ts.instanceCBConfig.RLock()\n\tinstCBConfig := s.instanceCBConfig.CBConfig\n\ts.instanceCBConfig.RUnlock()\n\n\tcbMap := make(map[string]interface{}, 2)\n\tcbMap[serviceCBKey] = svcCBMap\n\tcbMap[instanceCBKey] = instCBConfig\n\treturn cbMap\n}\n\n// RPCInfo2Key is to generate circuit breaker key through rpcinfo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func RPCInfo2Key(ri rpcinfo.RPCInfo) string {\n# Natural Language Annotations: // RPCInfo2Key is to generate circuit breaker key through rpcinfo\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc RPCInfo2Key(ri rpcinfo.RPCInfo) string {\n    return fmt.Sprintf(\"%s|%s|%s|%s\", ri.To().Address(), ri.ServiceName(), ri.MethodName(), ri.TransportProtocol())\n}\n```", "  ```\nfunc RPCInfo2Key(ri rpcinfo.RPCInfo) (key string) {\n\tkey = ri.ServiceName()\n\tkey += \"/\" + ri.MethodName()\n\tkey += \"/\" + ri.To().Address().String()\n\treturn\n}\n```", "  ```\nfunc RPCInfo2Key(ri rpcinfo.RPCInfo) string {\n    var b strings.Builder\n    fmt.Fprintf(&b, \"%s:\", ri.To().Service().Name())\n    fmt.Fprintf(&b, \"%s\", ri.To().Address().String())\n    fmt.Fprintf(&b, \"%s\", ri.To().Endpoint().Name())\n    return b.String()\n}\n```"]}, "kitex-pkg/circuitbreak/default.go-ErrorTypeOnServiceLevel": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/circuitbreak/default.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"errors\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\n// some types of error won't trigger circuit breaker\nvar ignoreErrTypes = map[error]ErrorType{\n\tkerrors.ErrInternalException: TypeIgnorable,\n\tkerrors.ErrServiceDiscovery:  TypeIgnorable,\n\tkerrors.ErrACL:               TypeIgnorable,\n\tkerrors.ErrLoadbalance:       TypeIgnorable,\n\tkerrors.ErrRPCFinish:         TypeIgnorable,\n}\n\n// ErrorTypeOnServiceLevel determines the error type with a service level criteria.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ErrorTypeOnInstanceLevel determines the error type with an instance level criteria.\n// Basically, it treats only the connection error as failure.\n\n\n\n\n\n\n\n// FailIfError return TypeFailure if err is not nil, otherwise TypeSuccess.\nfunc FailIfError(ctx context.Context, request, response interface{}, err error) ErrorType {\n\tif err != nil {\n\t\treturn TypeFailure\n\t}\n\treturn TypeSuccess\n}\n\n// NoDecoration returns the original err.\nfunc NoDecoration(ctx context.Context, request interface{}, err error) error {\n\treturn err\n}\n\n```\n# Function Signature: func ErrorTypeOnServiceLevel(ctx context.Context, request, response interface{}, err error) ErrorType {\n# Natural Language Annotations: // ErrorTypeOnServiceLevel determines the error type with a service level criteria.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc ErrorTypeOnServiceLevel(ctx context.Context, request, response interface{}, err error) ErrorType {\n\tif err != nil {\n\t\tswitch err.(type) {\n\t\tcase *kerrors.Error:\n\t\t\tif ignoreErrTypes[err] != TypeIgnorable {\n\t\t\t\treturn ignoreErrTypes[err]\n\t\t\t}\n\t\t\treturn TypeFailure\n\t\tdefault:\n\t\t\treturn TypeUnknown\n\t\t}\n\t}\n\treturn TypeSuccess\n}\n```", "  ```\nErrorTypeOnServiceLevel determines the error type with a service level criteria.\nIt returns TypeIgnorable if the err is in ignoreErrTypes map, otherwise it returns FailIfError.\n\nfunc ErrorTypeOnServiceLevel(ctx context.Context, request, response interface{}, err error) ErrorType {\n\tif err != nil {\n\t\tif ignoreErrTypes[err] == TypeIgnorable {\n\t\t\treturn TypeIgnorable\n\t\t}\n\t\treturn FailIfError(ctx, request, response, err)\n\t}\n\treturn TypeSuccess\n}\n```", "  ```\nfunc ErrorTypeOnServiceLevel(ctx context.Context, request, response interface{}, err error) ErrorType {\n\t// error type on service level criteria:\n\t// * all errors that are not kerrors.ErrTimeout or context errors will be treated as failure\n\t// * kerrors.ErrTimeout and context errors will be treated as success\n\t// * if there is no error, return TypeSuccess\n\n\tif err == kerrors.ErrTimeout || err == context.DeadlineExceeded {\n\t\treturn TypeSuccess\n\t}\n\n\t// check if the error is a context error\n\tif _, ok := err.(context.Context); err != nil && ok {\n\t\treturn TypeSuccess\n\t}\n\n\t// any other errors are treated as failure\n\treturn TypeFailure\n}\n```"]}, "kitex-pkg/circuitbreak/default.go-ErrorTypeOnInstanceLevel": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/circuitbreak/default.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"errors\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\n// some types of error won't trigger circuit breaker\nvar ignoreErrTypes = map[error]ErrorType{\n\tkerrors.ErrInternalException: TypeIgnorable,\n\tkerrors.ErrServiceDiscovery:  TypeIgnorable,\n\tkerrors.ErrACL:               TypeIgnorable,\n\tkerrors.ErrLoadbalance:       TypeIgnorable,\n\tkerrors.ErrRPCFinish:         TypeIgnorable,\n}\n\n// ErrorTypeOnServiceLevel determines the error type with a service level criteria.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ErrorTypeOnInstanceLevel determines the error type with an instance level criteria.\n// Basically, it treats only the connection error as failure.\n\n\n\n\n\n\n\n// FailIfError return TypeFailure if err is not nil, otherwise TypeSuccess.\nfunc FailIfError(ctx context.Context, request, response interface{}, err error) ErrorType {\n\tif err != nil {\n\t\treturn TypeFailure\n\t}\n\treturn TypeSuccess\n}\n\n// NoDecoration returns the original err.\nfunc NoDecoration(ctx context.Context, request interface{}, err error) error {\n\treturn err\n}\n\n```\n# Function Signature: func ErrorTypeOnInstanceLevel(ctx context.Context, request, response interface{}, err error) ErrorType {\n# Natural Language Annotations: // ErrorTypeOnInstanceLevel determines the error type with an instance level criteria.\n// Basically, it treats only the connection error as failure.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc ErrorTypeOnInstanceLevel(ctx context.Context, request, response interface{}, err error) ErrorType {\n\tif err == nil {\n\t\treturn TypeSuccess\n\t}\n\treturn TypeFailure\n}\n```", "  ```\nfunc ErrorTypeOnInstanceLevel(ctx context.Context, request, response interface{}, err error) ErrorType {\n\tif err != nil {\n\t\t// Check if the error is a connection error\n\t\tconnErr, ok := err.(*net.OpError)\n\t\tif ok && connErr.Op == \"dial\" && connErr.Net == \"tcp\" {\n\t\t\treturn TypeFailure\n\t\t}\n\t}\n\treturn TypeSuccess\n}\n```", "  ```\nfunc ErrorTypeOnInstanceLevel(ctx context.Context, request, response interface{}, err error) ErrorType {\n\tif err != nil {\n\t\t// connection error\n\t\tif kerrors.IsConnectionError(err) {\n\t\t\treturn TypeFailure\n\t\t}\n\t\t// other errors should not break the circuit\n\t\treturn TypeSuccess\n\t}\n\treturn TypeSuccess\n}\n```"]}, "kitex-pkg/limiter/qps_limiter.go-UpdateLimit": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/limiter/qps_limiter.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage limiter\n\nimport (\n\t\"context\"\n\t\"sync/atomic\"\n\t\"time\"\n)\n\nvar fixedWindowTime = time.Second\n\n// qpsLimiter implements the RateLimiter interface.\ntype qpsLimiter struct {\n\tlimit      int32\n\ttokens     int32\n\tinterval   time.Duration\n\tonce       int32\n\tticker     *time.Ticker\n\ttickerDone chan bool\n}\n\n// NewQPSLimiter creates qpsLimiter.\nfunc NewQPSLimiter(interval time.Duration, limit int) RateLimiter {\n\tonce := calcOnce(interval, limit)\n\tl := &qpsLimiter{\n\t\tlimit:    int32(limit),\n\t\tinterval: interval,\n\t\ttokens:   once,\n\t\tonce:     once,\n\t}\n\tgo l.startTicker(interval)\n\treturn l\n}\n\n// UpdateLimit update limitation of QPS. It is **not** concurrent-safe.\n\n\n\n\n\n\n\n// UpdateQPSLimit update the interval and limit. It is **not** concurrent-safe.\n\n\n\n\n\n\n\n\n\n\n\n\n// Acquire one token.\nfunc (l *qpsLimiter) Acquire(ctx context.Context) bool {\n\tif atomic.LoadInt32(&l.limit) <= 0 {\n\t\treturn true\n\t}\n\tif atomic.LoadInt32(&l.tokens) <= 0 {\n\t\treturn false\n\t}\n\treturn atomic.AddInt32(&l.tokens, -1) >= 0\n}\n\n// Status returns the current status.\nfunc (l *qpsLimiter) Status(ctx context.Context) (max, cur int, interval time.Duration) {\n\tmax = int(atomic.LoadInt32(&l.limit))\n\tcur = int(atomic.LoadInt32(&l.tokens))\n\tinterval = l.interval\n\treturn\n}\n\nfunc (l *qpsLimiter) startTicker(interval time.Duration) {\n\tl.ticker = time.NewTicker(interval)\n\tdefer l.ticker.Stop()\n\tl.tickerDone = make(chan bool, 1)\n\ttc := l.ticker.C\n\ttd := l.tickerDone\n\t// ticker and tickerDone can be reset, cannot use l.ticker or l.tickerDone directly\n\tfor {\n\t\tselect {\n\t\tcase <-tc:\n\t\t\tl.updateToken()\n\t\tcase <-td:\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (l *qpsLimiter) stopTicker() {\n\tif l.tickerDone == nil {\n\t\treturn\n\t}\n\tselect {\n\tcase l.tickerDone <- true:\n\tdefault:\n\t}\n}\n\n// Some deviation is allowed here to gain better performance.\nfunc (l *qpsLimiter) updateToken() {\n\tif atomic.LoadInt32(&l.limit) < atomic.LoadInt32(&l.tokens) {\n\t\treturn\n\t}\n\n\tonce := atomic.LoadInt32(&l.once)\n\n\tdelta := atomic.LoadInt32(&l.limit) - atomic.LoadInt32(&l.tokens)\n\n\tif delta > once || delta < 0 {\n\t\tdelta = once\n\t}\n\n\tnewTokens := atomic.AddInt32(&l.tokens, delta)\n\tif newTokens < once {\n\t\tatomic.StoreInt32(&l.tokens, once)\n\t}\n}\n\nfunc calcOnce(interval time.Duration, limit int) int32 {\n\tif interval > time.Second {\n\t\tinterval = time.Second\n\t}\n\tonce := int32(float64(limit) / (fixedWindowTime.Seconds() / interval.Seconds()))\n\tif once < 0 {\n\t\tonce = 0\n\t}\n\treturn once\n}\n\nfunc (l *qpsLimiter) resetTokens(once int32) {\n\tif atomic.LoadInt32(&l.tokens) > once {\n\t\tatomic.StoreInt32(&l.tokens, once)\n\t}\n}\n\n```\n# Function Signature: func (l *qpsLimiter) UpdateLimit(limit int) {\n# Natural Language Annotations: // UpdateLimit update limitation of QPS. It is **not** concurrent-safe.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (l *qpsLimiter) UpdateLimit(limit int) {\n\tatomic.StoreInt32(&l.limit, int32(limit))\n\tl.resetTokens(calcOnce(l.interval, limit))\n}\n```", "  ```\nfunc (l *qpsLimiter) UpdateLimit(limit int) {\n\tl.stopTicker()\n\tl.limit = int32(limit)\n\tl.once = calcOnce(l.interval, limit)\n\tl.startTicker(l.interval)\n}\n```", "  ```\nfunc (l *qpsLimiter) UpdateLimit(limit int) {\n\tl.limit = int32(limit)\n\tonce := calcOnce(l.interval, limit)\n\tl.tokens = once\n\tl.once = once\n\tl.updateToken()\n}\n```"]}, "kitex-pkg/limiter/qps_limiter.go-UpdateQPSLimit": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/limiter/qps_limiter.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage limiter\n\nimport (\n\t\"context\"\n\t\"sync/atomic\"\n\t\"time\"\n)\n\nvar fixedWindowTime = time.Second\n\n// qpsLimiter implements the RateLimiter interface.\ntype qpsLimiter struct {\n\tlimit      int32\n\ttokens     int32\n\tinterval   time.Duration\n\tonce       int32\n\tticker     *time.Ticker\n\ttickerDone chan bool\n}\n\n// NewQPSLimiter creates qpsLimiter.\nfunc NewQPSLimiter(interval time.Duration, limit int) RateLimiter {\n\tonce := calcOnce(interval, limit)\n\tl := &qpsLimiter{\n\t\tlimit:    int32(limit),\n\t\tinterval: interval,\n\t\ttokens:   once,\n\t\tonce:     once,\n\t}\n\tgo l.startTicker(interval)\n\treturn l\n}\n\n// UpdateLimit update limitation of QPS. It is **not** concurrent-safe.\n\n\n\n\n\n\n\n// UpdateQPSLimit update the interval and limit. It is **not** concurrent-safe.\n\n\n\n\n\n\n\n\n\n\n\n\n// Acquire one token.\nfunc (l *qpsLimiter) Acquire(ctx context.Context) bool {\n\tif atomic.LoadInt32(&l.limit) <= 0 {\n\t\treturn true\n\t}\n\tif atomic.LoadInt32(&l.tokens) <= 0 {\n\t\treturn false\n\t}\n\treturn atomic.AddInt32(&l.tokens, -1) >= 0\n}\n\n// Status returns the current status.\nfunc (l *qpsLimiter) Status(ctx context.Context) (max, cur int, interval time.Duration) {\n\tmax = int(atomic.LoadInt32(&l.limit))\n\tcur = int(atomic.LoadInt32(&l.tokens))\n\tinterval = l.interval\n\treturn\n}\n\nfunc (l *qpsLimiter) startTicker(interval time.Duration) {\n\tl.ticker = time.NewTicker(interval)\n\tdefer l.ticker.Stop()\n\tl.tickerDone = make(chan bool, 1)\n\ttc := l.ticker.C\n\ttd := l.tickerDone\n\t// ticker and tickerDone can be reset, cannot use l.ticker or l.tickerDone directly\n\tfor {\n\t\tselect {\n\t\tcase <-tc:\n\t\t\tl.updateToken()\n\t\tcase <-td:\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (l *qpsLimiter) stopTicker() {\n\tif l.tickerDone == nil {\n\t\treturn\n\t}\n\tselect {\n\tcase l.tickerDone <- true:\n\tdefault:\n\t}\n}\n\n// Some deviation is allowed here to gain better performance.\nfunc (l *qpsLimiter) updateToken() {\n\tif atomic.LoadInt32(&l.limit) < atomic.LoadInt32(&l.tokens) {\n\t\treturn\n\t}\n\n\tonce := atomic.LoadInt32(&l.once)\n\n\tdelta := atomic.LoadInt32(&l.limit) - atomic.LoadInt32(&l.tokens)\n\n\tif delta > once || delta < 0 {\n\t\tdelta = once\n\t}\n\n\tnewTokens := atomic.AddInt32(&l.tokens, delta)\n\tif newTokens < once {\n\t\tatomic.StoreInt32(&l.tokens, once)\n\t}\n}\n\nfunc calcOnce(interval time.Duration, limit int) int32 {\n\tif interval > time.Second {\n\t\tinterval = time.Second\n\t}\n\tonce := int32(float64(limit) / (fixedWindowTime.Seconds() / interval.Seconds()))\n\tif once < 0 {\n\t\tonce = 0\n\t}\n\treturn once\n}\n\nfunc (l *qpsLimiter) resetTokens(once int32) {\n\tif atomic.LoadInt32(&l.tokens) > once {\n\t\tatomic.StoreInt32(&l.tokens, once)\n\t}\n}\n\n```\n# Function Signature: func (l *qpsLimiter) UpdateQPSLimit(interval time.Duration, limit int) {\n# Natural Language Annotations: // UpdateQPSLimit update the interval and limit. It is **not** concurrent-safe.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (l *qpsLimiter) UpdateQPSLimit(interval time.Duration, limit int) {\n\tl.stopTicker()\n\tl.interval = interval\n\tl.limit = limit\n\tl.once = calcOnce(interval, limit)\n\tl.resetTokens(l.once)\n\tgo l.startTicker(interval)\n}\n```", "  ```\nfunc (l *qpsLimiter) UpdateQPSLimit(interval time.Duration, limit int) {\n\tonce := calcOnce(interval, limit)\n\tl.once = once\n\tl.limit = limit\n\tl.interval = interval\n\tl.resetTokens(once)\n}\n```", "  ```\nfunc (l *qpsLimiter) UpdateQPSLimit(interval time.Duration, limit int) {\n\t// update interval\n\tl.interval = interval\n\n\t// update limit\n\tatomic.StoreInt32(&l.limit, int32(limit))\n\n\t// reset tokens\n\tonce := calcOnce(interval, limit)\n\tl.resetTokens(once)\n}\n```"]}, "kitex-pkg/utils/ring.go-NewRing": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/ring.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"errors\"\n\t\"runtime\"\n)\n\n// ErrRingFull means the ring is full.\nvar ErrRingFull = errors.New(\"ring is full\")\n\n// Deprecated: it's not used by kitex anymore.\n// NewRing creates a ringbuffer with fixed size.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Deprecated: it's not used by kitex anymore.\n// Ring implements a fixed size hash list to manage data\ntype Ring struct {\n\tlength int\n\trings  []*ring\n}\n\n// Push appends item to the ring.\nfunc (r *Ring) Push(obj interface{}) error {\n\tif r.length == 1 {\n\t\treturn r.rings[0].Push(obj)\n\t}\n\n\tidx := getGoroutineID() % r.length\n\tfor i := 0; i < r.length; i, idx = i+1, (idx+1)%r.length {\n\t\terr := r.rings[idx].Push(obj)\n\t\tif err == nil {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn ErrRingFull\n}\n\n// Pop returns the last item and removes it from the ring.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Dump dumps the data in the ring.\nfunc (r *Ring) Dump() interface{} {\n\tm := &ringDump{}\n\tdumpList := make([]*ringDump, 0, r.length)\n\tidx := getGoroutineID() % r.length\n\tfor i := 0; i < r.length; i, idx = i+1, (idx+1)%r.length {\n\t\tcurDump := &ringDump{}\n\t\tr.rings[idx].Dump(curDump)\n\t\tdumpList = append(dumpList, curDump)\n\t\tm.Cap += curDump.Cap\n\t\tm.Len += curDump.Len\n\t}\n\tm.Array = make([]interface{}, 0, m.Len)\n\tfor _, shardData := range dumpList {\n\t\tfor i := 0; i < shardData.Len; i++ {\n\t\t\tm.Array = append(m.Array, shardData.Array[i])\n\t\t}\n\t}\n\treturn m\n}\n\n```\n# Function Signature: func NewRing(size int) *Ring {\n# Natural Language Annotations: // Deprecated: it's not used by kitex anymore.\n// NewRing creates a ringbuffer with fixed size.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\n// NewRing creates a ring buffer with the given size.\nfunc NewRing(size int) *Ring {\n\treturn &Ring{\n\t\tlength: size,\n\t\trings:  make([]*ring, size),\n\t}\n}\n```", "  ```\nfunc NewRing(size int) *Ring {\n\tr := &Ring{\n\t\tlength: size,\n\t\trings:  make([]*ring, size, size),\n\t}\n\tfor i := 0; i < size; i++ {\n\t\tr.rings[i] = NewRingBuffer(size)\n\t}\n\treturn r\n}\n```", "  ```\nfunc NewRing(size int) *Ring {\n\tif size <= 0 {\n\t\tpanic(\"NewRing: size should not be zero or negative\")\n\t}\n\trings := make([]*ring, size)\n\tfor i := range rings {\n\t\trings[i] = &ring{\n\t\t\tcapacity: 1,\n\t\t\tdata:     make([]interface{}, 1),\n\t\t}\n\t}\n\treturn &Ring{\n\t\tlength: size,\n\t\trings:  rings,\n\t}\n}\n```"]}, "kitex-pkg/utils/ring.go-Pop": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/ring.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"errors\"\n\t\"runtime\"\n)\n\n// ErrRingFull means the ring is full.\nvar ErrRingFull = errors.New(\"ring is full\")\n\n// Deprecated: it's not used by kitex anymore.\n// NewRing creates a ringbuffer with fixed size.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Deprecated: it's not used by kitex anymore.\n// Ring implements a fixed size hash list to manage data\ntype Ring struct {\n\tlength int\n\trings  []*ring\n}\n\n// Push appends item to the ring.\nfunc (r *Ring) Push(obj interface{}) error {\n\tif r.length == 1 {\n\t\treturn r.rings[0].Push(obj)\n\t}\n\n\tidx := getGoroutineID() % r.length\n\tfor i := 0; i < r.length; i, idx = i+1, (idx+1)%r.length {\n\t\terr := r.rings[idx].Push(obj)\n\t\tif err == nil {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn ErrRingFull\n}\n\n// Pop returns the last item and removes it from the ring.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Dump dumps the data in the ring.\nfunc (r *Ring) Dump() interface{} {\n\tm := &ringDump{}\n\tdumpList := make([]*ringDump, 0, r.length)\n\tidx := getGoroutineID() % r.length\n\tfor i := 0; i < r.length; i, idx = i+1, (idx+1)%r.length {\n\t\tcurDump := &ringDump{}\n\t\tr.rings[idx].Dump(curDump)\n\t\tdumpList = append(dumpList, curDump)\n\t\tm.Cap += curDump.Cap\n\t\tm.Len += curDump.Len\n\t}\n\tm.Array = make([]interface{}, 0, m.Len)\n\tfor _, shardData := range dumpList {\n\t\tfor i := 0; i < shardData.Len; i++ {\n\t\t\tm.Array = append(m.Array, shardData.Array[i])\n\t\t}\n\t}\n\treturn m\n}\n\n```\n# Function Signature: func (r *Ring) Pop() interface{} {\n# Natural Language Annotations: // Pop returns the last item and removes it from the ring.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (r *Ring) Pop() interface{} {\n\tif r.length == 1 {\n\t\treturn r.rings[0].Pop()\n\t}\n\n\tidx := getGoroutineID() % r.length\n\tfor i := 0; i < r.length; i, idx = i+1, (idx+1)%r.length {\n\t\tobj := r.rings[idx].Pop()\n\t\tif obj != nil {\n\t\t\treturn obj\n\t\t}\n\t}\n\treturn nil\n}\n```", "  ```\nfunc (r *Ring) Pop() interface{} {\n    if r.length == 1 {\n        return r.rings[0].Pop()\n    }\n\n    idx := getGoroutineID() % r.length\n    for i := 0; i < r.length; i, idx = i+1, (idx+1)%r.length {\n        obj := r.rings[idx].Pop()\n        if obj != nil {\n            return obj\n        }\n    }\n    return nil\n}\n```", "  ```\nfunc (r *Ring) Pop() interface{} {\n\tif r.length == 1 {\n\t\treturn r.rings[0].Pop()\n\t}\n\n\tvar item interface{}\n\tidx := getGoroutineID() % r.length\n\tfor i := 0; i < r.length; i, idx = i+1, (idx+1)%r.length {\n\t\tp := r.rings[idx].Pop()\n\t\tif p == nil {\n\t\t\tcontinue\n\t\t}\n\t\titem = p\n\t\tbreak\n\t}\n\treturn item\n}\n```"]}, "kitex-pkg/utils/thrift.go-Encode": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/thrift.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n)\n\n// ThriftMessageCodec is used to codec thrift messages.\ntype ThriftMessageCodec struct {\n\ttb    *thrift.TMemoryBuffer\n\ttProt thrift.TProtocol\n}\n\n// NewThriftMessageCodec creates a new ThriftMessageCodec.\nfunc NewThriftMessageCodec() *ThriftMessageCodec {\n\ttransport := thrift.NewTMemoryBufferLen(1024)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\n\treturn &ThriftMessageCodec{\n\t\ttb:    transport,\n\t\ttProt: tProt,\n\t}\n}\n\n// Encode do thrift message encode.\n// Notice! msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n// Notice! seqID will be reset in kitex if the buffer is used for generic call in client side, set seqID=0 is suggested\n// when you call this method as client.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Decode do thrift message decode, notice: msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Serialize serialize message into bytes. This is normal thrift serialize func.\n// Notice: Binary generic use Encode instead of Serialize.\n\n\n\n\n\n\n\n\n\n\n// Deserialize deserialize bytes into message. This is normal thrift deserialize func.\n// Notice: Binary generic use Decode instead of Deserialize.\n\n\n\n\n\n\n\n\n\n\n\n// MarshalError convert go error to thrift exception, and encode exception over buffered binary transport.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalError decode binary and return error message\nfunc UnmarshalError(b []byte) error {\n\ttrans := thrift.NewStreamTransportR(bytes.NewReader(b))\n\tproto := thrift.NewTBinaryProtocolTransport(trans)\n\tif _, _, _, err := proto.ReadMessageBegin(); err != nil {\n\t\treturn fmt.Errorf(\"read message begin error: %w\", err)\n\t}\n\te := thrift.NewTApplicationException(0, \"\")\n\tif err := e.Read(proto); err != nil {\n\t\treturn fmt.Errorf(\"read exception error: %w\", err)\n\t}\n\tif err := proto.ReadMessageEnd(); err != nil {\n\t\treturn fmt.Errorf(\"read message end error: %w\", err)\n\t}\n\treturn e\n}\n\n```\n# Function Signature: func (t *ThriftMessageCodec) Encode(method string, msgType thrift.TMessageType, seqID int32, msg thrift.TStruct) (b []byte, err error) {\n# Natural Language Annotations: // Encode do thrift message encode.\n// Notice! msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n// Notice! seqID will be reset in kitex if the buffer is used for generic call in client side, set seqID=0 is suggested\n// when you call this method as client.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (t *ThriftMessageCodec) Encode(method string, msgType thrift.TMessageType, seqID int32, msg thrift.TStruct) (b []byte, err error) {\n\t// check message format\n\tif !msg.IsThriftStruct() {\n\t\terr = errors.New(\"message must be struct\")\n\t\treturn\n\t}\n\t// reset sequence number in kitex\n\tif seqID == 0 {\n\t\tt.tb.Reset()\n\t\tseqID = 1\n\t}\n\t// write message\n\tif err = t.tb.WriteStructBegin(method); err != nil {\n\t\treturn\n\t}\n\tif err = t.tb.WriteStructFieldBegin(msgType, seqID); err != nil {\n\t\treturn\n\t}\n\tif err = t.tb.WriteStructField(msg); err != nil {\n\t\treturn\n\t}\n\tif err = t.tb.WriteStructEnd(); err != nil {\n\t\treturn\n\t}\n\t// set buffer content\n\tb = t.tb.Bytes()\n\treturn\n}\n```", "  ```\nfunc (t *ThriftMessageCodec) Encode(method string, msgType thrift.TMessageType, seqID int32, msg thrift.TStruct) (b []byte, err error) {\n\tif !IsTMessageTypeValid(msgType) {\n\t\treturn nil, fmt.Errorf(\"invalid thrift message type: %s\", msgType)\n\t}\n\n\tif method == \"\" {\n\t\treturn nil, errors.New(\"method is empty\")\n\t}\n\n\tif msg == nil {\n\t\treturn nil, errors.New(\"msg is nil\")\n\t}\n\n\tif !IsSupportedMessage(msg) {\n\t\treturn nil, fmt.Errorf(\"unsupported thrift message: %T\", msg)\n\t}\n\n\t// Write the message header\n\theader := ThriftMessageHeader{\n\t\tName:      method,\n\t\tType:      msgType,\n\t\tSeqID:     seqID,\n\t\tProtocol:  thrift.TProtocolIdCompact,\n\t\tMessageID: 0,\n\t}\n\tif err := t.tb.WriteHeader(header); err != nil {\n\t\treturn nil, fmt.Errorf(\"write header error: %w\", err)\n\t}\n\n\t// Write the message body\n\tif err := t.tProt.WriteStruct(msg); err != nil {\n\t\treturn nil, fmt.Errorf(\"write struct error: %w\", err)\n\t}\n\n\t// Flush the buffer\n\tif err := t.tb.Flush(); err != nil {\n\t\treturn nil, fmt.Errorf(\"flush buffer error: %w\", err)\n\t}\n\n\t// Return the encoded message\n\treturn t.tb.Bytes(), nil\n}\n```", "  ```\nfunc (t *ThriftMessageCodec) Encode(method string, msgType thrift.TMessageType, seqID int32, msg thrift.TStruct) (b []byte, err error) {\n\tif method == \"\" {\n\t\treturn nil, errors.New(\"method is empty\")\n\t}\n\tif msgType == \"\" {\n\t\treturn nil, errors.New(\"msgType is empty\")\n\t}\n\tif msg == nil {\n\t\treturn nil, errors.New(\"message is empty\")\n\t}\n\t// Check if the message is arg or result, if it's result check if the error is nil\n\targMsg := msg.GetTArgs()\n\tif argMsg == nil {\n\t\treturn nil, errors.New(\"message must be arg or result\")\n\t}\n\tvar msgBytes []byte\n\terr = msg.Write(t.tProt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tmsgBytes, err = t.tb.Bytes(msg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// Make the header and add it to the message\n\th := NewThriftMessageHeader(msgType, seqID)\n\th.Lenght = int32(len(msgBytes))\n\tmsgBytes = append(h.Bytes(), msgBytes...)\n\t// Write the message bytes to buffer\n\tn, err := t.tb.Write(msgBytes)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// Return the buffer with the full message\n\tif n != len(msgBytes) {\n\t\treturn nil, errors.New(\"message buffer length miss match\")\n\t}\n\treturn t.tb.Bytes(), nil\n}\n```"]}, "kitex-pkg/utils/thrift.go-Decode": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/thrift.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n)\n\n// ThriftMessageCodec is used to codec thrift messages.\ntype ThriftMessageCodec struct {\n\ttb    *thrift.TMemoryBuffer\n\ttProt thrift.TProtocol\n}\n\n// NewThriftMessageCodec creates a new ThriftMessageCodec.\nfunc NewThriftMessageCodec() *ThriftMessageCodec {\n\ttransport := thrift.NewTMemoryBufferLen(1024)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\n\treturn &ThriftMessageCodec{\n\t\ttb:    transport,\n\t\ttProt: tProt,\n\t}\n}\n\n// Encode do thrift message encode.\n// Notice! msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n// Notice! seqID will be reset in kitex if the buffer is used for generic call in client side, set seqID=0 is suggested\n// when you call this method as client.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Decode do thrift message decode, notice: msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Serialize serialize message into bytes. This is normal thrift serialize func.\n// Notice: Binary generic use Encode instead of Serialize.\n\n\n\n\n\n\n\n\n\n\n// Deserialize deserialize bytes into message. This is normal thrift deserialize func.\n// Notice: Binary generic use Decode instead of Deserialize.\n\n\n\n\n\n\n\n\n\n\n\n// MarshalError convert go error to thrift exception, and encode exception over buffered binary transport.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalError decode binary and return error message\nfunc UnmarshalError(b []byte) error {\n\ttrans := thrift.NewStreamTransportR(bytes.NewReader(b))\n\tproto := thrift.NewTBinaryProtocolTransport(trans)\n\tif _, _, _, err := proto.ReadMessageBegin(); err != nil {\n\t\treturn fmt.Errorf(\"read message begin error: %w\", err)\n\t}\n\te := thrift.NewTApplicationException(0, \"\")\n\tif err := e.Read(proto); err != nil {\n\t\treturn fmt.Errorf(\"read exception error: %w\", err)\n\t}\n\tif err := proto.ReadMessageEnd(); err != nil {\n\t\treturn fmt.Errorf(\"read message end error: %w\", err)\n\t}\n\treturn e\n}\n\n```\n# Function Signature: func (t *ThriftMessageCodec) Decode(b []byte, msg thrift.TStruct) (method string, seqID int32, err error) {\n# Natural Language Annotations: // Decode do thrift message decode, notice: msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (t *ThriftMessageCodec) Decode(b []byte, msg thrift.TStruct) (method string, seqID int32, err error) {\n    trans := thrift.NewStreamTransportR(bytes.NewReader(b))\n    proto := thrift.NewTBinaryProtocolTransport(trans)\n\n    method, _, seqID, err = proto.ReadMessageBegin()\n    if err != nil {\n        return \"\", 0, fmt.Errorf(\"read message begin error: %w\", err)\n    }\n\n    if err = msg.Read(proto); err != nil {\n        return \"\", 0, fmt.Errorf(\"read message error: %w\", err)\n    }\n\n    if err = proto.ReadMessageEnd(); err != nil {\n        return \"\", 0, fmt.Errorf(\"read message end error: %w\", err)\n    }\n\n    return method, seqID, nil\n}\n```", "  ```\nfunc (t *ThriftMessageCodec) Decode(b []byte, msg thrift.TStruct) (method string, seqID int32, err error) {\n    // Read message begin\n    trans := thrift.NewStreamTransportR(bytes.NewReader(b))\n    proto := thrift.NewTBinaryProtocolTransport(trans)\n    _, method, seqID, err := proto.ReadMessageBegin()\n    if err != nil {\n        return \"\", 0, fmt.Errorf(\"read message begin error: %w\", err)\n    }\n\n    // Decode message into msg\n    err = msg.Read(proto)\n    if err != nil {\n        return \"\", 0, fmt.Errorf(\"read message error: %w\", err)\n    }\n\n    // Read message end\n    err = proto.ReadMessageEnd()\n    if err != nil {\n        return \"\", 0, fmt.Errorf(\"read message end error: %w\", err)\n    }\n\n    return method, seqID, nil\n}\n```", "  ```\nfunc (t *ThriftMessageCodec) Decode(b []byte, msg thrift.TStruct) (method string, seqID int32, err error) {\n\tt.tb.Write(b)\n\tif err := t.tb.Flush(); err != nil {\n\t\treturn \"\", 0, err\n\t}\n\tif t.tb.Len() == 0 {\n\t\treturn \"\", 0, errors.New(\"input data is empty\")\n\t}\n\tt.tb.Reset()\n\n\tseqID, method, err = ReadMethodBegin(t.tProt)\n\tif err != nil {\n\t\treturn \"\", 0, err\n\t}\n\n\tif err := msg.(thrift.TStruct).Read(t.tProt); err != nil {\n\t\treturn \"\", 0, err\n\t}\n\tif err := t.tProt.ReadMessageEnd(); err != nil {\n\t\treturn \"\", 0, err\n\t}\n\n\treturn method, seqID, nil\n}\n```"]}, "kitex-pkg/utils/thrift.go-Serialize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/thrift.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n)\n\n// ThriftMessageCodec is used to codec thrift messages.\ntype ThriftMessageCodec struct {\n\ttb    *thrift.TMemoryBuffer\n\ttProt thrift.TProtocol\n}\n\n// NewThriftMessageCodec creates a new ThriftMessageCodec.\nfunc NewThriftMessageCodec() *ThriftMessageCodec {\n\ttransport := thrift.NewTMemoryBufferLen(1024)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\n\treturn &ThriftMessageCodec{\n\t\ttb:    transport,\n\t\ttProt: tProt,\n\t}\n}\n\n// Encode do thrift message encode.\n// Notice! msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n// Notice! seqID will be reset in kitex if the buffer is used for generic call in client side, set seqID=0 is suggested\n// when you call this method as client.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Decode do thrift message decode, notice: msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Serialize serialize message into bytes. This is normal thrift serialize func.\n// Notice: Binary generic use Encode instead of Serialize.\n\n\n\n\n\n\n\n\n\n\n// Deserialize deserialize bytes into message. This is normal thrift deserialize func.\n// Notice: Binary generic use Decode instead of Deserialize.\n\n\n\n\n\n\n\n\n\n\n\n// MarshalError convert go error to thrift exception, and encode exception over buffered binary transport.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalError decode binary and return error message\nfunc UnmarshalError(b []byte) error {\n\ttrans := thrift.NewStreamTransportR(bytes.NewReader(b))\n\tproto := thrift.NewTBinaryProtocolTransport(trans)\n\tif _, _, _, err := proto.ReadMessageBegin(); err != nil {\n\t\treturn fmt.Errorf(\"read message begin error: %w\", err)\n\t}\n\te := thrift.NewTApplicationException(0, \"\")\n\tif err := e.Read(proto); err != nil {\n\t\treturn fmt.Errorf(\"read exception error: %w\", err)\n\t}\n\tif err := proto.ReadMessageEnd(); err != nil {\n\t\treturn fmt.Errorf(\"read message end error: %w\", err)\n\t}\n\treturn e\n}\n\n```\n# Function Signature: func (t *ThriftMessageCodec) Serialize(msg thrift.TStruct) (b []byte, err error) {\n# Natural Language Annotations: // Serialize serialize message into bytes. This is normal thrift serialize func.\n// Notice: Binary generic use Encode instead of Serialize.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (t *ThriftMessageCodec) Serialize(msg thrift.TStruct) (b []byte, err error) {\n\tt.tb.Reset()\n\tif err := msg.Write(t.tProt); err != nil {\n\t\treturn nil, fmt.Errorf(\"write message error: %w\", err)\n\t}\n\tif err := t.tb.Flush(); err != nil {\n\t\treturn nil, fmt.Errorf(\"flush transport error: %w\", err)\n\t}\n\treturn t.tb.Get(), nil\n}\n```", "  ```\nfunc (t *ThriftMessageCodec) Serialize(msg thrift.TStruct) (b []byte, err error) {\n\tif t.tb == nil {\n\t\treturn nil, errors.New(\"*TMemoryBuffer is nil\")\n\t}\n\tif t.tProt == nil {\n\t\treturn nil, errors.New(\"*TMemoryBuffer is nil\")\n\t}\n\tname, typeID, seqID, err := t.tProt.WriteMessageBegin(msg.Name(), thrift.CALL, seqID)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"write message begin error: %w\", err)\n\t}\n\tif err := msg.Write(t.tProt); err != nil {\n\t\treturn nil, fmt.Errorf(\"write struct error: %w\", err)\n\t}\n\tif err := t.tProt.WriteMessageEnd(); err != nil {\n\t\treturn nil, fmt.Errorf(\"write message end error: %w\", err)\n\t}\n\tif err := t.tProt.Flush(t.tb); err != nil {\n\t\treturn nil, fmt.Errorf(\"flush transport error: %w\", err)\n\t}\n\treturn t.tb.Bytes(), nil\n}\n```", "  ```\nfunc (t *ThriftMessageCodec) Serialize(msg thrift.TStruct) (b []byte, err error) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr = errors.New(\"Serialize operation error\")\n\t\t}\n\t}()\n\n\t// write thrift message into buffered transport\n\tif _, err = t.tb.Write(msg); err != nil {\n\t\treturn\n\t}\n\n\t// reset the buffered transport\n\tif err = t.tb.ResetBuffer(); err != nil {\n\t\treturn\n\t}\n\n\t// get the buffer reader from the buffered transport\n\treader := t.tb.BufferReader()\n\tdefer reader.Close()\n\n\t// read bytes from the buffer reader\n\tb, err = ioutil.ReadAll(reader)\n\tif err != nil {\n\t\treturn\n\t}\n\n\treturn b, nil\n}\n```"]}, "kitex-pkg/utils/thrift.go-Deserialize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/thrift.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n)\n\n// ThriftMessageCodec is used to codec thrift messages.\ntype ThriftMessageCodec struct {\n\ttb    *thrift.TMemoryBuffer\n\ttProt thrift.TProtocol\n}\n\n// NewThriftMessageCodec creates a new ThriftMessageCodec.\nfunc NewThriftMessageCodec() *ThriftMessageCodec {\n\ttransport := thrift.NewTMemoryBufferLen(1024)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\n\treturn &ThriftMessageCodec{\n\t\ttb:    transport,\n\t\ttProt: tProt,\n\t}\n}\n\n// Encode do thrift message encode.\n// Notice! msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n// Notice! seqID will be reset in kitex if the buffer is used for generic call in client side, set seqID=0 is suggested\n// when you call this method as client.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Decode do thrift message decode, notice: msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Serialize serialize message into bytes. This is normal thrift serialize func.\n// Notice: Binary generic use Encode instead of Serialize.\n\n\n\n\n\n\n\n\n\n\n// Deserialize deserialize bytes into message. This is normal thrift deserialize func.\n// Notice: Binary generic use Decode instead of Deserialize.\n\n\n\n\n\n\n\n\n\n\n\n// MarshalError convert go error to thrift exception, and encode exception over buffered binary transport.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalError decode binary and return error message\nfunc UnmarshalError(b []byte) error {\n\ttrans := thrift.NewStreamTransportR(bytes.NewReader(b))\n\tproto := thrift.NewTBinaryProtocolTransport(trans)\n\tif _, _, _, err := proto.ReadMessageBegin(); err != nil {\n\t\treturn fmt.Errorf(\"read message begin error: %w\", err)\n\t}\n\te := thrift.NewTApplicationException(0, \"\")\n\tif err := e.Read(proto); err != nil {\n\t\treturn fmt.Errorf(\"read exception error: %w\", err)\n\t}\n\tif err := proto.ReadMessageEnd(); err != nil {\n\t\treturn fmt.Errorf(\"read message end error: %w\", err)\n\t}\n\treturn e\n}\n\n```\n# Function Signature: func (t *ThriftMessageCodec) Deserialize(msg thrift.TStruct, b []byte) (err error) {\n# Natural Language Annotations: // Deserialize deserialize bytes into message. This is normal thrift deserialize func.\n// Notice: Binary generic use Decode instead of Deserialize.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (t *ThriftMessageCodec) Deserialize(msg thrift.TStruct, b []byte) (err error) {\n\ttrans := thrift.NewStreamTransportR(bytes.NewReader(b))\n\tproto := thrift.NewTBinaryProtocolTransport(trans)\n\tif _, err = proto.ReadMessageBegin(); err != nil {\n\t\treturn fmt.Errorf(\"read message begin error: %w\", err)\n\t}\n\tif err = msg.Read(proto); err != nil {\n\t\treturn fmt.Errorf(\"read message error: %w\", err)\n\t}\n\tif err = proto.ReadMessageEnd(); err != nil {\n\t\treturn fmt.Errorf(\"read message end error: %w\", err)\n\t}\n\treturn nil\n}\n```", "  ```\nfunc (t *ThriftMessageCodec) Deserialize(msg thrift.TStruct, b []byte) error {\n    trans := thrift.NewStreamTransportR(bytes.NewReader(b))\n    proto := thrift.NewTBinaryProtocolTransport(trans)\n    _, seqID, err := proto.ReadMessageBegin()\n    if err != nil {\n        return fmt.Errorf(\"read message begin error: %w\", err)\n    }\n    if seqID != 0 {\n        return fmt.Errorf(\"invalid sequence ID\")\n    }\n    err = msg.ReadFields(proto)\n    if err != nil {\n        return fmt.Errorf(\"read fields error: %w\", err)\n    }\n    return proto.ReadMessageEnd()\n}\n```", "  ```\nfunc (t *ThriftMessageCodec) Deserialize(msg thrift.TStruct, b []byte) (err error) {\n\ttransport := thrift.NewTMemoryBufferLen(len(b))\n\tif _, err = transport.Write(b); err != nil {\n\t\treturn fmt.Errorf(\"write buffer error: %w\", err)\n\t}\n\tif err = transport.Flush(false); err != nil {\n\t\treturn fmt.Errorf(\"flush buffer error: %w\", err)\n\t}\n\tt.tProt = thrift.NewTBinaryProtocolFactoryDefault()\n\tif err = msg.Unmarshal(t.tProt); err != nil {\n\t\treturn fmt.Errorf(\"unmarshal message error: %w\", err)\n\t}\n\tif err = transport.Close(); err != nil {\n\t\treturn fmt.Errorf(\"close transport error: %w\", err)\n\t}\n\treturn nil\n}\n```"]}, "kitex-pkg/utils/thrift.go-MarshalError": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/thrift.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n)\n\n// ThriftMessageCodec is used to codec thrift messages.\ntype ThriftMessageCodec struct {\n\ttb    *thrift.TMemoryBuffer\n\ttProt thrift.TProtocol\n}\n\n// NewThriftMessageCodec creates a new ThriftMessageCodec.\nfunc NewThriftMessageCodec() *ThriftMessageCodec {\n\ttransport := thrift.NewTMemoryBufferLen(1024)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\n\treturn &ThriftMessageCodec{\n\t\ttb:    transport,\n\t\ttProt: tProt,\n\t}\n}\n\n// Encode do thrift message encode.\n// Notice! msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n// Notice! seqID will be reset in kitex if the buffer is used for generic call in client side, set seqID=0 is suggested\n// when you call this method as client.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Decode do thrift message decode, notice: msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Serialize serialize message into bytes. This is normal thrift serialize func.\n// Notice: Binary generic use Encode instead of Serialize.\n\n\n\n\n\n\n\n\n\n\n// Deserialize deserialize bytes into message. This is normal thrift deserialize func.\n// Notice: Binary generic use Decode instead of Deserialize.\n\n\n\n\n\n\n\n\n\n\n\n// MarshalError convert go error to thrift exception, and encode exception over buffered binary transport.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalError decode binary and return error message\nfunc UnmarshalError(b []byte) error {\n\ttrans := thrift.NewStreamTransportR(bytes.NewReader(b))\n\tproto := thrift.NewTBinaryProtocolTransport(trans)\n\tif _, _, _, err := proto.ReadMessageBegin(); err != nil {\n\t\treturn fmt.Errorf(\"read message begin error: %w\", err)\n\t}\n\te := thrift.NewTApplicationException(0, \"\")\n\tif err := e.Read(proto); err != nil {\n\t\treturn fmt.Errorf(\"read exception error: %w\", err)\n\t}\n\tif err := proto.ReadMessageEnd(); err != nil {\n\t\treturn fmt.Errorf(\"read message end error: %w\", err)\n\t}\n\treturn e\n}\n\n```\n# Function Signature: func MarshalError(method string, err error) []byte {\n# Natural Language Annotations: // MarshalError convert go error to thrift exception, and encode exception over buffered binary transport.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc MarshalError(method string, err error) []byte {\n\ttrans := thrift.NewStreamTransportW(bytes.NewBuffer(make([]byte, 0, 128)))\n\tproto := thrift.NewTBinaryProtocolTransport(trans)\n\tif err := proto.WriteMessageBegin(method, thrift.EXCEPTION, 0); err != nil {\n\t\treturn nil\n\t}\n\te := thrift.NewTApplicationException(int32(thrift.INTERNAL_ERROR), err.Error())\n\tif err := e.Write(proto); err != nil {\n\t\treturn nil\n\t}\n\tif err := proto.WriteMessageEnd(); err != nil {\n\t\treturn nil\n\t}\n\tif err := trans.Close(); err != nil {\n\t\treturn nil\n\t}\n\treturn trans.Bytes()\n}\n```", "  ```\npackage utils\n\nimport (\n\t\"fmt\"\n\t\"github.com/apache/thrift/lib/go/thrift\"\n)\n\n// MarshalError convert go error to thrift exception, and encode exception over buffered binary transport.\nfunc MarshalError(method string, err error) []byte {\n\ttransport := thrift.NewTMemoryBufferLen(1024)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\te := thrift.NewTApplicationException(0, err.Error())\n\tif err := e.Write(tProt); err != nil {\n\t\treturn nil\n\t}\n\tif err := tProt.WriteMessageEnd(); err != nil {\n\t\treturn nil\n\t}\n\treturn transport.Bytes()\n}\n```", "  ```\nfunc MarshalError(method string, err error) []byte {\n\ttrans := thrift.NewStreamTransportW()\n\tproto := thrift.NewTBinaryProtocolTransport(trans)\n\n\te := thrift.NewTApplicationException(0, \"\")\n\tif err != nil {\n\t\terrMsg := err.Error()\n\t\tif errMsg != \"\" {\n\t\t\te.Message = errMsg\n\t\t}\n\t}\n\tif method != \"\" {\n\t\te.MethodName = method\n\t}\n\te.Write(proto)\n\tproto.Flush()\n\n\treturn trans.Bytes()\n}\n```"]}, "kitex-pkg/utils/strbuf.go-WriteInt64ToStringBuilder": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/strbuf.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n//lint:file-ignore SA6002 allocations cannot be avoided\n\npackage utils\n\nimport (\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n)\n\nvar intBytesPool = sync.Pool{\n\tNew: func() interface{} {\n\t\treturn make([]byte, 0, 20)\n\t},\n}\n\n// WriteInt64ToStringBuilder writes a int64 to a strings.Builder.\n\n\n\n\n\n\n\n```\n# Function Signature: func WriteInt64ToStringBuilder(sb *strings.Builder, value int64) {\n# Natural Language Annotations: // WriteInt64ToStringBuilder writes a int64 to a strings.Builder.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WriteInt64ToStringBuilder(sb *strings.Builder, value int64) {\n\tb := intBytesPool.Get().([]byte)\n\tdefer intBytesPool.Put(b)\n\tb = strconv.AppendInt(b[:0], value, 10)\n\tsb.Write(b)\n}\n```", "  ```\nfunc WriteInt64ToStringBuilder(sb *strings.Builder, value int64) {\n\tsb.Grow(20)\n\tbuf := intBytesPool.Get().([]byte)[:0]\n\tdefer func() {\n\t\tintBytesPool.Put(buf[:0])\n\t}()\n\tbuf = strconv.AppendInt(buf, value, 10)\n\tsb.Write(buf)\n}\n```", "  ```\nfunc WriteInt64ToStringBuilder(sb *strings.Builder, value int64) {\n    var buf []byte\n    if value == 0 {\n        buf = intBytesPool.Get().([]byte)[:0]\n    } else {\n        buf = strconv.AppendInt(buf, value, 10)\n    }\n    sb.Write(buf)\n    intBytesPool.Put(buf)\n}\n```\nThis code uses the `strconv.AppendInt` function to convert the `int64` value to a string and append it to the `strings.Builder`. The `intBytesPool` is used to pool the array used for the conversion to avoid allocating new memory for each call. The function returns `nil` if the `sb` is `nil` or if an error occurs during the string conversion."]}, "kitex-pkg/utils/rpcstats.go-CalculateEventCost": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/rpcstats.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n)\n\n// CalculateEventCost get events from rpcstats, and calculates the time duration of (end - start).\n// It returns 0 when get nil rpcinfo event of either stats event.\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func CalculateEventCost(rpcstats rpcinfo.RPCStats, start, end stats.Event) time.Duration {\n# Natural Language Annotations: // CalculateEventCost get events from rpcstats, and calculates the time duration of (end - start).\n// It returns 0 when get nil rpcinfo event of either stats event.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\npackage utils\n\nimport (\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n)\n\n// CalculateEventCost gets events from rpcstats, and calculates the time duration of (end - start).\n// It returns 0 when get nil rpcinfo event of either stats event.\nfunc CalculateEventCost(rpcstats rpcinfo.RPCStats, start, end stats.Event) time.Duration {\n\tif rpcstats == nil || start == nil || end == nil {\n\t\treturn 0\n\t}\n\treturn end.Time().Sub(start.Time())\n}\n```", "  ```\nfunc CalculateEventCost(rpcstats rpcinfo.RPCStats, start, end stats.Event) time.Duration {\n    if rpcstats == nil || start == nil || end == nil {\n        return 0\n    }\n\n    var startEvent, endEvent *stats.Event\n    for _, e := range rpcstats.Events {\n        if e.IsEnd() {\n            endEvent = e\n        } else if e.IsStart() {\n            startEvent = e\n        }\n    }\n    if startEvent == nil || endEvent == nil {\n        return 0\n    }\n\n    return endEvent.Time().Sub(startEvent.Time())\n}\n```", "  ```\nfunc CalculateEventCost(rpcstats rpcinfo.RPCStats, start, end stats.Event) time.Duration {\n\tvar startEvent, endEvent, statsStartEvent, statsEndEvent *stats.Event\n\n\tif startEvent := start.GetRPCInfoEvent(rpcstats); startEvent != nil {\n\t\tif statsStartEvent := startEvent.GetStart(); statsStartEvent != nil {\n\t\t\tstatsStartEvent.GetCachedValue()\n\t\t}\n\t\tstartEvent.GetEnd()\n\t} else if endEvent := end.GetRPCInfoEvent(rpcstats); endEvent != nil {\n\t\tif statsEndEvent := endEvent.GetEnd(); statsEndEvent != nil {\n\t\t\tstatsEndEvent.GetCachedValue()\n\t\t}\n\t\tendEvent.GetStart()\n\t} else {\n\t\treturn 0\n\t}\n\n\tvar startTS, endTS time.Time\n\n\tif startTS = rpcstats.GetStart(start); !startTS.IsZero() {\n\t\tstartTS = startTS.Add(startEvent.GetCachedValue())\n\t}\n\n\tif endTS = rpcstats.GetEnd(end); !endTS.IsZero() {\n\t\tendTS = endTS.Add(endEvent.GetCachedValue())\n\t}\n\n\treturn endTS.Sub(startTS)\n}\n```"]}, "kitex-pkg/utils/json.go-_Map2JSONStr": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/json.go:\n```\n/*\n * Copyright 2024 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * MIT License\n *\n * Copyright (c) 2016 json-iterator\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in all\n * copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n * The source code of this file is written based on json-iterator,\n * all modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage utils\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\t\"unicode/utf16\"\n\t\"unicode/utf8\"\n\t\"unsafe\"\n)\n\n// const of json keyword char\nconst (\n\tEmptyJSON  = \"{}\"\n\tComma      = ','\n\tColon      = ':'\n\tDQuotation = '\"'\n\tLeftBrace  = '{'\n\tRightBrace = '}'\n)\n\nconst (\n\tt1 = 0x00 // 0000 0000\n\ttx = 0x80 // 1000 0000\n\tt2 = 0xC0 // 1100 0000\n\tt3 = 0xE0 // 1110 0000\n\tt4 = 0xF0 // 1111 0000\n\tt5 = 0xF8 // 1111 1000\n\n\tmaskx = 0x3F // 0011 1111\n\n\trune1Max = 1<<7 - 1\n\trune2Max = 1<<11 - 1\n\trune3Max = 1<<16 - 1\n\n\tsurrogateMin = 0xD800\n\tsurrogateMax = 0xDFFF\n\n\tmaxRune   = '\\U0010FFFF' // Maximum valid Unicode code point.\n\truneError = '\\uFFFD'     // the \"error\" Rune or \"Unicode replacement character\"\n\n\thex = \"0123456789abcdef\"\n)\n\n// Map2JSONStr transform map[string]string to json str, perf is better than use json lib directly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// JSONStr2Map transform json str to map[string]string, perf is better than use json lib directly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc readString(buf []byte, idx, lastIdx int) (string, int, error) {\n\tvar err error\n\tvar c byte\n\tvar isNull bool\n\tif c, idx, err = nextToken(buf, idx, lastIdx); err != nil {\n\t\treturn \"\", idx, err\n\t}\n\tvar str []byte\n\tif c == '\"' {\n\t\tstart := idx\n\t\tnoESC := true\n\t\tfor idx <= lastIdx {\n\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn \"\", idx, err\n\t\t\t}\n\t\t\tswitch c {\n\t\t\tcase '\"':\n\t\t\t\tif start < idx-1 {\n\t\t\t\t\tif noESC {\n\t\t\t\t\t\tstr = buf[start : idx-1]\n\t\t\t\t\t} else {\n\t\t\t\t\t\tstr = append(str, buf[start:idx-1]...)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn *(*string)(unsafe.Pointer(&str)), idx, nil\n\t\t\tcase '\\\\':\n\t\t\t\tif start < idx-1 {\n\t\t\t\t\tif noESC {\n\t\t\t\t\t\tstr = buf[start : idx-1]\n\t\t\t\t\t} else {\n\t\t\t\t\t\tstr = append(str, buf[start:idx-1]...)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\t\treturn \"\", idx, err\n\t\t\t\t}\n\t\t\t\tif str, idx, err = readEscapedChar(c, buf, idx, str, lastIdx); err != nil {\n\t\t\t\t\treturn \"\", 0, err\n\t\t\t\t}\n\t\t\t\tstart = idx\n\t\t\t\tnoESC = false\n\t\t\t}\n\t\t}\n\t} else if idx, isNull = checkNull(c, buf, idx, lastIdx); isNull {\n\t\treturn \"\", idx, nil\n\t}\n\terr = fmt.Errorf(\"json str is invalid, expects '\\\"' or n, but found %s\", string(c))\n\treturn *(*string)(unsafe.Pointer(&str)), idx, err\n}\n\nfunc readByte(buf []byte, idx, lastIdx int) (byte, int, error) {\n\tif lastIdx < idx {\n\t\treturn 0, -1, fmt.Errorf(\"readByte no more data\")\n\t}\n\tc := buf[idx]\n\tidx++\n\treturn c, idx, nil\n}\n\nfunc nextToken(buf []byte, idx, lastIdx int) (byte, int, error) {\n\tif lastIdx < idx {\n\t\treturn 0, -1, errors.New(\"nextToken no more data\")\n\t}\n\tvar c byte\n\tfor idx <= lastIdx {\n\t\tc = buf[idx]\n\t\tidx++\n\t\tswitch c {\n\t\tcase ' ', '\\n', '\\t', '\\r':\n\t\t\tcontinue\n\t\t}\n\t\treturn c, idx, nil\n\t}\n\treturn c, idx, nil\n}\n\nfunc checkNull(c byte, data []byte, idx, lastIdx int) (int, bool) {\n\tif c == 'n' {\n\t\tch, idx, _ := readByte(data, idx, lastIdx)\n\t\tif ch != 'u' {\n\t\t\tidx--\n\t\t\treturn idx, false\n\t\t}\n\t\tch, idx, _ = readByte(data, idx, lastIdx)\n\t\tif ch != 'l' {\n\t\t\tidx--\n\t\t\treturn idx, false\n\t\t}\n\t\tch, idx, _ = readByte(data, idx, lastIdx)\n\t\tif ch != 'l' {\n\t\t\tidx--\n\t\t\treturn idx, false\n\t\t}\n\t\treturn idx, true\n\t}\n\treturn idx, false\n}\n\nfunc readU4(buf []byte, idx, lastIdx int) (rune, int, error) {\n\tvar err error\n\tvar ret rune\n\tfor i := 0; i < 4; i++ {\n\t\tvar c byte\n\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\treturn ret, idx, err\n\t\t}\n\t\tif c >= '0' && c <= '9' {\n\t\t\tret = ret*16 + rune(c-'0')\n\t\t} else if c >= 'a' && c <= 'f' {\n\t\t\tret = ret*16 + rune(c-'a'+10)\n\t\t} else if c >= 'A' && c <= 'F' {\n\t\t\tret = ret*16 + rune(c-'A'+10)\n\t\t} else {\n\t\t\treturn ret, idx, fmt.Errorf(\"unicode invalid: expects 0~9 or a~f, but found %v\", string([]byte{c}))\n\t\t}\n\t}\n\treturn ret, idx, nil\n}\n\n// refer to json-iterator/go/iter_str readEscapedChar\nfunc readEscapedChar(c byte, buf []byte, idx int, str []byte, lastIdx int) ([]byte, int, error) {\n\tvar err error\n\tswitch c {\n\tcase 'u':\n\t\tvar r rune\n\t\tif r, idx, err = readU4(buf, idx, lastIdx); err != nil {\n\t\t\treturn str, idx, err\n\t\t}\n\t\t// \u662f\u5426\u662f\u6269\u5c55\u5b57\u7b26\n\t\tif utf16.IsSurrogate(r) {\n\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn str, idx, err\n\t\t\t}\n\t\t\tif c != '\\\\' {\n\t\t\t\tidx--\n\t\t\t\tstr = appendRune(str, r)\n\t\t\t\treturn str, idx, nil\n\t\t\t}\n\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn str, idx, err\n\t\t\t}\n\t\t\tif c != 'u' {\n\t\t\t\tstr = appendRune(str, r)\n\t\t\t\treturn readEscapedChar(c, buf, idx, str, lastIdx)\n\t\t\t}\n\t\t\tvar r2 rune\n\t\t\tif r2, idx, err = readU4(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn str, idx, err\n\t\t\t}\n\t\t\tcombined := utf16.DecodeRune(r, r2)\n\t\t\tif combined == '\\uFFFD' {\n\t\t\t\tstr = appendRune(str, r)\n\t\t\t\tstr = appendRune(str, r2)\n\t\t\t} else {\n\t\t\t\tstr = appendRune(str, combined)\n\t\t\t}\n\t\t} else {\n\t\t\tstr = appendRune(str, r)\n\t\t}\n\tcase '\"':\n\t\tstr = append(str, '\"')\n\tcase '\\\\':\n\t\tstr = append(str, '\\\\')\n\tcase '/':\n\t\tstr = append(str, '/')\n\tcase 'b':\n\t\tstr = append(str, '\\b')\n\tcase 'f':\n\t\tstr = append(str, '\\f')\n\tcase 'n':\n\t\tstr = append(str, '\\n')\n\tcase 'r':\n\t\tstr = append(str, '\\r')\n\tcase 't':\n\t\tstr = append(str, '\\t')\n\tdefault:\n\t\treturn str, idx, errors.New(\"invalid escape char after \\\\\")\n\t}\n\treturn str, idx, nil\n}\n\n// refer to json-iterator/go/stream_str writeStringSlowPath\nfunc wrapStrWithQuotation(s string, strBuilder *strings.Builder) {\n\tstrBuilder.WriteByte(DQuotation)\n\tvalLen := len(s)\n\ti := 0\n\tstart := i\n\tfor i < valLen {\n\t\tc := s[i]\n\t\tif c < utf8.RuneSelf && htmlSafeSet[c] {\n\t\t\ti++\n\t\t\tcontinue\n\t\t} else {\n\t\t\tif b := s[i]; b < utf8.RuneSelf {\n\t\t\t\tif start < i {\n\t\t\t\t\tstrBuilder.WriteString(s[start:i])\n\t\t\t\t}\n\t\t\t\tswitch b {\n\t\t\t\tcase '\\\\', '\"':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte(b)\n\t\t\t\tcase '\\n':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte('n')\n\t\t\t\tcase '\\r':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte('r')\n\t\t\t\tcase '\\t':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte('t')\n\t\t\t\tdefault:\n\t\t\t\t\t// This encodes bytes < 0x20 except for \\t, \\n and \\r.\n\t\t\t\t\t// If escapeHTML is set, it also escapes <, >, and &\n\t\t\t\t\t// because they can lead to security holes when\n\t\t\t\t\t// user-controlled strings are rendered into JSON\n\t\t\t\t\t// and served to some browsers.\n\t\t\t\t\tstrBuilder.WriteString(`\\u00`)\n\t\t\t\t\tstrBuilder.WriteByte(hex[b>>4])\n\t\t\t\t\tstrBuilder.WriteByte(hex[b&0xF])\n\t\t\t\t}\n\t\t\t\ti++\n\t\t\t\tstart = i\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc, size := utf8.DecodeRuneInString(s[i:])\n\t\t\tif c == utf8.RuneError && size == 1 {\n\t\t\t\tif start < i {\n\t\t\t\t\tstrBuilder.WriteString(s[start:i])\n\t\t\t\t}\n\t\t\t\tstrBuilder.WriteString(`\\ufffd`)\n\t\t\t\ti++\n\t\t\t\tstart = i\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// U+2028 is LINE SEPARATOR.\n\t\t\t// U+2029 is PARAGRAPH SEPARATOR.\n\t\t\t// They are both technically valid characters in JSON strings,\n\t\t\t// but don't work in JSONP, which has to be evaluated as JavaScript,\n\t\t\t// and can lead to security holes there. It is valid JSON to\n\t\t\t// escape them, so we do so unconditionally.\n\t\t\t// See http://timelessrepo.com/json-isnt-a-javascript-subset for discussion.\n\t\t\tif c == '\\u2028' || c == '\\u2029' {\n\t\t\t\tif start < i {\n\t\t\t\t\tstrBuilder.WriteString(s[start:i])\n\t\t\t\t}\n\t\t\t\tstrBuilder.WriteString(`\\u202`)\n\t\t\t\tstrBuilder.WriteByte(hex[c&0xF])\n\t\t\t\ti += size\n\t\t\t\tstart = i\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ti += size\n\t\t}\n\t}\n\tif start < valLen {\n\t\tstrBuilder.WriteString(s[start:])\n\t}\n\tstrBuilder.WriteByte(DQuotation)\n}\n\n// refer to json-iterator/go/iter_str appendRune\nfunc appendRune(p []byte, r rune) []byte {\n\t// Negative values are erroneous. Making it unsigned addresses the problem.\n\tswitch i := uint32(r); {\n\tcase i <= rune1Max:\n\t\tp = append(p, byte(r))\n\t\treturn p\n\tcase i <= rune2Max:\n\t\tp = append(p, t2|byte(r>>6))\n\t\tp = append(p, tx|byte(r)&maskx)\n\t\treturn p\n\tcase i > maxRune, surrogateMin <= i && i <= surrogateMax:\n\t\tr = runeError\n\t\tfallthrough\n\tcase i <= rune3Max:\n\t\tp = append(p, t3|byte(r>>12))\n\t\tp = append(p, tx|byte(r>>6)&maskx)\n\t\tp = append(p, tx|byte(r)&maskx)\n\t\treturn p\n\tdefault:\n\t\tp = append(p, t4|byte(r>>18))\n\t\tp = append(p, tx|byte(r>>12)&maskx)\n\t\tp = append(p, tx|byte(r>>6)&maskx)\n\t\tp = append(p, tx|byte(r)&maskx)\n\t\treturn p\n\t}\n}\n\nvar htmlSafeSet = [utf8.RuneSelf]bool{\n\t' ':      true,\n\t'!':      true,\n\t'\"':      false,\n\t'#':      true,\n\t'$':      true,\n\t'%':      true,\n\t'&':      false,\n\t'\\'':     true,\n\t'(':      true,\n\t')':      true,\n\t'*':      true,\n\t'+':      true,\n\t',':      true,\n\t'-':      true,\n\t'.':      true,\n\t'/':      true,\n\t'0':      true,\n\t'1':      true,\n\t'2':      true,\n\t'3':      true,\n\t'4':      true,\n\t'5':      true,\n\t'6':      true,\n\t'7':      true,\n\t'8':      true,\n\t'9':      true,\n\t':':      true,\n\t';':      true,\n\t'<':      false,\n\t'=':      true,\n\t'>':      false,\n\t'?':      true,\n\t'@':      true,\n\t'A':      true,\n\t'B':      true,\n\t'C':      true,\n\t'D':      true,\n\t'E':      true,\n\t'F':      true,\n\t'G':      true,\n\t'H':      true,\n\t'I':      true,\n\t'J':      true,\n\t'K':      true,\n\t'L':      true,\n\t'M':      true,\n\t'N':      true,\n\t'O':      true,\n\t'P':      true,\n\t'Q':      true,\n\t'R':      true,\n\t'S':      true,\n\t'T':      true,\n\t'U':      true,\n\t'V':      true,\n\t'W':      true,\n\t'X':      true,\n\t'Y':      true,\n\t'Z':      true,\n\t'[':      true,\n\t'\\\\':     false,\n\t']':      true,\n\t'^':      true,\n\t'_':      true,\n\t'`':      true,\n\t'a':      true,\n\t'b':      true,\n\t'c':      true,\n\t'd':      true,\n\t'e':      true,\n\t'f':      true,\n\t'g':      true,\n\t'h':      true,\n\t'i':      true,\n\t'j':      true,\n\t'k':      true,\n\t'l':      true,\n\t'm':      true,\n\t'n':      true,\n\t'o':      true,\n\t'p':      true,\n\t'q':      true,\n\t'r':      true,\n\t's':      true,\n\t't':      true,\n\t'u':      true,\n\t'v':      true,\n\t'w':      true,\n\t'x':      true,\n\t'y':      true,\n\t'z':      true,\n\t'{':      true,\n\t'|':      true,\n\t'}':      true,\n\t'~':      true,\n\t'\\u007f': true,\n}\n\n```\n# Function Signature: func _Map2JSONStr(mapInfo map[string]string) (str string, err error) {\n# Natural Language Annotations: // Map2JSONStr transform map[string]string to json str, perf is better than use json lib directly\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc Map2JSONStr(mapInfo map[string]string) (str string, err error) {\n\tvar strBuilder strings.Builder\n\tstrBuilder.WriteByte(LeftBrace)\n\tfor key, val := range mapInfo {\n\t\tif key != \"\" {\n\t\t\twrapStrWithQuotation(key, &strBuilder)\n\t\t\tstrBuilder.WriteByte(Colon)\n\t\t\twrapStrWithQuotation(val, &strBuilder)\n\t\t\tstrBuilder.WriteByte(Comma)\n\t\t}\n\t}\n\tif strBuilder.Len() > 0 {\n\t\tstrBuilder.Reset()\n\t}\n\tstrBuilder.WriteByte(RightBrace)\n\tstr = strBuilder.String()\n\treturn str, err\n}\n```", "  ```\nfunc Map2JSONStr(mapInfo map[string]string) (str string, err error) {\n\tvar strBuilder strings.Builder\n\tif len(mapInfo) == 0 {\n\t\treturn EmptyJSON, nil\n\t}\n\tstrBuilder.Grow(len(mapInfo)-1)\n\tstrBuilder.WriteByte(LeftBrace)\n\n\tidx := 0\n\tfor k, v := range mapInfo {\n\t\tif idx > 0 {\n\t\t\tstrBuilder.WriteByte(Comma)\n\t\t}\n\t\twrapStrWithQuotation(k, &strBuilder)\n\t\tstrBuilder.WriteByte(Colon)\n\t\twrapStrWithQuotation(v, &strBuilder)\n\t\tidx++\n\t}\n\n\tstrBuilder.WriteByte(RightBrace)\n\tstr = strBuilder.String()\n\treturn str, nil\n}\n```", "  ```\nfunc _Map2JSONStr(mapInfo map[string]string) (str string, err error) {\n\tvar strBuilder strings.Builder\n\tstrBuilder.Grow(len(mapInfo) * 2)\n\tstrBuilder.WriteByte(LeftBrace)\n\n\tfor k, v := range mapInfo {\n\t\tstrBuilder.WriteByte(DQuotation)\n\t\twrapStrWithQuotation(k, &strBuilder)\n\t\tstrBuilder.WriteByte(DQuotation)\n\t\tstrBuilder.WriteByte(Colon)\n\t\tstrBuilder.WriteByte(v)\n\t\tstrBuilder.WriteByte(Comma)\n\t}\n\tif strBuilder.Len() > 0 {\n\t\tstrBuilder.Truncate(strBuilder.Len() - 1)\n\t}\n\tstrBuilder.WriteByte(RightBrace)\n\treturn string(strBuilder.Bytes()), nil\n}\n```"]}, "kitex-pkg/utils/json.go-_JSONStr2Map": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/json.go:\n```\n/*\n * Copyright 2024 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * MIT License\n *\n * Copyright (c) 2016 json-iterator\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in all\n * copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n * The source code of this file is written based on json-iterator,\n * all modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage utils\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\t\"unicode/utf16\"\n\t\"unicode/utf8\"\n\t\"unsafe\"\n)\n\n// const of json keyword char\nconst (\n\tEmptyJSON  = \"{}\"\n\tComma      = ','\n\tColon      = ':'\n\tDQuotation = '\"'\n\tLeftBrace  = '{'\n\tRightBrace = '}'\n)\n\nconst (\n\tt1 = 0x00 // 0000 0000\n\ttx = 0x80 // 1000 0000\n\tt2 = 0xC0 // 1100 0000\n\tt3 = 0xE0 // 1110 0000\n\tt4 = 0xF0 // 1111 0000\n\tt5 = 0xF8 // 1111 1000\n\n\tmaskx = 0x3F // 0011 1111\n\n\trune1Max = 1<<7 - 1\n\trune2Max = 1<<11 - 1\n\trune3Max = 1<<16 - 1\n\n\tsurrogateMin = 0xD800\n\tsurrogateMax = 0xDFFF\n\n\tmaxRune   = '\\U0010FFFF' // Maximum valid Unicode code point.\n\truneError = '\\uFFFD'     // the \"error\" Rune or \"Unicode replacement character\"\n\n\thex = \"0123456789abcdef\"\n)\n\n// Map2JSONStr transform map[string]string to json str, perf is better than use json lib directly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// JSONStr2Map transform json str to map[string]string, perf is better than use json lib directly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc readString(buf []byte, idx, lastIdx int) (string, int, error) {\n\tvar err error\n\tvar c byte\n\tvar isNull bool\n\tif c, idx, err = nextToken(buf, idx, lastIdx); err != nil {\n\t\treturn \"\", idx, err\n\t}\n\tvar str []byte\n\tif c == '\"' {\n\t\tstart := idx\n\t\tnoESC := true\n\t\tfor idx <= lastIdx {\n\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn \"\", idx, err\n\t\t\t}\n\t\t\tswitch c {\n\t\t\tcase '\"':\n\t\t\t\tif start < idx-1 {\n\t\t\t\t\tif noESC {\n\t\t\t\t\t\tstr = buf[start : idx-1]\n\t\t\t\t\t} else {\n\t\t\t\t\t\tstr = append(str, buf[start:idx-1]...)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn *(*string)(unsafe.Pointer(&str)), idx, nil\n\t\t\tcase '\\\\':\n\t\t\t\tif start < idx-1 {\n\t\t\t\t\tif noESC {\n\t\t\t\t\t\tstr = buf[start : idx-1]\n\t\t\t\t\t} else {\n\t\t\t\t\t\tstr = append(str, buf[start:idx-1]...)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\t\treturn \"\", idx, err\n\t\t\t\t}\n\t\t\t\tif str, idx, err = readEscapedChar(c, buf, idx, str, lastIdx); err != nil {\n\t\t\t\t\treturn \"\", 0, err\n\t\t\t\t}\n\t\t\t\tstart = idx\n\t\t\t\tnoESC = false\n\t\t\t}\n\t\t}\n\t} else if idx, isNull = checkNull(c, buf, idx, lastIdx); isNull {\n\t\treturn \"\", idx, nil\n\t}\n\terr = fmt.Errorf(\"json str is invalid, expects '\\\"' or n, but found %s\", string(c))\n\treturn *(*string)(unsafe.Pointer(&str)), idx, err\n}\n\nfunc readByte(buf []byte, idx, lastIdx int) (byte, int, error) {\n\tif lastIdx < idx {\n\t\treturn 0, -1, fmt.Errorf(\"readByte no more data\")\n\t}\n\tc := buf[idx]\n\tidx++\n\treturn c, idx, nil\n}\n\nfunc nextToken(buf []byte, idx, lastIdx int) (byte, int, error) {\n\tif lastIdx < idx {\n\t\treturn 0, -1, errors.New(\"nextToken no more data\")\n\t}\n\tvar c byte\n\tfor idx <= lastIdx {\n\t\tc = buf[idx]\n\t\tidx++\n\t\tswitch c {\n\t\tcase ' ', '\\n', '\\t', '\\r':\n\t\t\tcontinue\n\t\t}\n\t\treturn c, idx, nil\n\t}\n\treturn c, idx, nil\n}\n\nfunc checkNull(c byte, data []byte, idx, lastIdx int) (int, bool) {\n\tif c == 'n' {\n\t\tch, idx, _ := readByte(data, idx, lastIdx)\n\t\tif ch != 'u' {\n\t\t\tidx--\n\t\t\treturn idx, false\n\t\t}\n\t\tch, idx, _ = readByte(data, idx, lastIdx)\n\t\tif ch != 'l' {\n\t\t\tidx--\n\t\t\treturn idx, false\n\t\t}\n\t\tch, idx, _ = readByte(data, idx, lastIdx)\n\t\tif ch != 'l' {\n\t\t\tidx--\n\t\t\treturn idx, false\n\t\t}\n\t\treturn idx, true\n\t}\n\treturn idx, false\n}\n\nfunc readU4(buf []byte, idx, lastIdx int) (rune, int, error) {\n\tvar err error\n\tvar ret rune\n\tfor i := 0; i < 4; i++ {\n\t\tvar c byte\n\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\treturn ret, idx, err\n\t\t}\n\t\tif c >= '0' && c <= '9' {\n\t\t\tret = ret*16 + rune(c-'0')\n\t\t} else if c >= 'a' && c <= 'f' {\n\t\t\tret = ret*16 + rune(c-'a'+10)\n\t\t} else if c >= 'A' && c <= 'F' {\n\t\t\tret = ret*16 + rune(c-'A'+10)\n\t\t} else {\n\t\t\treturn ret, idx, fmt.Errorf(\"unicode invalid: expects 0~9 or a~f, but found %v\", string([]byte{c}))\n\t\t}\n\t}\n\treturn ret, idx, nil\n}\n\n// refer to json-iterator/go/iter_str readEscapedChar\nfunc readEscapedChar(c byte, buf []byte, idx int, str []byte, lastIdx int) ([]byte, int, error) {\n\tvar err error\n\tswitch c {\n\tcase 'u':\n\t\tvar r rune\n\t\tif r, idx, err = readU4(buf, idx, lastIdx); err != nil {\n\t\t\treturn str, idx, err\n\t\t}\n\t\t// \u662f\u5426\u662f\u6269\u5c55\u5b57\u7b26\n\t\tif utf16.IsSurrogate(r) {\n\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn str, idx, err\n\t\t\t}\n\t\t\tif c != '\\\\' {\n\t\t\t\tidx--\n\t\t\t\tstr = appendRune(str, r)\n\t\t\t\treturn str, idx, nil\n\t\t\t}\n\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn str, idx, err\n\t\t\t}\n\t\t\tif c != 'u' {\n\t\t\t\tstr = appendRune(str, r)\n\t\t\t\treturn readEscapedChar(c, buf, idx, str, lastIdx)\n\t\t\t}\n\t\t\tvar r2 rune\n\t\t\tif r2, idx, err = readU4(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn str, idx, err\n\t\t\t}\n\t\t\tcombined := utf16.DecodeRune(r, r2)\n\t\t\tif combined == '\\uFFFD' {\n\t\t\t\tstr = appendRune(str, r)\n\t\t\t\tstr = appendRune(str, r2)\n\t\t\t} else {\n\t\t\t\tstr = appendRune(str, combined)\n\t\t\t}\n\t\t} else {\n\t\t\tstr = appendRune(str, r)\n\t\t}\n\tcase '\"':\n\t\tstr = append(str, '\"')\n\tcase '\\\\':\n\t\tstr = append(str, '\\\\')\n\tcase '/':\n\t\tstr = append(str, '/')\n\tcase 'b':\n\t\tstr = append(str, '\\b')\n\tcase 'f':\n\t\tstr = append(str, '\\f')\n\tcase 'n':\n\t\tstr = append(str, '\\n')\n\tcase 'r':\n\t\tstr = append(str, '\\r')\n\tcase 't':\n\t\tstr = append(str, '\\t')\n\tdefault:\n\t\treturn str, idx, errors.New(\"invalid escape char after \\\\\")\n\t}\n\treturn str, idx, nil\n}\n\n// refer to json-iterator/go/stream_str writeStringSlowPath\nfunc wrapStrWithQuotation(s string, strBuilder *strings.Builder) {\n\tstrBuilder.WriteByte(DQuotation)\n\tvalLen := len(s)\n\ti := 0\n\tstart := i\n\tfor i < valLen {\n\t\tc := s[i]\n\t\tif c < utf8.RuneSelf && htmlSafeSet[c] {\n\t\t\ti++\n\t\t\tcontinue\n\t\t} else {\n\t\t\tif b := s[i]; b < utf8.RuneSelf {\n\t\t\t\tif start < i {\n\t\t\t\t\tstrBuilder.WriteString(s[start:i])\n\t\t\t\t}\n\t\t\t\tswitch b {\n\t\t\t\tcase '\\\\', '\"':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte(b)\n\t\t\t\tcase '\\n':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte('n')\n\t\t\t\tcase '\\r':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte('r')\n\t\t\t\tcase '\\t':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte('t')\n\t\t\t\tdefault:\n\t\t\t\t\t// This encodes bytes < 0x20 except for \\t, \\n and \\r.\n\t\t\t\t\t// If escapeHTML is set, it also escapes <, >, and &\n\t\t\t\t\t// because they can lead to security holes when\n\t\t\t\t\t// user-controlled strings are rendered into JSON\n\t\t\t\t\t// and served to some browsers.\n\t\t\t\t\tstrBuilder.WriteString(`\\u00`)\n\t\t\t\t\tstrBuilder.WriteByte(hex[b>>4])\n\t\t\t\t\tstrBuilder.WriteByte(hex[b&0xF])\n\t\t\t\t}\n\t\t\t\ti++\n\t\t\t\tstart = i\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc, size := utf8.DecodeRuneInString(s[i:])\n\t\t\tif c == utf8.RuneError && size == 1 {\n\t\t\t\tif start < i {\n\t\t\t\t\tstrBuilder.WriteString(s[start:i])\n\t\t\t\t}\n\t\t\t\tstrBuilder.WriteString(`\\ufffd`)\n\t\t\t\ti++\n\t\t\t\tstart = i\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// U+2028 is LINE SEPARATOR.\n\t\t\t// U+2029 is PARAGRAPH SEPARATOR.\n\t\t\t// They are both technically valid characters in JSON strings,\n\t\t\t// but don't work in JSONP, which has to be evaluated as JavaScript,\n\t\t\t// and can lead to security holes there. It is valid JSON to\n\t\t\t// escape them, so we do so unconditionally.\n\t\t\t// See http://timelessrepo.com/json-isnt-a-javascript-subset for discussion.\n\t\t\tif c == '\\u2028' || c == '\\u2029' {\n\t\t\t\tif start < i {\n\t\t\t\t\tstrBuilder.WriteString(s[start:i])\n\t\t\t\t}\n\t\t\t\tstrBuilder.WriteString(`\\u202`)\n\t\t\t\tstrBuilder.WriteByte(hex[c&0xF])\n\t\t\t\ti += size\n\t\t\t\tstart = i\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ti += size\n\t\t}\n\t}\n\tif start < valLen {\n\t\tstrBuilder.WriteString(s[start:])\n\t}\n\tstrBuilder.WriteByte(DQuotation)\n}\n\n// refer to json-iterator/go/iter_str appendRune\nfunc appendRune(p []byte, r rune) []byte {\n\t// Negative values are erroneous. Making it unsigned addresses the problem.\n\tswitch i := uint32(r); {\n\tcase i <= rune1Max:\n\t\tp = append(p, byte(r))\n\t\treturn p\n\tcase i <= rune2Max:\n\t\tp = append(p, t2|byte(r>>6))\n\t\tp = append(p, tx|byte(r)&maskx)\n\t\treturn p\n\tcase i > maxRune, surrogateMin <= i && i <= surrogateMax:\n\t\tr = runeError\n\t\tfallthrough\n\tcase i <= rune3Max:\n\t\tp = append(p, t3|byte(r>>12))\n\t\tp = append(p, tx|byte(r>>6)&maskx)\n\t\tp = append(p, tx|byte(r)&maskx)\n\t\treturn p\n\tdefault:\n\t\tp = append(p, t4|byte(r>>18))\n\t\tp = append(p, tx|byte(r>>12)&maskx)\n\t\tp = append(p, tx|byte(r>>6)&maskx)\n\t\tp = append(p, tx|byte(r)&maskx)\n\t\treturn p\n\t}\n}\n\nvar htmlSafeSet = [utf8.RuneSelf]bool{\n\t' ':      true,\n\t'!':      true,\n\t'\"':      false,\n\t'#':      true,\n\t'$':      true,\n\t'%':      true,\n\t'&':      false,\n\t'\\'':     true,\n\t'(':      true,\n\t')':      true,\n\t'*':      true,\n\t'+':      true,\n\t',':      true,\n\t'-':      true,\n\t'.':      true,\n\t'/':      true,\n\t'0':      true,\n\t'1':      true,\n\t'2':      true,\n\t'3':      true,\n\t'4':      true,\n\t'5':      true,\n\t'6':      true,\n\t'7':      true,\n\t'8':      true,\n\t'9':      true,\n\t':':      true,\n\t';':      true,\n\t'<':      false,\n\t'=':      true,\n\t'>':      false,\n\t'?':      true,\n\t'@':      true,\n\t'A':      true,\n\t'B':      true,\n\t'C':      true,\n\t'D':      true,\n\t'E':      true,\n\t'F':      true,\n\t'G':      true,\n\t'H':      true,\n\t'I':      true,\n\t'J':      true,\n\t'K':      true,\n\t'L':      true,\n\t'M':      true,\n\t'N':      true,\n\t'O':      true,\n\t'P':      true,\n\t'Q':      true,\n\t'R':      true,\n\t'S':      true,\n\t'T':      true,\n\t'U':      true,\n\t'V':      true,\n\t'W':      true,\n\t'X':      true,\n\t'Y':      true,\n\t'Z':      true,\n\t'[':      true,\n\t'\\\\':     false,\n\t']':      true,\n\t'^':      true,\n\t'_':      true,\n\t'`':      true,\n\t'a':      true,\n\t'b':      true,\n\t'c':      true,\n\t'd':      true,\n\t'e':      true,\n\t'f':      true,\n\t'g':      true,\n\t'h':      true,\n\t'i':      true,\n\t'j':      true,\n\t'k':      true,\n\t'l':      true,\n\t'm':      true,\n\t'n':      true,\n\t'o':      true,\n\t'p':      true,\n\t'q':      true,\n\t'r':      true,\n\t's':      true,\n\t't':      true,\n\t'u':      true,\n\t'v':      true,\n\t'w':      true,\n\t'x':      true,\n\t'y':      true,\n\t'z':      true,\n\t'{':      true,\n\t'|':      true,\n\t'}':      true,\n\t'~':      true,\n\t'\\u007f': true,\n}\n\n```\n# Function Signature: func _JSONStr2Map(jsonStr string) (mapInfo map[string]string, err error) {\n# Natural Language Annotations: // JSONStr2Map transform json str to map[string]string, perf is better than use json lib directly\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc _JSONStr2Map(jsonStr string) (mapInfo map[string]string, err error) {\n\t// Remove whitespaces from the JSON string\n\tjsonStr = strings.ReplaceAll(jsonStr, \" \", \"\")\n\t// Check if the JSON string is valid\n\tif !strings.HasPrefix(jsonStr, LeftBrace) || !strings.HasSuffix(jsonStr, RightBrace) {\n\t\terr = errors.New(\"invalid JSON string\")\n\t\treturn\n\t}\n\t// Create a map to store the key-value pairs\n\tmapInfo = make(map[string]string)\n\t// Parse the JSON string and populate the map\n\tif err = json.Unmarshal([]byte(jsonStr), &mapInfo); err != nil {\n\t\treturn\n\t}\n\treturn\n}\n```", "  ```\nfunc _JSONStr2Map(jsonStr string) (mapInfo map[string]string, err error) {\n\tjsonStr = strings.TrimSpace(jsonStr)\n\tif !strings.HasPrefix(jsonStr, \"{\") || !strings.HasSuffix(jsonStr, \"}\") {\n\t\treturn nil, errors.New(\"expect { and } at the beginning and end of json str\")\n\t}\n\t// remove {}\n\tjsonStr = jsonStr[1 : len(jsonStr)-1]\n\t// remove quote\n\tjsonStr = strings.ReplaceAll(jsonStr, \"\\\"\", \"\")\n\t// split key-value pair\n\titems := strings.Split(jsonStr, \",\")\n\tmapInfo = make(map[string]string)\n\tfor _, item := range items {\n\t\tkeyVal := strings.Split(item, \":\")\n\t\tif len(keyVal) != 2 {\n\t\t\treturn nil, errors.New(\"invalid json str, key-value pair not found\")\n\t\t}\n\t\tkey := strings.TrimSpace(keyVal[0])\n\t\tvalue := strings.TrimSpace(keyVal[1])\n\t\tif len(key) == 0 || len(value) == 0 {\n\t\t\treturn nil, errors.New(\"invalid json str, key and value cannot be empty\")\n\t\t}\n\t\tmapInfo[key] = value\n\t}\n\treturn mapInfo, nil\n}\n```", "  ```\nfunc JSONStr2Map(jsonStr string) (mapInfo map[string]string, err error) {\n\tvar buf []byte\n\tvar c byte\n\tvar idx int\n\tvar lastIdx int\n\tvar str []byte\n\tvar isNull bool\n\tvar err error\n\tvar ret rune\n\n\tbuf = []byte(jsonStr)\n\tlastIdx = len(buf) - 1\n\tidx = 0\n\n\tif c, idx, err = nextToken(buf, idx, lastIdx); err != nil {\n\t\treturn nil, err\n\t}\n\n\tif c != LeftBrace {\n\t\terr = fmt.Errorf(\"json str don't start with '{'\")\n\t\treturn nil, err\n\t}\n\n\tmapInfo = make(map[string]string)\n\tfor idx <= lastIdx {\n\t\tc, idx, err = nextToken(buf, idx, lastIdx)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif c == RightBrace {\n\t\t\tbreak\n\t\t}\n\t\tif c != DQuotation {\n\t\t\terr = fmt.Errorf(\"mapKey must be a string, but got %s\", string([]byte{c}))\n\t\t\treturn nil, err\n\t\t}\n\t\tstart := idx\n\t\tnoESC := true\n\t\tfor idx <= lastIdx {\n\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tswitch c {\n\t\t\tcase DQuotation:\n\t\t\t\tif start < idx-1 {\n\t\t\t\t\tif noESC {\n\t\t\t\t\t\tstr = buf[start : idx-1]\n\t\t\t\t\t} else {\n\t\t\t\t\t\tstr = append(str, buf[start:idx-1]...)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tidx++\n\t\t\t\tbreak\n\t\t\tcase '\\\\':\n\t\t\t\tif start < idx-1 {\n\t\t\t\t\tif noESC {\n\t\t\t\t\t\tstr = buf[start : idx-1]\n\t\t\t\t\t} else {\n\t\t\t\t\t\tstr = append(str, buf[start:idx-1]...)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tif str, idx, err = readEscapedChar(c, buf, idx, str, lastIdx); err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tstart = idx\n\t\t\t\tnoESC = false\n\t\t\t}\n\t\t}\n\t\tmapKey := *(*string)(unsafe.Pointer(&str))\n\t\tstr = nil\n\n\t\tc, idx, err = nextToken(buf, idx, lastIdx)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif c != Colon {\n\t\t\terr = fmt.Errorf(\"json str don't contains ':' after mapKey\")\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif c, idx, err = nextToken(buf, idx, lastIdx); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tif idx, isNull = checkNull(c, buf, idx, lastIdx); isNull {\n\t\t\tmapInfo[mapKey] = \"\"\n\t\t\tcontinue\n\t\t}\n\n\t\tif c != DQuotation {\n\t\t\terr = fmt.Errorf(\"mapValue must be a string, but got %s\", string([]byte{c}))\n\t\t\treturn nil, err\n\t\t}\n\n\t\tstart = idx\n\t\tnoESC := true\n\t\tfor idx <= lastIdx {\n\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tswitch c {\n\t\t"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetCaller": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetCaller(ctx context.Context) (string, bool) {\n# Natural Language Annotations: // GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc GetCaller(ctx context.Context) (string, bool) {\n\tvar (\n\t\tsd *rpcinfo.Server\n\t\tok bool\n\t)\n\tif sd, ok = rpcinfo.GetServerInfo(ctx); !ok {\n\t\treturn \"\", false\n\t}\n\treturn sd.ServiceName, true\n}\n```", "  ```\nfunc GetCaller(ctx context.Context) (string, bool) {\n\tvar (\n\t\tcallerInfo string\n\t\tok         bool\n\t)\n\trpcinfo := rpcinfo.GetRPCInfo(ctx)\n\tif rpcinfo != nil {\n\t\tcallerInfo, ok = rpcinfo.From().GetServiceName()\n\t\tif !ok {\n\t\t\tcallerInfo, ok = rpcinfo.To().GetServiceName()\n\t\t}\n\t}\n\treturn callerInfo, ok\n}\n```", "  ```\nfunc GetCaller(ctx context.Context) (string, bool) {\n    if rpcinfo, ok := ctx.Get(\"RPCInfo\"); ok {\n        if serverInfo, ok := rpcinfo.(*rpcinfo.ServerInfo); ok {\n            return serverInfo.ServiceName, true\n        }\n    }\n    return \"\", false\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetMethod": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetMethod(ctx context.Context) (string, bool) {\n# Natural Language Annotations: // GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc GetMethod(ctx context.Context) (string, bool) {\n\tri := rpcinfo.GetRPCInfo(ctx)\n\tif ri == nil {\n\t\treturn \"\", false\n\t}\n\treturn ri.Method(), true\n}\n```", "  ```\nfunc GetMethod(ctx context.Context) (string, bool) {\n\trpcInfo, ok := ctx.Value(rpcinfo.ContextKey).(*rpcinfo.RPCInfo)\n\tif !ok {\n\t\treturn \"\", false\n\t}\n\treturn rpcInfo.Method, true\n}\n```", "  ```\nfunc GetMethod(ctx context.Context) (string, bool) {\n\tri, err := rpcinfo.GetServerRPCInfo(ctx)\n\tif err != nil {\n\t\treturn \"\", false\n\t}\n\tmethod, ok := ri.To()\n\treturn method, ok\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetCallerHandlerMethod": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetCallerHandlerMethod(ctx context.Context) (string, bool) {\n# Natural Language Annotations: // GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc GetCallerHandlerMethod(ctx context.Context) (string, bool) {\n    info := GetRPCInfo(ctx)\n    if info == nil {\n        return \"\", false\n    }\n    method, ok := info.Args().Get(K_METHOD)\n    if !ok {\n        return \"\", false\n    }\n    return method, true\n}\n```", "  ```\nfunc GetCallerHandlerMethod(ctx context.Context) (string, bool) {\n\tvar (\n\t\tmethod string\n\t\tok     bool\n\t)\n\n\tif method, ok = ctx.Value(K_METHOD).(string); ok {\n\t\treturn method, true\n\t}\n\n\tif rpcinfo.GetRPCInfo(ctx) != nil {\n\t\tmethod = rpcinfo.GetRPCInfo(ctx).Operation()\n\t\treturn method, true\n\t}\n\n\treturn \"\", false\n}\n```", "  ```\nfunc GetCallerHandlerMethod(ctx context.Context) (string, bool) {\n\tkitexRPCInfo, ok := twerpc.GetRPCInfo(ctx)\n\tif !ok {\n\t\treturn \"\", false\n\t}\n\tserviceName, ok := kitexRPCInfo.GetServiceName()\n\tif !ok {\n\t\treturn \"\", false\n\t}\n\tmethodName, ok := kitexRPCInfo.GetMethodName()\n\tif !ok {\n\t\treturn \"\", false\n\t}\n\treturn fmt.Sprintf(\"%s.%s\", serviceName, methodName), true\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetIDLServiceName": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetIDLServiceName(ctx context.Context) (string, bool) {\n# Natural Language Annotations: // GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc GetIDLServiceName(ctx context.Context) (string, bool) {\n\tri, ok := rpcinfo.GetRPCInfo(ctx)\n\tif !ok {\n\t\treturn \"\", false\n\t}\n\tsvc, ok := ri.To()\n\tif !ok {\n\t\treturn \"\", false\n\t}\n\treturn svc.ServiceName(), true\n}\n```", "  ```\nfunc GetIDLServiceName(ctx context.Context) (string, bool) {\n    return utils.FromContext(ctx, func(info *rpcinfo.RPCInfo) (string, bool) {\n        return info.ServiceName, true\n    })\n}\n```", "  ```\nfunc GetIDLServiceName(ctx context.Context) (string, bool) {\n\trpcInfo, ok := ctx.Value(rpcinfo.RPCInfoKey).(rpcinfo.RPCInfo)\n\tif !ok {\n\t\treturn \"\", false\n\t}\n\tserviceName, ok := rpcInfo.GetMetaIn().Get[\"serviceName\"]\n\tif !ok {\n\t\treturn \"\", false\n\t}\n\treturn serviceName, true\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetCallerAddr": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetCallerAddr(ctx context.Context) (net.Addr, bool) {\n# Natural Language Annotations: // GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc GetCallerAddr(ctx context.Context) (net.Addr, bool) {\n    return nil, false\n}\n```", "  ```\nfunc GetCallerAddr(ctx context.Context) (net.Addr, bool) {\n\tresp, ok := GetRPCInfo(ctx)\n\tif !ok {\n\t\treturn nil, false\n\t}\n\treturn resp.CallerAddress(), true\n}\n```", "  ```\nfunc GetCallerAddr(ctx context.Context) (net.Addr, bool) {\n\tif rpcinfo.GetRPCInfo(ctx) != nil { // Check if the RPCInfo exists in the context\n\t\tif serv := rpcinfo.GetRPCInfo(ctx).AsServerCallInfo(); serv != nil { // Check if the context is for a server call\n\t\t\treturn serv.GetAddr(), true // If it is a server call, return its Addr and true\n\t\t}\n\t}\n\treturn nil, false // If no Addr was found, return nil and false\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetCallerIP": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetCallerIP(ctx context.Context) (string, bool) {\n# Natural Language Annotations: // GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc GetCallerIP(ctx context.Context) (string, bool) {\n\tri, ok := utils.RPCInfoFromContext(ctx)\n\tif !ok {\n\t\treturn \"\", false\n\t}\n\tconn, ok := ri.RequestInfo.(utils.Connection)\n\tif !ok {\n\t\treturn \"\", false\n\t}\n\trawConn, ok := conn.(net.Conn)\n\tif !ok {\n\t\treturn \"\", false\n\t}\n\tremoteAddr := rawConn.RemoteAddr()\n\treturn remoteAddr.String(), true\n}\n```", "  ```\nfunc GetCallerIP(ctx context.Context) (string, bool) {\n\trpcInfo := GetRPCInfo(ctx)\n\tif rpcInfo == nil {\n\t\treturn \"\", false\n\t}\n\tcaller, err := utils.GetCCInfo(rpcInfo.GetCtx())(rpcInfo)\n\tif err != nil {\n\t\treturn \"\", false\n\t}\n\treturn caller.GetPeerAddr().String(), true\n}\n```", "  ```\nfunc GetCallerIP(ctx context.Context) (string, bool) {\n\tvar (\n\t\tok                  bool\n\t\tcaller, method, addr string\n\t)\n\n\t// get caller information\n\tif caller, method, ok = GetCaller(ctx); !ok {\n\t\treturn \"\", false\n\t}\n\n\t// get method information\n\tif methodInfo, ok := GetMethod(ctx); !ok {\n\t\treturn \"\", false\n\t}\n\n\t// get address information\n\tif addr, ok = GetCallerAddr(ctx); !ok {\n\t\treturn \"\", false\n\t}\n\n\t// assemble IP from caller, method, and address information\n\tcallerIP := net.JoinHostPort(caller, strconv.Itoa(methodInfo.Port))\n\tif !strings.HasPrefix(callerIP, address) {\n\t\treturn \"\", false\n\t}\n\n\treturn net.SplitHostPort(callerIP)\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetTransportProtocol": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetTransportProtocol(ctx context.Context) (string, bool) {\n# Natural Language Annotations: // GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc GetTransportProtocol(ctx context.Context) (string, bool) {\n\tif rpcInfo, ok := kitexutil.GetRPCInfo(ctx); ok {\n\t\treturn rpcInfo.Request.TransportProtocol, true\n\t}\n\treturn \"\", false\n}\n```", "  ```\nfunc GetTransportProtocol(ctx context.Context) (string, bool) {\n\tif rpcInfo, ok := GetRPCInfo(ctx); ok {\n\t\treturn rpcInfo.ServiceInfo().GetTransportProtocol(), true\n\t}\n\treturn \"\", false\n}\n```", "  ```\nfunc GetTransportProtocol(ctx context.Context) (string, bool) {\n\tvar errHandler = GetErrHandler(ctx)\n\tif md, ok := metadata.FromIncomingContext(ctx); ok {\n\t\tif val, ok := md[\"transport\"]; ok {\n\t\t\treturn string(val), true\n\t\t}\n\t}\n\terrHandler(context.Canceled, \"failed to obtain transport protocol from context\")\n\treturn \"\", false\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetRPCInfo": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetRPCInfo(ctx context.Context) (rpcinfo.RPCInfo, bool) {\n# Natural Language Annotations: // GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc GetRPCInfo(ctx context.Context) (rpcinfo.RPCInfo, bool) {\n\tinfo, ok := ctx.Value(rpcinfo.CtxRPCInfo).(rpcinfo.RPCInfo)\n\treturn info, ok\n}\n```", "  ```\nfunc GetRPCInfo(ctx context.Context) (rpcinfo.RPCInfo, bool) {\n\tri, ok := GetRPCInfoFromContext(ctx)\n\tif !ok {\n\t\treturn rpcinfo.RPCInfo{}, false\n\t}\n\treturn ri, true\n}\n```", "  ```\nfunc GetRPCInfo(ctx context.Context) (rpcinfo.RPCInfo, bool) {\n\tif ctx == nil {\n\t\treturn nil, false\n\t}\n\tif value := ctx.Value(rpcinfo.CtxRPCInfo); value != nil {\n\t\tif rpcinfo, ok := value.(rpcinfo.RPCInfo); ok {\n\t\t\treturn rpcinfo, true\n\t\t}\n\t}\n\treturn nil, false\n}\n```"]}, "kitex-pkg/remote/trans_errors.go-NewTransError": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans_errors.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage remote\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\n// corresponding with thrift TApplicationException, cannot change it\nconst (\n\tUnknownApplicationException = 0\n\tUnknownMethod               = 1\n\tInvalidMessageTypeException = 2\n\tWrongMethodName             = 3\n\tBadSequenceID               = 4\n\tMissingResult               = 5\n\tInternalError               = 6\n\tProtocolError               = 7\n\tInvalidTransform            = 8\n\tInvalidProtocol             = 9\n\tUnsupportedClientType       = 10\n\t// kitex's own type id from number 20\n\tUnknownService = 20\n\tNoServiceName  = 21\n)\n\nvar defaultTransErrorMessage = map[int32]string{\n\tUnknownApplicationException: \"unknown application exception\",\n\tUnknownMethod:               \"unknown method\",\n\tInvalidMessageTypeException: \"invalid message type\",\n\tWrongMethodName:             \"wrong method name\",\n\tBadSequenceID:               \"bad sequence ID\",\n\tMissingResult:               \"missing result\",\n\tInternalError:               \"unknown internal error\",\n\tProtocolError:               \"unknown protocol error\",\n\tInvalidTransform:            \"Invalid transform\",\n\tInvalidProtocol:             \"Invalid protocol\",\n\tUnsupportedClientType:       \"Unsupported client type\",\n\tUnknownService:              \"unknown service\",\n}\n\n// TransError is the error that can be transmitted, it corresponds to TApplicationException in Thrift\ntype TransError struct {\n\tmessage string\n\ttypeID  int32\n\trawErr  error\n}\n\n// Error implements the error interface.\nfunc (e TransError) Error() string {\n\tif e.message != \"\" {\n\t\treturn e.message\n\t}\n\treturn defaultTransErrorMessage[e.typeID]\n}\n\n// TypeID return err type id\nfunc (e TransError) TypeID() int32 {\n\treturn e.typeID\n}\n\n// Unwrap the transError to expose raw error\nfunc (e TransError) Unwrap() error {\n\treturn e.rawErr\n}\n\n// Is to check if inner error that transError wrap is target error\nfunc (e TransError) Is(target error) bool {\n\treturn e == target || errors.Is(e.rawErr, target)\n}\n\n// AppendMessage append extra msg for TransError\nfunc (e TransError) AppendMessage(extraMsg string) *TransError {\n\tif extraMsg == \"\" {\n\t\treturn &e\n\t}\n\tmsg := fmt.Sprintf(\"%s %s\", e.message, extraMsg)\n\t// should not modify origin error\n\treturn &TransError{message: msg, typeID: e.typeID, rawErr: e.rawErr}\n}\n\n// NewTransErrorWithMsg to build TransError with typeID and errMsg\nfunc NewTransErrorWithMsg(typeID int32, message string) *TransError {\n\treturn &TransError{message: message, typeID: typeID}\n}\n\n// NewTransError to build TransError with typeID and rawErr.\n// rawErr can be used by errors.Is(target) to check err type, like read timeout.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// TypeId is used to assert Error with has 'TypeID() int32'\ntype TypeID interface {\n\tTypeID() int32\n}\n\n// TypeId is used to assert Error with has 'TypeId() int32' like TApplicationException\ntype TypeId interface {\n\tTypeId() int32\n}\n\n```\n# Function Signature: func NewTransError(typeID int32, err error) *TransError {\n# Natural Language Annotations: // NewTransError to build TransError with typeID and rawErr.\n// rawErr can be used by errors.Is(target) to check err type, like read timeout.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewTransError(typeID int32, err error) *TransError {\n\treturn &TransError{typeID: typeID, rawErr: err}\n}\n```", "  ```\nfunc NewTransError(typeID int32, err error) *TransError {\n\treturn &TransError{\n\t\tmessage: defaultTransErrorMessage[typeID],\n\t\ttypeID:  typeID,\n\t\trawErr:  err,\n\t}\n}\n```", "  ```\nfunc NewTransError(typeID int32, err error) *TransError {\n\treturn &TransError{message: err.Error(), typeID: typeID, rawErr: err}\n}\n```"]}, "kitex-pkg/remote/trans/gonet/bytebuffer.go-NewBufferReader": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/gonet/bytebuffer.go:\n```\n/*\n * Copyright 2022 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage gonet\n\nimport (\n\t\"errors\"\n\t\"io\"\n\t\"sync\"\n\n\t\"github.com/cloudwego/netpoll\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n)\n\nvar rwPool sync.Pool\n\nfunc init() {\n\trwPool.New = newBufferReadWriter\n}\n\nvar _ remote.ByteBuffer = &bufferReadWriter{}\n\ntype bufferReadWriter struct {\n\treader netpoll.Reader\n\twriter netpoll.Writer\n\n\tioReader io.Reader\n\tioWriter io.Writer\n\n\treadSize int\n\tstatus   int\n}\n\nfunc newBufferReadWriter() interface{} {\n\treturn &bufferReadWriter{}\n}\n\n// NewBufferReader creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReader.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewBufferWriter creates a new remote.ByteBuffer using the given netpoll.ZeroCopyWriter.\nfunc NewBufferWriter(iw io.Writer) remote.ByteBuffer {\n\trw := rwPool.Get().(*bufferReadWriter)\n\trw.writer = netpoll.NewWriter(iw)\n\trw.ioWriter = iw\n\trw.status = remote.BitWritable\n\treturn rw\n}\n\n// NewBufferReadWriter creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReadWriter.\nfunc NewBufferReadWriter(irw io.ReadWriter) remote.ByteBuffer {\n\trw := rwPool.Get().(*bufferReadWriter)\n\trw.writer = netpoll.NewWriter(irw)\n\trw.reader = netpoll.NewReader(irw)\n\trw.ioWriter = irw\n\trw.ioReader = irw\n\trw.status = remote.BitWritable | remote.BitReadable\n\treturn rw\n}\n\nfunc (rw *bufferReadWriter) readable() bool {\n\treturn rw.status&remote.BitReadable != 0\n}\n\nfunc (rw *bufferReadWriter) writable() bool {\n\treturn rw.status&remote.BitWritable != 0\n}\n\nfunc (rw *bufferReadWriter) Next(n int) (p []byte, err error) {\n\tif !rw.readable() {\n\t\treturn nil, errors.New(\"unreadable buffer, cannot support Next\")\n\t}\n\tif p, err = rw.reader.Next(n); err == nil {\n\t\trw.readSize += n\n\t}\n\treturn\n}\n\nfunc (rw *bufferReadWriter) Peek(n int) (buf []byte, err error) {\n\tif !rw.readable() {\n\t\treturn nil, errors.New(\"unreadable buffer, cannot support Peek\")\n\t}\n\treturn rw.reader.Peek(n)\n}\n\nfunc (rw *bufferReadWriter) Skip(n int) (err error) {\n\tif !rw.readable() {\n\t\treturn errors.New(\"unreadable buffer, cannot support Skip\")\n\t}\n\treturn rw.reader.Skip(n)\n}\n\nfunc (rw *bufferReadWriter) ReadableLen() (n int) {\n\tif !rw.readable() {\n\t\treturn -1\n\t}\n\treturn rw.reader.Len()\n}\n\nfunc (rw *bufferReadWriter) ReadString(n int) (s string, err error) {\n\tif !rw.readable() {\n\t\treturn \"\", errors.New(\"unreadable buffer, cannot support ReadString\")\n\t}\n\tif s, err = rw.reader.ReadString(n); err == nil {\n\t\trw.readSize += n\n\t}\n\treturn\n}\n\nfunc (rw *bufferReadWriter) ReadBinary(n int) (p []byte, err error) {\n\tif !rw.readable() {\n\t\treturn p, errors.New(\"unreadable buffer, cannot support ReadBinary\")\n\t}\n\tif p, err = rw.reader.ReadBinary(n); err == nil {\n\t\trw.readSize += n\n\t}\n\treturn\n}\n\nfunc (rw *bufferReadWriter) Read(p []byte) (n int, err error) {\n\tif !rw.readable() {\n\t\treturn -1, errors.New(\"unreadable buffer, cannot support Read\")\n\t}\n\tif rw.ioReader != nil {\n\t\treturn rw.ioReader.Read(p)\n\t}\n\treturn -1, errors.New(\"ioReader is nil\")\n}\n\nfunc (rw *bufferReadWriter) ReadLen() (n int) {\n\treturn rw.readSize\n}\n\nfunc (rw *bufferReadWriter) Malloc(n int) (buf []byte, err error) {\n\tif !rw.writable() {\n\t\treturn nil, errors.New(\"unwritable buffer, cannot support Malloc\")\n\t}\n\treturn rw.writer.Malloc(n)\n}\n\nfunc (rw *bufferReadWriter) MallocLen() (length int) {\n\tif !rw.writable() {\n\t\treturn -1\n\t}\n\treturn rw.writer.MallocLen()\n}\n\nfunc (rw *bufferReadWriter) WriteString(s string) (n int, err error) {\n\tif !rw.writable() {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support WriteString\")\n\t}\n\treturn rw.writer.WriteString(s)\n}\n\nfunc (rw *bufferReadWriter) WriteBinary(b []byte) (n int, err error) {\n\tif !rw.writable() {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support WriteBinary\")\n\t}\n\treturn rw.writer.WriteBinary(b)\n}\n\nfunc (rw *bufferReadWriter) Flush() (err error) {\n\tif !rw.writable() {\n\t\treturn errors.New(\"unwritable buffer, cannot support Flush\")\n\t}\n\treturn rw.writer.Flush()\n}\n\nfunc (rw *bufferReadWriter) Write(p []byte) (n int, err error) {\n\tif !rw.writable() {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support Write\")\n\t}\n\tif rw.ioWriter != nil {\n\t\treturn rw.ioWriter.Write(p)\n\t}\n\treturn -1, errors.New(\"ioWriter is nil\")\n}\n\nfunc (rw *bufferReadWriter) Release(e error) (err error) {\n\tif rw.reader != nil {\n\t\terr = rw.reader.Release()\n\t}\n\trw.zero()\n\trwPool.Put(rw)\n\treturn\n}\n\n// WriteDirect is a way to write []byte without copying, and splits the original buffer.\n\n\n\n\n\n\n\nfunc (rw *bufferReadWriter) AppendBuffer(buf remote.ByteBuffer) (err error) {\n\tsubBuf, ok := buf.(*bufferReadWriter)\n\tif !ok {\n\t\treturn errors.New(\"AppendBuffer failed, Buffer is not bufferReadWriter\")\n\t}\n\tif err = rw.writer.Append(subBuf.writer); err != nil {\n\t\treturn\n\t}\n\treturn buf.Release(nil)\n}\n\n// NewBuffer returns a new writable remote.ByteBuffer.\nfunc (rw *bufferReadWriter) NewBuffer() remote.ByteBuffer {\n\tpanic(\"unimplemented\")\n}\n\nfunc (rw *bufferReadWriter) Bytes() (buf []byte, err error) {\n\tpanic(\"unimplemented\")\n}\n\nfunc (rw *bufferReadWriter) zero() {\n\trw.reader = nil\n\trw.writer = nil\n\trw.ioReader = nil\n\trw.ioWriter = nil\n\trw.readSize = 0\n\trw.status = 0\n}\n\n```\n# Function Signature: func NewBufferReader(ir io.Reader) remote.ByteBuffer {\n# Natural Language Annotations: // NewBufferReader creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReader.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewBufferReader(ir io.Reader) remote.ByteBuffer {\n\trw := rwPool.Get().(*bufferReadWriter)\n\trw.reader = netpoll.NewReader(ir)\n\trw.ioReader = ir\n\trw.status = remote.BitReadable\n\treturn rw\n}\n```", "  ```\nfunc NewBufferReader(ir io.Reader) remote.ByteBuffer {\n\trw := rwPool.Get().(*bufferReadWriter)\n\trw.reader = netpoll.NewReader(ir)\n\trw.ioReader = ir\n\trw.status = remote.BitReadable\n\treturn rw\n}\n```", "  ```\nfunc NewBufferReader(ir io.Reader) remote.ByteBuffer {\n    rw := rwPool.Get().(*bufferReadWriter)\n    rw.reader = netpoll.NewReader(ir)\n    rw.ioReader = ir\n    rw.status = remote.BitReadable\n    return rw\n}\n```"]}, "kitex-pkg/remote/trans/gonet/bytebuffer.go-WriteDirect": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/gonet/bytebuffer.go:\n```\n/*\n * Copyright 2022 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage gonet\n\nimport (\n\t\"errors\"\n\t\"io\"\n\t\"sync\"\n\n\t\"github.com/cloudwego/netpoll\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n)\n\nvar rwPool sync.Pool\n\nfunc init() {\n\trwPool.New = newBufferReadWriter\n}\n\nvar _ remote.ByteBuffer = &bufferReadWriter{}\n\ntype bufferReadWriter struct {\n\treader netpoll.Reader\n\twriter netpoll.Writer\n\n\tioReader io.Reader\n\tioWriter io.Writer\n\n\treadSize int\n\tstatus   int\n}\n\nfunc newBufferReadWriter() interface{} {\n\treturn &bufferReadWriter{}\n}\n\n// NewBufferReader creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReader.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewBufferWriter creates a new remote.ByteBuffer using the given netpoll.ZeroCopyWriter.\nfunc NewBufferWriter(iw io.Writer) remote.ByteBuffer {\n\trw := rwPool.Get().(*bufferReadWriter)\n\trw.writer = netpoll.NewWriter(iw)\n\trw.ioWriter = iw\n\trw.status = remote.BitWritable\n\treturn rw\n}\n\n// NewBufferReadWriter creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReadWriter.\nfunc NewBufferReadWriter(irw io.ReadWriter) remote.ByteBuffer {\n\trw := rwPool.Get().(*bufferReadWriter)\n\trw.writer = netpoll.NewWriter(irw)\n\trw.reader = netpoll.NewReader(irw)\n\trw.ioWriter = irw\n\trw.ioReader = irw\n\trw.status = remote.BitWritable | remote.BitReadable\n\treturn rw\n}\n\nfunc (rw *bufferReadWriter) readable() bool {\n\treturn rw.status&remote.BitReadable != 0\n}\n\nfunc (rw *bufferReadWriter) writable() bool {\n\treturn rw.status&remote.BitWritable != 0\n}\n\nfunc (rw *bufferReadWriter) Next(n int) (p []byte, err error) {\n\tif !rw.readable() {\n\t\treturn nil, errors.New(\"unreadable buffer, cannot support Next\")\n\t}\n\tif p, err = rw.reader.Next(n); err == nil {\n\t\trw.readSize += n\n\t}\n\treturn\n}\n\nfunc (rw *bufferReadWriter) Peek(n int) (buf []byte, err error) {\n\tif !rw.readable() {\n\t\treturn nil, errors.New(\"unreadable buffer, cannot support Peek\")\n\t}\n\treturn rw.reader.Peek(n)\n}\n\nfunc (rw *bufferReadWriter) Skip(n int) (err error) {\n\tif !rw.readable() {\n\t\treturn errors.New(\"unreadable buffer, cannot support Skip\")\n\t}\n\treturn rw.reader.Skip(n)\n}\n\nfunc (rw *bufferReadWriter) ReadableLen() (n int) {\n\tif !rw.readable() {\n\t\treturn -1\n\t}\n\treturn rw.reader.Len()\n}\n\nfunc (rw *bufferReadWriter) ReadString(n int) (s string, err error) {\n\tif !rw.readable() {\n\t\treturn \"\", errors.New(\"unreadable buffer, cannot support ReadString\")\n\t}\n\tif s, err = rw.reader.ReadString(n); err == nil {\n\t\trw.readSize += n\n\t}\n\treturn\n}\n\nfunc (rw *bufferReadWriter) ReadBinary(n int) (p []byte, err error) {\n\tif !rw.readable() {\n\t\treturn p, errors.New(\"unreadable buffer, cannot support ReadBinary\")\n\t}\n\tif p, err = rw.reader.ReadBinary(n); err == nil {\n\t\trw.readSize += n\n\t}\n\treturn\n}\n\nfunc (rw *bufferReadWriter) Read(p []byte) (n int, err error) {\n\tif !rw.readable() {\n\t\treturn -1, errors.New(\"unreadable buffer, cannot support Read\")\n\t}\n\tif rw.ioReader != nil {\n\t\treturn rw.ioReader.Read(p)\n\t}\n\treturn -1, errors.New(\"ioReader is nil\")\n}\n\nfunc (rw *bufferReadWriter) ReadLen() (n int) {\n\treturn rw.readSize\n}\n\nfunc (rw *bufferReadWriter) Malloc(n int) (buf []byte, err error) {\n\tif !rw.writable() {\n\t\treturn nil, errors.New(\"unwritable buffer, cannot support Malloc\")\n\t}\n\treturn rw.writer.Malloc(n)\n}\n\nfunc (rw *bufferReadWriter) MallocLen() (length int) {\n\tif !rw.writable() {\n\t\treturn -1\n\t}\n\treturn rw.writer.MallocLen()\n}\n\nfunc (rw *bufferReadWriter) WriteString(s string) (n int, err error) {\n\tif !rw.writable() {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support WriteString\")\n\t}\n\treturn rw.writer.WriteString(s)\n}\n\nfunc (rw *bufferReadWriter) WriteBinary(b []byte) (n int, err error) {\n\tif !rw.writable() {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support WriteBinary\")\n\t}\n\treturn rw.writer.WriteBinary(b)\n}\n\nfunc (rw *bufferReadWriter) Flush() (err error) {\n\tif !rw.writable() {\n\t\treturn errors.New(\"unwritable buffer, cannot support Flush\")\n\t}\n\treturn rw.writer.Flush()\n}\n\nfunc (rw *bufferReadWriter) Write(p []byte) (n int, err error) {\n\tif !rw.writable() {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support Write\")\n\t}\n\tif rw.ioWriter != nil {\n\t\treturn rw.ioWriter.Write(p)\n\t}\n\treturn -1, errors.New(\"ioWriter is nil\")\n}\n\nfunc (rw *bufferReadWriter) Release(e error) (err error) {\n\tif rw.reader != nil {\n\t\terr = rw.reader.Release()\n\t}\n\trw.zero()\n\trwPool.Put(rw)\n\treturn\n}\n\n// WriteDirect is a way to write []byte without copying, and splits the original buffer.\n\n\n\n\n\n\n\nfunc (rw *bufferReadWriter) AppendBuffer(buf remote.ByteBuffer) (err error) {\n\tsubBuf, ok := buf.(*bufferReadWriter)\n\tif !ok {\n\t\treturn errors.New(\"AppendBuffer failed, Buffer is not bufferReadWriter\")\n\t}\n\tif err = rw.writer.Append(subBuf.writer); err != nil {\n\t\treturn\n\t}\n\treturn buf.Release(nil)\n}\n\n// NewBuffer returns a new writable remote.ByteBuffer.\nfunc (rw *bufferReadWriter) NewBuffer() remote.ByteBuffer {\n\tpanic(\"unimplemented\")\n}\n\nfunc (rw *bufferReadWriter) Bytes() (buf []byte, err error) {\n\tpanic(\"unimplemented\")\n}\n\nfunc (rw *bufferReadWriter) zero() {\n\trw.reader = nil\n\trw.writer = nil\n\trw.ioReader = nil\n\trw.ioWriter = nil\n\trw.readSize = 0\n\trw.status = 0\n}\n\n```\n# Function Signature: func (rw *bufferReadWriter) WriteDirect(p []byte, remainCap int) error {\n# Natural Language Annotations: // WriteDirect is a way to write []byte without copying, and splits the original buffer.\nPlease return the generated code surrounded by ```\n", "response": ["  ```golang\nfunc (rw *bufferReadWriter) WriteDirect(p []byte, remainCap int) error {\n\tif !rw.writable() {\n\t\treturn errors.New(\"unwritable buffer, cannot support WriteDirect\")\n\t}\n\twriteSize := len(p)\n\tif writeSize > remainCap {\n\t\twriteSize = remainCap\n\t}\n\twrites := make([]byte, writeSize)\n\tcopy(writes, p[:writeSize])\n\tif _, err := rw.writer.Write(writes); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n```", "  ```\nfunc (rw *bufferReadWriter) WriteDirect(p []byte, remainCap int) error {\n\tif !rw.writable() {\n\t\treturn errors.New(\"unwritable buffer, cannot support WriteDirect\")\n\t}\n\tif remainCap >= len(p) {\n\t\treturn rw.writer.AppendBinary(p[:])\n\t}\n\tvar tmpBuf []byte\n\ttmpBuf = rw.writer.Malloc(len(p))\n\tcopy(tmpBuf, p)\n\tif rw.writer.AppendBinary(tmpBuf) != nil {\n\t\treturn nil\n\t}\n\treturn rw.writer.Flush()\n}\n```", "  ```\nfunc (rw *bufferReadWriter) WriteDirect(p []byte, remainCap int) error {\n\tif !rw.writable() {\n\t\treturn errors.New(\"unwritable buffer, cannot support WriteDirect\")\n\t}\n\t// if remainCap is larger than MallocLen, it will return an error\n\tif remainCap > rw.writer.MallocLen() {\n\t\treturn errors.New(\"WriteDirect failed, remainCap is larger than MallocLen\")\n\t}\n\t// if the []byte is empty, it will return an error\n\tif len(p) == 0 {\n\t\treturn errors.New(\"WriteDirect failed, []byte is empty\")\n\t}\n\n\t// splits the []byte in two parts: a []byte with the given remainCap, and the remaining []byte\n\t// if the given remainCap is 0, it will write the whole []byte without splitting\n\tif remainCap > 0 {\n\t\tif len(p) > remainCap {\n\t\t\tp0 := p[0:remainCap]\n\t\t\tp1 := p[remainCap:]\n\t\t\tif err := rw.writer.WriteBinary(p0); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif err := rw.writer.WriteString(string(p1)); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\tif err := rw.writer.WriteBinary(p); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif err := rw.writer.WriteString(string(p)); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n```"]}, "kitex-pkg/remote/trans/netpollmux/mux_transport.go-parseHeader": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/netpollmux/mux_transport.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage netpollmux\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\n\t\"github.com/cloudwego/netpoll\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/codec\"\n)\n\n// 0-4Byte length, 4-8Byte version check, 8-12Byte seqID\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func parseHeader(reader netpoll.Reader) (length int, seqID int32, err error) {\n# Natural Language Annotations: // 0-4Byte length, 4-8Byte version check, 8-12Byte seqID\nPlease return the generated code surrounded by ```\n", "response": ["  ```\npackage netpollmux\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\n\t\"github.com/cloudwego/netpoll\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/codec\"\n)\n\nfunc parseHeader(reader netpoll.Reader) (length int, seqID int32, err error) {\n\t// Read the length field (0-4Byte)\n\tlength, err = binary.ReadUvarint(reader)\n\tif err != nil {\n\t\treturn 0, 0, fmt.Errorf(\"failed to read length: %w\", err)\n\t}\n\n\t// Read the version check field (4-8Byte)\n\tversion, err := binary.ReadUvarint(reader)\n\tif err != nil {\n\t\treturn 0, 0, fmt.Errorf(\"failed to read version: %w\", err)\n\t}\n\tif version != 1 {\n\t\treturn 0, 0, fmt.Errorf(\"unsupported version: %d\", version)\n\t}\n\n\t// Read the seqID field (8-12Byte)\n\tseqID, err = binary.ReadVarint(reader)\n\tif err != nil {\n\t\treturn 0, 0, fmt.Errorf(\"failed to read seqID: %w\", err)\n\t}\n\n\treturn length, seqID, nil\n}\n```", "  ```\nfunc parseHeader(reader netpoll.Reader) (length int, seqID int32, err error) {\n\tvar buf [12]byte\n\t_, err = io.ReadFull(reader, buf[:])\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\t// 0-4Byte length\n\tlength = int(binary.LittleEndian.Uint32(buf[:4]))\n\n\t// 4-8Byte version check\n\tversion := binary.LittleEndian.Uint32(buf[4:8])\n\tif version != versionCheck {\n\t\treturn 0, 0, fmt.Errorf(\"unsupported version: %v\", version)\n\t}\n\n\t// 8-12Byte seqID\n\tseqID = int32(binary.LittleEndian.Uint32(buf[8:12]))\n\n\treturn length, seqID, nil\n}\n```", "  ```\nfunc parseHeader(reader netpoll.Reader) (length int, seqID int32, err error) {\n\tvar buffer [12]byte\n\tif _, err = reader.Read(buffer[:]); err != nil {\n\t\treturn 0, 0, err\n\t}\n\tlength = int(binary.BigEndian.Uint32(buffer[0:4]))\n\tseqID = int32(binary.BigEndian.Uint32(buffer[8:12]))\n\treturn length, seqID, nil\n}\n```\nThis function reads a header from the given reader, which is assumed to have a length of 12 bytes. It parses the header and returns the length of the encoded payload and the sequence ID of the message.\n\nThe function reads the first 12 bytes from the reader using the `Read` method, which returns the number of bytes read and an error. If an error occurs, the function returns 0 for the length, 0 for the sequence ID, and the error.\n\nOtherwise, the function uses the `binary.BigEndian.Uint32` function to extract the length of the payload and the sequence ID from the bytes read. The length is extracted from the first 4 bytes, and the sequence ID is extracted from the last 4 bytes.\n\nThe function returns the length and sequence ID, and a nil error."]}, "kitex-pkg/remote/trans/nphttp2/client_conn.go-fullMethodName": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/client_conn.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage nphttp2\n\nimport (\n\t\"context\"\n\t\"encoding/binary\"\n\t\"io\"\n\t\"net\"\n\t\"net/url\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/bytedance/gopkg/lang/dirtmake\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/codes\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n)\n\nconst (\n\tcontentSubTypeThrift   = \"thrift\"\n\tcontentSubTypeProtobuf = \"protobuf\"\n)\n\ntype streamDesc struct {\n\tisStreaming bool\n}\n\ntype clientConn struct {\n\ttr   grpc.ClientTransport\n\ts    *grpc.Stream\n\tdesc *streamDesc\n}\n\nvar _ GRPCConn = (*clientConn)(nil)\n\nfunc (c *clientConn) ReadFrame() (hdr, data []byte, err error) {\n\thdr = dirtmake.Bytes(5, 5)\n\t_, err = c.Read(hdr)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tdLen := int(binary.BigEndian.Uint32(hdr[1:]))\n\tdata = dirtmake.Bytes(dLen, dLen)\n\t_, err = c.Read(data)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn hdr, data, nil\n}\n\nfunc getContentSubType(codec serviceinfo.PayloadCodec) string {\n\tswitch codec {\n\tcase serviceinfo.Thrift:\n\t\treturn contentSubTypeThrift\n\tcase serviceinfo.Protobuf:\n\t\tfallthrough\n\tdefault:\n\t\treturn \"\" // default is protobuf, keep for backward compatibility\n\t}\n}\n\nfunc newClientConn(ctx context.Context, tr grpc.ClientTransport, addr string) (*clientConn, error) {\n\tri := rpcinfo.GetRPCInfo(ctx)\n\thost := ri.To().ServiceName()\n\tif rawURL, ok := ri.To().Tag(rpcinfo.HTTPURL); ok {\n\t\tu, err := url.Parse(rawURL)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\thost = u.Host\n\t}\n\tisStreaming := ri.Config().InteractionMode() == rpcinfo.Streaming\n\tinvocation := ri.Invocation()\n\tcallHdr := &grpc.CallHdr{\n\t\tHost: host,\n\t\t// grpc method format /package.Service/Method\n\t\tMethod:         fullMethodName(invocation.PackageName(), invocation.ServiceName(), invocation.MethodName()),\n\t\tSendCompress:   remote.GetSendCompressor(ri),\n\t\tContentSubtype: getContentSubType(ri.Config().PayloadCodec()),\n\t}\n\n\ts, err := tr.NewStream(ctx, callHdr)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &clientConn{\n\t\ttr:   tr,\n\t\ts:    s,\n\t\tdesc: &streamDesc{isStreaming: isStreaming},\n\t}, nil\n}\n\n// fullMethodName returns in the format of \"/[$pkg.]$svc/$methodName\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// impl net.Conn\nfunc (c *clientConn) Read(b []byte) (n int, err error) {\n\tn, err = c.s.Read(b)\n\tif err == io.EOF {\n\t\tif status := c.s.Status(); status.Code() != codes.OK {\n\t\t\tif bizStatusErr := c.s.BizStatusErr(); bizStatusErr != nil {\n\t\t\t\terr = bizStatusErr\n\t\t\t} else {\n\t\t\t\terr = status.Err()\n\t\t\t}\n\t\t}\n\t}\n\treturn n, err\n}\n\nfunc (c *clientConn) Write(b []byte) (n int, err error) {\n\tif len(b) < 5 {\n\t\treturn 0, io.ErrShortWrite\n\t}\n\treturn c.WriteFrame(b[:5], b[5:])\n}\n\nfunc (c *clientConn) WriteFrame(hdr, data []byte) (n int, err error) {\n\tgrpcConnOpt := &grpc.Options{Last: !c.desc.isStreaming}\n\terr = c.tr.Write(c.s, hdr, data, grpcConnOpt)\n\treturn len(hdr) + len(data), err\n}\n\nfunc (c *clientConn) LocalAddr() net.Addr                { return c.tr.LocalAddr() }\nfunc (c *clientConn) RemoteAddr() net.Addr               { return c.tr.RemoteAddr() }\nfunc (c *clientConn) SetDeadline(t time.Time) error      { return nil }\nfunc (c *clientConn) SetReadDeadline(t time.Time) error  { return nil }\nfunc (c *clientConn) SetWriteDeadline(t time.Time) error { return nil }\nfunc (c *clientConn) Close() error {\n\tc.tr.Write(c.s, nil, nil, &grpc.Options{Last: true})\n\t// Always return nil; io.EOF is the only error that might make sense\n\t// instead, but there is no need to signal the client to call Read\n\t// as the only use left for the stream after Close is to call\n\t// Read. This also matches historical behavior.\n\treturn nil\n}\n\nfunc (c *clientConn) Header() (metadata.MD, error) { return c.s.Header() }\nfunc (c *clientConn) Trailer() metadata.MD         { return c.s.Trailer() }\nfunc (c *clientConn) GetRecvCompress() string      { return c.s.RecvCompress() }\n\ntype hasGetRecvCompress interface {\n\tGetRecvCompress() string\n}\n\ntype hasHeader interface {\n\tHeader() (metadata.MD, error)\n}\n\ntype hasTrailer interface {\n\tTrailer() metadata.MD\n}\n\n```\n# Function Signature: func fullMethodName(pkg, svc, method string) string {\n# Natural Language Annotations: // fullMethodName returns in the format of \"/[$pkg.]$svc/$methodName\".\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc fullMethodName(pkg, svc, method string) string {\n    if pkg != \"\" {\n        return fmt.Sprintf(\"/%s.%s/%s\", pkg, svc, method)\n    }\n    return fmt.Sprintf(\"/%s/%s\", svc, method)\n}\n```", "  ```\nfunc fullMethodName(pkg, svc, method string) string {\n\tvar sb strings.Builder\n\tif pkg != \"\" {\n\t\tsb.WriteRune('[')\n\t\tsb.WriteString(pkg)\n\t\tsb.WriteRune(']')\n\t\tsb.WriteRune('.')\n\t}\n\tsb.WriteString(svc)\n\tsb.WriteRune('/')\n\tsb.WriteString(method)\n\treturn sb.String()\n}\n```", "  ```\nfunc fullMethodName(pkg, svc, method string) string {\n\treturn fmt.Sprintf(\"%s/%s%s\", strings.ReplaceAll(pkg, \".\", \"/\"), svc, strings.ReplaceAll(method, \".\", \"/\"))\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/server_handler.go-invokeStreamUnaryHandler": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/server_handler.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage nphttp2\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"runtime/debug\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/netpoll\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/gofunc\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/codes\"\n\tgrpcTransport \"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/status\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\ntype svrTransHandlerFactory struct{}\n\n// NewSvrTransHandlerFactory ...\nfunc NewSvrTransHandlerFactory() remote.ServerTransHandlerFactory {\n\treturn &svrTransHandlerFactory{}\n}\n\nfunc (f *svrTransHandlerFactory) NewTransHandler(opt *remote.ServerOption) (remote.ServerTransHandler, error) {\n\treturn newSvrTransHandler(opt)\n}\n\nfunc newSvrTransHandler(opt *remote.ServerOption) (*svrTransHandler, error) {\n\treturn &svrTransHandler{\n\t\topt:          opt,\n\t\tsvcSearchMap: opt.SvcSearchMap,\n\t\tcodec:        grpc.NewGRPCCodec(grpc.WithThriftCodec(opt.PayloadCodec)),\n\t}, nil\n}\n\nvar _ remote.ServerTransHandler = &svrTransHandler{}\n\ntype svrTransHandler struct {\n\topt          *remote.ServerOption\n\tsvcSearchMap map[string]*serviceinfo.ServiceInfo\n\tinkHdlFunc   endpoint.Endpoint\n\tcodec        remote.Codec\n}\n\nvar prefaceReadAtMost = func() int {\n\t// min(len(ClientPreface), len(flagBuf))\n\t// len(flagBuf) = 2 * codec.Size32\n\tif 2*codec.Size32 < grpcTransport.ClientPrefaceLen {\n\t\treturn 2 * codec.Size32\n\t}\n\treturn grpcTransport.ClientPrefaceLen\n}()\n\nfunc (t *svrTransHandler) ProtocolMatch(ctx context.Context, conn net.Conn) (err error) {\n\t// Check the validity of client preface.\n\tnpReader := conn.(interface{ Reader() netpoll.Reader }).Reader()\n\t// read at most avoid block\n\tpreface, err := npReader.Peek(prefaceReadAtMost)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif bytes.Equal(preface[:prefaceReadAtMost], grpcTransport.ClientPreface[:prefaceReadAtMost]) {\n\t\treturn nil\n\t}\n\treturn errors.New(\"error protocol not match\")\n}\n\nfunc (t *svrTransHandler) Write(ctx context.Context, conn net.Conn, msg remote.Message) (nctx context.Context, err error) {\n\tbuf := newBuffer(conn.(*serverConn))\n\tdefer buf.Release(err)\n\n\tif err = t.codec.Encode(ctx, msg, buf); err != nil {\n\t\treturn ctx, err\n\t}\n\treturn ctx, buf.Flush()\n}\n\nfunc (t *svrTransHandler) Read(ctx context.Context, conn net.Conn, msg remote.Message) (nctx context.Context, err error) {\n\tbuf := newBuffer(conn.(*serverConn))\n\tdefer buf.Release(err)\n\n\terr = t.codec.Decode(ctx, msg, buf)\n\treturn ctx, err\n}\n\n// \u53ea return write err\nfunc (t *svrTransHandler) OnRead(ctx context.Context, conn net.Conn) error {\n\tsvrTrans := ctx.Value(ctxKeySvrTransport).(*SvrTrans)\n\ttr := svrTrans.tr\n\n\ttr.HandleStreams(func(s *grpcTransport.Stream) {\n\t\tgofunc.GoFunc(ctx, func() {\n\t\t\tri := svrTrans.pool.Get().(rpcinfo.RPCInfo)\n\t\t\trCtx := rpcinfo.NewCtxWithRPCInfo(s.Context(), ri)\n\t\t\tdefer func() {\n\t\t\t\t// reset rpcinfo for performance (PR #584)\n\t\t\t\tif rpcinfo.PoolEnabled() {\n\t\t\t\t\tri = t.opt.InitOrResetRPCInfoFunc(ri, conn.RemoteAddr())\n\t\t\t\t\tsvrTrans.pool.Put(ri)\n\t\t\t\t}\n\t\t\t}()\n\n\t\t\tink := ri.Invocation().(rpcinfo.InvocationSetter)\n\t\t\tsm := s.Method()\n\t\t\tif sm != \"\" && sm[0] == '/' {\n\t\t\t\tsm = sm[1:]\n\t\t\t}\n\t\t\tpos := strings.LastIndex(sm, \"/\")\n\t\t\tif pos == -1 {\n\t\t\t\terrDesc := fmt.Sprintf(\"malformed method name, method=%q\", s.Method())\n\t\t\t\ttr.WriteStatus(s, status.New(codes.Internal, errDesc))\n\t\t\t\treturn\n\t\t\t}\n\t\t\tmethodName := sm[pos+1:]\n\t\t\tink.SetMethodName(methodName)\n\n\t\t\tif mutableTo := rpcinfo.AsMutableEndpointInfo(ri.To()); mutableTo != nil {\n\t\t\t\tif err := mutableTo.SetMethod(methodName); err != nil {\n\t\t\t\t\terrDesc := fmt.Sprintf(\"setMethod failed in streaming, method=%s, error=%s\", methodName, err.Error())\n\t\t\t\t\t_ = tr.WriteStatus(s, status.New(codes.Internal, errDesc))\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tvar serviceName string\n\t\t\tidx := strings.LastIndex(sm[:pos], \".\")\n\t\t\tif idx == -1 {\n\t\t\t\tink.SetPackageName(\"\")\n\t\t\t\tserviceName = sm[0:pos]\n\t\t\t} else {\n\t\t\t\tink.SetPackageName(sm[:idx])\n\t\t\t\tserviceName = sm[idx+1 : pos]\n\t\t\t}\n\t\t\tink.SetServiceName(serviceName)\n\n\t\t\t// set grpc transport flag before execute metahandler\n\t\t\trpcinfo.AsMutableRPCConfig(ri.Config()).SetTransportProtocol(transport.GRPC)\n\t\t\tvar err error\n\t\t\tfor _, shdlr := range t.opt.StreamingMetaHandlers {\n\t\t\t\trCtx, err = shdlr.OnReadStream(rCtx)\n\t\t\t\tif err != nil {\n\t\t\t\t\ttr.WriteStatus(s, convertStatus(err))\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\trCtx = t.startTracer(rCtx, ri)\n\t\t\tdefer func() {\n\t\t\t\tpanicErr := recover()\n\t\t\t\tif panicErr != nil {\n\t\t\t\t\tif conn != nil {\n\t\t\t\t\t\tklog.CtxErrorf(rCtx, \"KITEX: gRPC panic happened, close conn, remoteAddress=%s, error=%s\\nstack=%s\", conn.RemoteAddr(), panicErr, string(debug.Stack()))\n\t\t\t\t\t} else {\n\t\t\t\t\t\tklog.CtxErrorf(rCtx, \"KITEX: gRPC panic happened, error=%v\\nstack=%s\", panicErr, string(debug.Stack()))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tt.finishTracer(rCtx, ri, err, panicErr)\n\t\t\t}()\n\n\t\t\t// set recv grpc compressor at server to decode the pack from client\n\t\t\tremote.SetRecvCompressor(ri, s.RecvCompress())\n\t\t\t// set send grpc compressor at server to encode reply pack\n\t\t\tremote.SetSendCompressor(ri, s.SendCompress())\n\n\t\t\tsvcInfo := t.svcSearchMap[remote.BuildMultiServiceKey(serviceName, methodName)]\n\t\t\tvar methodInfo serviceinfo.MethodInfo\n\t\t\tif svcInfo != nil {\n\t\t\t\tmethodInfo = svcInfo.MethodInfo(methodName)\n\t\t\t}\n\n\t\t\trawStream := NewStream(rCtx, svcInfo, newServerConn(tr, s), t)\n\t\t\tst := newStreamWithMiddleware(rawStream, t.opt.RecvEndpoint, t.opt.SendEndpoint)\n\n\t\t\t// bind stream into ctx, in order to let user set header and trailer by provided api in meta_api.go\n\t\t\trCtx = streaming.NewCtxWithStream(rCtx, st)\n\n\t\t\tif methodInfo == nil {\n\t\t\t\tunknownServiceHandlerFunc := t.opt.GRPCUnknownServiceHandler\n\t\t\t\tif unknownServiceHandlerFunc != nil {\n\t\t\t\t\trpcinfo.Record(rCtx, ri, stats.ServerHandleStart, nil)\n\t\t\t\t\terr = unknownServiceHandlerFunc(rCtx, methodName, st)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\terr = kerrors.ErrBiz.WithCause(err)\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tif svcInfo == nil {\n\t\t\t\t\t\terr = remote.NewTransErrorWithMsg(remote.UnknownService, fmt.Sprintf(\"unknown service %s\", serviceName))\n\t\t\t\t\t} else {\n\t\t\t\t\t\terr = remote.NewTransErrorWithMsg(remote.UnknownMethod, fmt.Sprintf(\"unknown method %s\", methodName))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif streaming.UnaryCompatibleMiddleware(methodInfo.StreamingMode(), t.opt.CompatibleMiddlewareForUnary) {\n\t\t\t\t\t// making streaming unary APIs capable of using the same server middleware as non-streaming APIs\n\t\t\t\t\t// note: rawStream skips recv/send middleware for unary API requests to avoid confusion\n\t\t\t\t\terr = invokeStreamUnaryHandler(rCtx, rawStream, methodInfo, t.inkHdlFunc, ri)\n\t\t\t\t} else {\n\t\t\t\t\terr = t.inkHdlFunc(rCtx, &streaming.Args{Stream: st}, nil)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif err != nil {\n\t\t\t\ttr.WriteStatus(s, convertStatus(err))\n\t\t\t\tt.OnError(rCtx, err, conn)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif bizStatusErr := ri.Invocation().BizStatusErr(); bizStatusErr != nil {\n\t\t\t\tvar st *status.Status\n\t\t\t\tif sterr, ok := bizStatusErr.(status.Iface); ok {\n\t\t\t\t\tst = sterr.GRPCStatus()\n\t\t\t\t} else {\n\t\t\t\t\tst = status.New(codes.Internal, bizStatusErr.BizMessage())\n\t\t\t\t}\n\t\t\t\ts.SetBizStatusErr(bizStatusErr)\n\t\t\t\ttr.WriteStatus(s, st)\n\t\t\t\treturn\n\t\t\t}\n\t\t\ttr.WriteStatus(s, status.New(codes.OK, \"\"))\n\t\t})\n\t}, func(ctx context.Context, method string) context.Context {\n\t\treturn ctx\n\t})\n\treturn nil\n}\n\n// invokeStreamUnaryHandler allows unary APIs over HTTP2 to use the same server middleware as non-streaming APIs.\n// For thrift unary APIs over HTTP2, it's enabled by default.\n// For grpc(protobuf) unary APIs, it's disabled by default to keep backward compatibility.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// msg \u662f\u89e3\u7801\u540e\u7684\u5b9e\u4f8b\uff0c\u5982 Arg \u6216 Result, \u89e6\u53d1\u4e0a\u5c42\u5904\u7406\uff0c\u7528\u4e8e\u5f02\u6b65 \u548c \u670d\u52a1\u7aef\u5904\u7406\nfunc (t *svrTransHandler) OnMessage(ctx context.Context, args, result remote.Message) (context.Context, error) {\n\tpanic(\"unimplemented\")\n}\n\ntype svrTransKey int\n\nconst ctxKeySvrTransport svrTransKey = 1\n\ntype SvrTrans struct {\n\ttr   grpcTransport.ServerTransport\n\tpool *sync.Pool // value is rpcInfo\n}\n\n// \u65b0\u8fde\u63a5\u5efa\u7acb\u65f6\u89e6\u53d1\uff0c\u4e3b\u8981\u7528\u4e8e\u670d\u52a1\u7aef\uff0c\u5bf9\u5e94 netpoll onPrepare\nfunc (t *svrTransHandler) OnActive(ctx context.Context, conn net.Conn) (context.Context, error) {\n\t// set readTimeout to infinity to avoid streaming break\n\t// use keepalive to check the health of connection\n\tif npConn, ok := conn.(netpoll.Connection); ok {\n\t\tnpConn.SetReadTimeout(grpcTransport.Infinity)\n\t} else {\n\t\tconn.SetReadDeadline(time.Now().Add(grpcTransport.Infinity))\n\t}\n\n\ttr, err := grpcTransport.NewServerTransport(ctx, conn, t.opt.GRPCCfg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpool := &sync.Pool{\n\t\tNew: func() interface{} {\n\t\t\t// init rpcinfo\n\t\t\tri := t.opt.InitOrResetRPCInfoFunc(nil, conn.RemoteAddr())\n\t\t\treturn ri\n\t\t},\n\t}\n\tctx = context.WithValue(ctx, ctxKeySvrTransport, &SvrTrans{tr: tr, pool: pool})\n\treturn ctx, nil\n}\n\n// \u8fde\u63a5\u5173\u95ed\u65f6\u56de\u8c03\nfunc (t *svrTransHandler) OnInactive(ctx context.Context, conn net.Conn) {\n\ttr := ctx.Value(ctxKeySvrTransport).(*SvrTrans).tr\n\ttr.Close()\n}\n\n// \u4f20\u8f93\u5c42 error \u56de\u8c03\nfunc (t *svrTransHandler) OnError(ctx context.Context, err error, conn net.Conn) {\n\tvar de *kerrors.DetailedError\n\tif ok := errors.As(err, &de); ok && de.Stack() != \"\" {\n\t\tklog.CtxErrorf(ctx, \"KITEX: processing gRPC request error, remoteAddr=%s, error=%s\\nstack=%s\", conn.RemoteAddr(), err.Error(), de.Stack())\n\t} else {\n\t\tklog.CtxErrorf(ctx, \"KITEX: processing gRPC request error, remoteAddr=%s, error=%s\", conn.RemoteAddr(), err.Error())\n\t}\n}\n\nfunc (t *svrTransHandler) SetInvokeHandleFunc(inkHdlFunc endpoint.Endpoint) {\n\tt.inkHdlFunc = inkHdlFunc\n}\n\nfunc (t *svrTransHandler) SetPipeline(p *remote.TransPipeline) {\n}\n\nfunc (t *svrTransHandler) startTracer(ctx context.Context, ri rpcinfo.RPCInfo) context.Context {\n\tc := t.opt.TracerCtl.DoStart(ctx, ri)\n\treturn c\n}\n\nfunc (t *svrTransHandler) finishTracer(ctx context.Context, ri rpcinfo.RPCInfo, err error, panicErr interface{}) {\n\trpcStats := rpcinfo.AsMutableRPCStats(ri.Stats())\n\tif rpcStats == nil {\n\t\treturn\n\t}\n\tif panicErr != nil {\n\t\trpcStats.SetPanicked(panicErr)\n\t}\n\tt.opt.TracerCtl.DoFinish(ctx, ri, err)\n\trpcStats.Reset()\n}\n\n```\n# Function Signature: func invokeStreamUnaryHandler(ctx context.Context, st streaming.Stream, mi serviceinfo.MethodInfo,\n# Natural Language Annotations: // invokeStreamUnaryHandler allows unary APIs over HTTP2 to use the same server middleware as non-streaming APIs.\n// For thrift unary APIs over HTTP2, it's enabled by default.\n// For grpc(protobuf) unary APIs, it's disabled by default to keep backward compatibility.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc invokeStreamUnaryHandler(ctx context.Context, st streaming.Stream, mi serviceinfo.MethodInfo, inkHdlFunc endpoint.Endpoint) error {\n\treq, err := mi.Decode(ctx, st.Receiver())\n\tif err != nil {\n\t\treturn kerrors.ErrEncoding.WithCause(err)\n\t}\n\tresp, err := inkHdlFunc(ctx, st.Invoke(ctx, mi.Name(), req))\n\tif err != nil {\n\t\treturn kerrors.ErrBiz.WithCause(err)\n\t}\n\tif err := mi.Encode(ctx, resp, st.Sender()); err != nil {\n\t\treturn kerrors.ErrEncoding.WithCause(err)\n\t}\n\treturn nil\n}\n```", "  ```\nfunc invokeStreamUnaryHandler(ctx context.Context, st streaming.Stream, mi serviceinfo.MethodInfo, inkHdlFunc endpoint.Endpoint, ri rpcinfo.RPCInfo) error {\n\trawStream := NewStream(ctx, st.ServiceInfo(), st.ServerConn(), nil)\n\tserverStream := newServerStreamWithMiddleware(rawStream, t.opt.RecvEndpoint, t.opt.SendEndpoint)\n\n\t// bind stream into ctx, in order to let user set header and trailer by provided api in meta_api.go\n\tctx = streaming.NewCtxWithStream(ctx, serverStream)\n\n\t// set recv grpc compressor at server to decode the pack from client\n\tremote.SetRecvCompressor(ri, st.RecvCompress())\n\t// set send grpc compressor at server to encode reply pack\n\tremote.SetSendCompressor(ri, st.SendCompress())\n\n\t// invoke user handler\n\tmethodName := mi.Name()\n\tif strings.Contains(methodName, \"/\") {\n\t\terr := remote.NewTransErrorWithMsg(remote.UnknownMethod, fmt.Sprintf(\"unknown method %s\", methodName))\n\t\treturn err\n\t}\n\terr := inkHdlFunc(ctx, &streaming.Args{Stream: serverStream}, nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif bizStatusErr := ri.Invocation().BizStatusErr(); bizStatusErr != nil {\n\t\tvar st *status.Status\n\t\tif sterr, ok := bizStatusErr.(status.Iface); ok {\n\t\t\tst = sterr.GRPCStatus()\n\t\t} else {\n\t\t\tst = status.New(codes.Internal, bizStatusErr.BizMessage())\n\t\t}\n\t\tst.(*streaming.Args).Stream.SetBizStatusErr(bizStatusErr)\n\t\tserverStream.SetStatus(st)\n\t\treturn nil\n\t}\n\treturn nil\n} \n```", "  ```\nfunc invokeStreamUnaryHandler(ctx context.Context, st streaming.Stream, mi serviceinfo.MethodInfo, inkHdlFunc endpoint.Endpoint, ri rpcinfo.RPCInfo) error {\n\trpcStats := rpcinfo.AsMutableRPCStats(ri.Stats())\n\tif rpcStats == nil {\n\t\treturn remote.NewTransErrorWithMsg(remote.UnknownMethod, fmt.Sprintf(\"can't find invoker for method %s\", mi.Info().Name()))\n\t}\n\trpcStats.SetProtocol(transport.GRPC)\n\trpcStats.SetStreaming(streaming.IsStreamingInvocation(ri.Invocation()))\n\trpcinfo.Record(ctx, ri, stats.InvocationStart)\n\n\t// send initial metadata\n\tif md := mi.Metadata(); md != nil {\n\t\tmdMap := make(map[string]*structpb.Struct)\n\t\tfor _, mdConf := range md {\n\t\t\tmdMap[mdConf.Header] = mdConf.Content\n\t\t}\n\t\tst.SetInitialMetadata(mdMap)\n\t}\n\n\t// invoke stream unary handler\n\tvar err error\n\targ := mi.Arg\n\tresult := mi.Result\n\tif arg != nil {\n\t\targ.Reset(st.Request())\n\t}\n\tif result != nil {\n\t\tresult.Reset(st.Response())\n\t}\n\tfunc() {\n\t\tvar conn net.Conn\n\t\tif tr, ok := st.Conn().(*transport.Transport); ok {\n\t\t\tconn = tr.NetConn()\n\t\t}\n\t\terr = inkHdlFunc(ctx, &streaming.Args{Stream: st}, &streaming.Result{Stream: st})\n\t\tif err != nil && rs, ok := st.(remote.ScopedStream); ok {\n\t\t\tswitch err.(type) {\n\t\t\tcase frameDecoderError:\n\t\t\t\tsDesc := fmt.Sprintf(\n\t\t\t\t\t\"while decoding %s content of %s method, error happened, egress request can be cancelled.\",\n\t\t\t\t\targsOrResultDescribe(st.Request(), arg),\n\t\t\t\t\tmethodInfoName(mi.Info()),\n\t\t\t\t)\n\t\t\t\trs.Freeze(fmt.Errorf(\"%w: %s\", remote.ErrScopedStreamIncompatibleWithGRPC, sDesc))\n\t\t\tcase funcRespDecodeError:\n\t\t\t\tsDesc := fmt.Sprintf(\n\t\t\t\t\t\"while decoding %s content of %s method, error happened, egress request can be cancelled.\",\n\t\t\t\t\targsOrResultDescribe(st.Response(), result),\n\t\t\t\t\tmethodInfoName(mi.Info()),\n\t\t\t\t)\n\t\t\t\trs.Freeze(fmt.Errorf(\"%w: %s\", remote.ErrScopedStreamIncompatibleWithGRPC, sDesc))\n\t\t\t}\n\t\t}\n\t}()\n\n\tif err != nil {\n\t\trpcinfo.Record(ctx, ri, stats.InvocationErr)\n\t\treturn err\n\t}\n\n\t// send trailing metadata\n\tif tm := mi.Trailer; tm != nil {\n\t\ttmMap := make(map[string]string)\n\t\tfor _, tmConf := range tm {\n\t\t\ttmMap[tmConf.Header] = tmConf.Value\n\t\t}\n\t\tst.AddTrailingMetadata(tmMap)\n\t}\n\n\treturn nil\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/metadata/metadata.go-FromOutgoingContext": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/metadata/metadata.go:\n```\n/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\n// Package metadata define the structure of the metadata supported by gRPC library.\n// Please refer to https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md\n// for more information about custom-metadata.\npackage metadata\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n)\n\n// DecodeKeyValue returns k, v, nil.\n//\n// Deprecated: use k and v directly instead.\nfunc DecodeKeyValue(k, v string) (string, string, error) {\n\treturn k, v, nil\n}\n\n// MD is a mapping from metadata keys to values. Users should use the following\n// two convenience functions New and Pairs to generate MD.\ntype MD map[string][]string\n\n// New creates an MD from a given key-value map.\n//\n// Only the following ASCII characters are allowed in keys:\n//   - digits: 0-9\n//   - uppercase letters: A-Z (normalized to lower)\n//   - lowercase letters: a-z\n//   - special characters: -_.\n//\n// Uppercase letters are automatically converted to lowercase.\n//\n// Keys beginning with \"grpc-\" are reserved for grpc-internal use only and may\n// result in errors if set in metadata.\nfunc New(m map[string]string) MD {\n\tmd := MD{}\n\tfor k, val := range m {\n\t\tkey := strings.ToLower(k)\n\t\tmd[key] = append(md[key], val)\n\t}\n\treturn md\n}\n\n// Pairs returns an MD formed by the mapping of key, value ...\n// Pairs panics if len(kv) is odd.\n//\n// Only the following ASCII characters are allowed in keys:\n//   - digits: 0-9\n//   - uppercase letters: A-Z (normalized to lower)\n//   - lowercase letters: a-z\n//   - special characters: -_.\n//\n// Uppercase letters are automatically converted to lowercase.\n//\n// Keys beginning with \"grpc-\" are reserved for grpc-internal use only and may\n// result in errors if set in metadata.\nfunc Pairs(kv ...string) MD {\n\tif len(kv)%2 == 1 {\n\t\tpanic(fmt.Sprintf(\"metadata: Pairs got the odd number of input pairs for metadata: %d\", len(kv)))\n\t}\n\tmd := MD{}\n\tvar key string\n\tfor i, s := range kv {\n\t\tif i%2 == 0 {\n\t\t\tkey = strings.ToLower(s)\n\t\t\tcontinue\n\t\t}\n\t\tmd[key] = append(md[key], s)\n\t}\n\treturn md\n}\n\n// Len returns the number of items in md.\nfunc (md MD) Len() int {\n\treturn len(md)\n}\n\n// Copy returns a copy of md.\nfunc (md MD) Copy() MD {\n\tresult := make(MD, len(md))\n\tfor k, v := range md {\n\t\tvalues := make([]string, len(v))\n\t\tcopy(values, v)\n\t\tresult[k] = values\n\t}\n\treturn result\n}\n\n// Get obtains the values for a given key.\nfunc (md MD) Get(k string) []string {\n\tk = strings.ToLower(k)\n\treturn md[k]\n}\n\n// Set sets the value of a given key with a slice of values.\nfunc (md MD) Set(k string, vals ...string) {\n\tif len(vals) == 0 {\n\t\treturn\n\t}\n\tk = strings.ToLower(k)\n\tmd[k] = vals\n}\n\n// Append adds the values to key k, not overwriting what was already stored at that key.\nfunc (md MD) Append(k string, vals ...string) {\n\tif len(vals) == 0 {\n\t\treturn\n\t}\n\tk = strings.ToLower(k)\n\tmd[k] = append(md[k], vals...)\n}\n\n// Join joins any number of mds into a single MD.\n// The order of values for each key is determined by the order in which\n// the mds containing those values are presented to Join.\nfunc Join(mds ...MD) MD {\n\tn := 0\n\tfor _, md := range mds {\n\t\tn += len(md)\n\t}\n\tout := make(MD, n)\n\tfor _, md := range mds {\n\t\tfor k, v := range md {\n\t\t\tout[k] = append(out[k], v...)\n\t\t}\n\t}\n\treturn out\n}\n\n// AppendMD appends other into md, merging values of the same key.\nfunc AppendMD(md, other MD) MD {\n\tif md == nil {\n\t\tmd = make(MD, len(other))\n\t}\n\tfor k, v := range other {\n\t\tmd[k] = append(md[k], v...)\n\t}\n\treturn md\n}\n\ntype (\n\tmdIncomingKey struct{}\n\tmdOutgoingKey struct{}\n)\n\n// NewIncomingContext creates a new context with incoming md attached.\nfunc NewIncomingContext(ctx context.Context, md MD) context.Context {\n\treturn context.WithValue(ctx, mdIncomingKey{}, md)\n}\n\n// NewOutgoingContext creates a new context with outgoing md attached. If used\n// in conjunction with AppendToOutgoingContext, NewOutgoingContext will\n// overwrite any previously-appended metadata.\nfunc NewOutgoingContext(ctx context.Context, md MD) context.Context {\n\treturn context.WithValue(ctx, mdOutgoingKey{}, rawMD{md: md})\n}\n\n// AppendToOutgoingContext returns a new context with the provided kv merged\n// with any existing metadata in the context. Please refer to the\n// documentation of Pairs for a description of kv.\nfunc AppendToOutgoingContext(ctx context.Context, kv ...string) context.Context {\n\tif len(kv)%2 == 1 {\n\t\tpanic(fmt.Sprintf(\"metadata: AppendToOutgoingContext got an odd number of input pairs for metadata: %d\", len(kv)))\n\t}\n\tmd, _ := ctx.Value(mdOutgoingKey{}).(rawMD)\n\tadded := make([][]string, len(md.added)+1)\n\tcopy(added, md.added)\n\tadded[len(added)-1] = make([]string, len(kv))\n\tcopy(added[len(added)-1], kv)\n\treturn context.WithValue(ctx, mdOutgoingKey{}, rawMD{md: md.md, added: added})\n}\n\n// FromIncomingContext returns the incoming metadata in ctx if it exists.  The\n// returned MD should not be modified. Writing to it may cause races.\n// Modification should be made to copies of the returned MD.\nfunc FromIncomingContext(ctx context.Context) (md MD, ok bool) {\n\tmd, ok = ctx.Value(mdIncomingKey{}).(MD)\n\treturn\n}\n\n// FromOutgoingContextRaw returns the un-merged, intermediary contents\n// of rawMD. Remember to perform strings.ToLower on the keys. The returned\n// MD should not be modified. Writing to it may cause races. Modification\n// should be made to copies of the returned MD.\n//\n// This is intended for gRPC-internal use ONLY.\nfunc FromOutgoingContextRaw(ctx context.Context) (MD, [][]string, bool) {\n\traw, ok := ctx.Value(mdOutgoingKey{}).(rawMD)\n\tif !ok {\n\t\treturn nil, nil, false\n\t}\n\n\treturn raw.md, raw.added, true\n}\n\n// FromOutgoingContext returns the outgoing metadata in ctx if it exists.  The\n// returned MD should not be modified. Writing to it may cause races.\n// Modification should be made to copies of the returned MD.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype rawMD struct {\n\tmd    MD\n\tadded [][]string\n}\n\n```\n# Function Signature: func FromOutgoingContext(ctx context.Context) (MD, bool) {\n# Natural Language Annotations: // FromOutgoingContext returns the outgoing metadata in ctx if it exists.  The\n// returned MD should not be modified. Writing to it may cause races.\n// Modification should be made to copies of the returned MD.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc FromOutgoingContext(ctx context.Context) (MD, bool) {\n    raw, ok := ctx.Value(mdOutgoingKey{}).(rawMD)\n    if !ok {\n        return nil, false\n    }\n    return raw.md, true\n}\n```", "  ```\nfunc FromOutgoingContext(ctx context.Context) (MD, bool) {\n    raw, ok := ctx.Value(mdOutgoingKey{}).(rawMD)\n    if !ok {\n        return nil, false\n    }\n    return raw.md, true\n}\n```", "  ```\nfunc FromOutgoingContext(ctx context.Context) (MD, bool) {\n\treturn FromOutgoingContextRaw(ctx)\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/status/status.go-WithDetails": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/status/status.go:\n```\n/*\n *\n * Copyright 2020 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\n// Package status implements errors returned by gRPC. These errors are\n// serialized and transmitted on the wire between server and client, and allow\n// for additional data to be transmitted via the Details field in the status\n// proto. gRPC service handlers should return an error created by this\n// package, and gRPC clients should expect a corresponding error to be\n// returned from the RPC call.\n//\n// This package upholds the invariants that a non-nil error may not\n// contain an OK code, and an OK code must result in a nil error.\npackage status\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\tspb \"google.golang.org/genproto/googleapis/rpc/status\"\n\t\"google.golang.org/protobuf/proto\"\n\t\"google.golang.org/protobuf/types/known/anypb\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/codes\"\n)\n\ntype Iface interface {\n\tGRPCStatus() *Status\n}\n\n// Status represents an RPC status code, message, and details.  It is immutable\n// and should be created with New, Newf, or FromProto.\ntype Status struct {\n\ts *spb.Status\n}\n\n// New returns a Status representing c and msg.\nfunc New(c codes.Code, msg string) *Status {\n\treturn &Status{s: &spb.Status{Code: int32(c), Message: msg}}\n}\n\n// Newf returns New(c, fmt.Sprintf(format, a...)).\nfunc Newf(c codes.Code, format string, a ...interface{}) *Status {\n\treturn New(c, fmt.Sprintf(format, a...))\n}\n\n// ErrorProto returns an error representing s.  If s.Code is OK, returns nil.\nfunc ErrorProto(s *spb.Status) error {\n\treturn FromProto(s).Err()\n}\n\n// FromProto returns a Status representing s.\nfunc FromProto(s *spb.Status) *Status {\n\treturn &Status{s: proto.Clone(s).(*spb.Status)}\n}\n\n// Err returns an error representing c and msg.  If c is OK, returns nil.\nfunc Err(c codes.Code, msg string) error {\n\treturn New(c, msg).Err()\n}\n\n// Errorf returns Error(c, fmt.Sprintf(format, a...)).\nfunc Errorf(c codes.Code, format string, a ...interface{}) error {\n\treturn Err(c, fmt.Sprintf(format, a...))\n}\n\n// Code returns the status code contained in s.\nfunc (s *Status) Code() codes.Code {\n\tif s == nil || s.s == nil {\n\t\treturn codes.OK\n\t}\n\treturn codes.Code(s.s.Code)\n}\n\n// Message returns the message contained in s.\nfunc (s *Status) Message() string {\n\tif s == nil || s.s == nil {\n\t\treturn \"\"\n\t}\n\treturn s.s.Message\n}\n\n// AppendMessage append extra msg for Status\nfunc (s *Status) AppendMessage(extraMsg string) *Status {\n\tif s == nil || s.s == nil || extraMsg == \"\" {\n\t\treturn s\n\t}\n\ts.s.Message = fmt.Sprintf(\"%s %s\", s.s.Message, extraMsg)\n\treturn s\n}\n\n// Proto returns s's status as an spb.Status proto message.\nfunc (s *Status) Proto() *spb.Status {\n\tif s == nil {\n\t\treturn nil\n\t}\n\treturn proto.Clone(s.s).(*spb.Status)\n}\n\n// Err returns an immutable error representing s; returns nil if s.Code() is OK.\nfunc (s *Status) Err() error {\n\tif s.Code() == codes.OK {\n\t\treturn nil\n\t}\n\treturn &Error{e: s.Proto()}\n}\n\n// WithDetails returns a new status with the provided details messages appended to the status.\n// If any errors are encountered, it returns nil and the first error encountered.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Details returns a slice of details messages attached to the status.\n// If a detail cannot be decoded, the error is returned in place of the detail.\nfunc (s *Status) Details() []interface{} {\n\tif s == nil || s.s == nil {\n\t\treturn nil\n\t}\n\tdetails := make([]interface{}, 0, len(s.s.Details))\n\tfor _, any := range s.s.Details {\n\t\tdetail, err := any.UnmarshalNew()\n\t\tif err != nil {\n\t\t\tdetails = append(details, err)\n\t\t\tcontinue\n\t\t}\n\t\tdetails = append(details, detail)\n\t}\n\treturn details\n}\n\n// Error wraps a pointer of a status proto. It implements error and Status,\n// and a nil *Error should never be returned by this package.\ntype Error struct {\n\te *spb.Status\n}\n\nfunc (e *Error) Error() string {\n\treturn fmt.Sprintf(\"rpc error: code = %d desc = %s\", codes.Code(e.e.GetCode()), e.e.GetMessage())\n}\n\n// GRPCStatus returns the Status represented by se.\nfunc (e *Error) GRPCStatus() *Status {\n\treturn FromProto(e.e)\n}\n\n// Is implements future error.Is functionality.\n// A Error is equivalent if the code and message are identical.\nfunc (e *Error) Is(target error) bool {\n\ttse, ok := target.(*Error)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn proto.Equal(e.e, tse.e)\n}\n\n// FromError returns a Status representing err if it was produced from this\n// package or has a method `GRPCStatus() *Status`. Otherwise, ok is false and a\n// Status is returned with codes.Unknown and the original error message.\n\n\n\n\n\n\n\n\n\n\n\n// Convert is a convenience function which removes the need to handle the\n// boolean return value from FromError.\nfunc Convert(err error) *Status {\n\ts, _ := FromError(err)\n\treturn s\n}\n\n// Code returns the Code of the error if it is a Status error, codes.OK if err\n// is nil, or codes.Unknown otherwise.\nfunc Code(err error) codes.Code {\n\t// Don't use FromError to avoid allocation of OK status.\n\tif err == nil {\n\t\treturn codes.OK\n\t}\n\tvar se Iface\n\tif errors.As(err, &se) {\n\t\treturn se.GRPCStatus().Code()\n\t}\n\treturn codes.Unknown\n}\n\n// FromContextError converts a context error into a Status.  It returns a\n// Status with codes.OK if err is nil, or a Status with codes.Unknown if err is\n// non-nil and not a context error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (s *Status) WithDetails(details ...proto.Message) (*Status, error) {\n# Natural Language Annotations: // WithDetails returns a new status with the provided details messages appended to the status.\n// If any errors are encountered, it returns nil and the first error encountered.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (s *Status) WithDetails(details ...proto.Message) (*Status, error) {\n\tif s == nil {\n\t\treturn nil, errors.New(\"status is nil\")\n\t}\n\tsCopy := proto.Clone(s.s).(*spb.Status)\n\tfor _, detail := range details {\n\t\tif detail == nil {\n\t\t\treturn nil, errors.New(\"detail is nil\")\n\t\t}\n\t\tany, err := anypb.New(detail)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tsCopy.Details = append(sCopy.Details, any)\n\t}\n\treturn &Status{s: sCopy}, nil\n}\n```", "  ```\nfunc (s *Status) WithDetails(details ...proto.Message) (*Status, error) {\n\tif s == nil {\n\t\treturn nil, fmt.Errorf(\"cannot attach details to nil status\")\n\t}\n\ts.s.Details = make([]*anypb.Any, len(details))\n\tfor i, detail := range details {\n\t\tany, err := anypb.New(&detail)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"error marshaling detail: %v\", err)\n\t\t}\n\t\ts.s.Details[i] = any\n\t}\n\treturn s, nil\n}\n```", "  ```\nfunc (s *Status) WithDetails(details ...proto.Message) (*Status, error) {\n\tif s == nil || s.s == nil {\n\t\treturn nil, fmt.Errorf(\"status is invalid\")\n\t}\n\tif len(details) == 0 {\n\t\treturn s, nil\n\t}\n\tnewDetails := make([]*anypb.Any, len(details))\n\tfor i, d := range details {\n\t\ta, err := anypb.New(d)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tnewDetails[i] = a\n\t}\n\tnewStatus := proto.Clone(s.s).(*spb.Status)\n\tnewStatus.Details = append(newStatus.Details, newDetails...)\n\treturn &Status{s: newStatus}, nil\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/status/status.go-FromError": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/status/status.go:\n```\n/*\n *\n * Copyright 2020 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\n// Package status implements errors returned by gRPC. These errors are\n// serialized and transmitted on the wire between server and client, and allow\n// for additional data to be transmitted via the Details field in the status\n// proto. gRPC service handlers should return an error created by this\n// package, and gRPC clients should expect a corresponding error to be\n// returned from the RPC call.\n//\n// This package upholds the invariants that a non-nil error may not\n// contain an OK code, and an OK code must result in a nil error.\npackage status\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\tspb \"google.golang.org/genproto/googleapis/rpc/status\"\n\t\"google.golang.org/protobuf/proto\"\n\t\"google.golang.org/protobuf/types/known/anypb\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/codes\"\n)\n\ntype Iface interface {\n\tGRPCStatus() *Status\n}\n\n// Status represents an RPC status code, message, and details.  It is immutable\n// and should be created with New, Newf, or FromProto.\ntype Status struct {\n\ts *spb.Status\n}\n\n// New returns a Status representing c and msg.\nfunc New(c codes.Code, msg string) *Status {\n\treturn &Status{s: &spb.Status{Code: int32(c), Message: msg}}\n}\n\n// Newf returns New(c, fmt.Sprintf(format, a...)).\nfunc Newf(c codes.Code, format string, a ...interface{}) *Status {\n\treturn New(c, fmt.Sprintf(format, a...))\n}\n\n// ErrorProto returns an error representing s.  If s.Code is OK, returns nil.\nfunc ErrorProto(s *spb.Status) error {\n\treturn FromProto(s).Err()\n}\n\n// FromProto returns a Status representing s.\nfunc FromProto(s *spb.Status) *Status {\n\treturn &Status{s: proto.Clone(s).(*spb.Status)}\n}\n\n// Err returns an error representing c and msg.  If c is OK, returns nil.\nfunc Err(c codes.Code, msg string) error {\n\treturn New(c, msg).Err()\n}\n\n// Errorf returns Error(c, fmt.Sprintf(format, a...)).\nfunc Errorf(c codes.Code, format string, a ...interface{}) error {\n\treturn Err(c, fmt.Sprintf(format, a...))\n}\n\n// Code returns the status code contained in s.\nfunc (s *Status) Code() codes.Code {\n\tif s == nil || s.s == nil {\n\t\treturn codes.OK\n\t}\n\treturn codes.Code(s.s.Code)\n}\n\n// Message returns the message contained in s.\nfunc (s *Status) Message() string {\n\tif s == nil || s.s == nil {\n\t\treturn \"\"\n\t}\n\treturn s.s.Message\n}\n\n// AppendMessage append extra msg for Status\nfunc (s *Status) AppendMessage(extraMsg string) *Status {\n\tif s == nil || s.s == nil || extraMsg == \"\" {\n\t\treturn s\n\t}\n\ts.s.Message = fmt.Sprintf(\"%s %s\", s.s.Message, extraMsg)\n\treturn s\n}\n\n// Proto returns s's status as an spb.Status proto message.\nfunc (s *Status) Proto() *spb.Status {\n\tif s == nil {\n\t\treturn nil\n\t}\n\treturn proto.Clone(s.s).(*spb.Status)\n}\n\n// Err returns an immutable error representing s; returns nil if s.Code() is OK.\nfunc (s *Status) Err() error {\n\tif s.Code() == codes.OK {\n\t\treturn nil\n\t}\n\treturn &Error{e: s.Proto()}\n}\n\n// WithDetails returns a new status with the provided details messages appended to the status.\n// If any errors are encountered, it returns nil and the first error encountered.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Details returns a slice of details messages attached to the status.\n// If a detail cannot be decoded, the error is returned in place of the detail.\nfunc (s *Status) Details() []interface{} {\n\tif s == nil || s.s == nil {\n\t\treturn nil\n\t}\n\tdetails := make([]interface{}, 0, len(s.s.Details))\n\tfor _, any := range s.s.Details {\n\t\tdetail, err := any.UnmarshalNew()\n\t\tif err != nil {\n\t\t\tdetails = append(details, err)\n\t\t\tcontinue\n\t\t}\n\t\tdetails = append(details, detail)\n\t}\n\treturn details\n}\n\n// Error wraps a pointer of a status proto. It implements error and Status,\n// and a nil *Error should never be returned by this package.\ntype Error struct {\n\te *spb.Status\n}\n\nfunc (e *Error) Error() string {\n\treturn fmt.Sprintf(\"rpc error: code = %d desc = %s\", codes.Code(e.e.GetCode()), e.e.GetMessage())\n}\n\n// GRPCStatus returns the Status represented by se.\nfunc (e *Error) GRPCStatus() *Status {\n\treturn FromProto(e.e)\n}\n\n// Is implements future error.Is functionality.\n// A Error is equivalent if the code and message are identical.\nfunc (e *Error) Is(target error) bool {\n\ttse, ok := target.(*Error)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn proto.Equal(e.e, tse.e)\n}\n\n// FromError returns a Status representing err if it was produced from this\n// package or has a method `GRPCStatus() *Status`. Otherwise, ok is false and a\n// Status is returned with codes.Unknown and the original error message.\n\n\n\n\n\n\n\n\n\n\n\n// Convert is a convenience function which removes the need to handle the\n// boolean return value from FromError.\nfunc Convert(err error) *Status {\n\ts, _ := FromError(err)\n\treturn s\n}\n\n// Code returns the Code of the error if it is a Status error, codes.OK if err\n// is nil, or codes.Unknown otherwise.\nfunc Code(err error) codes.Code {\n\t// Don't use FromError to avoid allocation of OK status.\n\tif err == nil {\n\t\treturn codes.OK\n\t}\n\tvar se Iface\n\tif errors.As(err, &se) {\n\t\treturn se.GRPCStatus().Code()\n\t}\n\treturn codes.Unknown\n}\n\n// FromContextError converts a context error into a Status.  It returns a\n// Status with codes.OK if err is nil, or a Status with codes.Unknown if err is\n// non-nil and not a context error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func FromError(err error) (s *Status, ok bool) {\n# Natural Language Annotations: // FromError returns a Status representing err if it was produced from this\n// package or has a method `GRPCStatus() *Status`. Otherwise, ok is false and a\n// Status is returned with codes.Unknown and the original error message.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc FromError(err error) (s *Status, ok bool) {\n\tif err == nil {\n\t\treturn New(codes.OK, \"\"), true\n\t}\n\tvar se Iface\n\tif errors.As(err, &se) {\n\t\treturn se.GRPCStatus(), true\n\t}\n\treturn New(codes.Unknown, err.Error()), false\n}\n```", "  ```\nfunc FromError(err error) (s *Status, ok bool) {\n    if err == nil {\n        return nil, true\n    }\n    var se Iface\n    if errors.As(err, &se) {\n        return se.GRPCStatus(), true\n    }\n    return &Status{s: &spb.Status{Code: int32(codes.Unknown), Message: err.Error()}}, false\n}\n```", "  ```\nfunc FromError(err error) (s *Status, ok bool) {\n    // Don't use FromError to avoid allocation of OK status.\n    if err == nil {\n        return &Status{s: nil}, true\n    }\n    var se Iface\n    if errors.As(err, &se) {\n        return se.GRPCStatus(), true\n    }\n    return &Status{s: &spb.Status{Code: int32(codes.Unknown), Message: err.Error()}}, false\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/status/status.go-FromContextError": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/status/status.go:\n```\n/*\n *\n * Copyright 2020 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\n// Package status implements errors returned by gRPC. These errors are\n// serialized and transmitted on the wire between server and client, and allow\n// for additional data to be transmitted via the Details field in the status\n// proto. gRPC service handlers should return an error created by this\n// package, and gRPC clients should expect a corresponding error to be\n// returned from the RPC call.\n//\n// This package upholds the invariants that a non-nil error may not\n// contain an OK code, and an OK code must result in a nil error.\npackage status\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\tspb \"google.golang.org/genproto/googleapis/rpc/status\"\n\t\"google.golang.org/protobuf/proto\"\n\t\"google.golang.org/protobuf/types/known/anypb\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/codes\"\n)\n\ntype Iface interface {\n\tGRPCStatus() *Status\n}\n\n// Status represents an RPC status code, message, and details.  It is immutable\n// and should be created with New, Newf, or FromProto.\ntype Status struct {\n\ts *spb.Status\n}\n\n// New returns a Status representing c and msg.\nfunc New(c codes.Code, msg string) *Status {\n\treturn &Status{s: &spb.Status{Code: int32(c), Message: msg}}\n}\n\n// Newf returns New(c, fmt.Sprintf(format, a...)).\nfunc Newf(c codes.Code, format string, a ...interface{}) *Status {\n\treturn New(c, fmt.Sprintf(format, a...))\n}\n\n// ErrorProto returns an error representing s.  If s.Code is OK, returns nil.\nfunc ErrorProto(s *spb.Status) error {\n\treturn FromProto(s).Err()\n}\n\n// FromProto returns a Status representing s.\nfunc FromProto(s *spb.Status) *Status {\n\treturn &Status{s: proto.Clone(s).(*spb.Status)}\n}\n\n// Err returns an error representing c and msg.  If c is OK, returns nil.\nfunc Err(c codes.Code, msg string) error {\n\treturn New(c, msg).Err()\n}\n\n// Errorf returns Error(c, fmt.Sprintf(format, a...)).\nfunc Errorf(c codes.Code, format string, a ...interface{}) error {\n\treturn Err(c, fmt.Sprintf(format, a...))\n}\n\n// Code returns the status code contained in s.\nfunc (s *Status) Code() codes.Code {\n\tif s == nil || s.s == nil {\n\t\treturn codes.OK\n\t}\n\treturn codes.Code(s.s.Code)\n}\n\n// Message returns the message contained in s.\nfunc (s *Status) Message() string {\n\tif s == nil || s.s == nil {\n\t\treturn \"\"\n\t}\n\treturn s.s.Message\n}\n\n// AppendMessage append extra msg for Status\nfunc (s *Status) AppendMessage(extraMsg string) *Status {\n\tif s == nil || s.s == nil || extraMsg == \"\" {\n\t\treturn s\n\t}\n\ts.s.Message = fmt.Sprintf(\"%s %s\", s.s.Message, extraMsg)\n\treturn s\n}\n\n// Proto returns s's status as an spb.Status proto message.\nfunc (s *Status) Proto() *spb.Status {\n\tif s == nil {\n\t\treturn nil\n\t}\n\treturn proto.Clone(s.s).(*spb.Status)\n}\n\n// Err returns an immutable error representing s; returns nil if s.Code() is OK.\nfunc (s *Status) Err() error {\n\tif s.Code() == codes.OK {\n\t\treturn nil\n\t}\n\treturn &Error{e: s.Proto()}\n}\n\n// WithDetails returns a new status with the provided details messages appended to the status.\n// If any errors are encountered, it returns nil and the first error encountered.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Details returns a slice of details messages attached to the status.\n// If a detail cannot be decoded, the error is returned in place of the detail.\nfunc (s *Status) Details() []interface{} {\n\tif s == nil || s.s == nil {\n\t\treturn nil\n\t}\n\tdetails := make([]interface{}, 0, len(s.s.Details))\n\tfor _, any := range s.s.Details {\n\t\tdetail, err := any.UnmarshalNew()\n\t\tif err != nil {\n\t\t\tdetails = append(details, err)\n\t\t\tcontinue\n\t\t}\n\t\tdetails = append(details, detail)\n\t}\n\treturn details\n}\n\n// Error wraps a pointer of a status proto. It implements error and Status,\n// and a nil *Error should never be returned by this package.\ntype Error struct {\n\te *spb.Status\n}\n\nfunc (e *Error) Error() string {\n\treturn fmt.Sprintf(\"rpc error: code = %d desc = %s\", codes.Code(e.e.GetCode()), e.e.GetMessage())\n}\n\n// GRPCStatus returns the Status represented by se.\nfunc (e *Error) GRPCStatus() *Status {\n\treturn FromProto(e.e)\n}\n\n// Is implements future error.Is functionality.\n// A Error is equivalent if the code and message are identical.\nfunc (e *Error) Is(target error) bool {\n\ttse, ok := target.(*Error)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn proto.Equal(e.e, tse.e)\n}\n\n// FromError returns a Status representing err if it was produced from this\n// package or has a method `GRPCStatus() *Status`. Otherwise, ok is false and a\n// Status is returned with codes.Unknown and the original error message.\n\n\n\n\n\n\n\n\n\n\n\n// Convert is a convenience function which removes the need to handle the\n// boolean return value from FromError.\nfunc Convert(err error) *Status {\n\ts, _ := FromError(err)\n\treturn s\n}\n\n// Code returns the Code of the error if it is a Status error, codes.OK if err\n// is nil, or codes.Unknown otherwise.\nfunc Code(err error) codes.Code {\n\t// Don't use FromError to avoid allocation of OK status.\n\tif err == nil {\n\t\treturn codes.OK\n\t}\n\tvar se Iface\n\tif errors.As(err, &se) {\n\t\treturn se.GRPCStatus().Code()\n\t}\n\treturn codes.Unknown\n}\n\n// FromContextError converts a context error into a Status.  It returns a\n// Status with codes.OK if err is nil, or a Status with codes.Unknown if err is\n// non-nil and not a context error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func FromContextError(err error) *Status {\n# Natural Language Annotations: // FromContextError converts a context error into a Status.  It returns a\n// Status with codes.OK if err is nil, or a Status with codes.Unknown if err is\n// non-nil and not a context error.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc FromContextError(err error) *Status {\n    // Don't use FromError to avoid allocation of OK status.\n    if err == nil {\n        return newStatus(codes.OK, \"\")\n    }\n    var se Iface\n    if errors.As(err, &se) {\n        return se.GRPCStatus()\n    }\n    return newStatus(codes.Unknown, err.Error())\n}\n```", "  ```\nfunc FromContextError(err error) *Status {\n\tif err == nil {\n\t\treturn newStatus(codes.OK, \"\")\n\t}\n\tvar e *spb.Status\n\tif errors.As(err, &e) {\n\t\treturn fromProto(e)\n\t}\n\treturn newStatus(codes.Unknown, err.Error())\n}\n```", "  ```\nfunc FromContextError(err error) *Status {\n\tif err == nil {\n\t\treturn New(codes.OK, \"\")\n\t}\n\tvar ce interface {\n\t\t// Because NO error value should be nil, we force the ContextError\n\t\t// type to include Is(error) bool\n\t\tIs(error) bool\n\t}\n\tif errors.As(err, &ce) {\n\t\t// Because the Is() method, which is incompatible with `errorIs()`,\n\t\t// we force the implementation to include a method that follows `errorIs()`\n\t\tif ce.Is(err) {\n\t\t\terr = new(spb.Status)\n\t\t}\n\t\tcode := Codes.FromContextError(err)\n\t\treturn New(code, \"\")\n\t}\n\treturn New(codes.Unknown, err.Error())\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/grpc/bdp_estimator.go-calculate": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/grpc/bdp_estimator.go:\n```\n/*\n *\n * Copyright 2017 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage grpc\n\nimport (\n\t\"sync\"\n\t\"time\"\n)\n\nconst (\n\t// bdpLimit is the maximum value the flow control windows will be increased\n\t// to.  TCP typically limits this to 4MB, but some systems go up to 16MB.\n\t// Since this is only a limit, it is safe to make it optimistic.\n\tbdpLimit = (1 << 20) * 16\n\t// alpha is a constant factor used to keep a moving average\n\t// of RTTs.\n\talpha = 0.9\n\t// If the current bdp sample is greater than or equal to\n\t// our beta * our estimated bdp and the current bandwidth\n\t// sample is the maximum bandwidth observed so far, we\n\t// increase our bbp estimate by a factor of gamma.\n\tbeta = 0.66\n\t// To put our bdp to be smaller than or equal to twice the real BDP,\n\t// we should multiply our current sample with 4/3, however to round things out\n\t// we use 2 as the multiplication factor.\n\tgamma = 2\n)\n\n// Adding arbitrary data to ping so that its ack can be identified.\n// Easter-egg: what does the ping message say?\nvar bdpPing = &ping{data: [8]byte{2, 4, 16, 16, 9, 14, 7, 7}}\n\ntype bdpEstimator struct {\n\t// sentAt is the time when the ping was sent.\n\tsentAt time.Time\n\n\tmu sync.Mutex\n\t// bdp is the current bdp estimate.\n\tbdp uint32\n\t// sample is the number of bytes received in one measurement cycle.\n\tsample uint32\n\t// bwMax is the maximum bandwidth noted so far (bytes/sec).\n\tbwMax float64\n\t// bool to keep track of the beginning of a new measurement cycle.\n\tisSent bool\n\t// Callback to update the window sizes.\n\tupdateFlowControl func(n uint32)\n\t// sampleCount is the number of samples taken so far.\n\tsampleCount uint64\n\t// round trip time (seconds)\n\trtt float64\n}\n\n// timesnap registers the time bdp ping was sent out so that\n// network rtt can be calculated when its ack is received.\n// It is called (by controller) when the bdpPing is\n// being written on the wire.\nfunc (b *bdpEstimator) timesnap(d [8]byte) {\n\tif bdpPing.data != d {\n\t\treturn\n\t}\n\t// Locking here is to avoid DATA RACE in the unittest.\n\t// In fact, it would not bring the concurrency problem.\n\tb.mu.Lock()\n\tb.sentAt = time.Now()\n\tb.mu.Unlock()\n}\n\n// add adds bytes to the current sample for calculating bdp.\n// It returns true only if a ping must be sent. This can be used\n// by the caller (handleData) to make decision about batching\n// a window update with it.\nfunc (b *bdpEstimator) add(n uint32) bool {\n\tb.mu.Lock()\n\tdefer b.mu.Unlock()\n\tif b.bdp == bdpLimit {\n\t\treturn false\n\t}\n\tif !b.isSent {\n\t\tb.isSent = true\n\t\tb.sample = n\n\t\tb.sentAt = time.Time{}\n\t\tb.sampleCount++\n\t\treturn true\n\t}\n\tb.sample += n\n\treturn false\n}\n\n// calculate is called when an ack for a bdp ping is received.\n// Here we calculate the current bdp and bandwidth sample and\n// decide if the flow control windows should go up.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (b *bdpEstimator) calculate(d [8]byte) {\n# Natural Language Annotations: // calculate is called when an ack for a bdp ping is received.\n// Here we calculate the current bdp and bandwidth sample and\n// decide if the flow control windows should go up.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (b *bdpEstimator) calculate(d [8]byte) {\n\tif bdpPing.data != d {\n\t\treturn\n\t}\n\tb.mu.Lock()\n\tdefer b.mu.Unlock()\n\tif b.bdp == bdpLimit {\n\t\treturn\n\t}\n\n\tnow := time.Now()\n\trtt := now.Sub(b.sentAt).Seconds()\n\n\t// Calculate new bandwidth sample (bytes/sec)\n\tbw := float64(b.sample) / rtt\n\tif bw > b.bwMax {\n\t\tb.bwMax = bw\n\t}\n\n\t// Calculate new bdp sample (bytes)\n\tvar newBdp uint32\n\tif b.sample > 0 {\n\t\tnewBdp = uint32(float64(b.sample) * 2 / rtt)\n\t}\n\n\t// Update bdp estimate\n\tif newBdp >= b.bdp {\n\t\tb.bdp = newBdp\n\t}\n\n\t// Increase flow control window if needed\n\tif b.bwMax > bdpBps*beta && newBdp >= b.bdp*gamma {\n\t\tb.bdp = newBdp / gamma\n\t\tb.updateFlowControl(b.bdp)\n\t}\n\tb.sample = 0\n\tb.isSent = false\n\n\treturn\n}\n```", "  ```\nfunc (b *bdpEstimator) calculate(d [8]byte) {\n\tif d != bdpPing.data {\n\t\treturn\n\t}\n\t// Calculate rtt\n\trtt := time.Since(b.sentAt).Seconds()\n\t// Calculate bdp and bandwidth sample\n\tbdp := float64(b.sample) / rtt\n\tif bdp > b.bwMax {\n\t\tb.bwMax = bdp\n\t}\n\n\t// Update bdp if observed bw is greater than 1/3 of estimated\n\tif bdp > beta*b.bdp && b.bwMax == float64(b.sample) {\n\t\tb.bwMax *= gamma\n\t}\n\n\t// Update bdp\n\tif b.rtt == 0 {\n\t\tb.rtt = rtt\n\t} else {\n\t\tb.rtt = alpha*rtt + (1-alpha)*b.rtt\n\t}\n\tb.bdp = uint32(float32(bdp) * b.rtt)\n\n\t// Update flow control windows if necessary\n\tif b.bdp > 2*bdpLimit {\n\t\tb.bdp = 2*bdpLimit\n\t}\n\tb.updateFlowControl(b.bdp)\n\n\t// Reset sample and bw\n\tb.sample = 0\n\tb.bwMax = 0\n\tb.isSent = false\n\tb.sampleCount++\n}\n```", "  ```\nfunc (b *bdpEstimator) calculate(d [8]byte) {\n    if bdpPing.data != d {\n        return\n    }\n    // Locking here is to avoid DATA RACE in the unittest.\n    // In fact, it would not bring the concurrency problem.\n    b.mu.Lock()\n    b.bdp = b.sample // Update the bdp estimate\n    b.sample = 0     // Reset the sample\n    b.mu.Unlock()\n\n    // Calculate the RTT based on the time between the ping and ack\n    rtt := float64(time.Since(b.sentAt)) / float64(time.Second)\n    b.rtt = rtt\n\n    // Keep a exponenital moving average of the RTT\n    b.rtt = float64(alpha * b.rtt) + float64((1-alpha) * rtt)\n\n    // Keep a moving average of the bandwidth sample\n    // bwAvg := b.sampleCount / b.rtt\n    b.bwMax = math.Max(b.bwMax, float64(b.sample)/b.rtt)\n\n    // Check if the current bdp sample is greater than OR equal to beta*estimated bdp\n    // and the current bw is the maximum bandwidth observed so far\n    if b.sample >= uint32(float64(beta) * float64(b.bdpLimit)) && b.sampleCount == uint64(math.Max(b.bwMax)) {\n        // Increase the bdp estimate by a factor of gamm and update the flow control window\n        b.bdp = uint32(float64(b.bdp) * float64(gamma))\n        b.updateFlowControl(b.bdp)\n    }\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/grpc/http2_server.go-finishStream": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/grpc/http2_server.go:\n```\n/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage grpc\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"math/rand\"\n\t\"net\"\n\t\"strconv\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf/encoding\"\n\n\t\"github.com/cloudwego/netpoll\"\n\t\"golang.org/x/net/http2\"\n\t\"golang.org/x/net/http2/hpack\"\n\t\"google.golang.org/protobuf/proto\"\n\n\t\"github.com/cloudwego/kitex/pkg/gofunc\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc/grpcframe\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/status\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\nvar (\n\t// ErrIllegalHeaderWrite indicates that setting header is illegal because of\n\t// the stream's state.\n\tErrIllegalHeaderWrite = errors.New(\"transport: the stream is done or WriteHeader was already called\")\n\t// ErrHeaderListSizeLimitViolation indicates that the header list size is larger\n\t// than the limit set by peer.\n\tErrHeaderListSizeLimitViolation = errors.New(\"transport: trying to send header list size larger than the limit set by peer\")\n)\n\nfunc init() {\n\trand.Seed(time.Now().UnixNano())\n}\n\n// http2Server implements the ServerTransport interface with HTTP2.\ntype http2Server struct {\n\tlastRead    int64\n\tctx         context.Context\n\tdone        chan struct{}\n\tconn        net.Conn\n\tloopy       *loopyWriter\n\treaderDone  chan struct{} // sync point to enable testing.\n\twriterDone  chan struct{} // sync point to enable testing.\n\tremoteAddr  net.Addr\n\tlocalAddr   net.Addr\n\tmaxStreamID uint32 // max stream ID ever seen\n\tframer      *framer\n\t// The max number of concurrent streams.\n\tmaxStreams uint32\n\t// controlBuf delivers all the control related tasks (e.g., window\n\t// updates, reset streams, and various settings) to the controller.\n\tcontrolBuf *controlBuffer\n\tfc         *trInFlow\n\t// Keepalive and max-age parameters for the server.\n\tkp ServerKeepalive\n\t// Keepalive enforcement policy.\n\tkep EnforcementPolicy\n\t// The time instance last ping was received.\n\tlastPingAt time.Time\n\t// Number of times the client has violated keepalive ping policy so far.\n\tpingStrikes uint8\n\t// Flag to signify that number of ping strikes should be reset to 0.\n\t// This is set whenever data or header frames are sent.\n\t// 1 means yes.\n\tresetPingStrikes      uint32 // Accessed atomically.\n\tinitialWindowSize     int32\n\tbdpEst                *bdpEstimator\n\tmaxSendHeaderListSize *uint32\n\n\tmu sync.Mutex // guard the following\n\t// drainChan is initialized when drain(...) is called the first time.\n\t// After which the server writes out the first GoAway(with ID 2^31-1) frame.\n\t// Then an independent goroutine will be launched to later send the second GoAway.\n\t// During this time we don't want to write another first GoAway(with ID 2^31 -1) frame.\n\t// Thus call to drain(...) will be a no-op if drainChan is already initialized since draining is\n\t// already underway.\n\tdrainChan     chan struct{}\n\tstate         transportState\n\tactiveStreams map[uint32]*Stream\n\t// idle is the time instant when the connection went idle.\n\t// This is either the beginning of the connection or when the number of\n\t// RPCs go down to 0.\n\t// When the connection is busy, this value is set to 0.\n\tidle time.Time\n\n\tbufferPool *bufferPool\n}\n\n// newHTTP2Server constructs a ServerTransport based on HTTP2. ConnectionError is\n// returned if something goes wrong.\nfunc newHTTP2Server(ctx context.Context, conn net.Conn, config *ServerConfig) (_ ServerTransport, err error) {\n\tmaxHeaderListSize := defaultServerMaxHeaderListSize\n\tif config.MaxHeaderListSize != nil {\n\t\tmaxHeaderListSize = *config.MaxHeaderListSize\n\t}\n\n\tframer := newFramer(conn, config.WriteBufferSize, config.ReadBufferSize, maxHeaderListSize)\n\t// Send initial settings as connection preface to client.\n\tisettings := []http2.Setting{{\n\t\tID:  http2.SettingMaxFrameSize,\n\t\tVal: http2MaxFrameLen,\n\t}}\n\n\t// 0 is permitted in the HTTP2 spec.\n\tmaxStreams := config.MaxStreams\n\tif maxStreams == 0 {\n\t\tmaxStreams = math.MaxUint32\n\t} else {\n\t\tisettings = append(isettings, http2.Setting{\n\t\t\tID:  http2.SettingMaxConcurrentStreams,\n\t\t\tVal: maxStreams,\n\t\t})\n\t}\n\n\tdynamicWindow := true\n\tiwz := initialWindowSize\n\tif config.InitialWindowSize >= defaultWindowSize {\n\t\tiwz = config.InitialWindowSize\n\t\tdynamicWindow = false\n\n\t\tisettings = append(isettings, http2.Setting{\n\t\t\tID:  http2.SettingInitialWindowSize,\n\t\t\tVal: iwz,\n\t\t})\n\t}\n\ticwz := initialWindowSize\n\tif config.InitialConnWindowSize >= defaultWindowSize {\n\t\ticwz = config.InitialConnWindowSize\n\t\tdynamicWindow = false\n\t}\n\tif config.MaxHeaderListSize != nil {\n\t\tisettings = append(isettings, http2.Setting{\n\t\t\tID:  http2.SettingMaxHeaderListSize,\n\t\t\tVal: *config.MaxHeaderListSize,\n\t\t})\n\t}\n\n\tif err := framer.WriteSettings(isettings...); err != nil {\n\t\treturn nil, connectionErrorf(false, err, \"transport: %v\", err)\n\t}\n\n\t// Adjust the connection flow control window if needed.\n\tif icwz > defaultWindowSize {\n\t\tif delta := icwz - defaultWindowSize; delta > 0 {\n\t\t\tif err := framer.WriteWindowUpdate(0, delta); err != nil {\n\t\t\t\treturn nil, connectionErrorf(false, err, \"transport: %v\", err)\n\t\t\t}\n\t\t}\n\t}\n\tkp := config.KeepaliveParams\n\tif kp.MaxConnectionIdle == 0 {\n\t\tkp.MaxConnectionIdle = defaultMaxConnectionIdle\n\t}\n\tif kp.MaxConnectionAge == 0 {\n\t\tkp.MaxConnectionAge = defaultMaxConnectionAge\n\t}\n\tif kp.MaxConnectionAgeGrace == 0 {\n\t\tkp.MaxConnectionAgeGrace = defaultMaxConnectionAgeGrace\n\t}\n\tif kp.Time == 0 {\n\t\tkp.Time = defaultServerKeepaliveTime\n\t}\n\tif kp.Timeout == 0 {\n\t\tkp.Timeout = defaultServerKeepaliveTimeout\n\t}\n\tkep := config.KeepaliveEnforcementPolicy\n\tif kep.MinTime == 0 {\n\t\tkep.MinTime = defaultKeepalivePolicyMinTime\n\t}\n\n\tdone := make(chan struct{})\n\tt := &http2Server{\n\t\tctx:               ctx,\n\t\tdone:              done,\n\t\tconn:              conn,\n\t\tremoteAddr:        conn.RemoteAddr(),\n\t\tlocalAddr:         conn.LocalAddr(),\n\t\tframer:            framer,\n\t\treaderDone:        make(chan struct{}),\n\t\twriterDone:        make(chan struct{}),\n\t\tmaxStreams:        math.MaxUint32,\n\t\tfc:                &trInFlow{limit: icwz},\n\t\tstate:             reachable,\n\t\tactiveStreams:     make(map[uint32]*Stream),\n\t\tkp:                kp,\n\t\tkep:               kep,\n\t\tidle:              time.Now(),\n\t\tinitialWindowSize: int32(iwz),\n\t\tbufferPool:        newBufferPool(),\n\t}\n\tt.controlBuf = newControlBuffer(t.done)\n\tif dynamicWindow {\n\t\tt.bdpEst = &bdpEstimator{\n\t\t\tbdp:               initialWindowSize,\n\t\t\tupdateFlowControl: t.updateFlowControl,\n\t\t}\n\t}\n\n\tt.framer.writer.Flush()\n\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tt.Close()\n\t\t}\n\t}()\n\n\t// Check the validity of client preface.\n\tpreface := make([]byte, len(ClientPreface))\n\tif _, err := io.ReadFull(t.conn, preface); err != nil {\n\t\t// In deployments where a gRPC server runs behind a cloud load balancer\n\t\t// which performs regular TCP level health checks, the connection is\n\t\t// closed immediately by the latter.  Returning io.EOF here allows the\n\t\t// grpc server implementation to recognize this scenario and suppress\n\t\t// logging to reduce spam.\n\t\tif err == io.EOF {\n\t\t\treturn nil, io.EOF\n\t\t}\n\t\treturn nil, connectionErrorf(false, err, \"transport: http2Server.HandleStreams failed to receive the preface from client: %v\", err)\n\t}\n\tif !bytes.Equal(preface, ClientPreface) {\n\t\treturn nil, connectionErrorf(false, nil, \"transport: http2Server.HandleStreams received bogus greeting from client: %q\", preface)\n\t}\n\n\tframe, err := t.framer.ReadFrame()\n\tif err == io.EOF || err == io.ErrUnexpectedEOF {\n\t\treturn nil, err\n\t}\n\tif err != nil {\n\t\treturn nil, connectionErrorf(false, err, \"transport: http2Server.HandleStreams failed to read initial settings frame: %v\", err)\n\t}\n\tatomic.StoreInt64(&t.lastRead, time.Now().UnixNano())\n\tsf, ok := frame.(*grpcframe.SettingsFrame)\n\tif !ok {\n\t\treturn nil, connectionErrorf(false, nil, \"transport: http2Server.HandleStreams saw invalid preface type %T from client\", frame)\n\t}\n\tt.handleSettings(sf)\n\n\tgofunc.RecoverGoFuncWithInfo(ctx, func() {\n\t\tt.loopy = newLoopyWriter(serverSide, t.framer, t.controlBuf, t.bdpEst)\n\t\tt.loopy.ssGoAwayHandler = t.outgoingGoAwayHandler\n\t\tif err := t.loopy.run(conn.RemoteAddr().String()); err != nil {\n\t\t\tklog.CtxErrorf(ctx, \"KITEX: grpc server loopyWriter.run returning, error=%v\", err)\n\t\t}\n\t\tt.conn.Close()\n\t\tclose(t.writerDone)\n\t}, gofunc.NewBasicInfo(\"\", conn.RemoteAddr().String()))\n\n\tgofunc.RecoverGoFuncWithInfo(ctx, t.keepalive, gofunc.NewBasicInfo(\"\", conn.RemoteAddr().String()))\n\treturn t, nil\n}\n\n// operateHeader takes action on the decoded headers.\nfunc (t *http2Server) operateHeaders(frame *grpcframe.MetaHeadersFrame, handle func(*Stream), traceCtx func(context.Context, string) context.Context) (fatal bool) {\n\tstreamID := frame.Header().StreamID\n\tstate := &decodeState{\n\t\tserverSide: true,\n\t}\n\tif err := state.decodeHeader(frame); err != nil {\n\t\tif se, ok := status.FromError(err); ok {\n\t\t\tt.controlBuf.put(&cleanupStream{\n\t\t\t\tstreamID: streamID,\n\t\t\t\trst:      true,\n\t\t\t\trstCode:  statusCodeConvTab[se.Code()],\n\t\t\t\tonWrite:  func() {},\n\t\t\t})\n\t\t}\n\t\treturn false\n\t}\n\n\tbuf := newRecvBuffer()\n\ts := &Stream{\n\t\tid:             streamID,\n\t\tst:             t,\n\t\tbuf:            buf,\n\t\tfc:             &inFlow{limit: uint32(t.initialWindowSize)},\n\t\trecvCompress:   state.data.encoding,\n\t\tsendCompress:   state.data.acceptEncoding,\n\t\tmethod:         state.data.method,\n\t\tcontentSubtype: state.data.contentSubtype,\n\t}\n\tif frame.StreamEnded() {\n\t\t// s is just created by the caller. No lock needed.\n\t\ts.state = streamReadDone\n\t}\n\tif state.data.timeoutSet {\n\t\ts.ctx, s.cancel = context.WithTimeout(t.ctx, state.data.timeout)\n\t} else {\n\t\ts.ctx, s.cancel = context.WithCancel(t.ctx)\n\t}\n\t// Attach the received metadata to the context.\n\tif len(state.data.mdata) > 0 {\n\t\ts.ctx = metadata.NewIncomingContext(s.ctx, state.data.mdata)\n\t}\n\n\tt.mu.Lock()\n\tif t.state != reachable {\n\t\tt.mu.Unlock()\n\t\ts.cancel()\n\t\treturn false\n\t}\n\tif uint32(len(t.activeStreams)) >= t.maxStreams {\n\t\tt.mu.Unlock()\n\t\tt.controlBuf.put(&cleanupStream{\n\t\t\tstreamID: streamID,\n\t\t\trst:      true,\n\t\t\trstCode:  http2.ErrCodeRefusedStream,\n\t\t\tonWrite:  func() {},\n\t\t})\n\t\ts.cancel()\n\t\treturn false\n\t}\n\tif streamID%2 != 1 || streamID <= t.maxStreamID {\n\t\tt.mu.Unlock()\n\t\t// illegal gRPC stream id.\n\t\tklog.CtxErrorf(s.ctx, \"transport: http2Server.HandleStreams received an illegal stream id: %v\", streamID)\n\t\ts.cancel()\n\t\treturn true\n\t}\n\tt.maxStreamID = streamID\n\tt.activeStreams[streamID] = s\n\tif len(t.activeStreams) == 1 {\n\t\tt.idle = time.Time{}\n\t}\n\tt.mu.Unlock()\n\ts.requestRead = func(n int) {\n\t\tt.adjustWindow(s, uint32(n))\n\t}\n\ts.ctx = traceCtx(s.ctx, s.method)\n\ts.ctxDone = s.ctx.Done()\n\ts.wq = newWriteQuota(defaultWriteQuota, s.ctxDone)\n\ts.trReader = &transportReader{\n\t\treader: &recvBufferReader{\n\t\t\tctx:        s.ctx,\n\t\t\tctxDone:    s.ctxDone,\n\t\t\trecv:       s.buf,\n\t\t\tfreeBuffer: t.bufferPool.put,\n\t\t},\n\t\twindowHandler: func(n int) {\n\t\t\tt.updateWindow(s, uint32(n))\n\t\t},\n\t}\n\t// Register the stream with loopy.\n\tt.controlBuf.put(&registerStream{\n\t\tstreamID: s.id,\n\t\twq:       s.wq,\n\t})\n\thandle(s)\n\treturn false\n}\n\n// HandleStreams receives incoming streams using the given handler. This is\n// typically run in a separate goroutine.\n// traceCtx attaches trace to ctx and returns the new context.\nfunc (t *http2Server) HandleStreams(handle func(*Stream), traceCtx func(context.Context, string) context.Context) {\n\tdefer close(t.readerDone)\n\tfor {\n\t\tt.controlBuf.throttle()\n\t\tframe, err := t.framer.ReadFrame()\n\t\tatomic.StoreInt64(&t.lastRead, time.Now().UnixNano())\n\t\tif err != nil {\n\t\t\tif se, ok := err.(http2.StreamError); ok {\n\t\t\t\tklog.CtxWarnf(t.ctx, \"transport: http2Server.HandleStreams encountered http2.StreamError: %v\", se)\n\t\t\t\tt.mu.Lock()\n\t\t\t\ts := t.activeStreams[se.StreamID]\n\t\t\t\tt.mu.Unlock()\n\t\t\t\tif s != nil {\n\t\t\t\t\tt.closeStream(s, true, se.Code, false)\n\t\t\t\t} else {\n\t\t\t\t\tt.controlBuf.put(&cleanupStream{\n\t\t\t\t\t\tstreamID: se.StreamID,\n\t\t\t\t\t\trst:      true,\n\t\t\t\t\t\trstCode:  se.Code,\n\t\t\t\t\t\tonWrite:  func() {},\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err == io.EOF || err == io.ErrUnexpectedEOF || errors.Is(err, netpoll.ErrEOF) {\n\t\t\t\tt.Close()\n\t\t\t\treturn\n\t\t\t}\n\t\t\tklog.CtxWarnf(t.ctx, \"transport: http2Server.HandleStreams failed to read frame: %v\", err)\n\t\t\tt.Close()\n\t\t\treturn\n\t\t}\n\t\tswitch frame := frame.(type) {\n\t\tcase *grpcframe.MetaHeadersFrame:\n\t\t\tif t.operateHeaders(frame, handle, traceCtx) {\n\t\t\t\tt.Close()\n\t\t\t\tbreak\n\t\t\t}\n\t\tcase *grpcframe.DataFrame:\n\t\t\tt.handleData(frame)\n\t\tcase *http2.RSTStreamFrame:\n\t\t\tt.handleRSTStream(frame)\n\t\tcase *grpcframe.SettingsFrame:\n\t\t\tt.handleSettings(frame)\n\t\tcase *http2.PingFrame:\n\t\t\tt.handlePing(frame)\n\t\tcase *http2.WindowUpdateFrame:\n\t\t\tt.handleWindowUpdate(frame)\n\t\tcase *grpcframe.GoAwayFrame:\n\t\t\t// TODO: Handle GoAway from the client appropriately.\n\t\tdefault:\n\t\t\tklog.CtxErrorf(t.ctx, \"transport: http2Server.HandleStreams found unhandled frame type %v.\", frame)\n\t\t}\n\t\tt.framer.reader.Release()\n\t}\n}\n\nfunc (t *http2Server) getStream(f http2.Frame) (*Stream, bool) {\n\tt.mu.Lock()\n\tdefer t.mu.Unlock()\n\tif t.activeStreams == nil {\n\t\t// The transport is closing.\n\t\treturn nil, false\n\t}\n\ts, ok := t.activeStreams[f.Header().StreamID]\n\tif !ok {\n\t\t// The stream is already done.\n\t\treturn nil, false\n\t}\n\treturn s, true\n}\n\n// adjustWindow sends out extra window update over the initial window size\n// of stream if the application is requesting data larger in size than\n// the window.\nfunc (t *http2Server) adjustWindow(s *Stream, n uint32) {\n\tif w := s.fc.maybeAdjust(n); w > 0 {\n\t\tt.controlBuf.put(&outgoingWindowUpdate{streamID: s.id, increment: w})\n\t}\n}\n\n// updateFlowControl updates the incoming flow control windows\n// for the transport and the stream based on the current bdp\n// estimation.\nfunc (t *http2Server) updateFlowControl(n uint32) {\n\tt.mu.Lock()\n\tfor _, s := range t.activeStreams {\n\t\ts.fc.newLimit(n)\n\t}\n\tt.initialWindowSize = int32(n)\n\tt.mu.Unlock()\n\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\tstreamID:  0,\n\t\tincrement: t.fc.newLimit(n),\n\t})\n\tt.controlBuf.put(&outgoingSettings{\n\t\tss: []http2.Setting{\n\t\t\t{\n\t\t\t\tID:  http2.SettingInitialWindowSize,\n\t\t\t\tVal: n,\n\t\t\t},\n\t\t},\n\t})\n}\n\n// updateWindow adjusts the inbound quota for the stream and the transport.\n// Window updates will deliver to the controller for sending when\n// the cumulative quota exceeds the corresponding threshold.\nfunc (t *http2Server) updateWindow(s *Stream, n uint32) {\n\tif w := s.fc.onRead(n); w > 0 {\n\t\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\t\tstreamID:  s.id,\n\t\t\tincrement: w,\n\t\t})\n\t}\n}\n\nfunc (t *http2Server) handleData(f *grpcframe.DataFrame) {\n\tsize := f.Header().Length\n\tvar sendBDPPing bool\n\tif t.bdpEst != nil {\n\t\tsendBDPPing = t.bdpEst.add(size)\n\t}\n\t// Decouple connection's flow control from application's read.\n\t// An update on connection's flow control should not depend on\n\t// whether user application has read the data or not. Such a\n\t// restriction is already imposed on the stream's flow control,\n\t// and therefore the sender will be blocked anyways.\n\t// Decoupling the connection flow control will prevent other\n\t// active(fast) streams from starving in presence of slow or\n\t// inactive streams.\n\tif w := t.fc.onData(size); w > 0 {\n\t\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\t\tstreamID:  0,\n\t\t\tincrement: w,\n\t\t})\n\t}\n\tif sendBDPPing {\n\t\t// Avoid excessive ping detection (e.g. in an L7 proxy)\n\t\t// by sending a window update prior to the BDP ping.\n\t\tif w := t.fc.reset(); w > 0 {\n\t\t\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\t\t\tstreamID:  0,\n\t\t\t\tincrement: w,\n\t\t\t})\n\t\t}\n\t\tt.controlBuf.put(bdpPing)\n\t}\n\t// Select the right stream to dispatch.\n\ts, ok := t.getStream(f)\n\tif !ok {\n\t\treturn\n\t}\n\tif size > 0 {\n\t\tif err := s.fc.onData(size); err != nil {\n\t\t\tt.closeStream(s, true, http2.ErrCodeFlowControl, false)\n\t\t\treturn\n\t\t}\n\t\tif f.Header().Flags.Has(http2.FlagDataPadded) {\n\t\t\tif w := s.fc.onRead(size - uint32(len(f.Data()))); w > 0 {\n\t\t\t\tt.controlBuf.put(&outgoingWindowUpdate{s.id, w})\n\t\t\t}\n\t\t}\n\t\t// TODO(bradfitz, zhaoq): A copy is required here because there is no\n\t\t// guarantee f.Data() is consumed before the arrival of next frame.\n\t\t// Can this copy be eliminated?\n\t\tif len(f.Data()) > 0 {\n\t\t\tbuffer := t.bufferPool.get()\n\t\t\tbuffer.Reset()\n\t\t\tbuffer.Write(f.Data())\n\t\t\ts.write(recvMsg{buffer: buffer})\n\t\t}\n\t}\n\tif f.Header().Flags.Has(http2.FlagDataEndStream) {\n\t\t// Received the end of stream from the client.\n\t\ts.compareAndSwapState(streamActive, streamReadDone)\n\t\ts.write(recvMsg{err: io.EOF})\n\t}\n}\n\nfunc (t *http2Server) handleRSTStream(f *http2.RSTStreamFrame) {\n\t// If the stream is not deleted from the transport's active streams map, then do a regular close stream.\n\tif s, ok := t.getStream(f); ok {\n\t\tt.closeStream(s, false, 0, false)\n\t\treturn\n\t}\n\t// If the stream is already deleted from the active streams map, then put a cleanupStream item into controlbuf to delete the stream from loopy writer's established streams map.\n\tt.controlBuf.put(&cleanupStream{\n\t\tstreamID: f.Header().StreamID,\n\t\trst:      false,\n\t\trstCode:  0,\n\t\tonWrite:  func() {},\n\t})\n}\n\nfunc (t *http2Server) handleSettings(f *grpcframe.SettingsFrame) {\n\tif f.IsAck() {\n\t\treturn\n\t}\n\tvar ss []http2.Setting\n\tvar updateFuncs []func()\n\tf.ForeachSetting(func(s http2.Setting) error {\n\t\tswitch s.ID {\n\t\tcase http2.SettingMaxHeaderListSize:\n\t\t\tupdateFuncs = append(updateFuncs, func() {\n\t\t\t\tt.maxSendHeaderListSize = new(uint32)\n\t\t\t\t*t.maxSendHeaderListSize = s.Val\n\t\t\t})\n\t\tdefault:\n\t\t\tss = append(ss, s)\n\t\t}\n\t\treturn nil\n\t})\n\tt.controlBuf.executeAndPut(func(interface{}) bool {\n\t\tfor _, f := range updateFuncs {\n\t\t\tf()\n\t\t}\n\t\treturn true\n\t}, &incomingSettings{\n\t\tss: ss,\n\t})\n}\n\nconst (\n\tmaxPingStrikes\n```\n# Function Signature: func (t *http2Server) finishStream(s *Stream, rst bool, rstCode http2.ErrCode, hdr *headerFrame, eosReceived bool) {\n# Natural Language Annotations: // finishStream closes the stream and puts the trailing headerFrame into controlbuf.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (t *http2Server) finishStream(s *Stream, rst bool, rstCode http2.ErrCode, hdr *headerFrame, eosReceived bool) {\n\tdefer func() {\n\t\t// TODO: Handle GoAway from the client appropriately.\n\t\tt.mu.Lock()\n\t\tdelete(t.activeStreams, s.id)\n\t\tif len(t.activeStreams) == 0 {\n\t\t\tt.idle = time.Now()\n\t\t}\n\t\tt.mu.Unlock()\n\t\ts.cancel()\n\t\tt.controlBuf.finishStream()\n\t\tt.closeIfIdle()\n\t}()\n\tif !rst {\n\t\t// Send trailing header if the header hasn't been written yet.\n\t\tif hdr != nil && !hdr.written {\n\t\t\tt.controlBuf.put(hdr)\n\t\t\t// Send an empty data frame to satisfy END_STREAM from client.\n\t\t\tif eosReceived {\n\t\t\t\tt.controlBuf.put(&outgoingData{\n\t\t\t\t\tstreamID:  s.id,\n\t\t\t\t\th:         nil,\n\t\t\t\t\tendStream: true,\n\t\t\t\t\tonWrite:   func() {},\n\t\t\t\t})\n\t\t\t}\n\t\t}\n\t} else {\n\t\tt.controlBuf.put(&cleanupStream{\n\t\t\tstreamID: s.id,\n\t\t\trst:      rst,\n\t\t\trstCode:  rstCode,\n\t\t\tonWrite:  func() {},\n\t\t})\n\t}\n}\n```", "  ```\nfunc (t *http2Server) finishStream(s *Stream, rst bool, rstCode http2.ErrCode, hdr *headerFrame, eosReceived bool) {\n\tvar wq *writeQuota\n\tif rst {\n\t\twq = &writeQuota{}\n\t}\n\t// If this function is called, the stream is to be closed.\n\ts.cancel()\n\tt.mu.Lock()\n\tif rst {\n\t\tdelete(t.activeStreams, s.id)\n\t} else if eosReceived {\n\t\t// Received the end-of-stream from the client.\n\t\ts.compareAndSwapState(streamActive, streamReadDone)\n\t}\n\tt.mu.Unlock()\n\tclose(s.done)\n\tt.controlBuf.put(&cleanupStream{\n\t\tstreamID: s.id,\n\t\trst:      rst,\n\t\trstCode:  rstCode,\n\t\thdr:      hdr,\n\t\tonWrite: func() {\n\t\t\tif wq.available() > 0 {\n\t\t\t\tif onWriteErr := t.write(true, wq); onWriteErr != nil {\n\t\t\t\t\tklog.CtxErrorf(t.ctx, \"transport: %v\", onWriteErr)\n\t\t\t\t}\n\t\t\t}\n\t\t\tclose(s.finish)\n\t\t},\n\t})\n}\n```", "  ```\nfunc (t *http2Server) finishStream(s *Stream, rst bool, rstCode http2.ErrCode, hdr *headerFrame, eosReceived bool) {\n\tfor _, eq := range s.get() {\n\t\tif t.stopped && (rst || rstCode != http2.ErrCodeNo) {\n\t\t\teq.write(recvMsg{err: io.EOF})\n\t\t}\n\t\teq.done = true\n\t\teq.closeWithError(io.EOF)\n\t}\n\tif rst {\n\t\trstCode = s.rstCode\n\t} else if rstCode == http2.ErrCodeNo {\n\t\t// End stream if needed.\n\t\tif !eosReceived && !s.writeSched.IsEmpty() {\n\t\t\trstCode = http2.ErrCodeInternalError\n\t\t\thdr.endStream = false\n\t\t} else {\n\t\t\trstCode = http2.ErrCodeNo\n\t\t\thdr.endStream = true\n\t\t}\n\t}\n\t// Return the buffer to the pool.\n\ts.freeBuffer()\n\t// If header wasn't written in time, inform the peer.\n\tif rst && !s.headerWritten {\n\t\tt.controlBuf.put(&outgoingRSTStream{\n\t\t\tstreamID:  s.id,\n\t\t\trstCode:   s.rstCode,\n\t\t\thttpResp:  s.writeSched,\n\t\t\trstStream: true,\n\t\t})\n\t}\n\tt.controlBuf.put(hdr)\n\tgo func() {\n\t\tif !s.done.Done() {\n\t\t\ts.ctx.Done()\n\t\t}\n\t\ts.mu.Lock()\n\t\tdelete(t.activeStreams, s.id)\n\t\tif t.state == draining && len(t.activeStreams) == 0 {\n\t\t\tclose(t.drainChan)\n\t\t}\n\t\ts.mu.Unlock()\n\t}()\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/grpc/http2_server.go-closeStream": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/grpc/http2_server.go:\n```\n/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage grpc\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"math/rand\"\n\t\"net\"\n\t\"strconv\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf/encoding\"\n\n\t\"github.com/cloudwego/netpoll\"\n\t\"golang.org/x/net/http2\"\n\t\"golang.org/x/net/http2/hpack\"\n\t\"google.golang.org/protobuf/proto\"\n\n\t\"github.com/cloudwego/kitex/pkg/gofunc\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc/grpcframe\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/status\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\nvar (\n\t// ErrIllegalHeaderWrite indicates that setting header is illegal because of\n\t// the stream's state.\n\tErrIllegalHeaderWrite = errors.New(\"transport: the stream is done or WriteHeader was already called\")\n\t// ErrHeaderListSizeLimitViolation indicates that the header list size is larger\n\t// than the limit set by peer.\n\tErrHeaderListSizeLimitViolation = errors.New(\"transport: trying to send header list size larger than the limit set by peer\")\n)\n\nfunc init() {\n\trand.Seed(time.Now().UnixNano())\n}\n\n// http2Server implements the ServerTransport interface with HTTP2.\ntype http2Server struct {\n\tlastRead    int64\n\tctx         context.Context\n\tdone        chan struct{}\n\tconn        net.Conn\n\tloopy       *loopyWriter\n\treaderDone  chan struct{} // sync point to enable testing.\n\twriterDone  chan struct{} // sync point to enable testing.\n\tremoteAddr  net.Addr\n\tlocalAddr   net.Addr\n\tmaxStreamID uint32 // max stream ID ever seen\n\tframer      *framer\n\t// The max number of concurrent streams.\n\tmaxStreams uint32\n\t// controlBuf delivers all the control related tasks (e.g., window\n\t// updates, reset streams, and various settings) to the controller.\n\tcontrolBuf *controlBuffer\n\tfc         *trInFlow\n\t// Keepalive and max-age parameters for the server.\n\tkp ServerKeepalive\n\t// Keepalive enforcement policy.\n\tkep EnforcementPolicy\n\t// The time instance last ping was received.\n\tlastPingAt time.Time\n\t// Number of times the client has violated keepalive ping policy so far.\n\tpingStrikes uint8\n\t// Flag to signify that number of ping strikes should be reset to 0.\n\t// This is set whenever data or header frames are sent.\n\t// 1 means yes.\n\tresetPingStrikes      uint32 // Accessed atomically.\n\tinitialWindowSize     int32\n\tbdpEst                *bdpEstimator\n\tmaxSendHeaderListSize *uint32\n\n\tmu sync.Mutex // guard the following\n\t// drainChan is initialized when drain(...) is called the first time.\n\t// After which the server writes out the first GoAway(with ID 2^31-1) frame.\n\t// Then an independent goroutine will be launched to later send the second GoAway.\n\t// During this time we don't want to write another first GoAway(with ID 2^31 -1) frame.\n\t// Thus call to drain(...) will be a no-op if drainChan is already initialized since draining is\n\t// already underway.\n\tdrainChan     chan struct{}\n\tstate         transportState\n\tactiveStreams map[uint32]*Stream\n\t// idle is the time instant when the connection went idle.\n\t// This is either the beginning of the connection or when the number of\n\t// RPCs go down to 0.\n\t// When the connection is busy, this value is set to 0.\n\tidle time.Time\n\n\tbufferPool *bufferPool\n}\n\n// newHTTP2Server constructs a ServerTransport based on HTTP2. ConnectionError is\n// returned if something goes wrong.\nfunc newHTTP2Server(ctx context.Context, conn net.Conn, config *ServerConfig) (_ ServerTransport, err error) {\n\tmaxHeaderListSize := defaultServerMaxHeaderListSize\n\tif config.MaxHeaderListSize != nil {\n\t\tmaxHeaderListSize = *config.MaxHeaderListSize\n\t}\n\n\tframer := newFramer(conn, config.WriteBufferSize, config.ReadBufferSize, maxHeaderListSize)\n\t// Send initial settings as connection preface to client.\n\tisettings := []http2.Setting{{\n\t\tID:  http2.SettingMaxFrameSize,\n\t\tVal: http2MaxFrameLen,\n\t}}\n\n\t// 0 is permitted in the HTTP2 spec.\n\tmaxStreams := config.MaxStreams\n\tif maxStreams == 0 {\n\t\tmaxStreams = math.MaxUint32\n\t} else {\n\t\tisettings = append(isettings, http2.Setting{\n\t\t\tID:  http2.SettingMaxConcurrentStreams,\n\t\t\tVal: maxStreams,\n\t\t})\n\t}\n\n\tdynamicWindow := true\n\tiwz := initialWindowSize\n\tif config.InitialWindowSize >= defaultWindowSize {\n\t\tiwz = config.InitialWindowSize\n\t\tdynamicWindow = false\n\n\t\tisettings = append(isettings, http2.Setting{\n\t\t\tID:  http2.SettingInitialWindowSize,\n\t\t\tVal: iwz,\n\t\t})\n\t}\n\ticwz := initialWindowSize\n\tif config.InitialConnWindowSize >= defaultWindowSize {\n\t\ticwz = config.InitialConnWindowSize\n\t\tdynamicWindow = false\n\t}\n\tif config.MaxHeaderListSize != nil {\n\t\tisettings = append(isettings, http2.Setting{\n\t\t\tID:  http2.SettingMaxHeaderListSize,\n\t\t\tVal: *config.MaxHeaderListSize,\n\t\t})\n\t}\n\n\tif err := framer.WriteSettings(isettings...); err != nil {\n\t\treturn nil, connectionErrorf(false, err, \"transport: %v\", err)\n\t}\n\n\t// Adjust the connection flow control window if needed.\n\tif icwz > defaultWindowSize {\n\t\tif delta := icwz - defaultWindowSize; delta > 0 {\n\t\t\tif err := framer.WriteWindowUpdate(0, delta); err != nil {\n\t\t\t\treturn nil, connectionErrorf(false, err, \"transport: %v\", err)\n\t\t\t}\n\t\t}\n\t}\n\tkp := config.KeepaliveParams\n\tif kp.MaxConnectionIdle == 0 {\n\t\tkp.MaxConnectionIdle = defaultMaxConnectionIdle\n\t}\n\tif kp.MaxConnectionAge == 0 {\n\t\tkp.MaxConnectionAge = defaultMaxConnectionAge\n\t}\n\tif kp.MaxConnectionAgeGrace == 0 {\n\t\tkp.MaxConnectionAgeGrace = defaultMaxConnectionAgeGrace\n\t}\n\tif kp.Time == 0 {\n\t\tkp.Time = defaultServerKeepaliveTime\n\t}\n\tif kp.Timeout == 0 {\n\t\tkp.Timeout = defaultServerKeepaliveTimeout\n\t}\n\tkep := config.KeepaliveEnforcementPolicy\n\tif kep.MinTime == 0 {\n\t\tkep.MinTime = defaultKeepalivePolicyMinTime\n\t}\n\n\tdone := make(chan struct{})\n\tt := &http2Server{\n\t\tctx:               ctx,\n\t\tdone:              done,\n\t\tconn:              conn,\n\t\tremoteAddr:        conn.RemoteAddr(),\n\t\tlocalAddr:         conn.LocalAddr(),\n\t\tframer:            framer,\n\t\treaderDone:        make(chan struct{}),\n\t\twriterDone:        make(chan struct{}),\n\t\tmaxStreams:        math.MaxUint32,\n\t\tfc:                &trInFlow{limit: icwz},\n\t\tstate:             reachable,\n\t\tactiveStreams:     make(map[uint32]*Stream),\n\t\tkp:                kp,\n\t\tkep:               kep,\n\t\tidle:              time.Now(),\n\t\tinitialWindowSize: int32(iwz),\n\t\tbufferPool:        newBufferPool(),\n\t}\n\tt.controlBuf = newControlBuffer(t.done)\n\tif dynamicWindow {\n\t\tt.bdpEst = &bdpEstimator{\n\t\t\tbdp:               initialWindowSize,\n\t\t\tupdateFlowControl: t.updateFlowControl,\n\t\t}\n\t}\n\n\tt.framer.writer.Flush()\n\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tt.Close()\n\t\t}\n\t}()\n\n\t// Check the validity of client preface.\n\tpreface := make([]byte, len(ClientPreface))\n\tif _, err := io.ReadFull(t.conn, preface); err != nil {\n\t\t// In deployments where a gRPC server runs behind a cloud load balancer\n\t\t// which performs regular TCP level health checks, the connection is\n\t\t// closed immediately by the latter.  Returning io.EOF here allows the\n\t\t// grpc server implementation to recognize this scenario and suppress\n\t\t// logging to reduce spam.\n\t\tif err == io.EOF {\n\t\t\treturn nil, io.EOF\n\t\t}\n\t\treturn nil, connectionErrorf(false, err, \"transport: http2Server.HandleStreams failed to receive the preface from client: %v\", err)\n\t}\n\tif !bytes.Equal(preface, ClientPreface) {\n\t\treturn nil, connectionErrorf(false, nil, \"transport: http2Server.HandleStreams received bogus greeting from client: %q\", preface)\n\t}\n\n\tframe, err := t.framer.ReadFrame()\n\tif err == io.EOF || err == io.ErrUnexpectedEOF {\n\t\treturn nil, err\n\t}\n\tif err != nil {\n\t\treturn nil, connectionErrorf(false, err, \"transport: http2Server.HandleStreams failed to read initial settings frame: %v\", err)\n\t}\n\tatomic.StoreInt64(&t.lastRead, time.Now().UnixNano())\n\tsf, ok := frame.(*grpcframe.SettingsFrame)\n\tif !ok {\n\t\treturn nil, connectionErrorf(false, nil, \"transport: http2Server.HandleStreams saw invalid preface type %T from client\", frame)\n\t}\n\tt.handleSettings(sf)\n\n\tgofunc.RecoverGoFuncWithInfo(ctx, func() {\n\t\tt.loopy = newLoopyWriter(serverSide, t.framer, t.controlBuf, t.bdpEst)\n\t\tt.loopy.ssGoAwayHandler = t.outgoingGoAwayHandler\n\t\tif err := t.loopy.run(conn.RemoteAddr().String()); err != nil {\n\t\t\tklog.CtxErrorf(ctx, \"KITEX: grpc server loopyWriter.run returning, error=%v\", err)\n\t\t}\n\t\tt.conn.Close()\n\t\tclose(t.writerDone)\n\t}, gofunc.NewBasicInfo(\"\", conn.RemoteAddr().String()))\n\n\tgofunc.RecoverGoFuncWithInfo(ctx, t.keepalive, gofunc.NewBasicInfo(\"\", conn.RemoteAddr().String()))\n\treturn t, nil\n}\n\n// operateHeader takes action on the decoded headers.\nfunc (t *http2Server) operateHeaders(frame *grpcframe.MetaHeadersFrame, handle func(*Stream), traceCtx func(context.Context, string) context.Context) (fatal bool) {\n\tstreamID := frame.Header().StreamID\n\tstate := &decodeState{\n\t\tserverSide: true,\n\t}\n\tif err := state.decodeHeader(frame); err != nil {\n\t\tif se, ok := status.FromError(err); ok {\n\t\t\tt.controlBuf.put(&cleanupStream{\n\t\t\t\tstreamID: streamID,\n\t\t\t\trst:      true,\n\t\t\t\trstCode:  statusCodeConvTab[se.Code()],\n\t\t\t\tonWrite:  func() {},\n\t\t\t})\n\t\t}\n\t\treturn false\n\t}\n\n\tbuf := newRecvBuffer()\n\ts := &Stream{\n\t\tid:             streamID,\n\t\tst:             t,\n\t\tbuf:            buf,\n\t\tfc:             &inFlow{limit: uint32(t.initialWindowSize)},\n\t\trecvCompress:   state.data.encoding,\n\t\tsendCompress:   state.data.acceptEncoding,\n\t\tmethod:         state.data.method,\n\t\tcontentSubtype: state.data.contentSubtype,\n\t}\n\tif frame.StreamEnded() {\n\t\t// s is just created by the caller. No lock needed.\n\t\ts.state = streamReadDone\n\t}\n\tif state.data.timeoutSet {\n\t\ts.ctx, s.cancel = context.WithTimeout(t.ctx, state.data.timeout)\n\t} else {\n\t\ts.ctx, s.cancel = context.WithCancel(t.ctx)\n\t}\n\t// Attach the received metadata to the context.\n\tif len(state.data.mdata) > 0 {\n\t\ts.ctx = metadata.NewIncomingContext(s.ctx, state.data.mdata)\n\t}\n\n\tt.mu.Lock()\n\tif t.state != reachable {\n\t\tt.mu.Unlock()\n\t\ts.cancel()\n\t\treturn false\n\t}\n\tif uint32(len(t.activeStreams)) >= t.maxStreams {\n\t\tt.mu.Unlock()\n\t\tt.controlBuf.put(&cleanupStream{\n\t\t\tstreamID: streamID,\n\t\t\trst:      true,\n\t\t\trstCode:  http2.ErrCodeRefusedStream,\n\t\t\tonWrite:  func() {},\n\t\t})\n\t\ts.cancel()\n\t\treturn false\n\t}\n\tif streamID%2 != 1 || streamID <= t.maxStreamID {\n\t\tt.mu.Unlock()\n\t\t// illegal gRPC stream id.\n\t\tklog.CtxErrorf(s.ctx, \"transport: http2Server.HandleStreams received an illegal stream id: %v\", streamID)\n\t\ts.cancel()\n\t\treturn true\n\t}\n\tt.maxStreamID = streamID\n\tt.activeStreams[streamID] = s\n\tif len(t.activeStreams) == 1 {\n\t\tt.idle = time.Time{}\n\t}\n\tt.mu.Unlock()\n\ts.requestRead = func(n int) {\n\t\tt.adjustWindow(s, uint32(n))\n\t}\n\ts.ctx = traceCtx(s.ctx, s.method)\n\ts.ctxDone = s.ctx.Done()\n\ts.wq = newWriteQuota(defaultWriteQuota, s.ctxDone)\n\ts.trReader = &transportReader{\n\t\treader: &recvBufferReader{\n\t\t\tctx:        s.ctx,\n\t\t\tctxDone:    s.ctxDone,\n\t\t\trecv:       s.buf,\n\t\t\tfreeBuffer: t.bufferPool.put,\n\t\t},\n\t\twindowHandler: func(n int) {\n\t\t\tt.updateWindow(s, uint32(n))\n\t\t},\n\t}\n\t// Register the stream with loopy.\n\tt.controlBuf.put(&registerStream{\n\t\tstreamID: s.id,\n\t\twq:       s.wq,\n\t})\n\thandle(s)\n\treturn false\n}\n\n// HandleStreams receives incoming streams using the given handler. This is\n// typically run in a separate goroutine.\n// traceCtx attaches trace to ctx and returns the new context.\nfunc (t *http2Server) HandleStreams(handle func(*Stream), traceCtx func(context.Context, string) context.Context) {\n\tdefer close(t.readerDone)\n\tfor {\n\t\tt.controlBuf.throttle()\n\t\tframe, err := t.framer.ReadFrame()\n\t\tatomic.StoreInt64(&t.lastRead, time.Now().UnixNano())\n\t\tif err != nil {\n\t\t\tif se, ok := err.(http2.StreamError); ok {\n\t\t\t\tklog.CtxWarnf(t.ctx, \"transport: http2Server.HandleStreams encountered http2.StreamError: %v\", se)\n\t\t\t\tt.mu.Lock()\n\t\t\t\ts := t.activeStreams[se.StreamID]\n\t\t\t\tt.mu.Unlock()\n\t\t\t\tif s != nil {\n\t\t\t\t\tt.closeStream(s, true, se.Code, false)\n\t\t\t\t} else {\n\t\t\t\t\tt.controlBuf.put(&cleanupStream{\n\t\t\t\t\t\tstreamID: se.StreamID,\n\t\t\t\t\t\trst:      true,\n\t\t\t\t\t\trstCode:  se.Code,\n\t\t\t\t\t\tonWrite:  func() {},\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err == io.EOF || err == io.ErrUnexpectedEOF || errors.Is(err, netpoll.ErrEOF) {\n\t\t\t\tt.Close()\n\t\t\t\treturn\n\t\t\t}\n\t\t\tklog.CtxWarnf(t.ctx, \"transport: http2Server.HandleStreams failed to read frame: %v\", err)\n\t\t\tt.Close()\n\t\t\treturn\n\t\t}\n\t\tswitch frame := frame.(type) {\n\t\tcase *grpcframe.MetaHeadersFrame:\n\t\t\tif t.operateHeaders(frame, handle, traceCtx) {\n\t\t\t\tt.Close()\n\t\t\t\tbreak\n\t\t\t}\n\t\tcase *grpcframe.DataFrame:\n\t\t\tt.handleData(frame)\n\t\tcase *http2.RSTStreamFrame:\n\t\t\tt.handleRSTStream(frame)\n\t\tcase *grpcframe.SettingsFrame:\n\t\t\tt.handleSettings(frame)\n\t\tcase *http2.PingFrame:\n\t\t\tt.handlePing(frame)\n\t\tcase *http2.WindowUpdateFrame:\n\t\t\tt.handleWindowUpdate(frame)\n\t\tcase *grpcframe.GoAwayFrame:\n\t\t\t// TODO: Handle GoAway from the client appropriately.\n\t\tdefault:\n\t\t\tklog.CtxErrorf(t.ctx, \"transport: http2Server.HandleStreams found unhandled frame type %v.\", frame)\n\t\t}\n\t\tt.framer.reader.Release()\n\t}\n}\n\nfunc (t *http2Server) getStream(f http2.Frame) (*Stream, bool) {\n\tt.mu.Lock()\n\tdefer t.mu.Unlock()\n\tif t.activeStreams == nil {\n\t\t// The transport is closing.\n\t\treturn nil, false\n\t}\n\ts, ok := t.activeStreams[f.Header().StreamID]\n\tif !ok {\n\t\t// The stream is already done.\n\t\treturn nil, false\n\t}\n\treturn s, true\n}\n\n// adjustWindow sends out extra window update over the initial window size\n// of stream if the application is requesting data larger in size than\n// the window.\nfunc (t *http2Server) adjustWindow(s *Stream, n uint32) {\n\tif w := s.fc.maybeAdjust(n); w > 0 {\n\t\tt.controlBuf.put(&outgoingWindowUpdate{streamID: s.id, increment: w})\n\t}\n}\n\n// updateFlowControl updates the incoming flow control windows\n// for the transport and the stream based on the current bdp\n// estimation.\nfunc (t *http2Server) updateFlowControl(n uint32) {\n\tt.mu.Lock()\n\tfor _, s := range t.activeStreams {\n\t\ts.fc.newLimit(n)\n\t}\n\tt.initialWindowSize = int32(n)\n\tt.mu.Unlock()\n\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\tstreamID:  0,\n\t\tincrement: t.fc.newLimit(n),\n\t})\n\tt.controlBuf.put(&outgoingSettings{\n\t\tss: []http2.Setting{\n\t\t\t{\n\t\t\t\tID:  http2.SettingInitialWindowSize,\n\t\t\t\tVal: n,\n\t\t\t},\n\t\t},\n\t})\n}\n\n// updateWindow adjusts the inbound quota for the stream and the transport.\n// Window updates will deliver to the controller for sending when\n// the cumulative quota exceeds the corresponding threshold.\nfunc (t *http2Server) updateWindow(s *Stream, n uint32) {\n\tif w := s.fc.onRead(n); w > 0 {\n\t\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\t\tstreamID:  s.id,\n\t\t\tincrement: w,\n\t\t})\n\t}\n}\n\nfunc (t *http2Server) handleData(f *grpcframe.DataFrame) {\n\tsize := f.Header().Length\n\tvar sendBDPPing bool\n\tif t.bdpEst != nil {\n\t\tsendBDPPing = t.bdpEst.add(size)\n\t}\n\t// Decouple connection's flow control from application's read.\n\t// An update on connection's flow control should not depend on\n\t// whether user application has read the data or not. Such a\n\t// restriction is already imposed on the stream's flow control,\n\t// and therefore the sender will be blocked anyways.\n\t// Decoupling the connection flow control will prevent other\n\t// active(fast) streams from starving in presence of slow or\n\t// inactive streams.\n\tif w := t.fc.onData(size); w > 0 {\n\t\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\t\tstreamID:  0,\n\t\t\tincrement: w,\n\t\t})\n\t}\n\tif sendBDPPing {\n\t\t// Avoid excessive ping detection (e.g. in an L7 proxy)\n\t\t// by sending a window update prior to the BDP ping.\n\t\tif w := t.fc.reset(); w > 0 {\n\t\t\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\t\t\tstreamID:  0,\n\t\t\t\tincrement: w,\n\t\t\t})\n\t\t}\n\t\tt.controlBuf.put(bdpPing)\n\t}\n\t// Select the right stream to dispatch.\n\ts, ok := t.getStream(f)\n\tif !ok {\n\t\treturn\n\t}\n\tif size > 0 {\n\t\tif err := s.fc.onData(size); err != nil {\n\t\t\tt.closeStream(s, true, http2.ErrCodeFlowControl, false)\n\t\t\treturn\n\t\t}\n\t\tif f.Header().Flags.Has(http2.FlagDataPadded) {\n\t\t\tif w := s.fc.onRead(size - uint32(len(f.Data()))); w > 0 {\n\t\t\t\tt.controlBuf.put(&outgoingWindowUpdate{s.id, w})\n\t\t\t}\n\t\t}\n\t\t// TODO(bradfitz, zhaoq): A copy is required here because there is no\n\t\t// guarantee f.Data() is consumed before the arrival of next frame.\n\t\t// Can this copy be eliminated?\n\t\tif len(f.Data()) > 0 {\n\t\t\tbuffer := t.bufferPool.get()\n\t\t\tbuffer.Reset()\n\t\t\tbuffer.Write(f.Data())\n\t\t\ts.write(recvMsg{buffer: buffer})\n\t\t}\n\t}\n\tif f.Header().Flags.Has(http2.FlagDataEndStream) {\n\t\t// Received the end of stream from the client.\n\t\ts.compareAndSwapState(streamActive, streamReadDone)\n\t\ts.write(recvMsg{err: io.EOF})\n\t}\n}\n\nfunc (t *http2Server) handleRSTStream(f *http2.RSTStreamFrame) {\n\t// If the stream is not deleted from the transport's active streams map, then do a regular close stream.\n\tif s, ok := t.getStream(f); ok {\n\t\tt.closeStream(s, false, 0, false)\n\t\treturn\n\t}\n\t// If the stream is already deleted from the active streams map, then put a cleanupStream item into controlbuf to delete the stream from loopy writer's established streams map.\n\tt.controlBuf.put(&cleanupStream{\n\t\tstreamID: f.Header().StreamID,\n\t\trst:      false,\n\t\trstCode:  0,\n\t\tonWrite:  func() {},\n\t})\n}\n\nfunc (t *http2Server) handleSettings(f *grpcframe.SettingsFrame) {\n\tif f.IsAck() {\n\t\treturn\n\t}\n\tvar ss []http2.Setting\n\tvar updateFuncs []func()\n\tf.ForeachSetting(func(s http2.Setting) error {\n\t\tswitch s.ID {\n\t\tcase http2.SettingMaxHeaderListSize:\n\t\t\tupdateFuncs = append(updateFuncs, func() {\n\t\t\t\tt.maxSendHeaderListSize = new(uint32)\n\t\t\t\t*t.maxSendHeaderListSize = s.Val\n\t\t\t})\n\t\tdefault:\n\t\t\tss = append(ss, s)\n\t\t}\n\t\treturn nil\n\t})\n\tt.controlBuf.executeAndPut(func(interface{}) bool {\n\t\tfor _, f := range updateFuncs {\n\t\t\tf()\n\t\t}\n\t\treturn true\n\t}, &incomingSettings{\n\t\tss: ss,\n\t})\n}\n\nconst (\n\tmaxPingStrikes\n```\n# Function Signature: func (t *http2Server) closeStream(s *Stream, rst bool, rstCode http2.ErrCode, eosReceived bool) {\n# Natural Language Annotations: // closeStream clears the footprint of a stream when the stream is not needed any more.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (t *http2Server) closeStream(s *Stream, rst bool, rstCode http2.ErrCode, eosReceived bool) {\n\tt.mu.Lock()\n\tif t.activeStreams == nil {\n\t\tt.mu.Unlock()\n\t\treturn\n\t}\n\tif _, ok := t.activeStreams[s.id]; !ok {\n\t\tt.mu.Unlock()\n\t\treturn\n\t}\n\tdelete(t.activeStreams, s.id)\n\tif len(t.activeStreams) == 0 {\n\t\tt.idle = time.Now()\n\t}\n\tt.mu.Unlock()\n\n\tif rst {\n\t\t// Send a RST_STREAM to end the stream.\n\t\tt.controlBuf.put(&cleanupStream{\n\t\t\tstreamID: s.id,\n\t\t\trst:      true,\n\t\t\trstCode:  rstCode,\n\t\t\tonWrite:  func() {},\n\t\t})\n\t}\n\n\tif eosReceived {\n\t\t// Send a RST_STREAM to end the stream.\n\t\tt.controlBuf.put(&cleanupStream{\n\t\t\tstreamID: s.id,\n\t\t\trst:      false,\n\t\t\trstCode:  0,\n\t\t\tonWrite:  func() {},\n\t\t})\n\t}\n\n\t// Remove the stream from the write queue.\n\tt.controlBuf.put(&outgoingStreamReset{\n\t\tstreamID: s.id,\n\t\trstErrorCode: http2.ErrCodeCancel,\n\t})\n}\n```", "  ```\nfunc (t *http2Server) closeStream(s *Stream, rst bool, rstCode http2.ErrCode, eosReceived bool) {\n\tt.mu.Lock()\n\tdelete(t.activeStreams, s.id)\n\tif len(t.activeStreams) == 0 {\n\t\tt.idle = time.Now()\n\t}\n\tt.mu.Unlock()\n\tif rst {\n\t\t// Send a RST_STREAM to the client.\n\t\tt.controlBuf.put(&cleanupStream{\n\t\t\tstreamID: s.id,\n\t\t\trst:      true,\n\t\t\trstCode:  rstCode,\n\t\t\tonWrite:  func() {},\n\t\t})\n\t} else {\n\t\tt.controlBuf.put(&cleanupStream{\n\t\t\tstreamID: s.id,\n\t\t\trst:      false,\n\t\t\trstCode:  0,\n\t\t\tonWrite: func() {\n\t\t\t\t// The client expects 100-continue, but we didn't bother sending\n\t\t\t\t// a response, inform the client that the request was cancelled.\n\t\t\t\tt.framer.writeRSTStream(s.id, http2.ErrCodeCancel)\n\t\t\t},\n\t\t})\n\t}\n\ts.cancel()\n\tif !eosReceived {\n\t\ts.write(recvMsg{err: io.EOF})\n\t}\n\ts.ctx.Done()\n\ts.buf.finish(nil)\n}\n```", "  ```\nfunc (t *http2Server) closeStream(s *Stream, rst bool, rstCode http2.ErrCode, eosReceived bool) {\n\t// remove stream from the maps.\n\tt.mu.Lock()\n\tdelete(t.activeStreams, s.id)\n\tt.mu.Unlock()\n\tif rst {\n\t\t// Send a RST stream instead.\n\t\tt.controlBuf.put(&resetStream{\n\t\t\tstreamID:  s.id,\n\t\t\trstCode:   rstCode,\n\t\t\tonReset:   s.resetStream,\n\t\t\tonSuccess: func() {},\n\t\t})\n\t} else {\n\t\t// The stream is already deleted from active streams map.\n\t\t// If eosReceived is true, the library will send an HTTP2-level\n\t\t// RST_STREAM to the client and delete the stream on the peer side.\n\t\t// If eosReceived is false, the stream is still alive on the peer side.\n\t\tcleanup := &cleanupStream{\n\t\t\tstreamID: s.id,\n\t\t\trst:      eosReceived,\n\t\t\tonWrite:  func() {},\n\t\t}\n\t\tif !eosReceived {\n\t\t\t// Even if the remote side has received RST, we can still\n\t\t\t// write to this stream id. In that case, the cleanupStream\n\t\t\t// will be discarded during flush.\n\t\t\tcleanup.onWrite = s.ctxDone\n\t\t}\n\t\tt.controlBuf.put(cleanup)\n\t}\n\t// The server initiated stream reset, then counts it as streams started.\n\tif rst && !eosReceived {\n\t\tatomic.AddUint64(&t.stats.streamsSucceeded, 1)\n\t}\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/grpc/transport.go-TLSConfig": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/grpc/transport.go:\n```\n/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\n// Package grpc defines and implements message oriented communication\n// channel to complete various transactions (e.g., an RPC).  It is meant for\n// grpc-internal usage and is not intended to be imported directly by users.\npackage grpc\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto/tls\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/codes\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/status\"\n)\n\ntype bufferPool struct {\n\tpool sync.Pool\n}\n\nfunc newBufferPool() *bufferPool {\n\treturn &bufferPool{\n\t\tpool: sync.Pool{\n\t\t\tNew: func() interface{} {\n\t\t\t\treturn new(bytes.Buffer)\n\t\t\t},\n\t\t},\n\t}\n}\n\nfunc (p *bufferPool) get() *bytes.Buffer {\n\treturn p.pool.Get().(*bytes.Buffer)\n}\n\nfunc (p *bufferPool) put(b *bytes.Buffer) {\n\tp.pool.Put(b)\n}\n\n// recvMsg represents the received msg from the transport. All transport\n// protocol specific info has been removed.\ntype recvMsg struct {\n\tbuffer *bytes.Buffer\n\t// nil: received some data\n\t// io.EOF: stream is completed. data is nil.\n\t// other non-nil error: transport failure. data is nil.\n\terr error\n}\n\n// recvBuffer is an unbounded channel of recvMsg structs.\n//\n// Note: recvBuffer differs from buffer.Unbounded only in the fact that it\n// holds a channel of recvMsg structs instead of objects implementing \"item\"\n// interface. recvBuffer is written to much more often and using strict recvMsg\n// structs helps avoid allocation in \"recvBuffer.put\"\ntype recvBuffer struct {\n\tc       chan recvMsg\n\tmu      sync.Mutex\n\tbacklog []recvMsg\n\terr     error\n}\n\nfunc newRecvBuffer() *recvBuffer {\n\tb := &recvBuffer{\n\t\tc: make(chan recvMsg, 1),\n\t}\n\treturn b\n}\n\nfunc (b *recvBuffer) put(r recvMsg) {\n\tb.mu.Lock()\n\tif b.err != nil {\n\t\tb.mu.Unlock()\n\t\t// An error had occurred earlier, don't accept more\n\t\t// data or errors.\n\t\treturn\n\t}\n\tb.err = r.err\n\tif len(b.backlog) == 0 {\n\t\tselect {\n\t\tcase b.c <- r:\n\t\t\tb.mu.Unlock()\n\t\t\treturn\n\t\tdefault:\n\t\t}\n\t}\n\tb.backlog = append(b.backlog, r)\n\tb.mu.Unlock()\n}\n\nfunc (b *recvBuffer) load() {\n\tb.mu.Lock()\n\tif len(b.backlog) > 0 {\n\t\tselect {\n\t\tcase b.c <- b.backlog[0]:\n\t\t\tb.backlog[0] = recvMsg{}\n\t\t\tb.backlog = b.backlog[1:]\n\t\tdefault:\n\t\t}\n\t}\n\tb.mu.Unlock()\n}\n\n// get returns the channel that receives a recvMsg in the buffer.\n//\n// Upon receipt of a recvMsg, the caller should call load to send another\n// recvMsg onto the channel if there is any.\nfunc (b *recvBuffer) get() <-chan recvMsg {\n\treturn b.c\n}\n\n// recvBufferReader implements io.Reader interface to read the data from\n// recvBuffer.\ntype recvBufferReader struct {\n\tcloseStream func(error) // Closes the client transport stream with the given error and nil trailer metadata.\n\tctx         context.Context\n\tctxDone     <-chan struct{} // cache of ctx.Done() (for performance).\n\trecv        *recvBuffer\n\tlast        *bytes.Buffer // Stores the remaining data in the previous calls.\n\terr         error\n\tfreeBuffer  func(*bytes.Buffer)\n}\n\n// Read reads the next len(p) bytes from last. If last is drained, it tries to\n// read additional data from recv. It blocks if there no additional data available\n// in recv. If Read returns any non-nil error, it will continue to return that error.\nfunc (r *recvBufferReader) Read(p []byte) (n int, err error) {\n\tif r.err != nil {\n\t\treturn 0, r.err\n\t}\n\tif r.last != nil {\n\t\t// Read remaining data left in last call.\n\t\tcopied, _ := r.last.Read(p)\n\t\tif r.last.Len() == 0 {\n\t\t\tr.freeBuffer(r.last)\n\t\t\tr.last = nil\n\t\t}\n\t\treturn copied, nil\n\t}\n\tif r.closeStream != nil {\n\t\tn, r.err = r.readClient(p)\n\t} else {\n\t\tn, r.err = r.read(p)\n\t}\n\treturn n, r.err\n}\n\nfunc (r *recvBufferReader) read(p []byte) (n int, err error) {\n\tselect {\n\tcase <-r.ctxDone:\n\t\treturn 0, ContextErr(r.ctx.Err())\n\tcase m := <-r.recv.get():\n\t\treturn r.readAdditional(m, p)\n\t}\n}\n\nfunc (r *recvBufferReader) readClient(p []byte) (n int, err error) {\n\t// If the context is canceled, then closes the stream with nil metadata.\n\t// closeStream writes its error parameter to r.recv as a recvMsg.\n\t// r.readAdditional acts on that message and returns the necessary error.\n\tselect {\n\tcase <-r.ctxDone:\n\t\t// Note that this adds the ctx error to the end of recv buffer, and\n\t\t// reads from the head. This will delay the error until recv buffer is\n\t\t// empty, thus will delay ctx cancellation in Recv().\n\t\t//\n\t\t// It's done this way to fix a race between ctx cancel and trailer. The\n\t\t// race was, stream.Recv() may return ctx error if ctxDone wins the\n\t\t// race, but stream.Trailer() may return a non-nil md because the stream\n\t\t// was not marked as done when trailer is received. This closeStream\n\t\t// call will mark stream as done, thus fix the race.\n\t\t//\n\t\t// TODO: delaying ctx error seems like a unnecessary side effect. What\n\t\t// we really want is to mark the stream as done, and return ctx error\n\t\t// faster.\n\t\tr.closeStream(ContextErr(r.ctx.Err()))\n\t\tm := <-r.recv.get()\n\t\treturn r.readAdditional(m, p)\n\tcase m := <-r.recv.get():\n\t\treturn r.readAdditional(m, p)\n\t}\n}\n\nfunc (r *recvBufferReader) readAdditional(m recvMsg, p []byte) (n int, err error) {\n\tr.recv.load()\n\tif m.err != nil {\n\t\treturn 0, m.err\n\t}\n\tcopied, _ := m.buffer.Read(p)\n\tif m.buffer.Len() == 0 {\n\t\tr.freeBuffer(m.buffer)\n\t\tr.last = nil\n\t} else {\n\t\tr.last = m.buffer\n\t}\n\treturn copied, nil\n}\n\ntype streamState uint32\n\nconst (\n\tstreamActive    streamState = iota\n\tstreamWriteDone             // EndStream sent\n\tstreamReadDone              // EndStream received\n\tstreamDone                  // the entire stream is finished.\n)\n\n// Stream represents an RPC in the transport layer.\ntype Stream struct {\n\tid           uint32\n\tst           ServerTransport    // nil for client side Stream\n\tct           *http2Client       // nil for server side Stream\n\tctx          context.Context    // the associated context of the stream\n\tcancel       context.CancelFunc // always nil for client side Stream\n\tdone         chan struct{}      // closed at the end of stream to unblock writers. On the client side.\n\tctxDone      <-chan struct{}    // same as done chan but for server side. Cache of ctx.Done() (for performance)\n\tmethod       string             // the associated RPC method of the stream\n\trecvCompress string\n\tsendCompress string\n\tbuf          *recvBuffer\n\ttrReader     io.Reader\n\tfc           *inFlow\n\twq           *writeQuota\n\n\t// Callback to state application's intentions to read data. This\n\t// is used to adjust flow control, if needed.\n\trequestRead func(int)\n\n\theaderChan       chan struct{} // closed to indicate the end of header metadata.\n\theaderChanClosed uint32        // set when headerChan is closed. Used to avoid closing headerChan multiple times.\n\t// headerValid indicates whether a valid header was received.  Only\n\t// meaningful after headerChan is closed (always call waitOnHeader() before\n\t// reading its value).  Not valid on server side.\n\theaderValid bool\n\n\t// hdrMu protects header and trailer metadata on the server-side.\n\thdrMu sync.Mutex\n\t// On client side, header keeps the received header metadata.\n\t//\n\t// On server side, header keeps the header set by SetHeader(). The complete\n\t// header will merged into this after t.WriteHeader() is called.\n\theader  metadata.MD\n\ttrailer metadata.MD // the key-value map of trailer metadata.\n\n\tnoHeaders bool // set if the client never received headers (set only after the stream is done).\n\n\t// On the server-side, headerSent is atomically set to 1 when the headers are sent out.\n\theaderSent uint32\n\n\tstate streamState\n\n\t// On client-side it is the status error received from the server.\n\t// On server-side it is unused.\n\tstatus       *status.Status\n\tbizStatusErr kerrors.BizStatusErrorIface\n\n\tbytesReceived uint32 // indicates whether any bytes have been received on this stream\n\tunprocessed   uint32 // set if the server sends a refused stream or GOAWAY including this stream\n\n\t// contentSubtype is the content-subtype for requests.\n\t// this must be lowercase or the behavior is undefined.\n\tcontentSubtype string\n}\n\n// isHeaderSent is only valid on the server-side.\nfunc (s *Stream) isHeaderSent() bool {\n\treturn atomic.LoadUint32(&s.headerSent) == 1\n}\n\n// updateHeaderSent updates headerSent and returns true\n// if it was already set. It is valid only on server-side.\nfunc (s *Stream) updateHeaderSent() bool {\n\treturn atomic.SwapUint32(&s.headerSent, 1) == 1\n}\n\nfunc (s *Stream) swapState(st streamState) streamState {\n\treturn streamState(atomic.SwapUint32((*uint32)(&s.state), uint32(st)))\n}\n\nfunc (s *Stream) compareAndSwapState(oldState, newState streamState) bool {\n\treturn atomic.CompareAndSwapUint32((*uint32)(&s.state), uint32(oldState), uint32(newState))\n}\n\nfunc (s *Stream) getState() streamState {\n\treturn streamState(atomic.LoadUint32((*uint32)(&s.state)))\n}\n\nfunc (s *Stream) waitOnHeader() {\n\tif s.headerChan == nil {\n\t\t// On the server headerChan is always nil since a stream originates\n\t\t// only after having received headers.\n\t\treturn\n\t}\n\tselect {\n\tcase <-s.ctx.Done():\n\t\t// Close the stream to prevent headers/trailers from changing after\n\t\t// this function returns.\n\t\ts.ct.CloseStream(s, ContextErr(s.ctx.Err()))\n\t\t// headerChan could possibly not be closed yet if closeStream raced\n\t\t// with operateHeaders; wait until it is closed explicitly here.\n\t\t<-s.headerChan\n\tcase <-s.headerChan:\n\t}\n}\n\n// RecvCompress returns the compression algorithm applied to the inbound\n// message. It is empty string if there is no compression applied.\nfunc (s *Stream) RecvCompress() string {\n\ts.waitOnHeader()\n\treturn s.recvCompress\n}\n\n// SendCompress returns the compression algorithm applied to the outbound\n// message. It is empty string if there is no compression applied.\nfunc (s *Stream) SendCompress() string {\n\ts.waitOnHeader()\n\treturn s.sendCompress\n}\n\n// SetSendCompress sets the compression algorithm to the stream.\nfunc (s *Stream) SetSendCompress(str string) {\n\ts.sendCompress = str\n}\n\n// Done returns a channel which is closed when it receives the final status\n// from the server.\nfunc (s *Stream) Done() <-chan struct{} {\n\treturn s.done\n}\n\n// Header returns the header metadata of the stream.\n//\n// On client side, it acquires the key-value pairs of header metadata once it is\n// available. It blocks until i) the metadata is ready or ii) there is no header\n// metadata or iii) the stream is canceled/expired.\n//\n// On server side, it returns the out header after t.WriteHeader is called.  It\n// does not block and must not be called until after WriteHeader.\nfunc (s *Stream) Header() (metadata.MD, error) {\n\tif s.headerChan == nil {\n\t\t// On server side, return the header in stream. It will be the out\n\t\t// header after t.WriteHeader is called.\n\t\treturn s.header.Copy(), nil\n\t}\n\ts.waitOnHeader()\n\tif !s.headerValid {\n\t\treturn nil, s.status.Err()\n\t}\n\treturn s.header.Copy(), nil\n}\n\n// TrailersOnly blocks until a header or trailers-only frame is received and\n// then returns true if the stream was trailers-only.  If the stream ends\n// before headers are received, returns true, nil.  Client-side only.\nfunc (s *Stream) TrailersOnly() bool {\n\ts.waitOnHeader()\n\treturn s.noHeaders\n}\n\n// Trailer returns the cached trailer metedata. Note that if it is not called\n// after the entire stream is done, it could return an empty MD. Client\n// side only.\n// It can be safely read only after stream has ended that is either read\n// or write have returned io.EOF.\nfunc (s *Stream) Trailer() metadata.MD {\n\tc := s.trailer.Copy()\n\treturn c\n}\n\n// ContentSubtype returns the content-subtype for a request. For example, a\n// content-subtype of \"proto\" will result in a content-type of\n// \"application/grpc+proto\". This will always be lowercase.  See\n// https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests for\n// more details.\nfunc (s *Stream) ContentSubtype() string {\n\treturn s.contentSubtype\n}\n\n// Context returns the context of the stream.\nfunc (s *Stream) Context() context.Context {\n\treturn s.ctx\n}\n\n// Method returns the method for the stream.\nfunc (s *Stream) Method() string {\n\treturn s.method\n}\n\n// Status returns the status received from the server.\n// Status can be read safely only after the stream has ended,\n// that is, after Done() is closed.\nfunc (s *Stream) Status() *status.Status {\n\treturn s.status\n}\n\nfunc (s *Stream) SetBizStatusErr(bizStatusErr kerrors.BizStatusErrorIface) {\n\ts.bizStatusErr = bizStatusErr\n}\n\nfunc (s *Stream) BizStatusErr() kerrors.BizStatusErrorIface {\n\treturn s.bizStatusErr\n}\n\n// SetHeader sets the header metadata. This can be called multiple times.\n// Server side only.\n// This should not be called in parallel to other data writes.\nfunc (s *Stream) SetHeader(md metadata.MD) error {\n\tif md.Len() == 0 {\n\t\treturn nil\n\t}\n\tif s.isHeaderSent() || s.getState() == streamDone {\n\t\treturn ErrIllegalHeaderWrite\n\t}\n\ts.hdrMu.Lock()\n\ts.header = metadata.AppendMD(s.header, md)\n\ts.hdrMu.Unlock()\n\treturn nil\n}\n\n// SendHeader sends the given header metadata. The given metadata is\n// combined with any metadata set by previous calls to SetHeader and\n// then written to the transport stream.\nfunc (s *Stream) SendHeader(md metadata.MD) error {\n\treturn s.st.WriteHeader(s, md)\n}\n\n// SetTrailer sets the trailer metadata which will be sent with the RPC status\n// by the server. This can be called multiple times. Server side only.\n// This should not be called parallel to other data writes.\nfunc (s *Stream) SetTrailer(md metadata.MD) error {\n\tif md.Len() == 0 {\n\t\treturn nil\n\t}\n\tif s.getState() == streamDone {\n\t\treturn ErrIllegalHeaderWrite\n\t}\n\ts.hdrMu.Lock()\n\ts.trailer = metadata.AppendMD(s.trailer, md)\n\ts.hdrMu.Unlock()\n\treturn nil\n}\n\nfunc (s *Stream) write(m recvMsg) {\n\ts.buf.put(m)\n}\n\n// Read reads all p bytes from the wire for this stream.\nfunc (s *Stream) Read(p []byte) (n int, err error) {\n\t// Don't request a read if there was an error earlier\n\tif er := s.trReader.(*transportReader).er; er != nil {\n\t\treturn 0, er\n\t}\n\ts.requestRead(len(p))\n\treturn io.ReadFull(s.trReader, p)\n}\n\n// StreamWrite only used for unit test\nfunc StreamWrite(s *Stream, buffer *bytes.Buffer) {\n\ts.write(recvMsg{buffer: buffer})\n}\n\n// CreateStream only used for unit test. Create an independent stream out of http2client / http2server\nfunc CreateStream(id uint32, requestRead func(i int)) *Stream {\n\trecvBuffer := newRecvBuffer()\n\ttrReader := &transportReader{\n\t\treader: &recvBufferReader{\n\t\t\trecv: recvBuffer,\n\t\t\tfreeBuffer: func(buffer *bytes.Buffer) {\n\t\t\t\tbuffer.Reset()\n\t\t\t},\n\t\t},\n\t\twindowHandler: func(i int) {},\n\t}\n\n\tstream := &Stream{\n\t\tid:          id,\n\t\tbuf:         recvBuffer,\n\t\ttrReader:    trReader,\n\t\twq:          newWriteQuota(defaultWriteQuota, nil),\n\t\trequestRead: requestRead,\n\t\thdrMu:       sync.Mutex{},\n\t}\n\n\treturn stream\n}\n\n// transportReader reads all the data available for this Stream from the transport and\n// passes them into the decoder, which converts them into a gRPC message stream.\n// The error is io.EOF when the stream is done or another non-nil error if\n// the stream broke.\ntype transportReader struct {\n\treader io.Reader\n\t// The handler to control the window update procedure for both this\n\t// particular stream and the associated transport.\n\twindowHandler func(int)\n\ter            error\n}\n\nfunc (t *transportReader) Read(p []byte) (n int, err error) {\n\tn, err = t.reader.Read(p)\n\tif err != nil {\n\t\tt.er = err\n\t\treturn\n\t}\n\tt.windowHandler(n)\n\treturn\n}\n\n// BytesReceived indicates whether any bytes have been received on this stream.\nfunc (s *Stream) BytesReceived() bool {\n\treturn atomic.LoadUint32(&s.bytesReceived) == 1\n}\n\n// Unprocessed indicates whether the server did not process this stream --\n// i.e. it sent a refused stream or GOAWAY including this stream ID.\nfunc (s *Stream) Unprocessed() bool {\n\treturn atomic.LoadUint32(&s.unprocessed) == 1\n}\n\n// state of transport\ntype transportState int\n\nconst (\n\treachable transportState = iota\n\tclosing\n\tdraining\n)\n\n// ServerConfig consists of all the configurations to establish a server transport.\ntype ServerConfig struct {\n\tMaxStreams                 uint32\n\tKeepaliveParams            ServerKeepalive\n\tKeepaliveEnforcementPolicy EnforcementPolicy\n\tInitialWindowSize          uint32\n\tInitialConnWindowSize      uint32\n\tWriteBufferSize            uint32\n\tReadBufferSize             uint32\n\tMaxHeaderListSize          *uint32\n}\n\nfunc DefaultServerConfig() *ServerConfig {\n\treturn &ServerConfig{\n\t\tWriteBufferSize: defaultWriteBufferSize,\n\t\tReadBufferSize:  defaultReadBufferSize,\n\t}\n}\n\n// ConnectOptions covers all relevant options for communicating with the server.\ntype ConnectOptions struct {\n\t// KeepaliveParams stores the keepalive parameters.\n\tKeepaliveParams ClientKeepalive\n\t// InitialWindowSize sets the initial window size for a stream.\n\tInitialWindowSize uint32\n\t// InitialConnWindowSize sets the initial window size for a connection.\n\tInitialConnWindowSize uint32\n\t// WriteBufferSize sets the size of write buffer which in turn determines how much data can be batched before it's written on the wire.\n\tWriteBufferSize uint32\n\t// ReadBufferSize sets the size of read buffer, which in turn determines how much data can be read at most for one read syscall.\n\tReadBufferSize uint32\n\t// MaxHeaderListSize sets the max (uncompressed) size of header list that is prepared to be received.\n\tMaxHeaderListSize *uint32\n\t// ShortConn indicates whether the connection will be reused from grpc conn pool\n\tShortConn bool\n\t// TLSConfig\n\tTLSConfig *tls.Config\n}\n\n// NewServerTransport creates a ServerTransport with conn or non-nil error\n// if it fails.\nfunc NewServerTransport(ctx context.Context, conn net.Conn, cfg *ServerConfig) (ServerTransport, error) {\n\treturn newHTTP2Server(ctx, conn, cfg)\n}\n\n// NewClientTransport establishes the transport with the required ConnectOptions\n// and returns it to the caller.\nfunc NewClientTransport(ctx context.Context, conn net.Conn, opts ConnectOptions,\n\tremoteService string, onGoAway func(GoAwayReason), onClose func(),\n) (ClientTransport, error) {\n\treturn newHTTP2Client(ctx, conn, opts, remoteService, onGoAway, onClose)\n}\n\n// Options provides additional hints and information for message\n// transmission.\ntype Options struct {\n\t// Last indicates whether this write is the last piece for\n\t// this stream.\n\tLast bool\n}\n\n// CallHdr carries the information of a particular RPC.\ntype CallHdr struct {\n\t// Host specifies the peer's host.\n\tHost string\n\n\t// Method specifies the operation to perform.\n\tMethod string\n\n\t// SendCompress specifies the compression algorithm applied on\n\t// outbound message.\n\tSendCompress string\n\n\t// ContentSubtype specifies the content-subtype for a request. For example, a\n\t// content-subtype of \"proto\" will result in a content-type of\n\t// \"application/grpc+proto\". The value of ContentSubtype must be all\n\t// lowercase, otherwise the behavior is undefined. See\n\t// https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\n\t// for more details.\n\tContentSubtype string\n\n\tPreviousAttempts int // value of grpc-previous-rpc-attempts header to set\n}\n\n// IsActive is the interface that exposing the underlying connection's active status.\ntype IsActive interface {\n\tIsActive() bool\n}\n\n// ClientTransport is the common interface for all gRPC client-side transport\n// implementations.\ntype ClientTransport interface {\n\t// Close tears down this transport. Once it returns, the transport\n\t// should not be accessed any more. The caller must make sure this\n\t// is called only once.\n\tClose() error\n\n\t// GracefulClose starts to tear down\n```\n# Function Signature: func TLSConfig(tlsConfig *tls.Config) *tls.Config {\n# Natural Language Annotations: // TLSConfig checks and supplement the tls config provided by user.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc TLSConfig(tlsConfig *tls.Config) *tls.Config {\n    if tlsConfig == nil {\n        tlsConfig = &tls.Config{}\n    }\n    // Set time\n    if tlsConfig.Time == nil {\n        tlsConfig.Time = func(d time.Time) time.Duration {\n            return time.Until(d)\n        }\n    }\n    return tlsConfig\n}\n```", "  ```\nfunc TLSConfig(tlsConfig *tls.Config) *tls.Config {\n    if tlsConfig == nil {\n        tlsConfig = &tls.Config{}\n    }\n    // Set default TLS versions to 1.2 and 1.3 if the user does not specify otherwise.\n    if len(tlsConfig.MinVersion) == 0 {\n        tlsConfig.MinVersion = tls.VersionTLS12\n    }\n    if len(tlsConfig.MaxVersion) == 0 {\n        tlsConfig.MaxVersion = tls.VersionTLS13\n    }\n    // Add the default ciphersuite preferences if the user does not specify otherwise.\n    if len(tlsConfig.CipherSuites) == 0 {\n        tlsConfig.CipherSuites = defaultCipherSuites\n    }\n    return tlsConfig\n}\n```", "  ```\nfunc TLSConfig(tlsConfig *tls.Config) *tls.Config {\n\tif tlsConfig == nil {\n\t\ttlsConfig = &tls.Config{}\n\t}\n\tif tlsConfig.Time != nil && tlsConfig.Time().IsZero() {\n\t\t// If the time is not set, use current time.\n\t\ttlsConfig.Time = func() time.Time {\n\t\t\treturn time.Now()\n\t\t}\n\t}\n\tif tlsConfig.Rand == nil {\n\t\ttlsConfig.Rand = rand.Reader\n\t}\n\tif tlsConfig.ServerName == \"\" {\n\t\t// If the ServerName is not set, infer the ServerName\n\t\t// from the host parameter of the context.\n\t\ttlsConfig.ServerName = utils.GetHost(context.Context)\n\t}\n\treturn tlsConfig\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/grpc/http_util.go-encodeTimeout": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/grpc/http_util.go:\n```\n/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage grpc\n\nimport (\n\t\"bytes\"\n\t\"encoding/base64\"\n\t\"fmt\"\n\t\"math\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\t\"unicode/utf8\"\n\n\t\"golang.org/x/net/http2\"\n\t\"golang.org/x/net/http2/hpack\"\n\tspb \"google.golang.org/genproto/googleapis/rpc/status\"\n\t\"google.golang.org/protobuf/proto\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/codes\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc/grpcframe\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/status\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\nconst (\n\t// http2MaxFrameLen specifies the max length of a HTTP2 frame.\n\thttp2MaxFrameLen = 16384 // 16KB frame\n\t// http://http2.github.io/http2-spec/#SettingValues\n\thttp2InitHeaderTableSize = 4096\n\t// baseContentType is the base content-type for gRPC.  This is a valid\n\t// content-type on it's own, but can also include a content-subtype such as\n\t// \"proto\" as a suffix after \"+\" or \";\".  See\n\t// https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\n\t// for more details.\n\tbaseContentType = \"application/grpc\"\n)\n\nvar (\n\t// ClientPreface http2 preface message\n\tClientPreface = []byte(http2.ClientPreface)\n\t// ClientPrefaceLen preface length\n\tClientPrefaceLen = len(ClientPreface)\n\thttp2ErrConvTab  = map[http2.ErrCode]codes.Code{\n\t\thttp2.ErrCodeNo:                 codes.Internal,\n\t\thttp2.ErrCodeProtocol:           codes.Internal,\n\t\thttp2.ErrCodeInternal:           codes.Internal,\n\t\thttp2.ErrCodeFlowControl:        codes.ResourceExhausted,\n\t\thttp2.ErrCodeSettingsTimeout:    codes.Internal,\n\t\thttp2.ErrCodeStreamClosed:       codes.Internal,\n\t\thttp2.ErrCodeFrameSize:          codes.Internal,\n\t\thttp2.ErrCodeRefusedStream:      codes.Unavailable,\n\t\thttp2.ErrCodeCancel:             codes.Canceled,\n\t\thttp2.ErrCodeCompression:        codes.Internal,\n\t\thttp2.ErrCodeConnect:            codes.Internal,\n\t\thttp2.ErrCodeEnhanceYourCalm:    codes.ResourceExhausted,\n\t\thttp2.ErrCodeInadequateSecurity: codes.PermissionDenied,\n\t\thttp2.ErrCodeHTTP11Required:     codes.Internal,\n\t}\n\tstatusCodeConvTab = map[codes.Code]http2.ErrCode{\n\t\tcodes.Internal:          http2.ErrCodeInternal,\n\t\tcodes.Canceled:          http2.ErrCodeCancel,\n\t\tcodes.Unavailable:       http2.ErrCodeRefusedStream,\n\t\tcodes.ResourceExhausted: http2.ErrCodeEnhanceYourCalm,\n\t\tcodes.PermissionDenied:  http2.ErrCodeInadequateSecurity,\n\t}\n\t// HTTPStatusConvTab is the HTTP status code to gRPC error code conversion table.\n\tHTTPStatusConvTab = map[int]codes.Code{\n\t\t// 400 Bad Request - INTERNAL.\n\t\thttp.StatusBadRequest: codes.Internal,\n\t\t// 401 Unauthorized  - UNAUTHENTICATED.\n\t\thttp.StatusUnauthorized: codes.Unauthenticated,\n\t\t// 403 Forbidden - PERMISSION_DENIED.\n\t\thttp.StatusForbidden: codes.PermissionDenied,\n\t\t// 404 Not Found - UNIMPLEMENTED.\n\t\thttp.StatusNotFound: codes.Unimplemented,\n\t\t// 429 Too Many Requests - UNAVAILABLE.\n\t\thttp.StatusTooManyRequests: codes.Unavailable,\n\t\t// 502 Bad Gateway - UNAVAILABLE.\n\t\thttp.StatusBadGateway: codes.Unavailable,\n\t\t// 503 Service Unavailable - UNAVAILABLE.\n\t\thttp.StatusServiceUnavailable: codes.Unavailable,\n\t\t// 504 Gateway timeout - UNAVAILABLE.\n\t\thttp.StatusGatewayTimeout: codes.Unavailable,\n\t}\n)\n\ntype parsedHeaderData struct {\n\tencoding       string\n\tacceptEncoding string\n\t// statusGen caches the stream status received from the trailer the server\n\t// sent.  Client side only.  Do not access directly.  After all trailers are\n\t// parsed, use the status method to retrieve the status.\n\tstatusGen    *status.Status\n\tbizStatusErr kerrors.BizStatusErrorIface\n\t// rawStatusCode and rawStatusMsg are set from the raw trailer fields and are not\n\t// intended for direct access outside of parsing.\n\trawStatusCode  *int\n\trawStatusMsg   string\n\tbizStatusCode  *int\n\tbizStatusExtra map[string]string\n\thttpStatus     *int\n\t// Server side only fields.\n\ttimeoutSet bool\n\ttimeout    time.Duration\n\tmethod     string\n\t// key-value metadata map from the peer.\n\tmdata          map[string][]string\n\tstatsTags      []byte\n\tstatsTrace     []byte\n\tcontentSubtype string\n\n\t// isGRPC field indicates whether the peer is speaking gRPC (otherwise HTTP).\n\t//\n\t// We are in gRPC mode (peer speaking gRPC) if:\n\t// \t* We are client side and have already received a HEADER frame that indicates gRPC peer.\n\t//  * The header contains valid  a content-type, i.e. a string starts with \"application/grpc\"\n\t// And we should handle error specific to gRPC.\n\t//\n\t// Otherwise (i.e. a content-type string starts without \"application/grpc\", or does not exist), we\n\t// are in HTTP fallback mode, and should handle error specific to HTTP.\n\tisGRPC         bool\n\tgrpcErr        error\n\thttpErr        error\n\tcontentTypeErr string\n}\n\n// decodeState configures decoding criteria and records the decoded data.\ntype decodeState struct {\n\t// whether decoding on server side or not\n\tserverSide bool\n\n\t// Records the states during HPACK decoding. It will be filled with info parsed from HTTP HEADERS\n\t// frame once decodeHeader function has been invoked and returned.\n\tdata parsedHeaderData\n}\n\n// isReservedHeader checks whether hdr belongs to HTTP2 headers\n// reserved by gRPC protocol. Any other headers are classified as the\n// user-specified metadata.\nfunc isReservedHeader(hdr string) bool {\n\tif hdr != \"\" && hdr[0] == ':' {\n\t\treturn true\n\t}\n\tswitch hdr {\n\tcase \"content-type\",\n\t\t\"user-agent\",\n\t\t\"grpc-message-type\",\n\t\t\"grpc-encoding\",\n\t\t\"grpc-message\",\n\t\t\"grpc-status\",\n\t\t\"grpc-timeout\",\n\t\t\"grpc-status-details-bin\",\n\t\t// Intentionally exclude grpc-previous-rpc-attempts and\n\t\t// grpc-retry-pushback-ms, which are \"reserved\", but their API\n\t\t// intentionally works via metadata.\n\t\t\"te\":\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// isWhitelistedHeader checks whether hdr should be propagated into metadata\n// visible to users, even though it is classified as \"reserved\", above.\nfunc isWhitelistedHeader(hdr string) bool {\n\tswitch hdr {\n\tcase \":authority\", \"user-agent\":\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// contentSubtype returns the content-subtype for the given content-type.  The\n// given content-type must be a valid content-type that starts with\n// \"application/grpc\". A content-subtype will follow \"application/grpc\" after a\n// \"+\" or \";\". See\n// https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests for\n// more details.\n//\n// If contentType is not a valid content-type for gRPC, the boolean\n// will be false, otherwise true. If content-type == \"application/grpc\",\n// \"application/grpc+\", or \"application/grpc;\", the boolean will be true,\n// but no content-subtype will be returned.\n//\n// contentType is assumed to be lowercase already.\nfunc contentSubtype(contentType string) (string, bool) {\n\tif contentType == baseContentType {\n\t\treturn \"\", true\n\t}\n\tif !strings.HasPrefix(contentType, baseContentType) {\n\t\treturn \"\", false\n\t}\n\t// guaranteed since != baseContentType and has baseContentType prefix\n\tswitch contentType[len(baseContentType)] {\n\tcase '+', ';':\n\t\t// this will return true for \"application/grpc+\" or \"application/grpc;\"\n\t\t// which the previous validContentType function tested to be valid, so we\n\t\t// just say that no content-subtype is specified in this case\n\t\treturn contentType[len(baseContentType)+1:], true\n\tdefault:\n\t\treturn \"\", false\n\t}\n}\n\n// contentSubtype is assumed to be lowercase\nfunc contentType(contentSubtype string) string {\n\tif contentSubtype == \"\" {\n\t\treturn baseContentType\n\t}\n\treturn baseContentType + \"+\" + contentSubtype\n}\n\nfunc (d *decodeState) status() *status.Status {\n\tif d.data.statusGen == nil {\n\t\t// No status-details were provided; generate status using code/msg.\n\t\td.data.statusGen = status.New(codes.Code(safeCastInt32(*(d.data.rawStatusCode))), d.data.rawStatusMsg)\n\t}\n\treturn d.data.statusGen\n}\n\nfunc (d *decodeState) bizStatusErr() kerrors.BizStatusErrorIface {\n\tif d.data.bizStatusErr == nil && d.data.bizStatusCode != nil {\n\t\td.data.bizStatusErr = kerrors.NewGRPCBizStatusErrorWithExtra(\n\t\t\tsafeCastInt32(*(d.data.bizStatusCode)), d.data.rawStatusMsg, d.data.bizStatusExtra)\n\t\tif st, ok := d.data.bizStatusErr.(kerrors.GRPCStatusIface); ok {\n\t\t\tst.SetGRPCStatus(d.status())\n\t\t}\n\t}\n\treturn d.data.bizStatusErr\n}\n\n// safeCastInt32 casts the number from int to int32 in safety.\nfunc safeCastInt32(n int) int32 {\n\tif n > math.MaxInt32 || n < math.MinInt32 {\n\t\tpanic(fmt.Sprintf(\"Cast int to int32 failed, due to overflow, n=%d\", n))\n\t}\n\treturn int32(n)\n}\n\nconst binHdrSuffix = \"-bin\"\n\nfunc encodeBinHeader(v []byte) string {\n\treturn base64.RawStdEncoding.EncodeToString(v)\n}\n\nfunc decodeBinHeader(v string) ([]byte, error) {\n\tif len(v)%4 == 0 {\n\t\t// Input was padded, or padding was not necessary.\n\t\treturn base64.StdEncoding.DecodeString(v)\n\t}\n\treturn base64.RawStdEncoding.DecodeString(v)\n}\n\nfunc encodeMetadataHeader(k, v string) string {\n\tif strings.HasSuffix(k, binHdrSuffix) {\n\t\treturn encodeBinHeader(([]byte)(v))\n\t}\n\treturn v\n}\n\nfunc decodeMetadataHeader(k, v string) (string, error) {\n\tif strings.HasSuffix(k, binHdrSuffix) {\n\t\tb, err := decodeBinHeader(v)\n\t\treturn string(b), err\n\t}\n\treturn v, nil\n}\n\nfunc (d *decodeState) decodeHeader(frame *grpcframe.MetaHeadersFrame) error {\n\t// frame.Truncated is set to true when framer detects that the current header\n\t// list size hits MaxHeaderListSize limit.\n\tif frame.Truncated {\n\t\treturn status.New(codes.Internal, \"peer header list size exceeded limit\").Err()\n\t}\n\n\tfor _, hf := range frame.Fields {\n\t\td.processHeaderField(hf)\n\t}\n\n\tif d.data.isGRPC {\n\t\tif d.data.grpcErr != nil {\n\t\t\treturn d.data.grpcErr\n\t\t}\n\t\tif d.serverSide {\n\t\t\treturn nil\n\t\t}\n\t\tif d.data.rawStatusCode == nil && d.data.statusGen == nil {\n\t\t\t// gRPC status doesn't exist.\n\t\t\t// Set rawStatusCode to be unknown and return nil error.\n\t\t\t// So that, if the stream has ended this Unknown status\n\t\t\t// will be propagated to the user.\n\t\t\t// Otherwise, it will be ignored. In which case, status from\n\t\t\t// a later trailer, that has StreamEnded flag set, is propagated.\n\t\t\tcode := int(codes.Unknown)\n\t\t\td.data.rawStatusCode = &code\n\t\t}\n\t\treturn nil\n\t}\n\n\t// HTTP fallback mode\n\tif d.data.httpErr != nil {\n\t\treturn d.data.httpErr\n\t}\n\n\tvar (\n\t\tcode = codes.Internal // when header does not include HTTP status, return INTERNAL\n\t\tok   bool\n\t)\n\n\tif d.data.httpStatus != nil {\n\t\tcode, ok = HTTPStatusConvTab[*(d.data.httpStatus)]\n\t\tif !ok {\n\t\t\tcode = codes.Unknown\n\t\t}\n\t}\n\n\treturn status.New(code, d.constructHTTPErrMsg()).Err()\n}\n\n// constructErrMsg constructs error message to be returned in HTTP fallback mode.\n// Format: HTTP status code and its corresponding message + content-type error message.\nfunc (d *decodeState) constructHTTPErrMsg() string {\n\tvar errMsgs []string\n\n\tif d.data.httpStatus == nil {\n\t\terrMsgs = append(errMsgs, \"malformed header: missing HTTP status\")\n\t} else {\n\t\terrMsgs = append(errMsgs, fmt.Sprintf(\"%s: HTTP status code %d\", http.StatusText(*(d.data.httpStatus)), *d.data.httpStatus))\n\t}\n\n\tif d.data.contentTypeErr == \"\" {\n\t\terrMsgs = append(errMsgs, \"transport: missing content-type field\")\n\t} else {\n\t\terrMsgs = append(errMsgs, d.data.contentTypeErr)\n\t}\n\n\treturn strings.Join(errMsgs, \"; \")\n}\n\nfunc (d *decodeState) addMetadata(k, v string) {\n\tif d.data.mdata == nil {\n\t\td.data.mdata = make(map[string][]string)\n\t}\n\td.data.mdata[k] = append(d.data.mdata[k], v)\n}\n\nfunc (d *decodeState) processHeaderField(f hpack.HeaderField) {\n\tswitch f.Name {\n\tcase \"content-type\":\n\t\tcontentSubtype, validContentType := contentSubtype(f.Value)\n\t\tif !validContentType {\n\t\t\td.data.contentTypeErr = fmt.Sprintf(\"transport: received the unexpected content-type %q\", f.Value)\n\t\t\treturn\n\t\t}\n\t\td.data.contentSubtype = contentSubtype\n\t\t// TODO: do we want to propagate the whole content-type in the metadata,\n\t\t// or come up with a way to just propagate the content-subtype if it was set?\n\t\t// ie {\"content-type\": \"application/grpc+proto\"} or {\"content-subtype\": \"proto\"}\n\t\t// in the metadata?\n\t\td.addMetadata(f.Name, f.Value)\n\t\td.data.isGRPC = true\n\tcase \"grpc-encoding\":\n\t\td.data.encoding = f.Value\n\tcase \"grpc-accept-encoding\":\n\t\td.data.acceptEncoding = f.Value\n\tcase \"grpc-status\":\n\t\tcode, err := strconv.Atoi(f.Value)\n\t\tif err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed grpc-status: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.rawStatusCode = &code\n\tcase \"grpc-message\":\n\t\td.data.rawStatusMsg = decodeGrpcMessage(f.Value)\n\tcase \"biz-status\":\n\t\tcode, err := strconv.Atoi(f.Value)\n\t\tif err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed biz-status: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.bizStatusCode = &code\n\tcase \"biz-extra\":\n\t\textra, err := utils.JSONStr2Map(f.Value)\n\t\tif err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed biz-extra: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.bizStatusExtra = extra\n\tcase \"grpc-status-details-bin\":\n\t\tv, err := decodeBinHeader(f.Value)\n\t\tif err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed grpc-status-details-bin: %v\", err)\n\t\t\treturn\n\t\t}\n\t\ts := &spb.Status{}\n\t\tif err := proto.Unmarshal(v, s); err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed grpc-status-details-bin: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.statusGen = status.FromProto(s)\n\tcase \"grpc-timeout\":\n\t\td.data.timeoutSet = true\n\t\tvar err error\n\t\tif d.data.timeout, err = decodeTimeout(f.Value); err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed time-out: %v\", err)\n\t\t}\n\tcase \":path\":\n\t\td.data.method = f.Value\n\tcase \":status\":\n\t\tcode, err := strconv.Atoi(f.Value)\n\t\tif err != nil {\n\t\t\td.data.httpErr = status.Errorf(codes.Internal, \"transport: malformed http-status: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.httpStatus = &code\n\tcase \"grpc-tags-bin\":\n\t\tv, err := decodeBinHeader(f.Value)\n\t\tif err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed grpc-tags-bin: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.statsTags = v\n\t\td.addMetadata(f.Name, string(v))\n\tcase \"grpc-trace-bin\":\n\t\tv, err := decodeBinHeader(f.Value)\n\t\tif err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed grpc-trace-bin: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.statsTrace = v\n\t\td.addMetadata(f.Name, string(v))\n\tdefault:\n\t\tif isReservedHeader(f.Name) && !isWhitelistedHeader(f.Name) {\n\t\t\tbreak\n\t\t}\n\t\tv, err := decodeMetadataHeader(f.Name, f.Value)\n\t\tif err != nil {\n\t\t\tklog.Errorf(\"Failed to decode metadata header (%q, %q): %v\", f.Name, f.Value, err)\n\t\t\treturn\n\t\t}\n\t\td.addMetadata(f.Name, v)\n\t}\n}\n\ntype timeoutUnit uint8\n\nconst (\n\thour        timeoutUnit = 'H'\n\tminute      timeoutUnit = 'M'\n\tsecond      timeoutUnit = 'S'\n\tmillisecond timeoutUnit = 'm'\n\tmicrosecond timeoutUnit = 'u'\n\tnanosecond  timeoutUnit = 'n'\n)\n\nfunc timeoutUnitToDuration(u timeoutUnit) (d time.Duration, ok bool) {\n\tswitch u {\n\tcase hour:\n\t\treturn time.Hour, true\n\tcase minute:\n\t\treturn time.Minute, true\n\tcase second:\n\t\treturn time.Second, true\n\tcase millisecond:\n\t\treturn time.Millisecond, true\n\tcase microsecond:\n\t\treturn time.Microsecond, true\n\tcase nanosecond:\n\t\treturn time.Nanosecond, true\n\tdefault:\n\t}\n\treturn\n}\n\nconst maxTimeoutValue int64 = 100000000 - 1\n\n// div does integer division and round-up the result. Note that this is\n// equivalent to (d+r-1)/r but has less chance to overflow.\nfunc div(d, r time.Duration) int64 {\n\tif m := d % r; m > 0 {\n\t\treturn int64(d/r + 1)\n\t}\n\treturn int64(d / r)\n}\n\n// TODO(zhaoq): It is the simplistic and not bandwidth efficient. Improve it.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc decodeTimeout(s string) (time.Duration, error) {\n\tsize := len(s)\n\tif size < 2 {\n\t\treturn 0, fmt.Errorf(\"transport: timeout string is too short: %q\", s)\n\t}\n\tif size > 9 {\n\t\t// Spec allows for 8 digits plus the unit.\n\t\treturn 0, fmt.Errorf(\"transport: timeout string is too long: %q\", s)\n\t}\n\tunit := timeoutUnit(s[size-1])\n\td, ok := timeoutUnitToDuration(unit)\n\tif !ok {\n\t\treturn 0, fmt.Errorf(\"transport: timeout unit is not recognized: %q\", s)\n\t}\n\tt, err := strconv.ParseInt(s[:size-1], 10, 64)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tconst maxHours = math.MaxInt64 / int64(time.Hour)\n\tif d == time.Hour && t > maxHours {\n\t\t// This timeout would overflow math.MaxInt64; clamp it.\n\t\treturn time.Duration(math.MaxInt64), nil\n\t}\n\treturn d * time.Duration(t), nil\n}\n\nconst (\n\tspaceByte   = ' '\n\ttildeByte   = '~'\n\tpercentByte = '%'\n)\n\n// encodeGrpcMessage is used to encode status code in header field\n// \"grpc-message\". It does percent encoding and also replaces invalid utf-8\n// characters with Unicode replacement character.\n//\n// It checks to see if each individual byte in msg is an allowable byte, and\n// then either percent encoding or passing it through. When percent encoding,\n// the byte is converted into hexadecimal notation with a '%' prepended.\nfunc encodeGrpcMessage(msg string) string {\n\tif msg == \"\" {\n\t\treturn \"\"\n\t}\n\tlenMsg := len(msg)\n\tfor i := 0; i < lenMsg; i++ {\n\t\tc := msg[i]\n\t\tif !(c >= spaceByte && c <= tildeByte && c != percentByte) {\n\t\t\treturn encodeGrpcMessageUnchecked(msg)\n\t\t}\n\t}\n\treturn msg\n}\n\nfunc encodeGrpcMessageUnchecked(msg string) string {\n\tvar buf bytes.Buffer\n\tfor len(msg) > 0 {\n\t\tr, size := utf8.DecodeRuneInString(msg)\n\t\tfor _, b := range []byte(string(r)) {\n\t\t\tif size > 1 {\n\t\t\t\t// If size > 1, r is not ascii. Always do percent encoding.\n\t\t\t\tbuf.WriteString(fmt.Sprintf(\"%%%02X\", b))\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// The for loop is necessary even if size == 1. r could be\n\t\t\t// utf8.RuneError.\n\t\t\t//\n\t\t\t// fmt.Sprintf(\"%%%02X\", utf8.RuneError) gives \"%FFFD\".\n\t\t\tif b >= spaceByte && b <= tildeByte && b != percentByte {\n\t\t\t\tbuf.WriteByte(b)\n\t\t\t} else {\n\t\t\t\tbuf.WriteString(fmt.Sprintf(\"%%%02X\", b))\n\t\t\t}\n\t\t}\n\t\tmsg = msg[size:]\n\t}\n\treturn buf.String()\n}\n\n// decodeGrpcMessage decodes the msg encoded by encodeGrpcMessage.\nfunc decodeGrpcMessage(msg string) string {\n\tif msg == \"\" {\n\t\treturn \"\"\n\t}\n\tlenMsg := len(msg)\n\tfor i := 0; i < lenMsg; i++ {\n\t\tif msg[i] == percentByte && i+2 < lenMsg {\n\t\t\treturn decodeGrpcMessageUnchecked(msg)\n\t\t}\n\t}\n\treturn msg\n}\n\nfunc decodeGrpcMessageUnchecked(msg string) string {\n\tvar buf bytes.Buffer\n\tlenMsg := len(msg)\n\tfor i := 0; i < lenMsg\n```\n# Function Signature: func encodeTimeout(t time.Duration) string {\n# Natural Language Annotations: // TODO(zhaoq): It is the simplistic and not bandwidth efficient. Improve it.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc encodeTimeout(t time.Duration) string {\n\treturn strconv.FormatInt(int64(div(t, time.Hour)), 10) + \"H\" +\n\t\tstrconv.FormatInt(int64(div(t, time.Minute))%60, 10) + \"M\" +\n\t\tstrconv.FormatInt(int64(div(t, time.Second))%60, 10) + \"S\"\n}\n```", "  ```\nfunc encodeTimeout(t time.Duration) string {\n\ts := strconv.FormatInt(div(t, time.Hour), 10) + \"H\"\n\tt -= time.Duration(div(t, time.Hour)) * time.Hour\n\ts += strconv.FormatInt(div(t, time.Minute), 10) + \"M\"\n\tt -= time.Duration(div(t, time.Minute)) * time.Minute\n\ts += strconv.FormatInt(div(t, time.Second), 10) + \"S\"\n\tt -= time.Duration(div(t, time.Second)) * time.Second\n\ts += strconv.FormatInt(div(t, time.Millisecond), 10) + \"m\"\n\tt -= time.Duration(div(t, time.Millisecond)) * time.Millisecond\n\ts += strconv.FormatInt(div(t, time.Microsecond), 10) + \"u\"\n\tt -= time.Duration(div(t, time.Microsecond)) * time.Microsecond\n\ts += strconv.FormatInt(div(t, time.Nanosecond), 10) + \"n\"\n\treturn s\n}\n\n```", "  ```\nfunc encodeTimeout(t time.Duration) string {\n   if t <= 0 {\n      return \"0n\"\n   }\n   var (\n      buf [32]byte\n      n   int\n   )\n   d := uint64(t)\n   for {\n      buf[32-n-1] = \"0123456789\"[d%10]\n      d /= 10\n      if d == 0 {\n         break\n      }\n      n++\n   }\n   return string(buf[32-n:]) + string(byte(t.unit))\n}\n```"]}, "kitex-pkg/remote/trans/netpoll/bytebuf.go-AppendBuffer": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/netpoll/bytebuf.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage netpoll\n\nimport (\n\t\"errors\"\n\t\"io\"\n\t\"sync\"\n\n\t\"github.com/cloudwego/netpoll\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n)\n\nvar bytebufPool sync.Pool\n\nfunc init() {\n\tbytebufPool.New = newNetpollByteBuffer\n}\n\n// NewReaderByteBuffer creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReader.\nfunc NewReaderByteBuffer(r netpoll.Reader) remote.ByteBuffer {\n\tbytebuf := bytebufPool.Get().(*netpollByteBuffer)\n\tbytebuf.reader = r\n\t// TODO(wangtieju): fix me when netpoll support netpoll.Reader\n\t// and LinkBuffer not support io.Reader, type assertion would fail when r is from NewBuffer\n\tif ir, ok := r.(io.Reader); ok {\n\t\tbytebuf.ioReader = ir\n\t}\n\tbytebuf.status = remote.BitReadable\n\tbytebuf.readSize = 0\n\treturn bytebuf\n}\n\n// NewWriterByteBuffer creates a new remote.ByteBuffer using the given netpoll.ZeroCopyWriter.\nfunc NewWriterByteBuffer(w netpoll.Writer) remote.ByteBuffer {\n\tbytebuf := bytebufPool.Get().(*netpollByteBuffer)\n\tbytebuf.writer = w\n\t// TODO(wangtieju): fix me when netpoll support netpoll.Writer\n\t// and LinkBuffer not support io.Reader, type assertion would fail when w is from NewBuffer\n\tif iw, ok := w.(io.Writer); ok {\n\t\tbytebuf.ioWriter = iw\n\t}\n\tbytebuf.status = remote.BitWritable\n\treturn bytebuf\n}\n\n// NewReaderWriterByteBuffer creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReadWriter.\nfunc NewReaderWriterByteBuffer(rw netpoll.ReadWriter) remote.ByteBuffer {\n\tbytebuf := bytebufPool.Get().(*netpollByteBuffer)\n\tbytebuf.writer = rw\n\tbytebuf.reader = rw\n\t// TODO(wangtieju): fix me when netpoll support netpoll.ReadWriter\n\t// and LinkBuffer not support io.ReadWriter, type assertion would fail when rw is from NewBuffer\n\tif irw, ok := rw.(io.ReadWriter); ok {\n\t\tbytebuf.ioReader = irw\n\t\tbytebuf.ioWriter = irw\n\t}\n\tbytebuf.status = remote.BitWritable | remote.BitReadable\n\treturn bytebuf\n}\n\nfunc newNetpollByteBuffer() interface{} {\n\treturn &netpollByteBuffer{}\n}\n\ntype netpollByteBuffer struct {\n\twriter   netpoll.Writer\n\treader   netpoll.Reader\n\tioReader io.Reader\n\tioWriter io.Writer\n\tstatus   int\n\treadSize int\n}\n\nvar _ remote.ByteBuffer = &netpollByteBuffer{}\n\n// Next reads n bytes sequentially, returns the original address.\nfunc (b *netpollByteBuffer) Next(n int) (p []byte, err error) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn nil, errors.New(\"unreadable buffer, cannot support Next\")\n\t}\n\tif p, err = b.reader.Next(n); err == nil {\n\t\tb.readSize += n\n\t}\n\treturn\n}\n\n// Peek returns the next n bytes without advancing the reader.\nfunc (b *netpollByteBuffer) Peek(n int) (buf []byte, err error) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn nil, errors.New(\"unreadable buffer, cannot support Peek\")\n\t}\n\treturn b.reader.Peek(n)\n}\n\n// Skip is used to skip n bytes, it's much faster than Next.\n// Skip will not cause release.\nfunc (b *netpollByteBuffer) Skip(n int) (err error) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn errors.New(\"unreadable buffer, cannot support Skip\")\n\t}\n\treturn b.reader.Skip(n)\n}\n\n// ReadableLen returns the total length of readable buffer.\nfunc (b *netpollByteBuffer) ReadableLen() (n int) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn -1\n\t}\n\treturn b.reader.Len()\n}\n\n// Read implement io.Reader\nfunc (b *netpollByteBuffer) Read(p []byte) (n int, err error) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn -1, errors.New(\"unreadable buffer, cannot support Read\")\n\t}\n\tif b.ioReader != nil {\n\t\treturn b.ioReader.Read(p)\n\t}\n\treturn -1, errors.New(\"ioReader is nil\")\n}\n\n// ReadString is a more efficient way to read string than Next.\nfunc (b *netpollByteBuffer) ReadString(n int) (s string, err error) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn \"\", errors.New(\"unreadable buffer, cannot support ReadString\")\n\t}\n\tif s, err = b.reader.ReadString(n); err == nil {\n\t\tb.readSize += n\n\t}\n\treturn\n}\n\n// ReadBinary like ReadString.\n// Returns a copy of original buffer.\nfunc (b *netpollByteBuffer) ReadBinary(n int) (p []byte, err error) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn p, errors.New(\"unreadable buffer, cannot support ReadBinary\")\n\t}\n\tif p, err = b.reader.ReadBinary(n); err == nil {\n\t\tb.readSize += n\n\t}\n\treturn\n}\n\n// Malloc n bytes sequentially in the writer buffer.\nfunc (b *netpollByteBuffer) Malloc(n int) (buf []byte, err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn nil, errors.New(\"unwritable buffer, cannot support Malloc\")\n\t}\n\treturn b.writer.Malloc(n)\n}\n\n// MallocAck n bytes in the writer buffer.\nfunc (b *netpollByteBuffer) MallocAck(n int) (err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn errors.New(\"unwritable buffer, cannot support MallocAck\")\n\t}\n\treturn b.writer.MallocAck(n)\n}\n\n// MallocLen returns the total length of the buffer malloced.\nfunc (b *netpollByteBuffer) MallocLen() (length int) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn -1\n\t}\n\treturn b.writer.MallocLen()\n}\n\n// Write implement io.Writer\nfunc (b *netpollByteBuffer) Write(p []byte) (n int, err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support Write\")\n\t}\n\tif b.ioWriter != nil {\n\t\treturn b.ioWriter.Write(p)\n\t}\n\treturn -1, errors.New(\"ioWriter is nil\")\n}\n\n// WriteString is a more efficient way to write string, using the unsafe method to convert the string to []byte.\nfunc (b *netpollByteBuffer) WriteString(s string) (n int, err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support WriteString\")\n\t}\n\treturn b.writer.WriteString(s)\n}\n\n// WriteBinary writes the []byte directly. Callers must guarantee that the []byte doesn't change.\nfunc (b *netpollByteBuffer) WriteBinary(p []byte) (n int, err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support WriteBinary\")\n\t}\n\treturn b.writer.WriteBinary(p)\n}\n\n// WriteDirect is a way to write []byte without copying, and splits the original buffer.\nfunc (b *netpollByteBuffer) WriteDirect(p []byte, remainCap int) error {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn errors.New(\"unwritable buffer, cannot support WriteBinary\")\n\t}\n\treturn b.writer.WriteDirect(p, remainCap)\n}\n\n// ReadLen returns the size already read.\nfunc (b *netpollByteBuffer) ReadLen() (n int) {\n\treturn b.readSize\n}\n\n// Flush writes any malloc data to the underlying io.Writer.\n// The malloced buffer must be set correctly.\nfunc (b *netpollByteBuffer) Flush() (err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn errors.New(\"unwritable buffer, cannot support Flush\")\n\t}\n\treturn b.writer.Flush()\n}\n\n// NewBuffer returns a new writable remote.ByteBuffer.\nfunc (b *netpollByteBuffer) NewBuffer() remote.ByteBuffer {\n\treturn NewWriterByteBuffer(netpoll.NewLinkBuffer())\n}\n\n// AppendBuffer appends buf to the original buffer.\n\n\n\n\n\n\n\n// Bytes are not supported in netpoll bytebuf.\nfunc (b *netpollByteBuffer) Bytes() (buf []byte, err error) {\n\tif b.reader != nil {\n\t\treturn b.reader.Peek(b.reader.Len())\n\t}\n\treturn nil, errors.New(\"method Bytes() not support in netpoll bytebuf\")\n}\n\n// Release will free the buffer already read.\n// After release, buffer read by Next/Skip/Peek is invalid.\nfunc (b *netpollByteBuffer) Release(e error) (err error) {\n\tif b.reader != nil {\n\t\t// \u91cd\u590d\u6267\u884cRelease nil panic\n\t\terr = b.reader.Release()\n\t}\n\tb.zero()\n\tbytebufPool.Put(b)\n\treturn\n}\n\nfunc (b *netpollByteBuffer) zero() {\n\tb.writer = nil\n\tb.reader = nil\n\tb.ioReader = nil\n\tb.ioWriter = nil\n\tb.status = 0\n\tb.readSize = 0\n}\n\n```\n# Function Signature: func (b *netpollByteBuffer) AppendBuffer(buf remote.ByteBuffer) (err error) {\n# Natural Language Annotations: // AppendBuffer appends buf to the original buffer.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (b *netpollByteBuffer) AppendBuffer(buf remote.ByteBuffer) (err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn errors.New(\"unwritable buffer, cannot support AppendBuffer\")\n\t}\n\tif b.writer == nil {\n\t\treturn errors.New(\"writer is nil, cannot support AppendBuffer\")\n\t}\n\tb.writer.AppendBuffer(buf)\n\treturn nil\n}\n```", "  ```\nfunc (b *netpollByteBuffer) AppendBuffer(buf remote.ByteBuffer) (err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn errors.New(\"unwritable buffer, cannot support AppendBuffer\")\n\t}\n\tbuf2, ok := buf.(*netpollByteBuffer)\n\tif !ok {\n\t\treturn errors.New(\"buf is not netpollByteBuffer\")\n\t}\n\tif buf2.status&remote.BitReadable == 0 {\n\t\treturn errors.New(\"unreadable buffer, cannot support AppendBuffer\")\n\t}\n\terr = b.writer.WriteBuffer(buf2.reader)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbuf2.readSize = 0\n\treturn nil\n}\n```", "  ```\nfunc (b *netpollByteBuffer) AppendBuffer(buf remote.ByteBuffer) (err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn errors.New(\"unwritable buffer cannot support AppendBuffer\")\n\t}\n\tif newBuf, ok := buf.(*netpollByteBuffer); ok {\n\t\tif err = b.writer.SwapBuffer(newBuf.writer); err == nil {\n\t\t\tb.readSize += newBuf.readSize\n\t\t\tb.reader = newBuf.reader\n\t\t\tb.ioReader = newBuf.ioReader\n\t\t\tb.ioWriter = newBuf.ioWriter\n\t\t\tnewBuf.status = 0\n\t\t\tnewBuf.readSize = 0\n\t\t}\n\t} else {\n\t\terr = errors.New(\"input buffer is not a valid netpollByteBuffer\")\n\t}\n\treturn\n}\n```"]}, "kitex-pkg/remote/codec/util.go-SetOrCheckMethodName": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/util.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage codec\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\nconst (\n\t// FrontMask is used in protocol sniffing.\n\tFrontMask = 0x0000ffff\n)\n\n// SetOrCheckMethodName is used to set method name to invocation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// SetOrCheckSeqID is used to check the sequence ID.\nfunc SetOrCheckSeqID(seqID int32, message remote.Message) error {\n\tswitch message.MessageType() {\n\tcase remote.Call, remote.Oneway:\n\t\tif ink, ok := message.RPCInfo().Invocation().(rpcinfo.InvocationSetter); ok {\n\t\t\tink.SetSeqID(seqID)\n\t\t} else {\n\t\t\treturn errors.New(\"the interface Invocation doesn't implement InvocationSetter\")\n\t\t}\n\tcase remote.Reply:\n\t\texpectSeqID := message.RPCInfo().Invocation().SeqID()\n\t\tif expectSeqID != seqID {\n\t\t\tmethodName := message.RPCInfo().Invocation().MethodName()\n\t\t\treturn remote.NewTransErrorWithMsg(remote.BadSequenceID, fmt.Sprintf(\"method[%s] out of order sequence response, expect[%d], receive[%d]\", methodName, expectSeqID, seqID))\n\t\t}\n\tcase remote.Exception:\n\t\t// don't check, proxy may build Exception with seqID = 0\n\t\t// thrift 0.13 check seqID for Exception but thrift 0.9.2 doesn't check\n\t}\n\treturn nil\n}\n\n// UpdateMsgType updates msg type.\nfunc UpdateMsgType(msgType uint32, message remote.Message) error {\n\trpcRole := message.RPCRole()\n\tmt := remote.MessageType(msgType)\n\tif mt == message.MessageType() {\n\t\treturn nil\n\t}\n\tif rpcRole == remote.Server {\n\t\tif mt != remote.Call && mt != remote.Oneway && mt != remote.Stream {\n\t\t\treturn remote.NewTransErrorWithMsg(remote.InvalidMessageTypeException, fmt.Sprintf(\"server side, invalid message type %d\", mt))\n\t\t}\n\t} else {\n\t\tif mt != remote.Reply && mt != remote.Exception && mt != remote.Stream {\n\t\t\treturn remote.NewTransErrorWithMsg(remote.InvalidMessageTypeException, fmt.Sprintf(\"client side, invalid message type %d\", mt))\n\t\t}\n\t}\n\n\tmessage.SetMessageType(mt)\n\treturn nil\n}\n\n// NewDataIfNeeded is used to create the data if not exist.\nfunc NewDataIfNeeded(method string, message remote.Message) error {\n\tif message.Data() != nil {\n\t\treturn nil\n\t}\n\tif message.NewData(method) {\n\t\treturn nil\n\t}\n\treturn remote.NewTransErrorWithMsg(remote.InternalError, \"message data for codec is nil\")\n}\n\n```\n# Function Signature: func SetOrCheckMethodName(methodName string, message remote.Message) error {\n# Natural Language Annotations: // SetOrCheckMethodName is used to set method name to invocation.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc SetOrCheckMethodName(methodName string, message remote.Message) error {\n\tif ink, ok := message.RPCInfo().Invocation().(rpcinfo.InvocationSetter); ok {\n\t\tink.SetSeqID(seqID)\n\t} else {\n\t\treturn errors.New(\"the interface Invocation doesn't implement InvocationSetter\")\n\t}\n}\n```", "  ```\nfunc SetOrCheckMethodName(methodName string, message remote.Message) error {\n\tvar ink rpcinfo.Invocation\n\tswitch message.MessageType() {\n\tcase remote.Call, remote.Oneway:\n\t\tink, _ = message.RPCInfo().Invocation().(rpcinfo.InvocationSetter)\n\t\tif ink != nil {\n\t\t\tink.SetMethodName(methodName)\n\t\t} else {\n\t\t\treturn errors.New(\"the interface Invocation doesn't implement InvocationSetter\")\n\t\t}\n\tcase remote.Reply:\n\t\t// Don't check response method name\n\tcase remote.Exception:\n\t\t// Don't check response method name\n\tdefault:\n\t\treturn errors.New(\"invalid message type\")\n\t}\n\treturn nil\n}\n```", "  ```\nfunc SetOrCheckMethodName(methodName string, message remote.Message) error {\n    if methodName == \"\" {\n        return fmt.Errorf(\"invalid method name: %v\", methodName)\n    }\n    rpcrole := message.RPCRole()\n    if rpcrole == remote.Server {\n        var ink interface{} = nil\n        var ok bool\n        if ink, ok = message.RPCInfo().Invocation(); ok {\n            ink.(rpcinfo.InvocationSetter).SetMethodName(methodName)\n        } else {\n            return errors.New(\"the interface Invocation doesn't implement InvocationSetter\")\n        }\n    } else if rpcrole == remote.Client {\n        if methodName != message.RPCInfo().Invocation().MethodName() {\n            return remote.NewTransErrorWithMsg(remote.MethodMismatchException, fmt.Sprintf(\"method name doesn't match, expect: %s, got: %s\", message.RPCInfo().Invocation().MethodName(), methodName))\n        }\n    } else {\n        return fmt.Errorf(\"invalid RPC role: %v\", rpcrole)\n    }\n    return nil\n}\n\n```\nPlease note that I have removed some error handling and logging, which were not relevant to the specific function, but you can add them back as per your requirement."]}, "kitex-pkg/remote/codec/default_codec.go-Decode": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/default_codec.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage codec\n\nimport (\n\t\"context\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"sync/atomic\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// The byte count of 32 and 16 integer values.\nconst (\n\tSize32 = 4\n\tSize16 = 2\n)\n\nconst (\n\t// ThriftV1Magic is the magic code for thrift.VERSION_1\n\tThriftV1Magic = 0x80010000\n\t// ProtobufV1Magic is the magic code for kitex protobuf\n\tProtobufV1Magic = 0x90010000\n\n\t// MagicMask is bit mask for checking version.\n\tMagicMask = 0xffff0000\n)\n\nvar (\n\tttHeaderCodec   = ttHeader{}\n\tmeshHeaderCodec = meshHeader{}\n\n\t_ remote.Codec       = (*defaultCodec)(nil)\n\t_ remote.MetaDecoder = (*defaultCodec)(nil)\n)\n\n// NewDefaultCodec creates the default protocol sniffing codec supporting thrift and protobuf.\nfunc NewDefaultCodec() remote.Codec {\n\t// No size limit by default\n\treturn &defaultCodec{\n\t\tmaxSize: 0,\n\t}\n}\n\n// NewDefaultCodecWithSizeLimit creates the default protocol sniffing codec supporting thrift and protobuf but with size limit.\n// maxSize is in bytes\nfunc NewDefaultCodecWithSizeLimit(maxSize int) remote.Codec {\n\treturn &defaultCodec{\n\t\tmaxSize: maxSize,\n\t}\n}\n\ntype defaultCodec struct {\n\t// maxSize limits the max size of the payload\n\tmaxSize int\n}\n\n// EncodePayload encode payload\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// EncodeMetaAndPayload encode meta and payload\nfunc (c *defaultCodec) EncodeMetaAndPayload(ctx context.Context, message remote.Message, out remote.ByteBuffer, me remote.MetaEncoder) error {\n\tvar err error\n\tvar totalLenField []byte\n\ttp := message.ProtocolInfo().TransProto\n\n\t// 1. encode header and return totalLenField if needed\n\t// totalLenField will be filled after payload encoded\n\tif tp&transport.TTHeader == transport.TTHeader {\n\t\tif totalLenField, err = ttHeaderCodec.encode(ctx, message, out); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\t// 2. encode payload\n\tif err = me.EncodePayload(ctx, message, out); err != nil {\n\t\treturn err\n\t}\n\t// 3. fill totalLen field for header if needed\n\tif tp&transport.TTHeader == transport.TTHeader {\n\t\tif totalLenField == nil {\n\t\t\treturn perrors.NewProtocolErrorWithMsg(\"no buffer allocated for the header length field\")\n\t\t}\n\t\tpayloadLen := out.MallocLen() - Size32\n\t\tbinary.BigEndian.PutUint32(totalLenField, uint32(payloadLen))\n\t}\n\treturn nil\n}\n\n// Encode implements the remote.Codec interface, it does complete message encode include header and payload.\nfunc (c *defaultCodec) Encode(ctx context.Context, message remote.Message, out remote.ByteBuffer) (err error) {\n\treturn c.EncodeMetaAndPayload(ctx, message, out, c)\n}\n\n// DecodeMeta decode header\nfunc (c *defaultCodec) DecodeMeta(ctx context.Context, message remote.Message, in remote.ByteBuffer) (err error) {\n\tvar flagBuf []byte\n\tif flagBuf, err = in.Peek(2 * Size32); err != nil {\n\t\treturn perrors.NewProtocolErrorWithErrMsg(err, fmt.Sprintf(\"default codec read failed: %s\", err.Error()))\n\t}\n\n\tif err = checkRPCState(ctx, message); err != nil {\n\t\t// there is one call has finished in retry task, it doesn't need to do decode for this call\n\t\treturn err\n\t}\n\tisTTHeader := IsTTHeader(flagBuf)\n\t// 1. decode header\n\tif isTTHeader {\n\t\t// TTHeader\n\t\tif err = ttHeaderCodec.decode(ctx, message, in); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif flagBuf, err = in.Peek(2 * Size32); err != nil {\n\t\t\treturn perrors.NewProtocolErrorWithErrMsg(err, fmt.Sprintf(\"ttheader read payload first 8 byte failed: %s\", err.Error()))\n\t\t}\n\t} else if isMeshHeader(flagBuf) {\n\t\tmessage.Tags()[remote.MeshHeader] = true\n\t\t// MeshHeader\n\t\tif err = meshHeaderCodec.decode(ctx, message, in); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif flagBuf, err = in.Peek(2 * Size32); err != nil {\n\t\t\treturn perrors.NewProtocolErrorWithErrMsg(err, fmt.Sprintf(\"meshHeader read payload first 8 byte failed: %s\", err.Error()))\n\t\t}\n\t}\n\treturn checkPayload(flagBuf, message, in, isTTHeader, c.maxSize)\n}\n\n// DecodePayload decode payload\nfunc (c *defaultCodec) DecodePayload(ctx context.Context, message remote.Message, in remote.ByteBuffer) error {\n\tdefer func() {\n\t\tif ri := message.RPCInfo(); ri != nil {\n\t\t\tif ms := rpcinfo.AsMutableRPCStats(ri.Stats()); ms != nil {\n\t\t\t\tms.SetRecvSize(uint64(in.ReadLen()))\n\t\t\t}\n\t\t}\n\t}()\n\n\thasRead := in.ReadLen()\n\tpCodec, err := remote.GetPayloadCodec(message)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = pCodec.Unmarshal(ctx, message, in); err != nil {\n\t\treturn err\n\t}\n\tif message.PayloadLen() == 0 {\n\t\t// if protocol is PurePayload, should set payload length after decoded\n\t\tmessage.SetPayloadLen(in.ReadLen() - hasRead)\n\t}\n\treturn nil\n}\n\n// Decode implements the remote.Codec interface, it does complete message decode include header and payload.\n\n\n\n\n\n\n\n\n\n\nfunc (c *defaultCodec) Name() string {\n\treturn \"default\"\n}\n\n// Select to use thrift or protobuf according to the protocol.\nfunc (c *defaultCodec) encodePayload(ctx context.Context, message remote.Message, out remote.ByteBuffer) error {\n\tpCodec, err := remote.GetPayloadCodec(message)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn pCodec.Marshal(ctx, message, out)\n}\n\n/**\n * +------------------------------------------------------------+\n * |                  4Byte                 |       2Byte       |\n * +------------------------------------------------------------+\n * |   \t\t\t     Length\t\t\t    \t|   HEADER MAGIC    |\n * +------------------------------------------------------------+\n */\nfunc IsTTHeader(flagBuf []byte) bool {\n\treturn binary.BigEndian.Uint32(flagBuf[Size32:])&MagicMask == TTHeaderMagic\n}\n\n/**\n * +----------------------------------------+\n * |       2Byte        |       2Byte       |\n * +----------------------------------------+\n * |    HEADER MAGIC    |   HEADER SIZE     |\n * +----------------------------------------+\n */\nfunc isMeshHeader(flagBuf []byte) bool {\n\treturn binary.BigEndian.Uint32(flagBuf[:Size32])&MagicMask == MeshHeaderMagic\n}\n\n/**\n * Kitex protobuf has framed field\n * +------------------------------------------------------------+\n * |                  4Byte                 |       2Byte       |\n * +------------------------------------------------------------+\n * |   \t\t\t     Length\t\t\t    \t|   HEADER MAGIC    |\n * +------------------------------------------------------------+\n */\nfunc isProtobufKitex(flagBuf []byte) bool {\n\treturn binary.BigEndian.Uint32(flagBuf[Size32:])&MagicMask == ProtobufV1Magic\n}\n\n/**\n * +-------------------+\n * |       2Byte       |\n * +-------------------+\n * |   HEADER MAGIC    |\n * +-------------------\n */\nfunc isThriftBinary(flagBuf []byte) bool {\n\treturn binary.BigEndian.Uint32(flagBuf[:Size32])&MagicMask == ThriftV1Magic\n}\n\n/**\n * +------------------------------------------------------------+\n * |                  4Byte                 |       2Byte       |\n * +------------------------------------------------------------+\n * |   \t\t\t     Length\t\t\t    \t|   HEADER MAGIC    |\n * +------------------------------------------------------------+\n */\nfunc isThriftFramedBinary(flagBuf []byte) bool {\n\treturn binary.BigEndian.Uint32(flagBuf[Size32:])&MagicMask == ThriftV1Magic\n}\n\nfunc checkRPCState(ctx context.Context, message remote.Message) error {\n\tif message.RPCRole() == remote.Server {\n\t\treturn nil\n\t}\n\tif ctx.Err() == context.DeadlineExceeded || ctx.Err() == context.Canceled {\n\t\treturn kerrors.ErrRPCFinish\n\t}\n\tif respOp, ok := ctx.Value(retry.CtxRespOp).(*int32); ok {\n\t\tif !atomic.CompareAndSwapInt32(respOp, retry.OpNo, retry.OpDoing) {\n\t\t\t// previous call is being handling or done\n\t\t\t// this flag is used to check request status in retry(backup request) scene\n\t\t\treturn kerrors.ErrRPCFinish\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc checkPayload(flagBuf []byte, message remote.Message, in remote.ByteBuffer, isTTHeader bool, maxPayloadSize int) error {\n\tvar transProto transport.Protocol\n\tvar codecType serviceinfo.PayloadCodec\n\tif isThriftBinary(flagBuf) {\n\t\tcodecType = serviceinfo.Thrift\n\t\tif isTTHeader {\n\t\t\ttransProto = transport.TTHeader\n\t\t} else {\n\t\t\ttransProto = transport.PurePayload\n\t\t}\n\t} else if isThriftFramedBinary(flagBuf) {\n\t\tcodecType = serviceinfo.Thrift\n\t\tif isTTHeader {\n\t\t\ttransProto = transport.TTHeaderFramed\n\t\t} else {\n\t\t\ttransProto = transport.Framed\n\t\t}\n\t\tpayloadLen := binary.BigEndian.Uint32(flagBuf[:Size32])\n\t\tmessage.SetPayloadLen(int(payloadLen))\n\t\tif err := in.Skip(Size32); err != nil {\n\t\t\treturn err\n\t\t}\n\t} else if isProtobufKitex(flagBuf) {\n\t\tcodecType = serviceinfo.Protobuf\n\t\tif isTTHeader {\n\t\t\ttransProto = transport.TTHeaderFramed\n\t\t} else {\n\t\t\ttransProto = transport.Framed\n\t\t}\n\t\tpayloadLen := binary.BigEndian.Uint32(flagBuf[:Size32])\n\t\tmessage.SetPayloadLen(int(payloadLen))\n\t\tif err := in.Skip(Size32); err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\tfirst4Bytes := binary.BigEndian.Uint32(flagBuf[:Size32])\n\t\tsecond4Bytes := binary.BigEndian.Uint32(flagBuf[Size32:])\n\t\t// 0xfff4fffd is the interrupt message of telnet\n\t\terr := perrors.NewProtocolErrorWithMsg(fmt.Sprintf(\"invalid payload (first4Bytes=%#x, second4Bytes=%#x)\", first4Bytes, second4Bytes))\n\t\treturn err\n\t}\n\tif err := checkPayloadSize(message.PayloadLen(), maxPayloadSize); err != nil {\n\t\treturn err\n\t}\n\tmessage.SetProtocolInfo(remote.NewProtocolInfo(transProto, codecType))\n\tcfg := rpcinfo.AsMutableRPCConfig(message.RPCInfo().Config())\n\tif cfg != nil {\n\t\ttp := message.ProtocolInfo().TransProto\n\t\tcfg.SetTransportProtocol(tp)\n\t}\n\treturn nil\n}\n\nfunc checkPayloadSize(payloadLen, maxSize int) error {\n\tif maxSize > 0 && payloadLen > 0 && payloadLen > maxSize {\n\t\treturn perrors.NewProtocolErrorWithType(\n\t\t\tperrors.InvalidData,\n\t\t\tfmt.Sprintf(\"invalid data: payload size(%d) larger than the limit(%d)\", payloadLen, maxSize),\n\t\t)\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func (c *defaultCodec) Decode(ctx context.Context, message remote.Message, in remote.ByteBuffer) (err error) {\n# Natural Language Annotations: // Decode implements the remote.Codec interface, it does complete message decode include header and payload.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (c *defaultCodec) Decode(ctx context.Context, message remote.Message, in remote.ByteBuffer) (err error) {\n\tvar flagBuf []byte\n\tif flagBuf, err = in.Peek(2 * Size32); err != nil {\n\t\treturn perrors.NewProtocolErrorWithErrMsg(err, fmt.Sprintf(\"default codec read failed: %s\", err.Error()))\n\t}\n\n\tif err = checkRPCState(ctx, message); err != nil {\n\t\t// there is one call has finished in retry task, it doesn't need to do decode for this call\n\t\treturn err\n\t}\n\tisTTHeader := IsTTHeader(flagBuf)\n\t// 1. decode header\n\tif isTTHeader {\n\t\t// TTHeader\n\t\tif err = ttHeaderCodec.decode(ctx, message, in); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif flagBuf, err = in.Peek(2 * Size32); err != nil {\n\t\t\treturn perrors.NewProtocolErrorWithErrMsg(err, fmt.Sprintf(\"ttheader read payload first 8 byte failed: %s\", err.Error()))\n\t\t}\n\t} else if isMeshHeader(flagBuf) {\n\t\tmessage.Tags()[remote.MeshHeader] = true\n\t\t// MeshHeader\n\t\tif err = meshHeaderCodec.decode(ctx, message, in); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif flagBuf, err = in.Peek(2 * Size32); err != nil {\n\t\t\treturn perrors.NewProtocolErrorWithErrMsg(err, fmt.Sprintf(\"meshHeader read payload first 8 byte failed: %s\", err.Error()))\n\t\t}\n\t}\n\treturn checkPayload(flagBuf, message, in, isTTHeader, c.maxSize)\n}\n```", "  ```\nfunc (c *defaultCodec) Decode(ctx context.Context, message remote.Message, in remote.ByteBuffer) (err error) {\n    // 1. Decode the header\n    var flagBuf []byte\n    if flagBuf, err = in.Peek(2 * Size32); err != nil {\n        return perrors.NewProtocolErrorWithErrMsg(err, fmt.Sprintf(\"default codec read failed: %s\", err.Error()))\n    }\n\n    if err = checkRPCState(ctx, message); err != nil {\n        // there is one call has finished in retry task, it doesn't need to do decode for this call\n        return err\n    }\n    isTTHeader := IsTTHeader(flagBuf)\n\n    // 2. Decode the payload\n    if !isTTHeader {\n        return c.DecodeMetaAndPayload(ctx, message, in)\n    }\n\n    if err = ttHeaderCodec.decode(ctx, message, in); err != nil {\n        return err\n    }\n    if flagBuf, err = in.Peek(2 * Size32); err != nil {\n        return perrors.NewProtocolErrorWithErrMsg(err, fmt.Sprintf(\"ttheader read payload first 8 byte failed: %s\", err.Error()))\n    }\n\n    return c.DecodeMetaAndPayload(ctx, message, in)\n}\n```", "  ```\nfunc (c *defaultCodec) Decode(ctx context.Context, message remote.Message, in remote.ByteBuffer) (err error) {\n\tvar flagBuf []byte\n\tif flagBuf, err = in.Peek(2 * Size32); err != nil {\n\t\treturn perrors.NewProtocolErrorWithErrMsg(err, fmt.Sprintf(\"default codec read failed: %s\", err.Error()))\n\t}\n\n\t// 1. decode header \n\tif err = ttHeaderCodec.decode(ctx, message, in); err != nil {\n\t\treturn err\n\t}\n\t// 2. set payload\n\tif message.PayloadLen() == 0 {\n\t\tmessage.SetPayloadLen(in.ReadLen())\n\t}\n\tif message.RPCInfo().RPCRole() == remote.Server {\n\t\treturn nil\n\t}\n\t// 3. parse payload\n\tif message.ProtocolInfo().ContentType() == remote.ContentTypeThrift {\n\t\tmessage.SetPayload(thrift.NewTTransport(in))\n\t} else {\n\t\treturn nil\n\t}\n\treturn nil\n}\n```"]}, "kitex-pkg/remote/codec/thrift/binary_protocol.go-WriteMessageBegin": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/thrift/binary_protocol.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"encoding/binary\"\n\t\"math\"\n\t\"sync\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\n// must be strict read & strict write\nvar (\n\tbpPool sync.Pool\n\t_      thrift.TProtocol = (*BinaryProtocol)(nil)\n)\n\nfunc init() {\n\tbpPool.New = newBP\n}\n\nfunc newBP() interface{} {\n\treturn &BinaryProtocol{}\n}\n\n// NewBinaryProtocol ...\nfunc NewBinaryProtocol(t remote.ByteBuffer) *BinaryProtocol {\n\tbp := bpPool.Get().(*BinaryProtocol)\n\tbp.trans = t\n\treturn bp\n}\n\n// BinaryProtocol ...\ntype BinaryProtocol struct {\n\ttrans remote.ByteBuffer\n}\n\n// Recycle ...\nfunc (p *BinaryProtocol) Recycle() {\n\tp.trans = nil\n\tbpPool.Put(p)\n}\n\n/**\n * Writing Methods\n */\n\n// WriteMessageBegin ...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WriteMessageEnd ...\nfunc (p *BinaryProtocol) WriteMessageEnd() error {\n\treturn nil\n}\n\n// WriteStructBegin ...\nfunc (p *BinaryProtocol) WriteStructBegin(name string) error {\n\treturn nil\n}\n\n// WriteStructEnd ...\nfunc (p *BinaryProtocol) WriteStructEnd() error {\n\treturn nil\n}\n\n// WriteFieldBegin ...\nfunc (p *BinaryProtocol) WriteFieldBegin(name string, typeID thrift.TType, id int16) error {\n\te := p.WriteByte(int8(typeID))\n\tif e != nil {\n\t\treturn e\n\t}\n\te = p.WriteI16(id)\n\treturn e\n}\n\n// WriteFieldEnd ...\nfunc (p *BinaryProtocol) WriteFieldEnd() error {\n\treturn nil\n}\n\n// WriteFieldStop ...\nfunc (p *BinaryProtocol) WriteFieldStop() error {\n\te := p.WriteByte(thrift.STOP)\n\treturn e\n}\n\n// WriteMapBegin ...\nfunc (p *BinaryProtocol) WriteMapBegin(keyType, valueType thrift.TType, size int) error {\n\te := p.WriteByte(int8(keyType))\n\tif e != nil {\n\t\treturn e\n\t}\n\te = p.WriteByte(int8(valueType))\n\tif e != nil {\n\t\treturn e\n\t}\n\te = p.WriteI32(int32(size))\n\treturn e\n}\n\n// WriteMapEnd ...\nfunc (p *BinaryProtocol) WriteMapEnd() error {\n\treturn nil\n}\n\n// WriteListBegin ...\nfunc (p *BinaryProtocol) WriteListBegin(elemType thrift.TType, size int) error {\n\te := p.WriteByte(int8(elemType))\n\tif e != nil {\n\t\treturn e\n\t}\n\te = p.WriteI32(int32(size))\n\treturn e\n}\n\n// WriteListEnd ...\nfunc (p *BinaryProtocol) WriteListEnd() error {\n\treturn nil\n}\n\n// WriteSetBegin ...\nfunc (p *BinaryProtocol) WriteSetBegin(elemType thrift.TType, size int) error {\n\te := p.WriteByte(int8(elemType))\n\tif e != nil {\n\t\treturn e\n\t}\n\te = p.WriteI32(int32(size))\n\treturn e\n}\n\n// WriteSetEnd ...\nfunc (p *BinaryProtocol) WriteSetEnd() error {\n\treturn nil\n}\n\n// WriteBool ...\nfunc (p *BinaryProtocol) WriteBool(value bool) error {\n\tif value {\n\t\treturn p.WriteByte(1)\n\t}\n\treturn p.WriteByte(0)\n}\n\n// WriteByte ...\nfunc (p *BinaryProtocol) WriteByte(value int8) error {\n\tv, err := p.malloc(1)\n\tif err != nil {\n\t\treturn err\n\t}\n\tv[0] = byte(value)\n\treturn err\n}\n\n// WriteI16 ...\nfunc (p *BinaryProtocol) WriteI16(value int16) error {\n\tv, err := p.malloc(2)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbinary.BigEndian.PutUint16(v, uint16(value))\n\treturn err\n}\n\n// WriteI32 ...\nfunc (p *BinaryProtocol) WriteI32(value int32) error {\n\tv, err := p.malloc(4)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbinary.BigEndian.PutUint32(v, uint32(value))\n\treturn err\n}\n\n// WriteI64 ...\nfunc (p *BinaryProtocol) WriteI64(value int64) error {\n\tv, err := p.malloc(8)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbinary.BigEndian.PutUint64(v, uint64(value))\n\treturn err\n}\n\n// WriteDouble ...\nfunc (p *BinaryProtocol) WriteDouble(value float64) error {\n\treturn p.WriteI64(int64(math.Float64bits(value)))\n}\n\n// WriteString ...\nfunc (p *BinaryProtocol) WriteString(value string) error {\n\tlen := len(value)\n\te := p.WriteI32(int32(len))\n\tif e != nil {\n\t\treturn e\n\t}\n\t_, e = p.trans.WriteString(value)\n\treturn e\n}\n\n// WriteBinary ...\nfunc (p *BinaryProtocol) WriteBinary(value []byte) error {\n\te := p.WriteI32(int32(len(value)))\n\tif e != nil {\n\t\treturn e\n\t}\n\t_, e = p.trans.WriteBinary(value)\n\treturn e\n}\n\n// malloc ...\nfunc (p *BinaryProtocol) malloc(size int) ([]byte, error) {\n\tbuf, err := p.trans.Malloc(size)\n\tif err != nil {\n\t\treturn buf, perrors.NewProtocolError(err)\n\t}\n\treturn buf, nil\n}\n\n/**\n * Reading methods\n */\n\n// ReadMessageBegin ...\nfunc (p *BinaryProtocol) ReadMessageBegin() (name string, typeID thrift.TMessageType, seqID int32, err error) {\n\tsize, e := p.ReadI32()\n\tif e != nil {\n\t\treturn \"\", typeID, 0, perrors.NewProtocolError(e)\n\t}\n\tif size > 0 {\n\t\treturn name, typeID, seqID, perrors.NewProtocolErrorWithType(perrors.BadVersion, \"Missing version in ReadMessageBegin\")\n\t}\n\ttypeID = thrift.TMessageType(size & 0x0ff)\n\tversion := int64(int64(size) & thrift.VERSION_MASK)\n\tif version != thrift.VERSION_1 {\n\t\treturn name, typeID, seqID, perrors.NewProtocolErrorWithType(perrors.BadVersion, \"Bad version in ReadMessageBegin\")\n\t}\n\tname, e = p.ReadString()\n\tif e != nil {\n\t\treturn name, typeID, seqID, perrors.NewProtocolError(e)\n\t}\n\tseqID, e = p.ReadI32()\n\tif e != nil {\n\t\treturn name, typeID, seqID, perrors.NewProtocolError(e)\n\t}\n\treturn name, typeID, seqID, nil\n}\n\n// ReadMessageEnd ...\nfunc (p *BinaryProtocol) ReadMessageEnd() error {\n\treturn nil\n}\n\n// ReadStructBegin ...\nfunc (p *BinaryProtocol) ReadStructBegin() (name string, err error) {\n\treturn\n}\n\n// ReadStructEnd ...\nfunc (p *BinaryProtocol) ReadStructEnd() error {\n\treturn nil\n}\n\n// ReadFieldBegin ...\nfunc (p *BinaryProtocol) ReadFieldBegin() (name string, typeID thrift.TType, id int16, err error) {\n\tt, err := p.ReadByte()\n\ttypeID = thrift.TType(t)\n\tif err != nil {\n\t\treturn name, typeID, id, err\n\t}\n\tif t != thrift.STOP {\n\t\tid, err = p.ReadI16()\n\t}\n\treturn name, typeID, id, err\n}\n\n// ReadFieldEnd ...\nfunc (p *BinaryProtocol) ReadFieldEnd() error {\n\treturn nil\n}\n\n// ReadMapBegin ...\nfunc (p *BinaryProtocol) ReadMapBegin() (kType, vType thrift.TType, size int, err error) {\n\tk, e := p.ReadByte()\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tkType = thrift.TType(k)\n\tv, e := p.ReadByte()\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tvType = thrift.TType(v)\n\tsize32, e := p.ReadI32()\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tif size32 < 0 {\n\t\terr = perrors.InvalidDataLength\n\t\treturn\n\t}\n\tsize = int(size32)\n\treturn kType, vType, size, nil\n}\n\n// ReadMapEnd ...\nfunc (p *BinaryProtocol) ReadMapEnd() error {\n\treturn nil\n}\n\n// ReadListBegin ...\nfunc (p *BinaryProtocol) ReadListBegin() (elemType thrift.TType, size int, err error) {\n\tb, e := p.ReadByte()\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\telemType = thrift.TType(b)\n\tsize32, e := p.ReadI32()\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tif size32 < 0 {\n\t\terr = perrors.InvalidDataLength\n\t\treturn\n\t}\n\tsize = int(size32)\n\n\treturn\n}\n\n// ReadListEnd ...\nfunc (p *BinaryProtocol) ReadListEnd() error {\n\treturn nil\n}\n\n// ReadSetBegin ...\nfunc (p *BinaryProtocol) ReadSetBegin() (elemType thrift.TType, size int, err error) {\n\tb, e := p.ReadByte()\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\telemType = thrift.TType(b)\n\tsize32, e := p.ReadI32()\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tif size32 < 0 {\n\t\terr = perrors.InvalidDataLength\n\t\treturn\n\t}\n\tsize = int(size32)\n\treturn elemType, size, nil\n}\n\n// ReadSetEnd ...\nfunc (p *BinaryProtocol) ReadSetEnd() error {\n\treturn nil\n}\n\n// ReadBool ...\nfunc (p *BinaryProtocol) ReadBool() (bool, error) {\n\tb, e := p.ReadByte()\n\tv := true\n\tif b != 1 {\n\t\tv = false\n\t}\n\treturn v, e\n}\n\n// ReadByte ...\nfunc (p *BinaryProtocol) ReadByte() (value int8, err error) {\n\tbuf, err := p.next(1)\n\tif err != nil {\n\t\treturn value, err\n\t}\n\treturn int8(buf[0]), err\n}\n\n// ReadI16 ...\nfunc (p *BinaryProtocol) ReadI16() (value int16, err error) {\n\tbuf, err := p.next(2)\n\tif err != nil {\n\t\treturn value, err\n\t}\n\tvalue = int16(binary.BigEndian.Uint16(buf))\n\treturn value, err\n}\n\n// ReadI32 ...\nfunc (p *BinaryProtocol) ReadI32() (value int32, err error) {\n\tbuf, err := p.next(4)\n\tif err != nil {\n\t\treturn value, err\n\t}\n\tvalue = int32(binary.BigEndian.Uint32(buf))\n\treturn value, err\n}\n\n// ReadI64 ...\nfunc (p *BinaryProtocol) ReadI64() (value int64, err error) {\n\tbuf, err := p.next(8)\n\tif err != nil {\n\t\treturn value, err\n\t}\n\tvalue = int64(binary.BigEndian.Uint64(buf))\n\treturn value, err\n}\n\n// ReadDouble ...\nfunc (p *BinaryProtocol) ReadDouble() (value float64, err error) {\n\tbuf, err := p.next(8)\n\tif err != nil {\n\t\treturn value, err\n\t}\n\tvalue = math.Float64frombits(binary.BigEndian.Uint64(buf))\n\treturn value, err\n}\n\n// ReadString ...\nfunc (p *BinaryProtocol) ReadString() (value string, err error) {\n\tsize, e := p.ReadI32()\n\tif e != nil {\n\t\treturn \"\", e\n\t}\n\tif size < 0 {\n\t\terr = perrors.InvalidDataLength\n\t\treturn\n\t}\n\tvalue, err = p.trans.ReadString(int(size))\n\tif err != nil {\n\t\treturn value, perrors.NewProtocolError(err)\n\t}\n\treturn value, nil\n}\n\n// ReadBinary ...\nfunc (p *BinaryProtocol) ReadBinary() ([]byte, error) {\n\tsize, e := p.ReadI32()\n\tif e != nil {\n\t\treturn nil, e\n\t}\n\tif size < 0 {\n\t\treturn nil, perrors.InvalidDataLength\n\t}\n\treturn p.trans.ReadBinary(int(size))\n}\n\n// Flush ...\nfunc (p *BinaryProtocol) Flush(ctx context.Context) (err error) {\n\terr = p.trans.Flush()\n\tif err != nil {\n\t\treturn perrors.NewProtocolError(err)\n\t}\n\treturn nil\n}\n\n// Skip ...\nfunc (p *BinaryProtocol) Skip(fieldType thrift.TType) (err error) {\n\treturn thrift.SkipDefaultDepth(p, fieldType)\n}\n\n// Transport ...\nfunc (p *BinaryProtocol) Transport() thrift.TTransport {\n\t// not support\n\treturn nil\n}\n\n// ByteBuffer ...\nfunc (p *BinaryProtocol) ByteBuffer() remote.ByteBuffer {\n\treturn p.trans\n}\n\n// next ...\nfunc (p *BinaryProtocol) next(size int) ([]byte, error) {\n\tbuf, err := p.trans.Next(size)\n\tif err != nil {\n\t\treturn buf, perrors.NewProtocolError(err)\n\t}\n\treturn buf, nil\n}\n\n```\n# Function Signature: func (p *BinaryProtocol) WriteMessageBegin(name string, typeID thrift.TMessageType, seqID int32) error {\n# Natural Language Annotations: /**\n * Writing Methods\n */\n// WriteMessageBegin ...\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (p *BinaryProtocol) WriteMessageBegin(name string, typeID thrift.TMessageType, seqID int32) error {\n\te := p.WriteByte(int8(typeID))\n\tif e != nil {\n\t\treturn e\n\t}\n\te = p.WriteString(name)\n\tif e != nil {\n\t\treturn e\n\t}\n\te = p.WriteI32(seqID)\n\treturn e\n}\n```", "  ```\nfunc (p *BinaryProtocol) WriteMessageBegin(name string, typeID thrift.TMessageType, seqID int32) error {\n    e := p.WriteByte(int8(typeID))\n    if e != nil {\n        return e\n    }\n    _, e = p.WriteString(name)\n    if e != nil {\n        return e\n    }\n    e = p.WriteI32(seqID)\n    return e\n}\n```", "  ```\nfunc (p *BinaryProtocol) WriteMessageBegin(name string, typeID thrift.TMessageType, seqID int32) error {\n\tsize := -int32(int32(typeID) << thrift.VERSION_SHIFT)\n\te := p.WriteI32(size)\n\tif e != nil {\n\t\treturn perrors.Wrapf(e, \"failed to write message size\")\n\t}\n\te = p.WriteString(name)\n\tif e != nil {\n\t\treturn perrors.Wrapf(e, \"failed to write message name\")\n\t}\n\treturn p.WriteI32(seqID)\n}\n```"]}, "kitex-pkg/remote/codec/thrift/thrift_data.go-MarshalThriftData": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/thrift/thrift_data.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/bytedance/gopkg/lang/mcache\"\n\n\t\"github.com/cloudwego/kitex/pkg/protocol/bthrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\nconst marshalThriftBufferSize = 1024\n\n// MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\n\n\n\n\n\n\n\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It will allocate a new buffer and encode to it\nfunc (c thriftCodec) marshalThriftData(ctx context.Context, data interface{}) ([]byte, error) {\n\t// encode with hyper codec\n\t// NOTE: to ensure hyperMarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif c.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn c.hyperMarshalBody(data)\n\t}\n\n\t// encode with FastWrite\n\tif c.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif c.CodecType != Basic && hyperMarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn c.hyperMarshalBody(data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n\n// verifyMarshalBasicThriftDataType verifies whether data could be marshaled by old thrift way\nfunc verifyMarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageWriter:\n\tcase MessageWriterWithContext:\n\tdefault:\n\t\treturn errEncodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) fastMessageUnmarshalEnabled() bool {\n\treturn c.CodecType&FastRead != 0\n}\n\nfunc (c thriftCodec) fastMessageUnmarshalAvailable(data interface{}, payloadLen int) bool {\n\tif payloadLen == 0 && c.CodecType&EnableSkipDecoder == 0 {\n\t\treturn false\n\t}\n\t_, ok := data.(ThriftMsgFastCodec)\n\treturn ok\n}\n\nfunc (c thriftCodec) fastUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tmsg := data.(ThriftMsgFastCodec)\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\t_, err = msg.FastRead(buf)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = msg.FastRead(buf)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in FastCodec using SkipDecoder Buffer\")\n\t}\n\treturn err\n}\n\n// unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) hyperUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen - bthrift.Binary.MessageEndLength())\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in Frugal using SkipDecoder Buffer\")\n\t}\n\n\treturn nil\n}\n\n// verifyUnmarshalBasicThriftDataType verifies whether data could be unmarshal by old thrift way\nfunc verifyUnmarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageReader:\n\tcase MessageReaderWithMethodWithContext:\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// decodeBasicThriftData decode thrift body the old way (slow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getSkippedStructBuffer(tProt *BinaryProtocol) ([]byte, error) {\n\tsd := skipDecoder{ByteBuffer: tProt.trans}\n\tbuf, err := sd.NextStruct()\n\tif err != nil {\n\t\treturn nil, remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in SkipDecoder NextStruct phase\")\n\t}\n\treturn buf, nil\n}\n\n```\n# Function Signature: func MarshalThriftData(ctx context.Context, codec remote.PayloadCodec, data interface{}) ([]byte, error) {\n# Natural Language Annotations: // MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc MarshalThriftData(ctx context.Context, codec remote.PayloadCodec, data interface{}) ([]byte, error) {\n\t// encode with hyper codec\n\t// NOTE: to ensure hyperMarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif codec.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn codec.hyperMarshalBody(data)\n\t}\n\n\t// encode with FastWrite\n\tif codec.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif codec.CodecType != Basic && hyperMarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn codec.hyperMarshalBody(data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n```", "  ```\nfunc MarshalThriftData(ctx context.Context, codec remote.PayloadCodec, data interface{}) ([]byte, error) {\n\tif codec.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn codec.hyperMarshalBody(data)\n\t}\n\n\tif codec.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\tif codec.CodecType != Basic && hyperMarshalAvailable(data) {\n\t\t\treturn codec.hyperMarshalBody(data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n```", "  ```\nfunc MarshalThriftData(ctx context.Context, codec remote.PayloadCodec, data interface{}) ([]byte, error) {\n\tc, ok := codec.(thriftCodec)\n\tif !ok {\n\t\treturn nil, perrors.New(\"codec does not support Thrift encoding\")\n\t}\n\tvar err error\n\tvar buf []byte\n\tif c.HyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\tbuf, err = c.hyperMarshalBody(data)\n\t} else if c.CodecType&FastWrite != 0 && marshalBasicThriftDataType(data) {\n\t\tpayloadSize := data.(ThriftMsgFastCodec).BLength()\n\t\tpayload := mcache.Malloc(payloadSize)\n\t\tdata.(ThriftMsgFastCodec).FastWrite(payload)\n\t\tbuf = payload\n\t} else if c.CodecType&Basic != 0 {\n\t\tif c.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\t\tbuf, err = c.hyperMarshalBody(data)\n\t\t} else {\n\t\t\t// fallback to old thrift way (slow)\n\t\t\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\t\t\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\t\t\terr = marshalBasicThriftData(ctx, tProt, data)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tbuf = transport.Bytes()\n\t\t}\n\t} else {\n\t\treturn nil, perrors.New(\"unknown error when marshalling thrift data\")\n\t}\n\treturn buf, err\n}\n```"]}, "kitex-pkg/remote/codec/thrift/thrift_data.go-marshalBasicThriftData": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/thrift/thrift_data.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/bytedance/gopkg/lang/mcache\"\n\n\t\"github.com/cloudwego/kitex/pkg/protocol/bthrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\nconst marshalThriftBufferSize = 1024\n\n// MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\n\n\n\n\n\n\n\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It will allocate a new buffer and encode to it\nfunc (c thriftCodec) marshalThriftData(ctx context.Context, data interface{}) ([]byte, error) {\n\t// encode with hyper codec\n\t// NOTE: to ensure hyperMarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif c.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn c.hyperMarshalBody(data)\n\t}\n\n\t// encode with FastWrite\n\tif c.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif c.CodecType != Basic && hyperMarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn c.hyperMarshalBody(data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n\n// verifyMarshalBasicThriftDataType verifies whether data could be marshaled by old thrift way\nfunc verifyMarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageWriter:\n\tcase MessageWriterWithContext:\n\tdefault:\n\t\treturn errEncodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) fastMessageUnmarshalEnabled() bool {\n\treturn c.CodecType&FastRead != 0\n}\n\nfunc (c thriftCodec) fastMessageUnmarshalAvailable(data interface{}, payloadLen int) bool {\n\tif payloadLen == 0 && c.CodecType&EnableSkipDecoder == 0 {\n\t\treturn false\n\t}\n\t_, ok := data.(ThriftMsgFastCodec)\n\treturn ok\n}\n\nfunc (c thriftCodec) fastUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tmsg := data.(ThriftMsgFastCodec)\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\t_, err = msg.FastRead(buf)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = msg.FastRead(buf)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in FastCodec using SkipDecoder Buffer\")\n\t}\n\treturn err\n}\n\n// unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) hyperUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen - bthrift.Binary.MessageEndLength())\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in Frugal using SkipDecoder Buffer\")\n\t}\n\n\treturn nil\n}\n\n// verifyUnmarshalBasicThriftDataType verifies whether data could be unmarshal by old thrift way\nfunc verifyUnmarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageReader:\n\tcase MessageReaderWithMethodWithContext:\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// decodeBasicThriftData decode thrift body the old way (slow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getSkippedStructBuffer(tProt *BinaryProtocol) ([]byte, error) {\n\tsd := skipDecoder{ByteBuffer: tProt.trans}\n\tbuf, err := sd.NextStruct()\n\tif err != nil {\n\t\treturn nil, remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in SkipDecoder NextStruct phase\")\n\t}\n\treturn buf, nil\n}\n\n```\n# Function Signature: func marshalBasicThriftData(ctx context.Context, tProt thrift.TProtocol, data interface{}) error {\n# Natural Language Annotations: // marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc marshalBasicThriftData(ctx context.Context, tProt thrift.TProtocol, data interface{}) error {\n\t// encode with FastWrite\n\tif c.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\t// fallback to old thrift way (slow)\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\treturn err\n\t}\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err)}\n\t}\n\n\treturn nil\n}\n```", "  ```\nfunc marshalBasicThriftData(ctx context.Context, tProt thrift.TProtocol, data interface{}) error {\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\treturn err\n\t}\n\t_, ok := data.(MessageWriter)\n\tif ok {\n\t\treturn data.(MessageWriter).WriteMessage(tProt)\n\t}\n\t_, ok = data.(MessageWriterWithContext)\n\tif ok {\n\t\treturn data.(MessageWriterWithContext).WriteContextMessage(ctx, tProt)\n\t}\n\treturn remote.NewTransError(remote.ProtocolError, errors.New(\"message not implements either MessageWriter or MessageWriterWithContext\"))\n}\n```", "  ```\nfunc marshalBasicThriftData(ctx context.Context, tProt thrift.TProtocol, data interface{}) error {\n    switch d := data.(type) {\n    case *ThriftMsgBasicCodec: \n        return d.Write(ctx,iops)  \n    default: \n        return errEncodeMismatchMsgType\n    }\n}\n```"]}, "kitex-pkg/remote/codec/thrift/thrift_data.go-UnmarshalThriftException": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/thrift/thrift_data.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/bytedance/gopkg/lang/mcache\"\n\n\t\"github.com/cloudwego/kitex/pkg/protocol/bthrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\nconst marshalThriftBufferSize = 1024\n\n// MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\n\n\n\n\n\n\n\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It will allocate a new buffer and encode to it\nfunc (c thriftCodec) marshalThriftData(ctx context.Context, data interface{}) ([]byte, error) {\n\t// encode with hyper codec\n\t// NOTE: to ensure hyperMarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif c.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn c.hyperMarshalBody(data)\n\t}\n\n\t// encode with FastWrite\n\tif c.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif c.CodecType != Basic && hyperMarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn c.hyperMarshalBody(data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n\n// verifyMarshalBasicThriftDataType verifies whether data could be marshaled by old thrift way\nfunc verifyMarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageWriter:\n\tcase MessageWriterWithContext:\n\tdefault:\n\t\treturn errEncodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) fastMessageUnmarshalEnabled() bool {\n\treturn c.CodecType&FastRead != 0\n}\n\nfunc (c thriftCodec) fastMessageUnmarshalAvailable(data interface{}, payloadLen int) bool {\n\tif payloadLen == 0 && c.CodecType&EnableSkipDecoder == 0 {\n\t\treturn false\n\t}\n\t_, ok := data.(ThriftMsgFastCodec)\n\treturn ok\n}\n\nfunc (c thriftCodec) fastUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tmsg := data.(ThriftMsgFastCodec)\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\t_, err = msg.FastRead(buf)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = msg.FastRead(buf)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in FastCodec using SkipDecoder Buffer\")\n\t}\n\treturn err\n}\n\n// unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) hyperUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen - bthrift.Binary.MessageEndLength())\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in Frugal using SkipDecoder Buffer\")\n\t}\n\n\treturn nil\n}\n\n// verifyUnmarshalBasicThriftDataType verifies whether data could be unmarshal by old thrift way\nfunc verifyUnmarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageReader:\n\tcase MessageReaderWithMethodWithContext:\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// decodeBasicThriftData decode thrift body the old way (slow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getSkippedStructBuffer(tProt *BinaryProtocol) ([]byte, error) {\n\tsd := skipDecoder{ByteBuffer: tProt.trans}\n\tbuf, err := sd.NextStruct()\n\tif err != nil {\n\t\treturn nil, remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in SkipDecoder NextStruct phase\")\n\t}\n\treturn buf, nil\n}\n\n```\n# Function Signature: func UnmarshalThriftException(tProt thrift.TProtocol) error {\n# Natural Language Annotations: // UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc UnmarshalThriftException(tProt thrift.TProtocol) error {\n    // Ensure that the TProtocol implements the appropriate interface\n    thriftErr, ok := tProt.(thrift.TProtocolException)\n    if !ok {\n        return fmt.Errorf(\"TProtocol does not implement TProtocolException\")\n    }\n    return errors.New(thriftErr.String())\n}\n```", "  ```\nfunc UnmarshalThriftException(tProt thrift.TProtocol) error {\n\tvar typeID int32\n\tif err := tProt.ReadI32(&typeID); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t}\n\tif typeID != thrift.EXCEPTION {\n\t\treturn remote.NewTransError(remote.InternalError, fmt.Errorf(\"expected exception type: %d, but got %d\", thrift.EXCEPTION, typeID))\n\t}\n\tvar x thrift.TApplicationException\n\tif err := x.Read(tProt); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t}\n\terr := remote.NewTransError(remote.ApplicationError, &x)\n\tif typeID != int32(err.GetType()) {\n\t\treturn remote.NewTransError(remote.InternalError, fmt.Errorf(\"expected exception type: %d, but got %d\", typeID, err.GetType()))\n\t}\n\treturn err\n}\n```", "  ```\nfunc UnmarshalThriftException(tProt thrift.TProtocol) error {\n    transport := thrift.NewTMemoryBufferLen(1024)\n    tProt := thrift.NewTBinaryProtocol(transport, true, true)\n    if err := tProt.WriteMessageBegin(); err != nil {\n        return err\n    }\n    if err := tProt.WriteStructBegin(); err != nil {\n        return err\n    }\n\n    typedef ControlExceptionInfo = ExceptionInfo\n    info := ControlException{}\n\n    err := tProt.ReadFieldBegin()\n    if err != nil {\n        return err\n    }\n    if err := info.Unmarshal(tProt); err != nil {\n        return err\n    }\n\n    err = tProt.ReadStructEnd()\n    if err != nil {\n        return err\n    }\n    err = tProt.ReadMessageEnd()\n    if err != nil {\n        return err\n    }\n\n    return perrors.Errorf(\"Encountering thrift exception: %s (%s)\", info.GetMessage(), info.GetType().String())\n}\n```"]}, "kitex-pkg/remote/codec/thrift/thrift_data.go-UnmarshalThriftData": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/thrift/thrift_data.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/bytedance/gopkg/lang/mcache\"\n\n\t\"github.com/cloudwego/kitex/pkg/protocol/bthrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\nconst marshalThriftBufferSize = 1024\n\n// MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\n\n\n\n\n\n\n\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It will allocate a new buffer and encode to it\nfunc (c thriftCodec) marshalThriftData(ctx context.Context, data interface{}) ([]byte, error) {\n\t// encode with hyper codec\n\t// NOTE: to ensure hyperMarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif c.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn c.hyperMarshalBody(data)\n\t}\n\n\t// encode with FastWrite\n\tif c.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif c.CodecType != Basic && hyperMarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn c.hyperMarshalBody(data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n\n// verifyMarshalBasicThriftDataType verifies whether data could be marshaled by old thrift way\nfunc verifyMarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageWriter:\n\tcase MessageWriterWithContext:\n\tdefault:\n\t\treturn errEncodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) fastMessageUnmarshalEnabled() bool {\n\treturn c.CodecType&FastRead != 0\n}\n\nfunc (c thriftCodec) fastMessageUnmarshalAvailable(data interface{}, payloadLen int) bool {\n\tif payloadLen == 0 && c.CodecType&EnableSkipDecoder == 0 {\n\t\treturn false\n\t}\n\t_, ok := data.(ThriftMsgFastCodec)\n\treturn ok\n}\n\nfunc (c thriftCodec) fastUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tmsg := data.(ThriftMsgFastCodec)\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\t_, err = msg.FastRead(buf)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = msg.FastRead(buf)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in FastCodec using SkipDecoder Buffer\")\n\t}\n\treturn err\n}\n\n// unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) hyperUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen - bthrift.Binary.MessageEndLength())\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in Frugal using SkipDecoder Buffer\")\n\t}\n\n\treturn nil\n}\n\n// verifyUnmarshalBasicThriftDataType verifies whether data could be unmarshal by old thrift way\nfunc verifyUnmarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageReader:\n\tcase MessageReaderWithMethodWithContext:\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// decodeBasicThriftData decode thrift body the old way (slow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getSkippedStructBuffer(tProt *BinaryProtocol) ([]byte, error) {\n\tsd := skipDecoder{ByteBuffer: tProt.trans}\n\tbuf, err := sd.NextStruct()\n\tif err != nil {\n\t\treturn nil, remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in SkipDecoder NextStruct phase\")\n\t}\n\treturn buf, nil\n}\n\n```\n# Function Signature: func UnmarshalThriftData(ctx context.Context, codec remote.PayloadCodec, method string, buf []byte, data interface{}) error {\n# Natural Language Annotations: // UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc UnmarshalThriftData(ctx context.Context, codec remote.PayloadCodec, method string, buf []byte, data interface{}) error {\n\ttProt := thrift.NewTBinaryProtocolTransport(buf)\n\tif err := codec.fastMessageUnmarshalEnabled(); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t}\n\tif err := codec.fastMessageUnmarshalAvailable(data, len(buf)); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t}\n\tif err := codec.fastUnmarshal(tProt, data, len(buf)); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t}\n\treturn nil\n}\n```", "  ```\nfunc UnmarshalThriftData(ctx context.Context, codec remote.PayloadCodec, method string, buf []byte, data interface{}) error {\n\tc := codec.(*thriftCodec)\n\tif c.fastMessageUnmarshalEnabled() && c.fastMessageUnmarshalAvailable(data, len(buf)) {\n\t\treturn c.fastUnmarshal(context.Background(), data, len(buf))\n\t}\n\tif c.fastMessageUnmarshalAvailable(data, len(buf)) {\n\t\treturn c.fastUnmarshal(context.Background(), data, len(buf))\n\t}\n\tswitch data.(type) {\n\tcase MessageReader:\n\t\treturn c.hyperUnmarshal(context.Background(), data, len(buf))\n\tcase MessageReaderWithMethodWithContext:\n\t\treturn c.hyperUnmarshal(context.Background(), data, len(buf))\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"%T don't implement MessageReader or MessageReaderWithMethod\", data))\n\t}\n}\n```", "  ```\nfunc (c thriftCodec) UnmarshalThriftData(ctx context.Context, codec remote.PayloadCodec, method string, buf []byte, data interface{}) error {\n\t// get the thrift payload size from the buf and verify that it's not empty\n\tpayloadLen := len(buf)\n\tif payloadLen == 0 {\n\t\terr := perrors.NewErrorWithMsgf(perrors.CodecError, \"thrift payload size is zero\")\n\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t}\n\n\t// get the thrift codec type and verify that it's not basic\n\tcodecType := codec.CodecType()\n\tif codecType&Basic != 0 {\n\t\treturn remote.NewTransError(remote.ProtocolError, errors.New(\"basic codec is not allowed for thrift\"))\n\t}\n\n\t// check if we should use fastRead\n\tif data, ok := data.(ThriftMsgFastCodec); ok && codec.FastMessageUnmarshalEnabled() && codec.FastMessageUnmarshalAvailable(data, payloadLen) {\n\t\treturn codec.FastUnmarshal(TProtocolFactory(buf), data, payloadLen)\n\t}\n\n\t// check if we should use hyperMarshal\n\tif data, ok := data.(ThriftMsgHyperCodec); ok && codec.HyperMarshalEnabled() && codec.HyperMarshalAvailable(data, payloadLen) {\n\t\treturn codec.HyperUnmarshal(TProtocolFactory(buf), data, payloadLen)\n\t}\n\n\t// create the thrift protocol\n\ttrans := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProto := thrift.NewTBinaryProtocol(trans, true, true)\n\n\t// verify that the data type is valid for decoding\n\tif err := codec.VerifyUnmarshalBasicThriftDataType(buf); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t}\n\n\t// unmarshal the thrift data\n\tif err := codec.DecodeBasicThriftData(ctx, tProto, buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t}\n\n\t// return the data\n\treturn nil\n}\n```"]}, "kitex-pkg/remote/codec/thrift/thrift_data.go-unmarshalThriftData": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/thrift/thrift_data.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/bytedance/gopkg/lang/mcache\"\n\n\t\"github.com/cloudwego/kitex/pkg/protocol/bthrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\nconst marshalThriftBufferSize = 1024\n\n// MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\n\n\n\n\n\n\n\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It will allocate a new buffer and encode to it\nfunc (c thriftCodec) marshalThriftData(ctx context.Context, data interface{}) ([]byte, error) {\n\t// encode with hyper codec\n\t// NOTE: to ensure hyperMarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif c.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn c.hyperMarshalBody(data)\n\t}\n\n\t// encode with FastWrite\n\tif c.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif c.CodecType != Basic && hyperMarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn c.hyperMarshalBody(data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n\n// verifyMarshalBasicThriftDataType verifies whether data could be marshaled by old thrift way\nfunc verifyMarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageWriter:\n\tcase MessageWriterWithContext:\n\tdefault:\n\t\treturn errEncodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) fastMessageUnmarshalEnabled() bool {\n\treturn c.CodecType&FastRead != 0\n}\n\nfunc (c thriftCodec) fastMessageUnmarshalAvailable(data interface{}, payloadLen int) bool {\n\tif payloadLen == 0 && c.CodecType&EnableSkipDecoder == 0 {\n\t\treturn false\n\t}\n\t_, ok := data.(ThriftMsgFastCodec)\n\treturn ok\n}\n\nfunc (c thriftCodec) fastUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tmsg := data.(ThriftMsgFastCodec)\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\t_, err = msg.FastRead(buf)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = msg.FastRead(buf)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in FastCodec using SkipDecoder Buffer\")\n\t}\n\treturn err\n}\n\n// unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) hyperUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen - bthrift.Binary.MessageEndLength())\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in Frugal using SkipDecoder Buffer\")\n\t}\n\n\treturn nil\n}\n\n// verifyUnmarshalBasicThriftDataType verifies whether data could be unmarshal by old thrift way\nfunc verifyUnmarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageReader:\n\tcase MessageReaderWithMethodWithContext:\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// decodeBasicThriftData decode thrift body the old way (slow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getSkippedStructBuffer(tProt *BinaryProtocol) ([]byte, error) {\n\tsd := skipDecoder{ByteBuffer: tProt.trans}\n\tbuf, err := sd.NextStruct()\n\tif err != nil {\n\t\treturn nil, remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in SkipDecoder NextStruct phase\")\n\t}\n\treturn buf, nil\n}\n\n```\n# Function Signature: func (c thriftCodec) unmarshalThriftData(ctx context.Context, tProt *BinaryProtocol, method string, data interface{}, dataLen int) error {\n# Natural Language Annotations: // unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (c thriftCodec) unmarshalThriftData(ctx context.Context, tProt *BinaryProtocol, method string, data interface{}, dataLen int) error {\n\t// use fast codec when enabled\n\tif c.fastMessageUnmarshalEnabled() {\n\t\tif c.fastMessageUnmarshalAvailable(data, dataLen) {\n\t\t\treturn c.fastUnmarshal(tProt, data, dataLen)\n\t\t}\n\t}\n\n\t// use hyper codec when enabled\n\tif c.hyperMarshalEnabled() {\n\t\tif c.hyperUnmarshalAvailable(method, data, dataLen) {\n\t\t\treturn c.hyperUnmarshal(tProt, data, dataLen)\n\t\t}\n\t}\n\n\t// fallback to old thrift way (slow)\n\tswitch data.(type) {\n\tcase MessageReader:\n\t\treturn c.fastMessageUnmarshal(tProt, data)\n\tcase MessageReaderWithMethodWithContext:\n\t\treturn c.fastMessageUnmarshalWithMethodWithContext(ctx, tProt, data)\n\tdefault:\n\t\treturn c.fallbackUnmarshal(ctx, tProt, data, dataLen)\n\t}\n}\n```", "  ```\nfunc (c thriftCodec) unmarshalThriftData(ctx context.Context, tProt *BinaryProtocol, method string, data interface{}, dataLen int) error {\n\t// use fast codec if available and available data len > 0\n\tif c.fastMessageUnmarshalEnabled() && c.fastMessageUnmarshalAvailable(data, dataLen) {\n\t\treturn c.fastUnmarshal(tProt, data, dataLen)\n\t}\n\n\t// try to get []byte payload\n\tvar buf []byte\n\tvar ok bool\n\tif dataLen > 0 {\n\t\tbuf, ok = tProt.Next(dataLen - bthrift.Binary.MessageEndLength())\n\t\tif !ok {\n\t\t\treturn thrift.NewTProtocolException(thrift.INVALID_DATA, errDecoding)\n\t\t}\n\t} else {\n\t\tbuf, ok = getSkippedStructBuffer(tProt)\n\t\tif !ok {\n\t\t\treturn thrift.NewTProtocolException(thrift.INVALID_DATA, errDecoding)\n\t\t}\n\t}\n\n\t// use hyper codec if available\n\tif c.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn c.hyperUnmarshal(tProt, data, dataLen)\n\t}\n\n\t// fallback to old thrift way (slow)\n\tif err := verifyUnmarshalBasicThriftDataType(data); err != nil {\n\t\treturn err\n\t}\n\treturn decodeBasicThriftData(context.Background(), tProt, method, data, buf)\n}\n```", "  ```\nfunc (c thriftCodec) unmarshalThriftData(ctx context.Context, tProt *BinaryProtocol, method string, data interface{}, dataLen int) error {\n\tif c.structMarshalEnabled() && c.fastMessageUnmarshalEnabled() && c.fastMessageUnmarshalAvailable(data, dataLen) {\n\t\treturn c.fastUnmarshal(tProt, data, dataLen)\n\t}\n\tif c.structMarshalEnabled() && c.hyperMarshalEnabled() && bthrift.Binary.IsMessageEnd(tProt, method) {\n\t\treturn c.unmarshalErr(c.hyperUnmarshal(tProt, data, dataLen))\n\t}\n\tif c.structMarshalEnabled() && verifyUnmarshalBasicThriftDataType(data) == nil {\n\t\tif err := unmarshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t}\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\terr = thrift.Unmarshal(tProt, data, buf)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = thrift.Unmarshal(tProt, data, buf)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in old Thrift using SkipDecoder Buffer\")\n\t}\n\treturn nil\n}\n```"]}, "kitex-pkg/remote/codec/thrift/thrift_data.go-decodeBasicThriftData": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/thrift/thrift_data.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/bytedance/gopkg/lang/mcache\"\n\n\t\"github.com/cloudwego/kitex/pkg/protocol/bthrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\nconst marshalThriftBufferSize = 1024\n\n// MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\n\n\n\n\n\n\n\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It will allocate a new buffer and encode to it\nfunc (c thriftCodec) marshalThriftData(ctx context.Context, data interface{}) ([]byte, error) {\n\t// encode with hyper codec\n\t// NOTE: to ensure hyperMarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif c.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn c.hyperMarshalBody(data)\n\t}\n\n\t// encode with FastWrite\n\tif c.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif c.CodecType != Basic && hyperMarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn c.hyperMarshalBody(data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n\n// verifyMarshalBasicThriftDataType verifies whether data could be marshaled by old thrift way\nfunc verifyMarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageWriter:\n\tcase MessageWriterWithContext:\n\tdefault:\n\t\treturn errEncodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) fastMessageUnmarshalEnabled() bool {\n\treturn c.CodecType&FastRead != 0\n}\n\nfunc (c thriftCodec) fastMessageUnmarshalAvailable(data interface{}, payloadLen int) bool {\n\tif payloadLen == 0 && c.CodecType&EnableSkipDecoder == 0 {\n\t\treturn false\n\t}\n\t_, ok := data.(ThriftMsgFastCodec)\n\treturn ok\n}\n\nfunc (c thriftCodec) fastUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tmsg := data.(ThriftMsgFastCodec)\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\t_, err = msg.FastRead(buf)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = msg.FastRead(buf)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in FastCodec using SkipDecoder Buffer\")\n\t}\n\treturn err\n}\n\n// unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) hyperUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen - bthrift.Binary.MessageEndLength())\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in Frugal using SkipDecoder Buffer\")\n\t}\n\n\treturn nil\n}\n\n// verifyUnmarshalBasicThriftDataType verifies whether data could be unmarshal by old thrift way\nfunc verifyUnmarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageReader:\n\tcase MessageReaderWithMethodWithContext:\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// decodeBasicThriftData decode thrift body the old way (slow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getSkippedStructBuffer(tProt *BinaryProtocol) ([]byte, error) {\n\tsd := skipDecoder{ByteBuffer: tProt.trans}\n\tbuf, err := sd.NextStruct()\n\tif err != nil {\n\t\treturn nil, remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in SkipDecoder NextStruct phase\")\n\t}\n\treturn buf, nil\n}\n\n```\n# Function Signature: func decodeBasicThriftData(ctx context.Context, tProt thrift.TProtocol, method string, data interface{}) error {\n# Natural Language Annotations: // decodeBasicThriftData decode thrift body the old way (slow)\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc decodeBasicThriftData(ctx context.Context, tProt thrift.TProtocol, method string, data interface{}) error {\n\t// decode with slow thrift way\n\tif err := verifyUnmarshalBasicThriftDataType(data); err != nil {\n\t\treturn err\n\t}\n\n\tmsg := data.(MessageReader)\n\tvar err error\n\tif c.CodecType&FastRead != 0 {\n\t\tif msg, ok := data.(MessageReaderFastCodec); ok {\n\t\t\t_, err = msg.FastRead(tProt.BLength())\n\t\t}\n\t} else {\n\t\terr = msg.Read(ctx, tProt)\n\t}\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t}\n\n\treturn nil\n}\n```", "  ```\nfunc decodeBasicThriftData(ctx context.Context, tProt thrift.TProtocol, method string, data interface{}) error {\n\t// decode with the old thrift way (slow)\n\t// NOTE: to ensure hyperMarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif c.hyperUnmarshalEnabled() && hyperUnmarshalAvailable(data) {\n\t\treturn c.hyperUnmarshalBody(data)\n\t}\n\n\tif c.fastMessageUnmarshalEnabled() && c.fastMessageUnmarshalAvailable(data, int(msg.Length())) {\n\t\tif err := c.fastUnmarshal(tProt, data, int(msg.Length())); err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\t// fallback to slow decode\n\t\tif err := c.slowUnmarshal(tProt, data); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t}\n\treturn nil\n}\n```", "  ```\nfunc decodeBasicThriftData(ctx context.Context, tProt thrift.TProtocol, method string, data interface{}) error {\n\tif err := verifyUnmarshalBasicThriftDataType(data); err != nil {\n\t\treturn err\n\t}\n\n\t// decode the message type and sequence id\n\tmsgType, err := tProt.ReadMessageBegin(method)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t}\n\n\t// read the message body\n\tif err := tProt.ReadMessageEnd(); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t}\n\n\tmsgReader := ThriftMsgReaderForType(msgType)\n\tif msgReader != nil {\n\t\terr = msgReader.Read(ctx, tProt, data)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalThriftData(ctx, tProt, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t}\n\treturn transport.Bytes(), nil\n}\n\n```"]}, "kitex-pkg/remote/remotecli/conn_wrapper.go-GetConn": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/remotecli/conn_wrapper.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage remotecli\n\nimport (\n\t\"context\"\n\t\"net\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n)\n\nvar connWrapperPool sync.Pool\n\nfunc init() {\n\tconnWrapperPool.New = newConnWrapper\n}\n\nvar _ ConnReleaser = &ConnWrapper{}\n\n// ConnReleaser helps to release the raw connection.\ntype ConnReleaser interface {\n\tReleaseConn(err error, ri rpcinfo.RPCInfo)\n}\n\n// ConnWrapper wraps a connection.\ntype ConnWrapper struct {\n\tconnPool remote.ConnPool\n\tconn     net.Conn\n}\n\n// NewConnWrapper returns a new ConnWrapper using the given connPool and logger.\nfunc NewConnWrapper(connPool remote.ConnPool) *ConnWrapper {\n\tcm := connWrapperPool.Get().(*ConnWrapper)\n\tcm.connPool = connPool\n\treturn cm\n}\n\n// GetConn returns a connection using the given Dialer and RPCInfo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ReleaseConn should notice that ri may nil when oneway\n// TODO duplicate release may cause problem?\nfunc (cm *ConnWrapper) ReleaseConn(err error, ri rpcinfo.RPCInfo) {\n\tif cm.conn == nil {\n\t\treturn\n\t}\n\tif cm.connPool != nil {\n\t\tif err == nil {\n\t\t\t_, ok := ri.To().Tag(rpcinfo.ConnResetTag)\n\t\t\tif ok || ri.Config().InteractionMode() == rpcinfo.Oneway {\n\t\t\t\tcm.connPool.Discard(cm.conn)\n\t\t\t} else {\n\t\t\t\tcm.connPool.Put(cm.conn)\n\t\t\t}\n\t\t} else {\n\t\t\tcm.connPool.Discard(cm.conn)\n\t\t}\n\t} else {\n\t\tcm.conn.Close()\n\t}\n\n\tcm.zero()\n\tconnWrapperPool.Put(cm)\n}\n\nfunc newConnWrapper() interface{} {\n\treturn &ConnWrapper{}\n}\n\nfunc (cm *ConnWrapper) zero() {\n\tcm.connPool = nil\n\tcm.conn = nil\n}\n\nfunc (cm *ConnWrapper) getConnWithPool(ctx context.Context, cp remote.ConnPool, d remote.Dialer,\n\ttimeout time.Duration, ri rpcinfo.RPCInfo,\n) (net.Conn, error) {\n\taddr := ri.To().Address()\n\tif addr == nil {\n\t\treturn nil, kerrors.ErrNoDestAddress\n\t}\n\topt := remote.ConnOption{Dialer: d, ConnectTimeout: timeout}\n\tri.Stats().Record(ctx, stats.ClientConnStart, stats.StatusInfo, \"\")\n\tconn, err := cp.Get(ctx, addr.Network(), addr.String(), opt)\n\tif err != nil {\n\t\tri.Stats().Record(ctx, stats.ClientConnFinish, stats.StatusError, err.Error())\n\t\treturn nil, kerrors.ErrGetConnection.WithCause(err)\n\t}\n\tri.Stats().Record(ctx, stats.ClientConnFinish, stats.StatusInfo, \"\")\n\treturn conn, nil\n}\n\nfunc (cm *ConnWrapper) getConnWithDialer(ctx context.Context, d remote.Dialer,\n\ttimeout time.Duration, ri rpcinfo.RPCInfo,\n) (net.Conn, error) {\n\taddr := ri.To().Address()\n\tif addr == nil {\n\t\treturn nil, kerrors.ErrNoDestAddress\n\t}\n\n\tri.Stats().Record(ctx, stats.ClientConnStart, stats.StatusInfo, \"\")\n\tconn, err := d.DialTimeout(addr.Network(), addr.String(), timeout)\n\tif err != nil {\n\t\tri.Stats().Record(ctx, stats.ClientConnFinish, stats.StatusError, err.Error())\n\t\treturn nil, kerrors.ErrGetConnection.WithCause(err)\n\t}\n\tri.Stats().Record(ctx, stats.ClientConnFinish, stats.StatusInfo, \"\")\n\treturn conn, nil\n}\n\n```\n# Function Signature: func (cm *ConnWrapper) GetConn(ctx context.Context, d remote.Dialer, ri rpcinfo.RPCInfo) (net.Conn, error) {\n# Natural Language Annotations: // GetConn returns a connection using the given Dialer and RPCInfo.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (cm *ConnWrapper) GetConn(ctx context.Context, d remote.Dialer, ri rpcinfo.RPCInfo) (net.Conn, error) {\n\taddr := ri.To().Address()\n\tif addr == nil {\n\t\treturn nil, kerrors.ErrNoDestAddress\n\t}\n\n\topt := remote.ConnOption{Dialer: d, ConnectTimeout: timeout}\n\tri.Stats().Record(ctx, stats.ClientConnStart, stats.StatusInfo, \"\")\n\tconn, err := cm.connPool.Get(ctx, addr.Network(), addr.String(), opt)\n\tif err != nil {\n\t\tri.Stats().Record(ctx, stats.ClientConnFinish, stats.StatusError, err.Error())\n\t\treturn nil, kerrors.ErrGetConnection.WithCause(err)\n\t}\n\tri.Stats().Record(ctx, stats.ClientConnFinish, stats.StatusInfo, \"\")\n\treturn conn, nil\n}\n```", "  ```\nfunc (cm *ConnWrapper) GetConn(ctx context.Context, d remote.Dialer, ri rpcinfo.RPCInfo) (net.Conn, error) {\n\tif cm.connPool != nil {\n\t\tconn, err := cm.getConnWithPool(ctx, cm.connPool, d, ri.Config().ConnectTimeout(), ri)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn conn, nil\n\t}\n\n\tconn, err := cm.getConnWithDialer(ctx, d, ri.Config().ConnectTimeout(), ri)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn conn, nil\n}\n```", "  ```\nfunc (cm *ConnWrapper) GetConn(ctx context.Context, d remote.Dialer, ri rpcinfo.RPCInfo) (net.Conn, error) {\n\tvar conn net.Conn\n\tvar err error\n\tif cm.connPool != nil && cm.connPool.IsFull() {\n\t\tconn, err = cm.getConnWithPool(ctx, cm.connPool, d, ri)\n\t} else {\n\t\tconn, err = cm.getConnWithDialer(ctx, d, ri)\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tcm.conn = conn\n\treturn conn, nil\n}\n```"]}, "kitex-pkg/remote/remotecli/client.go-NewClient": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/remotecli/client.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage remotecli\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"net\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\n// Client implementation may be different between mux and non-mux clients.\ntype Client interface {\n\t// RPCInfo as param just avoid to get it from ctx\n\tSend(ctx context.Context, ri rpcinfo.RPCInfo, req remote.Message) (err error)\n\tRecv(ctx context.Context, ri rpcinfo.RPCInfo, resp remote.Message) (err error)\n\tRecycle()\n}\n\nvar clientPool = &sync.Pool{\n\tNew: func() interface{} {\n\t\treturn new(client)\n\t},\n}\n\ntype client struct {\n\ttransHdlr   remote.TransHandler\n\tconnManager *ConnWrapper\n\tconn        net.Conn\n}\n\n// NewClient creates a new Client using the given params.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c *client) Recycle() {\n\tc.transHdlr = nil\n\tc.connManager = nil\n\tc.conn = nil\n\tclientPool.Put(c)\n}\n\nfunc (c *client) init(handler remote.TransHandler, cm *ConnWrapper, conn net.Conn) {\n\tc.transHdlr = handler\n\tc.connManager = cm\n\tc.conn = conn\n}\n\n// Send is blocked.\nfunc (c *client) Send(ctx context.Context, ri rpcinfo.RPCInfo, req remote.Message) (err error) {\n\t_, err = c.transHdlr.Write(ctx, c.conn, req)\n\tif err != nil {\n\t\tc.connManager.ReleaseConn(err, ri)\n\t}\n\treturn err\n}\n\n// Recv is blocked.\nfunc (c *client) Recv(ctx context.Context, ri rpcinfo.RPCInfo, resp remote.Message) (err error) {\n\t// resp is nil means oneway\n\tif resp != nil {\n\t\tctx, err = c.transHdlr.Read(ctx, c.conn, resp)\n\t\tc.transHdlr.OnMessage(ctx, nil, resp)\n\t} else {\n\t\t// Wait for the request to be flushed out before closing the connection.\n\t\t// 500us is an acceptable duration to keep a minimal loss rate at best effort.\n\t\ttime.Sleep(time.Millisecond / 2)\n\t}\n\n\tc.connManager.ReleaseConn(err, ri)\n\treturn err\n}\n\n```\n# Function Signature: func NewClient(ctx context.Context, ri rpcinfo.RPCInfo, handler remote.TransHandler, opt *remote.ClientOption) (Client, error) {\n# Natural Language Annotations: // NewClient creates a new Client using the given params.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\n// NewClient creates a new Client using the given params.\nfunc NewClient(ctx context.Context, ri rpcinfo.RPCInfo, handler remote.TransHandler, opt *remote.ClientOption) (Client, error) {\n\tcm, err := opt.ConnManagerBuilder(ctx, ri)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tconn := cm.Acquire(ctx, ri)\n\tif conn == nil {\n\t\treturn nil, errors.New(\"conn is nil\")\n\t}\n\n\tc := clientPool.Get().(*client)\n\tc.init(handler, cm, conn)\n\n\t// set the connection type\n\tif opt.ConnType != \"\" {\n\t\tc.conn.SetConnType(opt.ConnType)\n\t}\n\n\treturn c, nil\n}\n```", "  ```\nfunc NewClient(ctx context.Context, ri rpcinfo.RPCInfo, handler remote.TransHandler, opt *remote.ClientOption) (Client, error) {\n\tvar cm *ConnWrapper\n\tvar conn net.Conn\n\n\tif opt.OnInited != nil {\n\t\tcm = opt.OnInited\n\t\tconn = cm.GetConn()\n\t} else {\n\t\tvar err error\n\t\tcm, err = NewConnWrapper(ctx, ri, handler)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tconn = cm.GetConn()\n\t}\n\n\tif opt.ClientPool != nil {\n\t\tcli := opt.ClientPool.Get().(*client)\n\t\tcli.transHdlr = handler\n\t\tcli.connManager = cm\n\t\tcli.conn = conn\n\t\treturn cli, nil\n\t}\n\n\tcli := clientPool.Get().(*client)\n\tcli.init(handler, cm, conn)\n\treturn cli, nil\n}\n```", "  ```\nfunc NewClient(ctx context.Context, ri rpcinfo.RPCInfo, handler remote.TransHandler, opt *remote.ClientOption) (Client, error) {\n  \n  // Get Conn.\n  conn, err := getConn(ctx, ri, handler, opt)\n  if err != nil {\n    return nil, err\n  }\n  \n  // Get TransHandler and ConnWrapper based on ClientOption.\n  transHdlr, cm, err := getTransHandlerAndConnWrapper(opt)\n  if err != nil {\n    return nil, err\n  }\n  \n  // Initialize Client.\n  c := clientPool.Get().(*client)\n  c.init(transHdlr, cm, conn)\n  return c, nil\n}\n\nfunc getConn(ctx context.Context, ri rpcinfo.RPCInfo, handler remote.TransHandler, opt *remote.ClientOption) (net.Conn, error) {\n  \n  // Get TargetIP and TargetPort based on ClientOption.\n  targetIP, targetPort, err := getTargetIPAndPort(opt)\n  if err != nil {\n    return nil, err\n  }\n  \n  // Create a connection manager.\n  cm := handler.ConnManager()\n  \n  // Get or create a connection based on ClientOption.\n  conn, err := cm.GetOrCreateConn(ctx, ri, handler.RemoteAddr(), targetIP, targetPort)\n  if err != nil {\n    return nil, err\n  }\n  return conn, nil\n}\n\nfunc getTransHandlerAndConnWrapper(opt *remote.ClientOption) (remote.TransHandler, *ConnWrapper, error) {\n  \n  // Get TransHandler based on ClientOption.\n  transHdlr, err := getTransHandler(opt)\n  if err != nil {\n    return nil, nil, err\n  }\n  \n  // Get ConnWrapper based on ClientOption.\n  cm, err := getConnWrapper(opt)\n  if err != nil {\n    return nil, nil, err\n  }\n  \n  return transHdlr, cm, nil\n}\n\nfunc getTransHandler(opt *remote.ClientOption) (remote.TransHandler, error) {\n  \n  // Get TransHandlerType based on ClientOption.\n  transHdlrType, err := getTransHandlerType(opt)\n  if err != nil {\n    return nil, err\n  }\n  \n  // Create a new instance of the TransHandlerType.\n  transHdlr, err := transHdlrType.NewTransHandler()\n  if err != nil {\n    return nil, err\n  }\n  return transHdlr, nil\n}\n\nfunc getConnWrapper(opt *remote.ClientOption) (*ConnWrapper, error) {\n  \n  // Get ConnWrapperType based on ClientOption.\n  cmType, err := getConnWrapperType(opt)\n  if err != nil {\n    return nil, err\n  }\n  \n  // Create a new instance of the ConnWrapperType.\n  cm, err := cmType.NewConnWrapper()\n  if err != nil {\n    return nil, err\n  }\n  return cm, nil\n}\n\nfunc getTargetIPAndPort(opt *remote.ClientOption) (string, int, error) {\n  \n  // Check that the ClientOption has a TargetIP and TargetPort.\n  if opt.targetIP == \"\" || opt.targetPort == 0 {\n    return \"\", 0, kerrors.ErrIllegalArguments\n  }\n  return opt.targetIP, opt.targetPort, nil\n}\n```"]}, "kitex-pkg/remote/connpool/long_pool.go-IsActive": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/connpool/long_pool.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package connpool provide short connection and long connection pool.\npackage connpool\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n)\n\nvar (\n\t_ net.Conn            = &longConn{}\n\t_ remote.LongConnPool = &LongPool{}\n\n\t// global shared tickers for different LongPool\n\tsharedTickers sync.Map\n)\n\nconst (\n\tconfigDumpKey = \"idle_config\"\n)\n\nfunc getSharedTicker(p *LongPool, refreshInterval time.Duration) *utils.SharedTicker {\n\tsti, ok := sharedTickers.Load(refreshInterval)\n\tif ok {\n\t\tst := sti.(*utils.SharedTicker)\n\t\tst.Add(p)\n\t\treturn st\n\t}\n\tsti, _ = sharedTickers.LoadOrStore(refreshInterval, utils.NewSharedTicker(refreshInterval))\n\tst := sti.(*utils.SharedTicker)\n\tst.Add(p)\n\treturn st\n}\n\n// netAddr implements the net.Addr interface and comparability.\ntype netAddr struct {\n\tnetwork string\n\taddress string\n}\n\n// Network implements the net.Addr interface.\nfunc (na netAddr) Network() string { return na.network }\n\n// String implements the net.Addr interface.\nfunc (na netAddr) String() string { return na.address }\n\n// longConn implements the net.Conn interface.\ntype longConn struct {\n\tnet.Conn\n\tsync.RWMutex\n\tdeadline time.Time\n\taddress  string\n}\n\n// Close implements the net.Conn interface.\nfunc (c *longConn) Close() error {\n\treturn c.Conn.Close()\n}\n\n// RawConn returns the real underlying net.Conn.\nfunc (c *longConn) RawConn() net.Conn {\n\treturn c.Conn\n}\n\n// IsActive indicates whether the connection is active.\n\n\n\n\n\n\n\n\n// Expired checks the deadline of the connection.\nfunc (c *longConn) Expired() bool {\n\treturn time.Now().After(c.deadline)\n}\n\ntype PoolDump struct {\n\tIdleNum       int         `json:\"idle_num\"`\n\tConnsDeadline []time.Time `json:\"conns_deadline\"`\n}\n\nfunc newPool(minIdle, maxIdle int, maxIdleTimeout time.Duration) *pool {\n\tp := &pool{\n\t\tidleList:       make([]*longConn, 0, maxIdle),\n\t\tminIdle:        minIdle,\n\t\tmaxIdle:        maxIdle,\n\t\tmaxIdleTimeout: maxIdleTimeout,\n\t}\n\treturn p\n}\n\n// pool implements a pool of long connections.\ntype pool struct {\n\tidleList []*longConn // idleList Get/Put by FILO(stack) but Evict by FIFO(queue)\n\tmu       sync.RWMutex\n\t// config\n\tminIdle        int\n\tmaxIdle        int           // currIdle <= maxIdle.\n\tmaxIdleTimeout time.Duration // the idle connection will be cleaned if the idle time exceeds maxIdleTimeout.\n}\n\n// Get gets the first active connection from the idleList. Return the number of connections decreased during the Get.\nfunc (p *pool) Get() (*longConn, bool, int) {\n\tp.mu.Lock()\n\t// Get the first active one\n\tn := len(p.idleList)\n\tselected := n - 1\n\tfor ; selected >= 0; selected-- {\n\t\to := p.idleList[selected]\n\t\t// reset slice element to nil, active conn object only could be hold reference by user function\n\t\tp.idleList[selected] = nil\n\t\tif o.IsActive() {\n\t\t\tp.idleList = p.idleList[:selected]\n\t\t\tp.mu.Unlock()\n\t\t\treturn o, true, n - selected\n\t\t}\n\t\t// inactive object\n\t\to.Close()\n\t}\n\t// in case all objects are inactive\n\tif selected < 0 {\n\t\tselected = 0\n\t}\n\tp.idleList = p.idleList[:selected]\n\tp.mu.Unlock()\n\treturn nil, false, n - selected\n}\n\n// Put puts back a connection to the pool.\n\n\n\n\n\n\n\n\n\n\n\n\n// Evict cleanups the expired connections.\n// Evict returns how many connections has been evicted.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Len returns the length of the pool.\nfunc (p *pool) Len() int {\n\tp.mu.RLock()\n\tl := len(p.idleList)\n\tp.mu.RUnlock()\n\treturn l\n}\n\n// Close closes the pool and all the objects in the pool.\nfunc (p *pool) Close() int {\n\tp.mu.Lock()\n\tnum := len(p.idleList)\n\tfor i := 0; i < num; i++ {\n\t\tp.idleList[i].Close()\n\t}\n\tp.idleList = nil\n\n\tp.mu.Unlock()\n\treturn num\n}\n\n// Dump dumps the info of all the objects in the pool.\nfunc (p *pool) Dump() PoolDump {\n\tp.mu.RLock()\n\tidleNum := len(p.idleList)\n\tconnsDeadline := make([]time.Time, idleNum)\n\tfor i := 0; i < idleNum; i++ {\n\t\tconnsDeadline[i] = p.idleList[i].deadline\n\t}\n\ts := PoolDump{\n\t\tIdleNum:       idleNum,\n\t\tConnsDeadline: connsDeadline,\n\t}\n\tp.mu.RUnlock()\n\treturn s\n}\n\nfunc newPeer(\n\tserviceName string,\n\taddr net.Addr,\n\tminIdle int,\n\tmaxIdle int,\n\tmaxIdleTimeout time.Duration,\n\tglobalIdle *utils.MaxCounter,\n) *peer {\n\treturn &peer{\n\t\tserviceName: serviceName,\n\t\taddr:        addr,\n\t\tglobalIdle:  globalIdle,\n\t\tpool:        newPool(minIdle, maxIdle, maxIdleTimeout),\n\t}\n}\n\n// peer has one address, it manages all connections base on this address\ntype peer struct {\n\t// info\n\tserviceName string\n\taddr        net.Addr\n\tglobalIdle  *utils.MaxCounter\n\t// pool\n\tpool *pool\n}\n\n// Get gets a connection with dialer and timeout. Dial a new connection if no idle connection in pool is available.\nfunc (p *peer) Get(d remote.Dialer, timeout time.Duration, reporter Reporter, addr string) (net.Conn, error) {\n\tvar c net.Conn\n\tc, reused, decNum := p.pool.Get()\n\tp.globalIdle.DecN(int64(decNum))\n\tif reused {\n\t\treporter.ReuseSucceed(Long, p.serviceName, p.addr)\n\t\treturn c, nil\n\t}\n\t// dial a new connection\n\tc, err := d.DialTimeout(p.addr.Network(), p.addr.String(), timeout)\n\tif err != nil {\n\t\treporter.ConnFailed(Long, p.serviceName, p.addr)\n\t\treturn nil, err\n\t}\n\treporter.ConnSucceed(Long, p.serviceName, p.addr)\n\treturn &longConn{\n\t\tConn:    c,\n\t\taddress: addr,\n\t}, nil\n}\n\n// Put puts a connection back to the peer.\nfunc (p *peer) Put(c *longConn) error {\n\tif !p.globalIdle.Inc() {\n\t\treturn c.Close()\n\t}\n\tif !p.pool.Put(c) {\n\t\tp.globalIdle.Dec()\n\t\treturn c.Close()\n\t}\n\treturn nil\n}\n\nfunc (p *peer) Len() int {\n\treturn p.pool.Len()\n}\n\nfunc (p *peer) Evict() {\n\tn := p.pool.Evict()\n\tp.globalIdle.DecN(int64(n))\n}\n\n// Close closes the peer and all the connections in the ring.\nfunc (p *peer) Close() {\n\tn := p.pool.Close()\n\tp.globalIdle.DecN(int64(n))\n}\n\n// NewLongPool creates a long pool using the given IdleConfig.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// LongPool manages a pool of long connections.\ntype LongPool struct {\n\treporter     Reporter\n\tpeerMap      sync.Map\n\tnewPeer      func(net.Addr) *peer\n\tglobalIdle   *utils.MaxCounter\n\tidleConfig   connpool.IdleConfig\n\tsharedTicker *utils.SharedTicker\n\tclosed       int32 // active: 0, closed: 1\n}\n\n// Get pick or generate a net.Conn and return\n// The context is not used but leave it for now.\nfunc (lp *LongPool) Get(ctx context.Context, network, address string, opt remote.ConnOption) (net.Conn, error) {\n\taddr := netAddr{network, address}\n\tp := lp.getPeer(addr)\n\treturn p.Get(opt.Dialer, opt.ConnectTimeout, lp.reporter, address)\n}\n\n// Put implements the ConnPool interface.\nfunc (lp *LongPool) Put(conn net.Conn) error {\n\tc, ok := conn.(*longConn)\n\tif !ok {\n\t\treturn conn.Close()\n\t}\n\n\taddr := conn.RemoteAddr()\n\tna := netAddr{addr.Network(), c.address}\n\tp, ok := lp.peerMap.Load(na)\n\tif ok {\n\t\tp.(*peer).Put(c)\n\t\treturn nil\n\t}\n\treturn c.Conn.Close()\n}\n\n// Discard implements the ConnPool interface.\nfunc (lp *LongPool) Discard(conn net.Conn) error {\n\tc, ok := conn.(*longConn)\n\tif ok {\n\t\treturn c.Close()\n\t}\n\treturn conn.Close()\n}\n\n// Clean implements the LongConnPool interface.\nfunc (lp *LongPool) Clean(network, address string) {\n\tna := netAddr{network, address}\n\tif p, ok := lp.peerMap.Load(na); ok {\n\t\tlp.peerMap.Delete(na)\n\t\tgo p.(*peer).Close()\n\t}\n}\n\n// Dump is used to dump current long pool info when needed, like debug query.\nfunc (lp *LongPool) Dump() interface{} {\n\tm := make(map[string]interface{})\n\tm[configDumpKey] = lp.idleConfig\n\tlp.peerMap.Range(func(key, value interface{}) bool {\n\t\tt := value.(*peer).pool.Dump()\n\t\tm[key.(netAddr).String()] = t\n\t\treturn true\n\t})\n\treturn m\n}\n\n// Close releases all peers in the pool, it is executed when client is closed.\nfunc (lp *LongPool) Close() error {\n\tif !atomic.CompareAndSwapInt32(&lp.closed, 0, 1) {\n\t\treturn fmt.Errorf(\"long pool is already closed\")\n\t}\n\t// close all peers\n\tlp.peerMap.Range(func(addr, value interface{}) bool {\n\t\tlp.peerMap.Delete(addr)\n\t\tv := value.(*peer)\n\t\tv.Close()\n\t\treturn true\n\t})\n\t// remove from the shared ticker\n\tlp.sharedTicker.Delete(lp)\n\treturn nil\n}\n\n// EnableReporter enable reporter for long connection pool.\nfunc (lp *LongPool) EnableReporter() {\n\tlp.reporter = GetCommonReporter()\n}\n\n// WarmUp implements the warmup.Pool interface.\nfunc (lp *LongPool) WarmUp(eh warmup.ErrorHandling, wuo *warmup.PoolOption, co remote.ConnOption) error {\n\th := &warmup.PoolHelper{ErrorHandling: eh}\n\treturn h.WarmUp(wuo, lp, co)\n}\n\n// Evict cleanups the idle connections in peers.\nfunc (lp *LongPool) Evict() {\n\tif atomic.LoadInt32(&lp.closed) == 0 {\n\t\t// Evict idle connections\n\t\tlp.peerMap.Range(func(key, value interface{}) bool {\n\t\t\tp := value.(*peer)\n\t\t\tp.Evict()\n\t\t\treturn true\n\t\t})\n\t}\n}\n\n// Tick implements the interface utils.TickerTask.\nfunc (lp *LongPool) Tick() {\n\tlp.Evict()\n}\n\n// getPeer gets a peer from the pool based on the addr, or create a new one if not exist.\nfunc (lp *LongPool) getPeer(addr netAddr) *peer {\n\tp, ok := lp.peerMap.Load(addr)\n\tif ok {\n\t\treturn p.(*peer)\n\t}\n\tp, _ = lp.peerMap.LoadOrStore(addr, lp.newPeer(addr))\n\treturn p.(*peer)\n}\n\n```\n# Function Signature: func (c *longConn) IsActive() bool {\n# Natural Language Annotations: // IsActive indicates whether the connection is active.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (c *longConn) IsActive() bool {\n\treturn time.Now().Before(c.deadline)\n}\n```", "  ```\nfunc (c *longConn) IsActive() bool {\n\treturn c.Conn != nil && !c.Expired()\n}\n```", "  ```\nfunc (c *longConn) IsActive() bool {\n\treturn time.Now().After(c.deadline)\n}\n```"]}, "kitex-pkg/remote/connpool/long_pool.go-Evict": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/connpool/long_pool.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package connpool provide short connection and long connection pool.\npackage connpool\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n)\n\nvar (\n\t_ net.Conn            = &longConn{}\n\t_ remote.LongConnPool = &LongPool{}\n\n\t// global shared tickers for different LongPool\n\tsharedTickers sync.Map\n)\n\nconst (\n\tconfigDumpKey = \"idle_config\"\n)\n\nfunc getSharedTicker(p *LongPool, refreshInterval time.Duration) *utils.SharedTicker {\n\tsti, ok := sharedTickers.Load(refreshInterval)\n\tif ok {\n\t\tst := sti.(*utils.SharedTicker)\n\t\tst.Add(p)\n\t\treturn st\n\t}\n\tsti, _ = sharedTickers.LoadOrStore(refreshInterval, utils.NewSharedTicker(refreshInterval))\n\tst := sti.(*utils.SharedTicker)\n\tst.Add(p)\n\treturn st\n}\n\n// netAddr implements the net.Addr interface and comparability.\ntype netAddr struct {\n\tnetwork string\n\taddress string\n}\n\n// Network implements the net.Addr interface.\nfunc (na netAddr) Network() string { return na.network }\n\n// String implements the net.Addr interface.\nfunc (na netAddr) String() string { return na.address }\n\n// longConn implements the net.Conn interface.\ntype longConn struct {\n\tnet.Conn\n\tsync.RWMutex\n\tdeadline time.Time\n\taddress  string\n}\n\n// Close implements the net.Conn interface.\nfunc (c *longConn) Close() error {\n\treturn c.Conn.Close()\n}\n\n// RawConn returns the real underlying net.Conn.\nfunc (c *longConn) RawConn() net.Conn {\n\treturn c.Conn\n}\n\n// IsActive indicates whether the connection is active.\n\n\n\n\n\n\n\n\n// Expired checks the deadline of the connection.\nfunc (c *longConn) Expired() bool {\n\treturn time.Now().After(c.deadline)\n}\n\ntype PoolDump struct {\n\tIdleNum       int         `json:\"idle_num\"`\n\tConnsDeadline []time.Time `json:\"conns_deadline\"`\n}\n\nfunc newPool(minIdle, maxIdle int, maxIdleTimeout time.Duration) *pool {\n\tp := &pool{\n\t\tidleList:       make([]*longConn, 0, maxIdle),\n\t\tminIdle:        minIdle,\n\t\tmaxIdle:        maxIdle,\n\t\tmaxIdleTimeout: maxIdleTimeout,\n\t}\n\treturn p\n}\n\n// pool implements a pool of long connections.\ntype pool struct {\n\tidleList []*longConn // idleList Get/Put by FILO(stack) but Evict by FIFO(queue)\n\tmu       sync.RWMutex\n\t// config\n\tminIdle        int\n\tmaxIdle        int           // currIdle <= maxIdle.\n\tmaxIdleTimeout time.Duration // the idle connection will be cleaned if the idle time exceeds maxIdleTimeout.\n}\n\n// Get gets the first active connection from the idleList. Return the number of connections decreased during the Get.\nfunc (p *pool) Get() (*longConn, bool, int) {\n\tp.mu.Lock()\n\t// Get the first active one\n\tn := len(p.idleList)\n\tselected := n - 1\n\tfor ; selected >= 0; selected-- {\n\t\to := p.idleList[selected]\n\t\t// reset slice element to nil, active conn object only could be hold reference by user function\n\t\tp.idleList[selected] = nil\n\t\tif o.IsActive() {\n\t\t\tp.idleList = p.idleList[:selected]\n\t\t\tp.mu.Unlock()\n\t\t\treturn o, true, n - selected\n\t\t}\n\t\t// inactive object\n\t\to.Close()\n\t}\n\t// in case all objects are inactive\n\tif selected < 0 {\n\t\tselected = 0\n\t}\n\tp.idleList = p.idleList[:selected]\n\tp.mu.Unlock()\n\treturn nil, false, n - selected\n}\n\n// Put puts back a connection to the pool.\n\n\n\n\n\n\n\n\n\n\n\n\n// Evict cleanups the expired connections.\n// Evict returns how many connections has been evicted.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Len returns the length of the pool.\nfunc (p *pool) Len() int {\n\tp.mu.RLock()\n\tl := len(p.idleList)\n\tp.mu.RUnlock()\n\treturn l\n}\n\n// Close closes the pool and all the objects in the pool.\nfunc (p *pool) Close() int {\n\tp.mu.Lock()\n\tnum := len(p.idleList)\n\tfor i := 0; i < num; i++ {\n\t\tp.idleList[i].Close()\n\t}\n\tp.idleList = nil\n\n\tp.mu.Unlock()\n\treturn num\n}\n\n// Dump dumps the info of all the objects in the pool.\nfunc (p *pool) Dump() PoolDump {\n\tp.mu.RLock()\n\tidleNum := len(p.idleList)\n\tconnsDeadline := make([]time.Time, idleNum)\n\tfor i := 0; i < idleNum; i++ {\n\t\tconnsDeadline[i] = p.idleList[i].deadline\n\t}\n\ts := PoolDump{\n\t\tIdleNum:       idleNum,\n\t\tConnsDeadline: connsDeadline,\n\t}\n\tp.mu.RUnlock()\n\treturn s\n}\n\nfunc newPeer(\n\tserviceName string,\n\taddr net.Addr,\n\tminIdle int,\n\tmaxIdle int,\n\tmaxIdleTimeout time.Duration,\n\tglobalIdle *utils.MaxCounter,\n) *peer {\n\treturn &peer{\n\t\tserviceName: serviceName,\n\t\taddr:        addr,\n\t\tglobalIdle:  globalIdle,\n\t\tpool:        newPool(minIdle, maxIdle, maxIdleTimeout),\n\t}\n}\n\n// peer has one address, it manages all connections base on this address\ntype peer struct {\n\t// info\n\tserviceName string\n\taddr        net.Addr\n\tglobalIdle  *utils.MaxCounter\n\t// pool\n\tpool *pool\n}\n\n// Get gets a connection with dialer and timeout. Dial a new connection if no idle connection in pool is available.\nfunc (p *peer) Get(d remote.Dialer, timeout time.Duration, reporter Reporter, addr string) (net.Conn, error) {\n\tvar c net.Conn\n\tc, reused, decNum := p.pool.Get()\n\tp.globalIdle.DecN(int64(decNum))\n\tif reused {\n\t\treporter.ReuseSucceed(Long, p.serviceName, p.addr)\n\t\treturn c, nil\n\t}\n\t// dial a new connection\n\tc, err := d.DialTimeout(p.addr.Network(), p.addr.String(), timeout)\n\tif err != nil {\n\t\treporter.ConnFailed(Long, p.serviceName, p.addr)\n\t\treturn nil, err\n\t}\n\treporter.ConnSucceed(Long, p.serviceName, p.addr)\n\treturn &longConn{\n\t\tConn:    c,\n\t\taddress: addr,\n\t}, nil\n}\n\n// Put puts a connection back to the peer.\nfunc (p *peer) Put(c *longConn) error {\n\tif !p.globalIdle.Inc() {\n\t\treturn c.Close()\n\t}\n\tif !p.pool.Put(c) {\n\t\tp.globalIdle.Dec()\n\t\treturn c.Close()\n\t}\n\treturn nil\n}\n\nfunc (p *peer) Len() int {\n\treturn p.pool.Len()\n}\n\nfunc (p *peer) Evict() {\n\tn := p.pool.Evict()\n\tp.globalIdle.DecN(int64(n))\n}\n\n// Close closes the peer and all the connections in the ring.\nfunc (p *peer) Close() {\n\tn := p.pool.Close()\n\tp.globalIdle.DecN(int64(n))\n}\n\n// NewLongPool creates a long pool using the given IdleConfig.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// LongPool manages a pool of long connections.\ntype LongPool struct {\n\treporter     Reporter\n\tpeerMap      sync.Map\n\tnewPeer      func(net.Addr) *peer\n\tglobalIdle   *utils.MaxCounter\n\tidleConfig   connpool.IdleConfig\n\tsharedTicker *utils.SharedTicker\n\tclosed       int32 // active: 0, closed: 1\n}\n\n// Get pick or generate a net.Conn and return\n// The context is not used but leave it for now.\nfunc (lp *LongPool) Get(ctx context.Context, network, address string, opt remote.ConnOption) (net.Conn, error) {\n\taddr := netAddr{network, address}\n\tp := lp.getPeer(addr)\n\treturn p.Get(opt.Dialer, opt.ConnectTimeout, lp.reporter, address)\n}\n\n// Put implements the ConnPool interface.\nfunc (lp *LongPool) Put(conn net.Conn) error {\n\tc, ok := conn.(*longConn)\n\tif !ok {\n\t\treturn conn.Close()\n\t}\n\n\taddr := conn.RemoteAddr()\n\tna := netAddr{addr.Network(), c.address}\n\tp, ok := lp.peerMap.Load(na)\n\tif ok {\n\t\tp.(*peer).Put(c)\n\t\treturn nil\n\t}\n\treturn c.Conn.Close()\n}\n\n// Discard implements the ConnPool interface.\nfunc (lp *LongPool) Discard(conn net.Conn) error {\n\tc, ok := conn.(*longConn)\n\tif ok {\n\t\treturn c.Close()\n\t}\n\treturn conn.Close()\n}\n\n// Clean implements the LongConnPool interface.\nfunc (lp *LongPool) Clean(network, address string) {\n\tna := netAddr{network, address}\n\tif p, ok := lp.peerMap.Load(na); ok {\n\t\tlp.peerMap.Delete(na)\n\t\tgo p.(*peer).Close()\n\t}\n}\n\n// Dump is used to dump current long pool info when needed, like debug query.\nfunc (lp *LongPool) Dump() interface{} {\n\tm := make(map[string]interface{})\n\tm[configDumpKey] = lp.idleConfig\n\tlp.peerMap.Range(func(key, value interface{}) bool {\n\t\tt := value.(*peer).pool.Dump()\n\t\tm[key.(netAddr).String()] = t\n\t\treturn true\n\t})\n\treturn m\n}\n\n// Close releases all peers in the pool, it is executed when client is closed.\nfunc (lp *LongPool) Close() error {\n\tif !atomic.CompareAndSwapInt32(&lp.closed, 0, 1) {\n\t\treturn fmt.Errorf(\"long pool is already closed\")\n\t}\n\t// close all peers\n\tlp.peerMap.Range(func(addr, value interface{}) bool {\n\t\tlp.peerMap.Delete(addr)\n\t\tv := value.(*peer)\n\t\tv.Close()\n\t\treturn true\n\t})\n\t// remove from the shared ticker\n\tlp.sharedTicker.Delete(lp)\n\treturn nil\n}\n\n// EnableReporter enable reporter for long connection pool.\nfunc (lp *LongPool) EnableReporter() {\n\tlp.reporter = GetCommonReporter()\n}\n\n// WarmUp implements the warmup.Pool interface.\nfunc (lp *LongPool) WarmUp(eh warmup.ErrorHandling, wuo *warmup.PoolOption, co remote.ConnOption) error {\n\th := &warmup.PoolHelper{ErrorHandling: eh}\n\treturn h.WarmUp(wuo, lp, co)\n}\n\n// Evict cleanups the idle connections in peers.\nfunc (lp *LongPool) Evict() {\n\tif atomic.LoadInt32(&lp.closed) == 0 {\n\t\t// Evict idle connections\n\t\tlp.peerMap.Range(func(key, value interface{}) bool {\n\t\t\tp := value.(*peer)\n\t\t\tp.Evict()\n\t\t\treturn true\n\t\t})\n\t}\n}\n\n// Tick implements the interface utils.TickerTask.\nfunc (lp *LongPool) Tick() {\n\tlp.Evict()\n}\n\n// getPeer gets a peer from the pool based on the addr, or create a new one if not exist.\nfunc (lp *LongPool) getPeer(addr netAddr) *peer {\n\tp, ok := lp.peerMap.Load(addr)\n\tif ok {\n\t\treturn p.(*peer)\n\t}\n\tp, _ = lp.peerMap.LoadOrStore(addr, lp.newPeer(addr))\n\treturn p.(*peer)\n}\n\n```\n# Function Signature: func (p *pool) Evict() (evicted int) {\n# Natural Language Annotations: // Evict cleanups the expired connections.\n// Evict returns how many connections has been evicted.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (p *pool) Evict() (evicted int) {\n\tp.mu.Lock()\n\tn := len(p.idleList)\n\tselected := n - 1\n\tfor ; selected >= 0; selected-- {\n\t\to := p.idleList[selected]\n\t\tif o.IsActive() {\n\t\t\tcontinue\n\t\t}\n\t\t// inactive object\n\t\to.Close()\n\t\tevicted++\n\t}\n\tp.idleList = p.idleList[:selected]\n\tp.mu.Unlock()\n\treturn evicted\n}\n```", "  ```\nfunc (p *pool) Evict() (evicted int) {\n\tp.mu.Lock()\n\tdefer p.mu.Unlock()\n\tvar numIdle, idleTimeout int\n\tfor _, conn := range p.idleList {\n\t\tif conn.Expired() {\n\t\t\tconn.Close()\n\t\t\tnumIdle++\n\t\t} else {\n\t\t\tidleTimeout++\n\t\t}\n\t}\n\tp.idleList = p.idleList[:cap(p.idleList)-numIdle]\n\treturn numIdle\n}\n```", "  ```\nfunc (p *pool) Evict() (evicted int) {\n\tp.mu.Lock()\n\tdefer p.mu.Unlock()\n\t// if peek is active\n\tif p.idleList.Front != nil {\n\t\tfor e := p.idleList.Front(); e != nil; {\n\t\t\tconn := e.Value.(*longConn)\n\t\t\tif conn.IsActive() {\n\t\t\t\te = e.Next()\n\t\t\t} else {\n\t\t\t\tevicted++\n\t\t\t\tp.idleList = p.idleList.Remove(e.Next())\n\t\t\t\te.Value = nil\n\t\t\t\te = e.Next()\n\t\t\t}\n\t\t}\n\t}\n\treturn evicted\n}\n```"]}, "kitex-pkg/remote/connpool/long_pool.go-NewLongPool": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/connpool/long_pool.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package connpool provide short connection and long connection pool.\npackage connpool\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n)\n\nvar (\n\t_ net.Conn            = &longConn{}\n\t_ remote.LongConnPool = &LongPool{}\n\n\t// global shared tickers for different LongPool\n\tsharedTickers sync.Map\n)\n\nconst (\n\tconfigDumpKey = \"idle_config\"\n)\n\nfunc getSharedTicker(p *LongPool, refreshInterval time.Duration) *utils.SharedTicker {\n\tsti, ok := sharedTickers.Load(refreshInterval)\n\tif ok {\n\t\tst := sti.(*utils.SharedTicker)\n\t\tst.Add(p)\n\t\treturn st\n\t}\n\tsti, _ = sharedTickers.LoadOrStore(refreshInterval, utils.NewSharedTicker(refreshInterval))\n\tst := sti.(*utils.SharedTicker)\n\tst.Add(p)\n\treturn st\n}\n\n// netAddr implements the net.Addr interface and comparability.\ntype netAddr struct {\n\tnetwork string\n\taddress string\n}\n\n// Network implements the net.Addr interface.\nfunc (na netAddr) Network() string { return na.network }\n\n// String implements the net.Addr interface.\nfunc (na netAddr) String() string { return na.address }\n\n// longConn implements the net.Conn interface.\ntype longConn struct {\n\tnet.Conn\n\tsync.RWMutex\n\tdeadline time.Time\n\taddress  string\n}\n\n// Close implements the net.Conn interface.\nfunc (c *longConn) Close() error {\n\treturn c.Conn.Close()\n}\n\n// RawConn returns the real underlying net.Conn.\nfunc (c *longConn) RawConn() net.Conn {\n\treturn c.Conn\n}\n\n// IsActive indicates whether the connection is active.\n\n\n\n\n\n\n\n\n// Expired checks the deadline of the connection.\nfunc (c *longConn) Expired() bool {\n\treturn time.Now().After(c.deadline)\n}\n\ntype PoolDump struct {\n\tIdleNum       int         `json:\"idle_num\"`\n\tConnsDeadline []time.Time `json:\"conns_deadline\"`\n}\n\nfunc newPool(minIdle, maxIdle int, maxIdleTimeout time.Duration) *pool {\n\tp := &pool{\n\t\tidleList:       make([]*longConn, 0, maxIdle),\n\t\tminIdle:        minIdle,\n\t\tmaxIdle:        maxIdle,\n\t\tmaxIdleTimeout: maxIdleTimeout,\n\t}\n\treturn p\n}\n\n// pool implements a pool of long connections.\ntype pool struct {\n\tidleList []*longConn // idleList Get/Put by FILO(stack) but Evict by FIFO(queue)\n\tmu       sync.RWMutex\n\t// config\n\tminIdle        int\n\tmaxIdle        int           // currIdle <= maxIdle.\n\tmaxIdleTimeout time.Duration // the idle connection will be cleaned if the idle time exceeds maxIdleTimeout.\n}\n\n// Get gets the first active connection from the idleList. Return the number of connections decreased during the Get.\nfunc (p *pool) Get() (*longConn, bool, int) {\n\tp.mu.Lock()\n\t// Get the first active one\n\tn := len(p.idleList)\n\tselected := n - 1\n\tfor ; selected >= 0; selected-- {\n\t\to := p.idleList[selected]\n\t\t// reset slice element to nil, active conn object only could be hold reference by user function\n\t\tp.idleList[selected] = nil\n\t\tif o.IsActive() {\n\t\t\tp.idleList = p.idleList[:selected]\n\t\t\tp.mu.Unlock()\n\t\t\treturn o, true, n - selected\n\t\t}\n\t\t// inactive object\n\t\to.Close()\n\t}\n\t// in case all objects are inactive\n\tif selected < 0 {\n\t\tselected = 0\n\t}\n\tp.idleList = p.idleList[:selected]\n\tp.mu.Unlock()\n\treturn nil, false, n - selected\n}\n\n// Put puts back a connection to the pool.\n\n\n\n\n\n\n\n\n\n\n\n\n// Evict cleanups the expired connections.\n// Evict returns how many connections has been evicted.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Len returns the length of the pool.\nfunc (p *pool) Len() int {\n\tp.mu.RLock()\n\tl := len(p.idleList)\n\tp.mu.RUnlock()\n\treturn l\n}\n\n// Close closes the pool and all the objects in the pool.\nfunc (p *pool) Close() int {\n\tp.mu.Lock()\n\tnum := len(p.idleList)\n\tfor i := 0; i < num; i++ {\n\t\tp.idleList[i].Close()\n\t}\n\tp.idleList = nil\n\n\tp.mu.Unlock()\n\treturn num\n}\n\n// Dump dumps the info of all the objects in the pool.\nfunc (p *pool) Dump() PoolDump {\n\tp.mu.RLock()\n\tidleNum := len(p.idleList)\n\tconnsDeadline := make([]time.Time, idleNum)\n\tfor i := 0; i < idleNum; i++ {\n\t\tconnsDeadline[i] = p.idleList[i].deadline\n\t}\n\ts := PoolDump{\n\t\tIdleNum:       idleNum,\n\t\tConnsDeadline: connsDeadline,\n\t}\n\tp.mu.RUnlock()\n\treturn s\n}\n\nfunc newPeer(\n\tserviceName string,\n\taddr net.Addr,\n\tminIdle int,\n\tmaxIdle int,\n\tmaxIdleTimeout time.Duration,\n\tglobalIdle *utils.MaxCounter,\n) *peer {\n\treturn &peer{\n\t\tserviceName: serviceName,\n\t\taddr:        addr,\n\t\tglobalIdle:  globalIdle,\n\t\tpool:        newPool(minIdle, maxIdle, maxIdleTimeout),\n\t}\n}\n\n// peer has one address, it manages all connections base on this address\ntype peer struct {\n\t// info\n\tserviceName string\n\taddr        net.Addr\n\tglobalIdle  *utils.MaxCounter\n\t// pool\n\tpool *pool\n}\n\n// Get gets a connection with dialer and timeout. Dial a new connection if no idle connection in pool is available.\nfunc (p *peer) Get(d remote.Dialer, timeout time.Duration, reporter Reporter, addr string) (net.Conn, error) {\n\tvar c net.Conn\n\tc, reused, decNum := p.pool.Get()\n\tp.globalIdle.DecN(int64(decNum))\n\tif reused {\n\t\treporter.ReuseSucceed(Long, p.serviceName, p.addr)\n\t\treturn c, nil\n\t}\n\t// dial a new connection\n\tc, err := d.DialTimeout(p.addr.Network(), p.addr.String(), timeout)\n\tif err != nil {\n\t\treporter.ConnFailed(Long, p.serviceName, p.addr)\n\t\treturn nil, err\n\t}\n\treporter.ConnSucceed(Long, p.serviceName, p.addr)\n\treturn &longConn{\n\t\tConn:    c,\n\t\taddress: addr,\n\t}, nil\n}\n\n// Put puts a connection back to the peer.\nfunc (p *peer) Put(c *longConn) error {\n\tif !p.globalIdle.Inc() {\n\t\treturn c.Close()\n\t}\n\tif !p.pool.Put(c) {\n\t\tp.globalIdle.Dec()\n\t\treturn c.Close()\n\t}\n\treturn nil\n}\n\nfunc (p *peer) Len() int {\n\treturn p.pool.Len()\n}\n\nfunc (p *peer) Evict() {\n\tn := p.pool.Evict()\n\tp.globalIdle.DecN(int64(n))\n}\n\n// Close closes the peer and all the connections in the ring.\nfunc (p *peer) Close() {\n\tn := p.pool.Close()\n\tp.globalIdle.DecN(int64(n))\n}\n\n// NewLongPool creates a long pool using the given IdleConfig.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// LongPool manages a pool of long connections.\ntype LongPool struct {\n\treporter     Reporter\n\tpeerMap      sync.Map\n\tnewPeer      func(net.Addr) *peer\n\tglobalIdle   *utils.MaxCounter\n\tidleConfig   connpool.IdleConfig\n\tsharedTicker *utils.SharedTicker\n\tclosed       int32 // active: 0, closed: 1\n}\n\n// Get pick or generate a net.Conn and return\n// The context is not used but leave it for now.\nfunc (lp *LongPool) Get(ctx context.Context, network, address string, opt remote.ConnOption) (net.Conn, error) {\n\taddr := netAddr{network, address}\n\tp := lp.getPeer(addr)\n\treturn p.Get(opt.Dialer, opt.ConnectTimeout, lp.reporter, address)\n}\n\n// Put implements the ConnPool interface.\nfunc (lp *LongPool) Put(conn net.Conn) error {\n\tc, ok := conn.(*longConn)\n\tif !ok {\n\t\treturn conn.Close()\n\t}\n\n\taddr := conn.RemoteAddr()\n\tna := netAddr{addr.Network(), c.address}\n\tp, ok := lp.peerMap.Load(na)\n\tif ok {\n\t\tp.(*peer).Put(c)\n\t\treturn nil\n\t}\n\treturn c.Conn.Close()\n}\n\n// Discard implements the ConnPool interface.\nfunc (lp *LongPool) Discard(conn net.Conn) error {\n\tc, ok := conn.(*longConn)\n\tif ok {\n\t\treturn c.Close()\n\t}\n\treturn conn.Close()\n}\n\n// Clean implements the LongConnPool interface.\nfunc (lp *LongPool) Clean(network, address string) {\n\tna := netAddr{network, address}\n\tif p, ok := lp.peerMap.Load(na); ok {\n\t\tlp.peerMap.Delete(na)\n\t\tgo p.(*peer).Close()\n\t}\n}\n\n// Dump is used to dump current long pool info when needed, like debug query.\nfunc (lp *LongPool) Dump() interface{} {\n\tm := make(map[string]interface{})\n\tm[configDumpKey] = lp.idleConfig\n\tlp.peerMap.Range(func(key, value interface{}) bool {\n\t\tt := value.(*peer).pool.Dump()\n\t\tm[key.(netAddr).String()] = t\n\t\treturn true\n\t})\n\treturn m\n}\n\n// Close releases all peers in the pool, it is executed when client is closed.\nfunc (lp *LongPool) Close() error {\n\tif !atomic.CompareAndSwapInt32(&lp.closed, 0, 1) {\n\t\treturn fmt.Errorf(\"long pool is already closed\")\n\t}\n\t// close all peers\n\tlp.peerMap.Range(func(addr, value interface{}) bool {\n\t\tlp.peerMap.Delete(addr)\n\t\tv := value.(*peer)\n\t\tv.Close()\n\t\treturn true\n\t})\n\t// remove from the shared ticker\n\tlp.sharedTicker.Delete(lp)\n\treturn nil\n}\n\n// EnableReporter enable reporter for long connection pool.\nfunc (lp *LongPool) EnableReporter() {\n\tlp.reporter = GetCommonReporter()\n}\n\n// WarmUp implements the warmup.Pool interface.\nfunc (lp *LongPool) WarmUp(eh warmup.ErrorHandling, wuo *warmup.PoolOption, co remote.ConnOption) error {\n\th := &warmup.PoolHelper{ErrorHandling: eh}\n\treturn h.WarmUp(wuo, lp, co)\n}\n\n// Evict cleanups the idle connections in peers.\nfunc (lp *LongPool) Evict() {\n\tif atomic.LoadInt32(&lp.closed) == 0 {\n\t\t// Evict idle connections\n\t\tlp.peerMap.Range(func(key, value interface{}) bool {\n\t\t\tp := value.(*peer)\n\t\t\tp.Evict()\n\t\t\treturn true\n\t\t})\n\t}\n}\n\n// Tick implements the interface utils.TickerTask.\nfunc (lp *LongPool) Tick() {\n\tlp.Evict()\n}\n\n// getPeer gets a peer from the pool based on the addr, or create a new one if not exist.\nfunc (lp *LongPool) getPeer(addr netAddr) *peer {\n\tp, ok := lp.peerMap.Load(addr)\n\tif ok {\n\t\treturn p.(*peer)\n\t}\n\tp, _ = lp.peerMap.LoadOrStore(addr, lp.newPeer(addr))\n\treturn p.(*peer)\n}\n\n```\n# Function Signature: func NewLongPool(serviceName string, idlConfig connpool.IdleConfig) *LongPool {\n# Natural Language Annotations: // NewLongPool creates a long pool using the given IdleConfig.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewLongPool(serviceName string, idleConfig connpool.IdleConfig) *LongPool {\n\treturn &LongPool{\n\t\treporter:     nil,\n\t\tpeerMap:      sync.Map{},\n\t\tnewPeer:      nil,\n\t\tglobalIdle:   utils.NewMaxCounter(),\n\t\tidleConfig:   idleConfig,\n\t\tsharedTicker: getSharedTicker(&LongPool{}, idleConfig.RefreshInterval),\n\t\tclosed:       0,\n\t}\n}\n```", "  ```\nfunc NewLongPool(service string, idleConfig connpool.IdleConfig) *LongPool {\n\tgIDLE := utils.NewMaxCounter()\n\tsharedTicker := utils.NewSharedTicker(idleConfig.AuthRefreshInterval)\n\n\tnewPeer := func(na net.Addr) *peer {\n\t\treturn newPeer(service, na, idleConfig.MinIdle, idleConfig.MaxIdle, idleConfig.MaxIdleTimeout, gIDLE)\n\t}\n\n\tlp := &LongPool{\n\t\treporter:     nil,\n\t\tpeerMap:      sync.Map{},\n\t\tnewPeer:      newPeer,\n\t\tglobalIdle:   gIDLE,\n\t\tidleConfig:   idleConfig,\n\t\tsharedTicker: sharedTicker,\n\t\tclosed:       0,\n\t}\n\tsharedTicker.Add(lp)\n\treturn lp\n}\n```", "  ```\nfunc NewLongPool(serviceName string, idleConfig connpool.IdleConfig) *LongPool {\n\t// init global idle counter\n\tglobalIdle := utils.NewMaxCounter(idleConfig.MaxIdle)\n\t// init shared ticker\n\tsharedTicker := getSharedTicker(nil, idleConfig.IdleTime())\n\n\tlp := &LongPool{\n\t\tidleConfig:   idleConfig,\n\t\tglobalIdle:   globalIdle,\n\t\tsharedTicker: sharedTicker,\n\t\tpeerMap:      sync.Map{},\n\t}\n\n\t// init new peer\n\tlp.newPeer = func(addr net.Addr) *peer {\n\t\t// #nosec\n\t\tnetAddr := addr.(netAddr)\n\t\treturn newPeer(serviceName, netAddr, idleConfig.MinIdle, idleConfig.MaxIdle, idleConfig.IdleTime(), globalIdle)\n\t}\n\n\t// add ticker task\n\tsharedTicker.Add(lp)\n\n\treturn lp\n}\n```"]}, "kitex-pkg/loadbalance/lbcache/cache.go-NewBalancerFactory": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/loadbalance/lbcache/cache.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package lbcache combine balancer with resolver and cache the resolve result\npackage lbcache\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"golang.org/x/sync/singleflight\"\n\n\t\"github.com/cloudwego/kitex/pkg/diagnosis\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\nconst (\n\tdefaultRefreshInterval = 5 * time.Second\n\tdefaultExpireInterval  = 15 * time.Second\n)\n\nvar (\n\tbalancerFactories    sync.Map // key: resolver name + loadbalance name\n\tbalancerFactoriesSfg singleflight.Group\n)\n\n// Options for create builder\ntype Options struct {\n\t// refresh discovery result timely\n\tRefreshInterval time.Duration\n\n\t// Balancer expire check interval\n\t// we need remove idle Balancers for resource saving\n\tExpireInterval time.Duration\n\n\t// DiagnosisService is used register info for diagnosis\n\tDiagnosisService diagnosis.Service\n\n\t// Cacheable is used to indicate that if the factory could be shared between multi clients\n\tCacheable bool\n}\n\nfunc (v *Options) check() {\n\tif v.RefreshInterval <= 0 {\n\t\tv.RefreshInterval = defaultRefreshInterval\n\t}\n\tif v.ExpireInterval <= 0 {\n\t\tv.ExpireInterval = defaultExpireInterval\n\t}\n}\n\n// Hookable add hook for rebalancer events\ntype Hookable interface {\n\t// register loadbalance rebalance hook for Rebalance events\n\tRegisterRebalanceHook(func(ch *discovery.Change)) (index int)\n\tDeregisterRebalanceHook(index int)\n\t// register loadbalance delete hook for Delete events\n\tRegisterDeleteHook(func(ch *discovery.Change)) (index int)\n\tDeregisterDeleteHook(index int)\n}\n\n// BalancerFactory get or create a balancer with given target\n// if it has the same key(reslover.Target(target)), we will cache and reuse the Balance\ntype BalancerFactory struct {\n\tHookable\n\topts       Options\n\tcache      sync.Map // key -> LoadBalancer\n\tresolver   discovery.Resolver\n\tbalancer   loadbalance.Loadbalancer\n\trebalancer loadbalance.Rebalancer\n\tsfg        singleflight.Group\n}\n\nfunc cacheKey(resolver, balancer string, opts Options) string {\n\treturn fmt.Sprintf(\"%s|%s|{%s %s}\", resolver, balancer, opts.RefreshInterval, opts.ExpireInterval)\n}\n\nfunc newBalancerFactory(resolver discovery.Resolver, balancer loadbalance.Loadbalancer, opts Options) *BalancerFactory {\n\tb := &BalancerFactory{\n\t\topts:     opts,\n\t\tresolver: resolver,\n\t\tbalancer: balancer,\n\t}\n\tif rb, ok := balancer.(loadbalance.Rebalancer); ok {\n\t\thrb := newHookRebalancer(rb)\n\t\tb.rebalancer = hrb\n\t\tb.Hookable = hrb\n\t} else {\n\t\tb.Hookable = noopHookRebalancer{}\n\t}\n\tgo b.watcher()\n\treturn b\n}\n\n// NewBalancerFactory get or create a balancer factory for balancer instance\n// cache key with resolver name, balancer name and options\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// watch expired balancer\nfunc (b *BalancerFactory) watcher() {\n\tfor range time.Tick(b.opts.ExpireInterval) {\n\t\tb.cache.Range(func(key, value interface{}) bool {\n\t\t\tbl := value.(*Balancer)\n\t\t\tif atomic.CompareAndSwapInt32(&bl.expire, 0, 1) {\n\t\t\t\t// 1. set expire flag\n\t\t\t\t// 2. wait next ticker for collect, maybe the balancer is used again\n\t\t\t\t// (avoid being immediate delete the balancer which had been created recently)\n\t\t\t} else {\n\t\t\t\tb.cache.Delete(key)\n\t\t\t\tbl.close()\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t}\n}\n\n// cache key with resolver name prefix avoid conflict for balancer\nfunc renameResultCacheKey(res *discovery.Result, resolverName string) {\n\tres.CacheKey = resolverName + \":\" + res.CacheKey\n}\n\n// Get create a new balancer if not exists\nfunc (b *BalancerFactory) Get(ctx context.Context, target rpcinfo.EndpointInfo) (*Balancer, error) {\n\tdesc := b.resolver.Target(ctx, target)\n\tval, ok := b.cache.Load(desc)\n\tif ok {\n\t\treturn val.(*Balancer), nil\n\t}\n\tval, err, _ := b.sfg.Do(desc, func() (interface{}, error) {\n\t\tres, err := b.resolver.Resolve(ctx, desc)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\trenameResultCacheKey(&res, b.resolver.Name())\n\t\tbl := &Balancer{\n\t\t\tb:      b,\n\t\t\ttarget: desc,\n\t\t}\n\t\tbl.res.Store(res)\n\t\tbl.sharedTicker = getSharedTicker(bl, b.opts.RefreshInterval)\n\t\tb.cache.Store(desc, bl)\n\t\treturn bl, nil\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn val.(*Balancer), nil\n}\n\n// Balancer same with loadbalance.Loadbalancer but without resolver.Result that\n// has been cached\ntype Balancer struct {\n\tb            *BalancerFactory\n\ttarget       string       // a description returned from the resolver's Target method\n\tres          atomic.Value // newest and previous discovery result\n\texpire       int32        // 0 = normal, 1 = expire and collect next ticker\n\tsharedTicker *utils.SharedTicker\n}\n\nfunc (bl *Balancer) Refresh() {\n\tres, err := bl.b.resolver.Resolve(context.Background(), bl.target)\n\tif err != nil {\n\t\tklog.Warnf(\"KITEX: resolver refresh failed, key=%s error=%s\", bl.target, err.Error())\n\t\treturn\n\t}\n\trenameResultCacheKey(&res, bl.b.resolver.Name())\n\tprev := bl.res.Load().(discovery.Result)\n\tif bl.b.rebalancer != nil {\n\t\tif ch, ok := bl.b.resolver.Diff(res.CacheKey, prev, res); ok {\n\t\t\tbl.b.rebalancer.Rebalance(ch)\n\t\t}\n\t}\n\t// replace previous result\n\tbl.res.Store(res)\n}\n\n// Tick implements the interface utils.TickerTask.\nfunc (bl *Balancer) Tick() {\n\tbl.Refresh()\n}\n\n// GetResult returns the discovery result that the Balancer holds.\nfunc (bl *Balancer) GetResult() (res discovery.Result, ok bool) {\n\tif v := bl.res.Load(); v != nil {\n\t\treturn v.(discovery.Result), true\n\t}\n\treturn\n}\n\n// GetPicker equal to loadbalance.Balancer without pass discovery.Result, because we cache the result\nfunc (bl *Balancer) GetPicker() loadbalance.Picker {\n\tatomic.StoreInt32(&bl.expire, 0)\n\tres := bl.res.Load().(discovery.Result)\n\treturn bl.b.balancer.GetPicker(res)\n}\n\nfunc (bl *Balancer) close() {\n\t// notice the under rebalancer\n\tif rb, ok := bl.b.balancer.(loadbalance.Rebalancer); ok {\n\t\t// notice to rebalancing\n\t\trb.Delete(discovery.Change{\n\t\t\tResult: discovery.Result{\n\t\t\t\tCacheable: true,\n\t\t\t\tCacheKey:  bl.res.Load().(discovery.Result).CacheKey,\n\t\t\t},\n\t\t})\n\t}\n\t// delete from sharedTicker\n\tbl.sharedTicker.Delete(bl)\n}\n\nconst unknown = \"unknown\"\n\nfunc Dump() interface{} {\n\ttype instInfo struct {\n\t\tAddress string\n\t\tWeight  int\n\t}\n\tcacheDump := make(map[string]interface{})\n\tbalancerFactories.Range(func(key, val interface{}) bool {\n\t\tcacheKey := key.(string)\n\t\tif bf, ok := val.(*BalancerFactory); ok {\n\t\t\trouteMap := make(map[string]interface{})\n\t\t\tcacheDump[cacheKey] = routeMap\n\t\t\tbf.cache.Range(func(k, v interface{}) bool {\n\t\t\t\trouteKey := k.(string)\n\t\t\t\tif bl, ok := v.(*Balancer); ok {\n\t\t\t\t\tif dr, ok := bl.res.Load().(discovery.Result); ok {\n\t\t\t\t\t\tinsts := make([]instInfo, 0, len(dr.Instances))\n\t\t\t\t\t\tfor i := range dr.Instances {\n\t\t\t\t\t\t\tinst := dr.Instances[i]\n\t\t\t\t\t\t\taddr := fmt.Sprintf(\"%s://%s\", inst.Address().Network(), inst.Address().String())\n\t\t\t\t\t\t\tinsts = append(insts, instInfo{Address: addr, Weight: inst.Weight()})\n\t\t\t\t\t\t}\n\t\t\t\t\t\trouteMap[routeKey] = insts\n\t\t\t\t\t} else {\n\t\t\t\t\t\trouteMap[routeKey] = unknown\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\trouteMap[routeKey] = unknown\n\t\t\t\t}\n\t\t\t\treturn true\n\t\t\t})\n\t\t} else {\n\t\t\tcacheDump[cacheKey] = unknown\n\t\t}\n\t\treturn true\n\t})\n\treturn cacheDump\n}\n\n```\n# Function Signature: func NewBalancerFactory(resolver discovery.Resolver, balancer loadbalance.Loadbalancer, opts Options) *BalancerFactory {\n# Natural Language Annotations: // NewBalancerFactory get or create a balancer factory for balancer instance\n// cache key with resolver name, balancer name and options\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewBalancerFactory(resolver discovery.Resolver, balancer loadbalance.Loadbalancer, opts Options) *BalancerFactory {\n\topts.check()\n\tkey := cacheKey(resolver.Name(), balancer.Name(), opts)\n\tv, _ := balancerFactories.LoadOrStore(key, &BalancerFactory{\n\t\topts:     opts,\n\t\tresolver: resolver,\n\t\tbalancer: balancer,\n\t})\n\treturn v.(*BalancerFactory)\n}\n```", "  ```\nfunc NewBalancerFactory(resolver discovery.Resolver, balancer loadbalance.Loadbalancer, opts Options) *BalancerFactory {\n\topts.check()\n\treturn &BalancerFactory{\n\t\topts:     opts,\n\t\tresolver: resolver,\n\t\tbalancer: balancer,\n\t\tsfg:      &singleflight.Group{},\n\t}\n}\n```", "  ```\nfunc NewBalancerFactory(resolver discovery.Resolver, balancer loadbalance.Loadbalancer, opts Options) *BalancerFactory {\n    opts.check()\n    key := cacheKey(resolver.Name(), balancer.Name(), opts)\n    if f, ok := balancerFactories.Load(key); ok {\n        return f.(*BalancerFactory)\n    }\n    b := &BalancerFactory{\n        cache:    sync.Map{},\n        resolver: resolver,\n        balancer: balancer,\n        opts:     opts,\n    }\n    if rb, ok := balancer.(loadbalance.Rebalancer); ok {\n        b.rebalancer = rb\n        b.Hookable = rb\n    } else {\n        b.Hookable = noopHookRebalancer{}\n    }\n    go b.watcher()\n    balancerFactories.Store(key, b)\n    return b\n}\n```"]}, "kitex-pkg/stats/event.go-DefineNewEvent": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/stats/event.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage stats\n\nimport (\n\t\"errors\"\n\t\"sync\"\n\t\"sync/atomic\"\n)\n\n// EventIndex indicates a unique event.\ntype EventIndex int\n\n// Level sets the record level.\ntype Level int\n\n// Event levels.\nconst (\n\tLevelDisabled Level = iota\n\tLevelBase\n\tLevelDetailed\n)\n\n// Event is used to indicate a specific event.\ntype Event interface {\n\tIndex() EventIndex\n\tLevel() Level\n}\n\ntype event struct {\n\tidx   EventIndex\n\tlevel Level\n}\n\n// Index implements the Event interface.\nfunc (e event) Index() EventIndex {\n\treturn e.idx\n}\n\n// Level implements the Event interface.\nfunc (e event) Level() Level {\n\treturn e.level\n}\n\nconst (\n\t_ EventIndex = iota\n\tserverHandleStart\n\tserverHandleFinish\n\tclientConnStart\n\tclientConnFinish\n\trpcStart\n\trpcFinish\n\treadStart\n\treadFinish\n\twaitReadStart\n\twaitReadFinish\n\twriteStart\n\twriteFinish\n\tstreamRecv\n\tstreamSend\n\n\t// NOTE: add new events before this line\n\tpredefinedEventNum\n)\n\n// Predefined events.\nvar (\n\tRPCStart  = newEvent(rpcStart, LevelBase)\n\tRPCFinish = newEvent(rpcFinish, LevelBase)\n\n\tServerHandleStart  = newEvent(serverHandleStart, LevelDetailed)\n\tServerHandleFinish = newEvent(serverHandleFinish, LevelDetailed)\n\tClientConnStart    = newEvent(clientConnStart, LevelDetailed)\n\tClientConnFinish   = newEvent(clientConnFinish, LevelDetailed)\n\tReadStart          = newEvent(readStart, LevelDetailed)\n\tReadFinish         = newEvent(readFinish, LevelDetailed)\n\tWaitReadStart      = newEvent(waitReadStart, LevelDetailed)\n\tWaitReadFinish     = newEvent(waitReadFinish, LevelDetailed)\n\tWriteStart         = newEvent(writeStart, LevelDetailed)\n\tWriteFinish        = newEvent(writeFinish, LevelDetailed)\n\n\t// Streaming Events\n\tStreamRecv = newEvent(streamRecv, LevelDetailed)\n\tStreamSend = newEvent(streamSend, LevelDetailed)\n)\n\n// errors\nvar (\n\tErrNotAllowed = errors.New(\"event definition is not allowed after initialization\")\n\tErrDuplicated = errors.New(\"event name is already defined\")\n)\n\nvar (\n\tlock        sync.RWMutex\n\tinited      int32\n\tuserDefined = make(map[string]Event)\n\tmaxEventNum = int(predefinedEventNum)\n)\n\n// FinishInitialization freezes all events defined and prevents further definitions to be added.\nfunc FinishInitialization() {\n\tlock.Lock()\n\tdefer lock.Unlock()\n\tatomic.StoreInt32(&inited, 1)\n}\n\n// DefineNewEvent allows user to add event definitions during program initialization.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// MaxEventNum returns the number of event defined.\nfunc MaxEventNum() int {\n\tlock.RLock()\n\tdefer lock.RUnlock()\n\treturn maxEventNum\n}\n\n// PredefinedEventNum returns the number of predefined events of kitex.\nfunc PredefinedEventNum() int {\n\treturn int(predefinedEventNum)\n}\n\nfunc newEvent(idx EventIndex, level Level) Event {\n\treturn event{\n\t\tidx:   idx,\n\t\tlevel: level,\n\t}\n}\n\n```\n# Function Signature: func DefineNewEvent(name string, level Level) (Event, error) {\n# Natural Language Annotations: // DefineNewEvent allows user to add event definitions during program initialization.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\npackage stats\n\nfunc DefineNewEvent(name string, level Level) (Event, error) {\n\tlock.Lock()\n\tdefer lock.Unlock()\n\n\tif atomic.LoadInt32(&inited) == 1 {\n\t\treturn nil, ErrNotAllowed\n\t}\n\n\tif _, ok := userDefined[name]; ok {\n\t\treturn nil, ErrDuplicated\n\t}\n\n\tevent := newEvent(EventIndex(maxEventNum), level)\n\tuserDefined[name] = event\n\tmaxEventNum++\n\treturn event, nil\n}\n```", "  ```\nfunc DefineNewEvent(name string, level Level) (Event, error) {\n\tlock.Lock()\n\tdefer lock.Unlock()\n\n\tif atomic.LoadInt32(&inited) == 1 {\n\t\treturn nil, ErrNotAllowed\n\t}\n\n\tif _, ok := userDefined[name]; ok {\n\t\treturn nil, ErrDuplicated\n\t}\n\n\tidx := EventIndex(maxEventNum)\n\tnewEvent := event{\n\t\tidx:   idx,\n\t\tlevel: level,\n\t}\n\tuserDefined[name] = newEvent\n\tmaxEventNum++\n\n\treturn newEvent, nil\n}\n\nfunc init() {\n\tFinishInitialization()\n}\n```", "  ```\nfunc DefineNewEvent(name string, level Level) (Event, error) {\n\tlock.Lock()\n\tdefer lock.Unlock()\n\n\tif atomic.LoadInt32(&inited) != 0 {\n\t\treturn nil, ErrNotAllowed\n\t}\n\n\tidx, ok := userDefined[name]\n\tif ok {\n\t\treturn nil, ErrDuplicated\n\t}\n\n\t// Create a new event and store it in the map\n\tidx = EventIndex(maxEventNum + 1)\n\tuserDefined[name] = event{\n\t\tidx:   idx,\n\t\tlevel: level,\n\t}\n\tmaxEventNum++\n\n\treturn userDefined[name], nil\n}\n```"]}, "kitex-pkg/generic/thriftidl_provider.go-NewThriftFileProvider": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/thriftidl_provider.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage generic\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"path/filepath\"\n\t\"sync\"\n\n\t\"github.com/cloudwego/dynamicgo/meta\"\n\tdthrift \"github.com/cloudwego/dynamicgo/thrift\"\n\t\"github.com/cloudwego/thriftgo/parser\"\n\n\t\"github.com/cloudwego/kitex/pkg/generic/descriptor\"\n\t\"github.com/cloudwego/kitex/pkg/generic/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n)\n\nvar (\n\t_ Closer = &ThriftContentProvider{}\n\t_ Closer = &ThriftContentWithAbsIncludePathProvider{}\n)\n\ntype thriftFileProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\n// NewThriftFileProvider create a ThriftIDLProvider by given path and include dirs\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewThriftFileProviderWithDynamicGo create a ThriftIDLProvider with dynamicgo by given path and include dirs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc newServiceDescriptorFromPath(path string, includeDirs ...string) (*descriptor.ServiceDescriptor, error) {\n\ttree, err := parser.ParseFile(path, includeDirs, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn svc, nil\n}\n\n// maybe watch the file change and reparse\n// p.thrifts <- some change\n// func (p *thriftFileProvider) watchAndUpdate() {}\n\nfunc (p *thriftFileProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *thriftFileProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\nfunc (p *thriftFileProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\n// ThriftContentProvider provide descriptor from contents\ntype ThriftContentProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\nvar _ DescriptorProvider = (*ThriftContentProvider)(nil)\n\nconst defaultMainIDLPath = \"main.thrift\"\n\n// NewThriftContentProvider builder\nfunc NewThriftContentProvider(main string, includes map[string]string) (*ThriftContentProvider, error) {\n\tp := &ThriftContentProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: false},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\n// NewThriftContentProviderWithDynamicGo builder\nfunc NewThriftContentProviderWithDynamicGo(main string, includes map[string]string) (*ThriftContentProvider, error) {\n\tp := &ThriftContentProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: true},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.newDynamicGoDsc(svc, defaultMainIDLPath, main, includes)\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\n// UpdateIDL ...\nfunc (p *ThriftContentProvider) UpdateIDL(main string, includes map[string]string) error {\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, defaultMainIDLPath, main, includes)\n\t}\n\n\tselect {\n\tcase <-p.svcs:\n\tdefault:\n\t}\n\tselect {\n\tcase p.svcs <- svc:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// Provide ...\nfunc (p *ThriftContentProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *ThriftContentProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\n// Option ...\nfunc (p *ThriftContentProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\nfunc (p *ThriftContentProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string) {\n\tif err := newDynamicGoDscFromContent(svc, path, content, includes, false); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc parseIncludes(tree *parser.Thrift, parsed map[string]*parser.Thrift, sources map[string]string, isAbsIncludePath bool) (err error) {\n\tfor _, i := range tree.Includes {\n\t\tp := i.Path\n\t\tif isAbsIncludePath {\n\t\t\tp = absPath(tree.Filename, i.Path)\n\t\t}\n\t\tref, ok := parsed[p] // avoid infinite recursion\n\t\tif ok {\n\t\t\ti.Reference = ref\n\t\t\tcontinue\n\t\t}\n\t\tif src, ok := sources[p]; !ok {\n\t\t\treturn fmt.Errorf(\"miss include path: %s for file: %s\", p, tree.Filename)\n\t\t} else {\n\t\t\tif ref, err = parser.ParseString(p, src); err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tparsed[p] = ref\n\t\ti.Reference = ref\n\t\tif err = parseIncludes(ref, parsed, sources, isAbsIncludePath); err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\treturn nil\n}\n\n// path := /a/b/c.thrift\n// includePath := ../d.thrift\n// result := /a/d.thrift\nfunc absPath(path, includePath string) string {\n\tif filepath.IsAbs(includePath) {\n\t\treturn includePath\n\t}\n\treturn filepath.Join(filepath.Dir(path), includePath)\n}\n\n// ParseContent parses the IDL from path and content using provided includes\nfunc ParseContent(path, content string, includes map[string]string, isAbsIncludePath bool) (*parser.Thrift, error) {\n\tif src := includes[path]; src != \"\" && src != content {\n\t\treturn nil, fmt.Errorf(\"provided main IDL content conflicts with includes: %q\", path)\n\t}\n\ttree, err := parser.ParseString(path, content)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tparsed := make(map[string]*parser.Thrift)\n\tparsed[path] = tree\n\tif err := parseIncludes(tree, parsed, includes, isAbsIncludePath); err != nil {\n\t\treturn nil, err\n\t}\n\tif cir := parser.CircleDetect(tree); cir != \"\" {\n\t\treturn tree, fmt.Errorf(\"IDL circular dependency: %s\", cir)\n\t}\n\treturn tree, nil\n}\n\n// ThriftContentWithAbsIncludePathProvider ...\ntype ThriftContentWithAbsIncludePathProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\nvar _ DescriptorProvider = (*ThriftContentWithAbsIncludePathProvider)(nil)\n\n// NewThriftContentWithAbsIncludePathProvider create abs include path DescriptorProvider\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewThriftContentWithAbsIncludePathProviderWithDynamicGo create abs include path DescriptorProvider with dynamicgo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UpdateIDL update idl by given args\nfunc (p *ThriftContentWithAbsIncludePathProvider) UpdateIDL(mainIDLPath string, includes map[string]string) error {\n\tmainIDLContent, ok := includes[mainIDLPath]\n\tif !ok {\n\t\treturn fmt.Errorf(\"miss main IDL content for main IDL path: %s\", mainIDLPath)\n\t}\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(mainIDLPath, mainIDLContent, includes, true)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, mainIDLPath, mainIDLContent, includes)\n\t}\n\n\t// drain the channel\n\tselect {\n\tcase <-p.svcs:\n\tdefault:\n\t}\n\tselect {\n\tcase p.svcs <- svc:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// Provide ...\nfunc (p *ThriftContentWithAbsIncludePathProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *ThriftContentWithAbsIncludePathProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\n// Option ...\nfunc (p *ThriftContentWithAbsIncludePathProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\nfunc (p *ThriftContentWithAbsIncludePathProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string) {\n\tif err := newDynamicGoDscFromContent(svc, path, content, includes, true); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc newServiceDescriptorFromContent(path, content string, includes map[string]string, isAbsIncludePath bool) (*descriptor.ServiceDescriptor, error) {\n\ttree, err := ParseContent(path, content, includes, isAbsIncludePath)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn svc, nil\n}\n\nfunc newDynamicGoDscFromContent(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string, isAbsIncludePath bool) error {\n\t// ServiceDescriptor of dynamicgo\n\tdOpts := dthrift.Options{EnableThriftBase: true}\n\tdsvc, err := dOpts.NewDescritorFromContent(context.Background(), path, content, includes, isAbsIncludePath)\n\tif err != nil {\n\t\tklog.CtxWarnf(context.Background(), \"KITEX: failed to get dynamicgo service descriptor, fall back to the original way, error=%s\", err)\n\t\treturn err\n\t}\n\tsvc.DynamicGoDsc = dsvc\n\treturn nil\n}\n\n```\n# Function Signature: func NewThriftFileProvider(path string, includeDirs ...string) (DescriptorProvider, error) {\n# Natural Language Annotations: // NewThriftFileProvider create a ThriftIDLProvider by given path and include dirs\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewThriftFileProvider(path string, includeDirs ...string) (DescriptorProvider, error) {\n\tp := &thriftFileProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: false},\n\t}\n\tsvc, err := newServiceDescriptorFromPath(path, includeDirs...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n```", "  ```\nfunc NewThriftFileProvider(path string, includeDirs ...string) (DescriptorProvider, error) {\n\treturn &thriftFileProvider{}, nil\n}\n```", "  ```\nfunc NewThriftFileProvider(path string, includeDirs ...string) (DescriptorProvider, error) {\n\treturn newThriftIDLProvider(path, includeDirs...)\n}\n\nfunc newThriftIDLProvider(path string, includeDirs ...string) (*thriftFileProvider, error) {\n\tp := &thriftFileProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: false},\n\t}\n\ttree, err := parser.ParseFile(path, includeDirs, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp.svcs <- svc\n\treturn p, nil\n}\n```"]}, "kitex-pkg/generic/thriftidl_provider.go-NewThriftFileProviderWithDynamicGo": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/thriftidl_provider.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage generic\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"path/filepath\"\n\t\"sync\"\n\n\t\"github.com/cloudwego/dynamicgo/meta\"\n\tdthrift \"github.com/cloudwego/dynamicgo/thrift\"\n\t\"github.com/cloudwego/thriftgo/parser\"\n\n\t\"github.com/cloudwego/kitex/pkg/generic/descriptor\"\n\t\"github.com/cloudwego/kitex/pkg/generic/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n)\n\nvar (\n\t_ Closer = &ThriftContentProvider{}\n\t_ Closer = &ThriftContentWithAbsIncludePathProvider{}\n)\n\ntype thriftFileProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\n// NewThriftFileProvider create a ThriftIDLProvider by given path and include dirs\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewThriftFileProviderWithDynamicGo create a ThriftIDLProvider with dynamicgo by given path and include dirs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc newServiceDescriptorFromPath(path string, includeDirs ...string) (*descriptor.ServiceDescriptor, error) {\n\ttree, err := parser.ParseFile(path, includeDirs, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn svc, nil\n}\n\n// maybe watch the file change and reparse\n// p.thrifts <- some change\n// func (p *thriftFileProvider) watchAndUpdate() {}\n\nfunc (p *thriftFileProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *thriftFileProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\nfunc (p *thriftFileProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\n// ThriftContentProvider provide descriptor from contents\ntype ThriftContentProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\nvar _ DescriptorProvider = (*ThriftContentProvider)(nil)\n\nconst defaultMainIDLPath = \"main.thrift\"\n\n// NewThriftContentProvider builder\nfunc NewThriftContentProvider(main string, includes map[string]string) (*ThriftContentProvider, error) {\n\tp := &ThriftContentProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: false},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\n// NewThriftContentProviderWithDynamicGo builder\nfunc NewThriftContentProviderWithDynamicGo(main string, includes map[string]string) (*ThriftContentProvider, error) {\n\tp := &ThriftContentProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: true},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.newDynamicGoDsc(svc, defaultMainIDLPath, main, includes)\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\n// UpdateIDL ...\nfunc (p *ThriftContentProvider) UpdateIDL(main string, includes map[string]string) error {\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, defaultMainIDLPath, main, includes)\n\t}\n\n\tselect {\n\tcase <-p.svcs:\n\tdefault:\n\t}\n\tselect {\n\tcase p.svcs <- svc:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// Provide ...\nfunc (p *ThriftContentProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *ThriftContentProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\n// Option ...\nfunc (p *ThriftContentProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\nfunc (p *ThriftContentProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string) {\n\tif err := newDynamicGoDscFromContent(svc, path, content, includes, false); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc parseIncludes(tree *parser.Thrift, parsed map[string]*parser.Thrift, sources map[string]string, isAbsIncludePath bool) (err error) {\n\tfor _, i := range tree.Includes {\n\t\tp := i.Path\n\t\tif isAbsIncludePath {\n\t\t\tp = absPath(tree.Filename, i.Path)\n\t\t}\n\t\tref, ok := parsed[p] // avoid infinite recursion\n\t\tif ok {\n\t\t\ti.Reference = ref\n\t\t\tcontinue\n\t\t}\n\t\tif src, ok := sources[p]; !ok {\n\t\t\treturn fmt.Errorf(\"miss include path: %s for file: %s\", p, tree.Filename)\n\t\t} else {\n\t\t\tif ref, err = parser.ParseString(p, src); err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tparsed[p] = ref\n\t\ti.Reference = ref\n\t\tif err = parseIncludes(ref, parsed, sources, isAbsIncludePath); err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\treturn nil\n}\n\n// path := /a/b/c.thrift\n// includePath := ../d.thrift\n// result := /a/d.thrift\nfunc absPath(path, includePath string) string {\n\tif filepath.IsAbs(includePath) {\n\t\treturn includePath\n\t}\n\treturn filepath.Join(filepath.Dir(path), includePath)\n}\n\n// ParseContent parses the IDL from path and content using provided includes\nfunc ParseContent(path, content string, includes map[string]string, isAbsIncludePath bool) (*parser.Thrift, error) {\n\tif src := includes[path]; src != \"\" && src != content {\n\t\treturn nil, fmt.Errorf(\"provided main IDL content conflicts with includes: %q\", path)\n\t}\n\ttree, err := parser.ParseString(path, content)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tparsed := make(map[string]*parser.Thrift)\n\tparsed[path] = tree\n\tif err := parseIncludes(tree, parsed, includes, isAbsIncludePath); err != nil {\n\t\treturn nil, err\n\t}\n\tif cir := parser.CircleDetect(tree); cir != \"\" {\n\t\treturn tree, fmt.Errorf(\"IDL circular dependency: %s\", cir)\n\t}\n\treturn tree, nil\n}\n\n// ThriftContentWithAbsIncludePathProvider ...\ntype ThriftContentWithAbsIncludePathProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\nvar _ DescriptorProvider = (*ThriftContentWithAbsIncludePathProvider)(nil)\n\n// NewThriftContentWithAbsIncludePathProvider create abs include path DescriptorProvider\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewThriftContentWithAbsIncludePathProviderWithDynamicGo create abs include path DescriptorProvider with dynamicgo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UpdateIDL update idl by given args\nfunc (p *ThriftContentWithAbsIncludePathProvider) UpdateIDL(mainIDLPath string, includes map[string]string) error {\n\tmainIDLContent, ok := includes[mainIDLPath]\n\tif !ok {\n\t\treturn fmt.Errorf(\"miss main IDL content for main IDL path: %s\", mainIDLPath)\n\t}\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(mainIDLPath, mainIDLContent, includes, true)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, mainIDLPath, mainIDLContent, includes)\n\t}\n\n\t// drain the channel\n\tselect {\n\tcase <-p.svcs:\n\tdefault:\n\t}\n\tselect {\n\tcase p.svcs <- svc:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// Provide ...\nfunc (p *ThriftContentWithAbsIncludePathProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *ThriftContentWithAbsIncludePathProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\n// Option ...\nfunc (p *ThriftContentWithAbsIncludePathProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\nfunc (p *ThriftContentWithAbsIncludePathProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string) {\n\tif err := newDynamicGoDscFromContent(svc, path, content, includes, true); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc newServiceDescriptorFromContent(path, content string, includes map[string]string, isAbsIncludePath bool) (*descriptor.ServiceDescriptor, error) {\n\ttree, err := ParseContent(path, content, includes, isAbsIncludePath)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn svc, nil\n}\n\nfunc newDynamicGoDscFromContent(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string, isAbsIncludePath bool) error {\n\t// ServiceDescriptor of dynamicgo\n\tdOpts := dthrift.Options{EnableThriftBase: true}\n\tdsvc, err := dOpts.NewDescritorFromContent(context.Background(), path, content, includes, isAbsIncludePath)\n\tif err != nil {\n\t\tklog.CtxWarnf(context.Background(), \"KITEX: failed to get dynamicgo service descriptor, fall back to the original way, error=%s\", err)\n\t\treturn err\n\t}\n\tsvc.DynamicGoDsc = dsvc\n\treturn nil\n}\n\n```\n# Function Signature: func NewThriftFileProviderWithDynamicGo(path string, includeDirs ...string) (DescriptorProvider, error) {\n# Natural Language Annotations: // NewThriftFileProviderWithDynamicGo create a ThriftIDLProvider with dynamicgo by given path and include dirs\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewThriftFileProviderWithDynamicGo(path string, includeDirs ...string) (DescriptorProvider, error) {\n\tp, err := newThriftProvider(path, includeDirs, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn p, nil\n}\n```", "  ```\nfunc NewThriftFileProviderWithDynamicGo(path string, includeDirs ...string) (DescriptorProvider, error) {\n\tp := &ThriftFileProvider{opts: &ProviderOption{DynamicGoEnabled: true}}\n\t_, err := p.newServiceDescriptorFromPath(path, includeDirs...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn p, nil\n}\n```", "  ```\nfunc NewThriftFileProviderWithDynamicGo(path string, includeDirs ...string) (DescriptorProvider, error) {\n\tmainPath := filepath.FromSlash(path)\n\tmainContent, err := os.ReadFile(mainPath)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tmainDir := filepath.Dir(mainPath)\n\tincludeDirsMap := make(map[string]string)\n\tfor _, includeDir := range includeDirs {\n\t\tincludesRealPaths, err := builtin.ResolvePath([]string{includeDir}, \".\", mainDir)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfor _, includeRealPath := range includesRealPaths {\n\t\t\t_, fileName := filepath.Split(includeRealPath)\n\t\t\tcontent, err := os.ReadFile(includeRealPath)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tincludeDirsMap[fileName] = string(content)\n\t\t}\n\t}\n\tmainSvc, err := newServiceDescriptorFromContent(mainPath, string(mainContent), includeDirsMap, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdOpts := dthrift.Options{EnableThriftBase: true}\n\tdsvc, err := dOpts.NewDescritorFromContent(context.Background(), mainPath, string(mainContent), includeDirsMap, false)\n\tif err != nil {\n\t\tklog.CtxWarnf(context.Background(), \"KITEX: failed to get dynamicgo service descriptor, fall back to the original way, error=%s\", err)\n\t\treturn nil, err\n\t}\n\tmainSvc.DynamicGoDsc = dsvc\n\tsvcs := make(chan *descriptor.ServiceDescriptor, 1) // unblock with buffered channel\n\t// new the dynamicgo thrift descriptor\n\tsvcs <- mainSvc\n\topts := &ProviderOption{DynamicGoEnabled: true}\n\tprovider := &ThriftFileProvider{\n\t\tsvcs: svcs,\n\t\topts: opts,\n\t}\n\treturn provider, nil\n}\n```"]}, "kitex-pkg/generic/thriftidl_provider.go-NewThriftContentWithAbsIncludePathProvider": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/thriftidl_provider.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage generic\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"path/filepath\"\n\t\"sync\"\n\n\t\"github.com/cloudwego/dynamicgo/meta\"\n\tdthrift \"github.com/cloudwego/dynamicgo/thrift\"\n\t\"github.com/cloudwego/thriftgo/parser\"\n\n\t\"github.com/cloudwego/kitex/pkg/generic/descriptor\"\n\t\"github.com/cloudwego/kitex/pkg/generic/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n)\n\nvar (\n\t_ Closer = &ThriftContentProvider{}\n\t_ Closer = &ThriftContentWithAbsIncludePathProvider{}\n)\n\ntype thriftFileProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\n// NewThriftFileProvider create a ThriftIDLProvider by given path and include dirs\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewThriftFileProviderWithDynamicGo create a ThriftIDLProvider with dynamicgo by given path and include dirs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc newServiceDescriptorFromPath(path string, includeDirs ...string) (*descriptor.ServiceDescriptor, error) {\n\ttree, err := parser.ParseFile(path, includeDirs, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn svc, nil\n}\n\n// maybe watch the file change and reparse\n// p.thrifts <- some change\n// func (p *thriftFileProvider) watchAndUpdate() {}\n\nfunc (p *thriftFileProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *thriftFileProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\nfunc (p *thriftFileProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\n// ThriftContentProvider provide descriptor from contents\ntype ThriftContentProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\nvar _ DescriptorProvider = (*ThriftContentProvider)(nil)\n\nconst defaultMainIDLPath = \"main.thrift\"\n\n// NewThriftContentProvider builder\nfunc NewThriftContentProvider(main string, includes map[string]string) (*ThriftContentProvider, error) {\n\tp := &ThriftContentProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: false},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\n// NewThriftContentProviderWithDynamicGo builder\nfunc NewThriftContentProviderWithDynamicGo(main string, includes map[string]string) (*ThriftContentProvider, error) {\n\tp := &ThriftContentProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: true},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.newDynamicGoDsc(svc, defaultMainIDLPath, main, includes)\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\n// UpdateIDL ...\nfunc (p *ThriftContentProvider) UpdateIDL(main string, includes map[string]string) error {\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, defaultMainIDLPath, main, includes)\n\t}\n\n\tselect {\n\tcase <-p.svcs:\n\tdefault:\n\t}\n\tselect {\n\tcase p.svcs <- svc:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// Provide ...\nfunc (p *ThriftContentProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *ThriftContentProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\n// Option ...\nfunc (p *ThriftContentProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\nfunc (p *ThriftContentProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string) {\n\tif err := newDynamicGoDscFromContent(svc, path, content, includes, false); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc parseIncludes(tree *parser.Thrift, parsed map[string]*parser.Thrift, sources map[string]string, isAbsIncludePath bool) (err error) {\n\tfor _, i := range tree.Includes {\n\t\tp := i.Path\n\t\tif isAbsIncludePath {\n\t\t\tp = absPath(tree.Filename, i.Path)\n\t\t}\n\t\tref, ok := parsed[p] // avoid infinite recursion\n\t\tif ok {\n\t\t\ti.Reference = ref\n\t\t\tcontinue\n\t\t}\n\t\tif src, ok := sources[p]; !ok {\n\t\t\treturn fmt.Errorf(\"miss include path: %s for file: %s\", p, tree.Filename)\n\t\t} else {\n\t\t\tif ref, err = parser.ParseString(p, src); err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tparsed[p] = ref\n\t\ti.Reference = ref\n\t\tif err = parseIncludes(ref, parsed, sources, isAbsIncludePath); err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\treturn nil\n}\n\n// path := /a/b/c.thrift\n// includePath := ../d.thrift\n// result := /a/d.thrift\nfunc absPath(path, includePath string) string {\n\tif filepath.IsAbs(includePath) {\n\t\treturn includePath\n\t}\n\treturn filepath.Join(filepath.Dir(path), includePath)\n}\n\n// ParseContent parses the IDL from path and content using provided includes\nfunc ParseContent(path, content string, includes map[string]string, isAbsIncludePath bool) (*parser.Thrift, error) {\n\tif src := includes[path]; src != \"\" && src != content {\n\t\treturn nil, fmt.Errorf(\"provided main IDL content conflicts with includes: %q\", path)\n\t}\n\ttree, err := parser.ParseString(path, content)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tparsed := make(map[string]*parser.Thrift)\n\tparsed[path] = tree\n\tif err := parseIncludes(tree, parsed, includes, isAbsIncludePath); err != nil {\n\t\treturn nil, err\n\t}\n\tif cir := parser.CircleDetect(tree); cir != \"\" {\n\t\treturn tree, fmt.Errorf(\"IDL circular dependency: %s\", cir)\n\t}\n\treturn tree, nil\n}\n\n// ThriftContentWithAbsIncludePathProvider ...\ntype ThriftContentWithAbsIncludePathProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\nvar _ DescriptorProvider = (*ThriftContentWithAbsIncludePathProvider)(nil)\n\n// NewThriftContentWithAbsIncludePathProvider create abs include path DescriptorProvider\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewThriftContentWithAbsIncludePathProviderWithDynamicGo create abs include path DescriptorProvider with dynamicgo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UpdateIDL update idl by given args\nfunc (p *ThriftContentWithAbsIncludePathProvider) UpdateIDL(mainIDLPath string, includes map[string]string) error {\n\tmainIDLContent, ok := includes[mainIDLPath]\n\tif !ok {\n\t\treturn fmt.Errorf(\"miss main IDL content for main IDL path: %s\", mainIDLPath)\n\t}\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(mainIDLPath, mainIDLContent, includes, true)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, mainIDLPath, mainIDLContent, includes)\n\t}\n\n\t// drain the channel\n\tselect {\n\tcase <-p.svcs:\n\tdefault:\n\t}\n\tselect {\n\tcase p.svcs <- svc:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// Provide ...\nfunc (p *ThriftContentWithAbsIncludePathProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *ThriftContentWithAbsIncludePathProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\n// Option ...\nfunc (p *ThriftContentWithAbsIncludePathProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\nfunc (p *ThriftContentWithAbsIncludePathProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string) {\n\tif err := newDynamicGoDscFromContent(svc, path, content, includes, true); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc newServiceDescriptorFromContent(path, content string, includes map[string]string, isAbsIncludePath bool) (*descriptor.ServiceDescriptor, error) {\n\ttree, err := ParseContent(path, content, includes, isAbsIncludePath)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn svc, nil\n}\n\nfunc newDynamicGoDscFromContent(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string, isAbsIncludePath bool) error {\n\t// ServiceDescriptor of dynamicgo\n\tdOpts := dthrift.Options{EnableThriftBase: true}\n\tdsvc, err := dOpts.NewDescritorFromContent(context.Background(), path, content, includes, isAbsIncludePath)\n\tif err != nil {\n\t\tklog.CtxWarnf(context.Background(), \"KITEX: failed to get dynamicgo service descriptor, fall back to the original way, error=%s\", err)\n\t\treturn err\n\t}\n\tsvc.DynamicGoDsc = dsvc\n\treturn nil\n}\n\n```\n# Function Signature: func NewThriftContentWithAbsIncludePathProvider(mainIDLPath string, includes map[string]string) (*ThriftContentWithAbsIncludePathProvider, error) {\n# Natural Language Annotations: // NewThriftContentWithAbsIncludePathProvider create abs include path DescriptorProvider\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewThriftContentWithAbsIncludePathProvider(mainIDLPath string, includes map[string]string) (*ThriftContentWithAbsIncludePathProvider, error) {\n\tp := &ThriftContentWithAbsIncludePathProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: false},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(mainIDLPath, includes[mainIDLPath], includes, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp.svcs <- svc\n\treturn p, nil\n}\n```", "  ```\nfunc NewThriftContentWithAbsIncludePathProvider(mainIDLPath string, includes map[string]string) (*ThriftContentWithAbsIncludePathProvider, error) {\n\tp := &ThriftContentWithAbsIncludePathProvider{\n\t\tsvcs:  make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts:  &ProviderOption{DynamicGoEnabled: false},\n\t}\n\n\tmainIDLContent, ok := includes[mainIDLPath]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"miss main IDL content for main IDL path: %s\", mainIDLPath)\n\t}\n\n\ttree, err := ParseContent(mainIDLPath, mainIDLContent, includes, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, mainIDLPath, mainIDLContent, includes)\n\t}\n\n\tp.svcs <- svc\n\n\treturn p, nil\n}\n```", "  ```\nfunc NewThriftContentWithAbsIncludePathProvider(mainIDLPath string, includes map[string]string) (*ThriftContentWithAbsIncludePathProvider, error) {\n\tp := &ThriftContentWithAbsIncludePathProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: false},\n\t}\n\tif mainIDLPath == \"\" {\n\t\treturn nil, fmt.Errorf(\"missing main IDL path\")\n\t}\n\tmainIDLContent, ok := includes[mainIDLPath]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"miss main IDL content for main IDL path: %s\", mainIDLPath)\n\t}\n\ttree, err := ParseContent(mainIDLPath, mainIDLContent, includes, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp.svcs <- svc\n\n\tif !p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, mainIDLPath, mainIDLContent, includes)\n\t}\n\n\treturn p, nil\n}\n```"]}, "kitex-pkg/generic/thriftidl_provider.go-NewThriftContentWithAbsIncludePathProviderWithDynamicGo": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/thriftidl_provider.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage generic\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"path/filepath\"\n\t\"sync\"\n\n\t\"github.com/cloudwego/dynamicgo/meta\"\n\tdthrift \"github.com/cloudwego/dynamicgo/thrift\"\n\t\"github.com/cloudwego/thriftgo/parser\"\n\n\t\"github.com/cloudwego/kitex/pkg/generic/descriptor\"\n\t\"github.com/cloudwego/kitex/pkg/generic/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n)\n\nvar (\n\t_ Closer = &ThriftContentProvider{}\n\t_ Closer = &ThriftContentWithAbsIncludePathProvider{}\n)\n\ntype thriftFileProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\n// NewThriftFileProvider create a ThriftIDLProvider by given path and include dirs\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewThriftFileProviderWithDynamicGo create a ThriftIDLProvider with dynamicgo by given path and include dirs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc newServiceDescriptorFromPath(path string, includeDirs ...string) (*descriptor.ServiceDescriptor, error) {\n\ttree, err := parser.ParseFile(path, includeDirs, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn svc, nil\n}\n\n// maybe watch the file change and reparse\n// p.thrifts <- some change\n// func (p *thriftFileProvider) watchAndUpdate() {}\n\nfunc (p *thriftFileProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *thriftFileProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\nfunc (p *thriftFileProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\n// ThriftContentProvider provide descriptor from contents\ntype ThriftContentProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\nvar _ DescriptorProvider = (*ThriftContentProvider)(nil)\n\nconst defaultMainIDLPath = \"main.thrift\"\n\n// NewThriftContentProvider builder\nfunc NewThriftContentProvider(main string, includes map[string]string) (*ThriftContentProvider, error) {\n\tp := &ThriftContentProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: false},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\n// NewThriftContentProviderWithDynamicGo builder\nfunc NewThriftContentProviderWithDynamicGo(main string, includes map[string]string) (*ThriftContentProvider, error) {\n\tp := &ThriftContentProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: true},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.newDynamicGoDsc(svc, defaultMainIDLPath, main, includes)\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\n// UpdateIDL ...\nfunc (p *ThriftContentProvider) UpdateIDL(main string, includes map[string]string) error {\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, defaultMainIDLPath, main, includes)\n\t}\n\n\tselect {\n\tcase <-p.svcs:\n\tdefault:\n\t}\n\tselect {\n\tcase p.svcs <- svc:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// Provide ...\nfunc (p *ThriftContentProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *ThriftContentProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\n// Option ...\nfunc (p *ThriftContentProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\nfunc (p *ThriftContentProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string) {\n\tif err := newDynamicGoDscFromContent(svc, path, content, includes, false); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc parseIncludes(tree *parser.Thrift, parsed map[string]*parser.Thrift, sources map[string]string, isAbsIncludePath bool) (err error) {\n\tfor _, i := range tree.Includes {\n\t\tp := i.Path\n\t\tif isAbsIncludePath {\n\t\t\tp = absPath(tree.Filename, i.Path)\n\t\t}\n\t\tref, ok := parsed[p] // avoid infinite recursion\n\t\tif ok {\n\t\t\ti.Reference = ref\n\t\t\tcontinue\n\t\t}\n\t\tif src, ok := sources[p]; !ok {\n\t\t\treturn fmt.Errorf(\"miss include path: %s for file: %s\", p, tree.Filename)\n\t\t} else {\n\t\t\tif ref, err = parser.ParseString(p, src); err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tparsed[p] = ref\n\t\ti.Reference = ref\n\t\tif err = parseIncludes(ref, parsed, sources, isAbsIncludePath); err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\treturn nil\n}\n\n// path := /a/b/c.thrift\n// includePath := ../d.thrift\n// result := /a/d.thrift\nfunc absPath(path, includePath string) string {\n\tif filepath.IsAbs(includePath) {\n\t\treturn includePath\n\t}\n\treturn filepath.Join(filepath.Dir(path), includePath)\n}\n\n// ParseContent parses the IDL from path and content using provided includes\nfunc ParseContent(path, content string, includes map[string]string, isAbsIncludePath bool) (*parser.Thrift, error) {\n\tif src := includes[path]; src != \"\" && src != content {\n\t\treturn nil, fmt.Errorf(\"provided main IDL content conflicts with includes: %q\", path)\n\t}\n\ttree, err := parser.ParseString(path, content)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tparsed := make(map[string]*parser.Thrift)\n\tparsed[path] = tree\n\tif err := parseIncludes(tree, parsed, includes, isAbsIncludePath); err != nil {\n\t\treturn nil, err\n\t}\n\tif cir := parser.CircleDetect(tree); cir != \"\" {\n\t\treturn tree, fmt.Errorf(\"IDL circular dependency: %s\", cir)\n\t}\n\treturn tree, nil\n}\n\n// ThriftContentWithAbsIncludePathProvider ...\ntype ThriftContentWithAbsIncludePathProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\nvar _ DescriptorProvider = (*ThriftContentWithAbsIncludePathProvider)(nil)\n\n// NewThriftContentWithAbsIncludePathProvider create abs include path DescriptorProvider\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewThriftContentWithAbsIncludePathProviderWithDynamicGo create abs include path DescriptorProvider with dynamicgo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UpdateIDL update idl by given args\nfunc (p *ThriftContentWithAbsIncludePathProvider) UpdateIDL(mainIDLPath string, includes map[string]string) error {\n\tmainIDLContent, ok := includes[mainIDLPath]\n\tif !ok {\n\t\treturn fmt.Errorf(\"miss main IDL content for main IDL path: %s\", mainIDLPath)\n\t}\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(mainIDLPath, mainIDLContent, includes, true)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, mainIDLPath, mainIDLContent, includes)\n\t}\n\n\t// drain the channel\n\tselect {\n\tcase <-p.svcs:\n\tdefault:\n\t}\n\tselect {\n\tcase p.svcs <- svc:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// Provide ...\nfunc (p *ThriftContentWithAbsIncludePathProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *ThriftContentWithAbsIncludePathProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\n// Option ...\nfunc (p *ThriftContentWithAbsIncludePathProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\nfunc (p *ThriftContentWithAbsIncludePathProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string) {\n\tif err := newDynamicGoDscFromContent(svc, path, content, includes, true); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc newServiceDescriptorFromContent(path, content string, includes map[string]string, isAbsIncludePath bool) (*descriptor.ServiceDescriptor, error) {\n\ttree, err := ParseContent(path, content, includes, isAbsIncludePath)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn svc, nil\n}\n\nfunc newDynamicGoDscFromContent(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string, isAbsIncludePath bool) error {\n\t// ServiceDescriptor of dynamicgo\n\tdOpts := dthrift.Options{EnableThriftBase: true}\n\tdsvc, err := dOpts.NewDescritorFromContent(context.Background(), path, content, includes, isAbsIncludePath)\n\tif err != nil {\n\t\tklog.CtxWarnf(context.Background(), \"KITEX: failed to get dynamicgo service descriptor, fall back to the original way, error=%s\", err)\n\t\treturn err\n\t}\n\tsvc.DynamicGoDsc = dsvc\n\treturn nil\n}\n\n```\n# Function Signature: func NewThriftContentWithAbsIncludePathProviderWithDynamicGo(mainIDLPath string, includes map[string]string) (*ThriftContentWithAbsIncludePathProvider, error) {\n# Natural Language Annotations: // NewThriftContentWithAbsIncludePathProviderWithDynamicGo create abs include path DescriptorProvider with dynamicgo\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewThriftContentWithAbsIncludePathProviderWithDynamicGo(mainIDLPath string, includes map[string]string) (*ThriftContentWithAbsIncludePathProvider, error) {\n    p := &ThriftContentWithAbsIncludePathProvider{\n        svcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n        opts: &ProviderOption{DynamicGoEnabled: true},\n    }\n    mainIDLContent, ok := includes[mainIDLPath]\n    if !ok {\n        return nil, fmt.Errorf(\"miss main IDL content for main IDL path: %s\", mainIDLPath)\n    }\n    var svc *descriptor.ServiceDescriptor\n    tree, err := ParseContent(mainIDLPath, mainIDLContent, includes, true)\n    if err != nil {\n        return nil, err\n    }\n    svc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n    if err != nil {\n        return nil, err\n    }\n\n    if p.opts.DynamicGoEnabled {\n        p.newDynamicGoDsc(svc, mainIDLPath, mainIDLContent, includes)\n    }\n\n    p.svcs <- svc\n    return p, nil\n}\n```", "  ```\nfunc NewThriftContentWithAbsIncludePathProviderWithDynamicGo(mainIDLPath string, includes map[string]string) (*ThriftContentWithAbsIncludePathProvider, error) {\n\tmainIDLContent, ok := includes[mainIDLPath]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"miss main IDL content for main IDL path: %s\", mainIDLPath)\n\t}\n\tsvc, err := newServiceDescriptorFromContent(mainIDLPath, mainIDLContent, includes, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif err := newDynamicGoDscFromContent(svc, mainIDLPath, mainIDLContent, includes, true); err != nil {\n\t\tklog.CtxWarnf(context.Background(), \"KITEX: failed to get dynamicgo service descriptor, fall back to the original way, error=%s\", err)\n\t}\n\n\tp := &ThriftContentWithAbsIncludePathProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1),\n\t\topts: &ProviderOption{DynamicGoEnabled: true},\n\t}\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n```", "  ```\nfunc NewThriftContentWithAbsIncludePathProviderWithDynamicGo(mainIDLPath string, includes map[string]string) (*ThriftContentWithAbsIncludePathProvider, error) {\n\tp := &ThriftContentWithAbsIncludePathProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: true},\n\t}\n\n\t// Parse the main IDL content\n\tmainIDLContent, ok := includes[mainIDLPath]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"miss main IDL content for main IDL path: %s\", mainIDLPath)\n\t}\n\ttree, err := parser.ParseString(mainIDLPath, mainIDLContent)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Parse the includes\n\tif err := parseIncludes(tree, includes, true); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Create the service descriptor\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Enable the dynamic go descriptor\n\tif err := newDynamicGoDsc(svc, mainIDLPath, mainIDLContent, includes); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Add the service descriptor to the provider\n\tp.svcs <- svc\n\n\treturn p, nil\n}\n```"]}, "kitex-pkg/generic/generic.go-MapThriftGeneric": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/generic.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package generic ...\npackage generic\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/cloudwego/dynamicgo/conv\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n)\n\n// Generic ...\ntype Generic interface {\n\tCloser\n\t// PayloadCodec return codec implement\n\tPayloadCodec() remote.PayloadCodec\n\t// PayloadCodecType return the type of codec\n\tPayloadCodecType() serviceinfo.PayloadCodec\n\t// RawThriftBinaryGeneric must be framed\n\tFramed() bool\n\t// GetMethod to get method name if need\n\tGetMethod(req interface{}, method string) (*Method, error)\n}\n\n// Method information\ntype Method struct {\n\tName   string\n\tOneway bool\n}\n\n// BinaryThriftGeneric raw thrift binary Generic\nfunc BinaryThriftGeneric() Generic {\n\treturn &binaryThriftGeneric{}\n}\n\n// MapThriftGeneric map mapping generic\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.MapThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n//\n// String value is returned for binary field by default. You can change the return value to []byte for binary field with SetBinaryWithByteSlice.\n// eg:\n//\n//\tSetBinaryWithByteSlice(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc MapThriftGenericForJSON(p DescriptorProvider) (Generic, error) {\n\tcodec, err := newMapThriftCodecForJSON(p, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &mapThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// HTTPThriftGeneric http mapping Generic.\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.HTTPThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc HTTPPbThriftGeneric(p DescriptorProvider, pbp PbDescriptorProvider) (Generic, error) {\n\tcodec, err := newHTTPPbThriftCodec(p, pbp, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &httpPbThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// JSONThriftGeneric json mapping generic.\n// Base64 codec for binary field is enabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.JSONThriftGeneric(p)\n//\tSetBinaryWithBase64(g, false)\n\n\n\n\n\n\n\n\n\n\n// JSONPbGeneric json mapping generic.\n// Uses dynamicgo for json to protobufs conversion, so DynamicGo field is true by default.\n\n\n\n\n\n\n\n\n\n\n\n// SetBinaryWithBase64 enable/disable Base64 codec for binary field.\nfunc SetBinaryWithBase64(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *httpThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t}\n\tcase *jsonThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithException.NoBase64Binary = !enable\n\t\t}\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"Base64Binary is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetBinaryWithByteSlice enable/disable returning []byte for binary field.\nfunc SetBinaryWithByteSlice(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithByteSlice = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"returning []byte for binary fields is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetFieldsForEmptyStructMode is a enum for EnableSetFieldsForEmptyStruct()\ntype SetFieldsForEmptyStructMode uint8\n\nconst (\n\t// NotSetFields means disable\n\tNotSetFields SetFieldsForEmptyStructMode = iota\n\t// SetNonOptiontionalFields means only set required and default fields\n\tSetNonOptiontionalFields\n\t// SetAllFields means set all fields\n\tSetAllFields\n)\n\n// EnableSetFieldsForEmptyStruct enable/disable set all fields of a struct even if it is empty.\n// This option is only applicable to map-generic response (reading) now.\n//\n//\tmode == 0 means disable\n//\tmode == 1 means only set required and default fields\n//\tmode == 2 means set all fields\nfunc EnableSetFieldsForEmptyStruct(g Generic, mode SetFieldsForEmptyStructMode) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.setFieldsForEmptyStruct = uint8(mode)\n\tdefault:\n\t\treturn fmt.Errorf(\"SetFieldsForEmptyStruct only supports map-generic at present\")\n\t}\n\treturn nil\n}\n\nvar thriftCodec = thrift.NewThriftCodec()\n\nvar pbCodec = protobuf.NewProtobufCodec()\n\ntype binaryThriftGeneric struct{}\n\nfunc (g *binaryThriftGeneric) Framed() bool {\n\treturn true\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\tpc := &binaryThriftCodec{thriftCodec}\n\treturn pc\n}\n\nfunc (g *binaryThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn &Method{method, false}, nil\n}\n\nfunc (g *binaryThriftGeneric) Close() error {\n\treturn nil\n}\n\ntype mapThriftGeneric struct {\n\tcodec *mapThriftCodec\n}\n\nfunc (g *mapThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *mapThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *mapThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *mapThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *mapThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonThriftGeneric struct {\n\tcodec *jsonThriftCodec\n}\n\nfunc (g *jsonThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonPbGeneric struct {\n\tcodec *jsonPbCodec\n}\n\nfunc (g *jsonPbGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *jsonPbGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Protobuf\n}\n\nfunc (g *jsonPbGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonPbGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonPbGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpThriftGeneric struct {\n\tcodec *httpThriftCodec\n}\n\nfunc (g *httpThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *httpThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpPbThriftGeneric struct {\n\tcodec *httpPbThriftCodec\n}\n\nfunc (g *httpPbThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpPbThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpPbThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\n```\n# Function Signature: func MapThriftGeneric(p DescriptorProvider) (Generic, error) {\n# Natural Language Annotations: // MapThriftGeneric map mapping generic\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.MapThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n//\n// String value is returned for binary field by default. You can change the return value to []byte for binary field with SetBinaryWithByteSlice.\n// eg:\n//\n//\tSetBinaryWithByteSlice(g, true)\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc MapThriftGeneric(p DescriptorProvider) (Generic, error) {\n    codec, err := newMapThriftCodecForJSON(p, thriftCodec)\n    if err != nil {\n        return nil, err\n    }\n    return &mapThriftGeneric{\n        codec: codec,\n    }, nil\n}\n\ntype mapThriftGeneric struct {\n    codec *mapThriftCodec\n}\n\nfunc (g *mapThriftGeneric) Framed() bool {\n    return false\n}\n\nfunc (g *mapThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n    return serviceinfo.Thrift\n}\n\nfunc (g *mapThriftGeneric) PayloadCodec() remote.PayloadCodec {\n    return g.codec\n}\n\nfunc (g *mapThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n    return g.codec.getMethod(req, method)\n}\n\nfunc (g *mapThriftGeneric) Close() error {\n    return g.codec.Close()\n}\n```", "  ```\nfunc MapThriftGeneric(p DescriptorProvider) (Generic, error) {\n\treturn NewMapThriftCodecGeneric(p, false, defaultConvOpts)\n}\n\nfunc NewMapThriftCodecGeneric(p DescriptorProvider, base64Enabled bool, copts conv.Options) (Generic, error) {\n\tcodec, err := newMapThriftCodec(p, copts)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &mapThriftGeneric{\n\t\tcodec: codec,\n\t\tbinaryWithBase64: base64Enabled,\n\t}, nil\n}\n\ntype mapThriftGeneric struct {\n\tcodec *mapThriftCodec\n\tbinaryWithBase64 bool\n}\n\nfunc (g *mapThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *mapThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *mapThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *mapThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *mapThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n```", "  https://gist.github.com/haozhest/b1f073c18381f803cdc8c78ec61dc03f"]}, "kitex-pkg/generic/generic.go-HTTPThriftGeneric": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/generic.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package generic ...\npackage generic\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/cloudwego/dynamicgo/conv\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n)\n\n// Generic ...\ntype Generic interface {\n\tCloser\n\t// PayloadCodec return codec implement\n\tPayloadCodec() remote.PayloadCodec\n\t// PayloadCodecType return the type of codec\n\tPayloadCodecType() serviceinfo.PayloadCodec\n\t// RawThriftBinaryGeneric must be framed\n\tFramed() bool\n\t// GetMethod to get method name if need\n\tGetMethod(req interface{}, method string) (*Method, error)\n}\n\n// Method information\ntype Method struct {\n\tName   string\n\tOneway bool\n}\n\n// BinaryThriftGeneric raw thrift binary Generic\nfunc BinaryThriftGeneric() Generic {\n\treturn &binaryThriftGeneric{}\n}\n\n// MapThriftGeneric map mapping generic\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.MapThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n//\n// String value is returned for binary field by default. You can change the return value to []byte for binary field with SetBinaryWithByteSlice.\n// eg:\n//\n//\tSetBinaryWithByteSlice(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc MapThriftGenericForJSON(p DescriptorProvider) (Generic, error) {\n\tcodec, err := newMapThriftCodecForJSON(p, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &mapThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// HTTPThriftGeneric http mapping Generic.\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.HTTPThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc HTTPPbThriftGeneric(p DescriptorProvider, pbp PbDescriptorProvider) (Generic, error) {\n\tcodec, err := newHTTPPbThriftCodec(p, pbp, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &httpPbThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// JSONThriftGeneric json mapping generic.\n// Base64 codec for binary field is enabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.JSONThriftGeneric(p)\n//\tSetBinaryWithBase64(g, false)\n\n\n\n\n\n\n\n\n\n\n// JSONPbGeneric json mapping generic.\n// Uses dynamicgo for json to protobufs conversion, so DynamicGo field is true by default.\n\n\n\n\n\n\n\n\n\n\n\n// SetBinaryWithBase64 enable/disable Base64 codec for binary field.\nfunc SetBinaryWithBase64(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *httpThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t}\n\tcase *jsonThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithException.NoBase64Binary = !enable\n\t\t}\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"Base64Binary is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetBinaryWithByteSlice enable/disable returning []byte for binary field.\nfunc SetBinaryWithByteSlice(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithByteSlice = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"returning []byte for binary fields is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetFieldsForEmptyStructMode is a enum for EnableSetFieldsForEmptyStruct()\ntype SetFieldsForEmptyStructMode uint8\n\nconst (\n\t// NotSetFields means disable\n\tNotSetFields SetFieldsForEmptyStructMode = iota\n\t// SetNonOptiontionalFields means only set required and default fields\n\tSetNonOptiontionalFields\n\t// SetAllFields means set all fields\n\tSetAllFields\n)\n\n// EnableSetFieldsForEmptyStruct enable/disable set all fields of a struct even if it is empty.\n// This option is only applicable to map-generic response (reading) now.\n//\n//\tmode == 0 means disable\n//\tmode == 1 means only set required and default fields\n//\tmode == 2 means set all fields\nfunc EnableSetFieldsForEmptyStruct(g Generic, mode SetFieldsForEmptyStructMode) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.setFieldsForEmptyStruct = uint8(mode)\n\tdefault:\n\t\treturn fmt.Errorf(\"SetFieldsForEmptyStruct only supports map-generic at present\")\n\t}\n\treturn nil\n}\n\nvar thriftCodec = thrift.NewThriftCodec()\n\nvar pbCodec = protobuf.NewProtobufCodec()\n\ntype binaryThriftGeneric struct{}\n\nfunc (g *binaryThriftGeneric) Framed() bool {\n\treturn true\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\tpc := &binaryThriftCodec{thriftCodec}\n\treturn pc\n}\n\nfunc (g *binaryThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn &Method{method, false}, nil\n}\n\nfunc (g *binaryThriftGeneric) Close() error {\n\treturn nil\n}\n\ntype mapThriftGeneric struct {\n\tcodec *mapThriftCodec\n}\n\nfunc (g *mapThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *mapThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *mapThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *mapThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *mapThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonThriftGeneric struct {\n\tcodec *jsonThriftCodec\n}\n\nfunc (g *jsonThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonPbGeneric struct {\n\tcodec *jsonPbCodec\n}\n\nfunc (g *jsonPbGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *jsonPbGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Protobuf\n}\n\nfunc (g *jsonPbGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonPbGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonPbGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpThriftGeneric struct {\n\tcodec *httpThriftCodec\n}\n\nfunc (g *httpThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *httpThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpPbThriftGeneric struct {\n\tcodec *httpPbThriftCodec\n}\n\nfunc (g *httpPbThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpPbThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpPbThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\n```\n# Function Signature: func HTTPThriftGeneric(p DescriptorProvider, opts ...Option) (Generic, error) {\n# Natural Language Annotations: // HTTPThriftGeneric http mapping Generic.\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.HTTPThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc HTTPThriftGeneric(p DescriptorProvider, opts ...Option) (Generic, error) {\n\tcodec, err := newHTTPThriftCodec(p, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &httpThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n```", "  ```\nfunc HTTPThriftGeneric(p DescriptorProvider, opts ...Option) (Generic, error) {\n\tcodec, err := newHTTPThriftCodec(p, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tg := &httpThriftGeneric{\n\t\tcodec: codec,\n\t}\n\t// Apply options\n\tfor _, opt := range opts {\n\t\tif err := opt(g); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\treturn g, nil\n}\n```", "  ```\nfunc HTTPThriftGeneric(p DescriptorProvider, opts ...Option) (Generic, error) {\n\tvar options Options\n\tif len(opts) > 0 {\n\t\toptions = opts[0]\n\t}\n\n\tcodec, err := newHTTPPbThriftCodec(p, options.PbDescriptorProvider, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &httpThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n```"]}, "kitex-pkg/generic/generic.go-JSONThriftGeneric": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/generic.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package generic ...\npackage generic\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/cloudwego/dynamicgo/conv\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n)\n\n// Generic ...\ntype Generic interface {\n\tCloser\n\t// PayloadCodec return codec implement\n\tPayloadCodec() remote.PayloadCodec\n\t// PayloadCodecType return the type of codec\n\tPayloadCodecType() serviceinfo.PayloadCodec\n\t// RawThriftBinaryGeneric must be framed\n\tFramed() bool\n\t// GetMethod to get method name if need\n\tGetMethod(req interface{}, method string) (*Method, error)\n}\n\n// Method information\ntype Method struct {\n\tName   string\n\tOneway bool\n}\n\n// BinaryThriftGeneric raw thrift binary Generic\nfunc BinaryThriftGeneric() Generic {\n\treturn &binaryThriftGeneric{}\n}\n\n// MapThriftGeneric map mapping generic\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.MapThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n//\n// String value is returned for binary field by default. You can change the return value to []byte for binary field with SetBinaryWithByteSlice.\n// eg:\n//\n//\tSetBinaryWithByteSlice(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc MapThriftGenericForJSON(p DescriptorProvider) (Generic, error) {\n\tcodec, err := newMapThriftCodecForJSON(p, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &mapThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// HTTPThriftGeneric http mapping Generic.\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.HTTPThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc HTTPPbThriftGeneric(p DescriptorProvider, pbp PbDescriptorProvider) (Generic, error) {\n\tcodec, err := newHTTPPbThriftCodec(p, pbp, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &httpPbThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// JSONThriftGeneric json mapping generic.\n// Base64 codec for binary field is enabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.JSONThriftGeneric(p)\n//\tSetBinaryWithBase64(g, false)\n\n\n\n\n\n\n\n\n\n\n// JSONPbGeneric json mapping generic.\n// Uses dynamicgo for json to protobufs conversion, so DynamicGo field is true by default.\n\n\n\n\n\n\n\n\n\n\n\n// SetBinaryWithBase64 enable/disable Base64 codec for binary field.\nfunc SetBinaryWithBase64(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *httpThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t}\n\tcase *jsonThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithException.NoBase64Binary = !enable\n\t\t}\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"Base64Binary is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetBinaryWithByteSlice enable/disable returning []byte for binary field.\nfunc SetBinaryWithByteSlice(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithByteSlice = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"returning []byte for binary fields is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetFieldsForEmptyStructMode is a enum for EnableSetFieldsForEmptyStruct()\ntype SetFieldsForEmptyStructMode uint8\n\nconst (\n\t// NotSetFields means disable\n\tNotSetFields SetFieldsForEmptyStructMode = iota\n\t// SetNonOptiontionalFields means only set required and default fields\n\tSetNonOptiontionalFields\n\t// SetAllFields means set all fields\n\tSetAllFields\n)\n\n// EnableSetFieldsForEmptyStruct enable/disable set all fields of a struct even if it is empty.\n// This option is only applicable to map-generic response (reading) now.\n//\n//\tmode == 0 means disable\n//\tmode == 1 means only set required and default fields\n//\tmode == 2 means set all fields\nfunc EnableSetFieldsForEmptyStruct(g Generic, mode SetFieldsForEmptyStructMode) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.setFieldsForEmptyStruct = uint8(mode)\n\tdefault:\n\t\treturn fmt.Errorf(\"SetFieldsForEmptyStruct only supports map-generic at present\")\n\t}\n\treturn nil\n}\n\nvar thriftCodec = thrift.NewThriftCodec()\n\nvar pbCodec = protobuf.NewProtobufCodec()\n\ntype binaryThriftGeneric struct{}\n\nfunc (g *binaryThriftGeneric) Framed() bool {\n\treturn true\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\tpc := &binaryThriftCodec{thriftCodec}\n\treturn pc\n}\n\nfunc (g *binaryThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn &Method{method, false}, nil\n}\n\nfunc (g *binaryThriftGeneric) Close() error {\n\treturn nil\n}\n\ntype mapThriftGeneric struct {\n\tcodec *mapThriftCodec\n}\n\nfunc (g *mapThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *mapThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *mapThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *mapThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *mapThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonThriftGeneric struct {\n\tcodec *jsonThriftCodec\n}\n\nfunc (g *jsonThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonPbGeneric struct {\n\tcodec *jsonPbCodec\n}\n\nfunc (g *jsonPbGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *jsonPbGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Protobuf\n}\n\nfunc (g *jsonPbGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonPbGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonPbGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpThriftGeneric struct {\n\tcodec *httpThriftCodec\n}\n\nfunc (g *httpThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *httpThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpPbThriftGeneric struct {\n\tcodec *httpPbThriftCodec\n}\n\nfunc (g *httpPbThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpPbThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpPbThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\n```\n# Function Signature: func JSONThriftGeneric(p DescriptorProvider, opts ...Option) (Generic, error) {\n# Natural Language Annotations: // JSONThriftGeneric json mapping generic.\n// Base64 codec for binary field is enabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.JSONThriftGeneric(p)\n//\tSetBinaryWithBase64(g, false)\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc JSONThriftGeneric(p DescriptorProvider, opts ...Option) (Generic, error) {\n\tcodec, err := newJSONThriftCodec(p, thriftCodec, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &jsonThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n```", "  ```\nfunc JSONThriftGeneric(p DescriptorProvider, opts ...Option) (Generic, error) {\n\tcodec, err := newJSONThriftCodec(p, thriftCodec, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &jsonThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n```", "  ```\nfunc JSONThriftGeneric(p DescriptorProvider, opts ...Option) (Generic, error) {\n\t// Set default options if not set\n\tif len(opts) == 0 {\n\t\topts = []Option{\n\t\t\tWithBinaryWithBase64(true),\n\t\t}\n\t}\n\n\t// Apply options to codec\n\tcodec, err := newJSONThriftCodec(p, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, opt := range opts {\n\t\topt(codec)\n\t}\n\n\treturn &jsonThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n```"]}, "kitex-pkg/generic/generic.go-JSONPbGeneric": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/generic.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package generic ...\npackage generic\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/cloudwego/dynamicgo/conv\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n)\n\n// Generic ...\ntype Generic interface {\n\tCloser\n\t// PayloadCodec return codec implement\n\tPayloadCodec() remote.PayloadCodec\n\t// PayloadCodecType return the type of codec\n\tPayloadCodecType() serviceinfo.PayloadCodec\n\t// RawThriftBinaryGeneric must be framed\n\tFramed() bool\n\t// GetMethod to get method name if need\n\tGetMethod(req interface{}, method string) (*Method, error)\n}\n\n// Method information\ntype Method struct {\n\tName   string\n\tOneway bool\n}\n\n// BinaryThriftGeneric raw thrift binary Generic\nfunc BinaryThriftGeneric() Generic {\n\treturn &binaryThriftGeneric{}\n}\n\n// MapThriftGeneric map mapping generic\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.MapThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n//\n// String value is returned for binary field by default. You can change the return value to []byte for binary field with SetBinaryWithByteSlice.\n// eg:\n//\n//\tSetBinaryWithByteSlice(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc MapThriftGenericForJSON(p DescriptorProvider) (Generic, error) {\n\tcodec, err := newMapThriftCodecForJSON(p, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &mapThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// HTTPThriftGeneric http mapping Generic.\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.HTTPThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc HTTPPbThriftGeneric(p DescriptorProvider, pbp PbDescriptorProvider) (Generic, error) {\n\tcodec, err := newHTTPPbThriftCodec(p, pbp, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &httpPbThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// JSONThriftGeneric json mapping generic.\n// Base64 codec for binary field is enabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.JSONThriftGeneric(p)\n//\tSetBinaryWithBase64(g, false)\n\n\n\n\n\n\n\n\n\n\n// JSONPbGeneric json mapping generic.\n// Uses dynamicgo for json to protobufs conversion, so DynamicGo field is true by default.\n\n\n\n\n\n\n\n\n\n\n\n// SetBinaryWithBase64 enable/disable Base64 codec for binary field.\nfunc SetBinaryWithBase64(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *httpThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t}\n\tcase *jsonThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithException.NoBase64Binary = !enable\n\t\t}\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"Base64Binary is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetBinaryWithByteSlice enable/disable returning []byte for binary field.\nfunc SetBinaryWithByteSlice(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithByteSlice = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"returning []byte for binary fields is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetFieldsForEmptyStructMode is a enum for EnableSetFieldsForEmptyStruct()\ntype SetFieldsForEmptyStructMode uint8\n\nconst (\n\t// NotSetFields means disable\n\tNotSetFields SetFieldsForEmptyStructMode = iota\n\t// SetNonOptiontionalFields means only set required and default fields\n\tSetNonOptiontionalFields\n\t// SetAllFields means set all fields\n\tSetAllFields\n)\n\n// EnableSetFieldsForEmptyStruct enable/disable set all fields of a struct even if it is empty.\n// This option is only applicable to map-generic response (reading) now.\n//\n//\tmode == 0 means disable\n//\tmode == 1 means only set required and default fields\n//\tmode == 2 means set all fields\nfunc EnableSetFieldsForEmptyStruct(g Generic, mode SetFieldsForEmptyStructMode) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.setFieldsForEmptyStruct = uint8(mode)\n\tdefault:\n\t\treturn fmt.Errorf(\"SetFieldsForEmptyStruct only supports map-generic at present\")\n\t}\n\treturn nil\n}\n\nvar thriftCodec = thrift.NewThriftCodec()\n\nvar pbCodec = protobuf.NewProtobufCodec()\n\ntype binaryThriftGeneric struct{}\n\nfunc (g *binaryThriftGeneric) Framed() bool {\n\treturn true\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\tpc := &binaryThriftCodec{thriftCodec}\n\treturn pc\n}\n\nfunc (g *binaryThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn &Method{method, false}, nil\n}\n\nfunc (g *binaryThriftGeneric) Close() error {\n\treturn nil\n}\n\ntype mapThriftGeneric struct {\n\tcodec *mapThriftCodec\n}\n\nfunc (g *mapThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *mapThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *mapThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *mapThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *mapThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonThriftGeneric struct {\n\tcodec *jsonThriftCodec\n}\n\nfunc (g *jsonThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonPbGeneric struct {\n\tcodec *jsonPbCodec\n}\n\nfunc (g *jsonPbGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *jsonPbGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Protobuf\n}\n\nfunc (g *jsonPbGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonPbGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonPbGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpThriftGeneric struct {\n\tcodec *httpThriftCodec\n}\n\nfunc (g *httpThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *httpThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpPbThriftGeneric struct {\n\tcodec *httpPbThriftCodec\n}\n\nfunc (g *httpPbThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpPbThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpPbThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\n```\n# Function Signature: func JSONPbGeneric(p PbDescriptorProviderDynamicGo, opts ...Option) (Generic, error) {\n# Natural Language Annotations: // JSONPbGeneric json mapping generic.\n// Uses dynamicgo for json to protobufs conversion, so DynamicGo field is true by default.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc JSONPbGeneric(p PbDescriptorProviderDynamicGo, opts ...Option) (Generic, error) {\n    codec, err := newJSONPbCodec(p, thriftCodec, opts...)\n    if err != nil {\n        return nil, err\n    }\n    return &jsonPbGeneric{\n        codec: codec,\n    }, nil\n}\n```\n", "  ```\nfunc JSONPbGeneric(p PbDescriptorProviderDynamicGo, opts ...Option) (Generic, error) {\n\tcodec, err := newJSONPbCodec(p)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tg := &jsonPbGeneric{codec: codec}\n\tfor _, opt := range opts {\n\t\tif opt == WithDynamicGo {\n\t\t\tg.codec.dynamicgoEnabled = true\n\t\t}\n\t}\n\treturn g, nil\n}\n```", "  ```\nfunc JSONPbGeneric(p PbDescriptorProviderDynamicGo, opts ...Option) (Generic, error) {\n\tgeneric := &jsonPbGeneric{\n\t\tcodec: newJSONPbCodec(p, opts),\n\t}\n\treturn generic, nil\n}\n```"]}, "kitex-pkg/generic/thrift/write.go-writeStruct": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/thrift/write.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/tidwall/gjson\"\n\n\t\"github.com/cloudwego/kitex/pkg/generic/descriptor\"\n\t\"github.com/cloudwego/kitex/pkg/generic/proto\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\ntype writerOption struct {\n\trequestBase *Base // request base from metahandler\n\t// decoding Base64 to binary\n\tbinaryWithBase64 bool\n}\n\ntype writer func(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error\n\ntype fieldGetter func(val interface{}, field *descriptor.FieldDescriptor) (interface{}, bool)\n\nvar mapGetter fieldGetter = func(val interface{}, field *descriptor.FieldDescriptor) (interface{}, bool) {\n\tst := val.(map[string]interface{})\n\tret, ok := st[field.FieldName()]\n\treturn ret, ok\n}\n\nvar pbGetter fieldGetter = func(val interface{}, field *descriptor.FieldDescriptor) (interface{}, bool) {\n\tst := val.(proto.Message)\n\tret, err := st.TryGetFieldByNumber(int(field.ID))\n\treturn ret, err == nil\n}\n\nfunc typeOf(sample interface{}, t *descriptor.TypeDescriptor, opt *writerOption) (descriptor.Type, writer, error) {\n\ttt := t.Type\n\tswitch sample.(type) {\n\tcase bool:\n\t\treturn descriptor.BOOL, writeBool, nil\n\tcase int8, byte:\n\t\tswitch tt {\n\t\tcase descriptor.I08, descriptor.I16, descriptor.I32, descriptor.I64:\n\t\t\treturn tt, writeInt8, nil\n\t\t}\n\tcase int16:\n\t\tswitch tt {\n\t\tcase descriptor.I08, descriptor.I16, descriptor.I32, descriptor.I64:\n\t\t\treturn tt, writeInt16, nil\n\t\t}\n\tcase int32:\n\t\tswitch tt {\n\t\tcase descriptor.I08, descriptor.I16, descriptor.I32, descriptor.I64:\n\t\t\treturn tt, writeInt32, nil\n\t\t}\n\tcase int64:\n\t\tswitch tt {\n\t\tcase descriptor.I08, descriptor.I16, descriptor.I32, descriptor.I64:\n\t\t\treturn tt, writeInt64, nil\n\t\t}\n\tcase float64:\n\t\t// maybe come from json decode\n\t\tswitch tt {\n\t\tcase descriptor.I08, descriptor.I16, descriptor.I32, descriptor.I64, descriptor.DOUBLE:\n\t\t\treturn tt, writeJSONFloat64, nil\n\t\t}\n\tcase json.Number:\n\t\tswitch tt {\n\t\tcase descriptor.I08, descriptor.I16, descriptor.I32, descriptor.I64, descriptor.DOUBLE:\n\t\t\treturn tt, writeJSONNumber, nil\n\t\t}\n\tcase string:\n\t\t// maybe a base64 string encoded from binary\n\t\tif t.Name == \"binary\" && opt.binaryWithBase64 {\n\t\t\treturn descriptor.STRING, writeBase64Binary, nil\n\t\t}\n\t\t// maybe a json number string\n\t\treturn descriptor.STRING, writeString, nil\n\tcase []byte:\n\t\tif tt == descriptor.LIST {\n\t\t\treturn descriptor.LIST, writeBinaryList, nil\n\t\t}\n\t\treturn descriptor.STRING, writeBinary, nil\n\tcase []interface{}:\n\t\treturn descriptor.LIST, writeList, nil\n\tcase map[interface{}]interface{}:\n\t\treturn descriptor.MAP, writeInterfaceMap, nil\n\tcase map[string]interface{}:\n\t\t//  4: optional map<i64, ReqItem> req_items (api.body='req_items')\n\t\t// need parse string into int64\n\t\tswitch tt {\n\t\tcase descriptor.STRUCT:\n\t\t\treturn descriptor.STRUCT, writeStruct, nil\n\t\tcase descriptor.MAP:\n\t\t\treturn descriptor.MAP, writeStringMap, nil\n\t\t}\n\tcase proto.Message:\n\t\treturn descriptor.STRUCT, writeStruct, nil\n\tcase *descriptor.HTTPRequest:\n\t\treturn descriptor.STRUCT, writeHTTPRequest, nil\n\tcase *gjson.Result:\n\t\treturn descriptor.STRUCT, writeJSON, nil\n\tcase nil, descriptor.Void: // nil and Void\n\t\treturn descriptor.VOID, writeVoid, nil\n\t}\n\treturn 0, nil, fmt.Errorf(\"unsupported type:%T, expected type:%s\", sample, tt)\n}\n\nfunc typeJSONOf(data *gjson.Result, t *descriptor.TypeDescriptor, opt *writerOption) (v interface{}, w writer, err error) {\n\ttt := t.Type\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr = perrors.NewProtocolErrorWithType(perrors.InvalidData, fmt.Sprintf(\"json convert error:%#+v\", r))\n\t\t}\n\t}()\n\tswitch tt {\n\tcase descriptor.BOOL:\n\t\tv = data.Bool()\n\t\tw = writeBool\n\t\treturn\n\tcase descriptor.I08:\n\t\tv = int8(data.Int())\n\t\tw = writeInt8\n\t\treturn\n\tcase descriptor.I16:\n\t\tv = int16(data.Int())\n\t\tw = writeInt16\n\t\treturn\n\tcase descriptor.I32:\n\t\tv = int32(data.Int())\n\t\tw = writeInt32\n\t\treturn\n\tcase descriptor.I64:\n\t\tv = data.Int()\n\t\tw = writeInt64\n\t\treturn\n\tcase descriptor.DOUBLE:\n\t\tv = data.Float()\n\t\tw = writeJSONFloat64\n\t\treturn\n\tcase descriptor.STRING:\n\t\tv = data.String()\n\t\tif t.Name == \"binary\" && opt.binaryWithBase64 {\n\t\t\tw = writeBase64Binary\n\t\t} else {\n\t\t\tw = writeString\n\t\t}\n\t\treturn\n\t// case descriptor.BINARY:\n\t//\treturn writeBinary, nil\n\tcase descriptor.SET, descriptor.LIST:\n\t\tv = data.Array()\n\t\tw = writeJSONList\n\t\treturn\n\tcase descriptor.MAP:\n\t\tv = data.Map()\n\t\tw = writeStringJSONMap\n\t\treturn\n\tcase descriptor.STRUCT:\n\t\tv = data\n\t\tw = writeJSON\n\t\treturn\n\tcase descriptor.VOID: // nil and Void\n\t\tv = data\n\t\tw = writeVoid\n\t\treturn\n\t}\n\treturn 0, nil, fmt.Errorf(\"data:%#v, expected type:%s, err:%#v\", data, tt, err)\n}\n\nfunc nextWriter(sample interface{}, t *descriptor.TypeDescriptor, opt *writerOption) (writer, error) {\n\ttt, fn, err := typeOf(sample, t, opt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif t.Type == thrift.SET && tt == thrift.LIST {\n\t\ttt = thrift.SET\n\t}\n\treturn fn, assertType(t.Type, tt)\n}\n\nfunc nextJSONWriter(data *gjson.Result, t *descriptor.TypeDescriptor, opt *writerOption) (interface{}, writer, error) {\n\tv, fn, err := typeJSONOf(data, t, opt)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn v, fn, nil\n}\n\nfunc writeEmptyValue(out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tswitch t.Type {\n\tcase descriptor.BOOL:\n\t\treturn out.WriteBool(false)\n\tcase descriptor.I08:\n\t\treturn out.WriteByte(0)\n\tcase descriptor.I16:\n\t\treturn out.WriteI16(0)\n\tcase descriptor.I32:\n\t\treturn out.WriteI32(0)\n\tcase descriptor.I64:\n\t\treturn out.WriteI64(0)\n\tcase descriptor.DOUBLE:\n\t\treturn out.WriteDouble(0)\n\tcase descriptor.STRING:\n\t\tif t.Name == \"binary\" && opt.binaryWithBase64 {\n\t\t\treturn out.WriteBinary([]byte{})\n\t\t} else {\n\t\t\treturn out.WriteString(\"\")\n\t\t}\n\tcase descriptor.LIST, descriptor.SET:\n\t\tif err := out.WriteListBegin(t.Elem.Type.ToThriftTType(), 0); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn out.WriteListEnd()\n\tcase descriptor.MAP:\n\t\tif err := out.WriteMapBegin(t.Key.Type.ToThriftTType(), t.Elem.Type.ToThriftTType(), 0); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn out.WriteMapEnd()\n\tcase descriptor.STRUCT:\n\t\tif err := out.WriteStructBegin(t.Name); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := out.WriteFieldStop(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn out.WriteStructEnd()\n\tcase descriptor.VOID:\n\t\treturn nil\n\t}\n\treturn fmt.Errorf(\"unsupported type:%T\", t)\n}\n\nfunc wrapStructWriter(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tif err := out.WriteStructBegin(t.Struct.Name); err != nil {\n\t\treturn err\n\t}\n\tfor name, field := range t.Struct.FieldsByName {\n\t\tif field.IsException {\n\t\t\t// generic server ignore the exception, because no description for exception\n\t\t\t// generic handler just return error\n\t\t\tcontinue\n\t\t}\n\t\tif val != nil {\n\t\t\tif err := out.WriteFieldBegin(field.Name, field.Type.Type.ToThriftTType(), int16(field.ID)); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\twriter, err := nextWriter(val, field.Type, opt)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"nextWriter of field[%s] error %w\", name, err)\n\t\t\t}\n\t\t\tif err := writer(ctx, val, out, field.Type, opt); err != nil {\n\t\t\t\treturn fmt.Errorf(\"writer of field[%s] error %w\", name, err)\n\t\t\t}\n\t\t\tif err := out.WriteFieldEnd(); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\tif err := out.WriteFieldStop(); err != nil {\n\t\treturn err\n\t}\n\treturn out.WriteStructEnd()\n}\n\nfunc wrapJSONWriter(ctx context.Context, val *gjson.Result, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tif err := out.WriteStructBegin(t.Struct.Name); err != nil {\n\t\treturn err\n\t}\n\tfor name, field := range t.Struct.FieldsByName {\n\t\tif field.IsException {\n\t\t\t// generic server ignore the exception, because no description for exception\n\t\t\t// generic handler just return error\n\t\t\tcontinue\n\t\t}\n\t\tif err := out.WriteFieldBegin(field.Name, field.Type.Type.ToThriftTType(), int16(field.ID)); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tv, writer, err := nextJSONWriter(val, field.Type, opt)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"nextJSONWriter of field[%s] error %w\", name, err)\n\t\t}\n\t\tif err := writer(ctx, v, out, field.Type, opt); err != nil {\n\t\t\treturn fmt.Errorf(\"writer of field[%s] error %w\", name, err)\n\t\t}\n\t\tif err := out.WriteFieldEnd(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif err := out.WriteFieldStop(); err != nil {\n\t\treturn err\n\t}\n\treturn out.WriteStructEnd()\n}\n\nfunc writeVoid(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\treturn writeStruct(ctx, map[string]interface{}{}, out, t, opt)\n}\n\nfunc writeBool(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\treturn out.WriteBool(val.(bool))\n}\n\nfunc writeInt8(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tvar i int8\n\tswitch val := val.(type) {\n\tcase int8:\n\t\ti = val\n\tcase uint8:\n\t\ti = int8(val)\n\tdefault:\n\t\treturn fmt.Errorf(\"unsupported type: %T\", val)\n\t}\n\t// compatible with lossless conversion\n\tswitch t.Type {\n\tcase thrift.I08:\n\t\treturn out.WriteByte(i)\n\tcase thrift.I16:\n\t\treturn out.WriteI16(int16(i))\n\tcase thrift.I32:\n\t\treturn out.WriteI32(int32(i))\n\tcase thrift.I64:\n\t\treturn out.WriteI64(int64(i))\n\t}\n\treturn fmt.Errorf(\"need int type, but got: %s\", t.Type)\n}\n\nfunc writeInt16(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\t// compatible with lossless conversion\n\ti := val.(int16)\n\tswitch t.Type {\n\tcase thrift.I08:\n\t\tif i&0xff != i {\n\t\t\treturn fmt.Errorf(\"value is beyond range of i8: %v\", i)\n\t\t}\n\t\treturn out.WriteByte(int8(i))\n\tcase thrift.I16:\n\t\treturn out.WriteI16(i)\n\tcase thrift.I32:\n\t\treturn out.WriteI32(int32(i))\n\tcase thrift.I64:\n\t\treturn out.WriteI64(int64(i))\n\t}\n\treturn fmt.Errorf(\"need int type, but got: %s\", t.Type)\n}\n\nfunc writeInt32(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\t// compatible with lossless conversion\n\ti := val.(int32)\n\tswitch t.Type {\n\tcase thrift.I08:\n\t\tif i&0xff != i {\n\t\t\treturn fmt.Errorf(\"value is beyond range of i8: %v\", i)\n\t\t}\n\t\treturn out.WriteByte(int8(i))\n\tcase thrift.I16:\n\t\tif i&0xffff != i {\n\t\t\treturn fmt.Errorf(\"value is beyond range of i16: %v\", i)\n\t\t}\n\t\treturn out.WriteI16(int16(i))\n\tcase thrift.I32:\n\t\treturn out.WriteI32(i)\n\tcase thrift.I64:\n\t\treturn out.WriteI64(int64(i))\n\t}\n\treturn fmt.Errorf(\"need int type, but got: %s\", t.Type)\n}\n\nfunc writeInt64(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\t// compatible with lossless conversion\n\ti := val.(int64)\n\tswitch t.Type {\n\tcase thrift.I08:\n\t\tif i&0xff != i {\n\t\t\treturn fmt.Errorf(\"value is beyond range of i8: %v\", i)\n\t\t}\n\t\treturn out.WriteByte(int8(i))\n\tcase thrift.I16:\n\t\tif i&0xffff != i {\n\t\t\treturn fmt.Errorf(\"value is beyond range of i16: %v\", i)\n\t\t}\n\t\treturn out.WriteI16(int16(i))\n\tcase thrift.I32:\n\t\tif i&0xffffffff != i {\n\t\t\treturn fmt.Errorf(\"value is beyond range of i32: %v\", i)\n\t\t}\n\t\treturn out.WriteI32(int32(i))\n\tcase thrift.I64:\n\t\treturn out.WriteI64(i)\n\t}\n\treturn fmt.Errorf(\"need int type, but got: %s\", t.Type)\n}\n\nfunc writeJSONNumber(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tjn := val.(json.Number)\n\tswitch t.Type {\n\tcase thrift.I08:\n\t\ti, err := jn.Int64()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn writeInt8(ctx, int8(i), out, t, opt)\n\tcase thrift.I16:\n\t\ti, err := jn.Int64()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn writeInt16(ctx, int16(i), out, t, opt)\n\tcase thrift.I32:\n\t\ti, err := jn.Int64()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn writeInt32(ctx, int32(i), out, t, opt)\n\tcase thrift.I64:\n\t\ti, err := jn.Int64()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn writeInt64(ctx, i, out, t, opt)\n\tcase thrift.DOUBLE:\n\t\ti, err := jn.Float64()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn writeFloat64(ctx, i, out, t, opt)\n\t}\n\treturn nil\n}\n\nfunc writeJSONFloat64(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\ti := val.(float64)\n\tswitch t.Type {\n\tcase thrift.I08:\n\t\treturn writeInt8(ctx, int8(i), out, t, opt)\n\tcase thrift.I16:\n\t\treturn writeInt16(ctx, int16(i), out, t, opt)\n\tcase thrift.I32:\n\t\treturn writeInt32(ctx, int32(i), out, t, opt)\n\tcase thrift.I64:\n\t\treturn writeInt64(ctx, int64(i), out, t, opt)\n\tcase thrift.DOUBLE:\n\t\treturn writeFloat64(ctx, i, out, t, opt)\n\t}\n\treturn fmt.Errorf(\"need number type, but got: %s\", t.Type)\n}\n\nfunc writeFloat64(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\treturn out.WriteDouble(val.(float64))\n}\n\nfunc writeString(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\treturn out.WriteString(val.(string))\n}\n\nfunc writeBase64Binary(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tbytes, err := base64.StdEncoding.DecodeString(val.(string))\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn out.WriteBinary(bytes)\n}\n\nfunc writeBinary(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\treturn out.WriteBinary(val.([]byte))\n}\n\nfunc writeBinaryList(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tl := val.([]byte)\n\tlength := len(l)\n\tif err := out.WriteListBegin(t.Elem.Type.ToThriftTType(), length); err != nil {\n\t\treturn err\n\t}\n\tfor _, b := range l {\n\t\tif err := out.WriteByte(int8(b)); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn out.WriteListEnd()\n}\n\nfunc writeList(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tl := val.([]interface{})\n\tlength := len(l)\n\tif err := out.WriteListBegin(t.Elem.Type.ToThriftTType(), length); err != nil {\n\t\treturn err\n\t}\n\tif length == 0 {\n\t\treturn out.WriteListEnd()\n\t}\n\tvar (\n\t\twriter writer\n\t\terr    error\n\t)\n\tfor _, elem := range l {\n\t\tif elem == nil {\n\t\t\tif err = writeEmptyValue(out, t.Elem, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\tif writer == nil {\n\t\t\t\tif writer, err = nextWriter(elem, t.Elem, opt); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err := writer(ctx, elem, out, t.Elem, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn out.WriteListEnd()\n}\n\nfunc writeJSONList(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tl := val.([]gjson.Result)\n\tlength := len(l)\n\tif err := out.WriteListBegin(t.Elem.Type.ToThriftTType(), length); err != nil {\n\t\treturn err\n\t}\n\tif length == 0 {\n\t\treturn out.WriteListEnd()\n\t}\n\tfor _, elem := range l {\n\t\tv, writer, err := nextJSONWriter(&elem, t.Elem, opt)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := writer(ctx, v, out, t.Elem, opt); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn out.WriteListEnd()\n}\n\nfunc writeInterfaceMap(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tm := val.(map[interface{}]interface{})\n\tlength := len(m)\n\tif err := out.WriteMapBegin(t.Key.Type.ToThriftTType(), t.Elem.Type.ToThriftTType(), length); err != nil {\n\t\treturn err\n\t}\n\tif length == 0 {\n\t\treturn out.WriteMapEnd()\n\t}\n\tvar (\n\t\tkeyWriter  writer\n\t\telemWriter writer\n\t\terr        error\n\t)\n\tfor key, elem := range m {\n\t\tif keyWriter == nil {\n\t\t\tif keyWriter, err = nextWriter(key, t.Key, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif err := keyWriter(ctx, key, out, t.Key, opt); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif elem == nil {\n\t\t\tif err = writeEmptyValue(out, t.Elem, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\tif elemWriter == nil {\n\t\t\t\tif elemWriter, err = nextWriter(elem, t.Elem, opt); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err := elemWriter(ctx, elem, out, t.Elem, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn out.WriteMapEnd()\n}\n\nfunc writeStringMap(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tm := val.(map[string]interface{})\n\tlength := len(m)\n\tif err := out.WriteMapBegin(t.Key.Type.ToThriftTType(), t.Elem.Type.ToThriftTType(), length); err != nil {\n\t\treturn err\n\t}\n\tif length == 0 {\n\t\treturn out.WriteMapEnd()\n\t}\n\n\tvar (\n\t\tkeyWriter  writer\n\t\telemWriter writer\n\t)\n\tfor key, elem := range m {\n\t\t_key, err := buildinTypeFromString(key, t.Key)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif keyWriter == nil {\n\t\t\tif keyWriter, err = nextWriter(_key, t.Key, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif err := keyWriter(ctx, _key, out, t.Key, opt); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif elem == nil {\n\t\t\tif err = writeEmptyValue(out, t.Elem, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\tif elemWriter == nil {\n\t\t\t\tif elemWriter, err = nextWriter(elem, t.Elem, opt); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err := elemWriter(ctx, elem, out, t.Elem, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn out.WriteMapEnd()\n}\n\nfunc writeStringJSONMap(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *\n```\n# Function Signature: func writeStruct(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n# Natural Language Annotations: // writeStruct iter with Descriptor, can check the field's required and others\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc writeStruct(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\treturn wrapStructWriter(ctx, val, out, t, opt)\n}\n```", "  ```\nfunc writeStruct(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tif t.Struct == nil {\n\t\t// field not a thrift struct, will fallback to the default method\n\t\treturn writeEmptyValue(out, t, opt)\n\t}\n\tif val != nil {\n\t\t// only write struct field when val is not nil\n\t\tif s, ok := val.(proto.Message); ok {\n\t\t\t// write a proto struct value\n\t\t\treturn wrapStructWriter(ctx, s, out, t, opt)\n\t\t}\n\t\tif j, ok := val.(*gjson.Result); ok {\n\t\t\t// write a json value\n\t\t\treturn wrapJSONWriter(ctx, j, out, t, opt)\n\t\t}\n\t}\n\treturn errNoValue\n}\n```", "  ```\nfunc writeStruct(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\t// writeStruct iter with Descriptor, can check the field's required and others\n\tswitch t.Type {\n\tcase descriptor.STRUCT:\n\t\tfields := t.Struct.FieldsByName\n\t\tif err := out.WriteStructBegin(fields.Name); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tvar (\n\t\t\twriter      writer\n\t\t\tfield       *descriptor.FieldDescriptor\n\t\t\tfieldsValue map[string]interface{}\n\t\t\tok          bool\n\t\t)\n\t\tfor name, field = range fields {\n\t\t\tif !field.IsRequired && val == nil {\n\t\t\t\tval = mapJSONGetter(ctx, field)\n\t\t\t}\n\t\t\tif field.IsException {\n\t\t\t\t// generic server ignore the exception, because no description for exception\n\t\t\t\t// generic handler just return error\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif val != nil {\n\t\t\t\tswitch val.(type) {\n\t\t\t\tcase map[string]interface{}:\n\t\t\t\t\tfieldsValue = val.(map[string]interface{})\n\t\t\t\t\tval, ok = fieldsValue[field.Name]\n\t\t\t\t\tif !ok {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif val == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err := out.WriteFieldBegin(contextFieldName, thrift.STRUCT, int16(field.ID)); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\twriter, err := nextWriter(val, field.Type, opt)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"nextWriter of field[%s] error %w\", name, err)\n\t\t\t}\n\t\t\tif err := writer(ctx, val, out, field.Type, opt); err != nil {\n\t\t\t\treturn fmt.Errorf(\"writer of field[%s] error %w\", name, err)\n\t\t\t}\n\t\t\tif err := out.WriteFieldEnd(); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif err := out.WriteFieldStop(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn out.WriteStructEnd()\n\tcase descriptor.MAP:\n\t\t// base on the key of map\n\t\tkeyType := t.Key.Type\n\t\tif keyType.Type == descriptor.STRUCT {\n\t\t\twriter, err := nextWriter(val, keyType, opt)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\treturn writer(ctx, val, out, keyType, opt)\n\t\t}\n\tcase descriptor.LIST:\n\t\t// base on the elem of list\n\t\telemType := t.Elem.Type\n\t\tif elemType.Type == descriptor.STRUCT {\n\t\t\twriter, err := nextWriter(val, elemType, opt)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\treturn writer(ctx, val, out, elemType, opt)\n\t\t}\n\t}\n\treturn fmt.Errorf(\"writeStruct of [%d] type %s error: unsupported type\", t.ID, t.Type)\n}\n```"]}, "kitex-pkg/generic/proto/json.go-Write": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/proto/json.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage proto\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/cloudwego/dynamicgo/conv\"\n\tdconvj2p \"github.com/cloudwego/dynamicgo/conv/j2p\"\n\tdconvp2j \"github.com/cloudwego/dynamicgo/conv/p2j\"\n\tdproto \"github.com/cloudwego/dynamicgo/proto\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// NewWriteJSON build WriteJSON according to ServiceDescriptor\nfunc NewWriteJSON(svc *dproto.ServiceDescriptor, method string, isClient bool, convOpts *conv.Options) (*WriteJSON, error) {\n\tfnDsc := svc.LookupMethodByName(method)\n\tif fnDsc == nil {\n\t\treturn nil, fmt.Errorf(\"missing method: %s in service: %s\", method, svc.Name())\n\t}\n\n\t// from the proto.ServiceDescriptor, get the TypeDescriptor\n\ttypeDescriptor := fnDsc.Input()\n\tif !isClient {\n\t\ttypeDescriptor = fnDsc.Output()\n\t}\n\n\tws := &WriteJSON{\n\t\tdynamicgoConvOpts: convOpts,\n\t\tdynamicgoTypeDsc:  typeDescriptor,\n\t\tisClient:          isClient,\n\t}\n\treturn ws, nil\n}\n\n// WriteJSON implement of MessageWriter\ntype WriteJSON struct {\n\tdynamicgoConvOpts *conv.Options\n\tdynamicgoTypeDsc  *dproto.TypeDescriptor\n\tisClient          bool\n}\n\nvar _ MessageWriter = (*WriteJSON)(nil)\n\n// Write converts msg to protobuf wire format and returns an output bytebuffer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewReadJSON build ReadJSON according to ServiceDescriptor\nfunc NewReadJSON(svc *dproto.ServiceDescriptor, isClient bool, convOpts *conv.Options) (*ReadJSON, error) {\n\t// extract svc to be used to convert later\n\treturn &ReadJSON{\n\t\tdynamicgoConvOpts: convOpts,\n\t\tdynamicgoSvcDsc:   svc,\n\t\tisClient:          isClient,\n\t}, nil\n}\n\n// ReadJSON implement of MessageReaderWithMethod\ntype ReadJSON struct {\n\tdynamicgoConvOpts *conv.Options\n\tdynamicgoSvcDsc   *dproto.ServiceDescriptor\n\tisClient          bool\n}\n\nvar _ MessageReader = (*ReadJSON)(nil)\n\n// Read reads data from actualMsgBuf and convert to json string\nfunc (m *ReadJSON) Read(ctx context.Context, method string, actualMsgBuf []byte) (interface{}, error) {\n\t// create dynamic message here, once method string has been extracted\n\tfnDsc := m.dynamicgoSvcDsc.LookupMethodByName(method)\n\tif fnDsc == nil {\n\t\treturn nil, fmt.Errorf(\"missing method: %s in service: %s\", method, m.dynamicgoSvcDsc.Name())\n\t}\n\n\t// from the dproto.ServiceDescriptor, get the TypeDescriptor\n\ttypeDescriptor := fnDsc.Output()\n\tif !m.isClient {\n\t\ttypeDescriptor = fnDsc.Input()\n\t}\n\n\tcv := dconvp2j.NewBinaryConv(*m.dynamicgoConvOpts)\n\tout, err := cv.Do(context.Background(), typeDescriptor, actualMsgBuf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn string(out), nil\n}\n\n```\n# Function Signature: func (m *WriteJSON) Write(ctx context.Context, msg interface{}) (interface{}, error) {\n# Natural Language Annotations: // Write converts msg to protobuf wire format and returns an output bytebuffer\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (m *WriteJSON) Write(ctx context.Context, msg interface{}) (interface{}, error) {\n\t// create dynamic message here, once method string has been extracted\n\tfnDsc := m.dynamicgoSvcDsc.LookupMethodByName(method)\n\tif fnDsc == nil {\n\t\treturn nil, fmt.Errorf(\"missing method: %s in service: %s\", method, m.dynamicgoSvcDsc.Name())\n\t}\n\n\t// from the dproto.ServiceDescriptor, get the TypeDescriptor\n\ttypeDescriptor := fnDsc.Output()\n\tif !m.isClient {\n\t\ttypeDescriptor = fnDsc.Input()\n\t}\n\n\tcv := dconvj2p.NewBinaryConv(*m.dynamicgoConvOpts)\n\tout, err := cv.Do(context.Background(), typeDescriptor, msg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn []byte(out), nil\n}\n```", "  ```\nfunc (m *WriteJSON) Write(ctx context.Context, msg interface{}) (interface{}, error) {\n\t// convert msg to json, using the TypeDescriptor and ConvOpts from the constructor\n\tcv := dconvj2p.NewBinaryConv(*m.dynamicgoConvOpts)\n\tjsonMsgBuf, err := cv.Do(context.Background(), m.dynamicgoTypeDsc, msg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// return the json buffer as the output\n\treturn jsonMsgBuf, nil\n}\n```", "  ```\nfunc (m *WriteJSON) Write(ctx context.Context, msg interface{}) ([]byte, error) {\n\t// convert msg to typed message\n\ttypedMsg, err := m.asmTypedMsg(msg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// convert typedMessage to binary\n\tb, err := dynamicgoConv.NewBinaryConv(*m.dynamicgoConvOpts).Do(context.Background(), m.dynamicgoTypeDsc, typedMsg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn b, nil\n}\n\n// asmTypedMsg assembles typed message based on msg\nfunc (m *WriteJSON) asmTypedMsg(msg interface{}) ([]byte, error) {\n\t// convert msg to json\n\tjsonMsg, err := dconvp2j.NewBinaryConv(*m.dynamicgoConvOpts).Do(context.Background(), m.dynamicgoTypeDsc, msg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// create typed message\n\ttypedMsg := dynamicgoTypeDsc.NewMessage()\n\terr = typedMsg.UnmarshalJSON(jsonMsg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn typedMsg, nil\n}\n```"]}, "kitex-pkg/generic/descriptor/tree.go-addRoute": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/descriptor/tree.go:\n```\n/*\n * Copyright 2013 Julien Schmidt. All rights reserved.\n * Use of this source code is governed by a BSD-style license that can be found\n * in the LICENSE file.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage descriptor\n\nimport (\n\t\"fmt\"\n\t\"net/url\"\n\t\"strings\"\n)\n\nfunc countParams(path string) uint16 {\n\tvar n uint\n\tfor i := range []byte(path) {\n\t\tswitch path[i] {\n\t\tcase ':', '*':\n\t\t\tn++\n\t\t}\n\t}\n\treturn uint16(n)\n}\n\ntype nodeType uint8\n\nconst (\n\tstatic nodeType = iota // default\n\tparam\n\tcatchAll\n\tparamLabel = byte(':')\n\tanyLabel   = byte('*')\n\tslash      = \"/\"\n\tnilString  = \"\"\n)\n\ntype (\n\tnode struct {\n\t\tnType    nodeType\n\t\tlabel    byte\n\t\tprefix   string\n\t\tparent   *node\n\t\tchildren children\n\t\t// original path\n\t\tppath string\n\t\t// param names\n\t\tpnames     []string\n\t\tfunction   *FunctionDescriptor\n\t\tparamChild *node\n\t\tanyChild   *node\n\t\t// isLeaf indicates that node does not have child routes\n\t\tisLeaf bool\n\t}\n\tchildren []*node\n)\n\nfunc checkPathValid(path string) {\n\tif path == nilString {\n\t\tpanic(\"empty path\")\n\t}\n\tif path[0] != '/' {\n\t\tpanic(\"path must begin with '/'\")\n\t}\n\tfor i, c := range []byte(path) {\n\t\tswitch c {\n\t\tcase ':':\n\t\t\tif (i < len(path)-1 && path[i+1] == '/') || i == (len(path)-1) {\n\t\t\t\tpanic(\"wildcards must be named with a non-empty name in path '\" + path + \"'\")\n\t\t\t}\n\t\t\ti++\n\t\t\tfor ; i < len(path) && path[i] != '/'; i++ {\n\t\t\t\tif path[i] == ':' || path[i] == '*' {\n\t\t\t\t\tpanic(\"only one wildcard per path segment is allowed, find multi in path '\" + path + \"'\")\n\t\t\t\t}\n\t\t\t}\n\t\tcase '*':\n\t\t\tif i == len(path)-1 {\n\t\t\t\tpanic(\"wildcards must be named with a non-empty name in path '\" + path + \"'\")\n\t\t\t}\n\t\t\tif i > 0 && path[i-1] != '/' {\n\t\t\t\tpanic(\" no / before wildcards in path \" + path)\n\t\t\t}\n\t\t\tfor ; i < len(path); i++ {\n\t\t\t\tif path[i] == '/' {\n\t\t\t\t\tpanic(\"catch-all routes are only allowed at the end of the path in path '\" + path + \"'\")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// addRoute adds a node with the given function to the path.\n// Not concurrency-safe!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (n *node) insert(path string, function *FunctionDescriptor, t nodeType, ppath string, pnames []string) {\n\tcurrentNode := n\n\tsearch := path\n\n\tfor {\n\t\tsearchLen := len(search)\n\t\tprefixLen := len(currentNode.prefix)\n\t\tlcpLen := 0\n\n\t\tmax := prefixLen\n\t\tif searchLen < max {\n\t\t\tmax = searchLen\n\t\t}\n\t\tfor ; lcpLen < max && search[lcpLen] == currentNode.prefix[lcpLen]; lcpLen++ {\n\t\t}\n\n\t\tif lcpLen == 0 {\n\t\t\tcurrentNode.label = search[0]\n\t\t\tcurrentNode.prefix = search\n\t\t\tif function != nil {\n\t\t\t\tcurrentNode.nType = t\n\t\t\t\tcurrentNode.function = function\n\t\t\t\tcurrentNode.ppath = ppath\n\t\t\t\tcurrentNode.pnames = pnames\n\t\t\t}\n\t\t\tcurrentNode.isLeaf = currentNode.children == nil && currentNode.paramChild == nil && currentNode.anyChild == nil\n\t\t} else if lcpLen < prefixLen {\n\t\t\t// Split node\n\t\t\tn := newNode(\n\t\t\t\tcurrentNode.nType,\n\t\t\t\tcurrentNode.prefix[lcpLen:],\n\t\t\t\tcurrentNode,\n\t\t\t\tcurrentNode.children,\n\t\t\t\tcurrentNode.function,\n\t\t\t\tcurrentNode.ppath,\n\t\t\t\tcurrentNode.pnames,\n\t\t\t\tcurrentNode.paramChild,\n\t\t\t\tcurrentNode.anyChild,\n\t\t\t)\n\t\t\t// Update parent path for all children to new node\n\t\t\tfor _, child := range currentNode.children {\n\t\t\t\tchild.parent = n\n\t\t\t}\n\t\t\tif currentNode.paramChild != nil {\n\t\t\t\tcurrentNode.paramChild.parent = n\n\t\t\t}\n\t\t\tif currentNode.anyChild != nil {\n\t\t\t\tcurrentNode.anyChild.parent = n\n\t\t\t}\n\n\t\t\t// Reset parent node\n\t\t\tcurrentNode.nType = static\n\t\t\tcurrentNode.label = currentNode.prefix[0]\n\t\t\tcurrentNode.prefix = currentNode.prefix[:lcpLen]\n\t\t\tcurrentNode.children = nil\n\t\t\tcurrentNode.function = nil\n\t\t\tcurrentNode.ppath = nilString\n\t\t\tcurrentNode.pnames = nil\n\t\t\tcurrentNode.paramChild = nil\n\t\t\tcurrentNode.anyChild = nil\n\t\t\tcurrentNode.isLeaf = false\n\n\t\t\t// Only Static children could reach here\n\t\t\tcurrentNode.children = append(currentNode.children, n)\n\n\t\t\tif lcpLen == searchLen {\n\t\t\t\t// At parent node\n\t\t\t\tcurrentNode.nType = t\n\t\t\t\tcurrentNode.function = function\n\t\t\t\tcurrentNode.ppath = ppath\n\t\t\t\tcurrentNode.pnames = pnames\n\t\t\t} else {\n\t\t\t\t// Create child node\n\t\t\t\tn = newNode(t, search[lcpLen:], currentNode, nil, function, ppath, pnames, nil, nil)\n\t\t\t\t// Only Static children could reach here\n\t\t\t\tcurrentNode.children = append(currentNode.children, n)\n\t\t\t}\n\t\t\tcurrentNode.isLeaf = currentNode.children == nil && currentNode.paramChild == nil && currentNode.anyChild == nil\n\t\t} else if lcpLen < searchLen {\n\t\t\tsearch = search[lcpLen:]\n\t\t\tc := currentNode.findChildWithLabel(search[0])\n\t\t\tif c != nil {\n\t\t\t\t// Go deeper\n\t\t\t\tcurrentNode = c\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Create child node\n\t\t\tn := newNode(t, search, currentNode, nil, function, ppath, pnames, nil, nil)\n\t\t\tswitch t {\n\t\t\tcase static:\n\t\t\t\tcurrentNode.children = append(currentNode.children, n)\n\t\t\tcase param:\n\t\t\t\tcurrentNode.paramChild = n\n\t\t\tcase catchAll:\n\t\t\t\tcurrentNode.anyChild = n\n\t\t\t}\n\t\t\tcurrentNode.isLeaf = currentNode.children == nil && currentNode.paramChild == nil && currentNode.anyChild == nil\n\t\t} else {\n\t\t\t// Node already exists\n\t\t\tif currentNode.function != nil && function != nil {\n\t\t\t\tpanic(\"handlers are already registered for path '\" + ppath + \"'\")\n\t\t\t}\n\n\t\t\tif function != nil {\n\t\t\t\tcurrentNode.function = function\n\t\t\t\tcurrentNode.ppath = ppath\n\t\t\t\tif len(currentNode.pnames) == 0 {\n\t\t\t\t\tcurrentNode.pnames = pnames\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn\n\t}\n}\n\n// Returns the function registered with the given path (key). The values of\n// wildcards are saved to a map.\n// If no function can be found, a TSR (trailing slash redirect) recommendation is\n// made if a function exists with an extra (without the) trailing slash for the\n// given path.\nfunc (n *node) getValue(path string, params func() *Params, unescape bool) (function *FunctionDescriptor, ps *Params, tsr bool) {\n\tvar (\n\t\tcn          = n    // current node\n\t\tsearch      = path // current path\n\t\tsearchIndex = 0\n\t\tparamIndex  int\n\t)\n\n\tbacktrackToNextNodeType := func(fromNodeType nodeType) (nextNodeType nodeType, valid bool) {\n\t\tprevious := cn\n\t\tcn = previous.parent\n\t\tvalid = cn != nil\n\n\t\t// Next node type by priority\n\t\tif previous.nType == catchAll {\n\t\t\tnextNodeType = static\n\t\t} else {\n\t\t\tnextNodeType = previous.nType + 1\n\t\t}\n\n\t\tif fromNodeType == static {\n\t\t\t// when backtracking is done from static type block we did not change search so nothing to restore\n\t\t\treturn\n\t\t}\n\n\t\t// restore search to value it was before we move to current node we are backtracking from.\n\t\tif previous.nType == static {\n\t\t\tsearchIndex -= len(previous.prefix)\n\t\t} else {\n\t\t\tparamIndex--\n\t\t\t// for param/any node.prefix value is always `:`/`*` so we cannot deduce searchIndex from that and must use pValue\n\t\t\t// for that index as it would also contain part of path we cut off before moving into node we are backtracking from\n\t\t\tsearchIndex -= len(ps.params[paramIndex].Value)\n\t\t\tps.params = ps.params[:paramIndex]\n\t\t}\n\t\tsearch = path[searchIndex:]\n\t\treturn\n\t}\n\n\t// search order: static > param > any\n\tfor {\n\t\tif cn.nType == static {\n\t\t\tif len(search) >= len(cn.prefix) && cn.prefix == search[:len(cn.prefix)] {\n\t\t\t\t// Continue search\n\t\t\t\tsearch = search[len(cn.prefix):]\n\t\t\t\tsearchIndex = searchIndex + len(cn.prefix)\n\t\t\t} else {\n\t\t\t\t// not equal\n\t\t\t\tif (len(cn.prefix) == len(search)+1) &&\n\t\t\t\t\t(cn.prefix[len(search)]) == '/' && cn.prefix[:len(search)] == search && (cn.function != nil || cn.anyChild != nil) {\n\t\t\t\t\ttsr = true\n\t\t\t\t}\n\t\t\t\t// No matching prefix, let's backtrack to the first possible alternative node of the decision path\n\t\t\t\tnk, ok := backtrackToNextNodeType(static)\n\t\t\t\tif !ok {\n\t\t\t\t\treturn // No other possibilities on the decision path\n\t\t\t\t} else if nk == param {\n\t\t\t\t\tgoto Param\n\t\t\t\t} else {\n\t\t\t\t\t// Not found (this should never be possible for static node we are looking currently)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif search == nilString && cn.function != nil {\n\t\t\tfunction = cn.function\n\t\t\tbreak\n\t\t}\n\n\t\t// Static node\n\t\tif search != nilString {\n\t\t\t// If it can execute that logic, there is handler registered on the current node and search is `/`.\n\t\t\tif search == \"/\" && cn.function != nil {\n\t\t\t\ttsr = true\n\t\t\t}\n\t\t\tif child := cn.findChild(search[0]); child != nil {\n\t\t\t\tcn = child\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tif search == nilString {\n\t\t\tif cd := cn.findChild('/'); cd != nil && (cd.function != nil || cd.anyChild != nil) {\n\t\t\t\ttsr = true\n\t\t\t}\n\t\t}\n\n\tParam:\n\t\t// Param node\n\t\tif child := cn.paramChild; search != nilString && child != nil {\n\t\t\tcn = child\n\t\t\ti := strings.Index(search, slash)\n\t\t\tif i == -1 {\n\t\t\t\ti = len(search)\n\t\t\t}\n\t\t\tif ps == nil {\n\t\t\t\tps = params()\n\t\t\t}\n\t\t\tval := search[:i]\n\t\t\tif unescape {\n\t\t\t\tif v, err := url.QueryUnescape(val); err == nil {\n\t\t\t\t\tval = v\n\t\t\t\t}\n\t\t\t}\n\t\t\tps.params = ps.params[:paramIndex+1]\n\t\t\tps.params[paramIndex].Value = val\n\t\t\tparamIndex++\n\t\t\tsearch = search[i:]\n\t\t\tsearchIndex = searchIndex + i\n\t\t\tif search == nilString {\n\t\t\t\tif cd := cn.findChild('/'); cd != nil && (cd.function != nil || cd.anyChild != nil) {\n\t\t\t\t\ttsr = true\n\t\t\t\t}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\tAny:\n\t\t// Any node\n\t\tif child := cn.anyChild; child != nil {\n\t\t\t// If any node is found, use remaining path for paramValues\n\t\t\tcn = child\n\t\t\tif ps == nil {\n\t\t\t\tps = params()\n\t\t\t}\n\t\t\tindex := len(cn.pnames) - 1\n\t\t\tval := search\n\t\t\tif unescape {\n\t\t\t\tif v, err := url.QueryUnescape(val); err == nil {\n\t\t\t\t\tval = v\n\t\t\t\t}\n\t\t\t}\n\t\t\tps.params = ps.params[:paramIndex+1]\n\t\t\tps.params[index].Value = val\n\t\t\t// update indexes/search in case we need to backtrack when no handler match is found\n\t\t\tparamIndex++\n\t\t\tsearchIndex += len(search)\n\t\t\tsearch = nilString\n\t\t\tfunction = cn.function\n\t\t\tbreak\n\t\t}\n\n\t\t// Let's backtrack to the first possible alternative node of the decision path\n\t\tnk, ok := backtrackToNextNodeType(catchAll)\n\t\tif !ok {\n\t\t\tbreak // No other possibilities on the decision path\n\t\t} else if nk == param {\n\t\t\tgoto Param\n\t\t} else if nk == catchAll {\n\t\t\tgoto Any\n\t\t} else {\n\t\t\t// Not found\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif cn != nil {\n\t\tfor i, name := range cn.pnames {\n\t\t\tps.params[i].Key = name\n\t\t}\n\t}\n\n\treturn\n}\n\nfunc (n *node) findChild(l byte) *node {\n\tfor _, c := range n.children {\n\t\tif c.label == l {\n\t\t\treturn c\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (n *node) findChildWithLabel(l byte) *node {\n\tfor _, c := range n.children {\n\t\tif c.label == l {\n\t\t\treturn c\n\t\t}\n\t}\n\tif l == paramLabel {\n\t\treturn n.paramChild\n\t}\n\tif l == anyLabel {\n\t\treturn n.anyChild\n\t}\n\treturn nil\n}\n\nfunc newNode(t nodeType, pre string, p *node, child children, f *FunctionDescriptor, ppath string, pnames []string, paramChildren, anyChildren *node) *node {\n\treturn &node{\n\t\tnType:      t,\n\t\tlabel:      pre[0],\n\t\tprefix:     pre,\n\t\tparent:     p,\n\t\tchildren:   child,\n\t\tppath:      ppath,\n\t\tpnames:     pnames,\n\t\tfunction:   f,\n\t\tparamChild: paramChildren,\n\t\tanyChild:   anyChildren,\n\t\tisLeaf:     child == nil && paramChildren == nil && anyChildren == nil,\n\t}\n}\n\n```\n# Function Signature: func (n *node) addRoute(path string, function *FunctionDescriptor) {\n# Natural Language Annotations: // addRoute adds a node with the given function to the path.\n// Not concurrency-safe!\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (n *node) addRoute(path string, function *FunctionDescriptor) {\n\tcurrentNode := n\n\tsearch := path\n\n\tfor {\n\t\tsearchLen := len(search)\n\t\tprefixLen := len(currentNode.prefix)\n\t\tlcpLen := 0\n\n\t\tmax := prefixLen\n\t\tif searchLen < max {\n\t\t\tmax = searchLen\n\t\t}\n\t\tfor ; lcpLen < max && search[lcpLen] == currentNode.prefix[lcpLen]; lcpLen++ {\n\t\t}\n\n\t\tif lcpLen == 0 {\n\t\t\tcurrentNode.label = search[0]\n\t\t\tcurrentNode.prefix = search\n\t\t\tif function != nil {\n\t\t\t\tcurrentNode.nType = static\n\t\t\t\tcurrentNode.function = function\n\t\t\t}\n\t\t\tcurrentNode.isLeaf = currentNode.children == nil && currentNode.paramChild == nil && currentNode.anyChild == nil\n\t\t} else if lcpLen < prefixLen {\n\t\t\t// Split node\n\t\t\tn := newNode(\n\t\t\t\tcurrentNode.nType,\n\t\t\t\tcurrentNode.prefix[lcpLen:],\n\t\t\t\tcurrentNode,\n\t\t\t\tcurrentNode.children,\n\t\t\t\tcurrentNode.function,\n\t\t\t\tcurrentNode.ppath,\n\t\t\t\tcurrentNode.pnames,\n\t\t\t\tcurrentNode.paramChild,\n\t\t\t\tcurrentNode.anyChild,\n\t\t\t)\n\t\t\t// Update parent path for all children to new node\n\t\t\tfor _, child := range currentNode.children {\n\t\t\t\tchild.parent = n\n\t\t\t}\n\t\t\tif currentNode.paramChild != nil {\n\t\t\t\tcurrentNode.paramChild.parent = n\n\t\t\t}\n\t\t\tif currentNode.anyChild != nil {\n\t\t\t\tcurrentNode.anyChild.parent = n\n\t\t\t}\n\n\t\t\t// Reset parent node\n\t\t\tcurrentNode.nType = static\n\t\t\tcurrentNode.label = currentNode.prefix[0]\n\t\t\tcurrentNode.prefix = currentNode.prefix[:lcpLen]\n\t\t\tcurrentNode.children = nil\n\t\t\tcurrentNode.function = nil\n\t\t\tcurrentNode.ppath = nilString\n\t\t\tcurrentNode.pnames = nil\n\t\t\tcurrentNode.paramChild = nil\n\t\t\tcurrentNode.anyChild = nil\n\t\t\tcurrentNode.isLeaf = false\n\n\t\t\t// Only Static children could reach here\n\t\t\tcurrentNode.children = append(currentNode.children, n)\n\n\t\t\tif lcpLen == searchLen {\n\t\t\t\t// At parent node\n\t\t\t\tcurrentNode.nType = static\n\t\t\t\tcurrentNode.function = function\n\t\t\t} else {\n\t\t\t\t// Create child node\n\t\t\t\tn = newNode(static, search[lcpLen:], currentNode, nil, function, \"\", nil, nil, nil)\n\t\t\t\t// Only Static children could reach here\n\t\t\t\tcurrentNode.children = append(currentNode.children, n)\n\t\t\t}\n\t\t\tcurrentNode.isLeaf = currentNode.children == nil && currentNode.paramChild == nil && currentNode.anyChild == nil\n\t\t} else if lcpLen < searchLen {\n\t\t\tsearch = search[lcpLen:]\n\t\t\tc := currentNode.findChildWithLabel(search[0])\n\t\t\tif c != nil {\n\t\t\t\t// Go deeper\n\t\t\t\tcurrentNode = c\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Create child node\n\t\t\tn := newNode(static, search, currentNode, nil, function, \"\", nil, nil, nil)\n\t\t\tswitch t {\n\t\t\tcase static:\n\t\t\t\tcurrentNode.children = append(currentNode.children, n)\n\t\t\tcase param:\n\t\t\t\tcurrentNode.paramChild = n\n\t\t\tcase catchAll:\n\t\t\t\tcurrentNode.anyChild = n\n\t\t\t}\n\t\t\tcurrentNode.isLeaf = currentNode.children == nil && currentNode.paramChild == nil && current", "  ```\nfunc (n *node) addRoute(path string, function *FunctionDescriptor) {\n\tcurrentNode := n\n\tsearch := path\n\n\tfor {\n\t\tsearchLen := len(search)\n\t\tprefixLen := len(currentNode.prefix)\n\t\tlcpLen := 0\n\n\t\tmax := prefixLen\n\t\tif searchLen < max {\n\t\t\tmax = searchLen\n\t\t}\n\t\tfor ; lcpLen < max && search[lcpLen] == currentNode.prefix[lcpLen]; lcpLen++ {}\n\n\t\tif lcpLen == 0 {\n\t\t\tcurrentNode.label = search[0]\n\t\t\tcurrentNode.prefix = search\n\t\t\tif function != nil {\n\t\t\t\tcurrentNode.nType = static\n\t\t\t\tcurrentNode.function = function\n\t\t\t\tcurrentNode.ppath = ppath\n\t\t\t\tcurrentNode.pnames = pnames\n\t\t\t}\n\t\t\tcurrentNode.isLeaf = currentNode.children == nil && currentNode.paramChild == nil && currentNode.anyChild == nil\n\t\t} else if lcpLen < prefixLen {\n\t\t\t// Split node\n\t\t\tn := newNode(\n\t\t\t\tcurrentNode.nType,\n\t\t\t\tcurrentNode.prefix[lcpLen:],\n\t\t\t\tcurrentNode,\n\t\t\t\tcurrentNode.children,\n\t\t\t\tcurrentNode.function,\n\t\t\t\tcurrentNode.ppath,\n\t\t\t\tcurrentNode.pnames,\n\t\t\t\tcurrentNode.paramChild,\n\t\t\t\tcurrentNode.anyChild,\n\t\t\t)\n\t\t\t// Update parent path for all children to new node\n\t\t\tfor _, child := range currentNode.children {\n\t\t\t\tchild.parent = n\n\t\t\t}\n\t\t\tif currentNode.paramChild != nil {\n\t\t\t\tcurrentNode.paramChild.parent = n\n\t\t\t}\n\t\t\tif currentNode.anyChild != nil {\n\t\t\t\tcurrentNode.anyChild.parent = n\n\t\t\t}\n\n\t\t\t// Reset parent node\n\t\t\tcurrentNode.nType = static\n\t\t\tcurrentNode.label = currentNode.prefix[0]\n\t\t\tcurrentNode.prefix = currentNode.prefix[:lcpLen]\n\t\t\tcurrentNode.children = nil\n\t\t\tcurrentNode.function = nil\n\t\t\tcurrentNode.ppath = nilString\n\t\t\tcurrentNode.pnames = nil\n\t\t\tcurrentNode.paramChild = nil\n\t\t\tcurrentNode.anyChild = nil\n\t\t\tcurrentNode.isLeaf = false\n\n\t\t\t// Only Static children could reach here\n\t\t\tcurrentNode.children = append(currentNode.children, n)\n\n\t\t\tif lcpLen == searchLen {\n\t\t\t\t// At parent node\n\t\t\t\tcurrentNode.nType = static\n\t\t\t\tcurrentNode.function = function\n\t\t\t\tcurrentNode.ppath = ppath\n\t\t\t\tcurrentNode.pnames = pnames\n\t\t\t} else {\n\t\t\t\t// Create child node\n\t\t\t\tn := newNode(static, search[lcpLen:], currentNode, nil, function, ppath, pnames, nil, nil)\n\t\t\t\t// Only Static children could reach here\n\t\t\t\tcurrentNode.children = append(currentNode.children, n)\n\t\t\t}\n\t\t\tcurrentNode.isLeaf = currentNode.children == nil && currentNode.paramChild == nil && currentNode.anyChild == nil\n\t\t} else if lcpLen < searchLen {\n\t\t\tsearch = search[lcpLen:]\n\t\t\tc := currentNode.findChildWithLabel(search[0])\n\t\t\tif c != nil {\n\t\t\t\t// Go deeper\n\t\t\t\tcurrentNode = c\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Create child node\n\t\t\tn := newNode(static, search, currentNode, nil, function, ppath, pnames, nil, nil)\n\t\t\tswitch t {\n\t\t\tcase static:\n\t\t\t\tcurrentNode.children = append(currentNode.children, n)\n\t\t\tcase param:\n\t\t\t\tcurrentNode.paramChild", "  ```\nfunc (n *node) addRoute(path string, function *FunctionDescriptor) {\n\tcurrentNode := n\n\tsearch := path\n\n\tfor {\n\t\tsearchLen := len(search)\n\t\tprefixLen := len(currentNode.prefix)\n\t\tlcpLen := 0\n\n\t\tmax := prefixLen\n\t\tif searchLen < max {\n\t\t\tmax = searchLen\n\t\t}\n\t\tfor ; lcpLen < max && search[lcpLen] == currentNode.prefix[lcpLen]; lcpLen++ {\n\t\t}\n\n\t\tif lcpLen == 0 {\n\t\t\tcurrentNode.label = search[0]\n\t\t\tcurrentNode.prefix = search\n\t\t\tcurrentNode.nType = static\n\t\t\tcurrentNode.function = function\n\t\t\tcurrentNode.ppath = path\n\t\t\tcurrentNode.isLeaf = currentNode.children == nil && currentNode.paramChild == nil && currentNode.anyChild == nil\n\t\t} else if lcpLen < prefixLen {\n\t\t\t// Split node\n\t\t\tn := &node{\n\t\t\t\tnType:      currentNode.nType,\n\t\t\t\tprefix:     currentNode.prefix[lcpLen:],\n\t\t\t\tparent:     currentNode,\n\t\t\t\tchildren:   currentNode.children,\n\t\t\t\tfunction:   currentNode.function,\n\t\t\t\tppath:      currentNode.ppath,\n\t\t\t\tpnames:     currentNode.pnames,\n\t\t\t\tparamChild: currentNode.paramChild,\n\t\t\t\tanyChild:   currentNode.anyChild,\n\t\t\t}\n\t\t\t// Update parent path for all children to new node\n\t\t\tfor _, child := range currentNode.children {\n\t\t\t\tchild.parent = n\n\t\t\t}\n\t\t\tif currentNode.paramChild != nil {\n\t\t\t\tcurrentNode.paramChild.parent = n\n\t\t\t}\n\t\t\tif currentNode.anyChild != nil {\n\t\t\t\tcurrentNode.anyChild.parent = n\n\t\t\t}\n\n\t\t\t// Reset parent node\n\t\t\tcurrentNode.nType = static\n\t\t\tcurrentNode.label = currentNode.prefix[0]\n\t\t\tcurrentNode.prefix = currentNode.prefix[:lcpLen]\n\t\t\tcurrentNode.children = nil\n\t\t\tcurrentNode.function = nil\n\t\t\tcurrentNode.ppath = nilString\n\t\t\tcurrentNode.pnames = nil\n\t\t\tcurrentNode.paramChild = nil\n\t\t\tcurrentNode.anyChild = nil\n\t\t\tcurrentNode.isLeaf = false\n\n\t\t\t// Only Static children could reach here\n\t\t\tcurrentNode.children = append(currentNode.children, n)\n\n\t\t\tif lcpLen == searchLen {\n\t\t\t\t// At parent node\n\t\t\t\tcurrentNode.nType = static\n\t\t\t\tcurrentNode.function = function\n\t\t\t\tcurrentNode.ppath = path\n\t\t\t} else {\n\t\t\t\t// Create child node\n\t\t\t\tn := newNode(\n\t\t\t\t\tstatic,\n\t\t\t\t\tsearch[lcpLen:],\n\t\t\t\t\tcurrentNode,\n\t\t\t\t\tnil,\n\t\t\t\t\tfunction,\n\t\t\t\t\tpath,\n\t\t\t\t\tnil,\n\t\t\t\t\tnil,\n\t\t\t\t\tnil)\n\t\t\t\t\t\t// Only Static children could reach here\n\t\t\t\t\t\tcurrentNode.children = append(currentNode.children, n)\n\t\t\t\t}\n\t\t\t\tcurrentNode.isLeaf = currentNode.children == nil && currentNode.paramChild == nil && currentNode.anyChild == nil\n\n\t\t\t} else if lcpLen < searchLen {\n\t\t\t\tsearch = search[lcpLen:]\n\t\t\t\tc := currentNode.findChildWithLabel(search[0])\n\t\t\t\tif c != nil {\n\t\t\t\t\t// Go deeper\n\t\t\t\t\tcurrentNode = c\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\t// Create child node\n\t\t\t\tn := newNode(\n\t\t\t\t\tstatic,\n\t\t\t"]}, "kitex-pkg/mem/span.go-NewSpanCache": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/mem/span.go:\n```\n/*\n * Copyright 2024 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage mem\n\nimport (\n\t\"math/bits\"\n\t\"sync/atomic\"\n\n\t\"github.com/bytedance/gopkg/lang/dirtmake\"\n)\n\n/* Span Cache: A thread-safe linear allocator\n\nDesign:\n1. [GC Friendly]: Centralize a batch of small bytes slice into a big size bytes slice to avoid malloc too many objects\n2. [Thread Safe]: Multi thread may share a same span, but it should fall back to the native allocator if lock conflict\n*/\n\nconst (\n\tspanCacheSize = 10\n\tminSpanObject = 128                                  // 128 B\n\tmaxSpanObject = (minSpanObject << spanCacheSize) - 1 // 128 KB\n\tminSpanClass  = 8                                    // = spanClass(minSpanObject)\n)\n\ntype spanCache struct {\n\tspans [spanCacheSize]*span\n}\n\n// NewSpanCache returns a spanCache with the given spanSize,\n// each span is used to allocate a binary of a specific size level.\n\n\n\n\n\n\n\n\n// Make returns a [:n:n] bytes slice from a cached buffer\n// NOTE: Make will not clear the underlay bytes for performance concern. So caller MUST set every byte before read.\nfunc (c *spanCache) Make(n int) []byte {\n\tsclass := spanClass(n) - minSpanClass\n\tif sclass < 0 || sclass >= len(c.spans) {\n\t\treturn dirtmake.Bytes(n, n)\n\t}\n\treturn c.spans[sclass].Make(n)\n}\n\n// Copy is an alias function for make-and-copy pattern\nfunc (c *spanCache) Copy(buf []byte) (p []byte) {\n\tp = c.Make(len(buf))\n\tcopy(p, buf)\n\treturn p\n}\n\n// NewSpan returns a span with given size\nfunc NewSpan(size int) *span {\n\tsp := new(span)\n\tsp.size = uint32(size)\n\tsp.buffer = dirtmake.Bytes(0, size)\n\treturn sp\n}\n\ntype span struct {\n\tlock   uint32\n\tread   uint32 // read index of buffer\n\tsize   uint32 // size of buffer\n\tbuffer []byte\n}\n\n// Make returns a [:n:n] bytes slice from a cached buffer\n// NOTE: Make will not clear the underlay bytes for performance concern. So caller MUST set every byte before read.\nfunc (b *span) Make(_n int) []byte {\n\tn := uint32(_n)\n\tif n >= b.size || !atomic.CompareAndSwapUint32(&b.lock, 0, 1) {\n\t\t// fallback path: make a new byte slice if current goroutine cannot get the lock or n is out of size\n\t\treturn dirtmake.Bytes(int(n), int(n))\n\t}\nSTART:\n\tb.read += n\n\t// fast path\n\tif b.read <= b.size {\n\t\tbuf := b.buffer[b.read-n : b.read : b.read]\n\t\tatomic.StoreUint32(&b.lock, 0)\n\t\treturn buf\n\t}\n\t// slow path: create a new buffer\n\tb.buffer = dirtmake.Bytes(int(b.size), int(b.size))\n\tb.read = 0\n\tgoto START\n}\n\n// Copy is an alias function for make-and-copy pattern\nfunc (b *span) Copy(buf []byte) (p []byte) {\n\tp = b.Make(len(buf))\n\tcopy(p, buf)\n\treturn p\n}\n\n// spanClass calc the minimum number of bits required to represent x\n// [2^sclass,2^(sclass+1)) bytes in a same span class\nfunc spanClass(size int) int {\n\tif size == 0 {\n\t\treturn 0\n\t}\n\treturn bits.Len(uint(size))\n}\n\n```\n# Function Signature: func NewSpanCache(spanSize int) *spanCache {\n# Natural Language Annotations: // NewSpanCache returns a spanCache with the given spanSize,\n// each span is used to allocate a binary of a specific size level.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewSpanCache(spanSize int) *spanCache {\n\tsc := new(spanCache)\n\tfor i := 0; i < len(sc.spans); i++ {\n\t\tsc.spans[i] = NewSpan(spanSize * (1 << i))\n\t}\n\treturn sc\n}\n```", "  ```\nfunc NewSpanCache(spanSize int) *spanCache {\n\tsc := new(spanCache)\n\tsc.spans = make([]*span, spanCacheSize)\n\tfor i := range sc.spans {\n\t\tsc.spans[i] = NewSpan(spanClass(minSpanObject << i))\n\t}\n\treturn sc\n}\n```", "  ```\npackage mem\n\n// NewSpanCache returns a spanCache with the given spanSize,\n// each span is used to allocate a binary of a specific size level.\nfunc NewSpanCache(spanSize int) *spanCache {\n\tc := new(spanCache)\n\tfor i := 1; i <= spanCacheSize; i++ {\n\t\tc.spans[i] = &span{size: uint32(spanSize << uint(i))}\n\t}\n\treturn c\n}\n```"]}, "kitex-pkg/profiler/profiler.go-Pause": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/profiler/profiler.go:\n```\n/*\n * Copyright 2022 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage profiler\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"runtime/pprof\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/google/pprof/profile\"\n\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n)\n\ntype profilerContextKey struct{}\n\ntype Profiler interface {\n\tRun(ctx context.Context) (err error)\n\tStop()\n\tPause()\n\tResume()\n\tPrepare(ctx context.Context) context.Context\n\tTag(ctx context.Context, tags ...string) context.Context\n\tUntag(ctx context.Context)\n\tLookup(ctx context.Context, key string) (string, bool)\n}\n\ntype Processor func(profiles []*TagsProfile) error\n\nfunc LogProcessor(profiles []*TagsProfile) error {\n\tif len(profiles) == 0 {\n\t\treturn nil\n\t}\n\tklog.Infof(\"KITEX: profiler collect %d records\", len(profiles))\n\tfor _, p := range profiles {\n\t\tif p.Key != \"\" {\n\t\t\tklog.Infof(\"KITEX: profiler - %s %.2f%%\", p.Key, p.Percent*100)\n\t\t} else {\n\t\t\tklog.Infof(\"KITEX: profiler - type=default %.2f%%\", p.Percent*100)\n\t\t}\n\t}\n\tklog.Info(\"---------------------------------\")\n\treturn nil\n}\n\nfunc NewProfiler(processor Processor, interval, window time.Duration) *profiler {\n\tif processor == nil {\n\t\tprocessor = LogProcessor\n\t}\n\treturn &profiler{\n\t\tstateCond: sync.NewCond(&sync.Mutex{}),\n\t\tprocessor: processor,\n\t\tinterval:  interval,\n\t\twindow:    window,\n\t}\n}\n\nvar _ Profiler = (*profiler)(nil)\n\nconst (\n\t// state changes:\n\t//   running => pausing => paused => resuming => running\n\t//           => stopped\n\tstateRunning  = 0\n\tstatePausing  = 1\n\tstatePaused   = 2\n\tstateResuming = 3\n\tstateStopped  = 4\n)\n\ntype profiler struct {\n\tdata      bytes.Buffer // protobuf\n\tstate     int\n\tstateCond *sync.Cond\n\t// settings\n\tprocessor Processor\n\tinterval  time.Duration // sleep time between every profiling window\n\twindow    time.Duration // profiling in the window, go pprof collect stack profile every 10ms\n}\n\n// Tag current goroutine with tagged ctx\n// it's used for reuse goroutine scenario\nfunc Tag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok {\n\t\tpc.profiler.Tag(ctx)\n\t}\n}\n\n// Untag current goroutine with tagged ctx\n// it's used for reuse goroutine scenario\nfunc Untag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok {\n\t\tpc.profiler.Untag(ctx)\n\t}\n}\n\ntype profilerContext struct {\n\tprofiler Profiler\n\tuntagCtx context.Context\n\ttags     []string\n}\n\nfunc newProfilerContext(profiler Profiler) *profilerContext {\n\treturn &profilerContext{\n\t\tprofiler: profiler,\n\t\ttags:     make([]string, 0, 12),\n\t}\n}\n\n// Prepare the profiler context\nfunc (p *profiler) Prepare(ctx context.Context) context.Context {\n\tif c := ctx.Value(profilerContextKey{}); c != nil {\n\t\treturn ctx\n\t}\n\treturn context.WithValue(ctx, profilerContextKey{}, newProfilerContext(p))\n}\n\n// State return current profiler state\nfunc (p *profiler) State() (state int) {\n\tp.stateCond.L.Lock()\n\tstate = p.state\n\tp.stateCond.L.Unlock()\n\treturn state\n}\n\n// Stop the profiler\nfunc (p *profiler) Stop() {\n\tif p.State() == stateStopped {\n\t\treturn\n\t}\n\t// stateRunning => stateStopped\n\tp.stateChange(stateRunning, stateStopped)\n}\n\n// Pause the profiler.\n// The profiler has been paused when Pause() return\n\n\n\n\n\n\n\n\n\n\n// Resume the profiler.\n// The profiler has been resumed when Resume() return\n\n\n\n\n\n\n\n\n\n\n// Run start analyse the pprof data with interval and window settings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Tag current goroutine with tags\n// If ctx already tagged, append the existed tags\nfunc (p *profiler) Tag(ctx context.Context, tags ...string) context.Context {\n\tpctx, ok := ctx.Value(profilerContextKey{}).(*profilerContext)\n\tif !ok {\n\t\tpctx = newProfilerContext(p)\n\t\tctx = context.WithValue(ctx, profilerContextKey{}, pctx)\n\t}\n\tif pctx.untagCtx == nil {\n\t\tpctx.untagCtx = ctx\n\t}\n\tpctx.tags = append(pctx.tags, tags...)\n\t// do not return pprof ctx\n\tpprof.SetGoroutineLabels(pprof.WithLabels(context.Background(), pprof.Labels(pctx.tags...)))\n\treturn ctx\n}\n\n// Untag current goroutine\n// Only untag if ctx already tagged, will not clear the goroutine labels if not tagged by profiler\nfunc (p *profiler) Untag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok && pc.untagCtx != nil {\n\t\t// if ctx have untagCtx, that means the current goroutine created by a tagged goroutine\n\t\t// we need to untag the goroutine when finished\n\t\t// else, do nothing\n\t\tpprof.SetGoroutineLabels(pc.untagCtx)\n\t}\n}\n\nfunc (p *profiler) Lookup(ctx context.Context, key string) (string, bool) {\n\treturn pprof.Label(ctx, key)\n}\n\nfunc (p *profiler) stateChange(from, to int) {\n\tp.stateCond.L.Lock()\n\tfor p.state != from { // wait state to from first\n\t\tp.stateCond.Wait()\n\t}\n\tp.state = to\n\tp.stateCond.L.Unlock()\n\tp.stateCond.Broadcast()\n}\n\nfunc (p *profiler) stateWait(to int) {\n\tp.stateCond.L.Lock()\n\tfor p.state != to {\n\t\tp.stateCond.Wait()\n\t}\n\tp.stateCond.L.Unlock()\n}\n\nfunc (p *profiler) startProfile() error {\n\tp.data.Reset()\n\treturn pprof.StartCPUProfile(&p.data)\n}\n\nfunc (p *profiler) stopProfile() {\n\tpprof.StopCPUProfile()\n}\n\nfunc (p *profiler) analyse() ([]*TagsProfile, error) {\n\t// parse protobuf data\n\tpf, err := profile.ParseData(p.data.Bytes())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// filter cpu value index\n\tsampleIdx := -1\n\tfor idx, st := range pf.SampleType {\n\t\tif st.Type == \"cpu\" {\n\t\t\tsampleIdx = idx\n\t\t\tbreak\n\t\t}\n\t}\n\tif sampleIdx < 0 {\n\t\treturn nil, errors.New(\"profiler: sample type not found\")\n\t}\n\n\t// calculate every sample expense\n\tcounter := map[string]*TagsProfile{} // map[tagsKey]funcProfile\n\tvar total int64\n\tfor _, sm := range pf.Sample {\n\t\tvalue := sm.Value[sampleIdx]\n\t\ttags := labelToTags(sm.Label)\n\t\ttagsKey := tagsToKey(tags)\n\t\ttp, ok := counter[tagsKey]\n\t\tif !ok {\n\t\t\ttp = &TagsProfile{}\n\t\t\tcounter[tagsKey] = tp\n\t\t\ttp.Key = tagsKey\n\t\t\ttp.Tags = tags\n\t\t}\n\t\ttp.Value += value\n\t\ttotal += value\n\t}\n\n\t// compensate value with duration date\n\tdurationRate := float64((p.interval + p.window) / p.window)\n\tprofiles := make([]*TagsProfile, 0, len(counter)) // flat to array\n\tfor _, l := range counter {\n\t\tl.Percent = float64(l.Value) / float64(total)\n\t\tl.Value = int64(durationRate * float64(l.Value))\n\t\tprofiles = append(profiles, l)\n\t}\n\treturn profiles, nil\n}\n\n// TagsProfile is the stats result group by tag key\ntype TagsProfile struct {\n\tKey     string   // eg: a=1,b=2\n\tTags    []string // eg: [\"a\", \"1\", \"b\", \"2\"]\n\tValue   int64    // pprof cpu times\n\tPercent float64  // <= 1.0\n}\n\nfunc labelToTags(label map[string][]string) []string {\n\ttags := make([]string, 0, len(label)*2)\n\tfor k, v := range label {\n\t\ttags = append(tags, k, strings.Join(v, \",\"))\n\t}\n\treturn tags\n}\n\nfunc tagsToKey(tags []string) string {\n\tif len(tags)%2 != 0 {\n\t\treturn \"\"\n\t}\n\ttagsPair := make([]string, 0, len(tags)/2)\n\tfor i := 0; i < len(tags); i += 2 {\n\t\ttagsPair = append(tagsPair, fmt.Sprintf(\"%s=%s\", tags[i], tags[i+1]))\n\t}\n\t// sort tags to make it a unique key\n\tsort.Strings(tagsPair)\n\treturn strings.Join(tagsPair, \"|\")\n}\n\n```\n# Function Signature: func (p *profiler) Pause() {\n# Natural Language Annotations: // Pause the profiler.\n// The profiler has been paused when Pause() return\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (p *profiler) Pause() {\n    // stateRunning => statePausing\n    p.stateChange(stateRunning, statePausing)\n    // stop pprof when state is paused\n    p.stopProfile()\n    // statePausing => statePaused\n    p.stateChange(statePausing, statePaused)\n}\n```", "  ```\nfunc (p *profiler) Pause() {\n\t// stateRunning => statePausing\n\tp.stateChange(stateRunning, statePausing)\n\t// wait the state to paused\n\tp.stateWait(statePaused)\n\t// profiling stopped\n\tp.stopProfile()\n}\n```", "  ```\nfunc (p *profiler) Pause() {\n\tif p.State() == stateRunning {\n\t\t// from stateRunning => statePausing\n\t\tp.stateChange(stateRunning, statePausing)\n\t}\n\t// do not return until the profiler state changed to stateStopped\n\tp.stateWait(stateStopped)\n}\n```"]}, "kitex-pkg/profiler/profiler.go-Resume": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/profiler/profiler.go:\n```\n/*\n * Copyright 2022 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage profiler\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"runtime/pprof\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/google/pprof/profile\"\n\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n)\n\ntype profilerContextKey struct{}\n\ntype Profiler interface {\n\tRun(ctx context.Context) (err error)\n\tStop()\n\tPause()\n\tResume()\n\tPrepare(ctx context.Context) context.Context\n\tTag(ctx context.Context, tags ...string) context.Context\n\tUntag(ctx context.Context)\n\tLookup(ctx context.Context, key string) (string, bool)\n}\n\ntype Processor func(profiles []*TagsProfile) error\n\nfunc LogProcessor(profiles []*TagsProfile) error {\n\tif len(profiles) == 0 {\n\t\treturn nil\n\t}\n\tklog.Infof(\"KITEX: profiler collect %d records\", len(profiles))\n\tfor _, p := range profiles {\n\t\tif p.Key != \"\" {\n\t\t\tklog.Infof(\"KITEX: profiler - %s %.2f%%\", p.Key, p.Percent*100)\n\t\t} else {\n\t\t\tklog.Infof(\"KITEX: profiler - type=default %.2f%%\", p.Percent*100)\n\t\t}\n\t}\n\tklog.Info(\"---------------------------------\")\n\treturn nil\n}\n\nfunc NewProfiler(processor Processor, interval, window time.Duration) *profiler {\n\tif processor == nil {\n\t\tprocessor = LogProcessor\n\t}\n\treturn &profiler{\n\t\tstateCond: sync.NewCond(&sync.Mutex{}),\n\t\tprocessor: processor,\n\t\tinterval:  interval,\n\t\twindow:    window,\n\t}\n}\n\nvar _ Profiler = (*profiler)(nil)\n\nconst (\n\t// state changes:\n\t//   running => pausing => paused => resuming => running\n\t//           => stopped\n\tstateRunning  = 0\n\tstatePausing  = 1\n\tstatePaused   = 2\n\tstateResuming = 3\n\tstateStopped  = 4\n)\n\ntype profiler struct {\n\tdata      bytes.Buffer // protobuf\n\tstate     int\n\tstateCond *sync.Cond\n\t// settings\n\tprocessor Processor\n\tinterval  time.Duration // sleep time between every profiling window\n\twindow    time.Duration // profiling in the window, go pprof collect stack profile every 10ms\n}\n\n// Tag current goroutine with tagged ctx\n// it's used for reuse goroutine scenario\nfunc Tag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok {\n\t\tpc.profiler.Tag(ctx)\n\t}\n}\n\n// Untag current goroutine with tagged ctx\n// it's used for reuse goroutine scenario\nfunc Untag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok {\n\t\tpc.profiler.Untag(ctx)\n\t}\n}\n\ntype profilerContext struct {\n\tprofiler Profiler\n\tuntagCtx context.Context\n\ttags     []string\n}\n\nfunc newProfilerContext(profiler Profiler) *profilerContext {\n\treturn &profilerContext{\n\t\tprofiler: profiler,\n\t\ttags:     make([]string, 0, 12),\n\t}\n}\n\n// Prepare the profiler context\nfunc (p *profiler) Prepare(ctx context.Context) context.Context {\n\tif c := ctx.Value(profilerContextKey{}); c != nil {\n\t\treturn ctx\n\t}\n\treturn context.WithValue(ctx, profilerContextKey{}, newProfilerContext(p))\n}\n\n// State return current profiler state\nfunc (p *profiler) State() (state int) {\n\tp.stateCond.L.Lock()\n\tstate = p.state\n\tp.stateCond.L.Unlock()\n\treturn state\n}\n\n// Stop the profiler\nfunc (p *profiler) Stop() {\n\tif p.State() == stateStopped {\n\t\treturn\n\t}\n\t// stateRunning => stateStopped\n\tp.stateChange(stateRunning, stateStopped)\n}\n\n// Pause the profiler.\n// The profiler has been paused when Pause() return\n\n\n\n\n\n\n\n\n\n\n// Resume the profiler.\n// The profiler has been resumed when Resume() return\n\n\n\n\n\n\n\n\n\n\n// Run start analyse the pprof data with interval and window settings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Tag current goroutine with tags\n// If ctx already tagged, append the existed tags\nfunc (p *profiler) Tag(ctx context.Context, tags ...string) context.Context {\n\tpctx, ok := ctx.Value(profilerContextKey{}).(*profilerContext)\n\tif !ok {\n\t\tpctx = newProfilerContext(p)\n\t\tctx = context.WithValue(ctx, profilerContextKey{}, pctx)\n\t}\n\tif pctx.untagCtx == nil {\n\t\tpctx.untagCtx = ctx\n\t}\n\tpctx.tags = append(pctx.tags, tags...)\n\t// do not return pprof ctx\n\tpprof.SetGoroutineLabels(pprof.WithLabels(context.Background(), pprof.Labels(pctx.tags...)))\n\treturn ctx\n}\n\n// Untag current goroutine\n// Only untag if ctx already tagged, will not clear the goroutine labels if not tagged by profiler\nfunc (p *profiler) Untag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok && pc.untagCtx != nil {\n\t\t// if ctx have untagCtx, that means the current goroutine created by a tagged goroutine\n\t\t// we need to untag the goroutine when finished\n\t\t// else, do nothing\n\t\tpprof.SetGoroutineLabels(pc.untagCtx)\n\t}\n}\n\nfunc (p *profiler) Lookup(ctx context.Context, key string) (string, bool) {\n\treturn pprof.Label(ctx, key)\n}\n\nfunc (p *profiler) stateChange(from, to int) {\n\tp.stateCond.L.Lock()\n\tfor p.state != from { // wait state to from first\n\t\tp.stateCond.Wait()\n\t}\n\tp.state = to\n\tp.stateCond.L.Unlock()\n\tp.stateCond.Broadcast()\n}\n\nfunc (p *profiler) stateWait(to int) {\n\tp.stateCond.L.Lock()\n\tfor p.state != to {\n\t\tp.stateCond.Wait()\n\t}\n\tp.stateCond.L.Unlock()\n}\n\nfunc (p *profiler) startProfile() error {\n\tp.data.Reset()\n\treturn pprof.StartCPUProfile(&p.data)\n}\n\nfunc (p *profiler) stopProfile() {\n\tpprof.StopCPUProfile()\n}\n\nfunc (p *profiler) analyse() ([]*TagsProfile, error) {\n\t// parse protobuf data\n\tpf, err := profile.ParseData(p.data.Bytes())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// filter cpu value index\n\tsampleIdx := -1\n\tfor idx, st := range pf.SampleType {\n\t\tif st.Type == \"cpu\" {\n\t\t\tsampleIdx = idx\n\t\t\tbreak\n\t\t}\n\t}\n\tif sampleIdx < 0 {\n\t\treturn nil, errors.New(\"profiler: sample type not found\")\n\t}\n\n\t// calculate every sample expense\n\tcounter := map[string]*TagsProfile{} // map[tagsKey]funcProfile\n\tvar total int64\n\tfor _, sm := range pf.Sample {\n\t\tvalue := sm.Value[sampleIdx]\n\t\ttags := labelToTags(sm.Label)\n\t\ttagsKey := tagsToKey(tags)\n\t\ttp, ok := counter[tagsKey]\n\t\tif !ok {\n\t\t\ttp = &TagsProfile{}\n\t\t\tcounter[tagsKey] = tp\n\t\t\ttp.Key = tagsKey\n\t\t\ttp.Tags = tags\n\t\t}\n\t\ttp.Value += value\n\t\ttotal += value\n\t}\n\n\t// compensate value with duration date\n\tdurationRate := float64((p.interval + p.window) / p.window)\n\tprofiles := make([]*TagsProfile, 0, len(counter)) // flat to array\n\tfor _, l := range counter {\n\t\tl.Percent = float64(l.Value) / float64(total)\n\t\tl.Value = int64(durationRate * float64(l.Value))\n\t\tprofiles = append(profiles, l)\n\t}\n\treturn profiles, nil\n}\n\n// TagsProfile is the stats result group by tag key\ntype TagsProfile struct {\n\tKey     string   // eg: a=1,b=2\n\tTags    []string // eg: [\"a\", \"1\", \"b\", \"2\"]\n\tValue   int64    // pprof cpu times\n\tPercent float64  // <= 1.0\n}\n\nfunc labelToTags(label map[string][]string) []string {\n\ttags := make([]string, 0, len(label)*2)\n\tfor k, v := range label {\n\t\ttags = append(tags, k, strings.Join(v, \",\"))\n\t}\n\treturn tags\n}\n\nfunc tagsToKey(tags []string) string {\n\tif len(tags)%2 != 0 {\n\t\treturn \"\"\n\t}\n\ttagsPair := make([]string, 0, len(tags)/2)\n\tfor i := 0; i < len(tags); i += 2 {\n\t\ttagsPair = append(tagsPair, fmt.Sprintf(\"%s=%s\", tags[i], tags[i+1]))\n\t}\n\t// sort tags to make it a unique key\n\tsort.Strings(tagsPair)\n\treturn strings.Join(tagsPair, \"|\")\n}\n\n```\n# Function Signature: func (p *profiler) Resume() {\n# Natural Language Annotations: // Resume the profiler.\n// The profiler has been resumed when Resume() return\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (p *profiler) Resume() {\n\tif p.State() == stateResuming {\n\t\treturn\n\t}\n\t// statePausing => stateResuming\n\tp.stateChange(statePausing, stateResuming)\n}\n```", "  ```\nfunc (p *profiler) Resume() {\n\tp.stateChange(statePaused, stateResuming)\n\tp.stopProfile() // stop current collect\n\t// wait for running state\n\tp.stateWait(stateRunning)\n}\n```", "  ```\nfunc (p *profiler) Resume() {\n\tif p.State() != stateStopped {\n\t\treturn\n\t}\n\t// stateStopped => statePausing\n\tp.stateChange(stateStopped, statePausing)\n\t// start cpu profile\n\terr := p.startProfile()\n\tif err != nil {\n\t\tklog.Errorf(\"KITEX: profiler resume error %v, resume failed\", err)\n\t\treturn\n\t}\n\t// statePausing => stateRunning\n\tp.stateChange(statePausing, stateRunning)\n}\n```"]}, "kitex-pkg/profiler/profiler.go-Run": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/profiler/profiler.go:\n```\n/*\n * Copyright 2022 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage profiler\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"runtime/pprof\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/google/pprof/profile\"\n\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n)\n\ntype profilerContextKey struct{}\n\ntype Profiler interface {\n\tRun(ctx context.Context) (err error)\n\tStop()\n\tPause()\n\tResume()\n\tPrepare(ctx context.Context) context.Context\n\tTag(ctx context.Context, tags ...string) context.Context\n\tUntag(ctx context.Context)\n\tLookup(ctx context.Context, key string) (string, bool)\n}\n\ntype Processor func(profiles []*TagsProfile) error\n\nfunc LogProcessor(profiles []*TagsProfile) error {\n\tif len(profiles) == 0 {\n\t\treturn nil\n\t}\n\tklog.Infof(\"KITEX: profiler collect %d records\", len(profiles))\n\tfor _, p := range profiles {\n\t\tif p.Key != \"\" {\n\t\t\tklog.Infof(\"KITEX: profiler - %s %.2f%%\", p.Key, p.Percent*100)\n\t\t} else {\n\t\t\tklog.Infof(\"KITEX: profiler - type=default %.2f%%\", p.Percent*100)\n\t\t}\n\t}\n\tklog.Info(\"---------------------------------\")\n\treturn nil\n}\n\nfunc NewProfiler(processor Processor, interval, window time.Duration) *profiler {\n\tif processor == nil {\n\t\tprocessor = LogProcessor\n\t}\n\treturn &profiler{\n\t\tstateCond: sync.NewCond(&sync.Mutex{}),\n\t\tprocessor: processor,\n\t\tinterval:  interval,\n\t\twindow:    window,\n\t}\n}\n\nvar _ Profiler = (*profiler)(nil)\n\nconst (\n\t// state changes:\n\t//   running => pausing => paused => resuming => running\n\t//           => stopped\n\tstateRunning  = 0\n\tstatePausing  = 1\n\tstatePaused   = 2\n\tstateResuming = 3\n\tstateStopped  = 4\n)\n\ntype profiler struct {\n\tdata      bytes.Buffer // protobuf\n\tstate     int\n\tstateCond *sync.Cond\n\t// settings\n\tprocessor Processor\n\tinterval  time.Duration // sleep time between every profiling window\n\twindow    time.Duration // profiling in the window, go pprof collect stack profile every 10ms\n}\n\n// Tag current goroutine with tagged ctx\n// it's used for reuse goroutine scenario\nfunc Tag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok {\n\t\tpc.profiler.Tag(ctx)\n\t}\n}\n\n// Untag current goroutine with tagged ctx\n// it's used for reuse goroutine scenario\nfunc Untag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok {\n\t\tpc.profiler.Untag(ctx)\n\t}\n}\n\ntype profilerContext struct {\n\tprofiler Profiler\n\tuntagCtx context.Context\n\ttags     []string\n}\n\nfunc newProfilerContext(profiler Profiler) *profilerContext {\n\treturn &profilerContext{\n\t\tprofiler: profiler,\n\t\ttags:     make([]string, 0, 12),\n\t}\n}\n\n// Prepare the profiler context\nfunc (p *profiler) Prepare(ctx context.Context) context.Context {\n\tif c := ctx.Value(profilerContextKey{}); c != nil {\n\t\treturn ctx\n\t}\n\treturn context.WithValue(ctx, profilerContextKey{}, newProfilerContext(p))\n}\n\n// State return current profiler state\nfunc (p *profiler) State() (state int) {\n\tp.stateCond.L.Lock()\n\tstate = p.state\n\tp.stateCond.L.Unlock()\n\treturn state\n}\n\n// Stop the profiler\nfunc (p *profiler) Stop() {\n\tif p.State() == stateStopped {\n\t\treturn\n\t}\n\t// stateRunning => stateStopped\n\tp.stateChange(stateRunning, stateStopped)\n}\n\n// Pause the profiler.\n// The profiler has been paused when Pause() return\n\n\n\n\n\n\n\n\n\n\n// Resume the profiler.\n// The profiler has been resumed when Resume() return\n\n\n\n\n\n\n\n\n\n\n// Run start analyse the pprof data with interval and window settings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Tag current goroutine with tags\n// If ctx already tagged, append the existed tags\nfunc (p *profiler) Tag(ctx context.Context, tags ...string) context.Context {\n\tpctx, ok := ctx.Value(profilerContextKey{}).(*profilerContext)\n\tif !ok {\n\t\tpctx = newProfilerContext(p)\n\t\tctx = context.WithValue(ctx, profilerContextKey{}, pctx)\n\t}\n\tif pctx.untagCtx == nil {\n\t\tpctx.untagCtx = ctx\n\t}\n\tpctx.tags = append(pctx.tags, tags...)\n\t// do not return pprof ctx\n\tpprof.SetGoroutineLabels(pprof.WithLabels(context.Background(), pprof.Labels(pctx.tags...)))\n\treturn ctx\n}\n\n// Untag current goroutine\n// Only untag if ctx already tagged, will not clear the goroutine labels if not tagged by profiler\nfunc (p *profiler) Untag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok && pc.untagCtx != nil {\n\t\t// if ctx have untagCtx, that means the current goroutine created by a tagged goroutine\n\t\t// we need to untag the goroutine when finished\n\t\t// else, do nothing\n\t\tpprof.SetGoroutineLabels(pc.untagCtx)\n\t}\n}\n\nfunc (p *profiler) Lookup(ctx context.Context, key string) (string, bool) {\n\treturn pprof.Label(ctx, key)\n}\n\nfunc (p *profiler) stateChange(from, to int) {\n\tp.stateCond.L.Lock()\n\tfor p.state != from { // wait state to from first\n\t\tp.stateCond.Wait()\n\t}\n\tp.state = to\n\tp.stateCond.L.Unlock()\n\tp.stateCond.Broadcast()\n}\n\nfunc (p *profiler) stateWait(to int) {\n\tp.stateCond.L.Lock()\n\tfor p.state != to {\n\t\tp.stateCond.Wait()\n\t}\n\tp.stateCond.L.Unlock()\n}\n\nfunc (p *profiler) startProfile() error {\n\tp.data.Reset()\n\treturn pprof.StartCPUProfile(&p.data)\n}\n\nfunc (p *profiler) stopProfile() {\n\tpprof.StopCPUProfile()\n}\n\nfunc (p *profiler) analyse() ([]*TagsProfile, error) {\n\t// parse protobuf data\n\tpf, err := profile.ParseData(p.data.Bytes())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// filter cpu value index\n\tsampleIdx := -1\n\tfor idx, st := range pf.SampleType {\n\t\tif st.Type == \"cpu\" {\n\t\t\tsampleIdx = idx\n\t\t\tbreak\n\t\t}\n\t}\n\tif sampleIdx < 0 {\n\t\treturn nil, errors.New(\"profiler: sample type not found\")\n\t}\n\n\t// calculate every sample expense\n\tcounter := map[string]*TagsProfile{} // map[tagsKey]funcProfile\n\tvar total int64\n\tfor _, sm := range pf.Sample {\n\t\tvalue := sm.Value[sampleIdx]\n\t\ttags := labelToTags(sm.Label)\n\t\ttagsKey := tagsToKey(tags)\n\t\ttp, ok := counter[tagsKey]\n\t\tif !ok {\n\t\t\ttp = &TagsProfile{}\n\t\t\tcounter[tagsKey] = tp\n\t\t\ttp.Key = tagsKey\n\t\t\ttp.Tags = tags\n\t\t}\n\t\ttp.Value += value\n\t\ttotal += value\n\t}\n\n\t// compensate value with duration date\n\tdurationRate := float64((p.interval + p.window) / p.window)\n\tprofiles := make([]*TagsProfile, 0, len(counter)) // flat to array\n\tfor _, l := range counter {\n\t\tl.Percent = float64(l.Value) / float64(total)\n\t\tl.Value = int64(durationRate * float64(l.Value))\n\t\tprofiles = append(profiles, l)\n\t}\n\treturn profiles, nil\n}\n\n// TagsProfile is the stats result group by tag key\ntype TagsProfile struct {\n\tKey     string   // eg: a=1,b=2\n\tTags    []string // eg: [\"a\", \"1\", \"b\", \"2\"]\n\tValue   int64    // pprof cpu times\n\tPercent float64  // <= 1.0\n}\n\nfunc labelToTags(label map[string][]string) []string {\n\ttags := make([]string, 0, len(label)*2)\n\tfor k, v := range label {\n\t\ttags = append(tags, k, strings.Join(v, \",\"))\n\t}\n\treturn tags\n}\n\nfunc tagsToKey(tags []string) string {\n\tif len(tags)%2 != 0 {\n\t\treturn \"\"\n\t}\n\ttagsPair := make([]string, 0, len(tags)/2)\n\tfor i := 0; i < len(tags); i += 2 {\n\t\ttagsPair = append(tagsPair, fmt.Sprintf(\"%s=%s\", tags[i], tags[i+1]))\n\t}\n\t// sort tags to make it a unique key\n\tsort.Strings(tagsPair)\n\treturn strings.Join(tagsPair, \"|\")\n}\n\n```\n# Function Signature: func (p *profiler) Run(ctx context.Context) (err error) {\n# Natural Language Annotations: // Run start analyse the pprof data with interval and window settings\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (p *profiler) Run(ctx context.Context) (err error) {\n\t// Run start analyse the pprof data with interval and window settings\n\t// 1. run profiler\n\tif err := p.startProfile(); err != nil {\n\t\treturn err\n\t}\n\t// 2. wait interval to collect data\n\tif err := p.stateWait(statePausing); err != nil {\n\t\treturn err\n\t}\n\tpprof.StopCPUProfile()\n\treturn nil\n}\n```", "  ```\nfunc (p *profiler) Run(ctx context.Context) (err error) {\n\tif p.State() != stateRunning {\n\t\treturn errors.New(\"profiler not started or stopped\")\n\t}\n\t// start profile\n\tif err = p.startProfile(); err != nil {\n\t\treturn err\n\t}\n\n\t// start timer or loop\n\ttimer := time.NewTimer(p.interval)\n\tdefer timer.Stop()\n\tdefer p.stopProfile()\n\n\t// loop start\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn nil\n\t\tcase <-timer.C:\n\t\t\t// calculate window expense\n\t\t\tif profiles, err := p.analyse(); err != nil {\n\t\t\t\tklog.Errorf(\"KITEX: profiler analyse err %v\", err)\n\t\t\t} else {\n\t\t\t\tif err := p.processor(profiles); err != nil {\n\t\t\t\t\tklog.Errorf(\"KITEX: profiler process err %v\", err)\n\t\t\t\t}\n\t\t\t}\n\t\t\ttimer.Reset(p.interval)\n\t\t}\n\t}\n}\n```", "  ```\nfunc (p *profiler) Run(ctx context.Context) (err error) {\n\tdefer func() { // protect inCtx canceled state when recover panic\n\t\tif p.State() == stateStopped {\n\t\t\treturn\n\t\t}\n\t\t// stateRunning => stateStopped\n\t\tp.stateChange(stateRunning, stateStopped)\n\t\treturn\n\t}()\n\n\t// run main loop\n\tfor {\n\t\tp.stateWait(stateRunning) // wait until stateRunning or stateStopped\n\t\tif p.State() == stateStopped {\n\t\t\treturn\n\t\t}\n\n\t\t// create a profiling window\n\t\twindow := time.After(p.interval)\n\t\t// start profile\n\t\tif err = p.startProfile(); err != nil {\n\t\t\treturn\n\t\t}\n\n\t\t// run in current profiling window\n\t\tselect {\n\t\tcase <-window:\n\t\t\tp.stopProfile()\n\t\t\tprofiles, err := p.analyse()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\t// handler profiles\n\t\t\tif err = p.processor(profiles); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tcontinue\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"profiler: wait profiling window, but time expired\")\n\t\t}\n\t}\n}\n```"]}, "kitex-pkg/acl/acl.go-NewACLMiddleware": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/acl/acl.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package acl implements ACL functionality.\npackage acl\n\nimport (\n\t\"context\"\n\t\"errors\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\n// RejectFunc judges if to reject a request by the given context and request.\n// Returns a reason if rejected, otherwise returns nil.\ntype RejectFunc func(ctx context.Context, request interface{}) (reason error)\n\n// NewACLMiddleware creates a new ACL middleware using the provided reject funcs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func NewACLMiddleware(rules []RejectFunc) endpoint.Middleware {\n# Natural Language Annotations: // NewACLMiddleware creates a new ACL middleware using the provided reject funcs.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewACLMiddleware(rules []RejectFunc) endpoint.Middleware {\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request interface{}) (response interface{}, err error) {\n\t\t\tfor _, rule := range rules {\n\t\t\t\tif reason := rule(ctx, request); reason != nil {\n\t\t\t\t\terr = kerrors.ErrACLReject.WithCause(reason)\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn next(ctx, request)\n\t\t}\n\t}\n}\n```", "  ```\npackage acl\n\nimport (\n\t\"context\"\n\t\"errors\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\n// NewACLMiddleware creates a new ACL middleware using the provided reject funcs.\nfunc NewACLMiddleware(rules []RejectFunc) endpoint.Middleware {\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request interface{}) (response interface{}, err error) {\n\t\t\tfor _, rule := range rules {\n\t\t\t\tif reason := rule(ctx, request); reason != nil {\n\t\t\t\t\terr = kerrors.ErrACLRejected\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\tresponse, err = next.Handle(ctx, request)\n\t\t\treturn\n\t\t}\n\t}\n}\n```", "  ```\nfunc NewACLMiddleware(rules []RejectFunc) endpoint.Middleware {\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request interface{}) (response interface{}, err error) {\n\t\t\trejected, reason := rejectByRules(ctx, request, rules)\n\t\t\tif rejected {\n\t\t\t\terr = kerrors.ErrRequestRejected.WithCause(reason)\n\t\t\t\treturn\n\t\t\t}\n\t\t\treturn next(ctx, request)\n\t\t}\n\t}\n}\n\nfunc rejectByRules(ctx context.Context, request interface{}, rules []RejectFunc) (bool, error) {\n\tfor _, rule := range rules {\n\t\treason := rule(ctx, request)\n\t\tif reason != nil {\n\t\t\treturn true, reason\n\t\t}\n\t}\n\treturn false, nil\n}\n```"]}, "kitex-client/context_middleware.go-WithContextMiddlewares": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/context_middleware.go:\n```\n/*\n * Copyright 2022 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n)\n\ntype ctxMWChainKey struct{}\n\n// WithContextMiddlewares add middlewares into current ctx\n// Every client receive this ctx will execute the middlewares.\n// Note that ContextMiddleware is prior to the ClientMiddleware.\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getContextMiddleware(ctx context.Context) endpoint.Middleware {\n\tmws, _ := ctx.Value(ctxMWChainKey{}).(endpoint.Middleware)\n\treturn mws\n}\n\n// contextMW execute the ContextMiddlewares in ctx\nfunc contextMW(next endpoint.Endpoint) endpoint.Endpoint {\n\treturn func(ctx context.Context, req, resp interface{}) (err error) {\n\t\tmw := getContextMiddleware(ctx)\n\t\tif mw != nil {\n\t\t\treturn mw(next)(ctx, req, resp)\n\t\t}\n\t\treturn next(ctx, req, resp)\n\t}\n}\n\n```\n# Function Signature: func WithContextMiddlewares(ctx context.Context, mws ...endpoint.Middleware) context.Context {\n# Natural Language Annotations: // WithContextMiddlewares add middlewares into current ctx\n// Every client receive this ctx will execute the middlewares.\n// Note that ContextMiddleware is prior to the ClientMiddleware.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\npackage client\n\nimport (\n\t\"context\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n)\n\n// WithContextMiddlewares add middlewares into current ctx\n// Every client receive this ctx will execute the middlewares.\n// Note that ContextMiddleware is prior to the ClientMiddleware.\nfunc WithContextMiddlewares(ctx context.Context, mws ...endpoint.Middleware) context.Context {\n\treturn context.WithValue(ctx, ctxMWChainKey{}, endpoint.Chain(mws...))\n}\n```", "  ```\nfunc WithContextMiddlewares(ctx context.Context, mws ...endpoint.Middleware) context.Context {\n    // Check if the middlewares is not empty\n    if len(mws) == 0 {\n        return ctx\n    }\n\n    // Get the current value of ctxMWChainKey in the context\n    var currentChain domain.Middleware\n    if val, ok := ctx.Value(ctxMWChainKey{}).(domain.Middleware); ok {\n        currentChain = val\n    }\n\n    // Check if the current value is already a chain of middlewares\n    if currentChain != nil {\n        // If it is, create a new chain that appends the new middlewares to the existing chain\n        currentChain = domain.ChainMiddleware(currentChain, mws...)\n    } else {\n        // If not, create a new chain of middlewares from the new middlewares\n        currentChain = domain.ChainMiddleware(nil, mws...)\n    }\n\n    // Create a new context with the updated chain of middlewares\n    newCtx := ctx.WithValue(ctxMWChainKey{}, currentChain)\n\n    return newCtx\n}\n```", "  ```\nfunc WithContextMiddlewares(ctx context.Context, mws ...endpoint.Middleware) context.Context {\n\tvar mw endpoint.Middleware\n\tif len(mws) == 0 {\n\t\treturn ctx\n\t}\n\tmw = mws[len(mws)-1]\n\tfor i := len(mws) - 2; i >= 0; i-- {\n\t\tmw = mws[i](mw)\n\t}\n\treturn context.WithValue(ctx, ctxMWChainKey{}, mw)\n}\n```\nThis function adds middlewares to the context, enabling them to be executed before the client receives the request. It is similar to `context.WithValue`, but it also creates a chain of middlewares that are executed sequentially before the client receives the request.\n\nThe function first checks if any middlewares are passed as arguments. If not, it simply returns the original context without modifying it.\n\nOtherwise, it creates a chain of middlewares by calling the `endpoint.Middleware` function for each middleware and passing the previous middleware as an argument. It then returns the modified context with the chain of middlewares.\n\nNote that the `ctxMWChainKey` struct is used to store the middleware chain in the context value. This ensures that the middlewares are stored in a way that can be easily retrieved and executed later."]}, "kitex-client/stream.go-Header": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/stream.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"sync/atomic\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/remotecli\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n)\n\n// Streaming client streaming interface for code generate\ntype Streaming interface {\n\tStream(ctx context.Context, method string, request, response interface{}) error\n}\n\n// Stream implements the Streaming interface\nfunc (kc *kClient) Stream(ctx context.Context, method string, request, response interface{}) error {\n\tif !kc.inited {\n\t\tpanic(\"client not initialized\")\n\t}\n\tif kc.closed {\n\t\tpanic(\"client is already closed\")\n\t}\n\tif ctx == nil {\n\t\tpanic(\"ctx is nil\")\n\t}\n\tvar ri rpcinfo.RPCInfo\n\tctx, ri, _ = kc.initRPCInfo(ctx, method, 0, nil)\n\n\trpcinfo.AsMutableRPCConfig(ri.Config()).SetInteractionMode(rpcinfo.Streaming)\n\tctx = rpcinfo.NewCtxWithRPCInfo(ctx, ri)\n\n\tctx = kc.opt.TracerCtl.DoStart(ctx, ri)\n\treturn kc.sEps(ctx, request, response)\n}\n\nfunc (kc *kClient) invokeSendEndpoint() endpoint.SendEndpoint {\n\treturn func(stream streaming.Stream, req interface{}) (err error) {\n\t\treturn stream.SendMsg(req)\n\t}\n}\n\nfunc (kc *kClient) invokeRecvEndpoint() endpoint.RecvEndpoint {\n\treturn func(stream streaming.Stream, resp interface{}) (err error) {\n\t\treturn stream.RecvMsg(resp)\n\t}\n}\n\nfunc (kc *kClient) invokeStreamingEndpoint() (endpoint.Endpoint, error) {\n\thandler, err := kc.opt.RemoteOpt.CliHandlerFactory.NewTransHandler(kc.opt.RemoteOpt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, h := range kc.opt.MetaHandlers {\n\t\tif shdlr, ok := h.(remote.StreamingMetaHandler); ok {\n\t\t\tkc.opt.RemoteOpt.StreamingMetaHandlers = append(kc.opt.RemoteOpt.StreamingMetaHandlers, shdlr)\n\t\t}\n\t}\n\n\trecvEndpoint := kc.opt.Streaming.BuildRecvInvokeChain(kc.invokeRecvEndpoint())\n\tsendEndpoint := kc.opt.Streaming.BuildSendInvokeChain(kc.invokeSendEndpoint())\n\n\treturn func(ctx context.Context, req, resp interface{}) (err error) {\n\t\t// req and resp as &streaming.Stream\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tst, scm, err := remotecli.NewStream(ctx, ri, handler, kc.opt.RemoteOpt)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tclientStream := newStream(st, scm, kc, ri, kc.getStreamingMode(ri), sendEndpoint, recvEndpoint)\n\t\tresp.(*streaming.Result).Stream = clientStream\n\t\treturn\n\t}, nil\n}\n\nfunc (kc *kClient) getStreamingMode(ri rpcinfo.RPCInfo) serviceinfo.StreamingMode {\n\tmethodInfo := kc.svcInfo.MethodInfo(ri.Invocation().MethodName())\n\tif methodInfo == nil {\n\t\treturn serviceinfo.StreamingNone\n\t}\n\treturn methodInfo.StreamingMode()\n}\n\ntype stream struct {\n\tstream streaming.Stream\n\tscm    *remotecli.StreamConnManager\n\tkc     *kClient\n\tri     rpcinfo.RPCInfo\n\n\tstreamingMode serviceinfo.StreamingMode\n\tsendEndpoint  endpoint.SendEndpoint\n\trecvEndpoint  endpoint.RecvEndpoint\n\tfinished      uint32\n}\n\nvar _ streaming.WithDoFinish = (*stream)(nil)\n\nfunc newStream(s streaming.Stream, scm *remotecli.StreamConnManager, kc *kClient, ri rpcinfo.RPCInfo,\n\tmode serviceinfo.StreamingMode, sendEP endpoint.SendEndpoint, recvEP endpoint.RecvEndpoint,\n) *stream {\n\treturn &stream{\n\t\tstream:        s,\n\t\tscm:           scm,\n\t\tkc:            kc,\n\t\tri:            ri,\n\t\tstreamingMode: mode,\n\t\tsendEndpoint:  sendEP,\n\t\trecvEndpoint:  recvEP,\n\t}\n}\n\nfunc (s *stream) SetTrailer(metadata.MD) {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SetHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SendHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\n// Header returns the header metadata sent by the server if any.\n// If a non-nil error is returned, stream.DoFinish() will be called to record the EndOfStream\n\n\n\n\n\n\n\nfunc (s *stream) Trailer() metadata.MD {\n\treturn s.stream.Trailer()\n}\n\nfunc (s *stream) Context() context.Context {\n\treturn s.stream.Context()\n}\n\n// RecvMsg receives a message from the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n\n\n\n\n\n\n// SendMsg sends a message to the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n// Close will send a frame with EndStream=true to the server.\n// It will always return a nil\nfunc (s *stream) Close() error {\n\treturn s.stream.Close()\n}\n\n// DoFinish implements the streaming.WithDoFinish interface, and it records the end of stream\n// It will release the connection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc isRPCError(err error) bool {\n\tif err == nil {\n\t\treturn false\n\t}\n\tif err == io.EOF {\n\t\treturn false\n\t}\n\t_, isBizStatusError := err.(kerrors.BizStatusErrorIface)\n\t// if a tracer needs to get the BizStatusError, it should read from rpcinfo.invocation.bizStatusErr\n\treturn !isBizStatusError\n}\n\n```\n# Function Signature: func (s *stream) Header() (md metadata.MD, err error) {\n# Natural Language Annotations: // Header returns the header metadata sent by the server if any.\n// If a non-nil error is returned, stream.DoFinish() will be called to record the EndOfStream\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (s *stream) Header() (md metadata.MD, err error) {\n\treturn s.stream.Header()\n}\n```", "  ```\nfunc (s *stream) Header() (md metadata.MD, err error) {\n\treturn s.stream.Header()\n}\n```", "  ```\nfunc (s *stream) Header() (md metadata.MD, err error) {\n\tif atomic.LoadUint32(&s.finished) == 1 {\n\t\treturn nil, io.EOF\n\t}\n\treturn s.stream.Header()\n}\n```"]}, "kitex-client/stream.go-RecvMsg": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/stream.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"sync/atomic\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/remotecli\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n)\n\n// Streaming client streaming interface for code generate\ntype Streaming interface {\n\tStream(ctx context.Context, method string, request, response interface{}) error\n}\n\n// Stream implements the Streaming interface\nfunc (kc *kClient) Stream(ctx context.Context, method string, request, response interface{}) error {\n\tif !kc.inited {\n\t\tpanic(\"client not initialized\")\n\t}\n\tif kc.closed {\n\t\tpanic(\"client is already closed\")\n\t}\n\tif ctx == nil {\n\t\tpanic(\"ctx is nil\")\n\t}\n\tvar ri rpcinfo.RPCInfo\n\tctx, ri, _ = kc.initRPCInfo(ctx, method, 0, nil)\n\n\trpcinfo.AsMutableRPCConfig(ri.Config()).SetInteractionMode(rpcinfo.Streaming)\n\tctx = rpcinfo.NewCtxWithRPCInfo(ctx, ri)\n\n\tctx = kc.opt.TracerCtl.DoStart(ctx, ri)\n\treturn kc.sEps(ctx, request, response)\n}\n\nfunc (kc *kClient) invokeSendEndpoint() endpoint.SendEndpoint {\n\treturn func(stream streaming.Stream, req interface{}) (err error) {\n\t\treturn stream.SendMsg(req)\n\t}\n}\n\nfunc (kc *kClient) invokeRecvEndpoint() endpoint.RecvEndpoint {\n\treturn func(stream streaming.Stream, resp interface{}) (err error) {\n\t\treturn stream.RecvMsg(resp)\n\t}\n}\n\nfunc (kc *kClient) invokeStreamingEndpoint() (endpoint.Endpoint, error) {\n\thandler, err := kc.opt.RemoteOpt.CliHandlerFactory.NewTransHandler(kc.opt.RemoteOpt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, h := range kc.opt.MetaHandlers {\n\t\tif shdlr, ok := h.(remote.StreamingMetaHandler); ok {\n\t\t\tkc.opt.RemoteOpt.StreamingMetaHandlers = append(kc.opt.RemoteOpt.StreamingMetaHandlers, shdlr)\n\t\t}\n\t}\n\n\trecvEndpoint := kc.opt.Streaming.BuildRecvInvokeChain(kc.invokeRecvEndpoint())\n\tsendEndpoint := kc.opt.Streaming.BuildSendInvokeChain(kc.invokeSendEndpoint())\n\n\treturn func(ctx context.Context, req, resp interface{}) (err error) {\n\t\t// req and resp as &streaming.Stream\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tst, scm, err := remotecli.NewStream(ctx, ri, handler, kc.opt.RemoteOpt)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tclientStream := newStream(st, scm, kc, ri, kc.getStreamingMode(ri), sendEndpoint, recvEndpoint)\n\t\tresp.(*streaming.Result).Stream = clientStream\n\t\treturn\n\t}, nil\n}\n\nfunc (kc *kClient) getStreamingMode(ri rpcinfo.RPCInfo) serviceinfo.StreamingMode {\n\tmethodInfo := kc.svcInfo.MethodInfo(ri.Invocation().MethodName())\n\tif methodInfo == nil {\n\t\treturn serviceinfo.StreamingNone\n\t}\n\treturn methodInfo.StreamingMode()\n}\n\ntype stream struct {\n\tstream streaming.Stream\n\tscm    *remotecli.StreamConnManager\n\tkc     *kClient\n\tri     rpcinfo.RPCInfo\n\n\tstreamingMode serviceinfo.StreamingMode\n\tsendEndpoint  endpoint.SendEndpoint\n\trecvEndpoint  endpoint.RecvEndpoint\n\tfinished      uint32\n}\n\nvar _ streaming.WithDoFinish = (*stream)(nil)\n\nfunc newStream(s streaming.Stream, scm *remotecli.StreamConnManager, kc *kClient, ri rpcinfo.RPCInfo,\n\tmode serviceinfo.StreamingMode, sendEP endpoint.SendEndpoint, recvEP endpoint.RecvEndpoint,\n) *stream {\n\treturn &stream{\n\t\tstream:        s,\n\t\tscm:           scm,\n\t\tkc:            kc,\n\t\tri:            ri,\n\t\tstreamingMode: mode,\n\t\tsendEndpoint:  sendEP,\n\t\trecvEndpoint:  recvEP,\n\t}\n}\n\nfunc (s *stream) SetTrailer(metadata.MD) {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SetHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SendHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\n// Header returns the header metadata sent by the server if any.\n// If a non-nil error is returned, stream.DoFinish() will be called to record the EndOfStream\n\n\n\n\n\n\n\nfunc (s *stream) Trailer() metadata.MD {\n\treturn s.stream.Trailer()\n}\n\nfunc (s *stream) Context() context.Context {\n\treturn s.stream.Context()\n}\n\n// RecvMsg receives a message from the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n\n\n\n\n\n\n// SendMsg sends a message to the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n// Close will send a frame with EndStream=true to the server.\n// It will always return a nil\nfunc (s *stream) Close() error {\n\treturn s.stream.Close()\n}\n\n// DoFinish implements the streaming.WithDoFinish interface, and it records the end of stream\n// It will release the connection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc isRPCError(err error) bool {\n\tif err == nil {\n\t\treturn false\n\t}\n\tif err == io.EOF {\n\t\treturn false\n\t}\n\t_, isBizStatusError := err.(kerrors.BizStatusErrorIface)\n\t// if a tracer needs to get the BizStatusError, it should read from rpcinfo.invocation.bizStatusErr\n\treturn !isBizStatusError\n}\n\n```\n# Function Signature: func (s *stream) RecvMsg(m interface{}) (err error) {\n# Natural Language Annotations: // RecvMsg receives a message from the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (s *stream) RecvMsg(m interface{}) (err error) {\n\tif err = s.stream.RecvMsg(m); err != nil {\n\t\tif isRPCError(err) {\n\t\t\ts.DoFinish()\n\t\t}\n\t}\n\treturn\n}\n```", "  ```\nfunc (s *stream) RecvMsg(m interface{}) (err error) {\n\terr = s.stream.RecvMsg(m)\n\tif err != nil && err != io.EOF {\n\t\ts.DoFinish(err, false)\n\t}\n\treturn\n}\n```", "  ```\nfunc (s *stream) RecvMsg(m interface{}) (err error) {\n\tif atomic.LoadUint32(&s.finished) == 1 {\n\t\treturn io.EOF\n\t}\n\tvar resp interface{}\n\tvar ok bool\n\tif resp, ok = m.(interface{}); !ok {\n\t\treturn fmt.Errorf(\"wrong type of message: %v\", resp)\n\t}\n\terr = s.stream.RecvMsg(resp)\n\tif err != nil {\n\t\tif isRPCError(err) {\n\t\t\ts.DoFinish()\n\t\t}\n\t\treturn err\n\t}\n\treturn nil\n}\n```"]}, "kitex-client/stream.go-SendMsg": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/stream.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"sync/atomic\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/remotecli\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n)\n\n// Streaming client streaming interface for code generate\ntype Streaming interface {\n\tStream(ctx context.Context, method string, request, response interface{}) error\n}\n\n// Stream implements the Streaming interface\nfunc (kc *kClient) Stream(ctx context.Context, method string, request, response interface{}) error {\n\tif !kc.inited {\n\t\tpanic(\"client not initialized\")\n\t}\n\tif kc.closed {\n\t\tpanic(\"client is already closed\")\n\t}\n\tif ctx == nil {\n\t\tpanic(\"ctx is nil\")\n\t}\n\tvar ri rpcinfo.RPCInfo\n\tctx, ri, _ = kc.initRPCInfo(ctx, method, 0, nil)\n\n\trpcinfo.AsMutableRPCConfig(ri.Config()).SetInteractionMode(rpcinfo.Streaming)\n\tctx = rpcinfo.NewCtxWithRPCInfo(ctx, ri)\n\n\tctx = kc.opt.TracerCtl.DoStart(ctx, ri)\n\treturn kc.sEps(ctx, request, response)\n}\n\nfunc (kc *kClient) invokeSendEndpoint() endpoint.SendEndpoint {\n\treturn func(stream streaming.Stream, req interface{}) (err error) {\n\t\treturn stream.SendMsg(req)\n\t}\n}\n\nfunc (kc *kClient) invokeRecvEndpoint() endpoint.RecvEndpoint {\n\treturn func(stream streaming.Stream, resp interface{}) (err error) {\n\t\treturn stream.RecvMsg(resp)\n\t}\n}\n\nfunc (kc *kClient) invokeStreamingEndpoint() (endpoint.Endpoint, error) {\n\thandler, err := kc.opt.RemoteOpt.CliHandlerFactory.NewTransHandler(kc.opt.RemoteOpt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, h := range kc.opt.MetaHandlers {\n\t\tif shdlr, ok := h.(remote.StreamingMetaHandler); ok {\n\t\t\tkc.opt.RemoteOpt.StreamingMetaHandlers = append(kc.opt.RemoteOpt.StreamingMetaHandlers, shdlr)\n\t\t}\n\t}\n\n\trecvEndpoint := kc.opt.Streaming.BuildRecvInvokeChain(kc.invokeRecvEndpoint())\n\tsendEndpoint := kc.opt.Streaming.BuildSendInvokeChain(kc.invokeSendEndpoint())\n\n\treturn func(ctx context.Context, req, resp interface{}) (err error) {\n\t\t// req and resp as &streaming.Stream\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tst, scm, err := remotecli.NewStream(ctx, ri, handler, kc.opt.RemoteOpt)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tclientStream := newStream(st, scm, kc, ri, kc.getStreamingMode(ri), sendEndpoint, recvEndpoint)\n\t\tresp.(*streaming.Result).Stream = clientStream\n\t\treturn\n\t}, nil\n}\n\nfunc (kc *kClient) getStreamingMode(ri rpcinfo.RPCInfo) serviceinfo.StreamingMode {\n\tmethodInfo := kc.svcInfo.MethodInfo(ri.Invocation().MethodName())\n\tif methodInfo == nil {\n\t\treturn serviceinfo.StreamingNone\n\t}\n\treturn methodInfo.StreamingMode()\n}\n\ntype stream struct {\n\tstream streaming.Stream\n\tscm    *remotecli.StreamConnManager\n\tkc     *kClient\n\tri     rpcinfo.RPCInfo\n\n\tstreamingMode serviceinfo.StreamingMode\n\tsendEndpoint  endpoint.SendEndpoint\n\trecvEndpoint  endpoint.RecvEndpoint\n\tfinished      uint32\n}\n\nvar _ streaming.WithDoFinish = (*stream)(nil)\n\nfunc newStream(s streaming.Stream, scm *remotecli.StreamConnManager, kc *kClient, ri rpcinfo.RPCInfo,\n\tmode serviceinfo.StreamingMode, sendEP endpoint.SendEndpoint, recvEP endpoint.RecvEndpoint,\n) *stream {\n\treturn &stream{\n\t\tstream:        s,\n\t\tscm:           scm,\n\t\tkc:            kc,\n\t\tri:            ri,\n\t\tstreamingMode: mode,\n\t\tsendEndpoint:  sendEP,\n\t\trecvEndpoint:  recvEP,\n\t}\n}\n\nfunc (s *stream) SetTrailer(metadata.MD) {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SetHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SendHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\n// Header returns the header metadata sent by the server if any.\n// If a non-nil error is returned, stream.DoFinish() will be called to record the EndOfStream\n\n\n\n\n\n\n\nfunc (s *stream) Trailer() metadata.MD {\n\treturn s.stream.Trailer()\n}\n\nfunc (s *stream) Context() context.Context {\n\treturn s.stream.Context()\n}\n\n// RecvMsg receives a message from the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n\n\n\n\n\n\n// SendMsg sends a message to the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n// Close will send a frame with EndStream=true to the server.\n// It will always return a nil\nfunc (s *stream) Close() error {\n\treturn s.stream.Close()\n}\n\n// DoFinish implements the streaming.WithDoFinish interface, and it records the end of stream\n// It will release the connection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc isRPCError(err error) bool {\n\tif err == nil {\n\t\treturn false\n\t}\n\tif err == io.EOF {\n\t\treturn false\n\t}\n\t_, isBizStatusError := err.(kerrors.BizStatusErrorIface)\n\t// if a tracer needs to get the BizStatusError, it should read from rpcinfo.invocation.bizStatusErr\n\treturn !isBizStatusError\n}\n\n```\n# Function Signature: func (s *stream) SendMsg(m interface{}) (err error) {\n# Natural Language Annotations: // SendMsg sends a message to the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (s *stream) SendMsg(m interface{}) (err error) {\n\terr = s.sendEndpoint(s.stream, m)\n\tif err != nil && isRPCError(err) {\n\t\t_ = s.DoFinish(nil)\n\t}\n\treturn err\n}\n```", "  ```\nfunc (s *stream) SendMsg(m interface{}) (err error) {\n\tif atomic.LoadUint32(&s.finished) > 0 {\n\t\tpanic(\"stream has been finished\")\n\t}\n\tif err = s.stream.SendMsg(m); err != nil {\n\t\treturn s.DoFinish(err)\n\t}\n\treturn nil\n}\n```", "  ```\nfunc (s *stream) SendMsg(m interface{}) (err error) {\n\terr = s.stream.SendMsg(m)\n\tif err != nil {\n\t\tatomic.StoreUint32(&s.finished, EndOfStream)\n\t\ts.kc.opt.Streaming.OnError(s.scm.OriginalDst(), s.stream,\n\t\t\tfmt.Errorf(\"stream.SendMsg: %w\", err))\n\t}\n\treturn err\n}\n```"]}, "kitex-client/stream.go-DoFinish": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/stream.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"sync/atomic\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/remotecli\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n)\n\n// Streaming client streaming interface for code generate\ntype Streaming interface {\n\tStream(ctx context.Context, method string, request, response interface{}) error\n}\n\n// Stream implements the Streaming interface\nfunc (kc *kClient) Stream(ctx context.Context, method string, request, response interface{}) error {\n\tif !kc.inited {\n\t\tpanic(\"client not initialized\")\n\t}\n\tif kc.closed {\n\t\tpanic(\"client is already closed\")\n\t}\n\tif ctx == nil {\n\t\tpanic(\"ctx is nil\")\n\t}\n\tvar ri rpcinfo.RPCInfo\n\tctx, ri, _ = kc.initRPCInfo(ctx, method, 0, nil)\n\n\trpcinfo.AsMutableRPCConfig(ri.Config()).SetInteractionMode(rpcinfo.Streaming)\n\tctx = rpcinfo.NewCtxWithRPCInfo(ctx, ri)\n\n\tctx = kc.opt.TracerCtl.DoStart(ctx, ri)\n\treturn kc.sEps(ctx, request, response)\n}\n\nfunc (kc *kClient) invokeSendEndpoint() endpoint.SendEndpoint {\n\treturn func(stream streaming.Stream, req interface{}) (err error) {\n\t\treturn stream.SendMsg(req)\n\t}\n}\n\nfunc (kc *kClient) invokeRecvEndpoint() endpoint.RecvEndpoint {\n\treturn func(stream streaming.Stream, resp interface{}) (err error) {\n\t\treturn stream.RecvMsg(resp)\n\t}\n}\n\nfunc (kc *kClient) invokeStreamingEndpoint() (endpoint.Endpoint, error) {\n\thandler, err := kc.opt.RemoteOpt.CliHandlerFactory.NewTransHandler(kc.opt.RemoteOpt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, h := range kc.opt.MetaHandlers {\n\t\tif shdlr, ok := h.(remote.StreamingMetaHandler); ok {\n\t\t\tkc.opt.RemoteOpt.StreamingMetaHandlers = append(kc.opt.RemoteOpt.StreamingMetaHandlers, shdlr)\n\t\t}\n\t}\n\n\trecvEndpoint := kc.opt.Streaming.BuildRecvInvokeChain(kc.invokeRecvEndpoint())\n\tsendEndpoint := kc.opt.Streaming.BuildSendInvokeChain(kc.invokeSendEndpoint())\n\n\treturn func(ctx context.Context, req, resp interface{}) (err error) {\n\t\t// req and resp as &streaming.Stream\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tst, scm, err := remotecli.NewStream(ctx, ri, handler, kc.opt.RemoteOpt)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tclientStream := newStream(st, scm, kc, ri, kc.getStreamingMode(ri), sendEndpoint, recvEndpoint)\n\t\tresp.(*streaming.Result).Stream = clientStream\n\t\treturn\n\t}, nil\n}\n\nfunc (kc *kClient) getStreamingMode(ri rpcinfo.RPCInfo) serviceinfo.StreamingMode {\n\tmethodInfo := kc.svcInfo.MethodInfo(ri.Invocation().MethodName())\n\tif methodInfo == nil {\n\t\treturn serviceinfo.StreamingNone\n\t}\n\treturn methodInfo.StreamingMode()\n}\n\ntype stream struct {\n\tstream streaming.Stream\n\tscm    *remotecli.StreamConnManager\n\tkc     *kClient\n\tri     rpcinfo.RPCInfo\n\n\tstreamingMode serviceinfo.StreamingMode\n\tsendEndpoint  endpoint.SendEndpoint\n\trecvEndpoint  endpoint.RecvEndpoint\n\tfinished      uint32\n}\n\nvar _ streaming.WithDoFinish = (*stream)(nil)\n\nfunc newStream(s streaming.Stream, scm *remotecli.StreamConnManager, kc *kClient, ri rpcinfo.RPCInfo,\n\tmode serviceinfo.StreamingMode, sendEP endpoint.SendEndpoint, recvEP endpoint.RecvEndpoint,\n) *stream {\n\treturn &stream{\n\t\tstream:        s,\n\t\tscm:           scm,\n\t\tkc:            kc,\n\t\tri:            ri,\n\t\tstreamingMode: mode,\n\t\tsendEndpoint:  sendEP,\n\t\trecvEndpoint:  recvEP,\n\t}\n}\n\nfunc (s *stream) SetTrailer(metadata.MD) {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SetHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SendHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\n// Header returns the header metadata sent by the server if any.\n// If a non-nil error is returned, stream.DoFinish() will be called to record the EndOfStream\n\n\n\n\n\n\n\nfunc (s *stream) Trailer() metadata.MD {\n\treturn s.stream.Trailer()\n}\n\nfunc (s *stream) Context() context.Context {\n\treturn s.stream.Context()\n}\n\n// RecvMsg receives a message from the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n\n\n\n\n\n\n// SendMsg sends a message to the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n// Close will send a frame with EndStream=true to the server.\n// It will always return a nil\nfunc (s *stream) Close() error {\n\treturn s.stream.Close()\n}\n\n// DoFinish implements the streaming.WithDoFinish interface, and it records the end of stream\n// It will release the connection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc isRPCError(err error) bool {\n\tif err == nil {\n\t\treturn false\n\t}\n\tif err == io.EOF {\n\t\treturn false\n\t}\n\t_, isBizStatusError := err.(kerrors.BizStatusErrorIface)\n\t// if a tracer needs to get the BizStatusError, it should read from rpcinfo.invocation.bizStatusErr\n\treturn !isBizStatusError\n}\n\n```\n# Function Signature: func (s *stream) DoFinish(err error) {\n# Natural Language Annotations: // DoFinish implements the streaming.WithDoFinish interface, and it records the end of stream\n// It will release the connection.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (s *stream) DoFinish(err error) {\n\tif atomic.CompareAndSwapUint32(&s.finished, 0, 1) {\n\t\ts.scm.Close()\n\t\ts.scm = nil\n\t\ts.stream = nil\n\t\ts.kc = nil\n\t\ts.sendEndpoint = nil\n\t\ts.recvEndpoint = nil\n\t}\n}\n```", "  ```\nfunc (s *stream) DoFinish(err error) {\n\tif !atomic.CompareAndSwapUint32(&s.finished, 0, 1) {\n\t\t// if finished is true, then DoFinish has already been called.\n\t\treturn\n\t}\n\tif isRPCError(err) {\n\t\t// only record the end of stream error if it is not a BizStatusError\n\t\ts.kc.errCh <- err\n\t}\n\ts.scm.Release(context.Background(), s.ri)\n}\n```", "  ```\nfunc (s *stream) DoFinish(err error) {\n\tif atomic.LoadUint32(&s.finished) == 1 {\n\t\treturn\n\t}\n\t// release the connection\n\ts.Close()\n\t// blocking until the request/response is sent/received\n\ts.scm.CloseWait()\n\tatomic.CompareAndSwapUint32(&s.finished, 0, 1)\n\t// stop the stopwatch\n\ts.ri.Stats().Stop()\n\t// check the error\n\tif err == io.EOF {\n\t\ts.ri.Stats().IsClientFinish = true\n\t} else if isRPCError(err) {\n\t\ts.ri.Invocation().SetErr(err)\n\t\ts.ri.Invocation().SetBizStatusErr(err)\n\t}\n\t// tracing the end of the streaming\n\ts.kc.opt.TracerCtl.DoEndStream(s.Context(), s.ri)\n}\n```"]}, "kitex-client/client.go-NewClient": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/client.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"runtime\"\n\t\"runtime/debug\"\n\t\"strconv\"\n\t\"sync/atomic\"\n\n\t\"github.com/bytedance/gopkg/cloud/metainfo\"\n\t\"github.com/cloudwego/localsession/backup\"\n\n\t\"github.com/cloudwego/kitex/client/callopt\"\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/acl\"\n\t\"github.com/cloudwego/kitex/pkg/consts\"\n\t\"github.com/cloudwego/kitex/pkg/diagnosis\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/proxy\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/bound\"\n\t\"github.com/cloudwego/kitex/pkg/remote/remotecli\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpctimeout\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Client is the core interface abstraction of kitex client.\n// It is designed for generated codes and should not be used directly.\n// Parameter method specifies the method of a RPC call.\n// Request is a packing of request parameters in the actual method defined in IDL, consist of zero, one\n// or multiple arguments. So is response to the actual result type.\n// Response may be nil to address oneway calls.\ntype Client interface {\n\tCall(ctx context.Context, method string, request, response interface{}) error\n}\n\ntype kClient struct {\n\tsvcInfo *serviceinfo.ServiceInfo\n\tmws     []endpoint.Middleware\n\teps     endpoint.Endpoint\n\tsEps    endpoint.Endpoint\n\topt     *client.Options\n\tlbf     *lbcache.BalancerFactory\n\n\tinited bool\n\tclosed bool\n}\n\n// Set finalizer on kClient does not take effect, because kClient has a circular reference problem\n// when construct the endpoint.Endpoint in the invokeHandleEndpoint,\n// so wrapping kClient as kcFinalizerClient, and set finalizer on kcFinalizerClient, it can solve this problem.\ntype kcFinalizerClient struct {\n\t*kClient\n}\n\nfunc (kf *kcFinalizerClient) Call(ctx context.Context, method string, request, response interface{}) error {\n\tdefer runtime.KeepAlive(kf)\n\treturn kf.kClient.Call(ctx, method, request, response)\n}\n\n// NewClient creates a kitex.Client with the given ServiceInfo, it is from generated code.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (kc *kClient) init() (err error) {\n\tinitTransportProtocol(kc.svcInfo, kc.opt.Configs)\n\tif err = kc.checkOptions(); err != nil {\n\t\treturn err\n\t}\n\tif err = kc.initCircuitBreaker(); err != nil {\n\t\treturn err\n\t}\n\tif err = kc.initRetryer(); err != nil {\n\t\treturn err\n\t}\n\tif err = kc.initProxy(); err != nil {\n\t\treturn err\n\t}\n\tif err = kc.initConnPool(); err != nil {\n\t\treturn err\n\t}\n\tif err = kc.initLBCache(); err != nil {\n\t\treturn err\n\t}\n\tctx := kc.initContext()\n\tkc.initMiddlewares(ctx)\n\tkc.initStreamMiddlewares(ctx)\n\tkc.initDebugService()\n\tkc.richRemoteOption()\n\tif err = kc.buildInvokeChain(); err != nil {\n\t\treturn err\n\t}\n\tif err = kc.warmingUp(); err != nil {\n\t\treturn err\n\t}\n\tkc.inited = true\n\treturn nil\n}\n\nfunc (kc *kClient) checkOptions() (err error) {\n\tif kc.opt.Svr.ServiceName == \"\" {\n\t\treturn errors.New(\"service name is required\")\n\t}\n\treturn nil\n}\n\nfunc (kc *kClient) initCircuitBreaker() error {\n\tif kc.opt.CBSuite != nil {\n\t\tkc.opt.CBSuite.SetEventBusAndQueue(kc.opt.Bus, kc.opt.Events)\n\t}\n\treturn nil\n}\n\nfunc (kc *kClient) initRetryer() error {\n\tif kc.opt.RetryContainer == nil {\n\t\tif kc.opt.RetryMethodPolicies == nil {\n\t\t\treturn nil\n\t\t}\n\t\tkc.opt.InitRetryContainer()\n\t}\n\treturn kc.opt.RetryContainer.Init(kc.opt.RetryMethodPolicies, kc.opt.RetryWithResult)\n}\n\nfunc (kc *kClient) initContext() context.Context {\n\tctx := context.Background()\n\tctx = context.WithValue(ctx, endpoint.CtxEventBusKey, kc.opt.Bus)\n\tctx = context.WithValue(ctx, endpoint.CtxEventQueueKey, kc.opt.Events)\n\tctx = context.WithValue(ctx, rpctimeout.TimeoutAdjustKey, &kc.opt.ExtraTimeout)\n\tif chr, ok := kc.opt.Proxy.(proxy.ContextHandler); ok {\n\t\tctx = chr.HandleContext(ctx)\n\t}\n\treturn ctx\n}\n\nfunc (kc *kClient) initProxy() error {\n\tif kc.opt.Proxy != nil {\n\t\tcfg := proxy.Config{\n\t\t\tServerInfo:   kc.opt.Svr,\n\t\t\tResolver:     kc.opt.Resolver,\n\t\t\tBalancer:     kc.opt.Balancer,\n\t\t\tPool:         kc.opt.RemoteOpt.ConnPool,\n\t\t\tFixedTargets: kc.opt.Targets,\n\t\t\tRPCConfig:    kc.opt.Configs,\n\t\t}\n\t\tif err := kc.opt.Proxy.Configure(&cfg); err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// update fields in the client option for further use.\n\t\tkc.opt.Resolver = cfg.Resolver\n\t\tkc.opt.Balancer = cfg.Balancer\n\t\t// close predefined pool when proxy init new pool.\n\t\tif cfg.Pool != kc.opt.RemoteOpt.ConnPool && kc.opt.RemoteOpt.ConnPool != nil {\n\t\t\tkc.opt.RemoteOpt.ConnPool.Close()\n\t\t}\n\t\tkc.opt.RemoteOpt.ConnPool = cfg.Pool\n\t\tkc.opt.Targets = cfg.FixedTargets\n\t}\n\treturn nil\n}\n\nfunc (kc *kClient) initConnPool() error {\n\tpool := kc.opt.RemoteOpt.ConnPool\n\tkc.opt.CloseCallbacks = append(kc.opt.CloseCallbacks, pool.Close)\n\n\tif df, ok := pool.(interface{ Dump() interface{} }); ok {\n\t\tkc.opt.DebugService.RegisterProbeFunc(diagnosis.ConnPoolKey, df.Dump)\n\t}\n\tif r, ok := pool.(remote.ConnPoolReporter); ok && kc.opt.RemoteOpt.EnableConnPoolReporter {\n\t\tr.EnableReporter()\n\t}\n\n\tif long, ok := pool.(remote.LongConnPool); ok {\n\t\tkc.opt.Bus.Watch(discovery.ChangeEventName, func(ev *event.Event) {\n\t\t\tch, ok := ev.Extra.(*discovery.Change)\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tfor _, inst := range ch.Removed {\n\t\t\t\tif addr := inst.Address(); addr != nil {\n\t\t\t\t\tlong.Clean(addr.Network(), addr.String())\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n\treturn nil\n}\n\nfunc (kc *kClient) initLBCache() error {\n\tif kc.opt.Proxy != nil && kc.opt.Resolver == nil {\n\t\treturn nil\n\t}\n\tonChange := discoveryEventHandler(discovery.ChangeEventName, kc.opt.Bus, kc.opt.Events)\n\tonDelete := discoveryEventHandler(discovery.DeleteEventName, kc.opt.Bus, kc.opt.Events)\n\tresolver := kc.opt.Resolver\n\tif resolver == nil {\n\t\t// fake a resolver instead of returning an error directly because users may use\n\t\t// callopt.WithHostPort to specify target addresses after NewClient.\n\t\tresolver = &discovery.SynthesizedResolver{\n\t\t\tResolveFunc: func(ctx context.Context, target string) (discovery.Result, error) {\n\t\t\t\treturn discovery.Result{}, kerrors.ErrNoResolver\n\t\t\t},\n\t\t\tNameFunc: func() string { return \"no_resolver\" },\n\t\t}\n\t}\n\t// because we cannot ensure that user's custom loadbalancer is cacheable, we need to disable it here\n\tcacheOpts := lbcache.Options{DiagnosisService: kc.opt.DebugService, Cacheable: false}\n\tbalancer := kc.opt.Balancer\n\tif balancer == nil {\n\t\t// default internal lb balancer is cacheable\n\t\tcacheOpts.Cacheable = true\n\t\tbalancer = loadbalance.NewWeightedBalancer()\n\t}\n\tif kc.opt.BalancerCacheOpt != nil {\n\t\tcacheOpts = *kc.opt.BalancerCacheOpt\n\t}\n\tkc.lbf = lbcache.NewBalancerFactory(resolver, balancer, cacheOpts)\n\trbIdx := kc.lbf.RegisterRebalanceHook(onChange)\n\tkc.opt.CloseCallbacks = append(kc.opt.CloseCallbacks, func() error {\n\t\tkc.lbf.DeregisterRebalanceHook(rbIdx)\n\t\treturn nil\n\t})\n\tdIdx := kc.lbf.RegisterDeleteHook(onDelete)\n\tkc.opt.CloseCallbacks = append(kc.opt.CloseCallbacks, func() error {\n\t\tkc.lbf.DeregisterDeleteHook(dIdx)\n\t\treturn nil\n\t})\n\treturn nil\n}\n\nfunc (kc *kClient) initMiddlewares(ctx context.Context) {\n\tbuilderMWs := richMWsWithBuilder(ctx, kc.opt.MWBs)\n\t// integrate xds if enabled\n\tif kc.opt.XDSEnabled && kc.opt.XDSRouterMiddleware != nil && kc.opt.Proxy == nil {\n\t\tkc.mws = append(kc.mws, kc.opt.XDSRouterMiddleware)\n\t}\n\tkc.mws = append(kc.mws, kc.opt.CBSuite.ServiceCBMW(), rpcTimeoutMW(ctx), contextMW)\n\tkc.mws = append(kc.mws, builderMWs...)\n\tkc.mws = append(kc.mws, acl.NewACLMiddleware(kc.opt.ACLRules))\n\tif kc.opt.Proxy == nil {\n\t\tkc.mws = append(kc.mws, newResolveMWBuilder(kc.lbf)(ctx))\n\t\tkc.mws = append(kc.mws, kc.opt.CBSuite.InstanceCBMW())\n\t\tkc.mws = append(kc.mws, richMWsWithBuilder(ctx, kc.opt.IMWBs)...)\n\t} else {\n\t\tif kc.opt.Resolver != nil { // customized service discovery\n\t\t\tkc.mws = append(kc.mws, newResolveMWBuilder(kc.lbf)(ctx))\n\t\t}\n\t\tkc.mws = append(kc.mws, newProxyMW(kc.opt.Proxy))\n\t}\n\tkc.mws = append(kc.mws, newIOErrorHandleMW(kc.opt.ErrHandle))\n}\n\nfunc (kc *kClient) initStreamMiddlewares(ctx context.Context) {\n\tkc.opt.Streaming.EventHandler = kc.opt.TracerCtl.GetStreamEventHandler()\n\tkc.opt.Streaming.InitMiddlewares(ctx)\n}\n\nfunc richMWsWithBuilder(ctx context.Context, mwBs []endpoint.MiddlewareBuilder) (mws []endpoint.Middleware) {\n\tfor i := range mwBs {\n\t\tmws = append(mws, mwBs[i](ctx))\n\t}\n\treturn\n}\n\n// initRPCInfo initializes the RPCInfo structure and attaches it to context.\nfunc (kc *kClient) initRPCInfo(ctx context.Context, method string, retryTimes int, firstRI rpcinfo.RPCInfo) (context.Context, rpcinfo.RPCInfo, *callopt.CallOptions) {\n\treturn initRPCInfo(ctx, method, kc.opt, kc.svcInfo, retryTimes, firstRI)\n}\n\nfunc applyCallOptions(ctx context.Context, cfg rpcinfo.MutableRPCConfig, svr remoteinfo.RemoteInfo, opt *client.Options) (context.Context, *callopt.CallOptions) {\n\tcos := CallOptionsFromCtx(ctx)\n\tif len(cos) > 0 {\n\t\tinfo, callOpts := callopt.Apply(cos, cfg, svr, opt.Locks, opt.HTTPResolver)\n\t\tctx = context.WithValue(ctx, ctxCallOptionInfoKey, info)\n\t\treturn ctx, callOpts\n\t}\n\topt.Locks.ApplyLocks(cfg, svr)\n\treturn ctx, nil\n}\n\n// Call implements the Client interface .\nfunc (kc *kClient) Call(ctx context.Context, method string, request, response interface{}) (err error) {\n\t// merge backup context if no metainfo found in ctx\n\tctx = backup.RecoverCtxOnDemands(ctx, kc.opt.CtxBackupHandler)\n\n\tvalidateForCall(ctx, kc.inited, kc.closed)\n\tvar ri rpcinfo.RPCInfo\n\tvar callOpts *callopt.CallOptions\n\tctx, ri, callOpts = kc.initRPCInfo(ctx, method, 0, nil)\n\n\tctx = kc.opt.TracerCtl.DoStart(ctx, ri)\n\tvar reportErr error\n\tvar recycleRI bool\n\tdefer func() {\n\t\tif panicInfo := recover(); panicInfo != nil {\n\t\t\terr = rpcinfo.ClientPanicToErr(ctx, panicInfo, ri, false)\n\t\t\treportErr = err\n\t\t}\n\t\tkc.opt.TracerCtl.DoFinish(ctx, ri, reportErr)\n\t\tif recycleRI {\n\t\t\t// why need check recycleRI to decide if recycle RPCInfo?\n\t\t\t// 1. no retry, rpc timeout happen will cause panic when response return\n\t\t\t// 2. retry success, will cause panic when first call return\n\t\t\t// 3. backup request may cause panic, cannot recycle first RPCInfo\n\t\t\t// RPCInfo will be recycled after rpc is finished,\n\t\t\t// holding RPCInfo in a new goroutine is forbidden.\n\t\t\trpcinfo.PutRPCInfo(ri)\n\t\t}\n\t\tcallOpts.Recycle()\n\t}()\n\n\tcallOptRetry := getCalloptRetryPolicy(callOpts)\n\tif kc.opt.RetryContainer == nil && callOptRetry != nil && callOptRetry.Enable {\n\t\t// setup retry in callopt\n\t\tkc.opt.InitRetryContainer()\n\t}\n\n\t// Add necessary keys to context for isolation between kitex client method calls\n\tctx = retry.PrepareRetryContext(ctx)\n\n\tif kc.opt.RetryContainer == nil {\n\t\t// call without retry policy\n\t\terr = kc.eps(ctx, request, response)\n\t\tif err == nil {\n\t\t\trecycleRI = true\n\t\t}\n\t} else {\n\t\tvar lastRI rpcinfo.RPCInfo\n\t\tlastRI, recycleRI, err = kc.opt.RetryContainer.WithRetryIfNeeded(ctx, callOptRetry, kc.rpcCallWithRetry(ri, method, request, response), ri, request)\n\t\tif ri != lastRI {\n\t\t\t// reset ri of ctx to lastRI\n\t\t\tctx = rpcinfo.NewCtxWithRPCInfo(ctx, lastRI)\n\t\t}\n\t\tri = lastRI\n\t}\n\n\t// do fallback if with setup\n\terr, reportErr = doFallbackIfNeeded(ctx, ri, request, response, err, kc.opt.Fallback, callOpts)\n\treturn err\n}\n\nfunc (kc *kClient) rpcCallWithRetry(ri rpcinfo.RPCInfo, method string, request, response interface{}) retry.RPCCallFunc {\n\t// call with retry policy\n\tvar callTimes int32\n\t// prevRI represents a value of rpcinfo.RPCInfo type.\n\tvar prevRI atomic.Value\n\treturn func(ctx context.Context, r retry.Retryer) (rpcinfo.RPCInfo, interface{}, error) {\n\t\tcurrCallTimes := int(atomic.AddInt32(&callTimes, 1))\n\t\tcRI := ri\n\t\tif currCallTimes > 1 {\n\t\t\tctx, cRI, _ = kc.initRPCInfo(ctx, method, currCallTimes-1, ri)\n\t\t\tctx = metainfo.WithPersistentValue(ctx, retry.TransitKey, strconv.Itoa(currCallTimes-1))\n\t\t\tif prevRI.Load() == nil {\n\t\t\t\tprevRI.Store(ri)\n\t\t\t}\n\t\t\tr.Prepare(ctx, prevRI.Load().(rpcinfo.RPCInfo), cRI)\n\t\t\tprevRI.Store(cRI)\n\t\t}\n\t\tcallErr := kc.eps(ctx, request, response)\n\t\treturn cRI, response, callErr\n\t}\n}\n\nfunc (kc *kClient) initDebugService() {\n\tif ds := kc.opt.DebugService; ds != nil {\n\t\tds.RegisterProbeFunc(diagnosis.DestServiceKey, diagnosis.WrapAsProbeFunc(kc.opt.Svr.ServiceName))\n\t\tds.RegisterProbeFunc(diagnosis.OptionsKey, diagnosis.WrapAsProbeFunc(kc.opt.DebugInfo))\n\t\tds.RegisterProbeFunc(diagnosis.ChangeEventsKey, kc.opt.Events.Dump)\n\t\tds.RegisterProbeFunc(diagnosis.ServiceInfosKey, diagnosis.WrapAsProbeFunc(map[string]*serviceinfo.ServiceInfo{kc.svcInfo.ServiceName: kc.svcInfo}))\n\t}\n}\n\nfunc (kc *kClient) richRemoteOption() {\n\tkc.opt.RemoteOpt.SvcInfo = kc.svcInfo\n\t// for client trans info handler\n\tif len(kc.opt.MetaHandlers) > 0 {\n\t\t// TODO in stream situations, meta is only assembled when the stream creates\n\t\t// metaHandler needs to be called separately.\n\t\t// (newClientStreamer: call WriteMeta before remotecli.NewClient)\n\t\ttransInfoHdlr := bound.NewTransMetaHandler(kc.opt.MetaHandlers)\n\t\tkc.opt.RemoteOpt.PrependBoundHandler(transInfoHdlr)\n\t}\n}\n\nfunc (kc *kClient) buildInvokeChain() error {\n\tinnerHandlerEp, err := kc.invokeHandleEndpoint()\n\tif err != nil {\n\t\treturn err\n\t}\n\tkc.eps = endpoint.Chain(kc.mws...)(innerHandlerEp)\n\n\tinnerStreamingEp, err := kc.invokeStreamingEndpoint()\n\tif err != nil {\n\t\treturn err\n\t}\n\tkc.sEps = endpoint.Chain(kc.mws...)(innerStreamingEp)\n\treturn nil\n}\n\nfunc (kc *kClient) invokeHandleEndpoint() (endpoint.Endpoint, error) {\n\ttransPipl, err := newCliTransHandler(kc.opt.RemoteOpt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn func(ctx context.Context, req, resp interface{}) (err error) {\n\t\tvar sendMsg remote.Message\n\t\tvar recvMsg remote.Message\n\t\tdefer func() {\n\t\t\tremote.RecycleMessage(sendMsg)\n\t\t\t// Notice, recycle and decode may race if decode exec in another goroutine.\n\t\t\t// No race now, it is ok to recycle. Or recvMsg recycle depend on recv err\n\t\t\tremote.RecycleMessage(recvMsg)\n\t\t}()\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tmethodName := ri.Invocation().MethodName()\n\n\t\tcli, err := remotecli.NewClient(ctx, ri, transPipl, kc.opt.RemoteOpt)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\n\t\tdefer cli.Recycle()\n\t\tconfig := ri.Config()\n\t\tm := kc.svcInfo.MethodInfo(methodName)\n\t\tif m == nil {\n\t\t\treturn fmt.Errorf(\"method info is nil, methodName=%s, serviceInfo=%+v\", methodName, kc.svcInfo)\n\t\t} else if m.OneWay() {\n\t\t\tsendMsg = remote.NewMessage(req, kc.svcInfo, ri, remote.Oneway, remote.Client)\n\t\t} else {\n\t\t\tsendMsg = remote.NewMessage(req, kc.svcInfo, ri, remote.Call, remote.Client)\n\t\t}\n\t\tprotocolInfo := remote.NewProtocolInfo(config.TransportProtocol(), kc.svcInfo.PayloadCodec)\n\t\tsendMsg.SetProtocolInfo(protocolInfo)\n\n\t\tif err = cli.Send(ctx, ri, sendMsg); err != nil {\n\t\t\treturn\n\t\t}\n\t\tif m.OneWay() {\n\t\t\tcli.Recv(ctx, ri, nil)\n\t\t\treturn nil\n\t\t}\n\n\t\trecvMsg = remote.NewMessage(resp, kc.opt.RemoteOpt.SvcInfo, ri, remote.Reply, remote.Client)\n\t\trecvMsg.SetProtocolInfo(protocolInfo)\n\t\terr = cli.Recv(ctx, ri, recvMsg)\n\t\treturn err\n\t}, nil\n}\n\n// Close is not concurrency safe.\nfunc (kc *kClient) Close() error {\n\tdefer func() {\n\t\tif err := recover(); err != nil {\n\t\t\tklog.Warnf(\"KITEX: panic when close client, error=%s, stack=%s\", err, string(debug.Stack()))\n\t\t}\n\t}()\n\tif kc.closed {\n\t\treturn nil\n\t}\n\tkc.closed = true\n\tvar errs utils.ErrChain\n\tfor _, cb := range kc.opt.CloseCallbacks {\n\t\tif err := cb(); err != nil {\n\t\t\terrs.Append(err)\n\t\t}\n\t}\n\tif kc.opt.CBSuite != nil {\n\t\tif err := kc.opt.CBSuite.Close(); err != nil {\n\t\t\terrs.Append(err)\n\t\t}\n\t}\n\tif errs.HasError() {\n\t\treturn errs\n\t}\n\treturn nil\n}\n\nfunc newCliTransHandler(opt *remote.ClientOption) (remote.ClientTransHandler, error) {\n\thandler, err := opt.CliHandlerFactory.NewTransHandler(opt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ttransPl := remote.NewTransPipeline(handler)\n\tfor _, ib := range opt.Inbounds {\n\t\ttransPl.AddInboundHandler(ib)\n\t}\n\tfor _, ob := range opt.Outbounds {\n\t\ttransPl.AddOutboundHandler(ob)\n\t}\n\treturn transPl, nil\n}\n\nfunc initTransportProtocol(svcInfo *serviceinfo.ServiceInfo, cfg rpcinfo.RPCConfig) {\n\tmutableRPCConfig := rpcinfo.AsMutableRPCConfig(cfg)\n\tmutableRPCConfig.SetPayloadCodec(svcInfo.PayloadCodec)\n\tif svcInfo.PayloadCodec == serviceinfo.Protobuf && cfg.TransportProtocol()&transport.GRPC != transport.GRPC {\n\t\t// pb use ttheader framed by default\n\t\tmutableRPCConfig.SetTransportProtocol(transport.TTHeaderFramed)\n\t}\n}\n\nfunc (kc *kClient) warmingUp() error {\n\tif kc.opt.WarmUpOption == nil {\n\t\treturn nil\n\t}\n\twuo := kc.opt.WarmUpOption\n\tdoWarmupPool := wuo.PoolOption != nil && kc.opt.Proxy == nil\n\n\t// service discovery\n\tif kc.opt.Resolver == nil {\n\t\treturn nil\n\t}\n\tnas := make(map[string][]string)\n\tctx := context.Background()\n\n\tvar dests []rpcinfo.EndpointInfo\n\tif ro := kc.opt.WarmUpOption.ResolverOption; ro != nil {\n\t\tfor _, d := range ro.Dests {\n\t\t\tdests = append(dests, rpcinfo.FromBasicInfo(d))\n\t\t}\n\t}\n\tif len(dests) == 0 && doWarmupPool && len(wuo.PoolOption.Targets) == 0 {\n\t\t// build a default destination for the resolver\n\t\tcfg :=\n```\n# Function Signature: func NewClient(svcInfo *serviceinfo.ServiceInfo, opts ...Option) (Client, error) {\n# Natural Language Annotations: // NewClient creates a kitex.Client with the given ServiceInfo, it is from generated code.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewClient(svcInfo *serviceinfo.ServiceInfo, opts ...Option) (Client, error) {\n\tkc := &kClient{svcInfo: svcInfo}\n\tfor _, opt := range opts {\n\t\topt.Apply(&kc.opt)\n\t}\n\tkc.opt.InitRPCInfo()\n\tif err := kc.init(); err != nil {\n\t\treturn nil, err\n\t}\n\truntime.SetFinalizer(kc, &func(kf *kcFinalizerClient)) {\n\t\tkf.Close()\n\t}\n\treturn kc, nil\n}\n```", "  func NewClient(svcInfo *serviceinfo.ServiceInfo, opts ...Option) (Client, error) {\n\tvar options client.Options\n\toptions.Check = true\n\tfor _, opt := range opts {\n\t\topt(&options)\n\t}\n\tkc := &kClient{\n\t\tsvcInfo: svcInfo,\n\t\topt:     &options,\n\t}\n\tif err := kc.init(); err != nil {\n\t\treturn nil, err\n\t}\n\tif kc.opt.Logger != nil && kc.opt.Logger.At(consts.DebugLevel) {\n\t\tkc.opt.Logger.Log(consts.DebugLevel, \"kitex.Client call init\")\n\t}\n\treturn kc, nil\n}\n```", "  ```\nfunc NewClient(svcInfo *serviceinfo.ServiceInfo, opts ...Option) (Client, error) {\n\topt := &Options{}\n\tfor _, o := range opts {\n\t\tif err := o(opt); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\tif err := opt.validate(); err != nil {\n\t\treturn nil, err\n\t}\n\tif opt.Resolver == nil {\n\t\tif opt.Svr.ServiceName == \"\" {\n\t\t\treturn nil, errors.New(\"service name is required\")\n\t\t}\n\t\topt.Resolver = localsession.NewResolver(opt.Svr.ServiceName)\n\t}\n\tif opt.TracerCtl == nil {\n\t\topt.TracerCtl = DefaultTracerControl\n\t}\n\tif opt.Streaming == nil {\n\t\topt.Streaming = DefaultStreamOption\n\t}\n\tif opt.MWBs == nil {\n\t\topt.MWBs = DefaultMWBuilders\n\t}\n\tif opt.IMWBs == nil {\n\t\topt.IMWBs = DefaultIMWBuilders\n\t}\n\tif opt.RemoteOpt == nil {\n\t\topt.RemoteOpt = DefaultRemoteOpt\n\t}\n\tif opt.Configs.MsgEncoding == \"\" {\n\t\topt.Configs.MsgEncoding = DefaultMessageEncoding\n\t}\n\tif opt.CBSuite == nil {\n\t\topt.CBSuite = NewDefaultCircuitBreaker()\n\t}\n\tif opt.Events == nil {\n\t\topt.Events = event.NewEventBus()\n\t}\n\tkf := &kcFinalizerClient{&kClient{svcInfo: svcInfo, opt: opt}}\n\terr := initKClient(kf)\n\treturn kf, err\n}\n\nfunc initKClient(kf *kcFinalizerClient) error {\n\terr := kf.init()\n\tif err != nil {\n\t\treturn err\n\t}\n\tsetFinalizer(kf.kClient, kf)\n\treturn nil\n}\n\nfunc setFinalizer(kc *kClient, kf *kcFinalizerClient) {\n\truntime.SetFinalizer(kc, func(k *kClient) { kf.Close() })\n}\n```"]}, "kitex-client/middlewares.go-newResolveMWBuilder": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/middlewares.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\n\t\"github.com/cloudwego/kitex/internal\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/proxy\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nconst maxRetry = 6\n\nfunc newProxyMW(prx proxy.ForwardProxy) endpoint.Middleware {\n\t// If you want to customize the processing logic of proxy middleware,\n\t// you can implement this interface to replace the default implementation.\n\tif p, ok := prx.(proxy.WithMiddleware); ok {\n\t\treturn p.ProxyMiddleware()\n\t}\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\terr := prx.ResolveProxyInstance(ctx)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\terr = next(ctx, request, response)\n\t\t\treturn err\n\t\t}\n\t}\n}\n\nfunc discoveryEventHandler(name string, bus event.Bus, queue event.Queue) func(d *discovery.Change) {\n\treturn func(d *discovery.Change) {\n\t\tnow := time.Now()\n\t\tbus.Dispatch(&event.Event{\n\t\t\tName:  name,\n\t\t\tTime:  now,\n\t\t\tExtra: d,\n\t\t})\n\t\tqueue.Push(&event.Event{\n\t\t\tName: name,\n\t\t\tTime: now,\n\t\t\tExtra: map[string]interface{}{\n\t\t\t\t\"Added\":   wrapInstances(d.Added),\n\t\t\t\t\"Updated\": wrapInstances(d.Updated),\n\t\t\t\t\"Removed\": wrapInstances(d.Removed),\n\t\t\t},\n\t\t})\n\t}\n}\n\n// newResolveMWBuilder creates a middleware for service discovery.\n// This middleware selects an appropriate instance based on the resolver and loadbalancer given.\n// If retryable error is encountered, it will retry until timeout or an unretryable error is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// newIOErrorHandleMW provides a hook point for io error handling.\nfunc newIOErrorHandleMW(errHandle func(context.Context, error) error) endpoint.Middleware {\n\tif errHandle == nil {\n\t\terrHandle = DefaultClientErrorHandler\n\t}\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) (err error) {\n\t\t\terr = next(ctx, request, response)\n\t\t\tif err == nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\treturn errHandle(ctx, err)\n\t\t}\n\t}\n}\n\n// DefaultClientErrorHandler is Default ErrorHandler for client\n// when no ErrorHandler is specified with Option `client.WithErrorHandler`, this ErrorHandler will be injected.\n// for thrift\u3001KitexProtobuf, >= v0.4.0 wrap protocol error to TransError, which will be more friendly.\n\n\n\n\n\n\n\n\n\n\n\n// ClientErrorHandlerWithAddr is ErrorHandler for client, which will add remote addr info into error\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype instInfo struct {\n\tAddress string\n\tWeight  int\n}\n\nfunc wrapInstances(insts []discovery.Instance) []*instInfo {\n\tif len(insts) == 0 {\n\t\treturn nil\n\t}\n\tinstInfos := make([]*instInfo, 0, len(insts))\n\tfor i := range insts {\n\t\tinst := insts[i]\n\t\taddr := fmt.Sprintf(\"%s://%s\", inst.Address().Network(), inst.Address().String())\n\t\tinstInfos = append(instInfos, &instInfo{Address: addr, Weight: inst.Weight()})\n\t}\n\treturn instInfos\n}\n\nfunc retryable(err error) bool {\n\treturn errors.Is(err, kerrors.ErrGetConnection) || errors.Is(err, kerrors.ErrCircuitBreak)\n}\n\nfunc getRemoteAddr(ctx context.Context) string {\n\tif ri := rpcinfo.GetRPCInfo(ctx); ri != nil && ri.To() != nil && ri.To().Address() != nil {\n\t\treturn ri.To().Address().String()\n\t}\n\treturn \"\"\n}\n\n```\n# Function Signature: func newResolveMWBuilder(lbf *lbcache.BalancerFactory) endpoint.MiddlewareBuilder {\n# Natural Language Annotations: // newResolveMWBuilder creates a middleware for service discovery.\n// This middleware selects an appropriate instance based on the resolver and loadbalancer given.\n// If retryable error is encountered, it will retry until timeout or an unretryable error is returned.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc newResolveMWBuilder(lbf *lbcache.BalancerFactory) endpoint.MiddlewareBuilder {\n\treturn func(ep endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\tl, err := lbf.GetBalancer(ctx)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tins, err := l.SelectInstance(ctx)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tprx := proxy.NewRemoteProxy(ins)\n\t\t\treturn newProxyMW(prx)(ep)(ctx, request, response)\n\t\t}\n\t}\n}\n```", "  ```\nfunc newResolveMWBuilder(lbf *lbcache.BalancerFactory) endpoint.MiddlewareBuilder {\n\treturn func(name string, next endpoint.Endpoint) endpoint.Endpoint {\n\t\tr := lbf.GetResolver(name)\n\t\tlb := lbf.GetLoadBalancer(name)\n\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\tins, err := r.Resolve(ctx)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tinstance, err := lb.SelectInstance(ctx, ins)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tctx = rpcinfo.SetServerAddress(ctx, instance.Address)\n\t\t\tctx = rpcinfo.SetServerId(ctx, instance.Id)\n\t\t\treturn next(ctx, request, response)\n\t\t}\n\t}\n}\n```", "  ```\npackage client\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/kstream\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nconst maxRetry = 6\n\nfunc newResolveMWBuilder(lbf *lbcache.BalancerFactory) endpoint.MiddlewareBuilder {\n\treturn func(stack *endpoint.MiddlewareStack) (endpoint.Middleware, error) {\n\t\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\t\tvar inst *discovery.Instance\n\t\t\t\tvar err error\n\t\t\t\tfor i := 0; i < maxRetry; i++ {\n\t\t\t\t\tif inst, err = lbf.Reload(ctx, request); err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t\tklog.Info(\"[kitex] newResolveMWBuilder: select instance \"+inst.Address().String()+\", weight \"+fmt.Sprintf(\"%d\", inst.Weight()), nil)\n\t\t\t\t\terr = inst.GetConnection(ctx)\n\t\t\t\t\tif err == nil {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t\tklog.Info(\"[kitex] newResolveMWBuilder: get connection from loadbalance failed, addr \"+inst.Address().String()+\", err \"+err.Error(), nil)\n\t\t\t\t}\n\t\t\t\tif err != nil {\n\t\t\t\t\tklog.Error(\"[kitex] newResolveMWBuilder: get connection from loadbalance failed, addr \"+inst.Address().String()+\", err \"+err.Error(), nil)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tremoteinfo.UpdateLocalAddr(ctx, inst.Address())\n\t\t\t\treturn next(ctx, request, response)\n\t\t\t}, nil\n\t\t}, nil\n\t}\n}\n```"]}, "kitex-client/middlewares.go-DefaultClientErrorHandler": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/middlewares.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\n\t\"github.com/cloudwego/kitex/internal\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/proxy\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nconst maxRetry = 6\n\nfunc newProxyMW(prx proxy.ForwardProxy) endpoint.Middleware {\n\t// If you want to customize the processing logic of proxy middleware,\n\t// you can implement this interface to replace the default implementation.\n\tif p, ok := prx.(proxy.WithMiddleware); ok {\n\t\treturn p.ProxyMiddleware()\n\t}\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\terr := prx.ResolveProxyInstance(ctx)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\terr = next(ctx, request, response)\n\t\t\treturn err\n\t\t}\n\t}\n}\n\nfunc discoveryEventHandler(name string, bus event.Bus, queue event.Queue) func(d *discovery.Change) {\n\treturn func(d *discovery.Change) {\n\t\tnow := time.Now()\n\t\tbus.Dispatch(&event.Event{\n\t\t\tName:  name,\n\t\t\tTime:  now,\n\t\t\tExtra: d,\n\t\t})\n\t\tqueue.Push(&event.Event{\n\t\t\tName: name,\n\t\t\tTime: now,\n\t\t\tExtra: map[string]interface{}{\n\t\t\t\t\"Added\":   wrapInstances(d.Added),\n\t\t\t\t\"Updated\": wrapInstances(d.Updated),\n\t\t\t\t\"Removed\": wrapInstances(d.Removed),\n\t\t\t},\n\t\t})\n\t}\n}\n\n// newResolveMWBuilder creates a middleware for service discovery.\n// This middleware selects an appropriate instance based on the resolver and loadbalancer given.\n// If retryable error is encountered, it will retry until timeout or an unretryable error is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// newIOErrorHandleMW provides a hook point for io error handling.\nfunc newIOErrorHandleMW(errHandle func(context.Context, error) error) endpoint.Middleware {\n\tif errHandle == nil {\n\t\terrHandle = DefaultClientErrorHandler\n\t}\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) (err error) {\n\t\t\terr = next(ctx, request, response)\n\t\t\tif err == nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\treturn errHandle(ctx, err)\n\t\t}\n\t}\n}\n\n// DefaultClientErrorHandler is Default ErrorHandler for client\n// when no ErrorHandler is specified with Option `client.WithErrorHandler`, this ErrorHandler will be injected.\n// for thrift\u3001KitexProtobuf, >= v0.4.0 wrap protocol error to TransError, which will be more friendly.\n\n\n\n\n\n\n\n\n\n\n\n// ClientErrorHandlerWithAddr is ErrorHandler for client, which will add remote addr info into error\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype instInfo struct {\n\tAddress string\n\tWeight  int\n}\n\nfunc wrapInstances(insts []discovery.Instance) []*instInfo {\n\tif len(insts) == 0 {\n\t\treturn nil\n\t}\n\tinstInfos := make([]*instInfo, 0, len(insts))\n\tfor i := range insts {\n\t\tinst := insts[i]\n\t\taddr := fmt.Sprintf(\"%s://%s\", inst.Address().Network(), inst.Address().String())\n\t\tinstInfos = append(instInfos, &instInfo{Address: addr, Weight: inst.Weight()})\n\t}\n\treturn instInfos\n}\n\nfunc retryable(err error) bool {\n\treturn errors.Is(err, kerrors.ErrGetConnection) || errors.Is(err, kerrors.ErrCircuitBreak)\n}\n\nfunc getRemoteAddr(ctx context.Context) string {\n\tif ri := rpcinfo.GetRPCInfo(ctx); ri != nil && ri.To() != nil && ri.To().Address() != nil {\n\t\treturn ri.To().Address().String()\n\t}\n\treturn \"\"\n}\n\n```\n# Function Signature: func DefaultClientErrorHandler(ctx context.Context, err error) error {\n# Natural Language Annotations: // DefaultClientErrorHandler is Default ErrorHandler for client\n// when no ErrorHandler is specified with Option `client.WithErrorHandler`, this ErrorHandler will be injected.\n// for thrift\u3001KitexProtobuf, >= v0.4.0 wrap protocol error to TransError, which will be more friendly.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc DefaultClientErrorHandler(ctx context.Context, err error) error {\n\tif err == nil {\n\t\treturn nil\n\t}\n\tif tErr, ok := err.(kerrors.TransError); ok {\n\t\treturn tErr\n\t}\n\tif tErr, ok := err.(thrift.TTransportException); ok {\n\t\treturn kerrors.TransError{\n\t\t\tCode:    tErr.Error(),\n\t\t\tMessage: tErr.Error(),\n\t\t}\n\t}\n\treturn err\n}\n```", "  ```\nfunc DefaultClientErrorHandler(ctx context.Context, err error) error {\n\tif errors.Is(err, thrift.ErrDecodeFail) || errors.Is(err, protobuf.ErrParseMsgFail) {\n\t\terr = kerrors.NewTransError(err, getRemoteAddr(ctx))\n\t}\n\treturn err\n}\n```", "  ```\nfunc DefaultClientErrorHandler(ctx context.Context, err error) error {\n    if kerrors.IsPermanentTrans(err) {\n        return err\n    } else if thriftErr, ok := err.(*remote.ThriftRequestError); ok {\n        return remote.NewThriftTransError(thriftErr)\n    } else if protoErr, ok := err.(*remote.ProtobufRequestError); ok {\n        return remote.NewProtobufTransError(protoErr)\n    } else {\n        return err\n    }\n}\n```"]}, "kitex-client/middlewares.go-ClientErrorHandlerWithAddr": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/middlewares.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\n\t\"github.com/cloudwego/kitex/internal\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/proxy\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nconst maxRetry = 6\n\nfunc newProxyMW(prx proxy.ForwardProxy) endpoint.Middleware {\n\t// If you want to customize the processing logic of proxy middleware,\n\t// you can implement this interface to replace the default implementation.\n\tif p, ok := prx.(proxy.WithMiddleware); ok {\n\t\treturn p.ProxyMiddleware()\n\t}\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\terr := prx.ResolveProxyInstance(ctx)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\terr = next(ctx, request, response)\n\t\t\treturn err\n\t\t}\n\t}\n}\n\nfunc discoveryEventHandler(name string, bus event.Bus, queue event.Queue) func(d *discovery.Change) {\n\treturn func(d *discovery.Change) {\n\t\tnow := time.Now()\n\t\tbus.Dispatch(&event.Event{\n\t\t\tName:  name,\n\t\t\tTime:  now,\n\t\t\tExtra: d,\n\t\t})\n\t\tqueue.Push(&event.Event{\n\t\t\tName: name,\n\t\t\tTime: now,\n\t\t\tExtra: map[string]interface{}{\n\t\t\t\t\"Added\":   wrapInstances(d.Added),\n\t\t\t\t\"Updated\": wrapInstances(d.Updated),\n\t\t\t\t\"Removed\": wrapInstances(d.Removed),\n\t\t\t},\n\t\t})\n\t}\n}\n\n// newResolveMWBuilder creates a middleware for service discovery.\n// This middleware selects an appropriate instance based on the resolver and loadbalancer given.\n// If retryable error is encountered, it will retry until timeout or an unretryable error is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// newIOErrorHandleMW provides a hook point for io error handling.\nfunc newIOErrorHandleMW(errHandle func(context.Context, error) error) endpoint.Middleware {\n\tif errHandle == nil {\n\t\terrHandle = DefaultClientErrorHandler\n\t}\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) (err error) {\n\t\t\terr = next(ctx, request, response)\n\t\t\tif err == nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\treturn errHandle(ctx, err)\n\t\t}\n\t}\n}\n\n// DefaultClientErrorHandler is Default ErrorHandler for client\n// when no ErrorHandler is specified with Option `client.WithErrorHandler`, this ErrorHandler will be injected.\n// for thrift\u3001KitexProtobuf, >= v0.4.0 wrap protocol error to TransError, which will be more friendly.\n\n\n\n\n\n\n\n\n\n\n\n// ClientErrorHandlerWithAddr is ErrorHandler for client, which will add remote addr info into error\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype instInfo struct {\n\tAddress string\n\tWeight  int\n}\n\nfunc wrapInstances(insts []discovery.Instance) []*instInfo {\n\tif len(insts) == 0 {\n\t\treturn nil\n\t}\n\tinstInfos := make([]*instInfo, 0, len(insts))\n\tfor i := range insts {\n\t\tinst := insts[i]\n\t\taddr := fmt.Sprintf(\"%s://%s\", inst.Address().Network(), inst.Address().String())\n\t\tinstInfos = append(instInfos, &instInfo{Address: addr, Weight: inst.Weight()})\n\t}\n\treturn instInfos\n}\n\nfunc retryable(err error) bool {\n\treturn errors.Is(err, kerrors.ErrGetConnection) || errors.Is(err, kerrors.ErrCircuitBreak)\n}\n\nfunc getRemoteAddr(ctx context.Context) string {\n\tif ri := rpcinfo.GetRPCInfo(ctx); ri != nil && ri.To() != nil && ri.To().Address() != nil {\n\t\treturn ri.To().Address().String()\n\t}\n\treturn \"\"\n}\n\n```\n# Function Signature: func ClientErrorHandlerWithAddr(ctx context.Context, err error) error {\n# Natural Language Annotations: // ClientErrorHandlerWithAddr is ErrorHandler for client, which will add remote addr info into error\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc ClientErrorHandlerWithAddr(ctx context.Context, err error) error {\n\terrMsg := err.Error()\n\tif addr := getRemoteAddr(ctx); addr != \"\" {\n\t\terrMsg = fmt.Sprintf(\"%v: %s\", err, addr)\n\t}\n\treturn errors.New(errMsg)\n}\n```", "  ```\nfunc ClientErrorHandlerWithAddr(ctx context.Context, err error) error {\n\tif errors.Is(err, kerrors.ErrGetConnection) || errors.Is(err, kerrors.ErrCircuitBreak) {\n\t\treturn err\n\t}\n\terrStr := fmt.Sprintf(\"%v\", err)\n\tif ri := rpcinfo.GetRPCInfo(ctx); ri != nil && ri.To() != nil && ri.To().Address() != nil {\n\t\terrStr = fmt.Sprintf(\"%s (addr: %s)\", err, ri.To().Address())\n\t}\n\treturn fmt.Errorf(errStr)\n}\n```", "  ```\nfunc ClientErrorHandlerWithAddr(ctx context.Context, err error) error {\n\tif err == nil {\n\t\treturn nil\n\t}\n\tif !kerrors.IsEOF(err) && !kerrors.IsTimeout(err) {\n\t\treturn NewTransError(err)\n\t}\n\tri := rpcinfo.GetRPCInfo(ctx)\n\tif ri == nil || ri.From() == nil {\n\t\treturn err\n\t}\n\taddr := ri.From().Address().String()\n\textra := NewEFEExtra(err, map[string]interface{}{\"addr\": addr})\n\treturn NewTransError(err, extra)\n}\n```"]}, "kitex-client/option.go-WithTransportProtocol": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithTransportProtocol(tp transport.Protocol) Option {\n# Natural Language Annotations: // WithTransportProtocol sets the transport protocol for client.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithTransportProtocol(tp transport.Protocol) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTransportProtocol(%+v)\", tp))\n\t\to.TransProto = tp\n\t}}\n}\n```", "  ```\nfunc WithTransportProtocol(tp transport.Protocol) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTransportProtocol(%T(%+v))\", tp, tp))\n\t\to.TransportProto = tp\n\t}}\n}\n```", "  ```\nfunc WithTransportProtocol(tp transport.Protocol) Option {\n    return Option{\n        F: func(o *client.Options, di *utils.Slice) {\n            di.Push(fmt.Sprintf(\"WithTransportProtocol(%s)\", tp.Name()))\n            if tp == \"\" {\n                mp         := make(map[string]retry.Policy)\n                mp[\"*\"]    = retry.NoRetryPolicy\n                o.RetryMethodPolicies = mp\n                fmt.Println(\"\\033[1;33m invalid input: WithTransportProtocol()\\033[0m\\n\")\n                return\n            }\n            var inserter func(k string, v retry.Policy)\n            inserter = func(k string, v retry.Policy) {\n                old := o.RetryMethodPolicies[k]\n                if old.FirstTry {\n                    old.MinDelay = v.MethodBackoff.MinDelay\n                    old.MaxDelayFunc = v.MethodBackoff.MaxDelayFunc\n                    // the large strategy will override the small one\n                    // minDelay > 0 has the high priority\n                    // if want to use old minDelay, please remove the damage statement \n                    if v.MethodBackoff.MinDelay > old.MethodBackoff.MinDelay {\n                        old.MinDelay = v.MethodBackoff.MinDelay\n                        old.MaxDelayFunc = v.MethodBackoff.MaxDelayFunc\n                    }\n                    original := o.RetryMethodPolicies[k]\n                    if retry.DeriveBackoff(original, v) != nil {\n                        old.MaxDelayFunc = nil\n                    }\n                }\n            }\n            v, exists := o.RetryMethodPolicies[\"*\"]\n                if exists {\n                inserter(\"*\", v)\n            }\n            v, exists = o.RetryMethodPolicies[tp.String()]\n            if exists {\n                inserter(tp.String(), v)\n            }\n        },\n    }\n}\n```"]}, "kitex-client/option.go-WithInstanceMW": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithInstanceMW(mw endpoint.Middleware) Option {\n# Natural Language Annotations: // WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithInstanceMW(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithInstanceMW(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n```", "  ```\nfunc WithInstanceMW(mw endpoint.Middleware) Option {\n    return Option{F: func(o *client.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithInstanceMW(%+v)\", utils.GetFuncName(mw)))\n        o.MWBs = append(o.MWBs, mw)\n    }}\n}\n```", "  ```\nfunc WithInstanceMW(mw endpoint.Middleware) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithInstanceMW(%+v)\", utils.GetFuncName(mw)))\n\t\to.IstMWBs = append(o.IstMWBs, mw)\n\t}}\n}\n```"]}, "kitex-client/option.go-WithHostPorts": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithHostPorts(hostports ...string) Option {\n# Natural Language Annotations: // WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithHostPorts(hostports ...string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(\"WithHostPorts(\" + strings.Join(hostports, \", \") + \")\")\n\t\to.HostPorts = hostports\n\t}}\n}\n```", "  ```\nfunc WithHostPorts(hostports ...string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHostPorts(%v)\", hostports))\n\n\t\to.Hostports = hostports\n\t}}\n}\n```", "  ```\nfunc WithHostPorts(hostports []string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHostPorts(%+v)\", hostports))\n\t\to.Svr.URLs = hostports\n\t}}\n}\n```"]}, "kitex-client/option.go-WithResolver": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithResolver(r discovery.Resolver) Option {\n# Natural Language Annotations: // WithResolver provides the Resolver for kitex client.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithResolver(r discovery.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithResolver(%T)\", r))\n\n\t\to.Resolver = r\n\t}}\n}\n```", "  ```\nfunc WithResolver(r discovery.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithResolver(%T)\", r))\n\n\t\to.Resolver = r\n\t}}\n}\n```", "  ```\nfunc WithResolver(r discovery.Resolver) Option {\n\tpanic(\"client.WithResolver is deprecated\")\n}\n```"]}, "kitex-client/option.go-WithShortConnection": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithShortConnection() Option {\n# Natural Language Annotations: // WithShortConnection forces kitex to close connection after each call is finished.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithShortConnection() Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithShortConnection()\"))\n\t\to.CloseConnectionAfterCall = true\n\t}}\n}\n```", "  ```\nfunc WithShortConnection() Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(\"WithShortConnection()\")\n\t\to.Client.ShortConn = true\n\t}}\n}\n```", "  ```\nfunc WithShortConnection() Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(\"WithShortConnection\")\n\t\to.Trans.ForceClose = true\n\t\t// ... other processing\n\t}}\n}\n```"]}, "kitex-client/option.go-WithLongConnection": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithLongConnection(cfg connpool.IdleConfig) Option {\n# Natural Language Annotations: // WithLongConnection enables long connection with kitex's built-in pooling implementation.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nreturn Option{\n\tF: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(\"WithLongConnection(\" + utils.GetFuncName(cfg) + \")\")\n\t\to.PoolCfg = &connpool.Config{\n\t\t\tIdleConfig: cfg,\n\t\t}\n\t},\n}\n```", "  ```\nOption{F: func(o *client.Options, di *utils.Slice) {\n\tdi.Push(fmt.Sprintf(\"WithLongConnection(%+v)\", cfg))\n\n\to.PoolOption = &client.PoolOption{\n\t\tPoolType:         connpool.ConnPoolTypeLongConn,\n\t\tIdleConfig:       cfg,\n\t\tEnable:           true,\n\t\tActivePoolType:   connpool.ACPATypeOpentrace,\n\t\tActiveConnMaxIdle: 0,\n\t}\n}}}\n```", "  ```\nOption{\n\t\tF: func(o *client.Options, di *utils.Slice) {\n\t\t\to.EnablePool = true\n\t\t\tif o.ConnPoolConfig == nil {\n\t\t\t\to.ConnPoolConfig = &connpool.Config{}\n\t\t\t}\n\t\t\tif di != nil {\n\t\t\t\tdi.Push(fmt.Sprintf(\"WithLongConnection(%+v)\", cfg))\n\t\t\t}\n\t\t\to.ConnPoolConfig.EnableToggle = true\n\t\t\t// convert IdleConfig to pool.Config\n\t\t\to.ConnPoolConfig.CoreSize = cfg.CoreSize\n\t\t\to.ConnPoolConfig.MaxSize = cfg.MaxSize\n\t\t\to.ConnPoolConfig.IdleTimeout = cfg.IdleTimeout\n\t\t\to.ConnPoolConfig.IdleCallback = cfg.IdleCallback\n\t\t\to.ConnPoolConfig.Keepalive = cfg.Keepalive\n\t\t},\n\t}\n```"]}, "kitex-client/option.go-WithMuxConnection": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithMuxConnection(connNum int) Option {\n# Natural Language Annotations: // WithMuxConnection specifies the transport type to be mux.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithMuxConnection(connNum int) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMuxConnection(%d)\", connNum))\n\t\to.TransMux = []int{connNum}\n\t}}\n}\n```", "  ```\nfunc WithMuxConnection(connNum int) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMuxConnection(%d)\", connNum))\n\t\to.ConnNum = connNum\n\t\to.MWs = append(o.MWs, func(ctx context.Context, e endpoint.Endpoint) endpoint.Endpoint {\n\t\t\tmuxer := netpollmux.NewMultiEndPoint(connNum, func() netpollmux.EndPoint {\n\t\t\t\treturn e\n\t\t\t})\n\t\t\treturn endpoint.EndpointFunc(func(ctx context.Context, req, resp interface{}) (err error) {\n\t\t\t\t_, err = muxer.Endpoint().Endpoint().Call(ctx, req, resp)\n\t\t\t\treturn\n\t\t\t})\n\t\t})\n\t}}\n}\n```", "  ```\nfunc WithMuxConnection(connNum int) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMuxConnection(%d)\", connNum))\n\t\tmux := netpollmux.New(connNum)\n\t\to.Tran = &remote.Transporter{Impl: mux}\n\t\to.ConnPoolOpt = new(connpool.Option)\n\t\to.ConnPoolOpt.PoolCtl = netpollmux.NewMuxPoolCtlImpl(mux, o.Locks.Lock, func(conn net.Conn) transfer.Transfer {\n\t\t\treturn grpc.NewNetpollTransfer(conn, o.Configs)\n\t\t})\n\t}}\n}\n```"]}, "kitex-client/option.go-WithTimeoutProvider": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithTimeoutProvider(p rpcinfo.TimeoutProvider) Option {\n# Natural Language Annotations: // WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithTimeoutProvider(p rpcinfo.TimeoutProvider) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTimeoutProvider(%+v)\", p))\n\n\t\to.TimeoutProvider = p\n\t}}\n}\n```", "  ```\nfunc WithTimeoutProvider(p rpcinfo.TimeoutProvider) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTimeoutProvider(%T{%+v})\", p, p))\n\n\t\to.TimeoutProvider = p\n\t}}\n}\n```", "  ```\nfunc WithTimeoutProvider(p rpcinfo.TimeoutProvider) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTimeoutProvider(%+v)\", p))\n\t\to.TimeoutProviders = append(o.TimeoutProviders, p)\n\t}}\n}\n```"]}, "kitex-client/option.go-WithCodec": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithCodec(c remote.Codec) Option {\n# Natural Language Annotations: // WithCodec to set a codec that handle other protocols which not support by kitex\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\t\to.Codec = c\n\t}}\n}\n```", "  ```\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%T{%+v})\", c, c))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetCodec(c)\n\t\to.Locks.Bits |= rpcinfo.BitCodec\n\t}}\n}\n```", "  ```\nOption{\n    F: func(o *client.Options, di *utils.Slice) {\n        di.Push(\"WithCodec(%T{%+v})\", c, c)\n        o.Codec = c\n    },\n}\n```"]}, "kitex-client/option.go-WithPayloadCodec": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithPayloadCodec(c remote.PayloadCodec) Option {\n# Natural Language Annotations: // WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithPayloadCodec(c remote.PayloadCodec) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithPayloadCodec(%+v)\", c))\n\t\to.PayloadCodec = c\n\t}}\n}\n```", "  ```\nfunc WithPayloadCodec(c remote.PayloadCodec) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithPayloadCodec(%T{%v})\", c, c))\n\t\to.PayloadCodec = c\n\t}}\n}\n```", "  ```\nfunc WithPayloadCodec(c remote.PayloadCodec) Option {\n\treturn Option{\n\t\tF: func(o *client.Options, di *utils.Slice) {\n\t\t\tif c == nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tdi.Push(fmt.Sprintf(\"WithPayloadCodec(%T{%+v})\", c, c))\n\t\t\to.PayloadCodec = c\n\t\t},\n\t}\n}\n```"]}, "kitex-client/option.go-WithConnReporterEnabled": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithConnReporterEnabled() Option {\n# Natural Language Annotations: // WithConnReporterEnabled to enable reporting connection pool stats.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithConnReporterEnabled() Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(\"WithConnReporterEnabled()\")\n\t\to.ConnReporterEnabled = true\n\t}}\n}\n```", "  ```\nfunc WithConnReporterEnabled() Option {\n\treturn Option{\n\t\tF: func(o *client.Options, di *utils.Slice) {\n\t\t\tdi.Push(\"WithConnReporterEnabled()\")\n\t\t\to.ConnReporterEnabled = true\n\t\t},\n\t}\n}\n```", "  ```\nreturn Option{\n\tF: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(\"WithConnReporterEnabled\")\n\t\to.ConnReporterEnabled = true\n\t},\n}\n```"]}, "kitex-client/option.go-WithFailureRetry": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithFailureRetry(p *retry.FailurePolicy) Option {\n# Natural Language Annotations: // WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithFailureRetry(p *retry.FailurePolicy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !retry.IsFailurePolicyValid(p) {\n\t\t\tpanic(fmt.Errorf(\"WithFailureRetry: invalid '%+v'\", p))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFailureRetry(%+v)\", p))\n\t\to.FailureRetry = p\n\t}}\n}\n```", "  ```\nfunc WithFailureRetry(p *retry.FailurePolicy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif p == nil {\n\t\t\tpanic(\"WithFailureRetry: invalid policy 'nil'\")\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFailureRetry(%+v)\", *p))\n\t\to.FailureRetryPolicy = p\n\t}}\n}\n```", "  ```\nfunc WithFailureRetry(p *retry.FailurePolicy) Option {\n    return Option{F: func(o *client.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithFailureRetry(%+v)\", p))\n        o.RetryMethodPolicies[retry.Wildcard] = retry.Policy{\n            FailureType:  retry.TypeFailure,\n            MaxAttempts:  p.MaxAttempts,\n            WaitTime:     p.WaitTime,\n            BackoffBase:  p.BackoffBase,\n            BackoffJitter: p.BackoffJitter,\n            StartingWaitTime: p.StartingWaitTime,\n            WaitMethod: p.WaitMethod,\n            ShouldRetry: p.ShouldRetry,\n            ReportAsFailure: p.ReportAsFailure,\n            RequireSample: p.RequireSample,\n        }\n    }}\n}\n```"]}, "kitex-client/option.go-WithBackupRequest": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithBackupRequest(p *retry.BackupPolicy) Option {\n# Natural Language Annotations: // WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithBackupRequest(p *retry.BackupPolicy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !retry.IsBackupPolicyValid(p) {\n\t\t\tpanic(fmt.Errorf(\"WithBackupRequest: invalid '%+v'\", p))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithBackupRequest(%+v)\", p))\n\t\to.BackupPolicy = p\n\t}}\n}\n```", "  ```\nfunc WithBackupRequest(p *retry.BackupPolicy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !retry.IsBackupPolicyValid(p) {\n\t\t\tpanic(fmt.Errorf(\"WithBackupRequest: invalid '%+v'\", p))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithBackupRequest(%+v)\", p))\n\t\to.RetryPolicy.BackupPolicy = p\n\t}\n}\n```", "  ```\nfunc WithBackupRequest(bp *retry.BackupPolicy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !(bp != nil) {\n\t\t\tpanic(fmt.Errorf(\"WithBackupRequest: invalid '%+v'\", bp))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithBackupRequest(%+v)\", bp))\n\t\to.BackupPolicy = bp\n\t\to.Locks.Bits |= rpcinfo.BitBackupRequest\n\t}}\n}\n```"]}, "kitex-client/option.go-WithSpecifiedResultRetry": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithSpecifiedResultRetry(rr *retry.ShouldResultRetry) Option {\n# Natural Language Annotations: // WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithSpecifiedResultRetry(rr *retry.ShouldResultRetry) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif rr == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithSpecifiedResultRetry(%+v)\", rr))\n\t\to.RetryResultRetry = rr\n\t}}\n}\n```", "  ```\nfunc WithSpecifiedResultRetry(rr *retry.ShouldResultRetry) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithSpecifiedResultRetry(%+v)\", rr))\n\t\tif o.RetryPolicy == nil {\n\t\t\to.RetryPolicy = &retry.Policy{}\n\t\t}\n\t\to.RetryPolicy.ShouldResultRetry = rr\n\t}}\n}\n```", "  ```\nfunc WithSpecifiedResultRetry(rr *retry.ShouldResultRetry) Option {\n\treturn Option{\n\t\tF: func(o *client.Options, di *utils.Slice) {\n\t\t\tdi.Push(fmt.Sprintf(\"WithSpecifiedResultRetry(%+v)\", rr))\n\t\t\tif !retry.IsResultRetryValid(rr) {\n\t\t\t\tpanic(fmt.Errorf(\"WithSpecifiedResultRetry: invalid '%+v'\", rr))\n\t\t\t}\n\t\t\to.RetryResultRetry = rr\n\t\t},\n\t}\n}\n```"]}, "kitex-client/option.go-WithCircuitBreaker": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithCircuitBreaker(s *circuitbreak.CBSuite) Option {\n# Natural Language Annotations: // WithCircuitBreaker adds a circuitbreaker suite for the client.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithCircuitBreaker(s *circuitbreak.CBSuite) Option {\n\treturn Option{\n\t\tF: func(o *client.Options, di *utils.Slice) {\n\t\t\tdi.Push(fmt.Sprintf(\"WithCircuitBreaker(%+v)\", s))\n\t\t\to.CircuitBreaker = s\n\t\t},\n\t}\n}\n```", "  ```\nfunc WithCircuitBreaker(s *circuitbreak.CBSuite) Option {\n\treturn Option{\n\t\tF: func(o *client.Options, di *utils.Slice) {\n\t\t\tif s == nil {\n\t\t\t\tpanic(\"WithCircuitBreaker: suite can not be nil\")\n\t\t\t}\n\t\t\to.CBSuite = s\n\t\t\tdi.Push(fmt.Sprintf(\"WithCircuitBreaker(%+v)\", s))\n\t\t},\n\t}\n}\n```", "  ```\nfunc WithCircuitBreaker(s *circuitbreak.CBSuite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif s == nil {\n\t\t\ts = circuitbreak.DefaultCBSuit()\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithCircuitBreaker(%v)\", s.String()))\n\t\to.CBSuit = s\n\t}}\n}\n```"]}, "kitex-client/option.go-WithGRPCConnPoolSize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCConnPoolSize(s uint32) Option {\n# Natural Language Annotations: // WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithGRPCConnPoolSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCConnPoolSize(%d)\", s))\n\t\to.GRPCConnectOpts.ConnPoolSize = s\n\t}}\n}\n```", "  ```\nfunc WithGRPCConnPoolSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCConnPoolSize(%d)\", s))\n\t\to.GRPCConnectOpts.ConnPoolSize = s\n\t}}\n}\n```", "  ```\nfunc WithGRPCConnPoolSize(s uint32) Option {\n    return Option{F: func(o *client.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithGRPCConnPoolSize(%d)\", s))\n        o.GRPCConnectOpts.PoolSize = s\n    }}\n}\n```"]}, "kitex-client/option.go-WithGRPCInitialWindowSize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCInitialWindowSize(s uint32) Option {\n# Natural Language Annotations: // WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithGRPCInitialWindowSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialWindowSize(%d)\", s))\n\t\to.GRPCStreamOptions.InitialWindowSize = s\n\t}}\n}\n```", "  ```\nfunc WithGRPCInitialWindowSize(s uint32) client.Option {\n\treturn client.Option{\n\t\tF: func(o *client.Options, di *utils.Slice) {\n\t\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialWindowSize(%d)\", s))\n\t\t\to.GRPCConnectOpts.InitialWindowSize = s\n\t\t},\n\t}\n}\n```", "  ```\nfunc WithGRPCInitialWindowSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(\"WithGRPCInitialWindowSize\")\n\t\to.GRPCStreamOpts.InitialWindowSize = s\n\t}}\n}\n```"]}, "kitex-client/option.go-WithGRPCInitialConnWindowSize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCInitialConnWindowSize(s uint32) Option {\n# Natural Language Annotations: // WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithGRPCInitialConnWindowSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialConnWindowSize(%d)\", s))\n\t\to.GRPCConnectOpts.InitialConnWindowSize = s\n\t}}\n}\n```", "  ```\nfunc WithGRPCInitialConnWindowSize(s uint32) Option {\n\treturn Option{\n\t\tF: func(o *client.Options, di *utils.Slice) {\n\t\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialConnWindowSize(%d)\", s))\n\t\t\to.GRPCConnectOpts.InitialConnWindowSize = s\n\t\t},\n\t}\n}\n```", "  ```\nfunc WithGRPCInitialConnWindowSize(s uint32) Option {\n    return Option{F: func(o *client.Options, di *utils.Slice) {\n        // Check and set the lower bound for window size as 64K.\n        if s < 64 * 1024 {\n            s = 64 * 1024\n        }\n        di.Push(fmt.Sprintf(\"WithGRPCInitialConnWindowSize(%d)\", s))\n        // Set the initial window size for the connection on gRPC.\n        o.SetDialOption(grpc.WithInitialConnWindowSize(s))\n    }\n}\n```"]}, "kitex-client/option.go-WithGRPCMaxHeaderListSize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCMaxHeaderListSize(s uint32) Option {\n# Natural Language Annotations: // WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithGRPCMaxHeaderListSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCMaxHeaderListSize(%d)\", s))\n\t\to.GRPCConnectOpts.MaxHeaderListSize = s\n\t}}\n}\n```", "  ```\nfunc WithGRPCMaxHeaderListSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCMaxHeaderListSize(%d)\", s))\n\t\to.GRPCConnectOpts.MaxHeaderListSize = s\n\t}}\n}\n```", "  ```\nfunc WithGRPCMaxHeaderListSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCMaxHeaderListSize(%d)\", s))\n\t\to.GRPCConnectOpts.MaxHeaderListSize = s\n\t}}\n}\n```"]}, "kitex-client/option.go-WithGRPCKeepaliveParams": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCKeepaliveParams(kp grpc.ClientKeepalive) Option {\n# Natural Language Annotations: // WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithGRPCKeepaliveParams(kp grpc.ClientKeepalive) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCKeepaliveParams(%+v)\", kp))\n\t\to.GRPCConnectOpts.KeepaliveParams = kp\n\t}}\n}\n```", "  ```\nfunc WithGRPCKeepaliveParams(kp grpc.ClientKeepalive) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCKeepaliveParams({%+v})\", kp))\n\t\to.GRPCKeepalive = kp\n\t}}\n}\n```", "  ```\nfunc WithGRPCKeepaliveParams(kp grpc.ClientKeepalive) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCKeepaliveParams(%+v)\", grpc.ClientKeepalive)\n\t\to.GRPCConnectOpts.KeepaliveParams = kp\n\t}}\n}\n```"]}, "kitex-client/option.go-WithWarmingUp": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithWarmingUp(wuo *warmup.ClientOption) Option {\n# Natural Language Annotations: // WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithWarmingUp(wuo *warmup.ClientOption) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithWarmingUp(%+v)\", wuo))\n\t\to.Warmup = wuo\n\t}}\n}\n```", "  ```\nfunc WithWarmingUp(wuo *warmup.ClientOption) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithWarmingUp(%+v)\", wuo))\n\t\to.WarmUpOption = wuo\n\t}}\n}\n```", "  ```\nfunc WithWarmingUp(wuo *warmup.ClientOption) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithWarmingUp(%+v)\", wuo))\n\n\t\tif wuo != nil {\n\t\t\to.WuEager = wuo.Eager\n\t\t\to.WuTimeout = wuo.Timeout\n\t\t\to.WuBTI = wuo.BTI\n\t\t}\n\t}}\n}\n```"]}, "kitex-client/option.go-WithXDSSuite": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithXDSSuite(suite xds.ClientSuite) Option {\n# Natural Language Annotations: // WithXDSSuite is used to set the xds suite for the client.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithXDSSuite(suite xds.ClientSuite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithXDSSuite(%+v)\", suite))\n\t\to.XDSSuite = suite\n\t}}\n}\n```", "  ```\nfunc WithXDSSuite(suite xds.ClientSuite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithXDSSuite(%T)\", suite))\n\t\to.XDSSuite = suite\n\t}}\n}\n```", "  ```\nfunc WithXDSSuite(suite xds.ClientSuite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithXDSSuite(%T{%+v})\", suite, suite))\n\t\to.XDSSuite = suite\n\t}}\n}\n\n```"]}, "kitex-client/callopt/options.go-WithHostPort": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/callopt/options.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package callopt contains options that control the behavior of client on request level.\npackage callopt\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nvar callOptionsPool = sync.Pool{\n\tNew: newOptions,\n}\n\ntype CallOptions struct {\n\tconfigs      rpcinfo.MutableRPCConfig\n\tsvr          remoteinfo.RemoteInfo\n\tlocks        *client.ConfigLocks\n\thttpResolver http.Resolver\n\n\t// export field for using in client\n\tRetryPolicy    retry.Policy\n\tFallback       *fallback.Policy\n\tCompressorName string\n}\n\nfunc newOptions() interface{} {\n\treturn &CallOptions{\n\t\tlocks: &client.ConfigLocks{\n\t\t\tTags: make(map[string]struct{}),\n\t\t},\n\t}\n}\n\n// Recycle zeros the call option and put it to the pool.\nfunc (co *CallOptions) Recycle() {\n\tif co == nil {\n\t\treturn\n\t}\n\tco.configs = nil\n\tco.svr = nil\n\tco.RetryPolicy = retry.Policy{}\n\tco.Fallback = nil\n\tco.locks.Zero()\n\tcallOptionsPool.Put(co)\n}\n\n// Option is a series of options used at the beginning of a RPC call.\ntype Option struct {\n\tf func(o *CallOptions, di *strings.Builder)\n}\n\n// F returns the function of the option.\n// It's useful for creating streamcall.Option from existing callopt.Option\n// Note: not all callopt.Option(s) are available for stream clients.\nfunc (o Option) F() func(o *CallOptions, di *strings.Builder) {\n\treturn o.f\n}\n\n// NewOption returns a new Option with the given function.\n// It's useful for converting streamcall.Option back to a callopt.Option\nfunc NewOption(f func(o *CallOptions, di *strings.Builder)) Option {\n\treturn Option{f: f}\n}\n\n// WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\nfunc setInstance(svr remoteinfo.RemoteInfo, hostport string) error {\n\tif _, err := net.ResolveTCPAddr(\"tcp\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"tcp\", hostport, discovery.DefaultWeight, nil))\n\t} else if _, err := net.ResolveUnixAddr(\"unix\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"unix\", hostport, discovery.DefaultWeight, nil))\n\t} else {\n\t\treturn fmt.Errorf(\"invalid '%s'\", hostport)\n\t}\n\treturn nil\n}\n\n// WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithHTTPHost specifies host in http header(work when RPC over http).\nfunc WithHTTPHost(host string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\to.svr.SetTag(rpcinfo.HTTPHost, host)\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithConnectTimeout specifies the connection timeout for a RPC call.\n\n\n\n\n\n\n\n\n\n// WithTag sets the tags for service discovery for a RPC call.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryPolicy sets the retry policy for a RPC call.\n// Build retry.Policy with retry.BuildFailurePolicy or retry.BuildBackupRequest instead of building retry.Policy directly.\n// Demos are provided below:\n//\n//\t  demo1. call with failure retry policy, default retry error is Timeout\n//\t  \t`resp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildFailurePolicy(retry.NewFailurePolicy())))`\n//\t  demo2. call with backup request policy\n//\t  \t`bp := retry.NewBackupPolicy(10)\n//\t\t \tbp.WithMaxRetryTimes(1)\n//\t  \tresp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildBackupRequest(bp)))`\nfunc WithRetryPolicy(p retry.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif !p.Enable {\n\t\t\treturn\n\t\t}\n\t\tif p.Type == retry.BackupType {\n\t\t\tdi.WriteString(\"WithBackupRequest\")\n\t\t} else {\n\t\t\tdi.WriteString(\"WithFailureRetry\")\n\t\t}\n\t\to.RetryPolicy = p\n\t}}\n}\n\n// WithFallback is used to set the fallback policy for a RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\n\n\n\n\n\n\n\n\n\n\n// WithGRPCCompressor specifies the compressor for the GRPC frame payload.\n// Supported compressor names: identity, gzip\n// Custom compressors can be registered via `encoding.RegisterCompressor`\nfunc WithGRPCCompressor(compressorName string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithGRPCCompressor\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(compressorName)\n\t\tdi.WriteByte(')')\n\t\to.CompressorName = compressorName\n\t}}\n}\n\n// Apply applies call options to the rpcinfo.RPCConfig and internal.RemoteInfo of kitex client.\n// The return value records the name and arguments of each option.\n// This function is for internal purpose only.\nfunc Apply(cos []Option, cfg rpcinfo.MutableRPCConfig, svr remoteinfo.RemoteInfo, locks *client.ConfigLocks, httpResolver http.Resolver) (string, *CallOptions) {\n\tvar buf strings.Builder\n\tbuf.Grow(64)\n\n\tco := callOptionsPool.Get().(*CallOptions)\n\tco.configs = cfg\n\tco.svr = svr\n\tco.locks.Merge(locks)\n\tco.httpResolver = httpResolver\n\n\tbuf.WriteByte('[')\n\tfor i := range cos {\n\t\tif i > 0 {\n\t\t\tbuf.WriteByte(',')\n\t\t}\n\t\tcos[i].f(co, &buf)\n\t}\n\tbuf.WriteByte(']')\n\n\tco.locks.ApplyLocks(cfg, svr)\n\treturn buf.String(), co\n}\n\n```\n# Function Signature: func WithHostPort(hostport string) Option {\n# Natural Language Annotations: // WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithHostPort(hostport string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithHostPort\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(hostport)\n\t\tdi.WriteByte(')')\n\t\to.svr.SetInstance(discovery.NewInstance(\"tcp\", hostport, discovery.DefaultWeight, nil))\n\t}}\n}\n```", "  ```\nfunc WithHostPort(hostport string) Option {\n    return Option{func(o *CallOptions, di *strings.Builder) {\n        o.svr.SetTag(rpcinfo.HostPortInstance, hostport)\n    }}\n}\n```", "  ```\nfunc WithHostPort(hostport string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif _, err := net.ResolveTCPAddr(\"tcp\", hostport); err == nil {\n\t\t\to.svr.SetInstance(discovery.NewInstance(\"tcp\", hostport, discovery.DefaultWeight, nil))\n\t\t} else if _, err := net.ResolveUnixAddr(\"unix\", hostport); err == nil {\n\t\t\to.svr.SetInstance(discovery.NewInstance(\"unix\", hostport, discovery.DefaultWeight, nil))\n\t\t} else {\n\t\t\tdi.WriteString(fmt.Sprintf(\"WithHostPort(%s),invalid argument\", hostport))\n\t\t\treturn\n\t\t}\n\t}}\n}\n```"]}, "kitex-client/callopt/options.go-WithURL": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/callopt/options.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package callopt contains options that control the behavior of client on request level.\npackage callopt\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nvar callOptionsPool = sync.Pool{\n\tNew: newOptions,\n}\n\ntype CallOptions struct {\n\tconfigs      rpcinfo.MutableRPCConfig\n\tsvr          remoteinfo.RemoteInfo\n\tlocks        *client.ConfigLocks\n\thttpResolver http.Resolver\n\n\t// export field for using in client\n\tRetryPolicy    retry.Policy\n\tFallback       *fallback.Policy\n\tCompressorName string\n}\n\nfunc newOptions() interface{} {\n\treturn &CallOptions{\n\t\tlocks: &client.ConfigLocks{\n\t\t\tTags: make(map[string]struct{}),\n\t\t},\n\t}\n}\n\n// Recycle zeros the call option and put it to the pool.\nfunc (co *CallOptions) Recycle() {\n\tif co == nil {\n\t\treturn\n\t}\n\tco.configs = nil\n\tco.svr = nil\n\tco.RetryPolicy = retry.Policy{}\n\tco.Fallback = nil\n\tco.locks.Zero()\n\tcallOptionsPool.Put(co)\n}\n\n// Option is a series of options used at the beginning of a RPC call.\ntype Option struct {\n\tf func(o *CallOptions, di *strings.Builder)\n}\n\n// F returns the function of the option.\n// It's useful for creating streamcall.Option from existing callopt.Option\n// Note: not all callopt.Option(s) are available for stream clients.\nfunc (o Option) F() func(o *CallOptions, di *strings.Builder) {\n\treturn o.f\n}\n\n// NewOption returns a new Option with the given function.\n// It's useful for converting streamcall.Option back to a callopt.Option\nfunc NewOption(f func(o *CallOptions, di *strings.Builder)) Option {\n\treturn Option{f: f}\n}\n\n// WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\nfunc setInstance(svr remoteinfo.RemoteInfo, hostport string) error {\n\tif _, err := net.ResolveTCPAddr(\"tcp\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"tcp\", hostport, discovery.DefaultWeight, nil))\n\t} else if _, err := net.ResolveUnixAddr(\"unix\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"unix\", hostport, discovery.DefaultWeight, nil))\n\t} else {\n\t\treturn fmt.Errorf(\"invalid '%s'\", hostport)\n\t}\n\treturn nil\n}\n\n// WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithHTTPHost specifies host in http header(work when RPC over http).\nfunc WithHTTPHost(host string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\to.svr.SetTag(rpcinfo.HTTPHost, host)\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithConnectTimeout specifies the connection timeout for a RPC call.\n\n\n\n\n\n\n\n\n\n// WithTag sets the tags for service discovery for a RPC call.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryPolicy sets the retry policy for a RPC call.\n// Build retry.Policy with retry.BuildFailurePolicy or retry.BuildBackupRequest instead of building retry.Policy directly.\n// Demos are provided below:\n//\n//\t  demo1. call with failure retry policy, default retry error is Timeout\n//\t  \t`resp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildFailurePolicy(retry.NewFailurePolicy())))`\n//\t  demo2. call with backup request policy\n//\t  \t`bp := retry.NewBackupPolicy(10)\n//\t\t \tbp.WithMaxRetryTimes(1)\n//\t  \tresp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildBackupRequest(bp)))`\nfunc WithRetryPolicy(p retry.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif !p.Enable {\n\t\t\treturn\n\t\t}\n\t\tif p.Type == retry.BackupType {\n\t\t\tdi.WriteString(\"WithBackupRequest\")\n\t\t} else {\n\t\t\tdi.WriteString(\"WithFailureRetry\")\n\t\t}\n\t\to.RetryPolicy = p\n\t}}\n}\n\n// WithFallback is used to set the fallback policy for a RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\n\n\n\n\n\n\n\n\n\n\n// WithGRPCCompressor specifies the compressor for the GRPC frame payload.\n// Supported compressor names: identity, gzip\n// Custom compressors can be registered via `encoding.RegisterCompressor`\nfunc WithGRPCCompressor(compressorName string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithGRPCCompressor\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(compressorName)\n\t\tdi.WriteByte(')')\n\t\to.CompressorName = compressorName\n\t}}\n}\n\n// Apply applies call options to the rpcinfo.RPCConfig and internal.RemoteInfo of kitex client.\n// The return value records the name and arguments of each option.\n// This function is for internal purpose only.\nfunc Apply(cos []Option, cfg rpcinfo.MutableRPCConfig, svr remoteinfo.RemoteInfo, locks *client.ConfigLocks, httpResolver http.Resolver) (string, *CallOptions) {\n\tvar buf strings.Builder\n\tbuf.Grow(64)\n\n\tco := callOptionsPool.Get().(*CallOptions)\n\tco.configs = cfg\n\tco.svr = svr\n\tco.locks.Merge(locks)\n\tco.httpResolver = httpResolver\n\n\tbuf.WriteByte('[')\n\tfor i := range cos {\n\t\tif i > 0 {\n\t\t\tbuf.WriteByte(',')\n\t\t}\n\t\tcos[i].f(co, &buf)\n\t}\n\tbuf.WriteByte(']')\n\n\tco.locks.ApplyLocks(cfg, svr)\n\treturn buf.String(), co\n}\n\n```\n# Function Signature: func WithURL(url string) Option {\n# Natural Language Annotations: // WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithURL(url string) Option {\n    return Option{f: func(o *CallOptions, di *strings.Builder) {\n        if svr, _, err := http.ResolveHTTPServiceURL(url); err == nil {\n            o.svr.SetTag(rpcinfo.HTTPHost, svr.Host)\n        } else {\n            panic(err)\n        }\n    }}\n}\n```", "  ```\nfunc WithURL(url string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\tu, err := rpcinfo.ResolveURI(url)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tif u.Type == discovery.TCP {\n\t\t\to.svr.SetInstance(discovery.NewInstance(u.Format(), u.Addr, u.Weight, u.Labels))\n\t\t} else {\n\t\t\to.svr.SetInstance(discovery.NewInstance(u.Format(), u.Addr, u.Weight, u.Labels))\n\t\t}\n\t}}\n}\n```", "  ```\n\nfunc WithURL(url string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithURL\")\n\t\thostport, err := resolveAddress(url)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tif err := setInstance(o.svr, hostport); err != nil {\n\t\t\treturn\n\t\t}\n\t\to.svr.SetTag(rpcinfo.URL, url)\n\t}}\n}\n\nfunc resolveAddress(url string) (string, error) {\n\tu, err := url.Parse(url)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tswitch u.Scheme {\n\tcase \"tcp\":\n\t\th := u.Host\n\t\tif h == \"\" {\n\t\t\th = u.Hostname()\n\t\t}\n\t\tif h == \"\" {\n\t\t\treturn \"\", fmt.Errorf(\"invalid url %s\", url)\n\t\t}\n\t\treturn fmt.Sprintf(\"%s:%d\", h, u.Port()), nil\n\tcase \"unix\":\n\t\treturn u.Path, nil\n\tdefault:\n\t\treturn \"\", fmt.Errorf(\"unsupported scheme %s in url %s\", u.Scheme, url)\n\t}\n}\n```"]}, "kitex-client/callopt/options.go-WithRPCTimeout": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/callopt/options.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package callopt contains options that control the behavior of client on request level.\npackage callopt\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nvar callOptionsPool = sync.Pool{\n\tNew: newOptions,\n}\n\ntype CallOptions struct {\n\tconfigs      rpcinfo.MutableRPCConfig\n\tsvr          remoteinfo.RemoteInfo\n\tlocks        *client.ConfigLocks\n\thttpResolver http.Resolver\n\n\t// export field for using in client\n\tRetryPolicy    retry.Policy\n\tFallback       *fallback.Policy\n\tCompressorName string\n}\n\nfunc newOptions() interface{} {\n\treturn &CallOptions{\n\t\tlocks: &client.ConfigLocks{\n\t\t\tTags: make(map[string]struct{}),\n\t\t},\n\t}\n}\n\n// Recycle zeros the call option and put it to the pool.\nfunc (co *CallOptions) Recycle() {\n\tif co == nil {\n\t\treturn\n\t}\n\tco.configs = nil\n\tco.svr = nil\n\tco.RetryPolicy = retry.Policy{}\n\tco.Fallback = nil\n\tco.locks.Zero()\n\tcallOptionsPool.Put(co)\n}\n\n// Option is a series of options used at the beginning of a RPC call.\ntype Option struct {\n\tf func(o *CallOptions, di *strings.Builder)\n}\n\n// F returns the function of the option.\n// It's useful for creating streamcall.Option from existing callopt.Option\n// Note: not all callopt.Option(s) are available for stream clients.\nfunc (o Option) F() func(o *CallOptions, di *strings.Builder) {\n\treturn o.f\n}\n\n// NewOption returns a new Option with the given function.\n// It's useful for converting streamcall.Option back to a callopt.Option\nfunc NewOption(f func(o *CallOptions, di *strings.Builder)) Option {\n\treturn Option{f: f}\n}\n\n// WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\nfunc setInstance(svr remoteinfo.RemoteInfo, hostport string) error {\n\tif _, err := net.ResolveTCPAddr(\"tcp\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"tcp\", hostport, discovery.DefaultWeight, nil))\n\t} else if _, err := net.ResolveUnixAddr(\"unix\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"unix\", hostport, discovery.DefaultWeight, nil))\n\t} else {\n\t\treturn fmt.Errorf(\"invalid '%s'\", hostport)\n\t}\n\treturn nil\n}\n\n// WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithHTTPHost specifies host in http header(work when RPC over http).\nfunc WithHTTPHost(host string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\to.svr.SetTag(rpcinfo.HTTPHost, host)\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithConnectTimeout specifies the connection timeout for a RPC call.\n\n\n\n\n\n\n\n\n\n// WithTag sets the tags for service discovery for a RPC call.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryPolicy sets the retry policy for a RPC call.\n// Build retry.Policy with retry.BuildFailurePolicy or retry.BuildBackupRequest instead of building retry.Policy directly.\n// Demos are provided below:\n//\n//\t  demo1. call with failure retry policy, default retry error is Timeout\n//\t  \t`resp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildFailurePolicy(retry.NewFailurePolicy())))`\n//\t  demo2. call with backup request policy\n//\t  \t`bp := retry.NewBackupPolicy(10)\n//\t\t \tbp.WithMaxRetryTimes(1)\n//\t  \tresp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildBackupRequest(bp)))`\nfunc WithRetryPolicy(p retry.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif !p.Enable {\n\t\t\treturn\n\t\t}\n\t\tif p.Type == retry.BackupType {\n\t\t\tdi.WriteString(\"WithBackupRequest\")\n\t\t} else {\n\t\t\tdi.WriteString(\"WithFailureRetry\")\n\t\t}\n\t\to.RetryPolicy = p\n\t}}\n}\n\n// WithFallback is used to set the fallback policy for a RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\n\n\n\n\n\n\n\n\n\n\n// WithGRPCCompressor specifies the compressor for the GRPC frame payload.\n// Supported compressor names: identity, gzip\n// Custom compressors can be registered via `encoding.RegisterCompressor`\nfunc WithGRPCCompressor(compressorName string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithGRPCCompressor\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(compressorName)\n\t\tdi.WriteByte(')')\n\t\to.CompressorName = compressorName\n\t}}\n}\n\n// Apply applies call options to the rpcinfo.RPCConfig and internal.RemoteInfo of kitex client.\n// The return value records the name and arguments of each option.\n// This function is for internal purpose only.\nfunc Apply(cos []Option, cfg rpcinfo.MutableRPCConfig, svr remoteinfo.RemoteInfo, locks *client.ConfigLocks, httpResolver http.Resolver) (string, *CallOptions) {\n\tvar buf strings.Builder\n\tbuf.Grow(64)\n\n\tco := callOptionsPool.Get().(*CallOptions)\n\tco.configs = cfg\n\tco.svr = svr\n\tco.locks.Merge(locks)\n\tco.httpResolver = httpResolver\n\n\tbuf.WriteByte('[')\n\tfor i := range cos {\n\t\tif i > 0 {\n\t\t\tbuf.WriteByte(',')\n\t\t}\n\t\tcos[i].f(co, &buf)\n\t}\n\tbuf.WriteByte(']')\n\n\tco.locks.ApplyLocks(cfg, svr)\n\treturn buf.String(), co\n}\n\n```\n# Function Signature: func WithRPCTimeout(d time.Duration) Option {\n# Natural Language Annotations: // WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithRPCTimeout(\")\n\t\tdi.WriteString(d.String())\n\t\tdi.WriteByte(')')\n\t\to.svr.SetTag(rpcinfo.RPCTimeout, d.String())\n\t}}\n}\n```", "  ```\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif d == 0 {\n\t\t\treturn\n\t\t}\n\t\tdi.WriteString(\"WithRPCTimeout\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(fmt.Sprintf(\"%dms\", d.Milliseconds()))\n\t\tdi.WriteByte(')')\n\t\to.configs.WithTimeout(d)\n\t}}\n}\n```", "  ```\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\tif d.Seconds() == 0 {\n\t\t\treturn\n\t\t}\n\t\to.configs.CallTimeout.Store(d)\n\t\tdi.WriteString(\"WithRPCTimeout\")\n\t\tdi.WriteString(d.String())\n\t}}\n}\n```"]}, "kitex-client/callopt/options.go-WithConnectTimeout": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/callopt/options.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package callopt contains options that control the behavior of client on request level.\npackage callopt\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nvar callOptionsPool = sync.Pool{\n\tNew: newOptions,\n}\n\ntype CallOptions struct {\n\tconfigs      rpcinfo.MutableRPCConfig\n\tsvr          remoteinfo.RemoteInfo\n\tlocks        *client.ConfigLocks\n\thttpResolver http.Resolver\n\n\t// export field for using in client\n\tRetryPolicy    retry.Policy\n\tFallback       *fallback.Policy\n\tCompressorName string\n}\n\nfunc newOptions() interface{} {\n\treturn &CallOptions{\n\t\tlocks: &client.ConfigLocks{\n\t\t\tTags: make(map[string]struct{}),\n\t\t},\n\t}\n}\n\n// Recycle zeros the call option and put it to the pool.\nfunc (co *CallOptions) Recycle() {\n\tif co == nil {\n\t\treturn\n\t}\n\tco.configs = nil\n\tco.svr = nil\n\tco.RetryPolicy = retry.Policy{}\n\tco.Fallback = nil\n\tco.locks.Zero()\n\tcallOptionsPool.Put(co)\n}\n\n// Option is a series of options used at the beginning of a RPC call.\ntype Option struct {\n\tf func(o *CallOptions, di *strings.Builder)\n}\n\n// F returns the function of the option.\n// It's useful for creating streamcall.Option from existing callopt.Option\n// Note: not all callopt.Option(s) are available for stream clients.\nfunc (o Option) F() func(o *CallOptions, di *strings.Builder) {\n\treturn o.f\n}\n\n// NewOption returns a new Option with the given function.\n// It's useful for converting streamcall.Option back to a callopt.Option\nfunc NewOption(f func(o *CallOptions, di *strings.Builder)) Option {\n\treturn Option{f: f}\n}\n\n// WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\nfunc setInstance(svr remoteinfo.RemoteInfo, hostport string) error {\n\tif _, err := net.ResolveTCPAddr(\"tcp\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"tcp\", hostport, discovery.DefaultWeight, nil))\n\t} else if _, err := net.ResolveUnixAddr(\"unix\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"unix\", hostport, discovery.DefaultWeight, nil))\n\t} else {\n\t\treturn fmt.Errorf(\"invalid '%s'\", hostport)\n\t}\n\treturn nil\n}\n\n// WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithHTTPHost specifies host in http header(work when RPC over http).\nfunc WithHTTPHost(host string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\to.svr.SetTag(rpcinfo.HTTPHost, host)\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithConnectTimeout specifies the connection timeout for a RPC call.\n\n\n\n\n\n\n\n\n\n// WithTag sets the tags for service discovery for a RPC call.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryPolicy sets the retry policy for a RPC call.\n// Build retry.Policy with retry.BuildFailurePolicy or retry.BuildBackupRequest instead of building retry.Policy directly.\n// Demos are provided below:\n//\n//\t  demo1. call with failure retry policy, default retry error is Timeout\n//\t  \t`resp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildFailurePolicy(retry.NewFailurePolicy())))`\n//\t  demo2. call with backup request policy\n//\t  \t`bp := retry.NewBackupPolicy(10)\n//\t\t \tbp.WithMaxRetryTimes(1)\n//\t  \tresp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildBackupRequest(bp)))`\nfunc WithRetryPolicy(p retry.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif !p.Enable {\n\t\t\treturn\n\t\t}\n\t\tif p.Type == retry.BackupType {\n\t\t\tdi.WriteString(\"WithBackupRequest\")\n\t\t} else {\n\t\t\tdi.WriteString(\"WithFailureRetry\")\n\t\t}\n\t\to.RetryPolicy = p\n\t}}\n}\n\n// WithFallback is used to set the fallback policy for a RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\n\n\n\n\n\n\n\n\n\n\n// WithGRPCCompressor specifies the compressor for the GRPC frame payload.\n// Supported compressor names: identity, gzip\n// Custom compressors can be registered via `encoding.RegisterCompressor`\nfunc WithGRPCCompressor(compressorName string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithGRPCCompressor\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(compressorName)\n\t\tdi.WriteByte(')')\n\t\to.CompressorName = compressorName\n\t}}\n}\n\n// Apply applies call options to the rpcinfo.RPCConfig and internal.RemoteInfo of kitex client.\n// The return value records the name and arguments of each option.\n// This function is for internal purpose only.\nfunc Apply(cos []Option, cfg rpcinfo.MutableRPCConfig, svr remoteinfo.RemoteInfo, locks *client.ConfigLocks, httpResolver http.Resolver) (string, *CallOptions) {\n\tvar buf strings.Builder\n\tbuf.Grow(64)\n\n\tco := callOptionsPool.Get().(*CallOptions)\n\tco.configs = cfg\n\tco.svr = svr\n\tco.locks.Merge(locks)\n\tco.httpResolver = httpResolver\n\n\tbuf.WriteByte('[')\n\tfor i := range cos {\n\t\tif i > 0 {\n\t\t\tbuf.WriteByte(',')\n\t\t}\n\t\tcos[i].f(co, &buf)\n\t}\n\tbuf.WriteByte(']')\n\n\tco.locks.ApplyLocks(cfg, svr)\n\treturn buf.String(), co\n}\n\n```\n# Function Signature: func WithConnectTimeout(d time.Duration) Option {\n# Natural Language Annotations: // WithConnectTimeout specifies the connection timeout for a RPC call.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithConnectTimeout\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(d.String())\n\t\tdi.WriteByte(')')\n\t}}\n}\n```", "  ```\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{\n\t\tf: func(o *CallOptions, di *strings.Builder) {\n\t\t\tdi.WriteString(\"WithConnectTimeout\")\n\t\t\tdi.WriteByte('(')\n\t\t\tdi.WriteString(d.String())\n\t\t\tdi.WriteByte(')')\n\t\t\to.configs.SetConnectTimeout(d)\n\t\t},\n\t}\n}\n```", "  ```\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithConnectTimeout\")\n\t\tdi.WriteByte('[')\n\t\tdi.WriteString(fmt.Sprint(d))\n\t\tdi.WriteByte(']')\n\t\to.svr.SetConnectTimeout(d)\n\t}}\n}\n```"]}, "kitex-client/callopt/options.go-WithTag": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/callopt/options.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package callopt contains options that control the behavior of client on request level.\npackage callopt\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nvar callOptionsPool = sync.Pool{\n\tNew: newOptions,\n}\n\ntype CallOptions struct {\n\tconfigs      rpcinfo.MutableRPCConfig\n\tsvr          remoteinfo.RemoteInfo\n\tlocks        *client.ConfigLocks\n\thttpResolver http.Resolver\n\n\t// export field for using in client\n\tRetryPolicy    retry.Policy\n\tFallback       *fallback.Policy\n\tCompressorName string\n}\n\nfunc newOptions() interface{} {\n\treturn &CallOptions{\n\t\tlocks: &client.ConfigLocks{\n\t\t\tTags: make(map[string]struct{}),\n\t\t},\n\t}\n}\n\n// Recycle zeros the call option and put it to the pool.\nfunc (co *CallOptions) Recycle() {\n\tif co == nil {\n\t\treturn\n\t}\n\tco.configs = nil\n\tco.svr = nil\n\tco.RetryPolicy = retry.Policy{}\n\tco.Fallback = nil\n\tco.locks.Zero()\n\tcallOptionsPool.Put(co)\n}\n\n// Option is a series of options used at the beginning of a RPC call.\ntype Option struct {\n\tf func(o *CallOptions, di *strings.Builder)\n}\n\n// F returns the function of the option.\n// It's useful for creating streamcall.Option from existing callopt.Option\n// Note: not all callopt.Option(s) are available for stream clients.\nfunc (o Option) F() func(o *CallOptions, di *strings.Builder) {\n\treturn o.f\n}\n\n// NewOption returns a new Option with the given function.\n// It's useful for converting streamcall.Option back to a callopt.Option\nfunc NewOption(f func(o *CallOptions, di *strings.Builder)) Option {\n\treturn Option{f: f}\n}\n\n// WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\nfunc setInstance(svr remoteinfo.RemoteInfo, hostport string) error {\n\tif _, err := net.ResolveTCPAddr(\"tcp\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"tcp\", hostport, discovery.DefaultWeight, nil))\n\t} else if _, err := net.ResolveUnixAddr(\"unix\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"unix\", hostport, discovery.DefaultWeight, nil))\n\t} else {\n\t\treturn fmt.Errorf(\"invalid '%s'\", hostport)\n\t}\n\treturn nil\n}\n\n// WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithHTTPHost specifies host in http header(work when RPC over http).\nfunc WithHTTPHost(host string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\to.svr.SetTag(rpcinfo.HTTPHost, host)\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithConnectTimeout specifies the connection timeout for a RPC call.\n\n\n\n\n\n\n\n\n\n// WithTag sets the tags for service discovery for a RPC call.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryPolicy sets the retry policy for a RPC call.\n// Build retry.Policy with retry.BuildFailurePolicy or retry.BuildBackupRequest instead of building retry.Policy directly.\n// Demos are provided below:\n//\n//\t  demo1. call with failure retry policy, default retry error is Timeout\n//\t  \t`resp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildFailurePolicy(retry.NewFailurePolicy())))`\n//\t  demo2. call with backup request policy\n//\t  \t`bp := retry.NewBackupPolicy(10)\n//\t\t \tbp.WithMaxRetryTimes(1)\n//\t  \tresp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildBackupRequest(bp)))`\nfunc WithRetryPolicy(p retry.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif !p.Enable {\n\t\t\treturn\n\t\t}\n\t\tif p.Type == retry.BackupType {\n\t\t\tdi.WriteString(\"WithBackupRequest\")\n\t\t} else {\n\t\t\tdi.WriteString(\"WithFailureRetry\")\n\t\t}\n\t\to.RetryPolicy = p\n\t}}\n}\n\n// WithFallback is used to set the fallback policy for a RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\n\n\n\n\n\n\n\n\n\n\n// WithGRPCCompressor specifies the compressor for the GRPC frame payload.\n// Supported compressor names: identity, gzip\n// Custom compressors can be registered via `encoding.RegisterCompressor`\nfunc WithGRPCCompressor(compressorName string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithGRPCCompressor\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(compressorName)\n\t\tdi.WriteByte(')')\n\t\to.CompressorName = compressorName\n\t}}\n}\n\n// Apply applies call options to the rpcinfo.RPCConfig and internal.RemoteInfo of kitex client.\n// The return value records the name and arguments of each option.\n// This function is for internal purpose only.\nfunc Apply(cos []Option, cfg rpcinfo.MutableRPCConfig, svr remoteinfo.RemoteInfo, locks *client.ConfigLocks, httpResolver http.Resolver) (string, *CallOptions) {\n\tvar buf strings.Builder\n\tbuf.Grow(64)\n\n\tco := callOptionsPool.Get().(*CallOptions)\n\tco.configs = cfg\n\tco.svr = svr\n\tco.locks.Merge(locks)\n\tco.httpResolver = httpResolver\n\n\tbuf.WriteByte('[')\n\tfor i := range cos {\n\t\tif i > 0 {\n\t\t\tbuf.WriteByte(',')\n\t\t}\n\t\tcos[i].f(co, &buf)\n\t}\n\tbuf.WriteByte(']')\n\n\tco.locks.ApplyLocks(cfg, svr)\n\treturn buf.String(), co\n}\n\n```\n# Function Signature: func WithTag(key, val string) Option {\n# Natural Language Annotations: // WithTag sets the tags for service discovery for a RPC call.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithTag(key, val string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\to.configs.SetTag(key, val)\n\t}}\n}\n```", "  ```\nfunc WithTag(key, val string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\to.configs.SetTag(key, val)\n\t\to.svr.SetTag(key, val)\n\t\tdi.WriteString(\"WithTag(\")\n\t\tdi.WriteString(key)\n\t\tdi.WriteString(\", \")\n\t\tdi.WriteString(val)\n\t\tdi.WriteString(\")\")\n\t}}\n}\n```", "  ```\nfunc WithTag(key, val string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\to.svr.SetTag(rpcinfo.ServiceDiscoveryTags, key, val)\n\t}}\n}\n```"]}, "kitex-client/callopt/options.go-WithFallback": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/callopt/options.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package callopt contains options that control the behavior of client on request level.\npackage callopt\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nvar callOptionsPool = sync.Pool{\n\tNew: newOptions,\n}\n\ntype CallOptions struct {\n\tconfigs      rpcinfo.MutableRPCConfig\n\tsvr          remoteinfo.RemoteInfo\n\tlocks        *client.ConfigLocks\n\thttpResolver http.Resolver\n\n\t// export field for using in client\n\tRetryPolicy    retry.Policy\n\tFallback       *fallback.Policy\n\tCompressorName string\n}\n\nfunc newOptions() interface{} {\n\treturn &CallOptions{\n\t\tlocks: &client.ConfigLocks{\n\t\t\tTags: make(map[string]struct{}),\n\t\t},\n\t}\n}\n\n// Recycle zeros the call option and put it to the pool.\nfunc (co *CallOptions) Recycle() {\n\tif co == nil {\n\t\treturn\n\t}\n\tco.configs = nil\n\tco.svr = nil\n\tco.RetryPolicy = retry.Policy{}\n\tco.Fallback = nil\n\tco.locks.Zero()\n\tcallOptionsPool.Put(co)\n}\n\n// Option is a series of options used at the beginning of a RPC call.\ntype Option struct {\n\tf func(o *CallOptions, di *strings.Builder)\n}\n\n// F returns the function of the option.\n// It's useful for creating streamcall.Option from existing callopt.Option\n// Note: not all callopt.Option(s) are available for stream clients.\nfunc (o Option) F() func(o *CallOptions, di *strings.Builder) {\n\treturn o.f\n}\n\n// NewOption returns a new Option with the given function.\n// It's useful for converting streamcall.Option back to a callopt.Option\nfunc NewOption(f func(o *CallOptions, di *strings.Builder)) Option {\n\treturn Option{f: f}\n}\n\n// WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\nfunc setInstance(svr remoteinfo.RemoteInfo, hostport string) error {\n\tif _, err := net.ResolveTCPAddr(\"tcp\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"tcp\", hostport, discovery.DefaultWeight, nil))\n\t} else if _, err := net.ResolveUnixAddr(\"unix\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"unix\", hostport, discovery.DefaultWeight, nil))\n\t} else {\n\t\treturn fmt.Errorf(\"invalid '%s'\", hostport)\n\t}\n\treturn nil\n}\n\n// WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithHTTPHost specifies host in http header(work when RPC over http).\nfunc WithHTTPHost(host string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\to.svr.SetTag(rpcinfo.HTTPHost, host)\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithConnectTimeout specifies the connection timeout for a RPC call.\n\n\n\n\n\n\n\n\n\n// WithTag sets the tags for service discovery for a RPC call.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryPolicy sets the retry policy for a RPC call.\n// Build retry.Policy with retry.BuildFailurePolicy or retry.BuildBackupRequest instead of building retry.Policy directly.\n// Demos are provided below:\n//\n//\t  demo1. call with failure retry policy, default retry error is Timeout\n//\t  \t`resp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildFailurePolicy(retry.NewFailurePolicy())))`\n//\t  demo2. call with backup request policy\n//\t  \t`bp := retry.NewBackupPolicy(10)\n//\t\t \tbp.WithMaxRetryTimes(1)\n//\t  \tresp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildBackupRequest(bp)))`\nfunc WithRetryPolicy(p retry.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif !p.Enable {\n\t\t\treturn\n\t\t}\n\t\tif p.Type == retry.BackupType {\n\t\t\tdi.WriteString(\"WithBackupRequest\")\n\t\t} else {\n\t\t\tdi.WriteString(\"WithFailureRetry\")\n\t\t}\n\t\to.RetryPolicy = p\n\t}}\n}\n\n// WithFallback is used to set the fallback policy for a RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\n\n\n\n\n\n\n\n\n\n\n// WithGRPCCompressor specifies the compressor for the GRPC frame payload.\n// Supported compressor names: identity, gzip\n// Custom compressors can be registered via `encoding.RegisterCompressor`\nfunc WithGRPCCompressor(compressorName string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithGRPCCompressor\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(compressorName)\n\t\tdi.WriteByte(')')\n\t\to.CompressorName = compressorName\n\t}}\n}\n\n// Apply applies call options to the rpcinfo.RPCConfig and internal.RemoteInfo of kitex client.\n// The return value records the name and arguments of each option.\n// This function is for internal purpose only.\nfunc Apply(cos []Option, cfg rpcinfo.MutableRPCConfig, svr remoteinfo.RemoteInfo, locks *client.ConfigLocks, httpResolver http.Resolver) (string, *CallOptions) {\n\tvar buf strings.Builder\n\tbuf.Grow(64)\n\n\tco := callOptionsPool.Get().(*CallOptions)\n\tco.configs = cfg\n\tco.svr = svr\n\tco.locks.Merge(locks)\n\tco.httpResolver = httpResolver\n\n\tbuf.WriteByte('[')\n\tfor i := range cos {\n\t\tif i > 0 {\n\t\t\tbuf.WriteByte(',')\n\t\t}\n\t\tcos[i].f(co, &buf)\n\t}\n\tbuf.WriteByte(']')\n\n\tco.locks.ApplyLocks(cfg, svr)\n\treturn buf.String(), co\n}\n\n```\n# Function Signature: func WithFallback(fb *fallback.Policy) Option {\n# Natural Language Annotations: // WithFallback is used to set the fallback policy for a RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithFallback(fb *fallback.Policy) Option {\n    return Option{\n        f: func(o *CallOptions, di *strings.Builder) {\n            di.WriteString(\"WithFallback\")\n            o.Fallback = fb\n        },\n    }\n}\n```", "  ```\nfunc WithFallback(fb *fallback.Policy) Option {\n    return Option{\n        f: func(o *CallOptions, di *strings.Builder) {\n            di.WriteString(\"WithFallback\")\n            di.WriteByte('(')\n            di.WriteString(fb.String())\n            di.WriteByte(')')\n            o.Fallback = fb\n        },\n    }\n}\n```", "  ```\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif fb == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.WriteString(\"WithFallback\")\n\t\tdi.WriteByte('[')\n\t\tdi.WriteString(fb.Type)\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(fmt.Sprintf(\"%p\", fb.Factory))\n\t\tdi.WriteByte(')')\n\t\tdi.WriteByte(']')\n\t\to.Fallback = fb\n\t}}\n}\n```"]}, "kitex-server/option_advanced.go-WithServerBasicInfo": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option_advanced.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package server defines the Options of server\npackage server\n\n// Notice!! This file defines the advanced Options of client, normal user should not use it.\n// It is used for customized extension.\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/acl\"\n\t\"github.com/cloudwego/kitex/pkg/diagnosis\"\n\t\"github.com/cloudwego/kitex/pkg/generic\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/profiler\"\n\t\"github.com/cloudwego/kitex/pkg/proxy\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// WithServerBasicInfo provides initial information for client endpoint in RPCInfo.\n\n\n\n\n\n\n\n\n\n\n// WithDiagnosisService sets the diagnosis service for gathering debug information.\nfunc WithDiagnosisService(ds diagnosis.Service) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDiagnosisService(%+v)\", ds))\n\n\t\to.DebugService = ds\n\t}}\n}\n\n// WithACLRules sets the ACL rules.\nfunc WithACLRules(rules ...acl.RejectFunc) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar names []string\n\t\tfor _, r := range rules {\n\t\t\tnames = append(names, utils.GetFuncName(r))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithACLRules(%+v)\", names))\n\n\t\to.ACLRules = append(o.ACLRules, rules...)\n\t}}\n}\n\n// WithMetaHandler adds a MetaHandler.\nfunc WithMetaHandler(h remote.MetaHandler) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMetaHandler(%T)\", h))\n\n\t\to.MetaHandlers = append(o.MetaHandlers, h)\n\t}}\n}\n\n// WithProxy sets the backward Proxy for server.\nfunc WithProxy(p proxy.ReverseProxy) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithProxy(%T)\", p))\n\n\t\tif o.Proxy != nil {\n\t\t\tpanic(fmt.Errorf(\"reassignment of Proxy is not allowed: %T -> %T\", o.Proxy, p))\n\t\t}\n\t\to.Proxy = p\n\t}}\n}\n\n// WithTransHandlerFactory sets the TransHandlerFactory for server.\nfunc WithTransHandlerFactory(f remote.ServerTransHandlerFactory) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithTransHandlerFactory(%T)\", f))\n\n\t\to.RemoteOpt.SvrHandlerFactory = f\n\t}}\n}\n\n// WithTransServerFactory sets the TransServerFactory for server.\nfunc WithTransServerFactory(f remote.TransServerFactory) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithTransServerFactory(%T)\", f))\n\n\t\to.RemoteOpt.TransServerFactory = f\n\t}}\n}\n\n// WithLimitReporter do report when server limit happen\nfunc WithLimitReporter(r limiter.LimitReporter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithLimitReporter(%T)\", r))\n\n\t\to.Limit.LimitReporter = r\n\t}}\n}\n\n// WithGeneric set Generic type for generic call\nfunc WithGeneric(g generic.Generic) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithGeneric(%T)\", g))\n\n\t\tif g == nil {\n\t\t\tpanic(\"invalid Generic: nil\")\n\t\t}\n\t\to.RemoteOpt.PayloadCodec = g.PayloadCodec()\n\t}}\n}\n\n// WithErrorHandler sets the error handler.\nfunc WithErrorHandler(f func(context.Context, error) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithErrorHandler(%+v)\", utils.GetFuncName(f)))\n\n\t\to.ErrHandle = f\n\t}}\n}\n\n// WithBoundHandler adds remote.BoundHandler for server.\nfunc WithBoundHandler(h remote.BoundHandler) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"AddBoundHandler(%T)\", h))\n\n\t\texist := false\n\t\tswitch handler := h.(type) {\n\t\tcase remote.InboundHandler:\n\t\t\tfor _, inboundHandler := range o.RemoteOpt.Inbounds {\n\t\t\t\tif reflect.DeepEqual(inboundHandler, handler) {\n\t\t\t\t\texist = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase remote.OutboundHandler:\n\t\t\tfor _, outboundHandler := range o.RemoteOpt.Outbounds {\n\t\t\t\tif reflect.DeepEqual(outboundHandler, handler) {\n\t\t\t\t\texist = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// prevent duplication\n\t\tif !exist {\n\t\t\tdoAddBoundHandler(h, o.RemoteOpt)\n\t\t} else {\n\t\t\tklog.Warnf(\"KITEX: BoundHandler already exists, BoundHandler=%v\", h)\n\t\t}\n\t}}\n}\n\n// WithExitSignal adds ExitSignal for server.\nfunc WithExitSignal(f func() <-chan error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"AddExitSignal(%+v)\", utils.GetFuncName(f)))\n\t\to.ExitSignal = f\n\t}}\n}\n\n// WithListener sets the listener for server, the priority is higher than WithServiceAddr\nfunc WithListener(ln net.Listener) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithListener(%+v)\", ln))\n\n\t\to.RemoteOpt.Listener = ln\n\t}}\n}\n\n// WithReusePort sets SO_REUSEPORT on listener, it is only used with Option `WithServiceAddr`.\n// It won't take effect when listener is specified by WithListener.\nfunc WithReusePort(reuse bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReusePort(%+v)\", reuse))\n\n\t\to.RemoteOpt.ReusePort = reuse\n\t}}\n}\n\n// WithSupportedTransportsFunc sets a function which converts supported transports from server option.\n\n\n\n\n\n\n\n\n\n// WithProfiler set a profiler to server.\nfunc WithProfiler(pc profiler.Profiler) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithProfiler(%T{%+v})\", pc, pc))\n\t\to.RemoteOpt.Profiler = pc\n\t}}\n}\n\n// WithProfilerTransInfoTagging set transinfo tagging function to profiler\n// TransInfoTagging extracting tags after TransInfo decoded but before message decoded.\n// At this stage, we can only get msg.TransInfo() and the real message payload is not decoded yet.\n// If upstream is not use TTHeader protocol, we can get nothing here.\n// So if you don't very care about the accuracy of statistics, we recommend to use WithProfilerMessageTagging to extract your custom tags.\nfunc WithProfilerTransInfoTagging(tagging remote.TransInfoTagging) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithProfilerTransInfoTagging(%+v)\", utils.GetFuncName(tagging)))\n\t\tinterTagging := o.RemoteOpt.ProfilerTransInfoTagging\n\t\tif interTagging == nil {\n\t\t\to.RemoteOpt.ProfilerTransInfoTagging = tagging\n\t\t\treturn\n\t\t}\n\t\to.RemoteOpt.ProfilerTransInfoTagging = func(ctx context.Context, msg remote.Message) (context.Context, []string) {\n\t\t\tc, t := tagging(ctx, msg)\n\t\t\tc2, t2 := interTagging(c, msg)\n\t\t\treturn c2, append(t, t2...)\n\t\t}\n\t}}\n}\n\n// WithProfilerMessageTagging set message tagging function to profiler\n// MessageTagging extracting tags after whole decode process finished.\n// At this stage, we can get the rpcInfo from ctx, and full complete message.\nfunc WithProfilerMessageTagging(tagging remote.MessageTagging) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithProfilerMessageTagging(%+v)\", utils.GetFuncName(tagging)))\n\t\tinterTagging := o.RemoteOpt.ProfilerMessageTagging\n\t\tif interTagging == nil {\n\t\t\to.RemoteOpt.ProfilerMessageTagging = tagging\n\t\t\treturn\n\t\t}\n\t\to.RemoteOpt.ProfilerMessageTagging = func(ctx context.Context, msg remote.Message) (context.Context, []string) {\n\t\t\tc, t := tagging(ctx, msg)\n\t\t\tc2, t2 := interTagging(c, msg)\n\t\t\treturn c2, append(t, t2...)\n\t\t}\n\t}}\n}\n\n```\n# Function Signature: func WithServerBasicInfo(ebi *rpcinfo.EndpointBasicInfo) Option {\n# Natural Language Annotations: // WithServerBasicInfo provides initial information for client endpoint in RPCInfo.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithServerBasicInfo(ebi *rpcinfo.EndpointBasicInfo) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithServerBasicInfo(%+v)\", ebi))\n\n\t\to.RPCInfo.ServerBasicInfo = ebi\n\t}}\n}\n```", "  ```\nfunc WithServerBasicInfo(ebi *rpcinfo.EndpointBasicInfo) Option {\n    return Option{F: func(o *internal_server.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithServerBasicInfo(%+v)\", ebi))\n\n        o.EBI = ebi\n    }}\n}\n```", "  ```\nfunc WithServerBasicInfo(ebi *rpcinfo.EndpointBasicInfo) Option {\n    return Option{F: func(o *internal_server.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithServerBasicInfo(%+v)\", ebi))\n\n        o.BasicEndpbInfo = ebi\n    }}\n}\n```"]}, "kitex-server/option_advanced.go-WithSupportedTransportsFunc": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option_advanced.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package server defines the Options of server\npackage server\n\n// Notice!! This file defines the advanced Options of client, normal user should not use it.\n// It is used for customized extension.\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/acl\"\n\t\"github.com/cloudwego/kitex/pkg/diagnosis\"\n\t\"github.com/cloudwego/kitex/pkg/generic\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/profiler\"\n\t\"github.com/cloudwego/kitex/pkg/proxy\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// WithServerBasicInfo provides initial information for client endpoint in RPCInfo.\n\n\n\n\n\n\n\n\n\n\n// WithDiagnosisService sets the diagnosis service for gathering debug information.\nfunc WithDiagnosisService(ds diagnosis.Service) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDiagnosisService(%+v)\", ds))\n\n\t\to.DebugService = ds\n\t}}\n}\n\n// WithACLRules sets the ACL rules.\nfunc WithACLRules(rules ...acl.RejectFunc) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar names []string\n\t\tfor _, r := range rules {\n\t\t\tnames = append(names, utils.GetFuncName(r))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithACLRules(%+v)\", names))\n\n\t\to.ACLRules = append(o.ACLRules, rules...)\n\t}}\n}\n\n// WithMetaHandler adds a MetaHandler.\nfunc WithMetaHandler(h remote.MetaHandler) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMetaHandler(%T)\", h))\n\n\t\to.MetaHandlers = append(o.MetaHandlers, h)\n\t}}\n}\n\n// WithProxy sets the backward Proxy for server.\nfunc WithProxy(p proxy.ReverseProxy) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithProxy(%T)\", p))\n\n\t\tif o.Proxy != nil {\n\t\t\tpanic(fmt.Errorf(\"reassignment of Proxy is not allowed: %T -> %T\", o.Proxy, p))\n\t\t}\n\t\to.Proxy = p\n\t}}\n}\n\n// WithTransHandlerFactory sets the TransHandlerFactory for server.\nfunc WithTransHandlerFactory(f remote.ServerTransHandlerFactory) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithTransHandlerFactory(%T)\", f))\n\n\t\to.RemoteOpt.SvrHandlerFactory = f\n\t}}\n}\n\n// WithTransServerFactory sets the TransServerFactory for server.\nfunc WithTransServerFactory(f remote.TransServerFactory) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithTransServerFactory(%T)\", f))\n\n\t\to.RemoteOpt.TransServerFactory = f\n\t}}\n}\n\n// WithLimitReporter do report when server limit happen\nfunc WithLimitReporter(r limiter.LimitReporter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithLimitReporter(%T)\", r))\n\n\t\to.Limit.LimitReporter = r\n\t}}\n}\n\n// WithGeneric set Generic type for generic call\nfunc WithGeneric(g generic.Generic) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithGeneric(%T)\", g))\n\n\t\tif g == nil {\n\t\t\tpanic(\"invalid Generic: nil\")\n\t\t}\n\t\to.RemoteOpt.PayloadCodec = g.PayloadCodec()\n\t}}\n}\n\n// WithErrorHandler sets the error handler.\nfunc WithErrorHandler(f func(context.Context, error) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithErrorHandler(%+v)\", utils.GetFuncName(f)))\n\n\t\to.ErrHandle = f\n\t}}\n}\n\n// WithBoundHandler adds remote.BoundHandler for server.\nfunc WithBoundHandler(h remote.BoundHandler) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"AddBoundHandler(%T)\", h))\n\n\t\texist := false\n\t\tswitch handler := h.(type) {\n\t\tcase remote.InboundHandler:\n\t\t\tfor _, inboundHandler := range o.RemoteOpt.Inbounds {\n\t\t\t\tif reflect.DeepEqual(inboundHandler, handler) {\n\t\t\t\t\texist = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase remote.OutboundHandler:\n\t\t\tfor _, outboundHandler := range o.RemoteOpt.Outbounds {\n\t\t\t\tif reflect.DeepEqual(outboundHandler, handler) {\n\t\t\t\t\texist = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// prevent duplication\n\t\tif !exist {\n\t\t\tdoAddBoundHandler(h, o.RemoteOpt)\n\t\t} else {\n\t\t\tklog.Warnf(\"KITEX: BoundHandler already exists, BoundHandler=%v\", h)\n\t\t}\n\t}}\n}\n\n// WithExitSignal adds ExitSignal for server.\nfunc WithExitSignal(f func() <-chan error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"AddExitSignal(%+v)\", utils.GetFuncName(f)))\n\t\to.ExitSignal = f\n\t}}\n}\n\n// WithListener sets the listener for server, the priority is higher than WithServiceAddr\nfunc WithListener(ln net.Listener) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithListener(%+v)\", ln))\n\n\t\to.RemoteOpt.Listener = ln\n\t}}\n}\n\n// WithReusePort sets SO_REUSEPORT on listener, it is only used with Option `WithServiceAddr`.\n// It won't take effect when listener is specified by WithListener.\nfunc WithReusePort(reuse bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReusePort(%+v)\", reuse))\n\n\t\to.RemoteOpt.ReusePort = reuse\n\t}}\n}\n\n// WithSupportedTransportsFunc sets a function which converts supported transports from server option.\n\n\n\n\n\n\n\n\n\n// WithProfiler set a profiler to server.\nfunc WithProfiler(pc profiler.Profiler) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithProfiler(%T{%+v})\", pc, pc))\n\t\to.RemoteOpt.Profiler = pc\n\t}}\n}\n\n// WithProfilerTransInfoTagging set transinfo tagging function to profiler\n// TransInfoTagging extracting tags after TransInfo decoded but before message decoded.\n// At this stage, we can only get msg.TransInfo() and the real message payload is not decoded yet.\n// If upstream is not use TTHeader protocol, we can get nothing here.\n// So if you don't very care about the accuracy of statistics, we recommend to use WithProfilerMessageTagging to extract your custom tags.\nfunc WithProfilerTransInfoTagging(tagging remote.TransInfoTagging) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithProfilerTransInfoTagging(%+v)\", utils.GetFuncName(tagging)))\n\t\tinterTagging := o.RemoteOpt.ProfilerTransInfoTagging\n\t\tif interTagging == nil {\n\t\t\to.RemoteOpt.ProfilerTransInfoTagging = tagging\n\t\t\treturn\n\t\t}\n\t\to.RemoteOpt.ProfilerTransInfoTagging = func(ctx context.Context, msg remote.Message) (context.Context, []string) {\n\t\t\tc, t := tagging(ctx, msg)\n\t\t\tc2, t2 := interTagging(c, msg)\n\t\t\treturn c2, append(t, t2...)\n\t\t}\n\t}}\n}\n\n// WithProfilerMessageTagging set message tagging function to profiler\n// MessageTagging extracting tags after whole decode process finished.\n// At this stage, we can get the rpcInfo from ctx, and full complete message.\nfunc WithProfilerMessageTagging(tagging remote.MessageTagging) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithProfilerMessageTagging(%+v)\", utils.GetFuncName(tagging)))\n\t\tinterTagging := o.RemoteOpt.ProfilerMessageTagging\n\t\tif interTagging == nil {\n\t\t\to.RemoteOpt.ProfilerMessageTagging = tagging\n\t\t\treturn\n\t\t}\n\t\to.RemoteOpt.ProfilerMessageTagging = func(ctx context.Context, msg remote.Message) (context.Context, []string) {\n\t\t\tc, t := tagging(ctx, msg)\n\t\t\tc2, t2 := interTagging(c, msg)\n\t\t\treturn c2, append(t, t2...)\n\t\t}\n\t}}\n}\n\n```\n# Function Signature: func WithSupportedTransportsFunc(f func(option remote.ServerOption) []string) Option {\n# Natural Language Annotations: // WithSupportedTransportsFunc sets a function which converts supported transports from server option.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithSupportedTransportsFunc(f func(option remote.ServerOption) []string) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithSupportedTransportsFunc(%+v)\", utils.GetFuncName(f)))\n\n\t\to.RemoteOpt.SupportedTransports = f\n\t}}\n}\n```", "  ```\nfunc WithSupportedTransportsFunc(f func(option remote.ServerOption) []string) Option {\n    return Option{F: func(o *internal_server.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithSupportedTransportsFunc(%+v)\", f))\n\n        o.RemoteOpt.TransportsFunc = f\n    }}\n}\n```", "  ```\nfunc WithSupportedTransportsFunc(f func(option remote.ServerOption) []string) Option {\n  return Option{F: func(o *internal_server.Options, di *utils.Slice) {\n    di.Push(fmt.Sprintf(\"WithSupportedTransportsFunc(%+v)\", utils.GetFuncName(f)))\n\n    o.RemoteOpt.SupportedTransportsFunc = f\n  }}\n}\n```"]}, "kitex-server/option.go-WithMuxTransport": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithMuxTransport() Option {\n# Natural Language Annotations: // WithMuxTransport specifies the transport type to be mux.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithMuxTransport() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMuxTransport()\"))\n\n\t\to.RemoteOpt.Transport = netpollmux.NewTransport()\n\t}}\n}\n```", "  ```\nfunc WithMuxTransport() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMuxTransport()\"))\n\n\t\to.RemoteOpt.TranName = \"mux\"\n\t}}\n}\n```", "  ```\nfunc WithMuxTransport() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMuxTransport\"))\n\n\t\to.RemoteOpt.Deprecated = true\n\t\to.RemoteOpt.NetworkTemplate = netpollmux.NewTransport()\n\t}}\n}\n```"]}, "kitex-server/option.go-WithMiddleware": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithMiddleware(mw endpoint.Middleware) Option {\n# Natural Language Annotations: // WithMiddleware adds middleware for server to handle request.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%T)\", mw))\n\t\to.Middlewares = append(o.Middlewares, mw)\n\t}}\n}\n```", "  ```\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", mw))\n\n\t\to.RemoteOpt.AMiddleware = append(o.RemoteOpt.AMiddleware, mw)\n\t}}\n}\n```", "  ```\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n    return Option{F: func(o *Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithMiddleware(%T, %+v)\", mw, mw))\n        o.Forwarder.AddMiddleware(mw)\n    }}\n}\n```"]}, "kitex-server/option.go-WithMiddlewareBuilder": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder, funcName ...string) Option {\n# Natural Language Annotations: // WithMiddlewareBuilder adds middleware that depend on context for server to handle request\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder, funcName ...string) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%T{%+v}, %+v)\", mwb, mwb, funcName))\n\n\t\to.RemoteOpt.MWBuilder = mwb\n\t\to.MWFuncNames = funcName\n\t}}\n}\n```", "  ```\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder, funcName ...string) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%T{%+v}, %v)\", mwb, mwb, funcName))\n\n\t\tmwbName := fmt.Sprintf(\"%v\", mwb)\n\t\tfor _, fn := range funcName {\n\t\t\tmwbName += fmt.Sprintf(\"(%v)\", fn)\n\t\t}\n\t\to.MiddlewareBuilder = append(o.MiddlewareBuilder, mwbName)\n\t}}\n}\n```", "  ```\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder, funcName ...string) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tfuncName := utils.DefaultIfEmpty(funcName, []string{\"\"})[0]\n\t\tif funcName == \"\" {\n\t\t\tfuncName = fmt.Sprintf(\"%T\", mwb)\n\t\t}\n\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%s)\", funcName))\n\n\t\tvar index *int\n\t\tif o.Middlewares == nil {\n\t\t\to.Middlewares = make(map[string][]endpoint.MiddlewareBuilder)\n\t\t} else {\n\t\t\tif dirEntries, ok := o.Middlewares[mwb.Name()]; !ok || len(dirEntries) == 0 {\n\t\t\t\tindex = utils.Uint64Ptr(uint64(len(dirEntries)))\n\t\t\t} else {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\n\t\t// save mwb as late as possible to ensure other middlewares which depends on `mwb` are also called\n\t\t// via funcName key to avoid duplicate key problem in doorkey map\n\t\tdi.Push(func() {\n\t\t\tmws := o.Middlewares[mwb.Name()]\n\t\t\tif index == nil {\n\t\t\t\tmws = append(mws, mwb)\n\t\t\t\to.Middlewares[mwb.Name()] = mws\n\t\t\t} else {\n\t\t\t\to.Middlewares[mwb.Name()] = append(mws, mwb)\n\t\t\t}\n\t\t})\n\t}}\n}\n```"]}, "kitex-server/option.go-WithExitWaitTime": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithExitWaitTime(timeout time.Duration) Option {\n# Natural Language Annotations: // WithExitWaitTime sets the wait duration for graceful shutdown.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithExitWaitTime(timeout time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithExitWaitTime(%v)\", timeout))\n\n\t\to.ExitWaitTime = timeout\n\t}}\n}\n```", "  ```\nfunc WithExitWaitTime(timeout time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithExitWaitTime(%v)\", timeout))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetExitWaitTime(timeout)\n\t\to.LockBits |= rpcinfo.BitExitWaitTime\n\t}}\n}\n```", "  ```\n// WithExitWaitTime sets the wait duration for graceful shutdown.\nfunc WithExitWaitTime(timeout time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithExitWaitTime(%v)\", timeout))\n\n\t\to.RemoteOpt.ExitWaitTime = timeout\n\t}}\n}\n```"]}, "kitex-server/option.go-WithMaxConnIdleTime": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithMaxConnIdleTime(timeout time.Duration) Option {\n# Natural Language Annotations: // WithMaxConnIdleTime sets the max idle time on connection from clients.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithMaxConnIdleTime(timeout time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMaxConnIdleTime(%v)\", timeout))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetMaxConnIdleTime(timeout)\n\t\to.LockBits |= rpcinfo.BitMaxConnIdleTime\n\t}}\n}\n```", "  ```\nfunc WithMaxConnIdleTime(timeout time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMaxConnIdleTime(%v)\", timeout))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetMaxConnIdleTime(timeout)\n\t}}\n}\n```", "  ```\nfunc WithMaxConnIdleTime(timeout time.Duration) Option {\n    return Option {\n        F: func(o *internal_server.Options, di *utils.Slice) {\n            di.Push(fmt.Sprintf(\"WithMaxConnIdleTime(%v)\", timeout))\n\n            o.RemoteOpt.MaxConnIdleTime = timeout\n        },\n    }\n}\n```"]}, "kitex-server/option.go-WithPayloadCodec": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithPayloadCodec(c remote.PayloadCodec) Option {\n# Natural Language Annotations: // WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithPayloadCodec(c remote.PayloadCodec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithPayloadCodec(%+v)\", c))\n\n\t\to.RemoteOpt.PayloadCodec = c\n\t}}\n}\n```", "  ```\nOption{\n    F: func(o *internal_server.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithPayloadCodec(%+v)\", c))\n\n        o.RemoteOpt.PayloadCodec = c\n    },\n}\n```", "  ```\nfunc WithPayloadCodec(c remote.PayloadCodec) Option {\n    return Option{F: func(o *internal_server.Options, di *utils.Slice) {\n        di.Push(\"WithPayloadCodec(\" + fmt.Sprintf(\"%+v\", c) + \")\")\n\n        o.RemoteOpt.PayloadCodec = c\n    }}\n}\n```"]}, "kitex-server/option.go-WithGRPCInitialWindowSize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCInitialWindowSize(s uint32) Option {\n# Natural Language Annotations: // WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithGRPCInitialWindowSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialWindowSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.InitialWindowSize = s\n\t}}\n}\n```", "  ```\nfunc WithGRPCInitialWindowSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialWindowSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.InitialWindowSize = s\n\t}}\n}\n```", "  ```\nfunc WithGRPCInitialWindowSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialWindowSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.InitialWindowSize = s\n\t}}\n}\n```"]}, "kitex-server/option.go-WithGRPCInitialConnWindowSize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCInitialConnWindowSize(s uint32) Option {\n# Natural Language Annotations: // WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithGRPCInitialConnWindowSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialConnWindowSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.InitialConnWindowSize = s\n\t}}\n}\n```", "  ```\nfunc WithGRPCInitialConnWindowSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialConnWindowSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.InitialConnWindowSize = s\n\t}}\n}\n```", "  ```\nfunc WithGRPCInitialConnWindowSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialConnWindowSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCInitialConnWindowSize = s\n\t}}\n}\n```"]}, "kitex-server/option.go-WithGRPCKeepaliveParams": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCKeepaliveParams(kp grpc.ServerKeepalive) Option {\n# Natural Language Annotations: // WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithGRPCKeepaliveParams(kp grpc.ServerKeepalive) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCKeepaliveParams(%+v)\", kp))\n\t\to.RemoteOpt.GRPCCfg.KeepaliveParams = kp\n\t}}\n}\n```", "  ```\nfunc WithGRPCKeepaliveParams(kp grpc.ServerKeepalive) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCKeepaliveParams(%+v)\", kp))\n\t\to.RemoteOpt.GRPCCfg.KeepaliveParams = kp\n\t}}\n}\n```", "  ```\nfunc WithGRPCKeepaliveParams(kp grpc.ServerKeepalive) Option {\n\treturn Option{\n\t\tF: func(o *internal_server.Options, di *utils.Slice) {\n\t\t\tdi.Push(fmt.Sprintf(\"WithGRPCKeepaliveParams(%+v)\", kp))\n\n\t\t\to.RemoteOpt.GRPCCfg.KeepaliveParams = kp\n\t\t},\n\t}\n}\n```"]}, "kitex-server/option.go-WithGRPCKeepaliveEnforcementPolicy": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCKeepaliveEnforcementPolicy(kep grpc.EnforcementPolicy) Option {\n# Natural Language Annotations: // WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithGRPCKeepaliveEnforcementPolicy(kep grpc.EnforcementPolicy) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCKeepaliveEnforcementPolicy(%+v)\", kep))\n\n\t\to.RemoteOpt.GRPCCfg.KeepaliveEnforcementPolicy = kep\n\t}}\n}\n```", "  ```\nfunc WithGRPCKeepaliveEnforcementPolicy(kep grpc.EnforcementPolicy) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCKeepaliveEnforcementPolicy(%+v)\", kep))\n\n\t\to.RemoteOpt.GRPCCfg.EnforcementPolicy = kep\n\t}}\n}\n```", "  ```\nfunc WithGRPCKeepaliveEnforcementPolicy(kep grpc.EnforcementPolicy) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCKeepaliveEnforcementPolicy(%+v)\", kep))\n\n\t\to.RemoteOpt.GRPCCfg.KeepaliveEnforcementPolicy = &kep\n\t}}\n}\n```"]}, "kitex-server/option.go-WithGRPCMaxConcurrentStreams": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCMaxConcurrentStreams(n uint32) Option {\n# Natural Language Annotations: // WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithGRPCMaxConcurrentStreams(n uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCMaxConcurrentStreams(%+v)\", n))\n\n\t\to.RemoteOpt.GRPCCfg.MaxConcurrentStreams = n\n\t}}\n}\n```", "  ```\nfunc WithGRPCMaxConcurrentStreams(n uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCMaxConcurrentStreams(%+v)\", n))\n\t\to.RemoteOpt.GRPCCfg.MaxConcurrentStreams = n\n\t}}\n}\n```", "  ```\nfunc WithGRPCMaxConcurrentStreams(n uint32) Option {\n    return Option{F: func(o *internal_server.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithGRPCMaxConcurrentStreams(%+v)\", n))\n\n        o.RemoteOpt.GRPCCfg.MaxConcurrentStreams = n\n    }}\n}\n```"]}, "kitex-server/option.go-WithGRPCMaxHeaderListSize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCMaxHeaderListSize(s uint32) Option {\n# Natural Language Annotations: // WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc WithGRPCMaxHeaderListSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCMaxHeaderListSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.MaxHeaderListSize = s\n\t}}\n}\n```", "  ```\nfunc WithGRPCMaxHeaderListSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCMaxHeaderListSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCMaxHeaderListSize = s\n\t}}\n}\n```", "  ```\nfunc WithGRPCMaxHeaderListSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCMaxHeaderListSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.MaxHeaderListSize = s\n\t}}\n}\n\n```"]}, "kitex-server/genericserver/server.go-NewServerWithServiceInfo": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/genericserver/server.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package genericserver ...\npackage genericserver\n\nimport (\n\t\"github.com/cloudwego/kitex/pkg/generic\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/server\"\n)\n\n// NewServer creates a generic server with the given handler and options.\nfunc NewServer(handler generic.Service, g generic.Generic, opts ...server.Option) server.Server {\n\tsvcInfo := generic.ServiceInfo(g.PayloadCodecType())\n\treturn NewServerWithServiceInfo(handler, g, svcInfo, opts...)\n}\n\n// NewServerWithServiceInfo creates a generic server with the given handler, serviceInfo and options.\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func NewServerWithServiceInfo(handler generic.Service, g generic.Generic, svcInfo *serviceinfo.ServiceInfo, opts ...server.Option) server.Server {\n# Natural Language Annotations: // NewServerWithServiceInfo creates a generic server with the given handler, serviceInfo and options.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\n// NewServerWithServiceInfo creates a new generic server with the given handler, serviceInfo, and options.\nfunc NewServerWithServiceInfo(handler generic.Service,\n\tg generic.Generic,\n\tsvcInfo *serviceinfo.ServiceInfo,\n\topts ...server.Option) server.Server {\n\n\t// Create a new generic server\n\tserver := &generic.Server{\n\t\tHandler:  handler,\n\t\tGeneric:  g,\n\t\tInfo:     svcInfo,\n\t\tOptions:  opts,\n\t}\n\n\t// Initialize the server\n\tif err := server.Init(server.Options); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn server, nil\n}\n```", "  ```\nfunc NewServerWithServiceInfo(handler generic.Service, g generic.Generic, svcInfo *serviceinfo.ServiceInfo, opts ...server.Option) server.Server {\n\t// get the payload codec type of the current generic.Generic\n\tpcd := g.PayloadCodecType()\n\t// get the type of the current generic.Generic\n\tt := g.Type()\n\t// get the package name of the current generic.Generic\n\tpkg := g.Package()\n\t// get the name of the current generic.Generic\n\tname := g.Name()\n\n\treturn &genericServer{\n\t\tgeneric:      g,\n\t\tpayloadCodec: pcd,\n\t\ttype:         t,\n\t\tpackage:      pkg,\n\t\tname:         name,\n\t\thandler:      handler,\n\t\topts:         opts,\n\t}\n}\n```", "  ```\nfunc NewServerWithServiceInfo(handler generic.Service, g generic.Generic, svcInfo *serviceinfo.ServiceInfo, opts ...server.Option) server.Server {\n\tsvr := &server{\n\t\tservice: handler,\n\t\tsvcInfo: svcInfo,\n\t\tg:       g,\n\t\toptions: opts,\n\t}\n\tif svr.svcInfo == nil {\n\t\tsvr.svcInfo = serviceinfo.GetGlobalServiceInfo(svr.g.PayloadCodecType())\n\t}\n\treturn svr\n}\n```\nThis code defines a new function called `NewServerWithServiceInfo` that takes in four parameters:\n\n* `handler`: A `generic.Service` that represents the handler for the new server.\n* `g`: A `generic.Generic` interface that provides the necessary generic functions for the server.\n* `svcInfo`: A pointer to a `serviceinfo.ServiceInfo` structure that contains information about the service.\n* `opts`: A list of options that will be used to create the server.\n\nThe function creates a new `server` struct and initializes its fields with the values of the input parameters. It then returns a `server.Server` interface that the caller can use to interact with the server.\n\nThe `svr.svcInfo` field is initialized with a non-nil value only if `svcInfo` is nil. If `svcInfo` is nil, the function uses the `serviceinfo.GetGlobalServiceInfo` function to retrieve a default service info object that is associated with the payload codec type of the `g` parameter.\n\nThe function returns a pointer to the new `server` struct, which is of type `server.Server`."]}, "weaver-validate.go-validateRegistrations": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/validate.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage weaver\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"unicode\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/reflection\"\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"golang.org/x/exp/slices\"\n)\n\n// validateRegistrations validates the provided registrations, returning an\n// diagnostic error if they are invalid. Note that some validation is performed\n// by 'weaver generate', but because users can run a Service Weaver app after\n// forgetting to run 'weaver generate', some checks have to be done at runtime.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// isValidListenerName returns whether the provided name is a valid\n// weaver.Listener name.\nfunc isValidListenerName(name string) bool {\n\t// We allow valid Go identifiers [1]. This code is taken from [2].\n\t//\n\t// [1]: https://go.dev/ref/spec#Identifiers\n\t// [2]: https://cs.opensource.google/go/go/+/refs/tags/go1.20.6:src/go/token/token.go;l=331-341;drc=19309779ac5e2f5a2fd3cbb34421dafb2855ac21\n\tif name == \"\" {\n\t\treturn false\n\t}\n\tfor i, c := range name {\n\t\tif !unicode.IsLetter(c) && c != '_' && (i == 0 || !unicode.IsDigit(c)) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n```\n# Function Signature: func validateRegistrations(regs []*codegen.Registration) error {\n# Natural Language Annotations: // validateRegistrations validates the provided registrations, returning an\n// diagnostic error if they are invalid. Note that some validation is performed\n// by 'weaver generate', but because users can run a Service Weaver app after\n// forgetting to run 'weaver generate', some checks have to be done at runtime.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc validateRegistrations(regs []*codegen.Registration) error {\n\tfor _, reg := range regs {\n\t\tif reg.Name == \"\" || !isValidListenerName(reg.Name) {\n\t\t\treturn fmt.Errorf(\"registration %q is not a valid listener name\", reg.Name)\n\t\t}\n\t\tif reg.Listener == nil {\n\t\t\treturn fmt.Errorf(\"registration %q does not have a valid listener\", reg.Name)\n\t\t}\n\t\tif reg.Handler == nil && len(reg.SvcMethods) == 0 {\n\t\t\treturn fmt.Errorf(\"registration %q does not have a valid message handler or Service methods\", reg.Name)\n\t\t}\n\t\tif reg.MessageType == \"\" && len(reg.SvcMethods) == 0 {\n\t\t\treturn fmt.Errorf(\"registration %q does not have a valid message type or Service methods\", reg.Name)\n\t\t}\n\t\tfor _, svc := range reg.SvcMethods {\n\t\t\tif svc.MethodName == \"\" {\n\t\t\t\treturn fmt.Errorf(\"registration %q has a Service method with an empty method name\", reg.Name)\n\t\t\t}\n\t\t\tif svc.Reflected == nil {\n\t\t\t\treturn fmt.Errorf(\"registration %q has a Service method with an invalid reflected service\", reg.Name)\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n```", "  ```\nfunc validateRegistrations(regs []*codegen.Registration) error {\n\tif regs == nil {\n\t\treturn errors.New(\"nil registrations passed to validateRegistrations\")\n\t}\n\n\tvar invalidRegs []*codegen.Registration\n\tfor _, reg := range regs {\n\t\tif !isValidListenerName(reg.FuncName) {\n\t\t\tinvalidRegs = append(invalidRegs, reg)\n\t\t}\n\t\tif _, ok := reg.Func.Call(nil); !ok {\n\t\t\tinvalidRegs = append(invalidRegs, reg)\n\t\t}\n\t\tif !reflect.ValueOf(reg.Func).Pointer().Type().Implements(reflect.TypeOf(func(c context.Context) io.WriteCloser { return nil }).Pointer().Type()) {\n\t\t\tinvalidRegs = append(invalidRegs, reg)\n\t\t}\n\t}\n\n\tif len(invalidRegs) != 0 {\n\t\treturn fmt.Errorf(\"found %d invalid registrations: %v\", len(invalidRegs), slices.Map(invalidRegs, func(reg *codegen.Registration) string { return reg.DiagnosticError() }))\n\t}\n\n\treturn nil\n}\n```", "  ```\nfunc validateRegistrations(regs []*codegen.Registration) error {\n\tvar err error\n\n\tfor _, reg := range regs {\n\t\tif reg.Service == \"\" {\n\t\t\treturn fmt.Errorf(\"registered service %q has an empty service name\", reg.Name)\n\t\t}\n\t\tif reg.Instance == nil {\n\t\t\treturn fmt.Errorf(\"registered service %q has a nil instance\", reg.Service)\n\t\t}\n\t\tif reflect.TypeOf(reg.Instance).Kind() == reflect.Ptr && reflect.ValueOf(reg.Instance).IsNil() {\n\t\t\treturn fmt.Errorf(\"registered service %q has a nil instance of %T\", reg.Service, reg.Instance)\n\t\t}\n\t\tif !isValidListenerName(reg.Listener) {\n\t\t\treturn fmt.Errorf(\"registered service %q has an invalid listener name %q\", reg.Service, reg.Listener)\n\t\t}\n\t}\n\n\tfor _, reg := range regs {\n\t\treg.Service = strings.TrimPrefix(reg.Service, \"/\")\n\t}\n\n\tseen := map[string]*codegen.Registration{}\n\tfor _, reg := range regs {\n\t\texisting, ok := seen[reg.Service]\n\t\tif ok {\n\t\t\terr = fmt.Errorf(\"registered service %q and %q both serve the same service\", reg.Service, existing.Service)\n\t\t}\n\t\tseen[reg.Service] = reg\n\t}\n\n\treturn err\n}\n```"]}, "weaver-sim/generators.go-Range": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/sim/generators.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage sim\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n)\n\n// TODO(mwhittaker): Add more generators as needed.\n\n// Booleans\n\n// Flip returns a [Generator] that returns true with probability p. Flip\n// panics if p is not in the range [0, 1].\nfunc Flip(p float64) Generator[bool] {\n\tif p < 0 || p > 1 {\n\t\tpanic(fmt.Errorf(\"Flip: probability p = %f not in range [0, 1]\", p))\n\t}\n\treturn generatorFunc[bool](func(r *rand.Rand) bool {\n\t\treturn r.Float64() <= p\n\t})\n}\n\n// Numerics\n\n// NonNegativeInt returns a [Generator] that returns non-negative integers.\n// Note that NonNegativeInt does not return all numbers. Instead, it biases\n// towards numbers closer to zero and other pathological numbers that are more\n// likely to induce bugs (e.g., math.MaxInt).\nfunc NonNegativeInt() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(0, 10)},\n\t\t{100, Range(10, 100)},\n\t\t{100, Range(100, 1000)},\n\t\t{100, Range(1000, 10_000)},\n\t\t{100, Range(10_000, 100_000)},\n\t\t{100, Range(100_000, 1_000_000)},\n\t\t{100, Range(1_000_000, 1_000_000_000)},\n\t\t{100, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{100, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Int returns a [Generator] that returns integers. Note that Int does not\n// return all integers equiprobably. Instead, it biases towards numbers closer\n// to zero and other pathological numbers that are more likely to induce bugs\n// (e.g., math.MaxInt, math.MinInt).\nfunc Int() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(-10, 10)},\n\t\t{50, Range(10, 100)},\n\t\t{50, Range(-100, -10)},\n\t\t{50, Range(100, 1000)},\n\t\t{50, Range(-1000, -100)},\n\t\t{50, Range(1000, 10_000)},\n\t\t{50, Range(-10_000, -1000)},\n\t\t{50, Range(10_000, 100_000)},\n\t\t{50, Range(-100_000, -10_000)},\n\t\t{50, Range(100_000, 1_000_000)},\n\t\t{50, Range(-1_000_000, -100_000)},\n\t\t{50, Range(1_000_000, 1_000_000_000)},\n\t\t{50, Range(-1_000_000_000, -1_000_000)},\n\t\t{50, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{50, Range(-1_000_000_000_000, -1_000_000_000)},\n\t\t{50, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{50, Range(math.MinInt, -1_000_000_000_000)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MinInt, math.MinInt+1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MinInt32-1, math.MinInt32, math.MinInt32+1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MinInt16-1, math.MinInt16, math.MinInt16+1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t\tmath.MinInt8-1, math.MinInt8, math.MinInt8+1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Float64 returns a [Generator] that returns 64-bit floats. Note that Float64\n// does not return all floats equiprobably. Instead, it biases towards numbers\n// closer to zero and other pathological numbers that are more likely to induce\n// bugs (e.g., NaN, infinity, -infinity, -0).\nfunc Float64() Generator[float64] {\n\tpathologies := OneOf(\n\t\tmath.NaN(),           // NaN\n\t\tmath.Inf(1),          // infinity\n\t\tmath.Inf(-1),         // -infinity\n\t\tmath.Copysign(0, -1), // -0\n\t)\n\treturn generatorFunc[float64](func(r *rand.Rand) float64 {\n\t\tif r.Intn(1000) == 0 {\n\t\t\t// 0.1% of the time, return a pathological float.\n\t\t\treturn pathologies.Generate(r)\n\t\t}\n\t\tconst stdev = 1e6\n\t\treturn r.NormFloat64() * stdev\n\t})\n}\n\n// Rune returns a [Generator] that returns runes equiprobably.\nfunc Rune() Generator[rune] {\n\treturn generatorFunc[rune](func(r *rand.Rand) rune {\n\t\t// Note that rune is an alias for int32.\n\t\treturn rune(r.Intn(math.MaxInt32 + 1))\n\t})\n}\n\n// Byte returns a [Generator] that returns bytes equiprobably.\nfunc Byte() Generator[byte] {\n\treturn generatorFunc[byte](func(r *rand.Rand) byte {\n\t\tvar buf [1]byte\n\t\tr.Read(buf[:])\n\t\treturn buf[0]\n\t})\n}\n\n// Range returns a [Generator] that returns integers equiprobably in the range\n// [low, high). Range panics if low >= high.\n\n\n\n\n\n\n\n\n\n\n// Strings\n\n// String returns a [Generator] that returns moderately sized readable strings,\n// with a bias towards smaller strings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Slices and Maps\n\n// Slice returns a [Generator] that returns slices of T. The size and contents\n// of the generated slices are determined by the provided generators.\nfunc Slice[T any](size Generator[int], values Generator[T]) Generator[[]T] {\n\treturn generatorFunc[[]T](func(r *rand.Rand) []T {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Slice: negative size %d\", n))\n\t\t}\n\t\txs := make([]T, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\txs[i] = values.Generate(r)\n\t\t}\n\t\treturn xs\n\t})\n}\n\n// Map returns a [Generator] that returns maps from K to V. The size and\n// contents of the the generated maps are determined by the provided\n// generators.\nfunc Map[K comparable, V any](size Generator[int], keys Generator[K], values Generator[V]) Generator[map[K]V] {\n\treturn generatorFunc[map[K]V](func(r *rand.Rand) map[K]V {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Map: negative size %d\", n))\n\t\t}\n\t\tkvs := make(map[K]V, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\tkvs[keys.Generate(r)] = values.Generate(r)\n\t\t}\n\t\treturn kvs\n\t})\n}\n\n// Combinators\n\n// OneOf returns a [Generator] that returns one of the provided values\n// equiprobably. OneOf panics if no values are provided.\n\n\n\n\n\n\n\n\n\n// Weight returns a [Generator] that generates values using the provided\n// generators. A generator is chosen with probability proportional to its\n// weight. For example, given the following choices:\n//\n//   - Weighted{1.0, OneOf(\"a\")}\n//   - Weighted{2.0, OneOf(\"b\")}\n//\n// Weight returns \"b\" twice as often as it returns \"a\". Note that the provided\n// weights do not have to sum to 1.\n//\n// Weight panics if no choices are provided, if any weight is negative, or if\n// the sum of all weight is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype Weighted[T any] struct {\n\tWeight float64\n\tGen    Generator[T]\n}\n\n// Filter returns a [Generator] that returns values from the provided generator\n// that satisfy the provided predicate.\nfunc Filter[T any](gen Generator[T], predicate func(T) bool) Generator[T] {\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\tfor {\n\t\t\tx := gen.Generate(r)\n\t\t\tif predicate(x) {\n\t\t\t\treturn x\n\t\t\t}\n\t\t}\n\t})\n}\n\n// generatorFunc[T] is an instance of Generator[T] that uses the provided\n// function to generate values.\ntype generatorFunc[T any] func(r *rand.Rand) T\n\n// Generate implements the Generator interface.\nfunc (g generatorFunc[T]) Generate(r *rand.Rand) T {\n\treturn g(r)\n}\n\n```\n# Function Signature: func Range(low, high int) Generator[int] {\n# Natural Language Annotations: // Range returns a [Generator] that returns integers equiprobably in the range\n// [low, high). Range panics if low >= high.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc Range(low, high int) Generator[int] {\n\tif low >= high {\n\t\tpanic(fmt.Errorf(\"Range: low >= high\"))\n\t}\n\treturn generatorFunc[int](func(r *rand.Rand) int {\n\t\treturn r.Intn(high-low) + low\n\t})\n}\n```", "  ```\nfunc Range(low int, high int) Generator[int] {\n\tif low >= high {\n\t\tpanic(fmt.Errorf(\"Range: low >= high (%d >= %d)\", low, high))\n\t}\n\treturn generatorFunc[int](func(r *rand.Rand) int {\n\t\treturn r.Intn(high-low) + low\n\t})\n}\n```", "  ```\nfunc Range(low, high int) Generator[int] {\n\tif low >= high {\n\t\tpanic(fmt.Errorf(\"Range: low (%d) is larger than or equal to high (%d)\", low, high))\n\t}\n\treturn generatorFunc[int](func(r *rand.Rand) int {\n\t\treturn r.Intn(high-low) + low\n\t})\n}\n```\nThis function takes in two integers `low` and `high` and returns a `Generator[int]` that generates integers in the range `[low, high)` inclusive. The generated integers will be equiprobable. If `low >= high`, then the function will panic.\n\nThe implementation of the function uses the `Intn` method of the `rand.Rand` struct to generate a random integer between 0 and `high-low`, and then adds `low` to it to get the final integer in the desired range."]}, "weaver-sim/generators.go-String": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/sim/generators.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage sim\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n)\n\n// TODO(mwhittaker): Add more generators as needed.\n\n// Booleans\n\n// Flip returns a [Generator] that returns true with probability p. Flip\n// panics if p is not in the range [0, 1].\nfunc Flip(p float64) Generator[bool] {\n\tif p < 0 || p > 1 {\n\t\tpanic(fmt.Errorf(\"Flip: probability p = %f not in range [0, 1]\", p))\n\t}\n\treturn generatorFunc[bool](func(r *rand.Rand) bool {\n\t\treturn r.Float64() <= p\n\t})\n}\n\n// Numerics\n\n// NonNegativeInt returns a [Generator] that returns non-negative integers.\n// Note that NonNegativeInt does not return all numbers. Instead, it biases\n// towards numbers closer to zero and other pathological numbers that are more\n// likely to induce bugs (e.g., math.MaxInt).\nfunc NonNegativeInt() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(0, 10)},\n\t\t{100, Range(10, 100)},\n\t\t{100, Range(100, 1000)},\n\t\t{100, Range(1000, 10_000)},\n\t\t{100, Range(10_000, 100_000)},\n\t\t{100, Range(100_000, 1_000_000)},\n\t\t{100, Range(1_000_000, 1_000_000_000)},\n\t\t{100, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{100, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Int returns a [Generator] that returns integers. Note that Int does not\n// return all integers equiprobably. Instead, it biases towards numbers closer\n// to zero and other pathological numbers that are more likely to induce bugs\n// (e.g., math.MaxInt, math.MinInt).\nfunc Int() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(-10, 10)},\n\t\t{50, Range(10, 100)},\n\t\t{50, Range(-100, -10)},\n\t\t{50, Range(100, 1000)},\n\t\t{50, Range(-1000, -100)},\n\t\t{50, Range(1000, 10_000)},\n\t\t{50, Range(-10_000, -1000)},\n\t\t{50, Range(10_000, 100_000)},\n\t\t{50, Range(-100_000, -10_000)},\n\t\t{50, Range(100_000, 1_000_000)},\n\t\t{50, Range(-1_000_000, -100_000)},\n\t\t{50, Range(1_000_000, 1_000_000_000)},\n\t\t{50, Range(-1_000_000_000, -1_000_000)},\n\t\t{50, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{50, Range(-1_000_000_000_000, -1_000_000_000)},\n\t\t{50, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{50, Range(math.MinInt, -1_000_000_000_000)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MinInt, math.MinInt+1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MinInt32-1, math.MinInt32, math.MinInt32+1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MinInt16-1, math.MinInt16, math.MinInt16+1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t\tmath.MinInt8-1, math.MinInt8, math.MinInt8+1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Float64 returns a [Generator] that returns 64-bit floats. Note that Float64\n// does not return all floats equiprobably. Instead, it biases towards numbers\n// closer to zero and other pathological numbers that are more likely to induce\n// bugs (e.g., NaN, infinity, -infinity, -0).\nfunc Float64() Generator[float64] {\n\tpathologies := OneOf(\n\t\tmath.NaN(),           // NaN\n\t\tmath.Inf(1),          // infinity\n\t\tmath.Inf(-1),         // -infinity\n\t\tmath.Copysign(0, -1), // -0\n\t)\n\treturn generatorFunc[float64](func(r *rand.Rand) float64 {\n\t\tif r.Intn(1000) == 0 {\n\t\t\t// 0.1% of the time, return a pathological float.\n\t\t\treturn pathologies.Generate(r)\n\t\t}\n\t\tconst stdev = 1e6\n\t\treturn r.NormFloat64() * stdev\n\t})\n}\n\n// Rune returns a [Generator] that returns runes equiprobably.\nfunc Rune() Generator[rune] {\n\treturn generatorFunc[rune](func(r *rand.Rand) rune {\n\t\t// Note that rune is an alias for int32.\n\t\treturn rune(r.Intn(math.MaxInt32 + 1))\n\t})\n}\n\n// Byte returns a [Generator] that returns bytes equiprobably.\nfunc Byte() Generator[byte] {\n\treturn generatorFunc[byte](func(r *rand.Rand) byte {\n\t\tvar buf [1]byte\n\t\tr.Read(buf[:])\n\t\treturn buf[0]\n\t})\n}\n\n// Range returns a [Generator] that returns integers equiprobably in the range\n// [low, high). Range panics if low >= high.\n\n\n\n\n\n\n\n\n\n\n// Strings\n\n// String returns a [Generator] that returns moderately sized readable strings,\n// with a bias towards smaller strings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Slices and Maps\n\n// Slice returns a [Generator] that returns slices of T. The size and contents\n// of the generated slices are determined by the provided generators.\nfunc Slice[T any](size Generator[int], values Generator[T]) Generator[[]T] {\n\treturn generatorFunc[[]T](func(r *rand.Rand) []T {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Slice: negative size %d\", n))\n\t\t}\n\t\txs := make([]T, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\txs[i] = values.Generate(r)\n\t\t}\n\t\treturn xs\n\t})\n}\n\n// Map returns a [Generator] that returns maps from K to V. The size and\n// contents of the the generated maps are determined by the provided\n// generators.\nfunc Map[K comparable, V any](size Generator[int], keys Generator[K], values Generator[V]) Generator[map[K]V] {\n\treturn generatorFunc[map[K]V](func(r *rand.Rand) map[K]V {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Map: negative size %d\", n))\n\t\t}\n\t\tkvs := make(map[K]V, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\tkvs[keys.Generate(r)] = values.Generate(r)\n\t\t}\n\t\treturn kvs\n\t})\n}\n\n// Combinators\n\n// OneOf returns a [Generator] that returns one of the provided values\n// equiprobably. OneOf panics if no values are provided.\n\n\n\n\n\n\n\n\n\n// Weight returns a [Generator] that generates values using the provided\n// generators. A generator is chosen with probability proportional to its\n// weight. For example, given the following choices:\n//\n//   - Weighted{1.0, OneOf(\"a\")}\n//   - Weighted{2.0, OneOf(\"b\")}\n//\n// Weight returns \"b\" twice as often as it returns \"a\". Note that the provided\n// weights do not have to sum to 1.\n//\n// Weight panics if no choices are provided, if any weight is negative, or if\n// the sum of all weight is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype Weighted[T any] struct {\n\tWeight float64\n\tGen    Generator[T]\n}\n\n// Filter returns a [Generator] that returns values from the provided generator\n// that satisfy the provided predicate.\nfunc Filter[T any](gen Generator[T], predicate func(T) bool) Generator[T] {\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\tfor {\n\t\t\tx := gen.Generate(r)\n\t\t\tif predicate(x) {\n\t\t\t\treturn x\n\t\t\t}\n\t\t}\n\t})\n}\n\n// generatorFunc[T] is an instance of Generator[T] that uses the provided\n// function to generate values.\ntype generatorFunc[T any] func(r *rand.Rand) T\n\n// Generate implements the Generator interface.\nfunc (g generatorFunc[T]) Generate(r *rand.Rand) T {\n\treturn g(r)\n}\n\n```\n# Function Signature: func String() Generator[string] {\n# Natural Language Annotations: // Strings\n// String returns a [Generator] that returns moderately sized readable strings,\n// with a bias towards smaller strings.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc String() Generator[string] {\n\treturn generatorFunc[string](func(r *rand.Rand) string {\n\t\tsize := r.Intn(10) + 1\n\t\tif size > 0 {\n\t\t\txs := make([]rune, size)\n\t\t\tfor i := 0; i < size; i++ {\n\t\t\t\txs[i] = rune(r.Intn(math.MaxInt32 + 1))\n\t\t\t}\n\t\t\treturn string(xs)\n\t\t}\n\t\treturn \"\"\n\t})\n}\n```", "  ```\nfunc String() Generator[string] {\n\treturn generatorFunc[string](func(r *rand.Rand) string {\n\t\tn := IntIn(1, 16).Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"String: negative size %d\", n))\n\t\t}\n\t\ts := make([]rune, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\ts[i] = Rune().Generate(r)\n\t\t}\n\t\treturn string(s)\n\t})\n}\n```", "  ```\npackage main\n\nimport (\n    \"fmt\"\n    \"math/rand\"\n)\n\n// String returns a [Generator] that returns moderately sized readable strings,\n// with a bias towards smaller strings.\nfunc String() Generator[string] {\n    g := []string(\"abcdefghijklmnopqrstuvwxyz\")\n    return generatorFunc[string](func(r *rand.Rand) string {\n        size := r.Intn(100)\n        if size < 1 || size > len(g) {\n            panic(fmt.Errorf(\"bad string size %d\", size))\n        }\n        return g[0:size]\n    })\n}\n\ntype Generator[T any] func(r *rand.Rand) T\n\nfunc (g Generator[T]) Generate(r *rand.Rand) T {\n    return g(r)\n}\n\nfunc main() {\n    g := String()\n    fmt.Println(g.Generate(rand.New(rand.NewSource(42))))\n}\n```"]}, "weaver-sim/generators.go-OneOf": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/sim/generators.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage sim\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n)\n\n// TODO(mwhittaker): Add more generators as needed.\n\n// Booleans\n\n// Flip returns a [Generator] that returns true with probability p. Flip\n// panics if p is not in the range [0, 1].\nfunc Flip(p float64) Generator[bool] {\n\tif p < 0 || p > 1 {\n\t\tpanic(fmt.Errorf(\"Flip: probability p = %f not in range [0, 1]\", p))\n\t}\n\treturn generatorFunc[bool](func(r *rand.Rand) bool {\n\t\treturn r.Float64() <= p\n\t})\n}\n\n// Numerics\n\n// NonNegativeInt returns a [Generator] that returns non-negative integers.\n// Note that NonNegativeInt does not return all numbers. Instead, it biases\n// towards numbers closer to zero and other pathological numbers that are more\n// likely to induce bugs (e.g., math.MaxInt).\nfunc NonNegativeInt() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(0, 10)},\n\t\t{100, Range(10, 100)},\n\t\t{100, Range(100, 1000)},\n\t\t{100, Range(1000, 10_000)},\n\t\t{100, Range(10_000, 100_000)},\n\t\t{100, Range(100_000, 1_000_000)},\n\t\t{100, Range(1_000_000, 1_000_000_000)},\n\t\t{100, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{100, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Int returns a [Generator] that returns integers. Note that Int does not\n// return all integers equiprobably. Instead, it biases towards numbers closer\n// to zero and other pathological numbers that are more likely to induce bugs\n// (e.g., math.MaxInt, math.MinInt).\nfunc Int() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(-10, 10)},\n\t\t{50, Range(10, 100)},\n\t\t{50, Range(-100, -10)},\n\t\t{50, Range(100, 1000)},\n\t\t{50, Range(-1000, -100)},\n\t\t{50, Range(1000, 10_000)},\n\t\t{50, Range(-10_000, -1000)},\n\t\t{50, Range(10_000, 100_000)},\n\t\t{50, Range(-100_000, -10_000)},\n\t\t{50, Range(100_000, 1_000_000)},\n\t\t{50, Range(-1_000_000, -100_000)},\n\t\t{50, Range(1_000_000, 1_000_000_000)},\n\t\t{50, Range(-1_000_000_000, -1_000_000)},\n\t\t{50, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{50, Range(-1_000_000_000_000, -1_000_000_000)},\n\t\t{50, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{50, Range(math.MinInt, -1_000_000_000_000)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MinInt, math.MinInt+1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MinInt32-1, math.MinInt32, math.MinInt32+1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MinInt16-1, math.MinInt16, math.MinInt16+1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t\tmath.MinInt8-1, math.MinInt8, math.MinInt8+1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Float64 returns a [Generator] that returns 64-bit floats. Note that Float64\n// does not return all floats equiprobably. Instead, it biases towards numbers\n// closer to zero and other pathological numbers that are more likely to induce\n// bugs (e.g., NaN, infinity, -infinity, -0).\nfunc Float64() Generator[float64] {\n\tpathologies := OneOf(\n\t\tmath.NaN(),           // NaN\n\t\tmath.Inf(1),          // infinity\n\t\tmath.Inf(-1),         // -infinity\n\t\tmath.Copysign(0, -1), // -0\n\t)\n\treturn generatorFunc[float64](func(r *rand.Rand) float64 {\n\t\tif r.Intn(1000) == 0 {\n\t\t\t// 0.1% of the time, return a pathological float.\n\t\t\treturn pathologies.Generate(r)\n\t\t}\n\t\tconst stdev = 1e6\n\t\treturn r.NormFloat64() * stdev\n\t})\n}\n\n// Rune returns a [Generator] that returns runes equiprobably.\nfunc Rune() Generator[rune] {\n\treturn generatorFunc[rune](func(r *rand.Rand) rune {\n\t\t// Note that rune is an alias for int32.\n\t\treturn rune(r.Intn(math.MaxInt32 + 1))\n\t})\n}\n\n// Byte returns a [Generator] that returns bytes equiprobably.\nfunc Byte() Generator[byte] {\n\treturn generatorFunc[byte](func(r *rand.Rand) byte {\n\t\tvar buf [1]byte\n\t\tr.Read(buf[:])\n\t\treturn buf[0]\n\t})\n}\n\n// Range returns a [Generator] that returns integers equiprobably in the range\n// [low, high). Range panics if low >= high.\n\n\n\n\n\n\n\n\n\n\n// Strings\n\n// String returns a [Generator] that returns moderately sized readable strings,\n// with a bias towards smaller strings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Slices and Maps\n\n// Slice returns a [Generator] that returns slices of T. The size and contents\n// of the generated slices are determined by the provided generators.\nfunc Slice[T any](size Generator[int], values Generator[T]) Generator[[]T] {\n\treturn generatorFunc[[]T](func(r *rand.Rand) []T {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Slice: negative size %d\", n))\n\t\t}\n\t\txs := make([]T, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\txs[i] = values.Generate(r)\n\t\t}\n\t\treturn xs\n\t})\n}\n\n// Map returns a [Generator] that returns maps from K to V. The size and\n// contents of the the generated maps are determined by the provided\n// generators.\nfunc Map[K comparable, V any](size Generator[int], keys Generator[K], values Generator[V]) Generator[map[K]V] {\n\treturn generatorFunc[map[K]V](func(r *rand.Rand) map[K]V {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Map: negative size %d\", n))\n\t\t}\n\t\tkvs := make(map[K]V, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\tkvs[keys.Generate(r)] = values.Generate(r)\n\t\t}\n\t\treturn kvs\n\t})\n}\n\n// Combinators\n\n// OneOf returns a [Generator] that returns one of the provided values\n// equiprobably. OneOf panics if no values are provided.\n\n\n\n\n\n\n\n\n\n// Weight returns a [Generator] that generates values using the provided\n// generators. A generator is chosen with probability proportional to its\n// weight. For example, given the following choices:\n//\n//   - Weighted{1.0, OneOf(\"a\")}\n//   - Weighted{2.0, OneOf(\"b\")}\n//\n// Weight returns \"b\" twice as often as it returns \"a\". Note that the provided\n// weights do not have to sum to 1.\n//\n// Weight panics if no choices are provided, if any weight is negative, or if\n// the sum of all weight is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype Weighted[T any] struct {\n\tWeight float64\n\tGen    Generator[T]\n}\n\n// Filter returns a [Generator] that returns values from the provided generator\n// that satisfy the provided predicate.\nfunc Filter[T any](gen Generator[T], predicate func(T) bool) Generator[T] {\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\tfor {\n\t\t\tx := gen.Generate(r)\n\t\t\tif predicate(x) {\n\t\t\t\treturn x\n\t\t\t}\n\t\t}\n\t})\n}\n\n// generatorFunc[T] is an instance of Generator[T] that uses the provided\n// function to generate values.\ntype generatorFunc[T any] func(r *rand.Rand) T\n\n// Generate implements the Generator interface.\nfunc (g generatorFunc[T]) Generate(r *rand.Rand) T {\n\treturn g(r)\n}\n\n```\n# Function Signature: func OneOf[T any](xs ...T) Generator[T] {\n# Natural Language Annotations: // Combinators\n// OneOf returns a [Generator] that returns one of the provided values\n// equiprobably. OneOf panics if no values are provided.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc OneOf[T any](xs ...T) Generator[T] {\n\tif len(xs) == 0 {\n\t\tpanic(\"OneOf: no values provided\")\n\t}\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\treturn xs[r.Intn(len(xs))]\n\t})\n}\n```", "  ```\nfunc OneOf[T any](xs ...T) Generator[T] {\n\tif len(xs) == 0 {\n\t\tpanic(\"OneOf: no values provided\")\n\t}\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\treturn xs[r.Intn(len(xs))]\n\t})\n}\n```", "  ```\nfunc OneOf[T any](xs ...T) Generator[T] {\n\tif len(xs) == 0 {\n\t\tpanic(fmt.Errorf(\"OneOf: no values provided\"))\n\t}\n\tchoices := make([]Weighted[T], len(xs))\n\tfor i, x := range xs {\n\t\tchoices[i] = Weighted{1, Generator(x)}\n\t}\n\treturn Weight(choices)\n}\n```"]}, "weaver-sim/generators.go-Weight": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/sim/generators.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage sim\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n)\n\n// TODO(mwhittaker): Add more generators as needed.\n\n// Booleans\n\n// Flip returns a [Generator] that returns true with probability p. Flip\n// panics if p is not in the range [0, 1].\nfunc Flip(p float64) Generator[bool] {\n\tif p < 0 || p > 1 {\n\t\tpanic(fmt.Errorf(\"Flip: probability p = %f not in range [0, 1]\", p))\n\t}\n\treturn generatorFunc[bool](func(r *rand.Rand) bool {\n\t\treturn r.Float64() <= p\n\t})\n}\n\n// Numerics\n\n// NonNegativeInt returns a [Generator] that returns non-negative integers.\n// Note that NonNegativeInt does not return all numbers. Instead, it biases\n// towards numbers closer to zero and other pathological numbers that are more\n// likely to induce bugs (e.g., math.MaxInt).\nfunc NonNegativeInt() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(0, 10)},\n\t\t{100, Range(10, 100)},\n\t\t{100, Range(100, 1000)},\n\t\t{100, Range(1000, 10_000)},\n\t\t{100, Range(10_000, 100_000)},\n\t\t{100, Range(100_000, 1_000_000)},\n\t\t{100, Range(1_000_000, 1_000_000_000)},\n\t\t{100, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{100, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Int returns a [Generator] that returns integers. Note that Int does not\n// return all integers equiprobably. Instead, it biases towards numbers closer\n// to zero and other pathological numbers that are more likely to induce bugs\n// (e.g., math.MaxInt, math.MinInt).\nfunc Int() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(-10, 10)},\n\t\t{50, Range(10, 100)},\n\t\t{50, Range(-100, -10)},\n\t\t{50, Range(100, 1000)},\n\t\t{50, Range(-1000, -100)},\n\t\t{50, Range(1000, 10_000)},\n\t\t{50, Range(-10_000, -1000)},\n\t\t{50, Range(10_000, 100_000)},\n\t\t{50, Range(-100_000, -10_000)},\n\t\t{50, Range(100_000, 1_000_000)},\n\t\t{50, Range(-1_000_000, -100_000)},\n\t\t{50, Range(1_000_000, 1_000_000_000)},\n\t\t{50, Range(-1_000_000_000, -1_000_000)},\n\t\t{50, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{50, Range(-1_000_000_000_000, -1_000_000_000)},\n\t\t{50, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{50, Range(math.MinInt, -1_000_000_000_000)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MinInt, math.MinInt+1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MinInt32-1, math.MinInt32, math.MinInt32+1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MinInt16-1, math.MinInt16, math.MinInt16+1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t\tmath.MinInt8-1, math.MinInt8, math.MinInt8+1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Float64 returns a [Generator] that returns 64-bit floats. Note that Float64\n// does not return all floats equiprobably. Instead, it biases towards numbers\n// closer to zero and other pathological numbers that are more likely to induce\n// bugs (e.g., NaN, infinity, -infinity, -0).\nfunc Float64() Generator[float64] {\n\tpathologies := OneOf(\n\t\tmath.NaN(),           // NaN\n\t\tmath.Inf(1),          // infinity\n\t\tmath.Inf(-1),         // -infinity\n\t\tmath.Copysign(0, -1), // -0\n\t)\n\treturn generatorFunc[float64](func(r *rand.Rand) float64 {\n\t\tif r.Intn(1000) == 0 {\n\t\t\t// 0.1% of the time, return a pathological float.\n\t\t\treturn pathologies.Generate(r)\n\t\t}\n\t\tconst stdev = 1e6\n\t\treturn r.NormFloat64() * stdev\n\t})\n}\n\n// Rune returns a [Generator] that returns runes equiprobably.\nfunc Rune() Generator[rune] {\n\treturn generatorFunc[rune](func(r *rand.Rand) rune {\n\t\t// Note that rune is an alias for int32.\n\t\treturn rune(r.Intn(math.MaxInt32 + 1))\n\t})\n}\n\n// Byte returns a [Generator] that returns bytes equiprobably.\nfunc Byte() Generator[byte] {\n\treturn generatorFunc[byte](func(r *rand.Rand) byte {\n\t\tvar buf [1]byte\n\t\tr.Read(buf[:])\n\t\treturn buf[0]\n\t})\n}\n\n// Range returns a [Generator] that returns integers equiprobably in the range\n// [low, high). Range panics if low >= high.\n\n\n\n\n\n\n\n\n\n\n// Strings\n\n// String returns a [Generator] that returns moderately sized readable strings,\n// with a bias towards smaller strings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Slices and Maps\n\n// Slice returns a [Generator] that returns slices of T. The size and contents\n// of the generated slices are determined by the provided generators.\nfunc Slice[T any](size Generator[int], values Generator[T]) Generator[[]T] {\n\treturn generatorFunc[[]T](func(r *rand.Rand) []T {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Slice: negative size %d\", n))\n\t\t}\n\t\txs := make([]T, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\txs[i] = values.Generate(r)\n\t\t}\n\t\treturn xs\n\t})\n}\n\n// Map returns a [Generator] that returns maps from K to V. The size and\n// contents of the the generated maps are determined by the provided\n// generators.\nfunc Map[K comparable, V any](size Generator[int], keys Generator[K], values Generator[V]) Generator[map[K]V] {\n\treturn generatorFunc[map[K]V](func(r *rand.Rand) map[K]V {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Map: negative size %d\", n))\n\t\t}\n\t\tkvs := make(map[K]V, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\tkvs[keys.Generate(r)] = values.Generate(r)\n\t\t}\n\t\treturn kvs\n\t})\n}\n\n// Combinators\n\n// OneOf returns a [Generator] that returns one of the provided values\n// equiprobably. OneOf panics if no values are provided.\n\n\n\n\n\n\n\n\n\n// Weight returns a [Generator] that generates values using the provided\n// generators. A generator is chosen with probability proportional to its\n// weight. For example, given the following choices:\n//\n//   - Weighted{1.0, OneOf(\"a\")}\n//   - Weighted{2.0, OneOf(\"b\")}\n//\n// Weight returns \"b\" twice as often as it returns \"a\". Note that the provided\n// weights do not have to sum to 1.\n//\n// Weight panics if no choices are provided, if any weight is negative, or if\n// the sum of all weight is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype Weighted[T any] struct {\n\tWeight float64\n\tGen    Generator[T]\n}\n\n// Filter returns a [Generator] that returns values from the provided generator\n// that satisfy the provided predicate.\nfunc Filter[T any](gen Generator[T], predicate func(T) bool) Generator[T] {\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\tfor {\n\t\t\tx := gen.Generate(r)\n\t\t\tif predicate(x) {\n\t\t\t\treturn x\n\t\t\t}\n\t\t}\n\t})\n}\n\n// generatorFunc[T] is an instance of Generator[T] that uses the provided\n// function to generate values.\ntype generatorFunc[T any] func(r *rand.Rand) T\n\n// Generate implements the Generator interface.\nfunc (g generatorFunc[T]) Generate(r *rand.Rand) T {\n\treturn g(r)\n}\n\n```\n# Function Signature: func Weight[T any](choices []Weighted[T]) Generator[T] {\n# Natural Language Annotations: // Weight returns a [Generator] that generates values using the provided\n// generators. A generator is chosen with probability proportional to its\n// weight. For example, given the following choices:\n//\n//   - Weighted{1.0, OneOf(\"a\")}\n//   - Weighted{2.0, OneOf(\"b\")}\n//\n// Weight returns \"b\" twice as often as it returns \"a\". Note that the provided\n// weights do not have to sum to 1.\n//\n// Weight panics if no choices are provided, if any weight is negative, or if\n// the sum of all weight is 0.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc Weight[T any](choices []Weighted[T]) Generator[T] {\n\tif len(choices) == 0 {\n\t\tpanic(fmt.Errorf(\"Weight: no choices provided\"))\n\t}\n\tfor _, choice := range choices {\n\t\tif choice.Weight < 0 {\n\t\t\tpanic(fmt.Errorf(\"Weight: weight %f is negative\", choice.Weight))\n\t\t}\n\t}\n\ttotal := 0.0\n\tfor _, choice := range choices {\n\t\ttotal += choice.Weight\n\t}\n\tif total == 0 {\n\t\tpanic(fmt.Errorf(\"Weight: sum of weights is 0\"))\n\t}\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\tthreshold := r.Float64() * total\n\t\tfor _, choice := range choices {\n\t\t\tthreshold -= choice.Weight\n\t\t\tif threshold <= 0 {\n\t\t\t\treturn choice.Gen.Generate(r)\n\t\t\t}\n\t\t}\n\t\tpanic(\"unreachable\")\n\t})\n}\n```", "  ```\nfunc Weight[T any](choices []Weighted[T]) Generator[T] {\n\tif len(choices) == 0 {\n\t\tpanic(\"Weight: no choices provided\")\n\t}\n\tvar totalWeight float64\n\tfor _, choice := range choices {\n\t\tif choice.Weight < 0 {\n\t\t\tpanic(fmt.Errorf(\"Weight: choice weight %f is negative\", choice.Weight))\n\t\t}\n\t\ttotalWeight += choice.Weight\n\t}\n\tif totalWeight == 0 {\n\t\tpanic(\"Weight: total weight is zero\")\n\t}\n\tweights := make([]float64, len(choices))\n\tweights[0] = choices[0].Weight / totalWeight\n\tfor i := 1; i < len(choices); i++ {\n\t\tweights[i] = weights[i-1] + choices[i].Weight/totalWeight\n\t}\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\tt := r.Float64()\n\t\tfor i := 0; i < len(choices); i++ {\n\t\t\tif t <= weights[i] {\n\t\t\t\treturn choices[i].Gen.Generate(r)\n\t\t\t}\n\t\t}\n\t\tpanic(\"Weight: this should never happen\")\n\t})\n}\n```", "  ```\nfunc Weight[T any](choices []Weighted[T]) Generator[T] {\n\tif len(choices) == 0 {\n\t\tpanic(fmt.Errorf(\"Weight: no generators provided\"))\n\t}\n\tvar totalWeight float64\n\tfor _, c := range choices {\n\t\tif c.Weight < 0 {\n\t\t\tpanic(fmt.Errorf(\"Weight: negative weight %f\", c.Weight))\n\t\t}\n\t\ttotalWeight += c.Weight\n\t}\n\tif totalWeight == 0 {\n\t\tpanic(fmt.Errorf(\"Weight: total weight 0\"))\n\t}\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\trng := r.Float64() * totalWeight\n\t\tfor _, c := range choices {\n\t\t\trng -= c.Weight\n\t\t\tif rng <= 0 {\n\t\t\t\treturn c.Gen.Generate(r)\n\t\t\t}\n\t\t}\n\t\tpanic(\"unreachable code\")\n\t})\n}\n```"]}, "weaver-sim/simulator.go-New": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/sim/simulator.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package sim implements deterministic simulation.\n//\n// [Deterministic simulation][1] is a type of randomized testing in which\n// millions of random operations are run against a system (with randomly\n// injected failures) in an attempt to find bugs. See\n// serviceweaver.dev/blog/testing.html for an overview of determistic\n// simulation and its implementation in the sim package.\n//\n// # Generators\n//\n// A key component of deterministic simulation is the ability to\n// deterministically generate \"random\" values. We accomplish this with the\n// [Generator] interface:\n//\n//\ttype Generator[T any] interface {\n//\t    Generate(*rand.Rand) T\n//\t}\n//\n// A Generator[T] generates random values of type T. For example, the [Int]\n// function returns a Generator[int] that generates random integers.\n//\n// While random, a Generator is also deterministic. Given a random number\n// generator with a particular seed, a Generator will always produce the same\n// value:\n//\n//\t// x and y are always equal.\n//\tvar gen Gen[int] = ...\n//\tx := gen.Generate(rand.New(rand.NewSource(42)))\n//\ty := gen.Generate(rand.New(rand.NewSource(42)))\n//\n// The sim package includes generators that generate booleans, ints,\n// floats, runes, strings, slices, and maps (e.g., [Flip], [Int], [Float64],\n// [Rune], [String], [Range], [Map]). It also contains generator combinators\n// that combine existing generators into new generators (e.g., [OneOf],\n// [Weight], [Filter]). You can also implement your own custom generators by\n// implementing the Generator interface.\n//\n// # Workloads\n//\n// Deterministic simulation verifies a system by running random operations\n// against the system, checking for invariant violations along the way. A\n// workload defines the set of operations to run and the set of invariants to\n// check.\n//\n// Concretely, a workload is a struct that implements the [Workload] interface.\n// When a simulator executes a workload, it will randomly call the exported\n// methods of the struct with randomly generated values. We call these methods\n// *operations*. If an operation ever encounters an invariant violation, it\n// returns a non-nil error and the simulation is aborted.\n//\n// Consider the following workload as an example.\n//\n//\tfunc even(x int) bool {\n//\t\treturn x%2 == 0\n//\t}\n//\n//\ttype EvenWorkload struct {\n//\t\tx int\n//\t}\n//\n//\tfunc (e *EvenWorkload) Add(_ context.Context, y int) error {\n//\t\te.x = e.x + y\n//\t\tif !even(e.x) {\n//\t\t\treturn fmt.Errorf(\"%d is not even\", e.x)\n//\t\t}\n//\t\treturn nil\n//\t}\n//\n//\tfunc (e *EvenWorkload) Multiply(_ context.Context, y int) error {\n//\t\te.x = e.x * y\n//\t\tif !even(e.x) {\n//\t\t\treturn fmt.Errorf(\"%d is not even\", e.x)\n//\t\t}\n//\t\treturn nil\n//\t}\n//\n// An EvenWorkload has an integer x as state and defines two operations: Add\n// and Multiply. Add adds a value to x, and Multiply multiplies x. Both\n// operations check the invariant that x is even. Of course, this invariant\n// does not hold if we add arbitrary values to x.\n//\n// However, we control the arguments on which which operations are called.\n// Specifically, we add an Init method that registers a set of generators. The\n// simulator will call the workload's operations on values produced by these\n// generators.\n//\n//\tfunc (e *EvenWorkload) Init(r sim.Registrar) error {\n//\t\tr.RegisterGenerators(\"Add\", sim.Filter(sim.Int(), even))\n//\t\tr.RegisterGenerators(\"Multiply\", sim.Int())\n//\t\treturn nil\n//\t}\n//\n// Note that we only call the Add operation on even integers. Finally, we can\n// construct a simulator and simulate the EvenWorkload.\n//\n//\tfunc TestEvenWorkload(t *testing.T) {\n//\t\ts := sim.New(t, &EvenWorkload{}, sim.Options{})\n//\t\tr := s.Run(5 * time.Second)\n//\t\tif r.Err != nil {\n//\t\t\tt.Fatal(r.Err)\n//\t\t}\n//\t}\n//\n// In this trivial example, our workload did not use any Service Weaver\n// components, but most realistic workloads do. A workload can get a reference\n// to a component using weaver.Ref. See serviceweaver.dev/blog/testing.html for\n// a complete example.\n//\n// # Graveyard\n//\n// When the simulator runs a failed execution, it persists the failing inputs\n// to disk. The collection of saved failing inputs is called a *graveyard*, and\n// each individual entry is called a *graveyard entry*. When a simulator is\n// created, the first thing it does is load and re-simulate all graveyard\n// entries.\n//\n// We borrow the design of go's fuzzing library's corpus with only minor\n// changes [2]. When a simulator runs as part of a test named TestFoo, it\n// stores its graveyard entries in testdata/sim/TestFoo. Every graveyard entry\n// is a JSON file. Filenames are derived from the hash of the contents of the\n// graveyard entry. Here's an example testdata directory:\n//\n//\ttestdata/\n//\t\u2514\u2500\u2500 sim\n//\t    \u251c\u2500\u2500 TestCancelledSimulation\n//\t    \u2502\u00a0\u00a0 \u2514\u2500\u2500 a52f5ec5f94e674d.json\n//\t    \u251c\u2500\u2500 TestSimulateGraveyardEntries\n//\t    \u2502\u00a0\u00a0 \u251c\u2500\u2500 2bfe847328319dae.json\n//\t    \u2502\u00a0\u00a0 \u2514\u2500\u2500 a52f5ec5f94e674d.json\n//\t    \u2514\u2500\u2500 TestUnsuccessfulSimulation\n//\t        \u251c\u2500\u2500 2bfe847328319dae.json\n//\t        \u2514\u2500\u2500 a52f5ec5f94e674d.json\n//\n// As with go's fuzzing library, graveyard entries are never garbage collected.\n// Users are responsible for manually deleting graveyard entries when\n// appropriate.\n//\n// TODO(mwhittaker): Move things to the weavertest package.\n//\n// [1]: https://asatarin.github.io/testing-distributed-systems/#deterministic-simulation\n// [2]: https://go.dev/security/fuzz\npackage sim\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math/rand\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"runtime\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/reflection\"\n\t\"github.com/ServiceWeaver/weaver/internal/weaver\"\n\tswruntime \"github.com/ServiceWeaver/weaver/runtime\"\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/logging\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"golang.org/x/exp/maps\"\n\t\"golang.org/x/text/language\"\n\t\"golang.org/x/text/message\"\n)\n\n// FakeComponent is a copy of weavertest.FakeComponent. It's needed to access\n// the unexported fields.\n//\n// TODO(mwhittaker): Remove this once we merge with weavertest.\ntype FakeComponent struct {\n\tintf reflect.Type\n\timpl any\n}\n\n// Fake is a copy of weavertest.Fake.\n//\n// TODO(mwhittaker): Remove this once we merge with the weavertest package.\nfunc Fake[T any](impl any) FakeComponent {\n\tt := reflection.Type[T]()\n\tif _, ok := impl.(T); !ok {\n\t\tpanic(fmt.Sprintf(\"%T does not implement %v\", impl, t))\n\t}\n\treturn FakeComponent{intf: t, impl: impl}\n}\n\n// A Generator[T] generates random values of type T.\ntype Generator[T any] interface {\n\t// Generate returns a randomly generated value of type T. While Generate is\n\t// \"random\", it must be deterministic. That is, given the same instance of\n\t// *rand.Rand, Generate must always return the same value.\n\t//\n\t// TODO(mwhittaker): Generate should maybe take something other than a\n\t// *rand.Rand?\n\tGenerate(*rand.Rand) T\n}\n\n// A Registrar is used to register fakes and generators with a [Simulator].\ntype Registrar interface {\n\t// RegisterFake registers a fake implementation of a component.\n\tRegisterFake(FakeComponent)\n\n\t// RegisterGenerators registers generators for a workload method, one\n\t// generator per method argument. The number and type of the registered\n\t// generators must match the method. For example, given the method:\n\t//\n\t//     Foo(context.Context, int, bool) error\n\t//\n\t// we must register a Generator[int] and a Generator[bool]:\n\t//\n\t//     var r Registrar = ...\n\t//     var i Generator[int] = ...\n\t//     var b Generator[bool] = ...\n\t//     r.RegisterGenerators(\"Foo\", i, b)\n\t//\n\t// TODO(mwhittaker): Allow people to register a func(*rand.Rand) T instead\n\t// of a Generator[T] for convenience.\n\tRegisterGenerators(method string, generators ...any)\n}\n\n// A Workload defines the set of operations to run as part of a simulation.\n// Every workload is defined as a named struct. To execute a workload, a\n// simulator constructs an instance of the struct, calls the struct's Init\n// method, and then randomly calls the struct's exported methods. For example,\n// the following is a simple workload:\n//\n//\ttype myWorkload struct {}\n//\tfunc (w *myWorkload) Init(r sim.Registrar) {...}\n//\tfunc (w *myWorkload) Foo(context.Context, int) error {...}\n//\tfunc (w *myWorkload) Bar(context.Context, bool, string) error {...}\n//\tfunc (w *myWorkload) baz(context.Context) error {...}\n//\n// When this workload is executed, its Foo and Bar methods will be called with\n// random values generated by the generators registered in the Init method (see\n// [Registrar] for details). Note that unexported methods, like baz, are\n// ignored.\n//\n// Note that every exported workload method must receive a [context.Context] as\n// its first argument and must return a single error value. A simulation is\n// aborted when a method returns a non-nil error.\n//\n// TODO(mwhittaker): For now, the Init method is required. In the future, we\n// could make it optional and use default generators for methods.\ntype Workload interface {\n\t// Init initializes a workload. The Init method must also register\n\t// generators for every exported method.\n\tInit(Registrar) error\n}\n\n// Options configure a Simulator.\ntype Options struct {\n\t// TOML config file contents.\n\tConfig string\n\n\t// The number of executions to run in parallel. If Parallelism is 0, the\n\t// simulator picks the degree of parallelism.\n\tParallelism int\n}\n\n// A Simulator deterministically simulates a Service Weaver application. See\n// the package documentation for instructions on how to use a Simulator.\ntype Simulator struct {\n\topts       Options                                // options\n\tt          testing.TB                             // underlying test\n\tw          reflect.Type                           // workload type\n\tregsByIntf map[reflect.Type]*codegen.Registration // components, by interface\n\tinfo       componentInfo                          // component metadata\n\tconfig     *protos.AppConfig                      // application config\n}\n\n// Results are the results of simulating a workload.\ntype Results struct {\n\tErr           error         // first non-nil error returned by an op\n\tHistory       []Event       // a history of the error inducing run, if Err is not nil\n\tNumExecutions int           // number of executions ran\n\tNumOps        int           // number of ops ran\n\tDuration      time.Duration // duration of simulation\n}\n\n// New returns a new Simulator that simulates the provided workload.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// validateWorkload validates a workload struct of the provided type.\nfunc validateWorkload(w reflect.Type) error {\n\tvar errs []error\n\tnumOps := 0\n\tfor i := 0; i < w.NumMethod(); i++ {\n\t\tm := w.Method(i)\n\t\tif m.Name == \"Init\" {\n\t\t\tcontinue\n\t\t}\n\t\tnumOps++\n\n\t\t// Method should have type func(context.Context, ...) error.\n\t\terr := fmt.Errorf(\"method %s has type '%v' but should have type 'func(%v, context.Context, ...) error'\", m.Name, m.Type, w)\n\t\tswitch {\n\t\tcase m.Type.NumIn() < 2:\n\t\t\terrs = append(errs, fmt.Errorf(\"%w: no arguments\", err))\n\t\tcase m.Type.In(1) != reflection.Type[context.Context]():\n\t\t\terrs = append(errs, fmt.Errorf(\"%w: first argument is not context.Context\", err))\n\t\tcase m.Type.NumOut() == 0:\n\t\t\terrs = append(errs, fmt.Errorf(\"%w: no return value\", err))\n\t\tcase m.Type.NumOut() > 1:\n\t\t\terrs = append(errs, fmt.Errorf(\"%w: too many return values\", err))\n\t\tcase m.Type.Out(0) != reflection.Type[error]():\n\t\t\terrs = append(errs, fmt.Errorf(\"%w: return value is not error\", err))\n\t\t}\n\t}\n\tif numOps == 0 {\n\t\terrs = append(errs, fmt.Errorf(\"no exported methods\"))\n\t}\n\treturn errors.Join(errs...)\n}\n\n// newExecutor returns a new executor.\nfunc (s *Simulator) newExecutor() *executor {\n\treturn newExecutor(s.t, s.w, s.regsByIntf, s.info, s.config)\n}\n\n// graveyardDir returns the graveyard directory for this simulator.\nfunc (s *Simulator) graveyardDir() string {\n\t// Test names often contain slashes (\"/\"). We replace \"/\" with \"#\" to\n\t// safely use the test name as a directory name.\n\t//\n\t// TODO(mwhittaker): This mapping is sensitive to collisions. A test named\n\t// \"foo/bar\" collides with a test named \"foo#bar\". I think in practice,\n\t// this will likely not be a big issue. But, if people are running into\n\t// problems, we can use a more complex collision resistant sanitization.\n\tsanitized := strings.ReplaceAll(s.t.Name(), \"/\", \"#\")\n\treturn filepath.Join(\"testdata\", \"sim\", sanitized)\n}\n\n// Run runs a simulation for the provided duration.\nfunc (s *Simulator) Run(duration time.Duration) Results {\n\tctx, cancel := context.WithTimeout(context.Background(), duration)\n\tdefer cancel()\n\n\ts.t.Logf(\"Simulating workload %v for %v.\", s.w, duration)\n\tstats := &stats{start: time.Now()}\n\tswitch result, err := s.run(ctx, stats); {\n\tcase err != nil && err == ctx.Err():\n\t\t// The simulation was cancelled.\n\t\tresults := Results{\n\t\t\tNumExecutions: int(stats.numExecutions),\n\t\t\tNumOps:        int(stats.numOps),\n\t\t\tDuration:      time.Since(stats.start),\n\t\t}\n\t\ts.t.Log(results.summary())\n\t\treturn results\n\n\tcase err != nil:\n\t\t// The simulation failed to run properly.\n\t\ts.t.Fatalf(\"Simulator.Run: %v\", err)\n\t\treturn Results{}\n\n\tcase result.err != nil:\n\t\t// The simulation found a failing execution.\n\t\tresults := Results{\n\t\t\tErr:           result.err,\n\t\t\tHistory:       result.history,\n\t\t\tNumExecutions: int(stats.numExecutions),\n\t\t\tNumOps:        int(stats.numOps),\n\t\t\tDuration:      time.Since(stats.start),\n\t\t}\n\t\ts.t.Log(results.summary())\n\n\t\tentry := graveyardEntry{\n\t\t\tVersion:     version,\n\t\t\tSeed:        result.params.Seed,\n\t\t\tNumReplicas: result.params.NumReplicas,\n\t\t\tNumOps:      result.params.NumOps,\n\t\t\tFailureRate: result.params.FailureRate,\n\t\t\tYieldRate:   result.params.YieldRate,\n\t\t}\n\t\tif filename, err := writeGraveyardEntry(s.graveyardDir(), entry); err == nil {\n\t\t\ts.t.Logf(\"Failing input written to %s.\", filename)\n\t\t}\n\t\treturn results\n\n\tdefault:\n\t\t// All executions passed.\n\t\tresults := Results{\n\t\t\tNumExecutions: int(stats.numExecutions),\n\t\t\tNumOps:        int(stats.numOps),\n\t\t\tDuration:      time.Since(stats.start),\n\t\t}\n\t\ts.t.Log(results.summary())\n\t\treturn results\n\t}\n}\n\n// stats contains simulation statistics.\ntype stats struct {\n\tstart         time.Time // start of simulation\n\tnumExecutions int64     // number of fully executed executions\n\tnumOps        int64     // number of fully executed ops\n}\n\n// run runs a simulation until the provided context is cancelled. It returns\n// the hyperparameters and result of a failing execution if any are found.\nfunc (s *Simulator) run(ctx context.Context, stats *stats) (result, error) {\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\n\t// Spawn a goroutine to periodically print progress.\n\tdone := sync.WaitGroup{}\n\tdone.Add(1)\n\tgo func() {\n\t\tdefer done.Done()\n\t\ts.printProgress(ctx, stats)\n\t}()\n\n\t// Execute the graveyard entries.\n\tif r, err := s.executeGraveyard(ctx, stats); err != nil || r.err != nil {\n\t\treturn r, err\n\t}\n\n\t// Spawn n concurrent executors which read hyperparamters from the params\n\t// channel. Simulation ends when:\n\t//\n\t//     1. the context is cancelled;\n\t//     2. an execution fails to run properly (written to errs); or\n\t//     3. a failing execution is found (written to failing).\n\tn := s.opts.Parallelism\n\tif n == 0 {\n\t\tn = 10 * runtime.NumCPU()\n\t}\n\tparams := make(chan hyperparameters, n)\n\terrs := make(chan error, n)\n\tfailing := make(chan result, n)\n\n\ts.t.Logf(\"Executing with %d executors.\", n)\n\tdone.Add(n)\n\tfor i := 0; i < n; i++ {\n\t\tgo func() {\n\t\t\tdefer done.Done()\n\t\t\tswitch r, err := s.execute(ctx, stats, params); {\n\t\t\tcase err != nil && err == ctx.Err():\n\t\t\t\treturn\n\t\t\tcase err != nil:\n\t\t\t\terrs <- err\n\t\t\t\treturn\n\t\t\tcase r.err != nil:\n\t\t\t\tfailing <- r\n\t\t\t}\n\t\t}()\n\t}\n\n\t// Spawn a goroutine that writes to the params channel.\n\t//\n\t// TODO(mwhittaker): Use a smarter algorithm to sweep over hyperparameters.\n\tdone.Add(1)\n\tgo func() {\n\t\tdefer done.Done()\n\t\tseed := time.Now().UnixNano()\n\t\tfor numOps := 1; ; numOps++ {\n\t\t\tfor _, numReplicas := range []int{1, 2, 3} {\n\t\t\t\tfor _, failureRate := range []float64{0.0, 0.01, 0.05, 0.1} {\n\t\t\t\t\tfor _, yieldRate := range []float64{0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0} {\n\t\t\t\t\t\tfor i := 0; i < 1000; i++ {\n\t\t\t\t\t\t\tseed++\n\t\t\t\t\t\t\tp := hyperparameters{\n\t\t\t\t\t\t\t\tSeed:        seed,\n\t\t\t\t\t\t\t\tNumOps:      numOps,\n\t\t\t\t\t\t\t\tNumReplicas: numReplicas,\n\t\t\t\t\t\t\t\tFailureRate: failureRate,\n\t\t\t\t\t\t\t\tYieldRate:   yieldRate,\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tselect {\n\t\t\t\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t\tcase params <- p:\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Wait for the simulation to end.\n\tselect {\n\tcase <-ctx.Done():\n\t\tdone.Wait()\n\t\treturn result{}, ctx.Err()\n\tcase err := <-errs:\n\t\tcancel()\n\t\tdone.Wait()\n\t\treturn result{}, err\n\tcase r := <-failing:\n\t\tcancel()\n\t\tdone.Wait()\n\t\treturn r, nil\n\t}\n}\n\n// printProgress periodically prints the progress of the simulation.\nfunc (s *Simulator) printProgress(ctx context.Context, stats *stats) {\n\tprinter := message.NewPrinter(language.AmericanEnglish)\n\tticker := time.NewTicker(time.Second)\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\tticker.Stop()\n\t\t\treturn\n\t\tcase <-ticker.C:\n\t\t\telapsed := time.Since(stats.start)\n\t\t\ttruncated := elapsed.Truncate(time.Second)\n\t\t\texecs := atomic.LoadInt64(&stats.numExecutions)\n\t\t\tops := atomic.LoadInt64(&stats.numOps)\n\t\t\texecRate := printer.Sprintf(\"%0.0f\", float64(execs)/elapsed.Seconds())\n\t\t\topRate := printer.Sprintf(\"%0.0f\", float64(ops)/elapsed.Seconds())\n\t\t\ts.t.Logf(\"[%v] %s execs (%s execs/s), %s ops (%s ops/s)\", truncated, printer.Sprint(execs), execRate, printer.Sprint(ops), opRate)\n\t\t}\n\t}\n}\n\n// executeGraveyardEntries executes graveyard entries serially. Executing\n// graveyard entries serially is important to make errors repeatable. If we\n// execute graveyard entries in multiple goroutines, the user might see a\n// different error every time they run \"go test\", which is discombobulating.\nfunc (s *Simulator) executeGraveyard(ctx context.Context, stats *stats) (result, error) {\n\tgraveyard, err := readGraveyard(s.graveyardDir())\n\tif err != nil {\n\t\treturn result{}, err\n\t}\n\n\ts.t.Logf(\"Executing %d graveyard entries.\", len(graveyard))\n\texec := s.newExecutor()\n\tfor _, entry := range graveyard {\n\t\tp := hyperparameters{\n\t\t\tSeed:        entry.Seed,\n\t\t\tNumReplicas: entry.NumReplicas,\n\t\t\tNumOps:      entry.NumOps,\n\t\t\tFailureRate: entry.FailureRate,\n\t\t\tYieldRate:   entry.YieldRate,\n\t\t}\n\t\tr, err := exec.execute(ctx, p)\n\t\tif err != nil {\n\t\t\treturn result{}, err\n\t\t}\n\t\tatomic.AddInt64(&stats.numExecutions, 1)\n\t\tatomic.AddInt64(&stats.numOps, int64(p.NumOps))\n\t\tif r.err != nil {\n\t\t\treturn r, nil\n\t\t}\n\t}\n\ts.t.Log(\"Done executing graveyard entries.\")\n\treturn result{}, nil\n}\n\n// execute repeatedly performs executions until the provided context is\n// cancelled or until a failing result is found. Hyperparameters for the\n// executions are read from the provided params channel.\nfunc (s *Simulator) execute(ctx context.Context, stats *stats, params <-chan hyperparameters) (result, error) {\n\texec\n```\n# Function Signature: func New(t testing.TB, x Workload, opts Options) *Simulator {\n# Natural Language Annotations: // New returns a new Simulator that simulates the provided workload.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc New[T Workload](t testing.TB, opts Options) *Simulator {\n\treturn &Simulator{\n\t\topts: opts,\n\t\tt:    t,\n\t\tw:    reflection.Type[T](),\n\t\tregsByIntf: map[reflect.Type]*codegen.Registration{},\n\t}\n}\n```", "  ```\nfunc New(t testing.TB, x Workload, opts Options) *Simulator {\n\tw := reflect.TypeOf(x)\n\tif err := validateWorkload(w); err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\tregsByIntf := map[reflect.Type]*codegen.Registration{}\n\tif opts.Config != \"\" {\n\t\tif _, err := toml.Decode(opts.Config, config); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t}\n\n\tinfo := componentInfo(config)\n\tfor _, c := range info.components {\n\t\treg := &codegen.Registration{\n\t\t\tName:     c.name,\n\t\t\tRevision: c.revision,\n\t\t\tService:  c.service,\n\t\t}\n\t\tintf := reflect.TypeOf(c.intf)\n\t\tif _, ok := regsByIntf[intf]; ok {\n\t\t\tpanic(fmt.Sprintf(\"multiple implementations registered for interface %v\", intf))\n\t\t}\n\t\tregsByIntf[intf] = reg\n\t}\n\n\ts := &Simulator{\n\t\topts:       opts,\n\t\tt:          t,\n\t\tw:          w,\n\t\tregsByIntf: regsByIntf,\n\t\tinfo:       info,\n\t\tconfig:     config,\n\t}\n\treturn s\n}\n```", "  ```\npackage sim\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math/rand\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"runtime\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/codegen\"\n\t\"github.com/ServiceWeaver/weaver/internal/logging\"\n\t\"github.com/ServiceWeaver/weaver/internal/protos\"\n\t\"github.com/ServiceWeaver/weaver/runtime\"\n\t\"github.com/ServiceWeaver/weaver/runtime/testing\"\n)\n\ntype Simulator struct {\n\topts       Options\n\tt          testing.TB\n\tw          reflect.Type\n\tregsByIntf map[reflect.Type]*codegen.Registration\n\tinfo       componentInfo\n\tconfig     *protos.AppConfig\n}\n\nfunc New(t testing.TB, w Workload, opts Options) *Simulator {\n\twType := reflect.TypeOf(w)\n\tif wType.Kind() != reflect.Struct {\n\t\tt.Fatalf(\"w must be a struct, but is %v\", wType)\n\t}\n\twValue := reflect.ValueOf(w)\n\tif wValue.Len() != 0 {\n\t\tt.Fatalf(\"w must be a zero-value struct\")\n\t}\n\n\tvar info componentInfo\n\tfor i := 0; i < wType.NumMethod(); i++ {\n\t\tm := wType.Method(i)\n\t\tswitch m.Name {\n\t\tcase \"Init\":\n\t\t\tinfo.initMethodIndex = i\n\n\t\tdefault:\n\t\t\tif m.Type.NumIn() < 1 {\n\t\t\t\tt.Fatalf(\"method %s has too few arguments\", m.Name)\n\t\t\t}\n\t\t\tif m.Type.In(0) != reflection.Type[context.Context]() {\n\t\t\t\tt.Fatalf(\"method %s's first argument is not context.Context\", m.Name)\n\t\t\t}\n\t\t\tif m.Type.NumOut() == 0 {\n\t\t\t\tt.Fatalf(\"method %s has no return values\", m.Name)\n\t\t\t}\n\t\t\tif m.Type.NumOut() > 2 {\n\t\t\t\tt.Fatalf(\"method %s has too many return values\", m.Name)\n\t\t\t}\n\t\t\tif !m.Type.Out(0).Implements(reflection.Type[error]()) {\n\t\t\t\tt.Fatalf(\"method %s's return value is not error\", m.Name)\n\t\t\t}\n\t\t\tinfo.opMethodIndices = append(info.opMethodIndices, i)\n\t\t}\n\t}\n\n\tif len(info.opMethodIndices) == 0 {\n\t\tt.Fatalf(\"no exported methods in workload\")\n\t}\n\n\tconfig := &protos.AppConfig{\n\t\tSeed: time.Now().UnixNano(),\n\t}\n\n\tregs := map[reflect.Type]*codegen.Registration{}\n\tfor i := 0; i < wType.NumField(); i++ {\n\t\tif !wValue.Field(i).CanInterface() || wValue.Field(i).IsNil() {\n\t\t\tcontinue\n\t\t}\n\t\tcomp := wValue.Field(i).Interface()\n\t\tintf := reflect.TypeOf(comp)\n\t\treg, err := codegen.Register(intf)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tregs[intf] = reg\n\t}\n\n\treturn &Simulator{\n\t\tt:          t,\n\t\tw:          wType,\n\t\tregsByIntf: regs,\n\t\tinfo:       info,\n\t\tconfig:     config,\n\t}\n}\n```"]}, "weaver-sim/rand.go-pick": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/sim/rand.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage sim\n\nimport (\n\t\"fmt\"\n\t\"math/bits\"\n\t\"math/rand\"\n)\n\n// pop pops and returns a randomly selected element from the provided slice.\n// pop panics if the provided slice is empty.\nfunc pop[T any](r *rand.Rand, xs []T) (T, []T) {\n\tif len(xs) == 0 {\n\t\tpanic(fmt.Errorf(\"pop: empty slice\"))\n\t}\n\ti := r.Intn(len(xs))\n\tx := xs[i]\n\treturn x, append(xs[:i], xs[i+1:]...)\n}\n\n// pick returns a randomly selected element from the provided slice. pick\n// panics if the provided slice is empty.\n\n\n\n\n\n\n\n// flip returns true with probability p. For example, flip(0) always returns\n// false, flip(1) always returns true, and flip(0.5) returns true half the\n// time. flip panics if p is not in the range [0, 1].\nfunc flip(r *rand.Rand, p float64) bool {\n\tif p < 0 || p > 1 {\n\t\tpanic(fmt.Errorf(\"flip: probability %f not in range [0, 1.0]\", p))\n\t}\n\treturn r.Float64() <= p\n}\n\n// ints represents a remove-only set of integers in the range [low, high).\ntype ints struct {\n\tlow, high int\n\n\t// The integers in the set in no particular order.\n\telements []int\n\n\t// indices[x-low] is the index of element x in elements, or -1 if x is not\n\t// in the set.\n\tindices []int\n}\n\n// reset resets a set of integers to the range [low, high).\n// reset panics if low >= high.\nfunc (i *ints) reset(low, high int) {\n\tif low >= high {\n\t\tpanic(fmt.Errorf(\"newInts: low (%d) >= high (%d)\", low, high))\n\t}\n\n\ti.low = low\n\ti.high = high\n\tn := high - low\n\tif i.elements == nil {\n\t\ti.elements = make([]int, n)\n\t}\n\ti.elements = i.elements[:0]\n\tif i.indices == nil {\n\t\ti.indices = make([]int, n)\n\t}\n\ti.indices = i.indices[:0]\n\n\tfor j := 0; j < n; j++ {\n\t\ti.elements = append(i.elements, low+j)\n\t\ti.indices = append(i.indices, j)\n\t}\n}\n\n// has returns whether the provided integer is in the set.\nfunc (i *ints) has(x int) bool {\n\treturn i.low <= x && x < i.high && i.indices[x-i.low] != -1\n}\n\n// size returns the size of the set.\nfunc (i *ints) size() int {\n\treturn len(i.elements)\n}\n\n// pick returns a random element of the set.\nfunc (i *ints) pick(r *rand.Rand) int {\n\treturn i.elements[r.Intn(len(i.elements))]\n}\n\n// remove removes the provided element from the set. remove is a noop if the\n// provided element is not in the set.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// wyrand is an implementation of the wyrand pseudorandom number generator\n// algorithm from [1, 2]. This implementation also borrows from [3] and [4].\n//\n// 1: https://github.com/wangyi-fudan/wyhash\n// 2: https://github.com/wangyi-fudan/wyhash/blob/master/Modern%20Non-Cryptographic%20Hash%20Function%20and%20Pseudorandom%20Number%20Generator.pdf\n// 3: https://github.com/lemon-mint/exp-pkgs/blob/v0.0.25/hash/wyhash/wyhash.go\n// 4: https://github.com/dsincl12/wyrand/blob/5f074aba21f4f9022d8d73139357bf816fdf1c93/wyrand.go\ntype wyrand struct {\n\tseed uint64\n}\n\nvar _ rand.Source = &wyrand{}\nvar _ rand.Source64 = &wyrand{}\n\n// Seed implements the rand.Source interface.\nfunc (w *wyrand) Seed(seed int64) {\n\tw.seed = uint64(seed)\n}\n\n// Int63 implements the rand.Source interface.\nfunc (w *wyrand) Int63() int64 {\n\treturn int64(w.Uint64() >> 1)\n}\n\n// Uint64 implements the rand.Source64 interface.\nfunc (w *wyrand) Uint64() uint64 {\n\tw.seed += 0xa0761d6478bd642f\n\treturn wymix(w.seed, w.seed^0xe7037ed1a0b428db)\n\n}\n\n// See https://github.com/wangyi-fudan/wyhash for explanation.\nfunc wymix(x uint64, y uint64) uint64 {\n\thi, lo := bits.Mul64(x, y)\n\treturn hi ^ lo\n}\n\n```\n# Function Signature: func pick[T any](r *rand.Rand, xs []T) T {\n# Natural Language Annotations: // pick returns a randomly selected element from the provided slice. pick\n// panics if the provided slice is empty.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc pick[T any](r *rand.Rand, xs []T) T {\n\tif len(xs) == 0 {\n\t\tpanic(fmt.Errorf(\"pick: empty slice\"))\n\t}\n\ti := r.Intn(len(xs))\n\treturn xs[i]\n}\n```", "  ```\nfunc pick[T any](r *rand.Rand, xs []T) T {\n\tif len(xs) == 0 {\n\t\tpanic(fmt.Errorf(\"pick: empty slice\"))\n\t}\n\ti := r.Intn(len(xs))\n\treturn xs[i]\n}\n```", "  ```\nfunc pick[T any](r *rand.Rand, xs []T) T {\n\tif len(xs) == 0 {\n\t\tpanic(errors.New(\"pick: empty slice\"))\n\t}\n\treturn xs[r.Intn(len(xs))]\n}\n```"]}, "weaver-sim/rand.go-remove": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/sim/rand.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage sim\n\nimport (\n\t\"fmt\"\n\t\"math/bits\"\n\t\"math/rand\"\n)\n\n// pop pops and returns a randomly selected element from the provided slice.\n// pop panics if the provided slice is empty.\nfunc pop[T any](r *rand.Rand, xs []T) (T, []T) {\n\tif len(xs) == 0 {\n\t\tpanic(fmt.Errorf(\"pop: empty slice\"))\n\t}\n\ti := r.Intn(len(xs))\n\tx := xs[i]\n\treturn x, append(xs[:i], xs[i+1:]...)\n}\n\n// pick returns a randomly selected element from the provided slice. pick\n// panics if the provided slice is empty.\n\n\n\n\n\n\n\n// flip returns true with probability p. For example, flip(0) always returns\n// false, flip(1) always returns true, and flip(0.5) returns true half the\n// time. flip panics if p is not in the range [0, 1].\nfunc flip(r *rand.Rand, p float64) bool {\n\tif p < 0 || p > 1 {\n\t\tpanic(fmt.Errorf(\"flip: probability %f not in range [0, 1.0]\", p))\n\t}\n\treturn r.Float64() <= p\n}\n\n// ints represents a remove-only set of integers in the range [low, high).\ntype ints struct {\n\tlow, high int\n\n\t// The integers in the set in no particular order.\n\telements []int\n\n\t// indices[x-low] is the index of element x in elements, or -1 if x is not\n\t// in the set.\n\tindices []int\n}\n\n// reset resets a set of integers to the range [low, high).\n// reset panics if low >= high.\nfunc (i *ints) reset(low, high int) {\n\tif low >= high {\n\t\tpanic(fmt.Errorf(\"newInts: low (%d) >= high (%d)\", low, high))\n\t}\n\n\ti.low = low\n\ti.high = high\n\tn := high - low\n\tif i.elements == nil {\n\t\ti.elements = make([]int, n)\n\t}\n\ti.elements = i.elements[:0]\n\tif i.indices == nil {\n\t\ti.indices = make([]int, n)\n\t}\n\ti.indices = i.indices[:0]\n\n\tfor j := 0; j < n; j++ {\n\t\ti.elements = append(i.elements, low+j)\n\t\ti.indices = append(i.indices, j)\n\t}\n}\n\n// has returns whether the provided integer is in the set.\nfunc (i *ints) has(x int) bool {\n\treturn i.low <= x && x < i.high && i.indices[x-i.low] != -1\n}\n\n// size returns the size of the set.\nfunc (i *ints) size() int {\n\treturn len(i.elements)\n}\n\n// pick returns a random element of the set.\nfunc (i *ints) pick(r *rand.Rand) int {\n\treturn i.elements[r.Intn(len(i.elements))]\n}\n\n// remove removes the provided element from the set. remove is a noop if the\n// provided element is not in the set.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// wyrand is an implementation of the wyrand pseudorandom number generator\n// algorithm from [1, 2]. This implementation also borrows from [3] and [4].\n//\n// 1: https://github.com/wangyi-fudan/wyhash\n// 2: https://github.com/wangyi-fudan/wyhash/blob/master/Modern%20Non-Cryptographic%20Hash%20Function%20and%20Pseudorandom%20Number%20Generator.pdf\n// 3: https://github.com/lemon-mint/exp-pkgs/blob/v0.0.25/hash/wyhash/wyhash.go\n// 4: https://github.com/dsincl12/wyrand/blob/5f074aba21f4f9022d8d73139357bf816fdf1c93/wyrand.go\ntype wyrand struct {\n\tseed uint64\n}\n\nvar _ rand.Source = &wyrand{}\nvar _ rand.Source64 = &wyrand{}\n\n// Seed implements the rand.Source interface.\nfunc (w *wyrand) Seed(seed int64) {\n\tw.seed = uint64(seed)\n}\n\n// Int63 implements the rand.Source interface.\nfunc (w *wyrand) Int63() int64 {\n\treturn int64(w.Uint64() >> 1)\n}\n\n// Uint64 implements the rand.Source64 interface.\nfunc (w *wyrand) Uint64() uint64 {\n\tw.seed += 0xa0761d6478bd642f\n\treturn wymix(w.seed, w.seed^0xe7037ed1a0b428db)\n\n}\n\n// See https://github.com/wangyi-fudan/wyhash for explanation.\nfunc wymix(x uint64, y uint64) uint64 {\n\thi, lo := bits.Mul64(x, y)\n\treturn hi ^ lo\n}\n\n```\n# Function Signature: func (i *ints) remove(x int) {\n# Natural Language Annotations: // remove removes the provided element from the set. remove is a noop if the\n// provided element is not in the set.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (i *ints) remove(x int) {\n\tif !i.has(x) {\n\t\treturn\n\t}\n\tidx := i.indices[x-i.low]\n\ti.elements = append(i.elements[:idx], i.elements[idx+1:]...)\n\ti.indices[x-i.low] = -1\n}\n```", "  ```\nfunc (i *ints) remove(x int) {\n\tif !i.has(x) {\n\t\treturn\n\t}\n\n\tidx := i.indices[x-i.low]\n\ti.indices[x-i.low] = -1\n\n\tif idx < len(i.elements)-1 {\n\t\ti.elements[idx] = i.elements[len(i.elements)-1]\n\t\ti.indices[i.elements[idx]-i.low] = idx\n\t}\n\ti.elements = i.elements[:len(i.elements)-1]\n}\n```", "  ```\nfunc (i *ints) remove(x int) {\n\tif !i.has(x) {\n\t\treturn\n\t}\n\n\tn := len(i.indices)\n\tlast := i.indices[n-1]\n\tif last == -1 {\n\t\tpanic(fmt.Errorf(\"remove: uninitialized element %d in set\", x))\n\t}\n\n\ti.indices[x-i.low] = -1\n\ti.elements[last-i.low] = x\n\ti.indices[last-i.low] = x - i.low\n}\n```"]}, "weaver-runtime/config.go-ParseConfig": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/config.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage runtime\n\nimport (\n\t\"fmt\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/BurntSushi/toml\"\n\t\"github.com/ServiceWeaver/weaver/internal/env\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n)\n\n// ParseConfig parses the specified configuration input, which should\n// hold a set of sections in TOML format from the specified file.\n// The section corresponding to the common Service Weaver application\n// configuration is parsed and returned as a *AppConfig.\n//\n// sectionValidator(key, val) is used to validate every section config entry.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ParseConfigSection parses the config section for key into dst.\n// If shortKey is not empty, either key or shortKey is accepted.\n// If the named section is not found, returns nil without changing dst.\nfunc ParseConfigSection(key, shortKey string, sections map[string]string, dst any) error {\n\tsection, ok := sections[key]\n\tif shortKey != \"\" {\n\t\t// Fetch section listed for shortKey, if any.\n\t\tif shortKeySection, ok2 := sections[shortKey]; ok2 {\n\t\t\tif ok {\n\t\t\t\treturn fmt.Errorf(\"conflicting sections %q and %q\", shortKey, key)\n\t\t\t}\n\t\t\tkey, section, ok = shortKey, shortKeySection, ok2\n\t\t}\n\t}\n\tif !ok { // not found\n\t\treturn nil\n\t}\n\n\t// Parse and validate the section.\n\tmd, err := toml.Decode(section, dst)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif unknown := md.Undecoded(); len(unknown) != 0 {\n\t\treturn fmt.Errorf(\"section %q has unknown keys %v\", key, unknown)\n\t}\n\tif x, ok := dst.(interface{ Validate() error }); ok {\n\t\tif err := x.Validate(); err != nil {\n\t\t\treturn fmt.Errorf(\"section %q: %w\", key, err)\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc extractApp(file string, config *protos.AppConfig) error {\n\tconst appKey = \"github.com/ServiceWeaver/weaver\"\n\tconst shortAppKey = \"serviceweaver\"\n\n\t// appConfig holds the data from under appKey in the TOML config.\n\t// It matches the contents of the Config proto.\n\ttype appConfig struct {\n\t\tName     string\n\t\tBinary   string\n\t\tArgs     []string\n\t\tEnv      []string\n\t\tColocate [][]string\n\t\tRollout  time.Duration\n\t}\n\n\tparsed := &appConfig{}\n\tif err := ParseConfigSection(appKey, shortAppKey, config.Sections, parsed); err != nil {\n\t\treturn err\n\t}\n\n\t// Move struct fields into proto.\n\tconfig.Name = parsed.Name\n\tconfig.Binary = parsed.Binary\n\tconfig.Args = parsed.Args\n\tconfig.Env = parsed.Env\n\tconfig.RolloutNanos = int64(parsed.Rollout)\n\tfor _, colocate := range parsed.Colocate {\n\t\tgroup := &protos.ComponentGroup{Components: colocate}\n\t\tconfig.Colocate = append(config.Colocate, group)\n\t}\n\n\t// Canonicalize the config.\n\tif err := canonicalizeConfig(config, filepath.Dir(file)); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// canonicalizeConfig updates the provided config to canonical\n// form. All relative paths inside the configuration are resolved\n// relative to the provided directory.\nfunc canonicalizeConfig(c *protos.AppConfig, dir string) error {\n\t// Fill in the application name if necessary.\n\tbin := c.GetBinary()\n\tif c.Name == \"\" && bin != \"\" {\n\t\tc.Name = filepath.Base(bin)\n\t}\n\n\t// Convert relative paths inside the application config to absolute paths\n\t// interpreted starting at the directory containing the config file.\n\tif !filepath.IsAbs(bin) {\n\t\tbin, err := filepath.Abs(filepath.Join(dir, bin))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tc.Binary = bin\n\t}\n\n\t// Validate the environment variables.\n\tif _, err := env.Parse(c.Env); err != nil {\n\t\treturn fmt.Errorf(\"invalid Env: %v\", err)\n\t}\n\n\t// Validate the same_process entry.\n\tif err := checkSameProcess(c); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// checkSameProcess checks that the same_process entry is valid.\nfunc checkSameProcess(c *protos.AppConfig) error {\n\tseen := map[string]struct{}{}\n\tfor _, components := range c.Colocate {\n\t\tfor _, component := range components.Components {\n\t\t\tif _, ok := seen[component]; ok {\n\t\t\t\treturn fmt.Errorf(\"component %q placed multiple times\", component)\n\t\t\t}\n\t\t\tseen[component] = struct{}{}\n\t\t}\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func ParseConfig(file string, input string, sectionValidator func(string, string) error) (*protos.AppConfig, error) {\n# Natural Language Annotations: // ParseConfig parses the specified configuration input, which should\n// hold a set of sections in TOML format from the specified file.\n// The section corresponding to the common Service Weaver application\n// configuration is parsed and returned as a *AppConfig.\n//\n// sectionValidator(key, val) is used to validate every section config entry.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc ParseConfig(file string, input string, sectionValidator func(string, string) error) (*protos.AppConfig, error) {\n\tcfg := &protos.AppConfig{}\n\tif err := toml.Unmarshal([]byte(input), cfg); err != nil {\n\t\treturn nil, err\n\t}\n\tif err := canonicalizeConfig(cfg, filepath.Dir(file)); err != nil {\n\t\treturn nil, err\n\t}\n\treturn cfg, nil\n}\n```", "  ```\nfunc ParseConfig(file string, input string, sectionValidator func(string, string) error) (*protos.AppConfig, error) {\n\t// Initialize result AppConfig.\n\tresult := &protos.AppConfig{}\n\n\t// Parse the input into a map of TOML sections.\n\tsections, err := toml.ParseSections(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Parse the Service Weaver section.\n\tif err := ParseConfigSection(\"github.com/ServiceWeaver/weaver\", \"\", sections, result); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Validate every section.\n\tfor key, val := range sections {\n\t\tif err := sectionValidator(key, val); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn result, nil\n}\n```", "  ```\nfunc ParseConfig(file string, input string, sectionValidator func(string, string) error) (*protos.AppConfig, error) {\n\tconfig, err := ParseConfigFile(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// Parse sections.\n\tsections := strings.Split(input, \"\\n\")\n\tsectionIndices := map[string]int{}\n\tfor i, section := range sections {\n\t\tkey := section[:strings.Index(section, \" =\")]\n\t\tsectionIndices[key] = i + 1\n\t}\n\n\t// Parse app section.\n\tappSection := ParseAppConfig(file, sections, sectionIndices)\n\tif appSection == nil {\n\t\treturn nil, fmt.Errorf(\"could not find app section in file %s\", file)\n\t}\n\n\t// Validate app section.\n\tif err := appSection.Validate(); err != nil {\n\t\treturn nil, fmt.Errorf(\"invalid app section: %w\", err)\n\t}\n\n\t// Parse other sections.\n\tfor _, section := range config.Sections {\n\t\tif section.Key == \"github.com/ServiceWeaver/weaver\" {\n\t\t\tcontinue\n\t\t}\n\t\tsectionConfig, err := sectionValidator(section.Key, section.Value)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"invalid section %q: %w\", section.Key, err)\n\t\t}\n\t\tappSection.Sections = append(appSection.Sections, sectionConfig)\n\t}\n\n\treturn appSection, nil\n}\n\n// ParseAppConfig parses the app section from the config file.\nfunc ParseAppConfig(file string, sections []string, sectionIndices map[string]int) *protos.AppConfig {\n\tappConfig := &protos.AppConfig{}\n\tappSection := ParseAppHeader(sections, sectionIndices)\n\tif appSection == nil {\n\t\treturn nil\n\t}\n\tappConfig.Name = appSection.Name\n\tappConfig.Binary = appSection.Binary\n\tappConfig.Args = appSection.Args\n\tappConfig.Env = appSection.Env\n\tappConfig.Colocate = appSection.Colocate\n\tappConfig.RolloutNanos = appSection.Rollout\n\treturn appConfig\n}\n\n// ParseAppHeader parses the app header from the config file.\nfunc ParseAppHeader(sections []string, sectionIndices map[string]int) *protos.AppConfig_AppHeader {\n\tappSection := ParseSection(sections, sectionIndices, \"github.com/ServiceWeaver/weaver\")\n\tif appSection == nil {\n\t\treturn nil\n\t}\n\tappConfig := &protos.AppConfig_AppHeader{}\n\tappConfig.Name = appSection.Name\n\tappConfig.Binary = appSection.Binary\n\tappConfig.Args = appSection.Args\n\tappConfig.Env = appSection.Env\n\tappConfig.Colocate = appSection.Colocate\n\tappConfig.Rollout = appSection.Rollout\n\treturn appConfig\n}\n\n// ParseSection parses a single section from the config file.\nfunc ParseSection(sections []string, sectionIndices map[string]int, key string) *protos.AppConfig_Section {\n\tsectionIndex, ok := sectionIndices[key]\n\tif !ok {\n\t\treturn nil\n\t}\n\tsection := sections[sectionIndex]\n\tsectionConfig := &protos.AppConfig_Section{}\n\tsectionConfig.Key = key\n\tsectionConfig.Value = section\n\treturn sectionConfig\n}\n\n// Validate validates a single section based on its content.\nfunc (s *protos.AppConfig_Section) Validate() error {\n\t// Validate section key.\n\tif s.Key == \"\" {\n\t\treturn fmt.Errorf(\"section key is empty\")\n\t}\n\n\t// Validate section contents.\n\t_, ok := toml.Decode(s.Value, s)\n\tif !ok {\n\t\treturn fmt.Errorf(\"section %s is not valid TOML: %s\", s.Key, s.Value)\n\t}\n\n\t// Validate field count.\n\tif s.Name == \"\" {\n\t\treturn fmt.Errorf(\"invalid"]}, "weaver-runtime/traces/db.go-OpenDB": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/traces/db.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package perfetto contains libraries for displaying trace information in the\n// Perfetto UI.\npackage traces\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/hex\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"net/http\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"time\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/ServiceWeaver/weaver/runtime/retry\"\n\t\"google.golang.org/protobuf/proto\"\n\t\"modernc.org/sqlite\"\n\tsqlite3 \"modernc.org/sqlite/lib\"\n)\n\n// DB is a trace database that stores traces on the local file system.\ntype DB struct {\n\t// Trace data is stored in a sqlite DB spread across two tables:\n\t// (1) traces:         serialized trace data, used for querying.\n\t// (2) encoded_spans:  full encoded span data, used for fetching all of the\n\t//                     spans that belong to a given trace.\n\tfname string\n\tdb    *sql.DB\n}\n\n// OpenDB opens the trace database persisted in the provided file. If the\n// file doesn't exist, this call creates it.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Close closes the trace database.\nfunc (d *DB) Close() error {\n\treturn d.db.Close()\n}\n\n// Store stores the given trace spans in the database.\nfunc (d *DB) Store(ctx context.Context, app, version string, spans *protos.TraceSpans) error {\n\t// NOTE: we insert all rows transactionally, as it is significantly faster\n\t// than inserting one row at a time [1].\n\t//\n\t// [1]: https://stackoverflow.com/questions/1711631/improve-insert-per-second-performance-of-sqlite\n\ttx, err := d.db.BeginTx(ctx, &sql.TxOptions{Isolation: sql.LevelLinearizable})\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer tx.Rollback()\n\tvar errs []error\n\tfor _, span := range spans.Span {\n\t\tif isRootSpan(span) {\n\t\t\tif err := d.storeTrace(ctx, tx, app, version, span); err != nil {\n\t\t\t\terrs = append(errs, err)\n\t\t\t}\n\t\t}\n\t\tif err := d.storeSpan(ctx, tx, span); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\tif errs != nil {\n\t\treturn errors.Join(errs...)\n\t}\n\treturn tx.Commit()\n}\n\nfunc (d *DB) storeTrace(ctx context.Context, tx *sql.Tx, app, version string, root *protos.Span) error {\n\tconst traceStmt = `INSERT INTO traces VALUES (?,?,?,?,?,?,?)`\n\t_, err := tx.ExecContext(ctx, traceStmt, hex.EncodeToString(root.TraceId), app, version, root.Name, root.StartMicros, root.EndMicros, spanStatus(root))\n\treturn err\n}\n\nfunc (d *DB) storeSpan(ctx context.Context, tx *sql.Tx, span *protos.Span) error {\n\tencoded, err := proto.Marshal(span)\n\tif err != nil {\n\t\treturn err\n\t}\n\tconst stmt = `INSERT INTO encoded_spans VALUES (?,?,?)`\n\t_, err = tx.ExecContext(ctx, stmt, hex.EncodeToString(span.TraceId), span.StartMicros, encoded)\n\treturn err\n}\n\n// TraceSummary stores summary information about a trace.\ntype TraceSummary struct {\n\tTraceID            string    // Unique trace identifier, in hex format.\n\tStartTime, EndTime time.Time // Start and end times for the trace.\n\tStatus             string    // Trace status string.\n}\n\n// QueryTraces returns the summaries of the traces that match the given\n// query arguments, namely:\n//   - That have been generated by the given application version.\n//   - That fit entirely in the given [startTime, endTime] time interval.\n//   - Whose duration is in the given [durationLower, durationUpper) range.\n//   - Who have an error status.\n//   - Who are in the most recent limit of trace spans.\n//\n// Any query argument that has a zero value (e.g., empty app or version,\n// zero endTime) is ignored, i.e., it matches all spans.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FetchSpans returns all of the spans that have a given trace id.\nfunc (d *DB) FetchSpans(ctx context.Context, traceID string) ([]*protos.Span, error) {\n\tconst query = `SELECT data FROM encoded_spans WHERE trace_id=?`\n\trows, err := d.queryDB(ctx, query, traceID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer rows.Close()\n\tvar spans []*protos.Span\n\tfor rows.Next() {\n\t\tvar encoded []byte\n\t\tif err := rows.Scan(&encoded); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tspan := &protos.Span{}\n\t\tif err := proto.Unmarshal(encoded, span); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tspans = append(spans, span)\n\t}\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\treturn spans, nil\n}\n\nfunc (d *DB) queryDB(ctx context.Context, query string, args ...any) (*sql.Rows, error) {\n\t// Keep retrying as long as we are getting the \"locked\" error.\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\trows, err := d.db.QueryContext(ctx, query, args...)\n\t\tif isLocked(err) {\n\t\t\tcontinue\n\t\t}\n\t\treturn rows, err\n\t}\n\treturn nil, ctx.Err()\n}\n\nfunc (d *DB) execDB(ctx context.Context, query string, args ...any) (sql.Result, error) {\n\t// Keep retrying as long as we are getting the \"locked\" error.\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\tres, err := d.db.ExecContext(ctx, query, args...)\n\t\tif isLocked(err) {\n\t\t\tcontinue\n\t\t}\n\t\treturn res, err\n\t}\n\treturn nil, ctx.Err()\n}\n\n// isLocked returns whether the error is a \"database is locked\" error.\nfunc isLocked(err error) bool {\n\tsqlError := &sqlite.Error{}\n\tok := errors.As(err, &sqlError)\n\treturn ok && (sqlError.Code() == sqlite3.SQLITE_BUSY || sqlError.Code() == sqlite3.SQLITE_LOCKED)\n}\n\n// isRootSpan returns true iff the given span is a root span.\nfunc isRootSpan(span *protos.Span) bool {\n\tvar nilSpanID [8]byte\n\treturn bytes.Equal(span.ParentSpanId, nilSpanID[:])\n}\n\n// spanStatus returns the span status string. It returns \"\" if the status is OK.\nfunc spanStatus(span *protos.Span) string {\n\t// Look for an error in the span status.\n\tif span.Status != nil && span.Status.Code == protos.Span_Status_ERROR {\n\t\tif span.Status.Error != \"\" {\n\t\t\treturn span.Status.Error\n\t\t} else {\n\t\t\treturn \"unknown error\"\n\t\t}\n\t}\n\n\t// Look for an HTTP error in the span attributes.\n\tfor _, attr := range span.Attributes {\n\t\tif attr.Key != \"http.status_code\" {\n\t\t\tcontinue\n\t\t}\n\t\tif attr.Value.Type != protos.Span_Attribute_Value_INT64 {\n\t\t\tcontinue\n\t\t}\n\t\tval, ok := attr.Value.Value.(*protos.Span_Attribute_Value_Num)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif val.Num >= 400 && val.Num < 600 {\n\t\t\treturn http.StatusText(int(val.Num))\n\t\t}\n\t}\n\n\t// No errors found.\n\treturn \"\"\n}\n\n```\n# Function Signature: func OpenDB(ctx context.Context, fname string) (*DB, error) {\n# Natural Language Annotations: // OpenDB opens the trace database persisted in the provided file. If the\n// file doesn't exist, this call creates it.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc OpenDB(ctx context.Context, fname string) (*DB, error) {\n\t// Open or create the SQLite database file.\n\tdb, err := sql.Open(\"sqlite\", fname)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to open DB file %q: %w\", fname, err)\n\t}\n\n\t// Create the database tables if they don't exist.\n\tif _, err := db.ExecContext(ctx, \"CREATE TABLE IF NOT EXISTS traces (trace_id TEXT PRIMARY KEY, app VARCHAR(255), version VARCHAR(255), name VARCHAR(255), start_micros INT, end_micros INT, status VARCHAR(255))\"); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create traces table: %w\", err)\n\t}\n\tif _, err := db.ExecContext(ctx, \"CREATE TABLE IF NOT EXISTS encoded_spans (trace_id TEXT, start_micros INT, data BLOB, PRIMARY KEY (trace_id, start_micros))\"); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create encoded_spans table: %w\", err)\n\t}\n\n\t// Create the DB object and return it.\n\treturn &DB{fname, db}, nil\n}\n```", "  ```\nfunc OpenDB(ctx context.Context, fname string) (*DB, error) {\n\t// If a file with the given name doesn't exist, create it.\n\tif _, err := os.Stat(fname); os.IsNotExist(err) {\n\t\tif err := os.MkdirAll(filepath.Dir(fname), 0755); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tf, err := os.Create(fname)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tdefer f.Close()\n\t}\n\n\t// Open the trace database.\n\tdb, err := sql.Open(\"sqlite3\", fname)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Validate that the schema is correct.\n\tif err := ensureSchema(ctx, db); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Return the new DB object.\n\treturn &DB{\n\t\tfname: fname,\n\t\tdb:    db,\n\t}, nil\n}\n\nfunc ensureSchema(ctx context.Context, db *sql.DB) error {\n\t_, err := db.ExecContext(ctx, schema)\n\treturn err\n}\n\nconst schema = `\nCREATE TABLE traces (\n  trace_id TEXT NOT NULL PRIMARY KEY,\n  app TEXT NOT NULL,\n  version TEXT NOT NULL,\n  root TEXT NOT NULL,\n  start_time INTEGER NOT NULL,\n  end_time INTEGER NOT NULL,\n  status TEXT NOT NULL\n);\n\nCREATE TABLE encoded_spans (\n  trace_id TEXT NOT NULL,\n  start_time INTEGER NOT NULL,\n  data BLOB NOT NULL,\n  PRIMARY KEY(trace_id, start_time)\n);\n`\n```", "  ```\nfunc OpenDB(ctx context.Context, fname string) (*DB, error) {\n\t// Open local sqlite file.\n\t// Create the file if it doesn't exist.\n\tf, err := os.OpenFile(fname, os.O_RDWR|os.O_CREATE|os.O_APPEND, 0666)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer f.Close()\n\n\t// Create a new DB instance.\n\tdb, err := sql.Open(\"sqlite3\", f.Name())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\td := &DB{\n\t\tfname: f.Name(),\n\t\tdb:    db,\n\t}\n\n\t// Prepare database.\n\tif err := d.prepare(ctx); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn d, nil\n}\n\nfunc (d *DB) prepare(ctx context.Context) error {\n\t// Create table for traces.\n\tif _, err := d.db.ExecContext(ctx, \"CREATE TABLE IF NOT EXISTS traces (trace_id TEXT PRIMARY KEY, app TEXT, version TEXT, name TEXT, start_micros INT, end_micros INT, status TEXT)\"); err != nil {\n\t\treturn err\n\t}\n\n\t// Create table for encoded spans.\n\tif _, err := d.db.ExecContext(ctx, \"CREATE TABLE IF NOT EXISTS encoded_spans (trace_id TEXT, start_micros INT, data BLOB, PRIMARY KEY (trace_id, start_micros))\"); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n```"]}, "weaver-runtime/traces/db.go-QueryTraces": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/traces/db.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package perfetto contains libraries for displaying trace information in the\n// Perfetto UI.\npackage traces\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/hex\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"net/http\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"time\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/ServiceWeaver/weaver/runtime/retry\"\n\t\"google.golang.org/protobuf/proto\"\n\t\"modernc.org/sqlite\"\n\tsqlite3 \"modernc.org/sqlite/lib\"\n)\n\n// DB is a trace database that stores traces on the local file system.\ntype DB struct {\n\t// Trace data is stored in a sqlite DB spread across two tables:\n\t// (1) traces:         serialized trace data, used for querying.\n\t// (2) encoded_spans:  full encoded span data, used for fetching all of the\n\t//                     spans that belong to a given trace.\n\tfname string\n\tdb    *sql.DB\n}\n\n// OpenDB opens the trace database persisted in the provided file. If the\n// file doesn't exist, this call creates it.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Close closes the trace database.\nfunc (d *DB) Close() error {\n\treturn d.db.Close()\n}\n\n// Store stores the given trace spans in the database.\nfunc (d *DB) Store(ctx context.Context, app, version string, spans *protos.TraceSpans) error {\n\t// NOTE: we insert all rows transactionally, as it is significantly faster\n\t// than inserting one row at a time [1].\n\t//\n\t// [1]: https://stackoverflow.com/questions/1711631/improve-insert-per-second-performance-of-sqlite\n\ttx, err := d.db.BeginTx(ctx, &sql.TxOptions{Isolation: sql.LevelLinearizable})\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer tx.Rollback()\n\tvar errs []error\n\tfor _, span := range spans.Span {\n\t\tif isRootSpan(span) {\n\t\t\tif err := d.storeTrace(ctx, tx, app, version, span); err != nil {\n\t\t\t\terrs = append(errs, err)\n\t\t\t}\n\t\t}\n\t\tif err := d.storeSpan(ctx, tx, span); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\tif errs != nil {\n\t\treturn errors.Join(errs...)\n\t}\n\treturn tx.Commit()\n}\n\nfunc (d *DB) storeTrace(ctx context.Context, tx *sql.Tx, app, version string, root *protos.Span) error {\n\tconst traceStmt = `INSERT INTO traces VALUES (?,?,?,?,?,?,?)`\n\t_, err := tx.ExecContext(ctx, traceStmt, hex.EncodeToString(root.TraceId), app, version, root.Name, root.StartMicros, root.EndMicros, spanStatus(root))\n\treturn err\n}\n\nfunc (d *DB) storeSpan(ctx context.Context, tx *sql.Tx, span *protos.Span) error {\n\tencoded, err := proto.Marshal(span)\n\tif err != nil {\n\t\treturn err\n\t}\n\tconst stmt = `INSERT INTO encoded_spans VALUES (?,?,?)`\n\t_, err = tx.ExecContext(ctx, stmt, hex.EncodeToString(span.TraceId), span.StartMicros, encoded)\n\treturn err\n}\n\n// TraceSummary stores summary information about a trace.\ntype TraceSummary struct {\n\tTraceID            string    // Unique trace identifier, in hex format.\n\tStartTime, EndTime time.Time // Start and end times for the trace.\n\tStatus             string    // Trace status string.\n}\n\n// QueryTraces returns the summaries of the traces that match the given\n// query arguments, namely:\n//   - That have been generated by the given application version.\n//   - That fit entirely in the given [startTime, endTime] time interval.\n//   - Whose duration is in the given [durationLower, durationUpper) range.\n//   - Who have an error status.\n//   - Who are in the most recent limit of trace spans.\n//\n// Any query argument that has a zero value (e.g., empty app or version,\n// zero endTime) is ignored, i.e., it matches all spans.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FetchSpans returns all of the spans that have a given trace id.\nfunc (d *DB) FetchSpans(ctx context.Context, traceID string) ([]*protos.Span, error) {\n\tconst query = `SELECT data FROM encoded_spans WHERE trace_id=?`\n\trows, err := d.queryDB(ctx, query, traceID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer rows.Close()\n\tvar spans []*protos.Span\n\tfor rows.Next() {\n\t\tvar encoded []byte\n\t\tif err := rows.Scan(&encoded); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tspan := &protos.Span{}\n\t\tif err := proto.Unmarshal(encoded, span); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tspans = append(spans, span)\n\t}\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\treturn spans, nil\n}\n\nfunc (d *DB) queryDB(ctx context.Context, query string, args ...any) (*sql.Rows, error) {\n\t// Keep retrying as long as we are getting the \"locked\" error.\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\trows, err := d.db.QueryContext(ctx, query, args...)\n\t\tif isLocked(err) {\n\t\t\tcontinue\n\t\t}\n\t\treturn rows, err\n\t}\n\treturn nil, ctx.Err()\n}\n\nfunc (d *DB) execDB(ctx context.Context, query string, args ...any) (sql.Result, error) {\n\t// Keep retrying as long as we are getting the \"locked\" error.\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\tres, err := d.db.ExecContext(ctx, query, args...)\n\t\tif isLocked(err) {\n\t\t\tcontinue\n\t\t}\n\t\treturn res, err\n\t}\n\treturn nil, ctx.Err()\n}\n\n// isLocked returns whether the error is a \"database is locked\" error.\nfunc isLocked(err error) bool {\n\tsqlError := &sqlite.Error{}\n\tok := errors.As(err, &sqlError)\n\treturn ok && (sqlError.Code() == sqlite3.SQLITE_BUSY || sqlError.Code() == sqlite3.SQLITE_LOCKED)\n}\n\n// isRootSpan returns true iff the given span is a root span.\nfunc isRootSpan(span *protos.Span) bool {\n\tvar nilSpanID [8]byte\n\treturn bytes.Equal(span.ParentSpanId, nilSpanID[:])\n}\n\n// spanStatus returns the span status string. It returns \"\" if the status is OK.\nfunc spanStatus(span *protos.Span) string {\n\t// Look for an error in the span status.\n\tif span.Status != nil && span.Status.Code == protos.Span_Status_ERROR {\n\t\tif span.Status.Error != \"\" {\n\t\t\treturn span.Status.Error\n\t\t} else {\n\t\t\treturn \"unknown error\"\n\t\t}\n\t}\n\n\t// Look for an HTTP error in the span attributes.\n\tfor _, attr := range span.Attributes {\n\t\tif attr.Key != \"http.status_code\" {\n\t\t\tcontinue\n\t\t}\n\t\tif attr.Value.Type != protos.Span_Attribute_Value_INT64 {\n\t\t\tcontinue\n\t\t}\n\t\tval, ok := attr.Value.Value.(*protos.Span_Attribute_Value_Num)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif val.Num >= 400 && val.Num < 600 {\n\t\t\treturn http.StatusText(int(val.Num))\n\t\t}\n\t}\n\n\t// No errors found.\n\treturn \"\"\n}\n\n```\n# Function Signature: func (d *DB) QueryTraces(ctx context.Context, app, version string, startTime, endTime time.Time, durationLower, durationUpper time.Duration, onlyErrors bool, limit int64) ([]TraceSummary, error) {\n# Natural Language Annotations: // QueryTraces returns the summaries of the traces that match the given\n// query arguments, namely:\n//   - That have been generated by the given application version.\n//   - That fit entirely in the given [startTime, endTime] time interval.\n//   - Whose duration is in the given [durationLower, durationUpper) range.\n//   - Who have an error status.\n//   - Who are in the most recent limit of trace spans.\n//\n// Any query argument that has a zero value (e.g., empty app or version,\n// zero endTime) is ignored, i.e., it matches all spans.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (d *DB) QueryTraces(ctx context.Context, app, version string, startTime, endTime time.Time, durationLower, durationUpper time.Duration, onlyErrors bool, limit int64) ([]TraceSummary, error) {\n\t// Build the query and parameters.\n\tvar query string\n\tvar params []any\n\tquery = \"SELECT DISTINCT trace_id, MIN(start_time) AS start_time, MAX(end_time) AS end_time FROM traces\"\n\tparams = append(params, app, version)\n\tif !startTime.IsZero() {\n\t\tquery += \" WHERE start_time <= ?\"\n\t\tparams = append(params, startTime)\n\t}\n\tif !endTime.IsZero() {\n\t\tquery += \" AND end_time >= ?\"\n\t\tparams = append(params, endTime)\n\t}\n\tif durationLower != 0 {\n\t\tquery += \" AND end_time-start_time >= ?\"\n\t\tparams = append(params, durationLower)\n\t}\n\tif durationUpper != 0 {\n\t\tquery += \" AND end_time-start_time < ?\"\n\t\tparams = append(params, durationUpper)\n\t}\n\tif onlyErrors {\n\t\tquery += \" AND status != ''\"\n\t}\n\tquery += \" GROUP BY trace_id\"\n\tif limit > 0 {\n\t\tquery += \" LIMIT ?\"\n\t\tparams = append(params, limit)\n\t}\n\n\t// Execute the query and retrieve the results.\n\trows, err := d.queryDB(ctx, query, params...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer rows.Close()\n\tvar traces []TraceSummary\n\tfor rows.Next() {\n\t\tvar trace TraceSummary\n\t\tif err := rows.Scan(&trace.TraceID, &trace.StartTime, &trace.EndTime); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\ttraces = append(traces, trace)\n\t}\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\treturn traces, nil\n}\n```", "  ```\nfunc (d *DB) QueryTraces(ctx context.Context, app, version string, startTime, endTime time.Time, durationLower, durationUpper time.Duration, onlyErrors bool, limit int64) ([]TraceSummary, error) {\n\tconst query = `\n\tSELECT t.trace_id, t.start_micros, t.end_micros, t.status \n\tFROM traces t \n\tINNER JOIN encoded_spans s ON t.trace_id = s.trace_id \n\tWHERE \n\t\t(t.app = ? OR ? = '') AND \n\t\t(t.version = ? OR ? = '') AND \n\t\t(t.start_micros >= ? OR ? = 0) AND \n\t\t(t.end_micros <= ? OR ? = 0) AND \n\t\t(t.duration_micros >= ? OR ? = 0) AND \n\t\t(t.duration_micros <= ? OR ? = 0) AND \n\t\t(t.status != '' OR ? = false) AND \n\t\t(t.trace_id IN (SELECT trace_id FROM traces LIMIT ?))\n    `\n  rows, err := d.queryDB(ctx, query, app, version, startTime, endTime, durationLower, durationUpper, onlyErrors, limit)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer rows.Close()\n\tvar summarys []TraceSummary\n\tfor rows.Next() {\n\t\tvar id string\n\t\tvar start, end time.Time\n\t\tvar status string\n\t\tif err := rows.Scan(&id, &start, &end, &status); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tsummarys = append(summarys, TraceSummary{\n\t\t\tTraceID:            id,\n\t\t\tStartTime:          start,\n\t\t\tEndTime:            end,\n\t\t\tStatus:             status,\n\t\t})\n\t}\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\treturn summarys, nil\n}\n\n```", "  ```\nfunc (d *DB) QueryTraces(ctx context.Context, app, version string, startTime, endTime time.Time, durationLower, durationUpper time.Duration, onlyErrors bool, limit int64) ([]TraceSummary, error) {\n\tconst query = `\n\tSELECT\n\t\tTraceID,\n\t\tMIN(StartTime) AS StartTime,\n\t\tMAX(EndTime) AS EndTime,\n\t\tCASE\n\t\t\tWHEN COUNT(TraceID) = 1 THEN \"OK\"\n\t\t\tELSE \"ERROR\"\n\t\tEND AS Status\n\tFROM traces\n\tWHERE\n\t\tStartTime BETWEEN ? AND ? AND\n\t\tEndTime BETWEEN ? AND ? AND\n\t\t(Version = ? OR ? IS \"\") AND\n\t\t(StartTime < ? OR ? IS \"\") AND\n\t\t(EndTime > ? OR ? IS \"\") AND\n\t\t( Status = \"ERROR\" OR ? IS FALSE) AND\n\t\t(Timestamp > ? OR ? IS  \"\")\n\tGROUP BY TraceID\n\tLIMIT ?\n\t`\n\trows, err := d.queryDB(ctx, query, startTime, endTime, startTime, endTime, version, version, endTime, endTime, app, app, version, startTime, time.Unix(0, int64(durationLower)*int64(1e9)), time.Unix(0, int64(durationUpper)*int64(1e9)), onlyErrors, endTime, limit)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer rows.Close()\n\n\tvar traces []TraceSummary\n\tfor rows.Next() {\n\t\tvar trace TraceSummary\n\t\tif err := rows.Scan(&trace.TraceID, &trace.StartTime, &trace.EndTime, &trace.Status); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\ttraces = append(traces, trace)\n\t}\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\treturn traces, nil\n}\n```"]}, "weaver-runtime/retry/retry.go-Continue": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/retry/retry.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package retry contains code to perform retries with exponential backoff.\n//\n// Example: loop until doSomething() returns true or context hits deadline or is canceled.\n//\n//\tfor r := retry.Begin(); r.Continue(ctx); {\n//\t  if doSomething() {\n//\t    break\n//\t  }\n//\t}\npackage retry\n\nimport (\n\t\"context\"\n\t\"math\"\n\t\"math/rand\"\n\t\"sync\"\n\t\"time\"\n)\n\n// Retry holds state for managing retry loops with exponential backoff and jitter.\ntype Retry struct {\n\toptions Options\n\tattempt int\n}\n\n// Options are the options that configure a retry loop. Before the ith\n// iteration of a retry loop, retry.Continue() sleeps for a duration of\n// BackoffMinDuration * BackoffMultiplier^i, with added jitter.\ntype Options struct {\n\tBackoffMultiplier  float64 // If specified, must be at least 1.\n\tBackoffMinDuration time.Duration\n}\n\n// DefaultOptions is the default set of Options.\nvar DefaultOptions = Options{\n\tBackoffMultiplier:  1.3,\n\tBackoffMinDuration: 10 * time.Millisecond,\n}\n\nvar (\n\trngMu sync.Mutex\n\trng   *rand.Rand\n)\n\n// Begin initiates a new retry loop.\nfunc Begin() *Retry {\n\treturn BeginWithOptions(DefaultOptions)\n}\n\n// BeginWithOptions returns a new retry loop configured with the provided\n// options.\n//\n// Example: Sleep 1 second, then 2 seconds, then 4 seconds, and so on.\n//\n//\topts := retry.Options{\n//\t  BackoffMultiplier: 2.0,\n//\t  BackoffMinDuration: time.Second,\n//\t}\n//\tfor r := retry.BeginWithOptions(opts); r.Continue(ctx); {\n//\t  // Do nothing.\n//\t}\nfunc BeginWithOptions(options Options) *Retry {\n\treturn &Retry{options: options}\n}\n\n// Continue sleeps for an exponentially increasing interval (with jitter). It\n// stops its sleep early and returns false if context becomes done. If the\n// return value is false, ctx.Err() is guaranteed to be non-nil. The first\n// call does not sleep.\n\n\n\n\n\n\n\n\n// Reset resets a Retry to its initial state. Reset is useful if you want to\n// retry an operation with exponential backoff, but only if it is failing. For\n// example:\n//\n//\tfor r := retry.Begin(); r.Continue(ctx); {\n//\t    if err := doSomething(); err != nil {\n//\t        // Retry with backoff if we fail.\n//\t        continue\n//\t    }\n//\t    // Retry immediately if we succeed.\n//\t    r.Reset()\n//\t}\nfunc (r *Retry) Reset() {\n\tr.attempt = 0\n}\n\nfunc backoffDelay(i int, opts Options) time.Duration {\n\tmult := math.Pow(opts.BackoffMultiplier, float64(i))\n\treturn time.Duration(float64(opts.BackoffMinDuration) * mult)\n}\n\n// randomized sleeps for a random duration close to d, or until context is done,\n// whichever occurs first.\nfunc randomized(ctx context.Context, d time.Duration) {\n\tconst jitter = 0.4\n\tmult := 1 - jitter*randomFloat() // Subtract up to 40%\n\tsleep(ctx, time.Duration(float64(d)*mult))\n}\n\n// sleep sleeps for the specified duration d, or until context is done,\n// whichever occurs first.\nfunc sleep(ctx context.Context, d time.Duration) {\n\tt := time.NewTimer(d)\n\tdefer t.Stop()\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn\n\tcase <-t.C:\n\t}\n}\n\nfunc randomFloat() float64 {\n\t// Do not use the default rng since we do not want different processes\n\t// to pick the same deterministic random sequence.\n\trngMu.Lock()\n\tdefer rngMu.Unlock()\n\tif rng == nil {\n\t\trng = rand.New(rand.NewSource(time.Now().UnixNano()))\n\t}\n\treturn rng.Float64()\n}\n\n```\n# Function Signature: func (r *Retry) Continue(ctx context.Context) bool {\n# Natural Language Annotations: // Continue sleeps for an exponentially increasing interval (with jitter). It\n// stops its sleep early and returns false if context becomes done. If the\n// return value is false, ctx.Err() is guaranteed to be non-nil. The first\n// call does not sleep.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (r *Retry) Continue(ctx context.Context) bool {\n\tif r.attempt == 0 {\n\t\treturn true\n\t}\n\td := backoffDelay(r.attempt, r.options)\n\trandomized(ctx, d)\n\tr.attempt++\n\treturn ctx.Err() == nil\n}\n```", "  ```\nfunc (r *Retry) Continue(ctx context.Context) bool {\n\t// If this is the first call, do nothing and return true.\n\tif r.attempt == 0 {\n\t\treturn true\n\t}\n\t// Determine the time to sleep.\n\tsleep := backoffDelay(r.attempt, r.options)\n\t// Apply jitter.\n\trandomized(ctx, sleep)\n\t// Check if context is done.\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn false\n\tdefault:\n\t\t// Continue the loop.\n\t\tr.attempt++\n\t\treturn true\n\t}\n}\n```", "  ```\nfunc (r *Retry) Continue(ctx context.Context) bool {\n\tif r.attempt > 0 {\n\t\t// The first iteration of the loop uses a randomized delay\n\t\t// to minimize contention with other processes.\n\t\trandomized(ctx, backoffDelay(r.attempt-1, r.options))\n\t}\n\tr.attempt++\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn false\n\tdefault:\n\t\treturn true\n\t}\n}\n```"]}, "weaver-runtime/colors/tabularizer.go-Row": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/colors/tabularizer.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage colors\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"strings\"\n)\n\nvar dimColor = Color256(245) // a light gray\n\n// An Atom is a segment of text with a single formatting style.\ntype Atom struct {\n\tS         string // the text\n\tUnderline bool   // is it underlined?\n\tBold      bool   // is it bold?\n\tColor     Code   // text color\n}\n\n// String returns the atom with the appropriate ANSI escape sequences.\nfunc (a Atom) String() string {\n\tif !Enabled() {\n\t\treturn a.S\n\t}\n\n\tvar b strings.Builder\n\tb.WriteString(string(a.Color))\n\tif a.Underline {\n\t\tb.WriteString(string(Underline))\n\t}\n\tif a.Bold {\n\t\tb.WriteString(string(Bold))\n\t}\n\tb.WriteString(a.S)\n\tb.WriteString(string(Reset))\n\treturn b.String()\n}\n\n// dimmed returns a copy of the atom with a dim gray color.\nfunc (a Atom) dimmed() Atom {\n\ta.Color = dimColor\n\treturn a\n}\n\n// Text represents a contiguous sequence of atoms.\ntype Text []Atom\n\n// len returns the length of the printable characters in the text's constituent\n// atoms. ANSI escape sequences are not counted as part of this length.\nfunc (t Text) len() int {\n\treturn len(t.raw())\n}\n\n// raw returns the raw underlying text without any ANSI escape sequences.\nfunc (t Text) raw() string {\n\tvar b strings.Builder\n\tfor _, a := range t {\n\t\tb.WriteString(a.S)\n\t}\n\treturn b.String()\n}\n\n// String returns the text with the appropriate ANSI escape sequences.\nfunc (t Text) String() string {\n\tvar b strings.Builder\n\tfor _, a := range t {\n\t\tb.WriteString(a.String())\n\t}\n\treturn b.String()\n}\n\n// dimmed returns a copy of the text with a dim gray color.\nfunc (t Text) dimmed() Text {\n\tcloned := make(Text, len(t))\n\tfor i, a := range t {\n\t\tcloned[i] = a.dimmed()\n\t}\n\treturn Text(cloned)\n}\n\n// A Tabularizer produces pretty-printed tabularized text. Unlike\n// tabwriter.Writer [1], Tabularizer properly handles text with ANSI escape\n// codes. Here's what an example table looks like:\n//\n//\t\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n//\t\u2502 CATS                  \u2502\n//\t\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n//\t\u2502 NAME   \u2502 AGE \u2502 COLOR  \u2502\n//\t\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n//\t\u2502 belle  \u2502 1y  \u2502 tortie \u2502\n//\t\u2502 sidney \u2502 2y  \u2502 calico \u2502\n//\t\u2502 dakota \u2502 8m  \u2502 tuxedo \u2502\n//\t\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n//\n// The table format comes from duf [2].\n//\n// [1]: https://pkg.go.dev/text/tabwriter\n// [2]: https://github.com/muesli/duf\ntype Tabularizer struct {\n\tw      io.Writer // where to write\n\ttitle  []Text    // table title\n\trows   [][]Text  // buffered rows\n\twidths []int     // column widths\n\tdim    func(prev, row []string) []bool\n}\n\n// NewTabularizer returns a new tabularizer. The provided dim function\n// determines which columns in a row, if any, are dimmed.\nfunc NewTabularizer(w io.Writer, title []Text, dim func(prev, row []string) []bool) *Tabularizer {\n\treturn &Tabularizer{w: w, title: title, dim: dim}\n}\n\n// Row buffers a new Row to be tabularized. The Row isn't written until Flush\n// is called. Note that every Row reported to a tabularizer must be the same\n// length. A value can be a text, atom, string, or fmt.Stringer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Flush writes all buffered rows. Flush should only be called once, after all\n// rows have been written.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NoDim doesn't dim any columns.\nfunc NoDim(prev, row []string) []bool {\n\treturn make([]bool, len(row))\n}\n\n// PrefixDim dims the longest prefix of row that is identical to prev.\nfunc PrefixDim(prev, row []string) []bool {\n\tcols := make([]bool, len(row))\n\tfor i := 0; i < len(prev); i++ {\n\t\tif prev[i] == row[i] {\n\t\t\tcols[i] = true\n\t\t} else {\n\t\t\treturn cols\n\t\t}\n\t}\n\treturn cols\n}\n\n// FullDim dims any columns that are identical in prev.\nfunc FullDim(prev, row []string) []bool {\n\tcols := make([]bool, len(row))\n\tfor i := 0; i < len(prev); i++ {\n\t\tif prev[i] == row[i] {\n\t\t\tcols[i] = true\n\t\t}\n\t}\n\treturn cols\n}\n\n```\n# Function Signature: func (t *Tabularizer) Row(values ...any) {\n# Natural Language Annotations: // Row buffers a new Row to be tabularized. The Row isn't written until Flush\n// is called. Note that every Row reported to a tabularizer must be the same\n// length. A value can be a text, atom, string, or fmt.Stringer.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (t *Tabularizer) Row(values ...any) {\n\tt.rows = append(t.rows, t.row(values...))\n}\n\nfunc (t *Tabularizer) row(values ...any) []Text {\n\trow := make([]Text, len(values))\n\tfor i, v := range values {\n\t\tswitch v := v.(type) {\n\t\tcase Atom:\n\t\t\trow[i] = Text{v}\n\t\tcase Text:\n\t\t\trow[i] = v\n\t\tcase string:\n\t\t\trow[i] = Text{NewAtom(v)}\n\t\tcase fmt.Stringer:\n\t\t\trow[i] = Text{NewAtom(v.String())}\n\t\tdefault:\n\t\t\tpanic(fmt.Sprintf(\"invalid row value: %v\", v))\n\t\t}\n\t}\n\treturn row\n}\n```", "  ```\nfunc (t *Tabularizer) Row(values ...any) {\n\tif len(values) != len(t.title) {\n\t\tpanic(fmt.Sprintf(\"tabularizer: invalid row length %d, expected %d\", len(values), len(t.title)))\n\t}\n\trow := make([]Text, len(values))\n\tfor i, value := range values {\n\t\tswitch v := value.(type) {\n\t\tcase Text:\n\t\t\trow[i] = v\n\t\tcase Atom:\n\t\t\trow[i] = Text{v}\n\t\tcase string:\n\t\t\trow[i] = Text{Atom{S: v}}\n\t\tcase fmt.Stringer:\n\t\t\trow[i] = Text{Atom{S: v.String()}}\n\t\tdefault:\n\t\t\tpanic(fmt.Sprintf(\"tabularizer: invalid row type: %T\", value))\n\t\t}\n\t}\n\tif len(t.rows) > 0 && len(row) != len(t.rows[0]) {\n\t\tpanic(fmt.Sprintf(\"tabularizer: invalid row length %d, expected %d\", len(row), len(t.rows[0])))\n\t}\n\tt.rows = append(t.rows, row)\n}\n```", "  ```\nfunc (t *Tabularizer) Row(values ...any) {\n\tif len(values) != len(t.rows[0]) {\n\t\tpanic(\"Row with different length than header\")\n\t}\n\trow := make([]Text, len(values))\n\tfor i, v := range values {\n\t\tswitch v.(type) {\n\t\tcase Text:\n\t\t\trow[i] = v.(Text)\n\t\tcase Atom:\n\t\t\trow[i] = Text{v.(Atom)}\n\t\tcase string:\n\t\t\trow[i] = Text{Atom{S: v.(string)}}\n\t\tcase fmt.Stringer:\n\t\t\trow[i] = Text{Atom{S: v.(fmt.Stringer).String()}}\n\t\tdefault:\n\t\t\tpanic(\"Row with unknown type\")\n\t\t}\n\t}\n\tt.rows = append(t.rows, row)\n}\n```"]}, "weaver-runtime/colors/tabularizer.go-Flush": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/colors/tabularizer.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage colors\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"strings\"\n)\n\nvar dimColor = Color256(245) // a light gray\n\n// An Atom is a segment of text with a single formatting style.\ntype Atom struct {\n\tS         string // the text\n\tUnderline bool   // is it underlined?\n\tBold      bool   // is it bold?\n\tColor     Code   // text color\n}\n\n// String returns the atom with the appropriate ANSI escape sequences.\nfunc (a Atom) String() string {\n\tif !Enabled() {\n\t\treturn a.S\n\t}\n\n\tvar b strings.Builder\n\tb.WriteString(string(a.Color))\n\tif a.Underline {\n\t\tb.WriteString(string(Underline))\n\t}\n\tif a.Bold {\n\t\tb.WriteString(string(Bold))\n\t}\n\tb.WriteString(a.S)\n\tb.WriteString(string(Reset))\n\treturn b.String()\n}\n\n// dimmed returns a copy of the atom with a dim gray color.\nfunc (a Atom) dimmed() Atom {\n\ta.Color = dimColor\n\treturn a\n}\n\n// Text represents a contiguous sequence of atoms.\ntype Text []Atom\n\n// len returns the length of the printable characters in the text's constituent\n// atoms. ANSI escape sequences are not counted as part of this length.\nfunc (t Text) len() int {\n\treturn len(t.raw())\n}\n\n// raw returns the raw underlying text without any ANSI escape sequences.\nfunc (t Text) raw() string {\n\tvar b strings.Builder\n\tfor _, a := range t {\n\t\tb.WriteString(a.S)\n\t}\n\treturn b.String()\n}\n\n// String returns the text with the appropriate ANSI escape sequences.\nfunc (t Text) String() string {\n\tvar b strings.Builder\n\tfor _, a := range t {\n\t\tb.WriteString(a.String())\n\t}\n\treturn b.String()\n}\n\n// dimmed returns a copy of the text with a dim gray color.\nfunc (t Text) dimmed() Text {\n\tcloned := make(Text, len(t))\n\tfor i, a := range t {\n\t\tcloned[i] = a.dimmed()\n\t}\n\treturn Text(cloned)\n}\n\n// A Tabularizer produces pretty-printed tabularized text. Unlike\n// tabwriter.Writer [1], Tabularizer properly handles text with ANSI escape\n// codes. Here's what an example table looks like:\n//\n//\t\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n//\t\u2502 CATS                  \u2502\n//\t\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n//\t\u2502 NAME   \u2502 AGE \u2502 COLOR  \u2502\n//\t\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n//\t\u2502 belle  \u2502 1y  \u2502 tortie \u2502\n//\t\u2502 sidney \u2502 2y  \u2502 calico \u2502\n//\t\u2502 dakota \u2502 8m  \u2502 tuxedo \u2502\n//\t\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n//\n// The table format comes from duf [2].\n//\n// [1]: https://pkg.go.dev/text/tabwriter\n// [2]: https://github.com/muesli/duf\ntype Tabularizer struct {\n\tw      io.Writer // where to write\n\ttitle  []Text    // table title\n\trows   [][]Text  // buffered rows\n\twidths []int     // column widths\n\tdim    func(prev, row []string) []bool\n}\n\n// NewTabularizer returns a new tabularizer. The provided dim function\n// determines which columns in a row, if any, are dimmed.\nfunc NewTabularizer(w io.Writer, title []Text, dim func(prev, row []string) []bool) *Tabularizer {\n\treturn &Tabularizer{w: w, title: title, dim: dim}\n}\n\n// Row buffers a new Row to be tabularized. The Row isn't written until Flush\n// is called. Note that every Row reported to a tabularizer must be the same\n// length. A value can be a text, atom, string, or fmt.Stringer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Flush writes all buffered rows. Flush should only be called once, after all\n// rows have been written.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NoDim doesn't dim any columns.\nfunc NoDim(prev, row []string) []bool {\n\treturn make([]bool, len(row))\n}\n\n// PrefixDim dims the longest prefix of row that is identical to prev.\nfunc PrefixDim(prev, row []string) []bool {\n\tcols := make([]bool, len(row))\n\tfor i := 0; i < len(prev); i++ {\n\t\tif prev[i] == row[i] {\n\t\t\tcols[i] = true\n\t\t} else {\n\t\t\treturn cols\n\t\t}\n\t}\n\treturn cols\n}\n\n// FullDim dims any columns that are identical in prev.\nfunc FullDim(prev, row []string) []bool {\n\tcols := make([]bool, len(row))\n\tfor i := 0; i < len(prev); i++ {\n\t\tif prev[i] == row[i] {\n\t\t\tcols[i] = true\n\t\t}\n\t}\n\treturn cols\n}\n\n```\n# Function Signature: func (t *Tabularizer) Flush() {\n# Natural Language Annotations: // Flush writes all buffered rows. Flush should only be called once, after all\n// rows have been written.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (t *Tabularizer) Flush() {\n\tif len(t.rows) == 0 {\n\t\treturn\n\t}\n\n\t// calculate column widths if necessary\n\tif t.widths == nil {\n\t\tt.widths = make([]int, 0, len(t.title))\n\t\tfor _, atom := range t.title {\n\t\t\tt.widths = append(t.widths, atom.len())\n\t\t}\n\t\tfor _, row := range t.rows {\n\t\t\tfor i, atom := range row {\n\t\t\t\tif len(atom.S) > t.widths[i] {\n\t\t\t\t\tt.widths[i] = atom.len()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// write title\n\tfmt.Fprintf(t.w, \"\\n%s\\n\", t.title)\n\n\t// write separator\n\tvar sep string\n\tfor _, width := range t.widths {\n\t\tsep += strings.Repeat(\"\u2500\", width+2) + \"\u253c\"\n\t}\n\tsep = sep[:len(sep)-1] + \"\u2524\\n\"\n\tfmt.Fprintf(t.w, \"\u2502 %s \u2502\\n\", sep)\n\n\t// write each row\n\tfor _, row := range t.rows {\n\t\tvar s string\n\t\tfor i, atom := range row {\n\t\t\tdimmed := t.dim(t.prevRow, row)\n\t\t\tif i > 0 {\n\t\t\t\ts += \"\u2502 \"\n\t\t\t}\n\t\t\ts += atom.String()\n\t\t\tif i > 0 && i < len(row)-1 {\n\t\t\t\ts += strings.Repeat(\" \", t.widths[i]-atom.len()+1)\n\t\t\t}\n\t\t\tif dimmed[i] {\n\t\t\t\ts = fmt.Sprintf(\"%s\\x1b[1;30m%s\\x1b[0m\", s[:4], s[4:])\n\t\t\t}\n\t\t}\n\t\tfmt.Fprintf(t.w, s+\"\\n\")\n\t\tt.prevRow = row\n\t}\n\n\t// write footer\n\tfmt.Fprintf(t.w, \"\u2570\")\n\tfor _, width := range t.widths {\n\t\tfmt.Fprintf(t.w, strings.Repeat(\"\u2500\", width+2)+\"\u2534\")\n\t}\n\tfmt.Fprintf(t.w, \"\u2518\\n\")\n\n\t// clear buffer\n\tt.clear()\n}\n```", "  ```\nfunc (t *Tabularizer) Flush() {\n    if t.widths == nil {\n        // If the widths are not specified, find the max widths for each column\n        t.widths = make([]int, len(t.rows[0]))\n        for i := range t.widths {\n            for _, row := range t.rows {\n                if len(row[i].String()) > t.widths[i] {\n                    t.widths[i] = len(row[i].String())\n                }\n            }\n        }\n    }\n    // Print the table title\n    print(\"\\n\")\n    fmt.Fprint(t.w, t.title.String())\n    // Print the header and column separators\n    for _, width := range t.widths {\n        fmt.Fprint(t.w, \"\u2502\")\n        fmt.Fprintf(t.w, \"%-*s\", width, \"\u2500\")\n    }\n    fmt.Fprint(t.w, \"\u2502\\n\")\n    // Print each row, dimming the columns if necessary\n    for i, row := range t.rows {\n        prev := []string{}\n        if i > 0 {\n            prev = t.rows[i-1].raw()\n        }\n        dims := t.dim(prev, row.raw())\n        for j := range row {\n            fmt.Fprint(t.w, \"\u2502\")\n            text := row[j]\n            if dims[j] {\n                text = text.dimmed()\n            }\n            fmt.Fprintf(t.w, \"%-*s\", t.widths[j], text.String())\n        }\n        fmt.Fprint(t.w, \"\u2502\\n\")\n    }\n    // Print the end of the table\n    print(\"\\n\")\n    fmt.Fprint(t.w, \"\u2570\")\n    for _, width := range t.widths {\n        fmt.Fprint(t.w, \"\u2500\")\n    }\n    fmt.Fprint(t.w, \"\u256f\\n\")\n}\n```", "  ```\nfunc (t *Tabularizer) Flush() {\n\tt.ensureActive()\n\n\t// offset from left edge\n\tvar offset int\n\tconst borderLen = 2\n\tconst padding = 2\n\n\t// Write the title row. Spacer only.\n\tfor _, atom := range t.title {\n\t\tfmt.Fprint(t.w, atom)\n\t}\n\tfmt.Fprintln(t.w)\n\n\t// Write the table border.\n\tfmt.Fprintln(t.w, string(box))\n\n\t// Write the content rows.\n\tfor ridx, row := range t.rows {\n\t\t// Skip the head row.\n\t\tif ridx >= 1 {\n\t\t\t// Write the border and spacing.\n\t\t\tfmt.Fprintf(t.w, TABTABLE_VERT, string(box))\n\t\t\tfor i := 0; i < borderLen; i++ {\n\t\t\t\tfmt.Fprint(t.w, strings.Repeat(\" \", padding))\n\t\t\t}\n\n\t\t\t// Calculate the column offsets.\n\t\t\tcolOffsets := make([]int, len(row)-1)\n\t\t\ttotal := 0\n\t\t\tfor i, col := range row {\n\t\t\t\tif i > 0 {\n\t\t\t\t\tcolOffsets[i-1] = offset\n\t\t\t\t\toffset += col.len() + padding\n\t\t\t\t}\n\t\t\t\ttotal += col.len() + padding\n\t\t\t}\n\n\t\t\t// Write each column.\n\t\t\tfor idx, col := range row {\n\t\t\t\t// Start on the correct offset.\n\t\t\t\tfmt.Fprintf(t.w, \"\\x1b[G\", colOffsets[idx])\n\n\t\t\t\t// Write the string.\n\t\t\t\tfmt.Fprint(t.w, col)\n\t\t\t}\n\n\t\t\t// Write the border and spacing.\n\t\t\tfmt.Fprintln(t.w)\n\t\t\tfmt.Fprintf(t.w, TABTABLE_VERT, string(box))\n\t\t}\n\n\t\t// Write the spacer.\n\t\tfmt.Fprintln(t.w)\n\t}\n\n\t// Write the table border.\n\tfmt.Fprintln(t.w, string(box))\n}\n```"]}, "weaver-runtime/protomsg/io.go-Write": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/protomsg/io.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package protomsg contains protobuf-related utilities.\npackage protomsg\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\n\t\"google.golang.org/protobuf/proto\"\n)\n\n// maxMsgSize is the largest supported protobuf message.\nconst maxMsgSize = math.MaxInt32\n\n// Write writes a length prefixed protobuf to dst. Use Read to read it.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Read reads a length-prefixed protobuf from src. Messages above maxMsgSize\n// are not supported and cause an error to be returned.\nfunc Read(src io.Reader, msg proto.Message) error {\n\tvar hdr [4]byte\n\tif _, err := io.ReadFull(src, hdr[:]); err != nil {\n\t\treturn fmt.Errorf(\"read protobuf length: %w\", err)\n\t}\n\tn := binary.LittleEndian.Uint32(hdr[:])\n\tif n > maxMsgSize {\n\t\treturn fmt.Errorf(\"read protobuf: message size %d is too large\", n)\n\t}\n\tdata := make([]byte, int(n))\n\tif _, err := io.ReadFull(src, data); err != nil {\n\t\treturn fmt.Errorf(\"read protobuf data %d: %w\", n, err)\n\t}\n\treturn proto.Unmarshal(data, msg)\n}\n\n```\n# Function Signature: func Write(dst io.Writer, msg proto.Message) error {\n# Natural Language Annotations: // Write writes a length prefixed protobuf to dst. Use Read to read it.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc Write(dst io.Writer, msg proto.Message) error {\n\tdata, err := proto.Marshal(msg)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"marshal protobuf: %w\", err)\n\t}\n\thdr := make([]byte, 4)\n\tbinary.LittleEndian.PutUint32(hdr, uint32(len(data)))\n\tif _, err := dst.Write(hdr); err != nil {\n\t\treturn fmt.Errorf(\"write protobuf size: %w\", err)\n\t}\n\tif _, err := dst.Write(data); err != nil {\n\t\treturn fmt.Errorf(\"write protobuf data: %w\", err)\n\t}\n\treturn nil\n}\n```", "  ```\nfunc Write(dst io.Writer, msg proto.Message) error {\n\t// encode the message with protobuf\n\tdata, err := proto.Marshal(msg)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"marshal protobuf: %w\", err)\n\t}\n\n\t// write the message length prefixed\n\thdr := make([]byte, 4)\n\tbinary.LittleEndian.PutUint32(hdr, uint32(len(data)))\n\tif _, err := dst.Write(hdr); err != nil {\n\t\treturn fmt.Errorf(\"write protobuf length: %w\", err)\n\t}\n\tif _, err := dst.Write(data); err != nil {\n\t\treturn fmt.Errorf(\"write protobuf data: %w\", err)\n\t}\n\treturn nil\n}\n```", "  ```\nfunc Write(dst io.Writer, msg proto.Message) error {\n\t// Write the length of the message as a 4-byte little-endian integer.\n\tb := make([]byte, 4)\n\tbinary.LittleEndian.PutUint32(b, uint32(proto.Size(msg)))\n\tif _, err := dst.Write(b); err != nil {\n\t\treturn fmt.Errorf(\"write protobuf length: %w\", err)\n\t}\n\n\t// Write the message itself.\n\tif _, err := dst.Write(msg.XXX_Unrecognized); err != nil {\n\t\treturn fmt.Errorf(\"write protobuf message: %w\", err)\n\t}\n\treturn nil\n}\n```"]}, "weaver-runtime/bin/bin.go-ReadComponentGraph": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/bin/bin.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package bin contains code to extract data from a Service Weaver binary.\npackage bin\n\nimport (\n\t\"bytes\"\n\t\"debug/buildinfo\"\n\t\"debug/elf\"\n\t\"debug/macho\"\n\t\"debug/pe\"\n\t\"fmt\"\n\t\"os\"\n\t\"regexp\"\n\t\"strconv\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/graph\"\n\t\"github.com/ServiceWeaver/weaver/runtime/version\"\n\t\"golang.org/x/exp/maps\"\n\t\"golang.org/x/exp/slices\"\n)\n\n// versionData exists to embed the weaver module version and deployer API\n// version into a Service Weaver binary. We split declaring and assigning\n// versionData to prevent the compiler from erasing it.\n//\n//lint:ignore U1000 See comment above.\nvar versionData string\n\nfunc init() {\n\t// NOTE that versionData must be assigned a string constant that reflects\n\t// the value of version.DeployerVersion. If the string is not a\n\t// constant---if we try to use fmt.Sprintf, for example---it will not be\n\t// embedded in a Service Weaver binary.\n\tversionData = \"\u27e6wEaVeRvErSiOn:deployer=v0.24.0\u27e7\"\n}\n\n// rodata returns the read-only data section of the provided binary.\nfunc rodata(file string) ([]byte, error) {\n\tf, err := os.Open(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer f.Close()\n\n\t// Look at first few bytes to determine the file format.\n\tprefix := make([]byte, 4)\n\tif _, err := f.ReadAt(prefix, 0); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Handle the file formats we support.\n\tswitch {\n\tcase bytes.HasPrefix(prefix, []byte(\"\\x7FELF\")): // Linux\n\t\tf, err := elf.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\".rodata\").Data()\n\tcase bytes.HasPrefix(prefix, []byte(\"MZ\")): // Windows\n\t\tf, err := pe.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\".rdata\").Data()\n\tcase bytes.HasPrefix(prefix, []byte(\"\\xFE\\xED\\xFA\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\"__rodata\").Data()\n\tcase bytes.HasPrefix(prefix[1:], []byte(\"\\xFA\\xED\\xFE\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\"__rodata\").Data()\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown format\")\n\t}\n}\n\n// ReadComponentGraph reads component graph information from the specified\n// binary. It returns a slice of components and a component graph whose nodes\n// are indices into that slice.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ReadListeners reads the sets of listeners associated with each component\n// in the specified binary.\n\n\n\n\n\n\n\n\ntype Versions struct {\n\tModuleVersion   string         // Service Weaver library's module version\n\tDeployerVersion version.SemVer // see version.DeployerVersion\n}\n\n// ReadVersions reads the module version and deployer API version from the\n// specified binary.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// extractModuleVersion returns the version of the Service Weaver library\n// embedded in data.\nfunc extractModuleVersion(filename string) (string, error) {\n\tinfo, err := buildinfo.ReadFile(filename)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Find the Service Weaver module.\n\tconst weaverModule = \"github.com/ServiceWeaver/weaver\"\n\tfor _, m := range append(info.Deps, &info.Main) {\n\t\tif m.Path == weaverModule {\n\t\t\treturn m.Version, nil\n\t\t}\n\t}\n\treturn \"\", fmt.Errorf(\"Service Weaver module was not linked into the application binary: that's an error\")\n}\n\n// extractDeployerVersion returns the deployer API version embedded\n// in data.\nfunc extractDeployerVersion(data []byte) (version.SemVer, error) {\n\tre := regexp.MustCompile(`\u27e6wEaVeRvErSiOn:deployer=v([0-9]*?)\\.([0-9]*?)\\.([0-9]*?)\u27e7`)\n\tm := re.FindSubmatch(data)\n\tif m == nil {\n\t\treturn version.SemVer{}, fmt.Errorf(\"embedded versions not found\")\n\t}\n\tv := version.SemVer{}\n\tfor _, segment := range []struct {\n\t\tname string\n\t\tdata []byte\n\t\tdst  *int\n\t}{\n\t\t{\"deployer major version\", m[1], &v.Major},\n\t\t{\"deployer minor version\", m[2], &v.Minor},\n\t\t{\"deployer patch version\", m[3], &v.Patch},\n\t} {\n\t\ts := string(segment.data)\n\t\tx, err := strconv.Atoi(s)\n\t\tif err != nil {\n\t\t\treturn version.SemVer{}, fmt.Errorf(\"invalid embedded %s %q: %w\", segment.name, s, err)\n\t\t}\n\t\t*segment.dst = x\n\t}\n\treturn v, nil\n}\n\n```\n# Function Signature: func ReadComponentGraph(file string) ([]string, graph.Graph, error) {\n# Natural Language Annotations: // ReadComponentGraph reads component graph information from the specified\n// binary. It returns a slice of components and a component graph whose nodes\n// are indices into that slice.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc ReadComponentGraph(file string) ([]string, graph.Graph, error) {\n\tdata, err := rodata(file)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\t// Extract the embeded version data.\n\tmoduleVersion, err := extractModuleVersion(file)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tdeployerVersion, err := extractDeployerVersion(data)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\t// Read the graph.\n\toff, err := codegen.ReadUint32(data)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tgraph := codegen.NewGraph()\n\tif err := codegen.ReadGRAPH(data[off:], graph); err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\treturn []string{moduleVersion, deployerVersion}, graph, nil\n}\n```", "  ```\nfunc ReadComponentGraph(file string) ([]string, graph.Graph, error) {\n\n    // Open the binary file\n\tf, err := os.Open(file)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tdefer f.Close()\n\n    // Look at the first few bytes to determine the file format.\n\tprefix := make([]byte, 4)\n\tif _, err := f.ReadAt(prefix, 0); err != nil {\n\t\treturn nil, nil, err\n\t}\n\n    // Handle the file formats we support.\n\tswitch {\n\tcase bytes.HasPrefix(prefix, []byte(\"\\x7FELF\")): // Linux\n\t\tf, err := elf.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n        // Extract the read-only data section\n\t\trodata, err := f.Section(\".rodata\")\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n        // Get the list of components\n\t\tcomponents, err := extractComponents(rodata.Data())\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n        // Create the component graph\n\t\tg, err := constructGraph(components)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n\t\treturn components, g, nil\n\tcase bytes.HasPrefix(prefix, []byte(\"MZ\")): // Windows\n\t\tf, err := pe.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n        // Extract the read-only data section\n\t\trodata, err := f.Section(\".rdata\")\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n        // Get the list of components\n\t\tcomponents, err := extractComponents(rodata.Data())\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n        // Create the component graph\n\t\tg, err := constructGraph(components)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n\t\treturn components, g, nil\n\tcase bytes.HasPrefix(prefix, []byte(\"\\xFE\\xED\\xFA\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n        // Extract the read-only data section\n\t\trodata, err := f.Section(\"__rodata\")\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n        // Get the list of components\n\t\tcomponents, err := extractComponents(rodata.Data())\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n        // Create the component graph\n\t\tg, err := constructGraph(components)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n\t\treturn components, g, nil\n\tcase bytes.HasPrefix(prefix[1:], []byte(\"\\xFA\\xED\\xFE\")): // MacOS\n\t\tf, err = macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n        // Extract the read-only data section\n\t\trodata, err := f.Section(\"__rodata\")\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n        // Get the list of components\n\t\tcomponents, err := extractComponents(rodata.Data())\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n        // Create the component graph\n\t\tg, err := constructGraph(components)\n\t\tif err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\n\t\treturn components, g, nil\n\tdefault:\n\t\treturn nil, nil, fmt.Errorf(\"unknown format\")\n\t}\n}\n\nfunc extractComponents(data []byte) ([]string, error) {\n\t// Search for", "  ```\nfunc ReadComponentGraph(file string) ([]string, graph.Graph, error) {\n\tdata, err := rodata(file)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tvar Versions Versions\n\tif err := extractVersions(data, &Versions); err != nil {\n\t\treturn nil, nil, err\n\t}\n\tcomponentIDs, err := readComponentIDs(data, Versions.ModuleVersion)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tcomponentNames, err := readComponentNames(data, Versions.ModuleVersion, componentIDs)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tcomponentListenerIDs, err := readComponentListenerIDs(data, Versions.ModuleVersion, componentIDs)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tgraph, err := assembleGraph(componentIDs, componentListenerIDs)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn componentNames, graph, nil\n}\n\nfunc extractVersions(data []byte, v *Versions) error {\n\tvar err error\n\tv.ModuleVersion, err = extractModuleVersion(data)\n\tif err != nil {\n\t\treturn err\n\t}\n\tv.DeployerVersion, err = extractDeployerVersion(data)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc readComponentIDs(data []byte, moduleVersion string) ([]int, error) {\n\treturn readServiceWeaverTable(data, moduleVersion, \".comp.\"+moduleVersion, func(buf []byte) (uint32, error) {\n\t\treturn readTableKeyUint32(buf)\n\t})\n}\n\nfunc readComponentNames(data []byte, moduleVersion string, componentIDs []int) ([]string, error) {\n\treturn readServiceWeaverTable(data, moduleVersion, \".comp.\"+moduleVersion, func(buf []byte) (string, error) {\n\t\treturn readTableValueString(buf)\n\t})\n}\n\nfunc readComponentListenerIDs(data []byte, moduleVersion string, componentIDs []int) ([][]int, error) {\n\ttype entry struct {\n\t\tComponentID int\n\t\tListenerID  []int\n\t}\n\titems := make([]entry, len(componentIDs))\n\tfor i, componentID := range componentIDs {\n\t\tvar listenerIDs []int\n\t\tvar err error\n\t\titems[i].ComponentID = componentID\n\t\tlistenerIDs, err = readServiceWeaverTable(data, moduleVersion, fmt.Sprintf(\". listener.%d.%d\", componentID, moduleVersion), func(buf []byte) (uint32, error) {\n\t\t\treturn readTableKeyUint32(buf)\n\t\t})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\titems[i].ListenerID = listenerIDs\n\t}\n\tgraph := make([][]int, len(componentIDs))\n\tfor _, item := range items {\n\t\tgraph[item.ComponentID] = item.ListenerID\n\t}\n\treturn graph, nil\n}\n\nfunc assembleGraph(componentIDs []int, componentListenerIDs [][]int) (graph.Graph, error) {\n\tvar graph graph.Graph\n\tif len(componentIDs) != len(componentListenerIDs) {\n\t\treturn graph, errors.New(\"mismatched lengths of component IDs and listener IDs\")\n\t}\n\tfor i, componentID := range componentIDs {\n\t\tfor _, listenerID := range componentListenerIDs[i] {\n\t\t\tif err := graph.AddEdge(componentID, listenerID); err != nil {\n\t\t\t\treturn graph, err\n\t\t\t}\n\t\t}\n\t}\n\treturn graph, nil\n}\n\nfunc readServiceWeaverTable(data []byte, moduleVersion, prefix string, processEntry func(buf []byte) (interface{}, error)) ([]interface{}, error) {\n\tre := regexp.MustCompile(`\u27e6(?\u201d)serviceweaver?2?.` + prefix + `:([a-z]*)\u27e7`)"]}, "weaver-runtime/bin/bin.go-ReadListeners": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/bin/bin.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package bin contains code to extract data from a Service Weaver binary.\npackage bin\n\nimport (\n\t\"bytes\"\n\t\"debug/buildinfo\"\n\t\"debug/elf\"\n\t\"debug/macho\"\n\t\"debug/pe\"\n\t\"fmt\"\n\t\"os\"\n\t\"regexp\"\n\t\"strconv\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/graph\"\n\t\"github.com/ServiceWeaver/weaver/runtime/version\"\n\t\"golang.org/x/exp/maps\"\n\t\"golang.org/x/exp/slices\"\n)\n\n// versionData exists to embed the weaver module version and deployer API\n// version into a Service Weaver binary. We split declaring and assigning\n// versionData to prevent the compiler from erasing it.\n//\n//lint:ignore U1000 See comment above.\nvar versionData string\n\nfunc init() {\n\t// NOTE that versionData must be assigned a string constant that reflects\n\t// the value of version.DeployerVersion. If the string is not a\n\t// constant---if we try to use fmt.Sprintf, for example---it will not be\n\t// embedded in a Service Weaver binary.\n\tversionData = \"\u27e6wEaVeRvErSiOn:deployer=v0.24.0\u27e7\"\n}\n\n// rodata returns the read-only data section of the provided binary.\nfunc rodata(file string) ([]byte, error) {\n\tf, err := os.Open(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer f.Close()\n\n\t// Look at first few bytes to determine the file format.\n\tprefix := make([]byte, 4)\n\tif _, err := f.ReadAt(prefix, 0); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Handle the file formats we support.\n\tswitch {\n\tcase bytes.HasPrefix(prefix, []byte(\"\\x7FELF\")): // Linux\n\t\tf, err := elf.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\".rodata\").Data()\n\tcase bytes.HasPrefix(prefix, []byte(\"MZ\")): // Windows\n\t\tf, err := pe.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\".rdata\").Data()\n\tcase bytes.HasPrefix(prefix, []byte(\"\\xFE\\xED\\xFA\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\"__rodata\").Data()\n\tcase bytes.HasPrefix(prefix[1:], []byte(\"\\xFA\\xED\\xFE\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\"__rodata\").Data()\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown format\")\n\t}\n}\n\n// ReadComponentGraph reads component graph information from the specified\n// binary. It returns a slice of components and a component graph whose nodes\n// are indices into that slice.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ReadListeners reads the sets of listeners associated with each component\n// in the specified binary.\n\n\n\n\n\n\n\n\ntype Versions struct {\n\tModuleVersion   string         // Service Weaver library's module version\n\tDeployerVersion version.SemVer // see version.DeployerVersion\n}\n\n// ReadVersions reads the module version and deployer API version from the\n// specified binary.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// extractModuleVersion returns the version of the Service Weaver library\n// embedded in data.\nfunc extractModuleVersion(filename string) (string, error) {\n\tinfo, err := buildinfo.ReadFile(filename)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Find the Service Weaver module.\n\tconst weaverModule = \"github.com/ServiceWeaver/weaver\"\n\tfor _, m := range append(info.Deps, &info.Main) {\n\t\tif m.Path == weaverModule {\n\t\t\treturn m.Version, nil\n\t\t}\n\t}\n\treturn \"\", fmt.Errorf(\"Service Weaver module was not linked into the application binary: that's an error\")\n}\n\n// extractDeployerVersion returns the deployer API version embedded\n// in data.\nfunc extractDeployerVersion(data []byte) (version.SemVer, error) {\n\tre := regexp.MustCompile(`\u27e6wEaVeRvErSiOn:deployer=v([0-9]*?)\\.([0-9]*?)\\.([0-9]*?)\u27e7`)\n\tm := re.FindSubmatch(data)\n\tif m == nil {\n\t\treturn version.SemVer{}, fmt.Errorf(\"embedded versions not found\")\n\t}\n\tv := version.SemVer{}\n\tfor _, segment := range []struct {\n\t\tname string\n\t\tdata []byte\n\t\tdst  *int\n\t}{\n\t\t{\"deployer major version\", m[1], &v.Major},\n\t\t{\"deployer minor version\", m[2], &v.Minor},\n\t\t{\"deployer patch version\", m[3], &v.Patch},\n\t} {\n\t\ts := string(segment.data)\n\t\tx, err := strconv.Atoi(s)\n\t\tif err != nil {\n\t\t\treturn version.SemVer{}, fmt.Errorf(\"invalid embedded %s %q: %w\", segment.name, s, err)\n\t\t}\n\t\t*segment.dst = x\n\t}\n\treturn v, nil\n}\n\n```\n# Function Signature: func ReadListeners(file string) ([]codegen.ComponentListeners, error) {\n# Natural Language Annotations: // ReadListeners reads the sets of listeners associated with each component\n// in the specified binary.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc ReadListeners(file string) ([]codegen.ComponentListeners, error) {\n\tf, err := os.Open(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer f.Close()\n\n\t// Look at first few bytes to determine the file format.\n\tprefix := make([]byte, 4)\n\tif _, err := f.ReadAt(prefix, 0); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Handle the file formats we support.\n\tswitch {\n\tcase bytes.HasPrefix(prefix, []byte(\"\\x7FELF\")): // Linux\n\t\tf, err := elf.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn extractListeners(f.Section(\".rodata\").Data())\n\tcase bytes.HasPrefix(prefix, []byte(\"MZ\")): // Windows\n\t\tf, err := pe.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn extractListeners(f.Section(\".rdata\").Data())\n\tcase bytes.HasPrefix(prefix, []byte(\"\\xFE\\xED\\xFA\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn extractListeners(f.Section(\"__rodata\").Data())\n\tcase bytes.HasPrefix(prefix[1:], []byte(\"\\xFA\\xED\\xFE\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn extractListeners(f.Section(\"__rodata\").Data())\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown format\")\n\t}\n}\n\nfunc extractListeners(data []byte) ([]codegen.ComponentListeners, error) {\n\tvar listeners []codegen.ComponentListeners\n\terr := json.Unmarshal(data, &listeners)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn listeners, nil\n}\n```", "  ```\nfunc ReadListeners(file string) ([]codegen.ComponentListeners, error) {\n\tdata, err := rodata(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tprefix := make([]byte, 4)\n\tif _, err := f.ReadAt(prefix, 0); err != nil {\n\t\treturn nil, err\n\t}\n\t// Handle the file formats we support.\n\tswitch {\n\tcase bytes.HasPrefix(prefix, []byte(\"\\x7FELF\")): // Linux\n\t\tf, err := elf.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tsection, err := f.Section(\".text\")\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn section.Data()\n\tcase bytes.HasPrefix(prefix, []byte(\"MZ\")): // Windows\n\t\tf, err := pe.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tsection, err := f.Section(\".text\")\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn section.Data()\n\tcase bytes.HasPrefix(prefix, []byte(\"\\xFE\\xED\\xFA\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tsection, err := f.Section(\"__text\")\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn section.Data()\n\tcase bytes.HasPrefix(prefix[1:], []byte(\"\\xFA\\xED\\xFE\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tsection, err := f.Section(\"__text\")\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn section.Data()\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown format\")\n\t}\n}\n```", "  ```\nfunc ReadListeners(file string) ([]codegen.ComponentListeners, error) {\n\tdata, err := rodata(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Try to find an array of ComponentListeners first.\n\tif n, err := findArrayOfComponentListeners(data); err == nil {\n\t\treturn data[n : n+int64(unsafe.Sizeof(codegen.ComponentListeners{})*len(components))], nil\n\t}\n\n\t// Next, try to find a [][][2]uint32.\n\tif n, err := findSliceOfSliceOfPairOfInt(data); err == nil {\n\t\treturn bits.ComponentListenersFromSliceOfSliceOfPairOfInt(data, n), nil\n\t}\n\n\t// Fallback to FastReflection.\n\tif n, err := findFastReflection(data); err == nil {\n\t\treturn bits.ComponentListenersFromFastReflection(data, n), nil\n\t}\n\n\treturn nil, fmt.Errorf(\"ReadListeners: found neither ComponentListeners nor slice of slice of [2]uint32\")\n}\n\nfunc findArrayOfComponentListeners(data []byte) (int64, error) {\n\tvar c component\n\tif err := c.decodeArrayOfComponentListeners(data); err == nil {\n\t\treturn c.listenerarray.off, nil\n\t}\n\n\treturn 0, fmt.Errorf(\"ReadListeners: could not find array of ComponentListeners\")\n}\n\nfunc findSliceOfSliceOfPairOfInt(data []byte) (int64, error) {\n\tvar c component\n\tif err := c.decodeSliceOfSliceOfPairOfInt32(data); err == nil {\n\t\treturn c.listenerslice.off, nil\n\t}\n\n\treturn 0, fmt.Errorf(\"ReadListeners: could not find slice of slice of [2]int32\")\n}\n\nfunc findFastReflection(data []byte) (int64, error) {\n\tvar c component\n\tif err := c.decodeFastReflection(data); err == nil {\n\t\treturn c.listenerarray.off, nil\n\t}\n\n\treturn 0, fmt.Errorf(\"ReadListeners: could not find fast reflection index\")\n}\n```"]}, "weaver-runtime/bin/bin.go-ReadVersions": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/bin/bin.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package bin contains code to extract data from a Service Weaver binary.\npackage bin\n\nimport (\n\t\"bytes\"\n\t\"debug/buildinfo\"\n\t\"debug/elf\"\n\t\"debug/macho\"\n\t\"debug/pe\"\n\t\"fmt\"\n\t\"os\"\n\t\"regexp\"\n\t\"strconv\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/graph\"\n\t\"github.com/ServiceWeaver/weaver/runtime/version\"\n\t\"golang.org/x/exp/maps\"\n\t\"golang.org/x/exp/slices\"\n)\n\n// versionData exists to embed the weaver module version and deployer API\n// version into a Service Weaver binary. We split declaring and assigning\n// versionData to prevent the compiler from erasing it.\n//\n//lint:ignore U1000 See comment above.\nvar versionData string\n\nfunc init() {\n\t// NOTE that versionData must be assigned a string constant that reflects\n\t// the value of version.DeployerVersion. If the string is not a\n\t// constant---if we try to use fmt.Sprintf, for example---it will not be\n\t// embedded in a Service Weaver binary.\n\tversionData = \"\u27e6wEaVeRvErSiOn:deployer=v0.24.0\u27e7\"\n}\n\n// rodata returns the read-only data section of the provided binary.\nfunc rodata(file string) ([]byte, error) {\n\tf, err := os.Open(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer f.Close()\n\n\t// Look at first few bytes to determine the file format.\n\tprefix := make([]byte, 4)\n\tif _, err := f.ReadAt(prefix, 0); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Handle the file formats we support.\n\tswitch {\n\tcase bytes.HasPrefix(prefix, []byte(\"\\x7FELF\")): // Linux\n\t\tf, err := elf.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\".rodata\").Data()\n\tcase bytes.HasPrefix(prefix, []byte(\"MZ\")): // Windows\n\t\tf, err := pe.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\".rdata\").Data()\n\tcase bytes.HasPrefix(prefix, []byte(\"\\xFE\\xED\\xFA\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\"__rodata\").Data()\n\tcase bytes.HasPrefix(prefix[1:], []byte(\"\\xFA\\xED\\xFE\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\"__rodata\").Data()\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown format\")\n\t}\n}\n\n// ReadComponentGraph reads component graph information from the specified\n// binary. It returns a slice of components and a component graph whose nodes\n// are indices into that slice.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ReadListeners reads the sets of listeners associated with each component\n// in the specified binary.\n\n\n\n\n\n\n\n\ntype Versions struct {\n\tModuleVersion   string         // Service Weaver library's module version\n\tDeployerVersion version.SemVer // see version.DeployerVersion\n}\n\n// ReadVersions reads the module version and deployer API version from the\n// specified binary.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// extractModuleVersion returns the version of the Service Weaver library\n// embedded in data.\nfunc extractModuleVersion(filename string) (string, error) {\n\tinfo, err := buildinfo.ReadFile(filename)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Find the Service Weaver module.\n\tconst weaverModule = \"github.com/ServiceWeaver/weaver\"\n\tfor _, m := range append(info.Deps, &info.Main) {\n\t\tif m.Path == weaverModule {\n\t\t\treturn m.Version, nil\n\t\t}\n\t}\n\treturn \"\", fmt.Errorf(\"Service Weaver module was not linked into the application binary: that's an error\")\n}\n\n// extractDeployerVersion returns the deployer API version embedded\n// in data.\nfunc extractDeployerVersion(data []byte) (version.SemVer, error) {\n\tre := regexp.MustCompile(`\u27e6wEaVeRvErSiOn:deployer=v([0-9]*?)\\.([0-9]*?)\\.([0-9]*?)\u27e7`)\n\tm := re.FindSubmatch(data)\n\tif m == nil {\n\t\treturn version.SemVer{}, fmt.Errorf(\"embedded versions not found\")\n\t}\n\tv := version.SemVer{}\n\tfor _, segment := range []struct {\n\t\tname string\n\t\tdata []byte\n\t\tdst  *int\n\t}{\n\t\t{\"deployer major version\", m[1], &v.Major},\n\t\t{\"deployer minor version\", m[2], &v.Minor},\n\t\t{\"deployer patch version\", m[3], &v.Patch},\n\t} {\n\t\ts := string(segment.data)\n\t\tx, err := strconv.Atoi(s)\n\t\tif err != nil {\n\t\t\treturn version.SemVer{}, fmt.Errorf(\"invalid embedded %s %q: %w\", segment.name, s, err)\n\t\t}\n\t\t*segment.dst = x\n\t}\n\treturn v, nil\n}\n\n```\n# Function Signature: func ReadVersions(filename string) (Versions, error) {\n# Natural Language Annotations: // ReadVersions reads the module version and deployer API version from the\n// specified binary.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc ReadVersions(filename string) (Versions, error) {\n\t// Read the module version from the binary.\n\tmoduleVersion, err := extractModuleVersion(filename)\n\tif err != nil {\n\t\treturn Versions{}, err\n\t}\n\n\t// Read the deployer API version from the binary.\n\tdata, err := rodata(filename)\n\tif err != nil {\n\t\treturn Versions{}, err\n\t}\n\tdeployerVersion, err := extractDeployerVersion(data)\n\tif err != nil {\n\t\treturn Versions{}, err\n\t}\n\n\treturn Versions{\n\t\tModuleVersion:   moduleVersion,\n\t\tDeployerVersion: deployerVersion,\n\t}, nil\n}\n```", "  ```\nfunc ReadVersions(filename string) (Versions, error) {\n\tf, err := os.Open(filename)\n\tif err != nil {\n\t\treturn Versions{}, err\n\t}\n\tdefer f.Close()\n\n\t// Look at first few bytes to determine the file format.\n\tprefix := make([]byte, 4)\n\tif _, err := f.ReadAt(prefix, 0); err != nil {\n\t\treturn Versions{}, err\n\t}\n\n\t// Handle the file formats we support.\n\tswitch {\n\tcase bytes.HasPrefix(prefix, []byte(\"\\x7FELF\")): // Linux\n\t\tf, err := elf.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn Versions{}, err\n\t\t}\n\t\trodata := f.Section(\".rodata\").Data()\n\t\tmoduleVersion, err := extractModuleVersion(filename)\n\t\tif err != nil {\n\t\t\treturn Versions{}, err\n\t\t}\n\t\tdeployerVersion, err := extractDeployerVersion(rodata)\n\t\tif err != nil {\n\t\t\treturn Versions{}, err\n\t\t}\n\t\treturn Versions{moduleVersion, deployerVersion}, nil\n\tcase bytes.HasPrefix(prefix, []byte(\"MZ\")): // Windows\n\t\tf, err := pe.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn Versions{}, err\n\t\t}\n\t\trodata := f.Section(\".rdata\").Data()\n\t\tmoduleVersion, err := extractModuleVersion(filename)\n\t\tif err != nil {\n\t\t\treturn Versions{}, err\n\t\t}\n\t\tdeployerVersion, err := extractDeployerVersion(rodata)\n\t\tif err != nil {\n\t\t\treturn Versions{}, err\n\t\t}\n\t\treturn Versions{moduleVersion, deployerVersion}, nil\n\tcase bytes.HasPrefix(prefix, []byte(\"\\xFE\\xED\\xFA\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn Versions{}, err\n\t\t}\n\t\trodata := f.Section(\"__rodata\").Data()\n\t\tmoduleVersion, err := extractModuleVersion(filename)\n\t\tif err != nil {\n\t\t\treturn Versions{}, err\n\t\t}\n\t\tdeployerVersion, err := extractDeployerVersion(rodata)\n\t\tif err != nil {\n\t\t\treturn Versions{}, err\n\t\t}\n\t\treturn Versions{moduleVersion, deployerVersion}, nil\n\tcase bytes.HasPrefix(prefix[1:], []byte(\"\\xFA\\xED\\xFE\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn Versions{}, err\n\t\t}\n\t\trodata := f.Section(\"__rodata\").Data()\n\t\tmoduleVersion, err := extractModuleVersion(filename)\n\t\tif err != nil {\n\t\t\treturn Versions{}, err\n\t\t}\n\t\tdeployerVersion, err := extractDeployerVersion(rodata)\n\t\tif err != nil {\n\t\t\treturn Versions{}, err\n\t\t}\n\t\treturn Versions{moduleVersion, deployerVersion}, nil\n\tdefault:\n\t\treturn Versions{}, fmt.Errorf(\"unknown format\")\n\t}\n}\n```", "  ```\nfunc ReadVersions(filename string) (Versions, error) {\n\t// Read the Service Weaver library's module version from the file.\n\tmoduleVersion, err := extractModuleVersion(filename)\n\tif err != nil {\n\t\treturn Versions{}, err\n\t}\n\n\t// Read the deployer API version from the file.\n\tdeployerVersion, err := extractDeployerVersion(filename)\n\tif err != nil {\n\t\treturn Versions{}, err\n\t}\n\n\t// Return the versions.\n\treturn Versions{\n\t\tModuleVersion:   moduleVersion,\n\t\tDeployerVersion: deployerVersion,\n\t}, nil\n}\n```"]}, "weaver-runtime/prometheus/prometheus.go-TranslateMetricsToPrometheusTextFormat": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/prometheus/prometheus.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage prometheus\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"math\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/logging\"\n\t\"github.com/ServiceWeaver/weaver/runtime/metrics\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"golang.org/x/exp/maps\"\n)\n\n// escaper is used to format the labels according to [1]. Prometheus labels can\n// be any sequence of UTF-8 characters, but the backslash (\\), double-quote (\"),\n// and line feed (\\n) characters have to be escaped as \\\\, \\\", and \\n, respectively.\n//\n// [1] https://github.com/prometheus/docs/blob/main/content/docs/instrumenting/exposition_formats.md#text-format-details\nvar escaper = strings.NewReplacer(\"\\\\\", `\\\\`, \"\\n\", `\\n`, \"\\\"\", `\\\"`)\n\n// TranslateMetricsToPrometheusTextFormat translates Service Weaver\n// metrics (keyed by weavelet id) to a text format that can be\n// scraped by Prometheus [1].\n//\n// [1] https://prometheus.io/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// writeHelper generates a config.yaml file that can be used by prometheus to\n// scrape the exported metrics.\nfunc writeHelper(w *bytes.Buffer, lisAddr, path string) {\n\tconst help = `# Metrics in Prometheus text format [1].\n#\n# To visualize and query the metrics, make sure Prometheus is installed on\n# your local machine and then add the following stanza to your Prometheus yaml\n# config file:\n#\n# scrape_configs:\n# - job_name: 'prometheus-serviceweaver-scraper'\n#   scrape_interval: 5s\n#   metrics_path: %s\n#   static_configs:\n#     - targets: ['%s']\n#\n# [1]: https://prometheus.io\n\n`\n\tfmt.Fprintf(w, help, path, lisAddr)\n}\n\n// translateMetrics translates a slice of metrics from the Service Weaver format\n// to the Prometheus text format. For more details regarding the metric text\n// format for Prometheus, see [1].\n//\n// [1] https://github.com/prometheus/docs/blob/main/content/docs/instrumenting/exposition_formats.md#text-format-details\nfunc translateMetrics(w *bytes.Buffer, metrics []*metrics.MetricSnapshot) string {\n\tmetric := metrics[0]\n\n\t// Write the metric HELP. Note that all metrics have the same metric name,\n\t// so we should display the help and the type only once.\n\tif len(metric.Help) > 0 {\n\t\tw.WriteString(\"# HELP \" + metric.Name + \" \" + metric.Help + \"\\n\")\n\t}\n\n\t// Write the metric TYPE.\n\tw.WriteString(\"# TYPE \" + metric.Name)\n\n\tisHistogram := false\n\tswitch metric.Type {\n\tcase protos.MetricType_COUNTER:\n\t\tw.WriteString(\" counter\\n\")\n\tcase protos.MetricType_GAUGE:\n\t\tw.WriteString(\" gauge\\n\")\n\tcase protos.MetricType_HISTOGRAM:\n\t\tw.WriteString(\" histogram\\n\")\n\t\tisHistogram = true\n\t}\n\n\tfor idx, metric := range metrics {\n\t\t// Trim labels.\n\t\tlabels := maps.Clone(metric.Labels)\n\t\tdelete(labels, \"serviceweaver_app\")\n\t\tdelete(labels, \"serviceweaver_version\")\n\t\tif node, ok := labels[\"serviceweaver_node\"]; ok {\n\t\t\tlabels[\"serviceweaver_node\"] = logging.Shorten(node)\n\t\t}\n\n\t\t// Write the metric definitions.\n\t\t//\n\t\t// For counter and gauge metrics the definition looks like:\n\t\t// metric_name [\n\t\t//  \"{\" label_name \"=\" `\"` label_value `\"` { \",\" label_name \"=\" `\"` label_value `\"` } [ \",\" ] \"}\"\n\t\t// ] value [ timestamp ]\n\t\t//\n\t\t// For histograms:\n\t\t//  Each bucket count of a histogram named x is given as a separate sample\n\t\t//  line with the name x_bucket and a label {le=\"y\"} (where y is the upper bound of the bucket).\n\t\t//\n\t\t//  The bucket with label {le=\"+Inf\"} must exist. Its value must be identical to the value of x_count.\n\t\t//\n\t\t//  The buckets must appear in increasing numerical order of their label values (for the le).\n\t\t//\n\t\t//  The sample sum for a summary or histogram named x is given as a separate sample named x_sum.\n\t\t//\n\t\t//  The sample count for a summary or histogram named x is given as a separate sample named x_count.\n\t\tif isHistogram {\n\t\t\thasInf := false\n\n\t\t\tvar count uint64\n\t\t\tfor idx, bound := range metric.Bounds {\n\t\t\t\tcount += metric.Counts[idx]\n\t\t\t\twriteEntry(w, metric.Name, float64(count), \"_bucket\", labels, \"le\", bound)\n\t\t\t\tif math.IsInf(bound, +1) {\n\t\t\t\t\thasInf = true\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Account for the +Inf bucket.\n\t\t\tcount += metric.Counts[len(metric.Bounds)]\n\t\t\tif !hasInf {\n\t\t\t\twriteEntry(w, metric.Name, float64(count), \"_bucket\", labels, \"le\", math.Inf(+1))\n\t\t\t}\n\t\t\twriteEntry(w, metric.Name, metric.Value, \"_sum\", labels, \"\", 0)\n\t\t\twriteEntry(w, metric.Name, float64(count), \"_count\", labels, \"\", 0)\n\t\t} else { // counter or gauge\n\t\t\twriteEntry(w, metric.Name, metric.Value, \"\", labels, \"\", 0)\n\t\t}\n\t\tif isHistogram && idx != len(metrics)-1 {\n\t\t\tw.WriteByte('\\n')\n\t\t}\n\t}\n\tw.WriteByte('\\n')\n\treturn w.String()\n}\n\n// writeEntry generates a metric definition entry.\nfunc writeEntry(w *bytes.Buffer, metricName string, value float64, suffix string,\n\tlabels map[string]string, extraLabelName string, extraLabelValue float64) {\n\tw.WriteString(metricName)\n\tif len(suffix) > 0 {\n\t\tw.WriteString(suffix)\n\t}\n\twriteLabels(w, labels, extraLabelName, extraLabelValue)\n\tw.WriteString(\" \" + strconv.FormatFloat(value, 'f', -1, 64) + \"\\n\")\n}\n\n// writeEntry generates the metric labels.\nfunc writeLabels(w *bytes.Buffer, labels map[string]string,\n\textraLabelName string, extraLabelValue float64) {\n\tif len(labels) == 0 && extraLabelName == \"\" {\n\t\treturn\n\t}\n\n\tsortedLabels := maps.Keys(labels)\n\tsort.Strings(sortedLabels)\n\n\tseparator := \"{\"\n\tfor _, l := range sortedLabels {\n\t\tw.WriteString(separator + l + `=\"`)\n\t\tescaper.WriteString(w, labels[l])\n\t\tw.WriteByte('\"')\n\t\tseparator = \",\"\n\t}\n\tif len(extraLabelName) > 0 {\n\t\t// Set for a histogram metric only.\n\t\tw.WriteString(separator + extraLabelName + `=\"`)\n\t\tw.WriteString(strconv.FormatFloat(extraLabelValue, 'f', -1, 64) + \"\\\"\")\n\t}\n\tw.WriteString(\"}\")\n}\n\n```\n# Function Signature: func TranslateMetricsToPrometheusTextFormat(w *bytes.Buffer, ms []*metrics.MetricSnapshot, lisAddr, path string) {\n# Natural Language Annotations: // TranslateMetricsToPrometheusTextFormat translates Service Weaver\n// metrics (keyed by weavelet id) to a text format that can be\n// scraped by Prometheus [1].\n//\n// [1] https://prometheus.io/\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc TranslateMetricsToPrometheusTextFormat(w *bytes.Buffer, ms []*metrics.MetricSnapshot, lisAddr, path string) {\n\twriteHelper(w, lisAddr, path)\n\tw.WriteString(\"# EOF\")\n\tw.WriteByte('\\n')\n\ttranslateMetrics(w, ms)\n}\n```", "  ```\nfunc TranslateMetricsToPrometheusTextFormat(w *bytes.Buffer, ms []*metrics.MetricSnapshot, lisAddr, path string) {\n\t// Translate each weavelet's metrics to Prometheus text format.\n\tfor _, m := range ms {\n\t\ttranslateMetrics(w, m)\n\t}\n\n\t// Write the Prometheus config.\n\twriteHelper(w, lisAddr, path)\n}\n```", "  ```\nfunc TranslateMetricsToPrometheusTextFormat(w *bytes.Buffer, metrics []*metrics.MetricSnapshot, lisAddr, path string) {\n\tw.WriteString(\"\")\n\tfor _, metric := range metrics {\n\t\twriteEntry(w, metric.Name, metric.Value, \"\", maps.Clone(metric.Labels), \"\", 0)\n\t\tif idx != len(metrics)-1 {\n\t\t\tw.WriteByte('\\n')\n\t\t}\n\t}\n\tw.WriteByte('\\n')\n\tw.String()\n}\n```"]}, "weaver-runtime/codegen/graph.go-ExtractEdges": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/codegen/graph.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage codegen\n\nimport (\n\t\"crypto/sha256\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"sort\"\n)\n\n// Component graph edges are embedded in the generated binary as\n// specially formatted strings. These strings can be extracted from\n// the binary to get the communication graph without having to execute\n// the binary.\n//\n// Each edge is represented by a string fragment that looks like:\n// \u27e6checksum:wEaVeReDgE:src\u2192dst\u27e7\n//\n// checksum is the first 8 bytes of the hex encoding of the SHA-256 of\n// the string \"wEaVeReDgE:src\u2192dst\" and src and dst are the fully qualified\n// component type names.\n\n// MakeEdgeString returns a string that should be emitted into generated\n// code to represent an edge from src to dst.\nfunc MakeEdgeString(src, dst string) string {\n\treturn fmt.Sprintf(\"\u27e6%s:wEaVeReDgE:%s\u2192%s\u27e7\\n\", checksumEdge(src, dst), src, dst)\n}\n\n// ExtractEdges returns the edges corresponding to MakeEdgeString() results\n// that occur in data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc checksumEdge(src, dst string) string {\n\tedge := fmt.Sprintf(\"wEaVeReDgE:%s\u2192%s\", src, dst)\n\tsum := sha256.Sum256([]byte(edge))\n\treturn fmt.Sprintf(\"%0x\", sum)[:8]\n}\n\n```\n# Function Signature: func ExtractEdges(data []byte) [][2]string {\n# Natural Language Annotations: // ExtractEdges returns the edges corresponding to MakeEdgeString() results\n// that occur in data.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\npackage codegen\n\nimport (\n\t\"crypto/sha256\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"sort\"\n)\n\n// ExtractEdges returns the edges corresponding to MakeEdgeString() results\n// that occur in data.\nfunc ExtractEdges(data []byte) [][2]string {\n\tvar edges [][2]string\n\n\t// Find all instances of edge strings in the data.\n\tre := regexp.MustCompile(`\u27e6checksum:wEaVeReDgE:([\\w:]+)\u2192([\\w:]+)\u27e7`)\n\tmatches := re.FindAllSubmatch(data, -1)\n\n\t// Convert each match into a [2]string.\n\tfor _, match := range matches {\n\t\tedges = append(edges, [2]string{string(match[1]), string(match[2])})\n\t}\n\n\treturn edges\n}\n```", "  ```\nfunc ExtractEdges(data []byte) [][2]string {\n\tvar edges [][2]string\n\tfor _, match := range regexp.MustCompile(`\u27e6[a-fA-F0-9]*:wEaVeReDgE:\\w+\u2192\\w+\u27e7`).FindAllSubmatch(data, -1) {\n\t\tsrcType := match[0][strings.Index(match[0], \":wEaVeReDgE:\")+11 : strings.Index(match[0], \"\u2192\")]\n\t\tdstType := match[0][strings.Index(match[0], \"\u2192\")+2:]\n\t\tedges = append(edges, [2]string{srcType, dstType})\n\t}\n\treturn edges\n}\n```", "  ```\npackage codegen\n\nimport (\n\t\"crypto/sha256\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"sort\"\n)\n\nfunc ExtractEdges(data []byte) [][2]string {\n\t// Extract all strings that look like edge descriptions.\n\tre := regexp.MustCompile(`\u27e6([a-f0-9]{8}):wEaVeReDgE:.+?\u2192(.+?)\u27e7`)\n\tmatches := re.FindAllSubmatch(data, -1)\n\n\t// Create a map for deduplication purposes.\n\tedgeMap := make(map[[2]string]bool)\n\n\t// Iterate over matches and extract the edges.\n\tfor _, m := range matches {\n\t\t// Extract the checksum, src, and dst from the edge description.\n\t\tsrc := string(m[2])\n\t\tdst := string(m[3])\n\t\tedgeChecksum := string(m[1])\n\n\t\t// Generate the edge name based on the checksum and the src and dst.\n\t\tedgeName := fmt.Sprintf(\"wEaVeReDgE:%s\u2192%s\", src, dst)\n\n\t\t// Check if the edge name is already present in the edge map.\n\t\tif _, ok := edgeMap[edgeName]; ok {\n\t\t\t// If the edge is a duplicate, ignore it.\n\t\t} else {\n\t\t\t// If the edge is not a duplicate, add it to the edge map.\n\t\t\tedgeMap[edgeName] = true\n\t\t}\n\t}\n\n\t// Extract the edges from the edge map and sort them.\n\tedges := make([][2]string, 0, len(edgeMap))\n\tfor edgeName := range edgeMap {\n\t\tedges = append(edges, [2]string{edgeName, \"\"})\n\t}\n\tsort.Slice(edges, func(i, j int) bool {\n\t\treturn edges[i][0] < edges[j][0]\n\t})\n\n\treturn edges\n}\n\n```"]}, "weaver-runtime/codegen/registry.go-ComponentConfigValidator": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/codegen/registry.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage codegen\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/config\"\n\t\"github.com/ServiceWeaver/weaver/runtime\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// globalRegistry is the global registry used by Register and Registered.\nvar globalRegistry registry\n\n// Register registers a Service Weaver component.\nfunc Register(reg Registration) {\n\tif err := globalRegistry.register(reg); err != nil {\n\t\tpanic(err)\n\t}\n}\n\n// Registered returns the components registered with Register.\nfunc Registered() []*Registration {\n\treturn globalRegistry.allComponents()\n}\n\n// Find returns the registration of the named component.\nfunc Find(name string) (*Registration, bool) {\n\treturn globalRegistry.find(name)\n}\n\n// registry is a repository for registered Service Weaver components.\n// Entries are typically added to the default registry by calls\n// to Register in init functions in code generated by \"weaver generate\".\ntype registry struct {\n\tm          sync.Mutex\n\tcomponents map[reflect.Type]*Registration // the set of registered components, by their interface types\n\tbyName     map[string]*Registration       // map from full component name to registration\n}\n\n// Registration is the configuration needed to register a Service Weaver component.\ntype Registration struct {\n\tName      string       // full package-prefixed component name\n\tIface     reflect.Type // interface type for the component\n\tImpl      reflect.Type // implementation type (struct)\n\tRouted    bool         // True if calls to this component should be routed\n\tListeners []string     // the names of any weaver.Listeners\n\tNoRetry   []int        // indices of methods that should not be retried\n\n\t// Functions that return different types of stubs.\n\tLocalStubFn   func(impl any, caller string, tracer trace.Tracer) any\n\tClientStubFn  func(stub Stub, caller string) any\n\tServerStubFn  func(impl any, load func(key uint64, load float64)) Server\n\tReflectStubFn func(func(method string, ctx context.Context, args []any, returns []any) error) any\n\n\t// RefData holds a string containing the result of MakeEdgeString(Name, Dst)\n\t// for all components named Dst used by this component.\n\tRefData string\n}\n\n// register registers a Service Weaver component. If the registry's close method was\n// previously called, Register will fail and return a non-nil error.\nfunc (r *registry) register(reg Registration) error {\n\tif err := verifyRegistration(reg); err != nil {\n\t\treturn fmt.Errorf(\"Register(%q): %w\", reg.Name, err)\n\t}\n\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\tif old, ok := r.components[reg.Iface]; ok {\n\t\treturn fmt.Errorf(\"component %s already registered for type %v when registering %v\",\n\t\t\treg.Name, old.Impl, reg.Impl)\n\t}\n\tif r.components == nil {\n\t\tr.components = map[reflect.Type]*Registration{}\n\t}\n\tif r.byName == nil {\n\t\tr.byName = map[string]*Registration{}\n\t}\n\tptr := &reg\n\tr.components[reg.Iface] = ptr\n\tr.byName[reg.Name] = ptr\n\treturn nil\n}\n\nfunc verifyRegistration(reg Registration) error {\n\tif reg.Iface == nil {\n\t\treturn errors.New(\"missing component type\")\n\t}\n\tif reg.Iface.Kind() != reflect.Interface {\n\t\treturn errors.New(\"component type is not an interface\")\n\t}\n\tif reg.Impl == nil {\n\t\treturn errors.New(\"missing implementation type\")\n\t}\n\tif reg.Impl.Kind() != reflect.Struct {\n\t\treturn errors.New(\"implementation type is not a struct\")\n\t}\n\tif reg.LocalStubFn == nil {\n\t\treturn errors.New(\"nil LocalStubFn\")\n\t}\n\tif reg.ClientStubFn == nil {\n\t\treturn errors.New(\"nil ClientStubFn\")\n\t}\n\tif reg.ServerStubFn == nil {\n\t\treturn errors.New(\"nil ServerStubFn\")\n\t}\n\treturn nil\n}\n\n// allComponents returns all of the registered components, keyed by name.\nfunc (r *registry) allComponents() []*Registration {\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\n\tcomponents := make([]*Registration, 0, len(r.components))\n\tfor _, info := range r.components {\n\t\tcomponents = append(components, info)\n\t}\n\treturn components\n}\n\nfunc (r *registry) find(path string) (*Registration, bool) {\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\treg, ok := r.byName[path]\n\treturn reg, ok\n}\n\n// ComponentConfigValidator checks that cfg is a valid configuration\n// for the component type whose fully qualified name is given by path.\n//\n// TODO(mwhittaker): Move out of codegen package? It's not used by the\n// generated code.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// CallEdge records that fact that the Caller component uses the\n// Callee component. Both types are types of the corresponding\n// component interfaces.\ntype CallEdge struct {\n\tCaller reflect.Type\n\tCallee reflect.Type\n}\n\n// CallGraph returns the component call graph (as a list of CallEdge values).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func ComponentConfigValidator(path, cfg string) error {\n# Natural Language Annotations: // ComponentConfigValidator checks that cfg is a valid configuration\n// for the component type whose fully qualified name is given by path.\n//\n// TODO(mwhittaker): Move out of codegen package? It's not used by the\n// generated code.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc ComponentConfigValidator(path, cfg string) error {\n\treg, ok := Find(path)\n\tif !ok {\n\t\treturn fmt.Errorf(\"component %s is not registered\", path)\n\t}\n\tif err := verifyComponentConfig(cfg, reg.Iface.String()); err != nil {\n\t\treturn fmt.Errorf(\"invalid configuration for component %s: %w\", path, err)\n\t}\n\treturn nil\n}\n\nfunc verifyComponentConfig(cfg, ifaceName string) error {\n\tjsonRdr := strings.NewReader(cfg)\n\tdec := json.NewDecoder(jsonRdr)\n\tvar jsonCfg interface{}\n\tif err := dec.Decode(&jsonCfg); err != nil {\n\t\treturn fmt.Errorf(\"error decoding JSON configuration: %w\", err)\n\t}\n\treturn nil\n}\n```", "  ```\nfunc ComponentConfigValidator(path, cfg string) error {\n\treg, found := globalRegistry.Find(path)\n\tif !found {\n\t\treturn fmt.Errorf(\"Component with path %q could not be found in the registry\", path)\n\t}\n\tif err := verifyConfig(reg.Iface, cfg); err != nil {\n\t\treturn fmt.Errorf(\"Incorrect configuration for component %s. %w\", path, err)\n\t}\n\treturn nil\n}\n\nfunc verifyConfig(iface reflect.Type, cfg string) error {\n\t// TODO: implement this function\n\t// For now, just assume that the config is valid and return nil\n\treturn nil\n}\n```", "  ```\nfunc ComponentConfigValidator(path, cfg string) error {\n\t// TODO(mwhittaker): Move out of codegen package? It's not used by the\n\t// generated code.\n\n\t// Get the component info from the registry\n\treg, found := find(path)\n\tif !found {\n\t\treturn fmt.Errorf(\"component not found: %s\", path)\n\t}\n\n\t// Create a new decoder for the component's config type\n\tdecoder := json.Decoder(&bytes.Buffer{})\n\n\t// Error if the config type is unsupported\n\tqtError, ok := stuff.Get(\"component_config_struct\").(interface{}); if !ok {\n\t\treturn errors.New(\"unsupported config type for component\")\n\t}\n\n\t// Decode the config\n\tconfig := reflect.New(qtError.X)\n\tif err := decoder.Decode(config.Interface()); err != nil {\n\t\treturn fmt.Errorf(\"error decoding config for component: %w\", err)\n\t}\n\n\t// Merge the config with the component's default config\n\t// TODO(mwhittaker): should we be merging instead?\n\tmergedConfig := runtime.Merge(reg.Defaults, config)\n\n\t// Create a new validator for the merged config\n\t// TODO(mwhittaker): create the validator based on the merged config map\n\tvalidator := config.ValidatorFor(mergedConfig)\n\n\t// Run the validator and return any errors\n\tif err := validator.Validate(mergedConfig); err != nil {\n\t\treturn fmt.Errorf(\"error validating config for component: %w\", err)\n\t}\n\n\treturn nil\n}\n```"]}, "weaver-runtime/codegen/registry.go-CallGraph": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/codegen/registry.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage codegen\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/config\"\n\t\"github.com/ServiceWeaver/weaver/runtime\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// globalRegistry is the global registry used by Register and Registered.\nvar globalRegistry registry\n\n// Register registers a Service Weaver component.\nfunc Register(reg Registration) {\n\tif err := globalRegistry.register(reg); err != nil {\n\t\tpanic(err)\n\t}\n}\n\n// Registered returns the components registered with Register.\nfunc Registered() []*Registration {\n\treturn globalRegistry.allComponents()\n}\n\n// Find returns the registration of the named component.\nfunc Find(name string) (*Registration, bool) {\n\treturn globalRegistry.find(name)\n}\n\n// registry is a repository for registered Service Weaver components.\n// Entries are typically added to the default registry by calls\n// to Register in init functions in code generated by \"weaver generate\".\ntype registry struct {\n\tm          sync.Mutex\n\tcomponents map[reflect.Type]*Registration // the set of registered components, by their interface types\n\tbyName     map[string]*Registration       // map from full component name to registration\n}\n\n// Registration is the configuration needed to register a Service Weaver component.\ntype Registration struct {\n\tName      string       // full package-prefixed component name\n\tIface     reflect.Type // interface type for the component\n\tImpl      reflect.Type // implementation type (struct)\n\tRouted    bool         // True if calls to this component should be routed\n\tListeners []string     // the names of any weaver.Listeners\n\tNoRetry   []int        // indices of methods that should not be retried\n\n\t// Functions that return different types of stubs.\n\tLocalStubFn   func(impl any, caller string, tracer trace.Tracer) any\n\tClientStubFn  func(stub Stub, caller string) any\n\tServerStubFn  func(impl any, load func(key uint64, load float64)) Server\n\tReflectStubFn func(func(method string, ctx context.Context, args []any, returns []any) error) any\n\n\t// RefData holds a string containing the result of MakeEdgeString(Name, Dst)\n\t// for all components named Dst used by this component.\n\tRefData string\n}\n\n// register registers a Service Weaver component. If the registry's close method was\n// previously called, Register will fail and return a non-nil error.\nfunc (r *registry) register(reg Registration) error {\n\tif err := verifyRegistration(reg); err != nil {\n\t\treturn fmt.Errorf(\"Register(%q): %w\", reg.Name, err)\n\t}\n\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\tif old, ok := r.components[reg.Iface]; ok {\n\t\treturn fmt.Errorf(\"component %s already registered for type %v when registering %v\",\n\t\t\treg.Name, old.Impl, reg.Impl)\n\t}\n\tif r.components == nil {\n\t\tr.components = map[reflect.Type]*Registration{}\n\t}\n\tif r.byName == nil {\n\t\tr.byName = map[string]*Registration{}\n\t}\n\tptr := &reg\n\tr.components[reg.Iface] = ptr\n\tr.byName[reg.Name] = ptr\n\treturn nil\n}\n\nfunc verifyRegistration(reg Registration) error {\n\tif reg.Iface == nil {\n\t\treturn errors.New(\"missing component type\")\n\t}\n\tif reg.Iface.Kind() != reflect.Interface {\n\t\treturn errors.New(\"component type is not an interface\")\n\t}\n\tif reg.Impl == nil {\n\t\treturn errors.New(\"missing implementation type\")\n\t}\n\tif reg.Impl.Kind() != reflect.Struct {\n\t\treturn errors.New(\"implementation type is not a struct\")\n\t}\n\tif reg.LocalStubFn == nil {\n\t\treturn errors.New(\"nil LocalStubFn\")\n\t}\n\tif reg.ClientStubFn == nil {\n\t\treturn errors.New(\"nil ClientStubFn\")\n\t}\n\tif reg.ServerStubFn == nil {\n\t\treturn errors.New(\"nil ServerStubFn\")\n\t}\n\treturn nil\n}\n\n// allComponents returns all of the registered components, keyed by name.\nfunc (r *registry) allComponents() []*Registration {\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\n\tcomponents := make([]*Registration, 0, len(r.components))\n\tfor _, info := range r.components {\n\t\tcomponents = append(components, info)\n\t}\n\treturn components\n}\n\nfunc (r *registry) find(path string) (*Registration, bool) {\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\treg, ok := r.byName[path]\n\treturn reg, ok\n}\n\n// ComponentConfigValidator checks that cfg is a valid configuration\n// for the component type whose fully qualified name is given by path.\n//\n// TODO(mwhittaker): Move out of codegen package? It's not used by the\n// generated code.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// CallEdge records that fact that the Caller component uses the\n// Callee component. Both types are types of the corresponding\n// component interfaces.\ntype CallEdge struct {\n\tCaller reflect.Type\n\tCallee reflect.Type\n}\n\n// CallGraph returns the component call graph (as a list of CallEdge values).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func CallGraph() []CallEdge {\n# Natural Language Annotations: // CallGraph returns the component call graph (as a list of CallEdge values).\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc CallGraph() []CallEdge {\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\tedges := make([]CallEdge, 0, len(r.byName))\n\tfor name, info := range r.byName {\n\t\t// Skip the reflect package because it is not a Service Weaver component.\n\t\tif !strings.HasPrefix(name, \".\"+reflect.Reflect.Name()) {\n\t\t\tedges = append(edges, CallEdge{Caller: info.Iface, Callee: info.Impl})\n\t\t}\n\t}\n\treturn edges\n}\n```", "  ```\n// CallGraph returns the component call graph (as a list of CallEdge values).\nfunc CallGraph() []CallEdge {\n\tregs := Registered()\n\tedges := make([]CallEdge, 0, len(regs))\n\tfor _, reg := range regs {\n\t\tfor _, name := range strings.Split(reg.RefData, \",\") {\n\t\t\tif strings.HasPrefix(name, \"dial:\") {\n\t\t\t\tedges = append(edges, CallEdge{Caller: reg.Iface, Callee: reflect.TypeOf(dialMux{}),})\n\t\t\t} else {\n\t\t\t\tedge, err := reg.find(name)\n\t\t\t\tif err != nil {\n\t\t\t\t\tpanic(err)\n\t\t\t\t}\n\t\t\t\tedges = append(edges, CallEdge{Caller: reg.Iface, Callee: edge.Iface,})\n\t\t\t}\n\t\t}\n\t}\n\treturn edges\n}\n```", "  ```\n// CallGraph returns the component call graph (as a list of CallEdge values).\nfunc CallGraph() []CallEdge {\n\t// Create a map to store the call graph. Each key is a component's full name,\n\t// and the value is a slice of CallEdge values representing the components\n\t// that the key component calls.\n\tcallGraph := make(map[string][]CallEdge)\n\n\t// Iterate over the registered components and add the call edges to the call graph.\n\tfor _, reg := range Registered() {\n\t\tvar iface string\n\t\t// Loop through each of the interfaces implemented by the component.\n\t\tfor i := 0; i < reg.Iface.NumMethod(); i++ {\n\t\t\tm := reg.Iface.Method(i)\n\t\t\t// If the method is called \"Service\", it is considered a call.\n\t\t\t// TODO: Make this more robust by checking the method name against some set of services.\n\t\t\tif m.Name == \"Service\" {\n\t\t\t\tiface = reg.Iface.Name()\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\t// If a service was found, add the call edge to the call graph.\n\t\tif len(iface) > 0 {\n\t\t\tcallGraph[iface] = append(callGraph[iface], CallEdge{\n\t\t\t\tCaller: reg.Iface,\n\t\t\t\tCallee: reg.Iface,\n\t\t\t})\n\t\t}\n\t}\n\n\t// Flatten the call graph into a single list of CallEdge values.\n\tvar callEdges []CallEdge\n\tfor _, edges := range callGraph {\n\t\tcallEdges = append(callEdges, edges...)\n\t}\n\n\treturn callEdges\n}\n```"]}, "weaver-runtime/codegen/listeners.go-MakeListenersString": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/codegen/listeners.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage codegen\n\nimport (\n\t\"crypto/sha256\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"sort\"\n\t\"strings\"\n)\n\n// All listeners used by a given component are embedded in the generated\n// binary as specially formatted strings. These strings can be extracted from\n// the binary to get the list of listeners associated with each component\n// without having to execute the binary.\n//\n// The set of listeners used by a given component is represented by a string\n// fragment that looks like:\n// \u27e6checksum:wEaVeRlIsTeNeRs:component\u2192listeners\u27e7\n//\n// checksum is the first 8 bytes of the hex encoding of the SHA-256 of\n// the string \"wEaVeRlIsTeNeRs:component\u2192listeners\"; component is the fully\n// qualified component type name; listeners is a comma-separated list of\n// all listener names associated with a given component.\n\n// MakeListenersString returns a string that should be emitted into generated\n// code to represent the set of listeners associated with a given component.\n\n\n\n\n\n\n\n// ComponentListeners represents a set of listeners for a given component.\ntype ComponentListeners struct {\n\t// Fully qualified component type name, e.g.,\n\t//   github.com/ServiceWeaver/weaver/Main.\n\tComponent string\n\n\t// The list of listener names associated with the component.\n\tListeners []string\n}\n\n// ExtractListeners returns the components and their listeners encoded using\n// MakeListenersString() in data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc checksumListeners(component, lisstr string) string {\n\tstr := fmt.Sprintf(\"wEaVeRlIsTeNeRs:%s\u2192%s\", component, lisstr)\n\tsum := sha256.Sum256([]byte(str))\n\treturn fmt.Sprintf(\"%0x\", sum)[:8]\n}\n\n```\n# Function Signature: func MakeListenersString(component string, listeners []string) string {\n# Natural Language Annotations: // All listeners used by a given component are embedded in the generated\n// binary as specially formatted strings. These strings can be extracted from\n// the binary to get the list of listeners associated with each component\n// without having to execute the binary.\n//\n// The set of listeners used by a given component is represented by a string\n// fragment that looks like:\n// \u27e6checksum:wEaVeRlIsTeNeRs:component\u2192listeners\u27e7\n//\n// checksum is the first 8 bytes of the hex encoding of the SHA-256 of\n// the string \"wEaVeRlIsTeNeRs:component\u2192listeners\"; component is the fully\n// qualified component type name; listeners is a comma-separated list of\n// all listener names associated with a given component.\n// MakeListenersString returns a string that should be emitted into generated\n// code to represent the set of listeners associated with a given component.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\npackage codegen\n\nimport (\n\t\"crypto/sha256\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"strings\"\n)\n\n// MakeListenersString returns a string that should be emitted into generated\n// code to represent the set of listeners associated with a given component.\nfunc MakeListenersString(component string, listeners []string) string {\n\tlisstr := strings.Join(listeners, \",\")\n\treturn fmt.Sprintf(\"\u27e6checksum:%s:component\u2192%s\u27e7\", checksumListeners(component, lisstr), lisstr)\n}\n\nfunc checksumListeners(component, lisstr string) string {\n\tstr := fmt.Sprintf(\"wEaVeRlIsTeNeRs:%s\u2192%s\", component, lisstr)\n\tsum := sha256.Sum256([]byte(str))\n\treturn fmt.Sprintf(\"%0x\", sum)[:8]\n}\n```", "  ```\nfunc MakeListenersString(component string, listeners []string) string {\n\tlisstr := strings.Join(listeners, \",\")\n\tchecksum := checksumListeners(component, lisstr)\n\treturn fmt.Sprintf(\"\u27e6%s:%s:%s\u27e7\", checksum, component, lisstr)\n}\n```\nThis function takes a component name and a slice of listener names as input, and returns a generated string that can be used to represent the set of listeners associated with the component in the generated binary. The generated string has the format `\u27e6checksum:wEaVeRlIsTeNeRs:component\u2192listeners\u27e7`, where `checksum` is the first 8 bytes of the SHA-256 sum of the string `\"wEaVeRlIsTeNeRs:component\u2192listeners\"`, `component` is the fully qualified component type name, and `listeners` is a comma-separated list of all listener names associated with the component.", "  ```\nfunc MakeListenersString(component string, listeners []string) string {\n\t// Sort the listeners to ensure the same order every time\n\tsort.Strings(listeners)\n\t// Join the listeners into a comma-separated list\n\tlisStr := strings.Join(listeners, \",\")\n\t// Get the hash of the component and listeners\n\tchecksum := checksumListeners(component, lisStr)\n\t// Format the string into a listeners fragment\n\treturn fmt.Sprintf(\"%0x:%s:%s\u2192%s\", checksum, component, lisStr)\n}\n```\nThis function takes in a component name string and a list of strings representing the listeners associated with that component. It sorts the listeners list to ensure the same order every time, joins the listeners into a comma-separated string, and then calculates the hash of the component and listeners using the `checksumListeners` function. This hash is then used to prefix the listeners string so that it can be recognized as a listeners fragment. The resulting string is then returned."]}, "weaver-runtime/codegen/listeners.go-ExtractListeners": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/codegen/listeners.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage codegen\n\nimport (\n\t\"crypto/sha256\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"sort\"\n\t\"strings\"\n)\n\n// All listeners used by a given component are embedded in the generated\n// binary as specially formatted strings. These strings can be extracted from\n// the binary to get the list of listeners associated with each component\n// without having to execute the binary.\n//\n// The set of listeners used by a given component is represented by a string\n// fragment that looks like:\n// \u27e6checksum:wEaVeRlIsTeNeRs:component\u2192listeners\u27e7\n//\n// checksum is the first 8 bytes of the hex encoding of the SHA-256 of\n// the string \"wEaVeRlIsTeNeRs:component\u2192listeners\"; component is the fully\n// qualified component type name; listeners is a comma-separated list of\n// all listener names associated with a given component.\n\n// MakeListenersString returns a string that should be emitted into generated\n// code to represent the set of listeners associated with a given component.\n\n\n\n\n\n\n\n// ComponentListeners represents a set of listeners for a given component.\ntype ComponentListeners struct {\n\t// Fully qualified component type name, e.g.,\n\t//   github.com/ServiceWeaver/weaver/Main.\n\tComponent string\n\n\t// The list of listener names associated with the component.\n\tListeners []string\n}\n\n// ExtractListeners returns the components and their listeners encoded using\n// MakeListenersString() in data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc checksumListeners(component, lisstr string) string {\n\tstr := fmt.Sprintf(\"wEaVeRlIsTeNeRs:%s\u2192%s\", component, lisstr)\n\tsum := sha256.Sum256([]byte(str))\n\treturn fmt.Sprintf(\"%0x\", sum)[:8]\n}\n\n```\n# Function Signature: func ExtractListeners(data []byte) []ComponentListeners {\n# Natural Language Annotations: // ExtractListeners returns the components and their listeners encoded using\n// MakeListenersString() in data.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc ExtractListeners(data []byte) []ComponentListeners {\n\t// Extract all occurrences of the listener embed string from the data\n\tlisteners := regexp.MustCompile(`wEaVeRlIsTeNeRs:\\w+\u2192[\\w,]*`).FindAllStringSubmatch(string(data), -1)\n\n\t// Iterate over the extracted listeners and extract the component and listener list\n\tvar componentListeners []ComponentListeners\n\tfor _, listener := range listeners {\n\t\t// Extract the component name\n\t\tcomponents := strings.Split(listener[0], \"\u2192\")\n\t\tcomponent := components[0]\n\n\t\t// Extract the listener list\n\t\tlisteners := strings.Split(components[1], \",\")\n\n\t\t// Add the component to the list of component listeners\n\t\tcomponentListeners = append(componentListeners, ComponentListeners{\n\t\t\tComponent: component,\n\t\t\tListeners: listeners,\n\t\t})\n\t}\n\n\t// Sort the component listeners by component name\n\tsort.Slice(componentListeners, func(i, j int) bool {\n\t\treturn componentListeners[i].Component < componentListeners[j].Component\n\t})\n\n\treturn componentListeners\n}\n```", "  ```\npackage codegen\n\nimport (\n\t\"crypto/sha256\"\n\t\"fmt\"\n\t\"strings\"\n)\n\n// ExtractListeners returns the components and their listeners encoded using\n// MakeListenersString() in data.\nfunc ExtractListeners(data []byte) []ComponentListeners {\n\tvar components []ComponentListeners\n\n\t// Split the data into individual listeners.\n\tlisteners := strings.Split(string(data), \"\u27e6\")\n\n\t// Parse each listener string.\n\tfor _, l := range listeners {\n\t\t// Split the listener string into the checksum and remainder.\n\t\tparts := strings.SplitN(l, \":\", 2)\n\t\tif len(parts) != 2 {\n\t\t\tcontinue\n\t\t}\n\t\tchecksum, remainder := parts[0], parts[1]\n\n\t\t// Verify the checksum.\n\t\tif checksum != checksumListeners(\"\", remainder) {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Extract the component and listeners from the remainder string.\n\t\tcomponentStart := strings.Index(remainder, \":component\u2192\")\n\t\tif componentStart == -1 {\n\t\t\tcontinue\n\t\t}\n\t\tcomponent := remainder[componentStart+len(\":component\u2192\"):]\n\t\tlistenerStart := strings.Index(remainder, \"\u2192listeners\")\n\t\tif listenerStart == -1 {\n\t\t\tcontinue\n\t\t}\n\t\tlisteners := remainder[listenerStart+len(\"\u2192listeners\"):]\n\n\t\t// Add the component and its listeners to the list.\n\t\tcomponents = append(components, ComponentListeners{\n\t\t\tComponent: component,\n\t\t\tListeners: strings.Split(listeners, \",\"),\n\t\t})\n\t}\n\n\treturn components\n}\n\n```", "  ```\nfunc ExtractListeners(data []byte) []ComponentListeners {\n\t// Extract the encoding string from the given data.\n\tsn := reListeners.FindSubmatch(data)\n\tif len(sn) < 3 {\n\t\treturn nil\n\t}\n\n\t// Decode the component type name and the list of listener names.\n\tcomp := sn[1]\n\tlistr := sn[2]\n\n\t// Create a new ComponentListeners struct for each unwrapped emission.\n\tvar cls []ComponentListeners\n\tfor _, l := range strings.Split(listr, \",\") {\n\t\tcls = append(cls, ComponentListeners{\n\t\t\tComponent: comp,\n\t\t\tListeners: []string{l},\n\t\t})\n\t}\n\n\treturn cls\n}\n```"]}, "weaver-runtime/codegen/hash.go-Sum64": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/codegen/hash.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage codegen\n\nimport (\n\t\"crypto/sha256\"\n\t\"encoding/binary\"\n\t\"math\"\n)\n\n// Hasher computes a non-cryptographic hash of the sequence of values\n// added to it.\n//\n// If the same sequence of values is added to two differ Hashers, they\n// will produce the same result, even if they are in different processes.\ntype Hasher struct {\n\t// TODO: improve performance:\n\t// - do not accumulate everything; hash as we go\n\t// - use a non-cryptographically safe hasher\n\tenc Encoder\n}\n\n// Sum64 returns the 64-bit hash of the sequence of values added so far.\n// The resulting is in the range [1,2^64-2], i.e., it is never 0 or math.MaxUint64.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WriteString adds a string to the hasher.\nfunc (h *Hasher) WriteString(v string) { h.enc.String(v) }\n\n// WriteFloat32 adds a float32 to the hasher.\nfunc (h *Hasher) WriteFloat32(v float32) { h.enc.Float32(v) }\n\n// WriteFloat64 adds a float64 to the hasher.\nfunc (h *Hasher) WriteFloat64(v float64) { h.enc.Float64(v) }\n\n// WriteInt adds a int to the hasher.\nfunc (h *Hasher) WriteInt(v int) { h.enc.Int(v) }\n\n// WriteInt8 adds a int8 to the hasher.\nfunc (h *Hasher) WriteInt8(v int8) { h.enc.Int8(v) }\n\n// WriteInt16 adds a int16 to the hasher.\nfunc (h *Hasher) WriteInt16(v int16) { h.enc.Int16(v) }\n\n// WriteInt32 adds a int32 to the hasher.\nfunc (h *Hasher) WriteInt32(v int32) { h.enc.Int32(v) }\n\n// WriteInt64 adds a int64 to the hasher.\nfunc (h *Hasher) WriteInt64(v int64) { h.enc.Int64(v) }\n\n// WriteUint adds a uint to the hasher.\nfunc (h *Hasher) WriteUint(v uint) { h.enc.Uint(v) }\n\n// WriteUint8 adds a uint8 to the hasher.\nfunc (h *Hasher) WriteUint8(v uint8) { h.enc.Uint8(v) }\n\n// WriteUint16 adds a uint16 to the hasher.\nfunc (h *Hasher) WriteUint16(v uint16) { h.enc.Uint16(v) }\n\n// WriteUint32 adds a uint32 to the hasher.\nfunc (h *Hasher) WriteUint32(v uint32) { h.enc.Uint32(v) }\n\n// WriteUint64 adds a uint64 to the hasher.\nfunc (h *Hasher) WriteUint64(v uint64) { h.enc.Uint64(v) }\n\n```\n# Function Signature: func (h *Hasher) Sum64() uint64 {\n# Natural Language Annotations: // Sum64 returns the 64-bit hash of the sequence of values added so far.\n// The resulting is in the range [1,2^64-2], i.e., it is never 0 or math.MaxUint64.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (h *Hasher) Sum64() uint64 {\n\t// TODO: improve performance:\n\t// - do not accumulate everything; hash as we go\n\t// - use a non-cryptographically safe hasher\n\treturn binary.BigEndian.Uint64(h.enc.Sum64())\n}\n```", "  ```\nfunc (h *Hasher) Sum64() uint64 {\n\t// TODO: make this more efficient by only hashing what's necessary\n\tsum := uint64(0)\n\tfor _, v := range h.enc.buff {\n\t\tsum += v\n\t}\n\treturn sum % math.MaxUint64\n}\n```", "  ```\nfunc (h *Hasher) Sum64() uint64 {\n\t// Complete the hash of the accumulated values.\n\th.enc.Flush()\n\t// Use the SHA256 hash function to convert the encoded bytes to a 64-bit hash value.\n\thash := sha256.Sum256(h.enc.Bytes())\n\t// Convert the hash to a uint64 value.\n\treturn binary.BigEndian.Uint64(hash[:])\n}\n```"]}, "weaver-runtime/graph/adjacency.go-NewAdjacencyGraph": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/graph/adjacency.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage graph\n\nimport (\n\t\"fmt\"\n\n\t\"golang.org/x/exp/slices\"\n)\n\ntype adjacencyGraph struct {\n\t// out[n] stores a list of nodes that n has an outgoing edge to.\n\t// out[n] == nil means that n is not a node in the graph.\n\t// out[n] == []Node{} means that node n exists but has no outgoing edges.\n\tout [][]Node\n}\n\nvar _ Graph = &adjacencyGraph{}\n\n// NewAdjacencyGraph returns a Graph represented using adjacency lists.\n//\n// It panics if it specified edge nodes aren't in nodes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar _ Graph = &adjacencyGraph{}\n\n// PerNode implements the Graph interface.\nfunc (g *adjacencyGraph) PerNode(fn func(n Node)) {\n\tfor n, dsts := range g.out {\n\t\tif dsts == nil { // not a node\n\t\t\tcontinue\n\t\t}\n\t\tfn(Node(n))\n\t}\n}\n\n// PerOutEdge implements the Graph interface.\nfunc (g *adjacencyGraph) PerOutEdge(src Node, fn func(e Edge)) {\n\tif !isNode(src, g.out) {\n\t\tpanic(fmt.Sprintf(\"src %d is not a node\", src))\n\t}\n\tfor _, dst := range g.out[src] {\n\t\tfn(Edge{Src: src, Dst: dst})\n\t}\n}\n\n// NodeLimit implements the Graph interface.\nfunc (g *adjacencyGraph) NodeLimit() int {\n\treturn len(g.out)\n}\n\nfunc isNode(n Node, out [][]Node) bool {\n\treturn n >= 0 && int(n) < len(out) && out[n] != nil\n}\n\n```\n# Function Signature: func NewAdjacencyGraph(nodes []Node, edges []Edge) Graph {\n# Natural Language Annotations: // NewAdjacencyGraph returns a Graph represented using adjacency lists.\n//\n// It panics if it specified edge nodes aren't in nodes.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewAdjacencyGraph(nodes []Node, edges []Edge) Graph {\n\tg := &adjacencyGraph{\n\t\tout: make([][]Node, len(nodes)),\n\t}\n\tfor _, n := range nodes {\n\t\tg.out[n] = []Node{}\n\t}\n\tfor _, e := range edges {\n\t\tsrc := e.Src\n\t\tdst := e.Dst\n\t\tif !isNode(src, g.out) {\n\t\t\tpanic(fmt.Sprintf(\"src %d is not a node\", src))\n\t\t}\n\t\tif !isNode(dst, g.out) {\n\t\t\tpanic(fmt.Sprintf(\"dst %d is not a node\", dst))\n\t\t}\n\t\tg.out[src] = append(g.out[src], dst)\n\t}\n\treturn g\n}\n```", "  ```\nfunc NewAdjacencyGraph(nodes []Node, edges []Edge) Graph {\n\tg := &adjacencyGraph{\n\t\tout: make([][]Node, len(nodes)),\n\t}\n\tfor _, src := range nodes {\n\t\tg.out[src] = []Node{}\n\t}\n\tfor _, edge := range edges {\n\t\tsrc := edge[0]\n\t\tdst := edge[1]\n\t\tif !isNode(src, g.out) {\n\t\t\tpanic(fmt.Sprintf(\"src %d is not a node\", src))\n\t\t}\n\t\tif !isNode(dst, g.out) {\n\t\t\tpanic(fmt.Sprintf(\"dst %d is not a node\", dst))\n\t\t}\n\t\tg.out[src] = append(g.out[src], dst)\n\t}\n\treturn g\n}\n```", "  ```\nfunc NewAdjacencyGraph(nodes []Node, edges []Edge) Graph {\n\tout := make([][]Node, len(nodes))\n\tfor i, n := range nodes {\n\t\tout[i] = []Node{}\n\t}\n\tfor _, e := range edges {\n\t\tif !isNode(e.Src, out) || !isNode(e.Dst, out) {\n\t\t\tpanic(fmt.Sprintf(\"edge source/dest %d %d is not a node\", e.Src, e.Dst))\n\t\t}\n\t\tout[e.Src] = append(out[e.Src], e.Dst)\n\t}\n\treturn &adjacencyGraph{out: out}\n}\n```"]}, "weaver-runtime/graph/dfs.go-DFSAll": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/graph/dfs.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage graph\n\nimport \"slices\"\n\n// DFSAll performs a depth first search of all nodes in g.\n// If enter is non-nil, it is called on entry to a node.\n// If exit is non-nil, it is called on exit from a node.\n\n\n\n\n\n\n\n\n// PostOrder returns nodes in g in post-order.\nfunc PostOrder(g Graph) []Node {\n\tvar result []Node\n\tDFSAll(g, nil, func(n Node) {\n\t\tresult = append(result, n)\n\t})\n\treturn result\n}\n\n// ReversePostOrder returns nodes in g in reverse-post-order.\nfunc ReversePostOrder(g Graph) []Node {\n\tresult := PostOrder(g)\n\tslices.Reverse(result)\n\treturn result\n}\n\nfunc dfs(g Graph, roots []Node, enter, exit func(Node)) {\n\t// Stack holds nodes to traverse.  If we need to call exit, we\n\t// leave a negative marker at the appropriate place in the stack.\n\tvar stack []Node\n\tvisited := make([]bool, g.NodeLimit())\n\tfor _, r := range roots {\n\t\tstack = append(stack, r)\n\t\tfor len(stack) > 0 {\n\t\t\tn := stack[len(stack)-1]\n\t\t\tstack = stack[:len(stack)-1]\n\t\t\tif n < 0 {\n\t\t\t\texit(-n - 1)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif visited[n] {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tvisited[n] = true\n\t\t\tif exit != nil {\n\t\t\t\tstack = append(stack, -n-1) // Exit marker\n\t\t\t}\n\t\t\tif enter != nil {\n\t\t\t\tenter(n)\n\t\t\t}\n\t\t\tg.PerOutEdge(n, func(e Edge) {\n\t\t\t\tstack = append(stack, e.Dst)\n\t\t\t})\n\t\t}\n\t}\n}\n\n```\n# Function Signature: func DFSAll(g Graph, enter, exit func(Node)) {\n# Natural Language Annotations: // DFSAll performs a depth first search of all nodes in g.\n// If enter is non-nil, it is called on entry to a node.\n// If exit is non-nil, it is called on exit from a node.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc DFSAll(g Graph, enter, exit func(Node)) {\n\tdfs(g, nil, enter, exit)\n}\n```", "  ```\nfunc DFSAll(g Graph, enter, exit func(Node)) {\n    // Stack holds nodes to traverse.  If we need to call exit, we\n    // leave a negative marker at the appropriate place in the stack.\n    var stack []Node\n    visited := make([]bool, g.NodeLimit())\n    for n := range g.Nodes() {\n        stack = append(stack, n)\n        for len(stack) > 0 {\n            n := stack[len(stack)-1]\n            stack = stack[:len(stack)-1]\n            if n < 0 {\n                exit(-n - 1)\n                continue\n            }\n            if visited[n] {\n                continue\n            }\n            visited[n] = true\n            if exit != nil {\n                stack = append(stack, -n-1) // Exit marker\n            }\n            if enter != nil {\n                enter(n)\n            }\n            g.PerOutEdge(n, func(e Edge) {\n                stack = append(stack, e.Dst)\n            })\n        }\n    }\n}\n```", "  ```\nfunc DFSAll(g Graph, enter, exit func(Node)) {\n    // Stack holds nodes to traverse.  If we need to call exit, we\n    // leave a negative marker at the appropriate place in the stack.\n    var stack []Node\n    visited := make([]bool, g.NodeLimit())\n    for n := 0; n < g.NodeLimit(); n++ {\n        if !visited[n] {\n            stack = append(stack, n)\n            for len(stack) > 0 {\n                n := stack[len(stack)-1]\n                stack = stack[:len(stack)-1]\n                if n < 0 {\n                    exit(-n - 1)\n                    continue\n                }\n                if visited[n] {\n                    continue\n                }\n                visited[n] = true\n                if exit != nil {\n                    stack = append(stack, -n-1) // Exit marker\n                }\n                if enter != nil {\n                    enter(n)\n                }\n                g.PerOutEdge(n, func(e Edge) {\n                    stack = append(stack, e.Dst)\n                })\n            }\n        }\n    }\n}\n```"]}, "weaver-runtime/metrics/labels.go-newLabelExtractor": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/metrics/labels.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage metrics\n\nimport (\n\t\"fmt\"\n\t\"reflect\"\n\t\"unicode\"\n\t\"unicode/utf8\"\n)\n\n// unexport returns a copy of s with the first letter lowercased.\nfunc unexport(s string) string {\n\t// NOTE(mwhittaker): Handling unicode complicates the implementation of\n\t// this function. I took this implementation from [1].\n\t//\n\t// [1]: https://groups.google.com/g/golang-nuts/c/WfpmVDQFecU/m/-1IBD5KI7GEJ.\n\tif s == \"\" {\n\t\treturn \"\"\n\t}\n\tr, n := utf8.DecodeRuneInString(s)\n\treturn string(unicode.ToLower(r)) + s[n:]\n}\n\n// typecheckLabels checks that L is a valid label struct type. See metricMap\n// for a description of valid label struct types.\nfunc typecheckLabels[L comparable]() error {\n\tvar x L\n\tt := reflect.TypeOf(x)\n\tif t.Kind() != reflect.Struct {\n\t\treturn fmt.Errorf(\"metric labels: type %T is not a struct\", x)\n\t}\n\n\tnames := map[string]struct{}{}\n\tfor i := 0; i < t.NumField(); i++ {\n\t\tfi := t.Field(i)\n\n\t\t// Check the type.\n\t\tif fi.Type.PkgPath() != \"\" {\n\t\t\t// Avoid named types like `type foo string`\n\t\t\treturn fmt.Errorf(\"metric labels: field %q of type %T has unsupported type %v\", fi.Name, x, fi.Type.Name())\n\t\t}\n\t\tswitch fi.Type.Kind() {\n\t\tcase reflect.String,\n\t\t\treflect.Bool,\n\t\t\treflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64,\n\t\t\treflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64:\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"metric labels: field %q of type %T has unsupported type %v\", fi.Name, x, fi.Type.Name())\n\t\t}\n\n\t\t// Check the visibility.\n\t\tif !fi.IsExported() {\n\t\t\treturn fmt.Errorf(\"metric labels: field %q of type %T is unexported\", fi.Name, x)\n\t\t}\n\n\t\t// Check for duplicate fields.\n\t\tname := unexport(fi.Name)\n\t\tif alias, ok := fi.Tag.Lookup(\"weaver\"); ok {\n\t\t\tname = alias\n\t\t}\n\t\tif _, ok := names[name]; ok {\n\t\t\treturn fmt.Errorf(\"metric labels: type %T has duplicate field %q\", x, fi.Name)\n\t\t}\n\t\tnames[name] = struct{}{}\n\t}\n\n\treturn nil\n}\n\n// labelExtractor extracts labels from a label struct of type L.\ntype labelExtractor[L comparable] struct {\n\tfields []field\n}\n\ntype field struct {\n\tf    reflect.StructField // struct field\n\tname string              // field name, or alias if present\n}\n\n// newLabelExtractor returns a new labelExtractor that can extract the labels\n// from a label struct of type L. L must be a valid label struct type.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Extract extracts the labels from a label struct. The provided labels must be\n// the same type used to construct the labelExtractor.\nfunc (l *labelExtractor[L]) Extract(labels L) map[string]string {\n\tv := reflect.ValueOf(labels)\n\textracted := map[string]string{}\n\tfor _, field := range l.fields {\n\t\textracted[field.name] = fmt.Sprint(v.FieldByIndex(field.f.Index).Interface())\n\t}\n\treturn extracted\n}\n\n```\n# Function Signature: func newLabelExtractor[L comparable]() *labelExtractor[L] {\n# Natural Language Annotations: // newLabelExtractor returns a new labelExtractor that can extract the labels\n// from a label struct of type L. L must be a valid label struct type.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc newLabelExtractor[L comparable]() *labelExtractor[L] {\n\treturn &labelExtractor[L]{}\n}\n```", "  ```\nfunc newLabelExtractor[L comparable]() *labelExtractor[L] {\n\treturn &labelExtractor[L]{}\n}\n```", "  ```\nfunc newLabelExtractor[L comparable]() *labelExtractor[L] {\n\treturn &labelExtractor[L]{\n\t\tfields: []field{},\n\t}\n}\n```\nThis function signature indicates that the function is a constructor for a struct with type parameters. It returns a pointer to a `labelExtractor[L]` struct, which has a field `fields` of type `[]field`. The `field` type is also a struct, with a field `f` of type `reflect.StructField` and a field `name` of type `string`.\n\nThe natural language annotations indicate that the function returns a new `labelExtractor` that can extract labels from a label struct of a generic type `L` that must be a valid label struct type. The `[]field` slice is initialized with an empty slice, and the `field` struct is populated with the appropriate values for the `f` field, which is of type `reflect.StructField`, and for the `name` field, which is a string representation of the field name or alias if present."]}, "weaver-runtime/metrics/metrics.go-Snapshot": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/metrics/metrics.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package metrics implements Service Weaver metrics.\npackage metrics\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"math\"\n\t\"slices\"\n\t\"sort\"\n\t\"sync\"\n\t\"sync/atomic\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/google/uuid\"\n\t\"golang.org/x/exp/maps\"\n)\n\nvar (\n\t// metricNames stores the name of every metric (labeled or not).\n\tmetricNamesMu sync.RWMutex\n\tmetricNames   = map[string]bool{}\n\n\t// metrics stores every metric.\n\tmetricsMu sync.RWMutex\n\tmetrics   = []*Metric{}\n)\n\n// Metric is a thread-safe readable and writeable metric. It is the underlying\n// implementation of the user-facing metrics like Counter and Gauge.\n//\n// Every metric has a unique name assigned by the user. For example, the user\n// may create a histogram called \"http_request_duration\". Every metric also has\n// a fixed, possibly empty, set of labels. For example, the user may assign an\n// \"endpoint\" label to their \"http_request_duration\" to differentiate the\n// latency of different HTTP endpoints. A metric name and set of label values\n// uniquely identify a metric. For example, the following two metrics are\n// different:\n//\n//\thttp_request_duration{endpoint=\"/\"}\n//\thttp_request_duration{endpoint=\"/foo\"}\ntype Metric struct {\n\ttyp         protos.MetricType        // the type of the metric\n\tname        string                   // the globally unique metric name\n\thelp        string                   // a short description of the metric\n\tlabelsThunk func() map[string]string // the (deferred) metric labels\n\n\t// Users may call Get on the critical path of their application, so we want\n\t// a call of `Get(labels)` to be as fast as possible. Converting `labels`\n\t// into a map[string]string requires reflection and can be slow. Computing\n\t// the metric's id is similarly slow. We avoid doing either of these in the\n\t// call to Get and instead initialize them only when needed (i.e. before\n\t// exporting).\n\tonce   sync.Once         // used to initialize id and labels\n\tid     uint64            // globally unique metric id\n\tlabels map[string]string // materialized labels from calling labelsThunk\n\n\tfvalue atomicFloat64 // value for Counter and Gauge, sum for Histogram\n\tivalue atomic.Uint64 // integer increments for Counter (separated for speed)\n\n\t// For histograms only:\n\tputCount atomic.Uint64   // incremented on every Put, for change detection\n\tbounds   []float64       // histogram bounds\n\tcounts   []atomic.Uint64 // histogram counts\n}\n\n// A MetricSnapshot is a snapshot of a metric.\ntype MetricSnapshot struct {\n\tId     uint64\n\tType   protos.MetricType\n\tName   string\n\tLabels map[string]string\n\tHelp   string\n\n\tValue  float64\n\tBounds []float64\n\tCounts []uint64\n}\n\n// MetricDef returns a MetricDef derived from the metric.\nfunc (m *MetricSnapshot) MetricDef() *protos.MetricDef {\n\treturn &protos.MetricDef{\n\t\tId:     m.Id,\n\t\tName:   m.Name,\n\t\tTyp:    m.Type,\n\t\tHelp:   m.Help,\n\t\tLabels: m.Labels,\n\t\tBounds: m.Bounds,\n\t}\n}\n\n// MetricValue returns a MetricValue derived from the metric.\nfunc (m *MetricSnapshot) MetricValue() *protos.MetricValue {\n\treturn &protos.MetricValue{\n\t\tId:     m.Id,\n\t\tValue:  m.Value,\n\t\tCounts: m.Counts,\n\t}\n}\n\n// ToProto converts a MetricSnapshot to its proto equivalent.\nfunc (m *MetricSnapshot) ToProto() *protos.MetricSnapshot {\n\treturn &protos.MetricSnapshot{\n\t\tId:     m.Id,\n\t\tName:   m.Name,\n\t\tTyp:    m.Type,\n\t\tHelp:   m.Help,\n\t\tLabels: m.Labels,\n\t\tBounds: m.Bounds,\n\t\tValue:  m.Value,\n\t\tCounts: m.Counts,\n\t}\n}\n\n// UnProto converts a protos.MetricSnapshot into a metrics.MetricSnapshot.\nfunc UnProto(m *protos.MetricSnapshot) *MetricSnapshot {\n\treturn &MetricSnapshot{\n\t\tId:     m.Id,\n\t\tType:   m.Typ,\n\t\tName:   m.Name,\n\t\tLabels: m.Labels,\n\t\tHelp:   m.Help,\n\t\tValue:  m.Value,\n\t\tBounds: m.Bounds,\n\t\tCounts: m.Counts,\n\t}\n}\n\n// Clone returns a deep copy of m.\nfunc (m *MetricSnapshot) Clone() *MetricSnapshot {\n\tc := *m\n\tc.Labels = maps.Clone(m.Labels)\n\tc.Bounds = slices.Clone(m.Bounds)\n\tc.Counts = slices.Clone(m.Counts)\n\treturn &c\n}\n\n// config configures the creation of a metric.\ntype config struct {\n\tType   protos.MetricType\n\tName   string\n\tLabels func() map[string]string\n\tBounds []float64\n\tHelp   string\n}\n\n// Register registers and returns a new metric. Panics if a metric with the same name\n// has already been registered.\nfunc Register(typ protos.MetricType, name string, help string, bounds []float64) *Metric {\n\tm := RegisterMap[struct{}](typ, name, help, bounds)\n\treturn m.Get(struct{}{})\n}\n\n// newMetric registers and returns a new metric.\nfunc newMetric(config config) *Metric {\n\tmetricsMu.Lock()\n\tdefer metricsMu.Unlock()\n\tmetric := &Metric{\n\t\ttyp:         config.Type,\n\t\tname:        config.Name,\n\t\thelp:        config.Help,\n\t\tlabelsThunk: config.Labels,\n\t\tbounds:      config.Bounds,\n\t}\n\tif config.Type == protos.MetricType_HISTOGRAM {\n\t\tmetric.counts = make([]atomic.Uint64, len(config.Bounds)+1)\n\t}\n\tmetrics = append(metrics, metric)\n\treturn metric\n}\n\n// Name returns the name of the metric.\nfunc (m *Metric) Name() string {\n\treturn m.name\n}\n\n// Inc adds one to the metric value.\nfunc (m *Metric) Inc() {\n\tm.ivalue.Add(1)\n}\n\n// Add adds the provided delta to the metric's value.\nfunc (m *Metric) Add(delta float64) {\n\tm.fvalue.add(delta)\n}\n\n// Sub subtracts the provided delta from the metric's value.\nfunc (m *Metric) Sub(delta float64) {\n\tm.fvalue.add(-delta)\n}\n\n// Set sets the metric's value.\nfunc (m *Metric) Set(val float64) {\n\tm.fvalue.set(val)\n}\n\n// Put adds the provided value to the metric's histogram.\nfunc (m *Metric) Put(val float64) {\n\tvar idx int\n\tif len(m.bounds) == 0 || val < m.bounds[0] {\n\t\t// Skip binary search for values that fall in the first bucket\n\t\t// (often true for short latency operations).\n\t} else {\n\t\tidx = sort.SearchFloat64s(m.bounds, val)\n\t\tif idx < len(m.bounds) && val == m.bounds[idx] {\n\t\t\tidx++\n\t\t}\n\t}\n\tm.counts[idx].Add(1)\n\n\t// Microsecond latencies are often zero for very fast functions.\n\tif val != 0 {\n\t\tm.fvalue.add(val)\n\t}\n\tm.putCount.Add(1)\n}\n\n// initIdAndLabels initializes the id and labels of a metric.\n// We delay this initialization until the first time we export a\n// metric to avoid slowing down a Get() call.\nfunc (m *Metric) initIdAndLabels() {\n\tm.once.Do(func() {\n\t\tif labels := m.labelsThunk(); len(labels) > 0 {\n\t\t\tm.labels = labels\n\t\t}\n\t\tvar id [16]byte = uuid.New()\n\t\tm.id = binary.LittleEndian.Uint64(id[:8])\n\t})\n}\n\n// get returns the current value (sum of all added values for histograms).\nfunc (m *Metric) get() float64 {\n\treturn m.fvalue.get() + float64(m.ivalue.Load())\n}\n\n// Snapshot returns a snapshot of the metric. You must call Init at least once\n// before calling Snapshot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// MetricDef returns a MetricDef derived from the metric. You must call Init at\n// least once before calling Snapshot.\nfunc (m *Metric) MetricDef() *protos.MetricDef {\n\treturn &protos.MetricDef{\n\t\tId:     m.id,\n\t\tName:   m.name,\n\t\tTyp:    m.typ,\n\t\tHelp:   m.help,\n\t\tLabels: maps.Clone(m.labels),\n\t\tBounds: slices.Clone(m.bounds),\n\t}\n}\n\n// MetricValue returns a MetricValue derived from the metric.\nfunc (m *Metric) MetricValue() *protos.MetricValue {\n\tvar counts []uint64\n\tif n := len(m.counts); n > 0 {\n\t\tcounts = make([]uint64, n)\n\t\tfor i := range m.counts {\n\t\t\tcounts[i] = m.counts[i].Load()\n\t\t}\n\t}\n\treturn &protos.MetricValue{\n\t\tId:     m.id,\n\t\tValue:  m.get(),\n\t\tCounts: counts,\n\t}\n}\n\n// MetricMap is a collection of metrics with the same name and label schema\n// but with different label values. See public metric documentation for\n// an explanation of labels.\n//\n// TODO(mwhittaker): Understand the behavior of prometheus and Google Cloud\n// Metrics when we add or remove metric labels over time.\ntype MetricMap[L comparable] struct {\n\tconfig    config             // configures the metrics returned by Get\n\textractor *labelExtractor[L] // extracts labels from a value of type L\n\tmu        sync.Mutex         // guards metrics\n\tmetrics   map[L]*Metric      // cache of metrics, by label\n}\n\nfunc RegisterMap[L comparable](typ protos.MetricType, name string, help string, bounds []float64) *MetricMap[L] {\n\tif err := typecheckLabels[L](); err != nil {\n\t\tpanic(err)\n\t}\n\tif name == \"\" {\n\t\tpanic(fmt.Errorf(\"empty metric name\"))\n\t}\n\tif typ == protos.MetricType_INVALID {\n\t\tpanic(fmt.Errorf(\"metric %q: invalid metric type %v\", name, typ))\n\t}\n\tfor _, x := range bounds {\n\t\tif math.IsNaN(x) {\n\t\t\tpanic(fmt.Errorf(\"metric %q: NaN histogram bound\", name))\n\t\t}\n\t}\n\tfor i := 0; i < len(bounds)-1; i++ {\n\t\tif bounds[i] >= bounds[i+1] {\n\t\t\tpanic(fmt.Errorf(\"metric %q: non-ascending histogram bounds %v\", name, bounds))\n\t\t}\n\t}\n\n\tmetricNamesMu.Lock()\n\tdefer metricNamesMu.Unlock()\n\tif metricNames[name] {\n\t\tpanic(fmt.Errorf(\"metric %q already exists\", name))\n\t}\n\tmetricNames[name] = true\n\treturn &MetricMap[L]{\n\t\tconfig:    config{Type: typ, Name: name, Help: help, Bounds: bounds},\n\t\textractor: newLabelExtractor[L](),\n\t\tmetrics:   map[L]*Metric{},\n\t}\n}\n\n// Name returns the name of the metricMap.\nfunc (mm *MetricMap[L]) Name() string {\n\treturn mm.config.Name\n}\n\n// Get returns the metric with the provided labels, constructing it if it\n// doesn't already exist. Multiple calls to Get with the same labels will\n// return the same metric.\nfunc (mm *MetricMap[L]) Get(labels L) *Metric {\n\tmm.mu.Lock()\n\tdefer mm.mu.Unlock()\n\tif metric, ok := mm.metrics[labels]; ok {\n\t\treturn metric\n\t}\n\tconfig := mm.config\n\tconfig.Labels = func() map[string]string {\n\t\treturn mm.extractor.Extract(labels)\n\t}\n\tmetric := newMetric(config)\n\tmm.metrics[labels] = metric\n\treturn metric\n}\n\n// Snapshot returns a snapshot of all currently registered metrics. The\n// snapshot is not guaranteed to be atomic.\nfunc Snapshot() []*MetricSnapshot {\n\tmetricsMu.RLock()\n\tdefer metricsMu.RUnlock()\n\tsnapshots := make([]*MetricSnapshot, 0, len(metrics))\n\tfor _, metric := range metrics {\n\t\tmetric.initIdAndLabels()\n\t\tsnapshots = append(snapshots, metric.Snapshot())\n\t}\n\treturn snapshots\n}\n\n```\n# Function Signature: func (m *Metric) Snapshot() *MetricSnapshot {\n# Natural Language Annotations: // Snapshot returns a snapshot of the metric. You must call Init at least once\n// before calling Snapshot.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (m *Metric) Snapshot() *MetricSnapshot {\n\tm.initIdAndLabels()\n\treturn &MetricSnapshot{\n\t\tId:     m.id,\n\t\tType:   m.typ,\n\t\tName:   m.name,\n\t\tLabels: maps.Clone(m.labels),\n\t\tHelp:   m.help,\n\t\tBounds: slices.Clone(m.bounds),\n\t\tValue:  m.get(),\n\t\tCounts: slices.Clone(m.counts),\n\t}\n}\n```", "  ```\nfunc (m *Metric) Snapshot() *MetricSnapshot {\n\tvar counts []uint64\n\tif n := len(m.counts); n > 0 {\n\t\tcounts = make([]uint64, n)\n\t\tfor i := range m.counts {\n\t\t\tcounts[i] = m.counts[i].Load()\n\t\t}\n\t}\n\tm.initIdAndLabels()\n\treturn &MetricSnapshot{\n\t\tId:     m.id,\n\t\tType:   m.typ,\n\t\tName:   m.name,\n\t\tLabels: maps.Clone(m.labels),\n\t\tValue:  m.get(),\n\t\tCounts: counts,\n\t}\n}\n```", "  ```\nfunc (m *Metric) Snapshot() *MetricSnapshot {\n\tm.mu.RLock()\n\tdefer m.mu.RUnlock()\n\treturn &MetricSnapshot{\n\t\tId:     m.id,\n\t\tType:   m.typ,\n\t\tName:   m.name,\n\t\tLabels: maps.Clone(m.labels),\n\t\tHelp:   m.help,\n\t\tValue:  m.get(),\n\t\tCounts: slices.Clone(m.counts),\n\t}\n}\n```"]}, "weaver-internal/tool/generate/types.go-sizeOfType": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/tool/generate/types.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generate\n\nimport (\n\t\"fmt\"\n\t\"go/types\"\n\t\"path\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"golang.org/x/tools/go/packages\"\n\t\"golang.org/x/tools/go/types/typeutil\"\n)\n\nconst weaverPackagePath = \"github.com/ServiceWeaver/weaver\"\n\n// typeSet holds type information needed by the code generator.\ntype typeSet struct {\n\tpkg            *packages.Package\n\timported       []importPkg          // imported packages\n\timportedByPath map[string]importPkg // imported, indexed by path\n\timportedByName map[string]importPkg // imported, indexed by name\n\n\tautomarshals          *typeutil.Map // types that implement AutoMarshal\n\tautomarshalCandidates *typeutil.Map // types that declare themselves AutoMarshal\n\n\t// If checked[t] != nil, then checked[t] is the cached result of calling\n\t// check(pkg, t, string[]{}). Otherwise, if checked[t] == nil, then t has\n\t// not yet been checked for serializability. Read typeutil.Map's\n\t// documentation for why checked shouldn't be a map[types.Type]bool.\n\tchecked typeutil.Map\n\n\t// If sizes[t] != nil, then sizes[t] == sizeOfType(t).\n\tsizes typeutil.Map\n\n\t// If measurable[t] != nil, then measurable[t] == isMeasurableType(t).\n\tmeasurable typeutil.Map\n}\n\n// importPkg is a package imported by the generated code.\ntype importPkg struct {\n\tpath  string // e.g., \"github.com/ServiceWeaver/weaver\"\n\tpkg   string // e.g., \"weaver\", \"context\", \"time\"\n\talias string // e.g., foo in `import foo \"context\"`\n\tlocal bool   // are we in this package?\n}\n\n// name returns the name by which the imported package should be referenced in\n// the generated code. If the package is imported without an alias, like this:\n//\n//\timport \"context\"\n//\n// then the name is the same as the package name (e.g., \"context\"). However, if\n// a package is imported with an alias, then the name is the alias:\n//\n//\timport thisIsAnAlias \"context\"\n//\n// If the package is local, an empty string is returned.\nfunc (i importPkg) name() string {\n\tif i.local {\n\t\treturn \"\"\n\t} else if i.alias != \"\" {\n\t\treturn i.alias\n\t}\n\treturn i.pkg\n}\n\n// qualify returns the provided member of the package, qualified with the\n// package name. For example, the \"Context\" type inside the \"context\" package\n// is qualified \"context.Context\". The \"Now\" function inside the \"time\" package\n// is qualified \"time.Now\". Note that the package name is not prefixed when\n// qualifying members of the local package.\nfunc (i importPkg) qualify(member string) string {\n\tif i.local {\n\t\treturn member\n\t}\n\treturn fmt.Sprintf(\"%s.%s\", i.name(), member)\n}\n\n// newTypeSet returns the container for types found in pkg.\nfunc newTypeSet(pkg *packages.Package, automarshals, automarshalCandidates *typeutil.Map) *typeSet {\n\treturn &typeSet{\n\t\tpkg:                   pkg,\n\t\timported:              []importPkg{},\n\t\timportedByPath:        map[string]importPkg{},\n\t\timportedByName:        map[string]importPkg{},\n\t\tautomarshals:          automarshals,\n\t\tautomarshalCandidates: automarshalCandidates,\n\t}\n}\n\n// importPackage imports a package with the provided path and package name. The\n// package is imported with an alias if there is a package name clash.\nfunc (tset *typeSet) importPackage(path, pkg string) importPkg {\n\tnewImportPkg := func(path, pkg, alias string, local bool) importPkg {\n\t\ti := importPkg{path: path, pkg: pkg, alias: alias, local: local}\n\t\ttset.imported = append(tset.imported, i)\n\t\ttset.importedByPath[i.path] = i\n\t\ttset.importedByName[i.name()] = i\n\t\treturn i\n\t}\n\n\tif imp, ok := tset.importedByPath[path]; ok {\n\t\t// This package has already been imported.\n\t\treturn imp\n\t}\n\n\tif _, ok := tset.importedByName[pkg]; !ok {\n\t\t// Import the package without an alias.\n\t\treturn newImportPkg(path, pkg, \"\", path == tset.pkg.PkgPath)\n\t}\n\n\t// Find an unused alias.\n\tvar alias string\n\tcounter := 1\n\tfor {\n\t\talias = fmt.Sprintf(\"%s%d\", pkg, counter)\n\t\tif _, ok := tset.importedByName[alias]; !ok {\n\t\t\tbreak\n\t\t}\n\t\tcounter++\n\t}\n\treturn newImportPkg(path, pkg, alias, path == tset.pkg.PkgPath)\n}\n\n// imports returns the list of packages to import in generated code.\nfunc (tset *typeSet) imports() []importPkg {\n\tsort.Slice(tset.imported, func(i, j int) bool {\n\t\treturn tset.imported[i].path < tset.imported[j].path\n\t})\n\treturn tset.imported\n}\n\n// checkSerializable checks that type t is serializable.\nfunc (tset *typeSet) checkSerializable(t types.Type) []error {\n\t// lineage can generate a human readable description of the lineage of a\n\t// checked type. As check recurses on type t, it encounters a number of\n\t// nested types. For example, if we have the following type A\n\t//\n\t//     type A struct{ x []chan int }\n\t//\n\t// then check(A) will encounter the types A, struct{ x []chan int }, []chan\n\t// int, chan int, and int. We associate each of these types with a\n\t// corresponding \"path\", a concise description of the relationship between\n\t// the root type and the nested types. For example, the type chan int has\n\t// path A.x[0].\n\t//\n\t// lineage is a stack that stores a history of these paths as check\n\t// traverses a type. For example, if we call check(A), then lineage will\n\t// look like this when the chan int is discovered:\n\t//\n\t//     []pathAndType{\n\t//         pathAndType{\"A\", A},\n\t//         pathAndType{\"A.x\", []chan int},\n\t//         pathAndType{\"A.x[0]\", chan int},\n\t//     }\n\t//\n\t// This lineage is printed in error messages as:\n\t//\n\t//     A (type A)\n\t//     A.x (type []chan int)\n\t//     A.x[0] (type chan int)\n\t//\n\t// Note that for brevity, not every encountered type is entered into the\n\t// lineage.\n\ttype pathAndType struct {\n\t\tpath string\n\t\tt    types.Type\n\t}\n\tvar lineage []pathAndType\n\n\tvar errors []error\n\taddError := func(err error) {\n\t\tvar builder strings.Builder\n\n\t\t// If the lineage is trivial, then don't show it.\n\t\tif len(lineage) > 1 {\n\t\t\tfmt.Fprintf(&builder, \"\\n    \")\n\t\t\tfor i, pn := range lineage {\n\t\t\t\tfmt.Fprintf(&builder, \"%v (type %v)\", pn.path, pn.t.String())\n\t\t\t\tif i < len(lineage)-1 {\n\t\t\t\t\tfmt.Fprintf(&builder, \"\\n    \")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tqualifier := func(pkg *types.Package) string { return pkg.Name() }\n\t\terr = fmt.Errorf(\"%s: %w%s\", types.TypeString(t, qualifier), err, builder.String())\n\t\terrors = append(errors, err)\n\t}\n\n\t// stack contains the set of types encountered in the call stack of check.\n\t// It's used to detect recursive types.\n\t//\n\t// More specifically, the check function below is performing an implicit\n\t// depth first search of the graph of types formed by t. We record the\n\t// stack of visited types in stack and know we have a recursive type if we\n\t// ever run into a type that is already in stack.\n\t//\n\t// For example, consider the following types:\n\t//\n\t//   type A struct { b: *B }\n\t//   type B struct { a: *A }\n\t//\n\t// Calling check on A will yield a call stack that looks something like:\n\t//\n\t//   check(A)\n\t//     check(struct { b: *B })\n\t//       check(*B)\n\t//         check(B)\n\t//           check(struct { a: *A })\n\t//             check(*A)\n\t//               check(A)\n\t//\n\t// When performing the second check(A) call, stack includes A, struct { b:\n\t// *B }, *B, B, struct { a: *A }, and *A. Because we called check on A and\n\t// A is already in stack, we detect a recursive type and mark A as not\n\t// serializable.\n\tvar stack typeutil.Map\n\n\t// check recursively checks whether a type t is serializable. See lineage\n\t// above for a description of path. record is true if the current type\n\t// should be recorded in lineage. There are a few things worth noting:\n\t//\n\t//   (1) The results of calling check are memoized in tset.checked, but not\n\t//       for some trivial arguments. Some arguments like chan int are not\n\t//       memoized because they are trivial to check and because not\n\t//       memoizing can lead to a clearer error message.\n\t//\n\t//   (2) Consider the type t = struct { x chan int; y chan int }. t is not\n\t//       serializable because neither x nor y is serializable. check\n\t//       reports errors for both x and y as not serializable.\n\t//       Alternatively, check could find that x is not serializable and\n\t//       then immediately report that t is not serializable, skipping y\n\t//       completely. check doesn't do this. check will inspect a type fully\n\t//       to report the full set of errors.\n\t//\n\t// Note that the function also takes the parent type pt. This is needed in cases\n\t// whether we need to know the type of the parent type t (e.g., a named type\n\t// that is a proto is serializable iff the parent type is a pointer).\n\tvar check func(t types.Type, path string, record bool) bool\n\n\tcheck = func(t types.Type, path string, record bool) bool {\n\t\tif record {\n\t\t\tlineage = append(lineage, pathAndType{path, t})\n\t\t\tdefer func() { lineage = lineage[:len(lineage)-1] }()\n\t\t}\n\n\t\t// Return early if we've already checked this type.\n\t\tif result := tset.checked.At(t); result != nil {\n\t\t\tb := result.(bool)\n\t\t\tif b {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\t// We've already encountered type t and determined that it is not\n\t\t\t// serializable. We won't recurse down type t to compute the full\n\t\t\t// lineage and explanation of why t isn't serializable because we\n\t\t\t// already did that when determining t wasn't serializable in the\n\t\t\t// first place. Instead, we instruct the user to read the\n\t\t\t// previously reported error.\n\t\t\taddError(fmt.Errorf(\"not a serializable type; see above for details\"))\n\t\t\treturn false\n\t\t}\n\n\t\t// Check for recursive types.\n\t\tif stack.At(t) != nil {\n\t\t\taddError(fmt.Errorf(\"serialization of recursive types not currently supported\"))\n\t\t\ttset.checked.Set(t, false)\n\t\t\treturn false\n\t\t}\n\t\tstack.Set(t, struct{}{})\n\t\tdefer func() { stack.Delete(t) }()\n\n\t\tswitch x := t.(type) {\n\t\tcase *types.Named:\n\t\t\t// No need to check if x is an unexported type from another package\n\t\t\t// since the Go compiler takes care of that.\n\n\t\t\t// Check if the type implements one of the marshaler interfaces.\n\t\t\tif tset.isProto(x) || tset.automarshals.At(t) != nil || tset.implementsAutoMarshal(x) || tset.hasMarshalBinary(x) {\n\t\t\t\ttset.checked.Set(t, true)\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is not a struct, then we simply recurse\n\t\t\t// on the underlying type.\n\t\t\ts, ok := x.Underlying().(*types.Struct)\n\t\t\tif !ok {\n\t\t\t\ttset.checked.Set(t, check(x.Underlying(), path, false))\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is a struct that has not been declared to\n\t\t\t// implement the AutoMarshal interface, then it is not\n\t\t\t// serializable.\n\t\t\tif tset.automarshalCandidates.At(t) == nil {\n\t\t\t\t// TODO(mwhittaker): Print out a link to documentation on\n\t\t\t\t// weaver.AutoMarshal.\n\t\t\t\taddError(fmt.Errorf(\"named structs are not serializable by default. Consider using weaver.AutoMarshal.\"))\n\t\t\t\ttset.checked.Set(t, false)\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is a struct that has been declared to\n\t\t\t// implement the AutoMarshal interface but hasn't yet been checked,\n\t\t\t// then we need to recurse to detect cycles.\n\t\t\tserializable := true\n\t\t\tfor i := 0; i < s.NumFields(); i++ {\n\t\t\t\tf := s.Field(i)\n\t\t\t\t// We store the result of calling check in b rather than\n\t\t\t\t// writing serializable = serializable && check(...) because we\n\t\t\t\t// don't want to short circuit and avoid calling check.\n\t\t\t\tb := check(f.Type(), path+\".\"+f.Name(), true)\n\t\t\t\tserializable = serializable && b\n\t\t\t}\n\t\t\ttset.checked.Set(t, serializable)\n\n\t\tcase *types.Interface:\n\t\t\t// TODO(sanjay): Support types.Interface only if we can figure out\n\t\t\t// a way to instantiate the type.\n\t\t\taddError(fmt.Errorf(\"serialization of interfaces not currently supported\"))\n\t\t\ttset.checked.Set(t, false)\n\n\t\tcase *types.Struct:\n\t\t\taddError(fmt.Errorf(\"struct literals are not serializable\"))\n\t\t\ttset.checked.Set(t, false)\n\n\t\tcase *types.Basic:\n\t\t\tswitch x.Kind() {\n\t\t\tcase types.Bool,\n\t\t\t\ttypes.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\t\ttypes.Float32, types.Float64,\n\t\t\t\ttypes.Complex64, types.Complex128,\n\t\t\t\ttypes.String:\n\t\t\t\t// Supported.\n\t\t\t\ttset.checked.Set(t, true)\n\t\t\tdefault:\n\t\t\t\tif isInvalid(t) {\n\t\t\t\t\taddError(fmt.Errorf(\"Maybe you forgot to run `go mod tidy`? Also try running `go build` to diagnose further.\"))\n\t\t\t\t} else {\n\t\t\t\t\taddError(fmt.Errorf(\"unsupported basic type\"))\n\t\t\t\t}\n\t\t\t\t// For a better error message, we don't memoize this.\n\t\t\t\treturn false\n\t\t\t}\n\n\t\tcase *types.Array:\n\t\t\ttset.checked.Set(t, check(x.Elem(), path+\"[0]\", true))\n\n\t\tcase *types.Slice:\n\t\t\ttset.checked.Set(t, check(x.Elem(), path+\"[0]\", true))\n\n\t\tcase *types.Pointer:\n\t\t\ttset.checked.Set(t, check(x.Elem(), \"(*\"+path+\")\", true))\n\n\t\tcase *types.Map:\n\t\t\tkeySerializable := check(x.Key(), path+\".key\", true)\n\t\t\tvalSerializable := check(x.Elem(), path+\".value\", true)\n\t\t\ttset.checked.Set(t, keySerializable && valSerializable)\n\n\t\tdefault:\n\t\t\taddError(fmt.Errorf(\"not a serializable type\"))\n\t\t\t// For a better error message, we don't memoize this.\n\t\t\treturn false\n\t\t}\n\n\t\treturn tset.checked.At(t).(bool)\n\t}\n\n\tcheck(t, t.String(), true)\n\treturn errors\n}\n\n// isFixedSizeType returns whether the provided type has a fixed serialization\n// size. Here is a summary of which types are fixed sized:\n//\n//   - Every basic type (e.g., bool, int) except string is fixed sized.\n//   - The array type [N]t is fixed sized if t is fixed sized.\n//   - A struct is fixed sized if the types of its fields are fixed sized.\n//   - A named type is fixed sized if its underlying type is fixed sized.\nfunc (tset *typeSet) isFixedSizeType(t types.Type) bool {\n\treturn tset.sizeOfType(t) >= 0\n}\n\n// sizeOfType returns the size of the serialization of t if t is fixed size, or\n// returns -1 otherwise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// isMeasurable returns whether the provided type is measurable.\n//\n// Informally, we say a type is measurable if we can cheaply compute the size\n// of its serialization at runtime. Some examples:\n//\n//   - Every fixed size type (e.g., int, bool, [3]int, struct{x, y int}) is\n//     measurable (with some restrictions on package locality; see below).\n//   - Strings are not fixed size, but they are measurable because we can\n//     cheaply compute the length of a string at runtime.\n//   - []string is not measurable because computing the size of the\n//     serialization of a []string would require us to compute the length of\n//     every string in the slice. This is a potentially expensive operation\n//     if the slice contains a large number of strings, so we consider\n//     []string to be not measurable.\n//   - For simplicity, we only consider a type measurable if the type and all\n//     its nested types are package local. For example, a struct { x\n//     otherpackage.T } is not measurable, even if otherpackage.T is\n//     measurable. We make an exception for weaver.AutoMarshal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// genTypeString returns the string representation of t as to be printed\n// in the generated code, updating import definitions to account for the\n// returned type string.\n//\n// Since this call has side-effects (i.e., updating import definitions), it\n// should only be called when the returned type string is written into\n// the generated file; otherwise, the generated code may end up with spurious\n// imports.\nfunc (tset *typeSet) genTypeString(t types.Type) string {\n\t// qualifier is passed to types.TypeString(Type, Qualifier) to determine\n\t// how packages are printed when pretty printing types. For this qualifier,\n\t// types in the root package are printed without their package name, while\n\t// types outside the root package are printed with their package name. For\n\t// example, if we're in root package foo, then the type foo.Bar is printed\n\t// as Bar, while the type io.Reader is printed as io.Reader. See [1] for\n\t// more information on qualifiers and pretty printing types.\n\t//\n\t// [1]: https://github.com/golang/example/tree/master/gotypes#formatting-support\n\tvar qualifier = func(pkg *types.Package) string {\n\t\tif pkg == tset.pkg.Types {\n\t\t\treturn \"\"\n\t\t}\n\t\treturn tset.importPackage(pkg.Path(), pkg.Name()).name()\n\t}\n\treturn types.TypeString(t, qualifier)\n}\n\n// isInvalid returns true iff the given type is invalid.\nfunc isInvalid(t types.Type) bool {\n\treturn t.String() == \"invalid type\"\n}\n\n// implementsError returns whether the provided type is a concrete type that\n// implements error.\nfunc (tset *typeSet) implementsError(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\treturn false\n\t}\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"Error\")\n\tmethod, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tsig, ok := method.Type().(*types.Signature)\n\tif !ok {\n\t\treturn false\n\t}\n\tif args := sig.Params(); args.Len() != 0 {\n\t\treturn false\n\t}\n\tif results := sig.Results(); results.Len() != 1 || !isString(results.At(0).Type()) {\n\t\treturn false\n\t}\n\treturn true\n}\n\n// isProto returns whether the provided type is a concrete type that implements\n// the proto.Message interface.\nfunc (tset *typeSet) isProto(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\t// A superinterface of proto.Message does \"implement\" proto.Message,\n\t\t// but we only accept concrete types that implement the interface.\n\t\treturn false\n\t}\n\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"ProtoReflect\")\n\tmethod, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tsig, ok := method.Type().(*types.Signature)\n\tif !ok {\n\t\treturn false\n\t}\n\trecv, args, results := sig.Recv(), sig.Params(), sig.Results()\n\tif args.Len() != 0 || results.Len() != 1 {\n\t\treturn false\n\t}\n\tif !isProtoMessage(results.At(0).Type()) {\n\t\treturn false\n\t}\n\t// Check the receiver. We avoid complicated embeddings by requiring that\n\t// the method is defined on the type itself.\n\t//\n\t// TODO(mwhittaker): Relax this requirement if it becomes annoying.\n\tif p, ok := recv.Type().(*types.Pointer); ok {\n\t\treturn types.Identical(p.Elem(), t)\n\t} else {\n\t\treturn types.Identical(recv.Type(), t)\n\t}\n}\n\nfunc isProtoMessage(t types.Type) bool {\n\tn, ok := t.(*types.Named)\n\tif !ok {\n\t\treturn false\n\t}\n\tconst protoreflect = \"google.golang.org/protobuf/reflect/protoreflect\"\n\treturn n.Obj().Pkg().Path() == protoreflect && n.Obj().Name() == \"Message\"\n}\n\n// implementsAutoMarshal returns whether the provided type is a concrete\n// type that implements the weaver.AutoMarshal interface.\nfunc (tset *typeSet) implementsAutoMarshal(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\t// A superinterface of AutoMarshal does \"implement\" the interface,\n\t\t// but we only accept concrete types that implement the interface.\n\t\treturn false\n\t}\n\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"WeaverMarshal\")\n\tmarshal, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tobj, _, _ = types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"WeaverUnmarshal\")\n\tunmarshal, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn is\n```\n# Function Signature: func (tset *typeSet) sizeOfType(t types.Type) int {\n# Natural Language Annotations: // sizeOfType returns the size of the serialization of t if t is fixed size, or\n// returns -1 otherwise.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (tset *typeSet) sizeOfType(t types.Type) int {\n\t// shortcircuit if t has already been cached.\n\tif s, ok := tset.sizes.At(t).(int); ok {\n\t\treturn s\n\t}\n\n\tswitch x := t.(type) {\n\tcase *types.Basic:\n\t\tswitch x.Kind() {\n\t\tcase types.Bool:\n\t\t\treturn 1\n\t\tcase types.Int:\n\t\t\treturn 8\n\t\tcase types.Int8:\n\t\t\treturn 1\n\t\tcase types.Int16:\n\t\t\treturn 2\n\t\tcase types.Int32:\n\t\t\treturn 4\n\t\tcase types.Int64:\n\t\t\treturn 8\n\t\tcase types.Uint:\n\t\t\treturn 8\n\t\tcase types.Uint8:\n\t\t\treturn 1\n\t\tcase types.Uint16:\n\t\t\treturn 2\n\t\tcase types.Uint32:\n\t\t\treturn 4\n\t\tcase types.Uint64:\n\t\t\treturn 8\n\t\tcase types.Float32:\n\t\t\treturn 4\n\t\tcase types.Float64:\n\t\t\treturn 8\n\t\tcase types.Complex64:\n\t\t\treturn 8\n\t\tcase types.Complex128:\n\t\t\treturn 16\n\t\tcase types.String:\n\t\t\treturn -1\n\t\tcase types.UnsafePointer:\n\t\t\treturn -1\n\t\tcase types.UntypedBool:\n\t\t\treturn -1\n\t\tcase types.UntypedInt:\n\t\t\treturn -1\n\t\tcase types.UntypedRune:\n\t\t\treturn -1\n\t\tcase types.UntypedFloat:\n\t\t\treturn -1\n\t\tcase types.UntypedComplex:\n\t\t\treturn -1\n\t\tcase types.UntypedString:\n\t\t\treturn -1\n\t\tcase types.UntypedNil:\n\t\t\treturn -1\n\t\tcase types.Invalid:\n\t\t\treturn -1\n\t\tdefault:\n\t\t\tpanic(fmt.Sprintf(\"unexpected kind %v\", x.Kind()))\n\t\t}\n\n\tcase *types.Array:\n\t\tn := tset.sizeOfType(x.Elem())\n\t\tif n < 0 {\n\t\t\treturn -1\n\t\t}\n\t\treturn n * x.Len()\n\n\tcase *types.Slice:\n\t\tn := tset.sizeOfType(x.Elem())\n\t\tif n < 0 {\n\t\t\treturn -1\n\t\t}\n\t\treturn -1\n\n\tcase *types.Pointer:\n\t\tn := tset.sizeOfType(x.Elem())\n\t\tif n < 0 {\n\t\t\treturn -1\n\t\t}\n\t\treturn n\n\n\tcase *types.Struct:\n\t\tvar total int\n\t\tfor i := 0; i < x.NumFields(); i++ {\n\t\t\tf := x.Field(i)\n\t\t\tn := tset.sizeOfType(f.Type())\n\t\t\tif n < 0 {\n\t\t\t\treturn -1\n\t\t\t}\n\t\t\ttotal += n\n\t\t}\n\t\treturn total\n\n\tcase *types.Interface:\n\t\treturn -1\n\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"unexpected type %T\", t))\n\t}\n}\n```", "  ```\nfunc (tset *typeSet) sizeOfType(t types.Type) int {\n\tif tset.measurable.At(t) != nil {\n\t\t// t is measurable.\n\t\treturn tset.measurable.At(t).(int)\n\t}\n\n\tvar size int\n\tswitch t.(type) {\n\tcase *types.Basic:\n\t\tx := t.\n\t\tswitch x.Kind() {\n\t\tcase types.Bool,\n\t\t\ttypes.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\ttypes.Float32, types.Float64,\n\t\t\ttypes.Complex64, types.Complex128,\n\t\t\ttypes.String:\n\t\t\t// Sizes of builtin types\n\t\t\tsize = 0\n\t\tdefault:\n\t\t\tsize = -1\n\t\t}\n\tcase *types.Struct:\n\t\t// TODO(#737): Restrict to fixed structs once we implement field-field\n\t\t// padding.\n\t\tsize = 0\n\t\tfor i := 0; i < t.NumFields(); i++ {\n\t\t\tsizeOfType := tset.sizeOfType(t.Field(i).Type())\n\t\t\tif sizeOfType < 0 {\n\t\t\t\treturn -1\n\t\t\t}\n\t\t\tsize += sizeOfType\n\t\t}\n\tcase *types.Named:\n\t\tif tset.isProto(t) {\n\t\t\tsize = 0\n\t\t} else {\n\t\t\treturn tset.sizeOfType(t.Underlying())\n\t\t}\n\tcase *types.Map:\n\t\t// TODO(#739): Restrict to fixed map offsets once we implement data type\n\t\t// serialization.\n\t\tsize = 8\n\tcase *types.Array:\n\t\t// TODO(#739): Restrict to fixed array sizes once we implement data type\n\t\t// serialization.\n\t\tsize = tset.sizeOfType(t.Elem()) * t.Len()\n\tcase *types.Slice:\n\t\t// TODO(#739): Restrict to fixed array sizes once we implement data type\n\t\t// serialization.\n\t\tsize = 8\n\tcase *types.Pointer:\n\t\tswitch t.Elem().Underlying().(type) {\n\t\tcase *types.Basic:\n\t\t\t// TODO(#739): Restrict to fixed pointer offsets once we implement\n\t\t\t// data type serialization.\n\t\t\tsize = 8\n\t\tdefault:\n\t\t\tsize = -1\n\t\t}\n\tdefault:\n\t\tsize = -1\n\t}\n\tif size >= 0 {\n\t\ttset.measurable.Set(t, size)\n\t}\n\treturn size\n}\n```", "  ```\nfunc (tset *typeSet) sizeOfType(t types.Type) int {\n\t// If the type has already been computed, return the cached result.\n\tif result := tset.sizes.At(t); result != nil {\n\t\treturn result.(int)\n\t}\n\n\tif tset.measurable.At(t) == nil {\n\t\treturn -1\n\t}\n\n\t// Check the size of the type recursively and memoize the result.\n\tswitch x := t.(type) {\n\tcase *types.Basic:\n\t\treturn sizeOfBasicType(x)\n\tcase *types.Named:\n\t\treturn sizeOfNamedType(x)\n\tcase *types.Struct:\n\t\treturn sizeOfStruct(x)\n\tcase *types.Array:\n\t\treturn sizeOfArray(x)\n\tcase *types.Slice:\n\t\treturn sizeOfSlice(x)\n\tcase *types.Pointer:\n\t\treturn sizeOfPointer(x)\n\tcase *types.Map:\n\t\treturn sizeOfMap(x)\n\tdefault:\n\t\treturn -1\n\t}\n}\n\nfunc sizeOfBasicType(b *types.Basic) int {\n\tswitch b.Kind() {\n\tcase types.Uint8:\n\t\treturn 1\n\tcase types.Uint16:\n\t\treturn 2\n\tcase types.Uint32:\n\t\treturn 4\n\tcase types.Uint64:\n\t\treturn 8\n\tcase types.Int8:\n\t\treturn 1\n\tcase types.Int16:\n\t\treturn 2\n\tcase types.Int32:\n\t\treturn 4\n\tcase types.Int64:\n\t\treturn 8\n\tcase types.Float32:\n\t\treturn 4\n\tcase types.Float64:\n\t\treturn 8\n\tcase types.String:\n\t\t// Size of a string is size of a pointer + size of data area.\n\t\t// For certain architectures (e.g. 32-bit machines), larger strings\n\t\t// are stored in two words (32 or 64 bits, respectively), whereas\n\t\t// for other (e.g. 64 bit machines), larger strings are stored\n\t\t// in three words.\n\t\treturn 16\n\tdefault:\n\t\treturn -1\n\t}\n}\n\nfunc sizeOfNamedType(x *types.Named) int {\n\tif x.Type() == nil {\n\t\treturn -1\n\t}\n\tu := x.Underlying()\n\tif _, ok := u.(*types.Basic); ok {\n\t\treturn sizeOfBasicType(u.(*types.Basic))\n\t} else if _, ok := u.(*types.Pointer); ok {\n\t\treturn sizeOfPointer(u.(*types.Pointer))\n\t} else {\n\t\treturn sizeOfType(u)\n\t}\n}\n\nfunc sizeOfStruct(x *types.Struct) int {\n\tsize := 0\n\tfor i := 0; i < x.NumFields(); i++ {\n\t\tfield := x.Field(i)\n\t\tfv := field.Type()\n\t\tif _, ok := fv.(*types.Basic); ok {\n\t\t\treturn sizeOfBasicType(fv.(*types.Basic))\n\t\t} else if _, ok := fv.(*types.Pointer); ok {\n\t\t\treturn sizeOfPointer(fv.(*types.Pointer))\n\t\t} else {\n\t\t\tsz := sizOfType(fv)\n\t\t\tif sz < 0 {\n\t\t\t\treturn -1\n\t\t\t}\n\t\t\tsize += sz\n\t\t}\n\t}\n\treturn size\n}\n\nfunc sizeOfArray(x *types.Array) int {\n\treturn sizeOfType(x.Elem()) * x.Len()\n}\n\nfunc sizeOfSlice(x *types.Slice) int {\n\treturn -1\n}\n\nfunc sizeOfPointer(x *types.Pointer) int {\n\treturn -1\n}\n\nfunc sizeOfMap(x *types.Map) int {\n\treturn -1\n}\n```"]}, "weaver-internal/tool/generate/types.go-isMeasurable": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/tool/generate/types.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generate\n\nimport (\n\t\"fmt\"\n\t\"go/types\"\n\t\"path\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"golang.org/x/tools/go/packages\"\n\t\"golang.org/x/tools/go/types/typeutil\"\n)\n\nconst weaverPackagePath = \"github.com/ServiceWeaver/weaver\"\n\n// typeSet holds type information needed by the code generator.\ntype typeSet struct {\n\tpkg            *packages.Package\n\timported       []importPkg          // imported packages\n\timportedByPath map[string]importPkg // imported, indexed by path\n\timportedByName map[string]importPkg // imported, indexed by name\n\n\tautomarshals          *typeutil.Map // types that implement AutoMarshal\n\tautomarshalCandidates *typeutil.Map // types that declare themselves AutoMarshal\n\n\t// If checked[t] != nil, then checked[t] is the cached result of calling\n\t// check(pkg, t, string[]{}). Otherwise, if checked[t] == nil, then t has\n\t// not yet been checked for serializability. Read typeutil.Map's\n\t// documentation for why checked shouldn't be a map[types.Type]bool.\n\tchecked typeutil.Map\n\n\t// If sizes[t] != nil, then sizes[t] == sizeOfType(t).\n\tsizes typeutil.Map\n\n\t// If measurable[t] != nil, then measurable[t] == isMeasurableType(t).\n\tmeasurable typeutil.Map\n}\n\n// importPkg is a package imported by the generated code.\ntype importPkg struct {\n\tpath  string // e.g., \"github.com/ServiceWeaver/weaver\"\n\tpkg   string // e.g., \"weaver\", \"context\", \"time\"\n\talias string // e.g., foo in `import foo \"context\"`\n\tlocal bool   // are we in this package?\n}\n\n// name returns the name by which the imported package should be referenced in\n// the generated code. If the package is imported without an alias, like this:\n//\n//\timport \"context\"\n//\n// then the name is the same as the package name (e.g., \"context\"). However, if\n// a package is imported with an alias, then the name is the alias:\n//\n//\timport thisIsAnAlias \"context\"\n//\n// If the package is local, an empty string is returned.\nfunc (i importPkg) name() string {\n\tif i.local {\n\t\treturn \"\"\n\t} else if i.alias != \"\" {\n\t\treturn i.alias\n\t}\n\treturn i.pkg\n}\n\n// qualify returns the provided member of the package, qualified with the\n// package name. For example, the \"Context\" type inside the \"context\" package\n// is qualified \"context.Context\". The \"Now\" function inside the \"time\" package\n// is qualified \"time.Now\". Note that the package name is not prefixed when\n// qualifying members of the local package.\nfunc (i importPkg) qualify(member string) string {\n\tif i.local {\n\t\treturn member\n\t}\n\treturn fmt.Sprintf(\"%s.%s\", i.name(), member)\n}\n\n// newTypeSet returns the container for types found in pkg.\nfunc newTypeSet(pkg *packages.Package, automarshals, automarshalCandidates *typeutil.Map) *typeSet {\n\treturn &typeSet{\n\t\tpkg:                   pkg,\n\t\timported:              []importPkg{},\n\t\timportedByPath:        map[string]importPkg{},\n\t\timportedByName:        map[string]importPkg{},\n\t\tautomarshals:          automarshals,\n\t\tautomarshalCandidates: automarshalCandidates,\n\t}\n}\n\n// importPackage imports a package with the provided path and package name. The\n// package is imported with an alias if there is a package name clash.\nfunc (tset *typeSet) importPackage(path, pkg string) importPkg {\n\tnewImportPkg := func(path, pkg, alias string, local bool) importPkg {\n\t\ti := importPkg{path: path, pkg: pkg, alias: alias, local: local}\n\t\ttset.imported = append(tset.imported, i)\n\t\ttset.importedByPath[i.path] = i\n\t\ttset.importedByName[i.name()] = i\n\t\treturn i\n\t}\n\n\tif imp, ok := tset.importedByPath[path]; ok {\n\t\t// This package has already been imported.\n\t\treturn imp\n\t}\n\n\tif _, ok := tset.importedByName[pkg]; !ok {\n\t\t// Import the package without an alias.\n\t\treturn newImportPkg(path, pkg, \"\", path == tset.pkg.PkgPath)\n\t}\n\n\t// Find an unused alias.\n\tvar alias string\n\tcounter := 1\n\tfor {\n\t\talias = fmt.Sprintf(\"%s%d\", pkg, counter)\n\t\tif _, ok := tset.importedByName[alias]; !ok {\n\t\t\tbreak\n\t\t}\n\t\tcounter++\n\t}\n\treturn newImportPkg(path, pkg, alias, path == tset.pkg.PkgPath)\n}\n\n// imports returns the list of packages to import in generated code.\nfunc (tset *typeSet) imports() []importPkg {\n\tsort.Slice(tset.imported, func(i, j int) bool {\n\t\treturn tset.imported[i].path < tset.imported[j].path\n\t})\n\treturn tset.imported\n}\n\n// checkSerializable checks that type t is serializable.\nfunc (tset *typeSet) checkSerializable(t types.Type) []error {\n\t// lineage can generate a human readable description of the lineage of a\n\t// checked type. As check recurses on type t, it encounters a number of\n\t// nested types. For example, if we have the following type A\n\t//\n\t//     type A struct{ x []chan int }\n\t//\n\t// then check(A) will encounter the types A, struct{ x []chan int }, []chan\n\t// int, chan int, and int. We associate each of these types with a\n\t// corresponding \"path\", a concise description of the relationship between\n\t// the root type and the nested types. For example, the type chan int has\n\t// path A.x[0].\n\t//\n\t// lineage is a stack that stores a history of these paths as check\n\t// traverses a type. For example, if we call check(A), then lineage will\n\t// look like this when the chan int is discovered:\n\t//\n\t//     []pathAndType{\n\t//         pathAndType{\"A\", A},\n\t//         pathAndType{\"A.x\", []chan int},\n\t//         pathAndType{\"A.x[0]\", chan int},\n\t//     }\n\t//\n\t// This lineage is printed in error messages as:\n\t//\n\t//     A (type A)\n\t//     A.x (type []chan int)\n\t//     A.x[0] (type chan int)\n\t//\n\t// Note that for brevity, not every encountered type is entered into the\n\t// lineage.\n\ttype pathAndType struct {\n\t\tpath string\n\t\tt    types.Type\n\t}\n\tvar lineage []pathAndType\n\n\tvar errors []error\n\taddError := func(err error) {\n\t\tvar builder strings.Builder\n\n\t\t// If the lineage is trivial, then don't show it.\n\t\tif len(lineage) > 1 {\n\t\t\tfmt.Fprintf(&builder, \"\\n    \")\n\t\t\tfor i, pn := range lineage {\n\t\t\t\tfmt.Fprintf(&builder, \"%v (type %v)\", pn.path, pn.t.String())\n\t\t\t\tif i < len(lineage)-1 {\n\t\t\t\t\tfmt.Fprintf(&builder, \"\\n    \")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tqualifier := func(pkg *types.Package) string { return pkg.Name() }\n\t\terr = fmt.Errorf(\"%s: %w%s\", types.TypeString(t, qualifier), err, builder.String())\n\t\terrors = append(errors, err)\n\t}\n\n\t// stack contains the set of types encountered in the call stack of check.\n\t// It's used to detect recursive types.\n\t//\n\t// More specifically, the check function below is performing an implicit\n\t// depth first search of the graph of types formed by t. We record the\n\t// stack of visited types in stack and know we have a recursive type if we\n\t// ever run into a type that is already in stack.\n\t//\n\t// For example, consider the following types:\n\t//\n\t//   type A struct { b: *B }\n\t//   type B struct { a: *A }\n\t//\n\t// Calling check on A will yield a call stack that looks something like:\n\t//\n\t//   check(A)\n\t//     check(struct { b: *B })\n\t//       check(*B)\n\t//         check(B)\n\t//           check(struct { a: *A })\n\t//             check(*A)\n\t//               check(A)\n\t//\n\t// When performing the second check(A) call, stack includes A, struct { b:\n\t// *B }, *B, B, struct { a: *A }, and *A. Because we called check on A and\n\t// A is already in stack, we detect a recursive type and mark A as not\n\t// serializable.\n\tvar stack typeutil.Map\n\n\t// check recursively checks whether a type t is serializable. See lineage\n\t// above for a description of path. record is true if the current type\n\t// should be recorded in lineage. There are a few things worth noting:\n\t//\n\t//   (1) The results of calling check are memoized in tset.checked, but not\n\t//       for some trivial arguments. Some arguments like chan int are not\n\t//       memoized because they are trivial to check and because not\n\t//       memoizing can lead to a clearer error message.\n\t//\n\t//   (2) Consider the type t = struct { x chan int; y chan int }. t is not\n\t//       serializable because neither x nor y is serializable. check\n\t//       reports errors for both x and y as not serializable.\n\t//       Alternatively, check could find that x is not serializable and\n\t//       then immediately report that t is not serializable, skipping y\n\t//       completely. check doesn't do this. check will inspect a type fully\n\t//       to report the full set of errors.\n\t//\n\t// Note that the function also takes the parent type pt. This is needed in cases\n\t// whether we need to know the type of the parent type t (e.g., a named type\n\t// that is a proto is serializable iff the parent type is a pointer).\n\tvar check func(t types.Type, path string, record bool) bool\n\n\tcheck = func(t types.Type, path string, record bool) bool {\n\t\tif record {\n\t\t\tlineage = append(lineage, pathAndType{path, t})\n\t\t\tdefer func() { lineage = lineage[:len(lineage)-1] }()\n\t\t}\n\n\t\t// Return early if we've already checked this type.\n\t\tif result := tset.checked.At(t); result != nil {\n\t\t\tb := result.(bool)\n\t\t\tif b {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\t// We've already encountered type t and determined that it is not\n\t\t\t// serializable. We won't recurse down type t to compute the full\n\t\t\t// lineage and explanation of why t isn't serializable because we\n\t\t\t// already did that when determining t wasn't serializable in the\n\t\t\t// first place. Instead, we instruct the user to read the\n\t\t\t// previously reported error.\n\t\t\taddError(fmt.Errorf(\"not a serializable type; see above for details\"))\n\t\t\treturn false\n\t\t}\n\n\t\t// Check for recursive types.\n\t\tif stack.At(t) != nil {\n\t\t\taddError(fmt.Errorf(\"serialization of recursive types not currently supported\"))\n\t\t\ttset.checked.Set(t, false)\n\t\t\treturn false\n\t\t}\n\t\tstack.Set(t, struct{}{})\n\t\tdefer func() { stack.Delete(t) }()\n\n\t\tswitch x := t.(type) {\n\t\tcase *types.Named:\n\t\t\t// No need to check if x is an unexported type from another package\n\t\t\t// since the Go compiler takes care of that.\n\n\t\t\t// Check if the type implements one of the marshaler interfaces.\n\t\t\tif tset.isProto(x) || tset.automarshals.At(t) != nil || tset.implementsAutoMarshal(x) || tset.hasMarshalBinary(x) {\n\t\t\t\ttset.checked.Set(t, true)\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is not a struct, then we simply recurse\n\t\t\t// on the underlying type.\n\t\t\ts, ok := x.Underlying().(*types.Struct)\n\t\t\tif !ok {\n\t\t\t\ttset.checked.Set(t, check(x.Underlying(), path, false))\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is a struct that has not been declared to\n\t\t\t// implement the AutoMarshal interface, then it is not\n\t\t\t// serializable.\n\t\t\tif tset.automarshalCandidates.At(t) == nil {\n\t\t\t\t// TODO(mwhittaker): Print out a link to documentation on\n\t\t\t\t// weaver.AutoMarshal.\n\t\t\t\taddError(fmt.Errorf(\"named structs are not serializable by default. Consider using weaver.AutoMarshal.\"))\n\t\t\t\ttset.checked.Set(t, false)\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is a struct that has been declared to\n\t\t\t// implement the AutoMarshal interface but hasn't yet been checked,\n\t\t\t// then we need to recurse to detect cycles.\n\t\t\tserializable := true\n\t\t\tfor i := 0; i < s.NumFields(); i++ {\n\t\t\t\tf := s.Field(i)\n\t\t\t\t// We store the result of calling check in b rather than\n\t\t\t\t// writing serializable = serializable && check(...) because we\n\t\t\t\t// don't want to short circuit and avoid calling check.\n\t\t\t\tb := check(f.Type(), path+\".\"+f.Name(), true)\n\t\t\t\tserializable = serializable && b\n\t\t\t}\n\t\t\ttset.checked.Set(t, serializable)\n\n\t\tcase *types.Interface:\n\t\t\t// TODO(sanjay): Support types.Interface only if we can figure out\n\t\t\t// a way to instantiate the type.\n\t\t\taddError(fmt.Errorf(\"serialization of interfaces not currently supported\"))\n\t\t\ttset.checked.Set(t, false)\n\n\t\tcase *types.Struct:\n\t\t\taddError(fmt.Errorf(\"struct literals are not serializable\"))\n\t\t\ttset.checked.Set(t, false)\n\n\t\tcase *types.Basic:\n\t\t\tswitch x.Kind() {\n\t\t\tcase types.Bool,\n\t\t\t\ttypes.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\t\ttypes.Float32, types.Float64,\n\t\t\t\ttypes.Complex64, types.Complex128,\n\t\t\t\ttypes.String:\n\t\t\t\t// Supported.\n\t\t\t\ttset.checked.Set(t, true)\n\t\t\tdefault:\n\t\t\t\tif isInvalid(t) {\n\t\t\t\t\taddError(fmt.Errorf(\"Maybe you forgot to run `go mod tidy`? Also try running `go build` to diagnose further.\"))\n\t\t\t\t} else {\n\t\t\t\t\taddError(fmt.Errorf(\"unsupported basic type\"))\n\t\t\t\t}\n\t\t\t\t// For a better error message, we don't memoize this.\n\t\t\t\treturn false\n\t\t\t}\n\n\t\tcase *types.Array:\n\t\t\ttset.checked.Set(t, check(x.Elem(), path+\"[0]\", true))\n\n\t\tcase *types.Slice:\n\t\t\ttset.checked.Set(t, check(x.Elem(), path+\"[0]\", true))\n\n\t\tcase *types.Pointer:\n\t\t\ttset.checked.Set(t, check(x.Elem(), \"(*\"+path+\")\", true))\n\n\t\tcase *types.Map:\n\t\t\tkeySerializable := check(x.Key(), path+\".key\", true)\n\t\t\tvalSerializable := check(x.Elem(), path+\".value\", true)\n\t\t\ttset.checked.Set(t, keySerializable && valSerializable)\n\n\t\tdefault:\n\t\t\taddError(fmt.Errorf(\"not a serializable type\"))\n\t\t\t// For a better error message, we don't memoize this.\n\t\t\treturn false\n\t\t}\n\n\t\treturn tset.checked.At(t).(bool)\n\t}\n\n\tcheck(t, t.String(), true)\n\treturn errors\n}\n\n// isFixedSizeType returns whether the provided type has a fixed serialization\n// size. Here is a summary of which types are fixed sized:\n//\n//   - Every basic type (e.g., bool, int) except string is fixed sized.\n//   - The array type [N]t is fixed sized if t is fixed sized.\n//   - A struct is fixed sized if the types of its fields are fixed sized.\n//   - A named type is fixed sized if its underlying type is fixed sized.\nfunc (tset *typeSet) isFixedSizeType(t types.Type) bool {\n\treturn tset.sizeOfType(t) >= 0\n}\n\n// sizeOfType returns the size of the serialization of t if t is fixed size, or\n// returns -1 otherwise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// isMeasurable returns whether the provided type is measurable.\n//\n// Informally, we say a type is measurable if we can cheaply compute the size\n// of its serialization at runtime. Some examples:\n//\n//   - Every fixed size type (e.g., int, bool, [3]int, struct{x, y int}) is\n//     measurable (with some restrictions on package locality; see below).\n//   - Strings are not fixed size, but they are measurable because we can\n//     cheaply compute the length of a string at runtime.\n//   - []string is not measurable because computing the size of the\n//     serialization of a []string would require us to compute the length of\n//     every string in the slice. This is a potentially expensive operation\n//     if the slice contains a large number of strings, so we consider\n//     []string to be not measurable.\n//   - For simplicity, we only consider a type measurable if the type and all\n//     its nested types are package local. For example, a struct { x\n//     otherpackage.T } is not measurable, even if otherpackage.T is\n//     measurable. We make an exception for weaver.AutoMarshal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// genTypeString returns the string representation of t as to be printed\n// in the generated code, updating import definitions to account for the\n// returned type string.\n//\n// Since this call has side-effects (i.e., updating import definitions), it\n// should only be called when the returned type string is written into\n// the generated file; otherwise, the generated code may end up with spurious\n// imports.\nfunc (tset *typeSet) genTypeString(t types.Type) string {\n\t// qualifier is passed to types.TypeString(Type, Qualifier) to determine\n\t// how packages are printed when pretty printing types. For this qualifier,\n\t// types in the root package are printed without their package name, while\n\t// types outside the root package are printed with their package name. For\n\t// example, if we're in root package foo, then the type foo.Bar is printed\n\t// as Bar, while the type io.Reader is printed as io.Reader. See [1] for\n\t// more information on qualifiers and pretty printing types.\n\t//\n\t// [1]: https://github.com/golang/example/tree/master/gotypes#formatting-support\n\tvar qualifier = func(pkg *types.Package) string {\n\t\tif pkg == tset.pkg.Types {\n\t\t\treturn \"\"\n\t\t}\n\t\treturn tset.importPackage(pkg.Path(), pkg.Name()).name()\n\t}\n\treturn types.TypeString(t, qualifier)\n}\n\n// isInvalid returns true iff the given type is invalid.\nfunc isInvalid(t types.Type) bool {\n\treturn t.String() == \"invalid type\"\n}\n\n// implementsError returns whether the provided type is a concrete type that\n// implements error.\nfunc (tset *typeSet) implementsError(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\treturn false\n\t}\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"Error\")\n\tmethod, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tsig, ok := method.Type().(*types.Signature)\n\tif !ok {\n\t\treturn false\n\t}\n\tif args := sig.Params(); args.Len() != 0 {\n\t\treturn false\n\t}\n\tif results := sig.Results(); results.Len() != 1 || !isString(results.At(0).Type()) {\n\t\treturn false\n\t}\n\treturn true\n}\n\n// isProto returns whether the provided type is a concrete type that implements\n// the proto.Message interface.\nfunc (tset *typeSet) isProto(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\t// A superinterface of proto.Message does \"implement\" proto.Message,\n\t\t// but we only accept concrete types that implement the interface.\n\t\treturn false\n\t}\n\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"ProtoReflect\")\n\tmethod, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tsig, ok := method.Type().(*types.Signature)\n\tif !ok {\n\t\treturn false\n\t}\n\trecv, args, results := sig.Recv(), sig.Params(), sig.Results()\n\tif args.Len() != 0 || results.Len() != 1 {\n\t\treturn false\n\t}\n\tif !isProtoMessage(results.At(0).Type()) {\n\t\treturn false\n\t}\n\t// Check the receiver. We avoid complicated embeddings by requiring that\n\t// the method is defined on the type itself.\n\t//\n\t// TODO(mwhittaker): Relax this requirement if it becomes annoying.\n\tif p, ok := recv.Type().(*types.Pointer); ok {\n\t\treturn types.Identical(p.Elem(), t)\n\t} else {\n\t\treturn types.Identical(recv.Type(), t)\n\t}\n}\n\nfunc isProtoMessage(t types.Type) bool {\n\tn, ok := t.(*types.Named)\n\tif !ok {\n\t\treturn false\n\t}\n\tconst protoreflect = \"google.golang.org/protobuf/reflect/protoreflect\"\n\treturn n.Obj().Pkg().Path() == protoreflect && n.Obj().Name() == \"Message\"\n}\n\n// implementsAutoMarshal returns whether the provided type is a concrete\n// type that implements the weaver.AutoMarshal interface.\nfunc (tset *typeSet) implementsAutoMarshal(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\t// A superinterface of AutoMarshal does \"implement\" the interface,\n\t\t// but we only accept concrete types that implement the interface.\n\t\treturn false\n\t}\n\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"WeaverMarshal\")\n\tmarshal, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tobj, _, _ = types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"WeaverUnmarshal\")\n\tunmarshal, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn is\n```\n# Function Signature: func (tset *typeSet) isMeasurable(t types.Type) bool {\n# Natural Language Annotations: // isMeasurable returns whether the provided type is measurable.\n//\n// Informally, we say a type is measurable if we can cheaply compute the size\n// of its serialization at runtime. Some examples:\n//\n//   - Every fixed size type (e.g., int, bool, [3]int, struct{x, y int}) is\n//     measurable (with some restrictions on package locality; see below).\n//   - Strings are not fixed size, but they are measurable because we can\n//     cheaply compute the length of a string at runtime.\n//   - []string is not measurable because computing the size of the\n//     serialization of a []string would require us to compute the length of\n//     every string in the slice. This is a potentially expensive operation\n//     if the slice contains a large number of strings, so we consider\n//     []string to be not measurable.\n//   - For simplicity, we only consider a type measurable if the type and all\n//     its nested types are package local. For example, a struct { x\n//     otherpackage.T } is not measurable, even if otherpackage.T is\n//     measurable. We make an exception for weaver.AutoMarshal.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\n// isMeasurable returns whether the provided type is measurable.\n//\n// Informally, we say a type is measurable if we can cheaply compute the size\n// of its serialization at runtime. Some examples:\n//\n//   - Every fixed size type (e.g., int, bool, [3]int, struct{x, y int}) is\n//     measurable (with some restrictions on package locality; see below).\n//   - Strings are not fixed size, but they are measurable because we can\n//     cheaply compute the length of a string at runtime.\n//   - []string is not measurable because computing the size of the\n//     serialization of a []string would require us to compute the length of\n//     every string in the slice. This is a potentially expensive operation\n//     if the slice contains a large number of strings, so we consider\n//     []string to be not measurable.\n//   - For simplicity, we only consider a type measurable if the type and all\n//     its nested types are package local. For example, a struct { x\n//     otherpackage.T } is not measurable, even if otherpackage.T is\n//     measurable. We make an exception for weaver.AutoMarshal.\nfunc (tset *typeSet) isMeasurable(t types.Type) bool {\n\tswitch x := t.(type) {\n\tcase *types.Basic:\n\t\tswitch x.Kind() {\n\t\tcase types.Bool,\n\t\t\ttypes.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\ttypes.Float32, types.Float64,\n\t\t\ttypes.Complex64, types.Complex128,\n\t\t\ttypes.String:\n\t\t\treturn true\n\t\t}\n\tcase *types.Array:\n\t\treturn tset.isFixedSizeType(t) && tset.isMeasurable(x.Elem())\n\tcase *types.Slice:\n\t\treturn tset.isFixedSizeType(t) && tset.isMeasurable(x.Elem())\n\tcase *types.Struct:\n\t\tfor i := 0; i < x.NumFields(); i++ {\n\t\t\tif !tset.isMeasurable(x.Field(i).Type()) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\treturn true\n\tcase *types.Named:\n\t\treturn tset.isFixedSizeType(x.Underlying()) && tset.isMeasurable(x.Underlying())\n\tcase *types.Interface:\n\t\treturn false\n\t}\n\treturn false\n}\n```", "  ```\nfunc (tset *typeSet) isMeasurable(t types.Type) bool {\n\t// Return early if the type has an associated size.\n\tif tset.sizes.At(t) != nil {\n\t\treturn tset.sizes.At(t).(bool)\n\t}\n\n\t// At this point, we know that t does not have an associated size, so we\n\t// need to check. We only want to update the tset.measurable map (which\n\t// is memoized), so we need to call isMeasurable recursively.\n\tif isFixedSizeType(t) {\n\t\t// All fixed size types are measurable except strings, which are\n\t\t// neither fixed size nor measurable.\n\t\tif isString(t) {\n\t\t\ttset.measurable.Set(t, false)\n\t\t\treturn false\n\t\t}\n\t\ttset.measurable.Set(t, true)\n\t\treturn true\n\t}\n\n\t// Check if t is a package local type or implements weaver.AutoMarshal.\n\tif info := tset.pkg.TypesInfo; info.TypeOf(info.ObjectOf(t)).Pkg() == tset.pkg.Types || tset.implementsAutoMarshal(t) {\n\t\t// Return early if the tset.measurable map already has an associated\n\t\t// size for t.\n\t\tif result := tset.measurable.At(t); result != nil {\n\t\t\treturn result.(bool)\n\t\t}\n\n\t\tmeasurable := false\n\t\tswitch x := t.(type) {\n\t\tcase *types.Struct:\n\t\t\t// If all the struct fields are measurable, then the struct is\n\t\t\t// measurable. Also, if the struct is named and we haven't checked\n\t\t\t// whether it implements the weaver.AutoMarshal interface, then we\n\t\t\t// need to check whether it implements the interface.\n\t\t\tmeasurable = true\n\t\t\tfor i := 0; i < x.NumFields(); i++ {\n\t\t\t\tf := x.Field(i)\n\t\t\t\tif !isMeasurable(f.Type()) {\n\t\t\t\t\tmeasurable = false\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif named, ok := x.Underlying().(*types.Named); ok && tset.measurable.At(named) == nil {\n\t\t\t\t// The named type has not yet been checked. Call\n\t\t\t\t// implementsAutoMarshal to check if the named type\n\t\t\t\t// implements weaver.AutoMarshal.\n\t\t\t\tif !implementsAutoMarshal(named) {\n\t\t\t\t\tmeasurable = false\n\t\t\t\t}\n\t\t\t}\n\t\tcase *types.Slice:\n\t\t\t// A slice is measurable only if its element type is measurable.\n\t\t\tmeasurable = isMeasurable(x.Elem())\n\t\tcase *types.Array:\n\t\t\t// An array is measurable only if its element type is\n\t\t\t// measurable.\n\t\t\tmeasurable = isMeasurable(x.Elem())\n\t\tcase *types.Pointer:\n\t\t\t// A pointer is measurable if its element type is measurable.\n\t\t\tmeasurable = isMeasurable(x.Elem())\n\t\tcase *types.Named:\n\t\t\t// All types implemented by a named type are measurable unless\n\t\t\t// they are themselves non-measurable.\n\t\t\tmeasurable = true\n\t\t\tfor n := 0; n < x.NumMethods(); n++ {\n\t\t\t\tif !isMeasurable(x.Method(n).Type()) {\n\t\t\t\t\tmeasurable = false\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tdefault:\n\t\t\t// We do not yet support other types.\n\t\t\ttset.measurable.Set(t, false)\n\t\t\treturn false\n\t\t}\n\t\ttset.measurable.Set(t, measurable)\n\t\treturn measurable\n\t}\n\ttset.measurable.Set(t, false)\n\treturn false\n", "  ```\nfunc (tset *typeSet) isMeasurable(t types.Type) bool {\n\t// For simplicity, we only consider a type measurable if the type and all\n\t// its nested types are package local. For example, a struct { x\n\t// otherpackage.T } is not measurable, even if otherpackage.T is\n\t// measurable. We make an exception for weaver.AutoMarshal.\n\n\t// Supported types: Array types, basic types except string,\n\t// pointers, slices, and structs whose fields are all fixed size.\n\n\t// TODO(sanjay): Is traversing this type tree the most efficient way to\n\t// determine whether a type is measurable? Perhaps we could use the\n\t// typeutil.Map?\n\n\t// TODO(sanjay): Check for cycles in the type tree. If we encounter a\n\t// cycle, log a warning.\n\n\t// TODO(sanjay): Be careful with interfaces. If an interface contains\n\t// another interface, is this considered a cycle? What about a\n\t// package-local interface that contains a non-package-local interface?\n\t// For now, we'll conservatively assume a interface + cycle implies\n\t// non-measurable.\n\n\tseen := make(map[types.Type]struct{})\n\treturn tset.walkMeasurable(t, seen)\n}\n\n// walkMeasurable traverses a type tree and determines whether the type and all\n// its nested types are measurable.\n//\n// seen maps types to the fact that they've been seen already. This is used to\n// avoid infinite recursion in cases where a nested type references itself,\n// like [T]\nfunc (tset *typeSet) walkMeasurable(t types.Type, seen map[types.Type]struct{}) bool {\n\t// Early-out if this type has already been visited.\n\tif _, ok := seen[t]; ok {\n\t\treturn true\n\t}\n\tseen[t] = struct{}{}\n\n\tswitch x := t.(type) {\n\tcase *types.Named:\n\t\t// No need to check if x is an unexported type from another package\n\t\t// since the Go compiler takes care of that.\n\t\ts, ok := x.Underlying().(*types.Struct)\n\t\tif !ok {\n\t\t\treturn tset.isFixedSizeType(x.Underlying())\n\t\t}\n\n\t\t// Don't check if x.Underlying() implements AutoMarshal. We want to\n\t\t// determine whether x is measurable from scratch, not take advantage\n\t\t// of its implementation of AutoMarshal.\n\n\t\t// If the underlying type is a struct that has been declared to\n\t\t// implement the AutoMarshal interface but hasn't yet been\n\t\t// checked, then we need to recurse to detect cycles.\n\t\tfor i := 0; i < s.NumFields(); i++ {\n\t\t\tif !tset.walkMeasurable(s.Field(i).Type(), seen) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\n\tcase *types.Array:\n\t\treturn tset.isFixedSizeType(x.Elem())\n\n\tcase *types.Slice:\n\t\treturn tset.isFixedSizeType(x.Elem())\n\n\tcase *types.Pointer:\n\t\treturn tset.isFixedSizeType(x.Elem())\n\n\tcase *types.Struct:\n\t\tfor i := 0; i < x.NumFields(); i++ {\n\t\t\tif !tset.walkMeasurable(x.Field(i).Type(), seen) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\n\tcase *types.Basic:\n\t\tswitch x.Kind() {\n\t\tcase types.Bool,\n\t\t\ttypes.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\ttypes.Float32, types.Float64,\n\t\t\ttypes.Complex64, types.Complex128,\n\t\t\ttypes.String:\n\t\t\t// Supported.\n\t\t\treturn true\n\t\tdefault:"]}, "weaver-internal/tool/generate/types.go-isValidRouterType": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/tool/generate/types.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generate\n\nimport (\n\t\"fmt\"\n\t\"go/types\"\n\t\"path\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"golang.org/x/tools/go/packages\"\n\t\"golang.org/x/tools/go/types/typeutil\"\n)\n\nconst weaverPackagePath = \"github.com/ServiceWeaver/weaver\"\n\n// typeSet holds type information needed by the code generator.\ntype typeSet struct {\n\tpkg            *packages.Package\n\timported       []importPkg          // imported packages\n\timportedByPath map[string]importPkg // imported, indexed by path\n\timportedByName map[string]importPkg // imported, indexed by name\n\n\tautomarshals          *typeutil.Map // types that implement AutoMarshal\n\tautomarshalCandidates *typeutil.Map // types that declare themselves AutoMarshal\n\n\t// If checked[t] != nil, then checked[t] is the cached result of calling\n\t// check(pkg, t, string[]{}). Otherwise, if checked[t] == nil, then t has\n\t// not yet been checked for serializability. Read typeutil.Map's\n\t// documentation for why checked shouldn't be a map[types.Type]bool.\n\tchecked typeutil.Map\n\n\t// If sizes[t] != nil, then sizes[t] == sizeOfType(t).\n\tsizes typeutil.Map\n\n\t// If measurable[t] != nil, then measurable[t] == isMeasurableType(t).\n\tmeasurable typeutil.Map\n}\n\n// importPkg is a package imported by the generated code.\ntype importPkg struct {\n\tpath  string // e.g., \"github.com/ServiceWeaver/weaver\"\n\tpkg   string // e.g., \"weaver\", \"context\", \"time\"\n\talias string // e.g., foo in `import foo \"context\"`\n\tlocal bool   // are we in this package?\n}\n\n// name returns the name by which the imported package should be referenced in\n// the generated code. If the package is imported without an alias, like this:\n//\n//\timport \"context\"\n//\n// then the name is the same as the package name (e.g., \"context\"). However, if\n// a package is imported with an alias, then the name is the alias:\n//\n//\timport thisIsAnAlias \"context\"\n//\n// If the package is local, an empty string is returned.\nfunc (i importPkg) name() string {\n\tif i.local {\n\t\treturn \"\"\n\t} else if i.alias != \"\" {\n\t\treturn i.alias\n\t}\n\treturn i.pkg\n}\n\n// qualify returns the provided member of the package, qualified with the\n// package name. For example, the \"Context\" type inside the \"context\" package\n// is qualified \"context.Context\". The \"Now\" function inside the \"time\" package\n// is qualified \"time.Now\". Note that the package name is not prefixed when\n// qualifying members of the local package.\nfunc (i importPkg) qualify(member string) string {\n\tif i.local {\n\t\treturn member\n\t}\n\treturn fmt.Sprintf(\"%s.%s\", i.name(), member)\n}\n\n// newTypeSet returns the container for types found in pkg.\nfunc newTypeSet(pkg *packages.Package, automarshals, automarshalCandidates *typeutil.Map) *typeSet {\n\treturn &typeSet{\n\t\tpkg:                   pkg,\n\t\timported:              []importPkg{},\n\t\timportedByPath:        map[string]importPkg{},\n\t\timportedByName:        map[string]importPkg{},\n\t\tautomarshals:          automarshals,\n\t\tautomarshalCandidates: automarshalCandidates,\n\t}\n}\n\n// importPackage imports a package with the provided path and package name. The\n// package is imported with an alias if there is a package name clash.\nfunc (tset *typeSet) importPackage(path, pkg string) importPkg {\n\tnewImportPkg := func(path, pkg, alias string, local bool) importPkg {\n\t\ti := importPkg{path: path, pkg: pkg, alias: alias, local: local}\n\t\ttset.imported = append(tset.imported, i)\n\t\ttset.importedByPath[i.path] = i\n\t\ttset.importedByName[i.name()] = i\n\t\treturn i\n\t}\n\n\tif imp, ok := tset.importedByPath[path]; ok {\n\t\t// This package has already been imported.\n\t\treturn imp\n\t}\n\n\tif _, ok := tset.importedByName[pkg]; !ok {\n\t\t// Import the package without an alias.\n\t\treturn newImportPkg(path, pkg, \"\", path == tset.pkg.PkgPath)\n\t}\n\n\t// Find an unused alias.\n\tvar alias string\n\tcounter := 1\n\tfor {\n\t\talias = fmt.Sprintf(\"%s%d\", pkg, counter)\n\t\tif _, ok := tset.importedByName[alias]; !ok {\n\t\t\tbreak\n\t\t}\n\t\tcounter++\n\t}\n\treturn newImportPkg(path, pkg, alias, path == tset.pkg.PkgPath)\n}\n\n// imports returns the list of packages to import in generated code.\nfunc (tset *typeSet) imports() []importPkg {\n\tsort.Slice(tset.imported, func(i, j int) bool {\n\t\treturn tset.imported[i].path < tset.imported[j].path\n\t})\n\treturn tset.imported\n}\n\n// checkSerializable checks that type t is serializable.\nfunc (tset *typeSet) checkSerializable(t types.Type) []error {\n\t// lineage can generate a human readable description of the lineage of a\n\t// checked type. As check recurses on type t, it encounters a number of\n\t// nested types. For example, if we have the following type A\n\t//\n\t//     type A struct{ x []chan int }\n\t//\n\t// then check(A) will encounter the types A, struct{ x []chan int }, []chan\n\t// int, chan int, and int. We associate each of these types with a\n\t// corresponding \"path\", a concise description of the relationship between\n\t// the root type and the nested types. For example, the type chan int has\n\t// path A.x[0].\n\t//\n\t// lineage is a stack that stores a history of these paths as check\n\t// traverses a type. For example, if we call check(A), then lineage will\n\t// look like this when the chan int is discovered:\n\t//\n\t//     []pathAndType{\n\t//         pathAndType{\"A\", A},\n\t//         pathAndType{\"A.x\", []chan int},\n\t//         pathAndType{\"A.x[0]\", chan int},\n\t//     }\n\t//\n\t// This lineage is printed in error messages as:\n\t//\n\t//     A (type A)\n\t//     A.x (type []chan int)\n\t//     A.x[0] (type chan int)\n\t//\n\t// Note that for brevity, not every encountered type is entered into the\n\t// lineage.\n\ttype pathAndType struct {\n\t\tpath string\n\t\tt    types.Type\n\t}\n\tvar lineage []pathAndType\n\n\tvar errors []error\n\taddError := func(err error) {\n\t\tvar builder strings.Builder\n\n\t\t// If the lineage is trivial, then don't show it.\n\t\tif len(lineage) > 1 {\n\t\t\tfmt.Fprintf(&builder, \"\\n    \")\n\t\t\tfor i, pn := range lineage {\n\t\t\t\tfmt.Fprintf(&builder, \"%v (type %v)\", pn.path, pn.t.String())\n\t\t\t\tif i < len(lineage)-1 {\n\t\t\t\t\tfmt.Fprintf(&builder, \"\\n    \")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tqualifier := func(pkg *types.Package) string { return pkg.Name() }\n\t\terr = fmt.Errorf(\"%s: %w%s\", types.TypeString(t, qualifier), err, builder.String())\n\t\terrors = append(errors, err)\n\t}\n\n\t// stack contains the set of types encountered in the call stack of check.\n\t// It's used to detect recursive types.\n\t//\n\t// More specifically, the check function below is performing an implicit\n\t// depth first search of the graph of types formed by t. We record the\n\t// stack of visited types in stack and know we have a recursive type if we\n\t// ever run into a type that is already in stack.\n\t//\n\t// For example, consider the following types:\n\t//\n\t//   type A struct { b: *B }\n\t//   type B struct { a: *A }\n\t//\n\t// Calling check on A will yield a call stack that looks something like:\n\t//\n\t//   check(A)\n\t//     check(struct { b: *B })\n\t//       check(*B)\n\t//         check(B)\n\t//           check(struct { a: *A })\n\t//             check(*A)\n\t//               check(A)\n\t//\n\t// When performing the second check(A) call, stack includes A, struct { b:\n\t// *B }, *B, B, struct { a: *A }, and *A. Because we called check on A and\n\t// A is already in stack, we detect a recursive type and mark A as not\n\t// serializable.\n\tvar stack typeutil.Map\n\n\t// check recursively checks whether a type t is serializable. See lineage\n\t// above for a description of path. record is true if the current type\n\t// should be recorded in lineage. There are a few things worth noting:\n\t//\n\t//   (1) The results of calling check are memoized in tset.checked, but not\n\t//       for some trivial arguments. Some arguments like chan int are not\n\t//       memoized because they are trivial to check and because not\n\t//       memoizing can lead to a clearer error message.\n\t//\n\t//   (2) Consider the type t = struct { x chan int; y chan int }. t is not\n\t//       serializable because neither x nor y is serializable. check\n\t//       reports errors for both x and y as not serializable.\n\t//       Alternatively, check could find that x is not serializable and\n\t//       then immediately report that t is not serializable, skipping y\n\t//       completely. check doesn't do this. check will inspect a type fully\n\t//       to report the full set of errors.\n\t//\n\t// Note that the function also takes the parent type pt. This is needed in cases\n\t// whether we need to know the type of the parent type t (e.g., a named type\n\t// that is a proto is serializable iff the parent type is a pointer).\n\tvar check func(t types.Type, path string, record bool) bool\n\n\tcheck = func(t types.Type, path string, record bool) bool {\n\t\tif record {\n\t\t\tlineage = append(lineage, pathAndType{path, t})\n\t\t\tdefer func() { lineage = lineage[:len(lineage)-1] }()\n\t\t}\n\n\t\t// Return early if we've already checked this type.\n\t\tif result := tset.checked.At(t); result != nil {\n\t\t\tb := result.(bool)\n\t\t\tif b {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\t// We've already encountered type t and determined that it is not\n\t\t\t// serializable. We won't recurse down type t to compute the full\n\t\t\t// lineage and explanation of why t isn't serializable because we\n\t\t\t// already did that when determining t wasn't serializable in the\n\t\t\t// first place. Instead, we instruct the user to read the\n\t\t\t// previously reported error.\n\t\t\taddError(fmt.Errorf(\"not a serializable type; see above for details\"))\n\t\t\treturn false\n\t\t}\n\n\t\t// Check for recursive types.\n\t\tif stack.At(t) != nil {\n\t\t\taddError(fmt.Errorf(\"serialization of recursive types not currently supported\"))\n\t\t\ttset.checked.Set(t, false)\n\t\t\treturn false\n\t\t}\n\t\tstack.Set(t, struct{}{})\n\t\tdefer func() { stack.Delete(t) }()\n\n\t\tswitch x := t.(type) {\n\t\tcase *types.Named:\n\t\t\t// No need to check if x is an unexported type from another package\n\t\t\t// since the Go compiler takes care of that.\n\n\t\t\t// Check if the type implements one of the marshaler interfaces.\n\t\t\tif tset.isProto(x) || tset.automarshals.At(t) != nil || tset.implementsAutoMarshal(x) || tset.hasMarshalBinary(x) {\n\t\t\t\ttset.checked.Set(t, true)\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is not a struct, then we simply recurse\n\t\t\t// on the underlying type.\n\t\t\ts, ok := x.Underlying().(*types.Struct)\n\t\t\tif !ok {\n\t\t\t\ttset.checked.Set(t, check(x.Underlying(), path, false))\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is a struct that has not been declared to\n\t\t\t// implement the AutoMarshal interface, then it is not\n\t\t\t// serializable.\n\t\t\tif tset.automarshalCandidates.At(t) == nil {\n\t\t\t\t// TODO(mwhittaker): Print out a link to documentation on\n\t\t\t\t// weaver.AutoMarshal.\n\t\t\t\taddError(fmt.Errorf(\"named structs are not serializable by default. Consider using weaver.AutoMarshal.\"))\n\t\t\t\ttset.checked.Set(t, false)\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is a struct that has been declared to\n\t\t\t// implement the AutoMarshal interface but hasn't yet been checked,\n\t\t\t// then we need to recurse to detect cycles.\n\t\t\tserializable := true\n\t\t\tfor i := 0; i < s.NumFields(); i++ {\n\t\t\t\tf := s.Field(i)\n\t\t\t\t// We store the result of calling check in b rather than\n\t\t\t\t// writing serializable = serializable && check(...) because we\n\t\t\t\t// don't want to short circuit and avoid calling check.\n\t\t\t\tb := check(f.Type(), path+\".\"+f.Name(), true)\n\t\t\t\tserializable = serializable && b\n\t\t\t}\n\t\t\ttset.checked.Set(t, serializable)\n\n\t\tcase *types.Interface:\n\t\t\t// TODO(sanjay): Support types.Interface only if we can figure out\n\t\t\t// a way to instantiate the type.\n\t\t\taddError(fmt.Errorf(\"serialization of interfaces not currently supported\"))\n\t\t\ttset.checked.Set(t, false)\n\n\t\tcase *types.Struct:\n\t\t\taddError(fmt.Errorf(\"struct literals are not serializable\"))\n\t\t\ttset.checked.Set(t, false)\n\n\t\tcase *types.Basic:\n\t\t\tswitch x.Kind() {\n\t\t\tcase types.Bool,\n\t\t\t\ttypes.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\t\ttypes.Float32, types.Float64,\n\t\t\t\ttypes.Complex64, types.Complex128,\n\t\t\t\ttypes.String:\n\t\t\t\t// Supported.\n\t\t\t\ttset.checked.Set(t, true)\n\t\t\tdefault:\n\t\t\t\tif isInvalid(t) {\n\t\t\t\t\taddError(fmt.Errorf(\"Maybe you forgot to run `go mod tidy`? Also try running `go build` to diagnose further.\"))\n\t\t\t\t} else {\n\t\t\t\t\taddError(fmt.Errorf(\"unsupported basic type\"))\n\t\t\t\t}\n\t\t\t\t// For a better error message, we don't memoize this.\n\t\t\t\treturn false\n\t\t\t}\n\n\t\tcase *types.Array:\n\t\t\ttset.checked.Set(t, check(x.Elem(), path+\"[0]\", true))\n\n\t\tcase *types.Slice:\n\t\t\ttset.checked.Set(t, check(x.Elem(), path+\"[0]\", true))\n\n\t\tcase *types.Pointer:\n\t\t\ttset.checked.Set(t, check(x.Elem(), \"(*\"+path+\")\", true))\n\n\t\tcase *types.Map:\n\t\t\tkeySerializable := check(x.Key(), path+\".key\", true)\n\t\t\tvalSerializable := check(x.Elem(), path+\".value\", true)\n\t\t\ttset.checked.Set(t, keySerializable && valSerializable)\n\n\t\tdefault:\n\t\t\taddError(fmt.Errorf(\"not a serializable type\"))\n\t\t\t// For a better error message, we don't memoize this.\n\t\t\treturn false\n\t\t}\n\n\t\treturn tset.checked.At(t).(bool)\n\t}\n\n\tcheck(t, t.String(), true)\n\treturn errors\n}\n\n// isFixedSizeType returns whether the provided type has a fixed serialization\n// size. Here is a summary of which types are fixed sized:\n//\n//   - Every basic type (e.g., bool, int) except string is fixed sized.\n//   - The array type [N]t is fixed sized if t is fixed sized.\n//   - A struct is fixed sized if the types of its fields are fixed sized.\n//   - A named type is fixed sized if its underlying type is fixed sized.\nfunc (tset *typeSet) isFixedSizeType(t types.Type) bool {\n\treturn tset.sizeOfType(t) >= 0\n}\n\n// sizeOfType returns the size of the serialization of t if t is fixed size, or\n// returns -1 otherwise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// isMeasurable returns whether the provided type is measurable.\n//\n// Informally, we say a type is measurable if we can cheaply compute the size\n// of its serialization at runtime. Some examples:\n//\n//   - Every fixed size type (e.g., int, bool, [3]int, struct{x, y int}) is\n//     measurable (with some restrictions on package locality; see below).\n//   - Strings are not fixed size, but they are measurable because we can\n//     cheaply compute the length of a string at runtime.\n//   - []string is not measurable because computing the size of the\n//     serialization of a []string would require us to compute the length of\n//     every string in the slice. This is a potentially expensive operation\n//     if the slice contains a large number of strings, so we consider\n//     []string to be not measurable.\n//   - For simplicity, we only consider a type measurable if the type and all\n//     its nested types are package local. For example, a struct { x\n//     otherpackage.T } is not measurable, even if otherpackage.T is\n//     measurable. We make an exception for weaver.AutoMarshal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// genTypeString returns the string representation of t as to be printed\n// in the generated code, updating import definitions to account for the\n// returned type string.\n//\n// Since this call has side-effects (i.e., updating import definitions), it\n// should only be called when the returned type string is written into\n// the generated file; otherwise, the generated code may end up with spurious\n// imports.\nfunc (tset *typeSet) genTypeString(t types.Type) string {\n\t// qualifier is passed to types.TypeString(Type, Qualifier) to determine\n\t// how packages are printed when pretty printing types. For this qualifier,\n\t// types in the root package are printed without their package name, while\n\t// types outside the root package are printed with their package name. For\n\t// example, if we're in root package foo, then the type foo.Bar is printed\n\t// as Bar, while the type io.Reader is printed as io.Reader. See [1] for\n\t// more information on qualifiers and pretty printing types.\n\t//\n\t// [1]: https://github.com/golang/example/tree/master/gotypes#formatting-support\n\tvar qualifier = func(pkg *types.Package) string {\n\t\tif pkg == tset.pkg.Types {\n\t\t\treturn \"\"\n\t\t}\n\t\treturn tset.importPackage(pkg.Path(), pkg.Name()).name()\n\t}\n\treturn types.TypeString(t, qualifier)\n}\n\n// isInvalid returns true iff the given type is invalid.\nfunc isInvalid(t types.Type) bool {\n\treturn t.String() == \"invalid type\"\n}\n\n// implementsError returns whether the provided type is a concrete type that\n// implements error.\nfunc (tset *typeSet) implementsError(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\treturn false\n\t}\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"Error\")\n\tmethod, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tsig, ok := method.Type().(*types.Signature)\n\tif !ok {\n\t\treturn false\n\t}\n\tif args := sig.Params(); args.Len() != 0 {\n\t\treturn false\n\t}\n\tif results := sig.Results(); results.Len() != 1 || !isString(results.At(0).Type()) {\n\t\treturn false\n\t}\n\treturn true\n}\n\n// isProto returns whether the provided type is a concrete type that implements\n// the proto.Message interface.\nfunc (tset *typeSet) isProto(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\t// A superinterface of proto.Message does \"implement\" proto.Message,\n\t\t// but we only accept concrete types that implement the interface.\n\t\treturn false\n\t}\n\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"ProtoReflect\")\n\tmethod, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tsig, ok := method.Type().(*types.Signature)\n\tif !ok {\n\t\treturn false\n\t}\n\trecv, args, results := sig.Recv(), sig.Params(), sig.Results()\n\tif args.Len() != 0 || results.Len() != 1 {\n\t\treturn false\n\t}\n\tif !isProtoMessage(results.At(0).Type()) {\n\t\treturn false\n\t}\n\t// Check the receiver. We avoid complicated embeddings by requiring that\n\t// the method is defined on the type itself.\n\t//\n\t// TODO(mwhittaker): Relax this requirement if it becomes annoying.\n\tif p, ok := recv.Type().(*types.Pointer); ok {\n\t\treturn types.Identical(p.Elem(), t)\n\t} else {\n\t\treturn types.Identical(recv.Type(), t)\n\t}\n}\n\nfunc isProtoMessage(t types.Type) bool {\n\tn, ok := t.(*types.Named)\n\tif !ok {\n\t\treturn false\n\t}\n\tconst protoreflect = \"google.golang.org/protobuf/reflect/protoreflect\"\n\treturn n.Obj().Pkg().Path() == protoreflect && n.Obj().Name() == \"Message\"\n}\n\n// implementsAutoMarshal returns whether the provided type is a concrete\n// type that implements the weaver.AutoMarshal interface.\nfunc (tset *typeSet) implementsAutoMarshal(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\t// A superinterface of AutoMarshal does \"implement\" the interface,\n\t\t// but we only accept concrete types that implement the interface.\n\t\treturn false\n\t}\n\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"WeaverMarshal\")\n\tmarshal, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tobj, _, _ = types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"WeaverUnmarshal\")\n\tunmarshal, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn is\n```\n# Function Signature: func isValidRouterType(t types.Type) bool {\n# Natural Language Annotations: // isValidRouterType returns whether the provided type is a valid router type.\n// A router type can be one of the following: an integer (signed or unsigned),\n// a float, or a string. Alternatively, it can be a struct that may optioanly\n// embed the weaver.AutoMarshal struct and rest of the fields must be either\n// integers, floats, or strings.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc isValidRouterType(t types.Type) bool {\n\tswitch x := t.(type) {\n\tcase *types.Basic:\n\t\treturn x.Kind() == types.Int || x.Kind() == types.Uint || x.Kind() == types.Float || x.Kind() == types.String\n\tcase *types.Struct:\n\t\tif tset.isAutoMarshal(x) {\n\t\t\treturn false\n\t\t}\n\t\tfor i := 0; i < t.NumFields(); i++ {\n\t\t\tf := x.Field(i)\n\t\t\tif !isValidRouterType(f.Type()) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n```", "  ```\nfunc isValidRouterType(t types.Type) bool {\n\tswitch t.(type) {\n\tcase types.Basic:\n\t\tswitch t.Underlying().Kind() {\n\t\tcase types.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\ttypes.Float32, types.Float64:\n\t\t\treturn true\n\t\t}\n\tcase types.Struct:\n\t\ts, ok := t.Underlying().(*types.Struct)\n\t\tif !ok || len(s.Fields()) == 0 || len(s.Fields()) > structBufLenMax {\n\t\t\treturn false\n\t\t}\n\t\t// First check if the struct embeds AutoMarshal.\n\t\tif _, ok := s.Field(0).Type().Underlying().(*types.Struct); !ok {\n\t\t\treturn false\n\t\t}\n\t\tif !isEmbeddedAutoMarshal(s.Field(0).Type().Underlying().(*types.Struct)) {\n\t\t\treturn false\n\t\t}\n\t\t// Check if the rest of the fields are either integers, floats, or strings.\n\t\tfor i := 1; i < len(s.Fields()); i++ {\n\t\t\tf := s.Field(i)\n\t\t\tswitch f.Type().Underlying().Kind() {\n\t\t\tcase types.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\t\ttypes.Float32, types.Float64, types.String:\n\t\t\tdefault:\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc isEmbeddedAutoMarshal(s *types.Struct) bool {\n\tobj, _, _ := types.LookupFieldOrMethod(s, true, tagPkg.Types, \"WeaverMarshal\")\n\tmarshal, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tobj, _, _ = types.LookupFieldOrMethod(s, true, tagPkg.Types, \"WeaverUnmarshal\")\n\tunmarshal, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn is\n}\n```", "  ```\nfunc isValidRouterType(t types.Type, path string, tset *typeSet) error {\n\t// Check if the type is a basic type.\n\tif _, ok := t.(*types.Basic); ok {\n\t\tif !isRouterFixedSize(t) {\n\t\t\treturn fmt.Errorf(\"%s: router type %v is fix-sized\", path, t.String())\n\t\t}\n\t\treturn nil\n\t}\n\n\t// If the type is a named type, we need to check if it's a proto or\n\t// an auto marshaler\n\tswitch x := t.(type) {\n\tcase *types.Named:\n\t\t// Check if the named type is a proto.\n\t\tif tset.isProto(x) {\n\t\t\tif !isRouterFixedSize(types.NewPointer(x)) {\n\t\t\t\treturn fmt.Errorf(\"%s: router type %v is fix-sized\", path, t.String())\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t\t// Check if the named type is a marshaler.\n\t\tif tset.implementsAutoMarshal(x) {\n\t\t\treturn nil\n\t\t}\n\t}\n\n\t// If the type is a struct, we need to check if it's a marshaler\n\t// and if all the fields are valid router types.\n\tif s, ok := t.(*types.Struct); ok {\n\t\tif tset.implementsAutoMarshal(t) {\n\t\t\treturn nil\n\t\t}\n\t\tfor i := 0; i < s.NumFields(); i++ {\n\t\t\tf := s.Field(i)\n\t\t\tif err := isValidRouterType(f.Type(), path+\".\"+f.Name(), tset); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif !isRouterFixedSize(t) {\n\t\t\treturn fmt.Errorf(\"%s: router type %v is fix-sized\", path, t.String())\n\t\t}\n\t\treturn nil\n\t}\n\n\treturn fmt.Errorf(\"%s: cannot handle type %v\", path, t.String())\n}\n```"]}, "weaver-internal/tool/generate/generator.go-sanitize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/tool/generate/generator.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generate\n\nimport (\n\t\"bytes\"\n\t\"crypto/sha256\"\n\t\"errors\"\n\t\"fmt\"\n\t\"go/ast\"\n\t\"go/format\"\n\t\"go/parser\"\n\t\"go/token\"\n\t\"go/types\"\n\t\"io\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"unicode\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/files\"\n\t\"github.com/ServiceWeaver/weaver/internal/tool\"\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/colors\"\n\t\"github.com/ServiceWeaver/weaver/runtime/version\"\n\t\"golang.org/x/exp/maps\"\n\t\"golang.org/x/tools/go/packages\"\n\t\"golang.org/x/tools/go/types/typeutil\"\n)\n\n// TODO(rgrandl): Modify the generator code to use only the types package. Right\n// now we are doing code generation relying both on the types and ast packages,\n// which can be confusing and also we might do unnecessary work.\n\nconst (\n\tgeneratedCodeFile = \"weaver_gen.go\"\n\n\tUsage = `Generate code for a Service Weaver application.\n\nUsage:\n  weaver generate [packages]\n\nDescription:\n  \"weaver generate\" generates code for the Service Weaver applications in the\n  provided packages. For example, \"weaver generate . ./foo\" will generate code\n  for the Service Weaver applications in the current directory and in the ./foo\n  directory. For every package, the generated code is placed in a weaver_gen.go\n  file in the package's directory. For example, \"weaver generate . ./foo\" will\n  create ./weaver_gen.go and ./foo/weaver_gen.go.\n\n  You specify packages for \"weaver generate\" in the same way you specify\n  packages for go build, go test, go vet, etc. See \"go help packages\" for more\n  information.\n\n  Rather than invoking \"weaver generate\" directly, you can place a line of the\n  following form in one of the .go files in the package:\n\n      //go:generate weaver generate\n\n  and then use the normal \"go generate\" command.\n\nExamples:\n  # Generate code for the package in the current directory.\n  weaver generate\n\n  # Same as \"weaver generate\".\n  weaver generate .\n\n  # Generate code for the package in the ./foo directory.\n  weaver generate ./foo\n\n  # Generate code for all packages in all subdirectories of current directory.\n  weaver generate ./...`\n)\n\n// Options controls the operation of Generate.\ntype Options struct {\n\t// If non-nil, use the specified function to report warnings.\n\tWarn func(error)\n}\n\n// Generate generates Service Weaver code for the specified packages.\n// The list of supplied packages are treated similarly to the arguments\n// passed to \"go build\" (see \"go help packages\" for details).\nfunc Generate(dir string, pkgs []string, opt Options) error {\n\tif opt.Warn == nil {\n\t\topt.Warn = func(err error) { fmt.Fprintln(os.Stderr, err) }\n\t}\n\tfset := token.NewFileSet()\n\tcfg := &packages.Config{\n\t\tMode:       packages.NeedName | packages.NeedSyntax | packages.NeedImports | packages.NeedTypes | packages.NeedTypesInfo,\n\t\tDir:        dir,\n\t\tFset:       fset,\n\t\tParseFile:  parseNonWeaverGenFile,\n\t\tBuildFlags: []string{\"--tags=ignoreWeaverGen\"},\n\t}\n\tpkgList, err := packages.Load(cfg, pkgs...)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"packages.Load: %w\", err)\n\t}\n\n\tvar automarshals typeutil.Map\n\tvar errs []error\n\tfor _, pkg := range pkgList {\n\t\tg, err := newGenerator(opt, pkg, fset, &automarshals)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t\tcontinue\n\t\t}\n\t\tif err := g.generate(); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\treturn errors.Join(errs...)\n}\n\n// parseNonWeaverGenFile parses a Go file, except for weaver_gen.go files whose\n// contents are ignored since those contents may reference types that no longer\n// exist.\nfunc parseNonWeaverGenFile(fset *token.FileSet, filename string, src []byte) (*ast.File, error) {\n\tif filepath.Base(filename) == generatedCodeFile {\n\t\treturn parser.ParseFile(fset, filename, src, parser.PackageClauseOnly)\n\t}\n\treturn parser.ParseFile(fset, filename, src, parser.ParseComments|parser.DeclarationErrors)\n}\n\ntype generator struct {\n\tpkg            *packages.Package\n\ttset           *typeSet\n\tfileset        *token.FileSet\n\tcomponents     []*component\n\tsizeFuncNeeded typeutil.Map // types that need a serviceweaver_size_* function\n\tgenerated      typeutil.Map // memo cache for generateEncDecMethodsFor\n}\n\n// errorf is like fmt.Errorf but prefixes the error with the provided position.\nfunc errorf(fset *token.FileSet, pos token.Pos, format string, args ...interface{}) error {\n\t// Rewrite the position's filename relative to the current directory. This\n\t// replaces long filenames like \"/home/foo/ServiceWeaver/weaver/weaver.go\"\n\t// with much shorter filenames like \"./weaver.go\".\n\tposition := fset.Position(pos)\n\tif cwd, err := filepath.Abs(\".\"); err == nil {\n\t\tif filename, err := filepath.Rel(cwd, position.Filename); err == nil {\n\t\t\tposition.Filename = filename\n\t\t}\n\t}\n\n\tprefix := position.String()\n\tif colors.Enabled() {\n\t\t// Color the filename red when colors are enabled.\n\t\tprefix = fmt.Sprintf(\"%s%v%s\", colors.Color256(160), position, colors.Reset)\n\t}\n\treturn fmt.Errorf(\"%s: %w\", prefix, fmt.Errorf(format, args...))\n}\n\nfunc newGenerator(opt Options, pkg *packages.Package, fset *token.FileSet, automarshals *typeutil.Map) (*generator, error) {\n\t// Abort if there were any errors loading the package.\n\tvar errs []error\n\tfor _, err := range pkg.Errors {\n\t\terrs = append(errs, err)\n\t}\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Search every file in the package for types that embed the\n\t// weaver.AutoMarshal struct.\n\ttset := newTypeSet(pkg, automarshals, &typeutil.Map{})\n\tfor _, file := range pkg.Syntax {\n\t\tfilename := fset.Position(file.Package).Filename\n\t\tif filepath.Base(filename) == generatedCodeFile {\n\t\t\t// Ignore weaver_gen.go files.\n\t\t\tcontinue\n\t\t}\n\t\tts, err := findAutoMarshals(pkg, file)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t\tcontinue\n\t\t}\n\t\tfor _, t := range ts {\n\t\t\ttset.automarshalCandidates.Set(t, struct{}{})\n\t\t}\n\t}\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Just because a type embeds weaver.AutoMarshal doesn't mean we can\n\t// automatically marshal it. Some types, like `struct { x chan int }`, are\n\t// just not serializable. Here, we check that every type that embeds\n\t// weaver.AutoMarshal is actually serializable.\n\tfor _, t := range tset.automarshalCandidates.Keys() {\n\t\tn := t.(*types.Named)\n\t\tif err := errors.Join(tset.checkSerializable(n)...); err != nil {\n\t\t\terrs = append(errs, errorf(fset, n.Obj().Pos(), \"type %v is not serializable\\n%w\", t, err))\n\t\t\tcontinue\n\t\t}\n\t\ttset.automarshals.Set(t, struct{}{})\n\t}\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Find and process all components.\n\tcomponents := map[string]*component{}\n\tfor _, file := range pkg.Syntax {\n\t\tfilename := fset.Position(file.Package).Filename\n\t\tif filepath.Base(filename) == generatedCodeFile {\n\t\t\t// Ignore weaver_gen.go files.\n\t\t\tcontinue\n\t\t}\n\n\t\tfileComponents, err := findComponents(opt, pkg, file, tset)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tfor _, c := range fileComponents {\n\t\t\t// Check for component duplicates, two components that embed the\n\t\t\t// same weaver.Implements[T].\n\t\t\t//\n\t\t\t// TODO(mwhittaker): This code relies on the fact that a component\n\t\t\t// interface and component implementation have to be in the same\n\t\t\t// package. If we lift this requirement, then this code will break.\n\t\t\tif existing, ok := components[c.fullIntfName()]; ok {\n\t\t\t\terrs = append(errs, errorf(pkg.Fset, c.impl.Obj().Pos(),\n\t\t\t\t\t\"Duplicate implementation for component %s, other declaration: %v\",\n\t\t\t\t\tc.fullIntfName(), fset.Position(existing.impl.Obj().Pos())))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcomponents[c.fullIntfName()] = c\n\t\t}\n\t}\n\n\t// Find method attributes.\n\tfor _, file := range pkg.Syntax {\n\t\tfilename := fset.Position(file.Package).Filename\n\t\tif filepath.Base(filename) == generatedCodeFile {\n\t\t\t// Ignore weaver_gen.go files.\n\t\t\tcontinue\n\t\t}\n\t\tif err := findMethodAttributes(pkg, file, components); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &generator{\n\t\tpkg:        pkg,\n\t\ttset:       tset,\n\t\tfileset:    fset,\n\t\tcomponents: maps.Values(components),\n\t}, nil\n}\n\n// findComponents returns the components in the provided file. For example,\n// findComponents will find and return the following component.\n//\n//\ttype something struct {\n//\t    weaver.Implements[SomeComponentType]\n//\t    ...\n//\t}\nfunc findComponents(opt Options, pkg *packages.Package, f *ast.File, tset *typeSet) ([]*component, error) {\n\tvar components []*component\n\tvar errs []error\n\tfor _, d := range f.Decls {\n\t\tgendecl, ok := d.(*ast.GenDecl)\n\t\tif !ok || gendecl.Tok != token.TYPE {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, spec := range gendecl.Specs {\n\t\t\tts, ok := spec.(*ast.TypeSpec)\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcomponent, err := extractComponent(opt, pkg, f, tset, ts)\n\t\t\tif err != nil {\n\t\t\t\terrs = append(errs, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif component != nil {\n\t\t\t\tcomponents = append(components, component)\n\t\t\t}\n\t\t}\n\t}\n\treturn components, errors.Join(errs...)\n}\n\nfunc findMethodAttributes(pkg *packages.Package, f *ast.File, components map[string]*component) error {\n\t// Look for declarations of the form:\n\t//\tvar _ weaver.NotRetriable = Component.Method\n\tvar errs []error\n\tfor _, decl := range f.Decls {\n\t\tgendecl, ok := decl.(*ast.GenDecl)\n\t\tif !ok || gendecl.Tok != token.VAR {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, spec := range gendecl.Specs {\n\t\t\tvalspec, ok := spec.(*ast.ValueSpec)\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttypeAndValue, ok := pkg.TypesInfo.Types[valspec.Type]\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tt := typeAndValue.Type\n\t\t\tif !isWeaverNotRetriable(t) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfor _, val := range valspec.Values {\n\t\t\t\t// We allow non-blank vars for uniformity.\n\t\t\t\tcomp, method, ok := findComponentMethod(pkg, components, val)\n\t\t\t\tif !ok {\n\t\t\t\t\terrs = append(errs, errorf(pkg.Fset, valspec.Pos(), \"weaver.NonRetriable should only be assigned a value that identifies a method of a component implemented by this package\"))\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif comp.noretry == nil {\n\t\t\t\t\tcomp.noretry = map[string]struct{}{}\n\t\t\t\t}\n\t\t\t\tcomp.noretry[method] = struct{}{}\n\t\t\t}\n\t\t}\n\t}\n\treturn errors.Join(errs...)\n}\n\n// findComponentMethod returns the component and method if val is an expression of\n// the form C.M where C is a component listed in components and C has a method named M.\nfunc findComponentMethod(pkg *packages.Package, components map[string]*component, val ast.Expr) (*component, string, bool) {\n\tsel, ok := val.(*ast.SelectorExpr)\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tctypeval, ok := pkg.TypesInfo.Types[sel.X]\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tctype, ok := ctypeval.Type.(*types.Named)\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tcname := fullName(ctype)\n\tc, ok := components[cname]\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tmethod := sel.Sel.Name\n\tfor _, m := range c.methods() {\n\t\tif m.Name() == method {\n\t\t\treturn c, method, true\n\t\t}\n\t}\n\treturn nil, \"\", false\n}\n\n// findAutoMarshals returns the types in the provided file which embed the\n// weaver.AutoMarshal struct.\nfunc findAutoMarshals(pkg *packages.Package, f *ast.File) ([]*types.Named, error) {\n\tvar automarshals []*types.Named\n\tvar errs []error\n\tfor _, decl := range f.Decls {\n\t\tgendecl, ok := decl.(*ast.GenDecl)\n\t\tif !ok || gendecl.Tok != token.TYPE {\n\t\t\t// This is not a type declaration.\n\t\t\tcontinue\n\t\t}\n\n\t\t// Recall that a type declaration can have multiple type specs. We have\n\t\t// to iterate over all of them. For example:\n\t\t//\n\t\t//     type (\n\t\t//         a struct{} // Spec 1\n\t\t//         b struct{} // Spec 2\n\t\t//     )\n\t\tfor _, spec := range gendecl.Specs {\n\t\t\ttypespec, ok := spec.(*ast.TypeSpec)\n\t\t\tif !ok {\n\t\t\t\tpanic(errorf(pkg.Fset, spec.Pos(), \"type declaration has non-TypeSpec spec: %v\", spec))\n\t\t\t}\n\n\t\t\t// Extract the type's name.\n\t\t\tdef, ok := pkg.TypesInfo.Defs[typespec.Name]\n\t\t\tif !ok {\n\t\t\t\tpanic(errorf(pkg.Fset, spec.Pos(), \"name %v not found\", typespec.Name))\n\t\t\t}\n\t\t\tn, ok := def.Type().(*types.Named)\n\t\t\tif !ok {\n\t\t\t\t// For type aliases like `type Int = int`, Int has type int and\n\t\t\t\t// not type Named. We ignore these.\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Check if the type of the expression is struct.\n\t\t\tt, ok := pkg.TypesInfo.Types[typespec.Type].Type.(*types.Struct)\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Check for an embedded weaver.AutoMarshal field.\n\t\t\tautomarshal := false\n\t\t\tfor i := 0; i < t.NumFields(); i++ {\n\t\t\t\tf := t.Field(i)\n\t\t\t\tif f.Embedded() && isWeaverAutoMarshal(f.Type()) {\n\t\t\t\t\tautomarshal = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !automarshal {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Ignore generic types. Generic types don't play well with\n\t\t\t// embedded AutoMarshals. For example, consider the following type\n\t\t\t// declaration:\n\t\t\t//\n\t\t\t//     type Register[A any] struct {\n\t\t\t//         weaver.AutoMarshal\n\t\t\t//         a A\n\t\t\t//     }\n\t\t\t//\n\t\t\t// Is Register[A] serializable? It depends on A. Plus, we cannot\n\t\t\t// really generate WeaverMarshal and WeaverUnmarshal methods for\n\t\t\t// specific instantiations of Register[A]. Because of these\n\t\t\t// complications, we ignore generic types.\n\t\t\t//\n\t\t\t// TODO(mwhittaker): Handle generics somehow?\n\t\t\tif n.TypeParams() != nil { // generics have non-nil TypeParams()\n\t\t\t\terrs = append(errs, errorf(pkg.Fset, spec.Pos(),\n\t\t\t\t\t\"generic struct %v cannot embed weaver.AutoMarshal. See serviceweaver.dev/docs.html#serializable-types for more information.\",\n\t\t\t\t\tformatType(pkg, n)))\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tautomarshals = append(automarshals, n)\n\t\t}\n\t}\n\treturn automarshals, errors.Join(errs...)\n}\n\n// extractComponent attempts to extract a component from the provided TypeSpec.\n// It returns a nil component if the TypeSpec doesn't define a component.\nfunc extractComponent(opt Options, pkg *packages.Package, file *ast.File, tset *typeSet, spec *ast.TypeSpec) (*component, error) {\n\t// Check that the type spec is of the form `type t struct {...}`.\n\ts, ok := spec.Type.(*ast.StructType)\n\tif !ok {\n\t\t// This type declaration does not involve a struct. For example, it\n\t\t// might look like `type t int`. These non-struct type declarations\n\t\t// cannot be components.\n\t\treturn nil, nil\n\t}\n\tdef, ok := pkg.TypesInfo.Defs[spec.Name]\n\tif !ok {\n\t\tpanic(errorf(pkg.Fset, spec.Pos(), \"name %v not found\", spec.Name))\n\t}\n\timpl, ok := def.Type().(*types.Named)\n\tif !ok {\n\t\t// For type aliases like `type t = struct{}`, t has type *types.Struct\n\t\t// and not type *types.Named. We ignore these.\n\t\treturn nil, nil\n\t}\n\n\t// Find any weaver.Implements[T] or weaver.WithRouter[T] embedded fields.\n\tvar intf *types.Named   // The component interface type\n\tvar router *types.Named // Router type (if any)\n\tvar isMain bool         // Is intf weaver.Main?\n\tvar refs []*types.Named // T for which weaver.Ref[T] exists in struct\n\tvar listeners []string  // Names of all listener fields declared in struct\n\tfor _, f := range s.Fields.List {\n\t\ttypeAndValue, ok := pkg.TypesInfo.Types[f.Type]\n\t\tif !ok {\n\t\t\tpanic(errorf(pkg.Fset, f.Pos(), \"type %v not found\", f.Type))\n\t\t}\n\t\tt := typeAndValue.Type\n\n\t\tif isWeaverRef(t) {\n\t\t\t// The field f has type weaver.Ref[T].\n\t\t\targ := t.(*types.Named).TypeArgs().At(0)\n\t\t\tif isWeaverMain(arg) {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"components cannot contain a reference to weaver.Main\")\n\t\t\t}\n\t\t\tnamed, ok := arg.(*types.Named)\n\t\t\tif !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Ref argument %s is not a named type.\",\n\t\t\t\t\tformatType(pkg, arg))\n\t\t\t}\n\t\t\trefs = append(refs, named)\n\t\t} else if isWeaverListener(t) {\n\t\t\tlis, err := getListenerNamesFromStructField(pkg, f)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tlisteners = append(listeners, lis...)\n\t\t}\n\n\t\tif len(f.Names) != 0 {\n\t\t\t// Ignore unembedded fields.\n\t\t\t//\n\t\t\t// TODO(mwhittaker): Warn the user about unembedded\n\t\t\t// weaver.Implements, weaver.WithConfig, or weaver.WithRouter?\n\t\t\tcontinue\n\t\t}\n\n\t\tswitch {\n\t\t// The field f is an embedded weaver.Implements[T].\n\t\tcase isWeaverImplements(t):\n\t\t\t// Check that T is a named interface type inside the package.\n\t\t\targ := t.(*types.Named).TypeArgs().At(0)\n\t\t\tnamed, ok := arg.(*types.Named)\n\t\t\tif !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Implements argument %s is not a named type.\",\n\t\t\t\t\tformatType(pkg, arg))\n\t\t\t}\n\t\t\tisMain = isWeaverMain(arg)\n\t\t\tif !isMain && named.Obj().Pkg() != pkg.Types {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Implements argument %s is a type outside the current package. A component interface and implementation must be in the same package. If you can't move them into the same package, you can add `type %s %v` to the implementation's package and embed `weaver.Implements[%s]` instead of `weaver.Implements[%s]`.\",\n\t\t\t\t\tformatType(pkg, named), named.Obj().Name(), formatType(pkg, named), named.Obj().Name(), formatType(pkg, named))\n\t\t\t}\n\t\t\tif _, ok := named.Underlying().(*types.Interface); !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Implements argument %s is not an interface.\",\n\t\t\t\t\tformatType(pkg, named))\n\t\t\t}\n\t\t\tintf = named\n\n\t\t// The field f is an embedded weaver.WithRouter[T].\n\t\tcase isWeaverWithRouter(t):\n\t\t\t// Check that T is a named type inside the package.\n\t\t\targ := t.(*types.Named).TypeArgs().At(0)\n\t\t\tnamed, ok := arg.(*types.Named)\n\t\t\tif !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.WithRouter argument %s is not a named type.\",\n\t\t\t\t\tformatType(pkg, arg))\n\t\t\t}\n\t\t\tif named.Obj().Pkg() != pkg.Types {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.WithRouter argument %s is a type outside the current package.\",\n\t\t\t\t\tformatType(pkg, named))\n\t\t\t}\n\t\t\trouter = named\n\t\t}\n\t}\n\n\tif intf == nil {\n\t\t// TODO(mwhittaker): Warn the user if they embed weaver.WithRouter or\n\t\t// weaver.WithConfig but don't embed weaver.Implements.\n\t\treturn nil, nil\n\t}\n\n\t// Check that that the component implementation implements the component\n\t// interface.\n\tif !types.Implements(types.NewPointer(impl), intf.Underlying().(*types.Interface)) {\n\t\treturn nil, errorf(pkg.Fset, spec.Pos(),\n\t\t\t\"type %s embeds weaver\n```\n# Function Signature: func sanitize(t types.Type) string {\n# Natural Language Annotations: // sanitize generates a (somewhat pretty printed) name for the provided type\n// that is a valid go identifier [1]. sanitize also produces unique names. That\n// is, if u != t, then sanitize(u) != sanitize(t).\n//\n// Some examples:\n//\n//   - map[int]string -> map_int_string_589aebd1\n//   - map[int][]X    -> map_int_slice_X_ac498abc\n//   - []int          -> slice_int_1048ebf9\n//   - [][]string     -> slice_slice_string_13efa8aa\n//   - [20]int        -> array_20_int_00ae9a0a\n//   - *int           -> ptr_int_916711b2\n//\n// [1]: https://go.dev/ref/spec#Identifiers\nPlease return the generated code surrounded by ```\n", "response": ["  func sanitize(t types.Type) string {\n\th := sha256.New()\n\tio.WriteString(h, fmt.Sprintf(\"%T\", t))\n\treturn fmt.Sprintf(\"%x\", h.Sum(nil))\n}", "  ```\nfunc sanitize(t types.Type) string {\n\tv := reflect.ValueOf(t)\n\th := sha256.New()\n\tswitch t.Underlying().(type) {\n\tcase *types.Basic:\n\t\t// no need to dig further for basic types\n\t\th.Write([]byte(\"basic\"))\n\t\th.Write([]byte(t.String()))\n\tcase *types.Array:\n\t\tat := t.Underlying().(*types.Array)\n\t\th.Write([]byte(\"array\"))\n\t\th.Write([]byte(strconv.Itoa(at.Len())))\n\t\th.Write([]byte(sanitize(at.Elem())))\n\tcase *types.Slice:\n\t\tst := t.Underlying().(*types.Slice)\n\t\th.Write([]byte(\"slice\"))\n\t\th.Write([]byte(sanitize(st.Elem())))\n\tcase *types.Map:\n\t\tmt := t.Underlying().(*types.Map)\n\t\th.Write([]byte(\"map\"))\n\t\th.Write([]byte(sanitize(mt.Key())))\n\t\th.Write([]byte(sanitize(mt.Elem())))\n\tcase *types.Pointer:\n\t\tpt := t.Underlying().(*types.Pointer)\n\t\th.Write([]byte(\"ptr\"))\n\t\th.Write([]byte(sanitize(pt.Elem())))\n\tcase *types.Named:\n\t\tnt := t.Underlying().(*types.Named)\n\t\th.Write([]byte(\"named\"))\n\t\th.Write([]byte(sanitize(nt.Underlying())))\n\tcase *types.Struct:\n\t\t// iterate over the struct's fields\n\t\tst := t.Underlying().(*types.Struct)\n\t\th.Write([]byte(\"struct\"))\n\t\tfor i := 0; i < st.NumFields(); i++ {\n\t\t\tf := st.Field(i)\n\t\t\th.Write([]byte(sanitize(f.Type())))\n\t\t}\n\tcase *types.Interface:\n\t\tit := t.Underlying().(*types.Interface)\n\t\th.Write([]byte(\"interface\"))\n\t\tfor i := 0; i < it.NumMethods(); i++ {\n\t\t\tm := it.Method(i)\n\t\t\th.Write([]byte(sanitize(m.Type().(*types.Func).Result(0))))\n\t\t}\n\tdefault:\n\t\treturn \"\"\n\t}\n\tvar id [32]byte\n\tcopy(id[:], h.Sum(nil))\n\treturn fmt.Sprintf(\"%s_%x\", v.Type().Name(), id)\n}\n```", "  func sanitize(t types.Type) string {\n\tvar buf bytes.Buffer\n\n\t// Remove pointers.\n\tswitch p := t.(type) {\n\tcase *types.Pointer:\n\t\tt = p.Elem()\n\t}\n\t// Remove enclosing parentheses.\n\tswitch p := t.(type) {\n\tcase *types.Paren:\n\t\tt = p.Elem()\n\t}\n\n\t// Remove package qualifiers.\n\tswitch t := t.(type) {\n\tcase *types.Basic:\n\t\tbuf.WriteString(t.Name())\n\tcase *types.Named:\n\t\tbuf.WriteString(t.Obj().Name())\n\tcase *types.Pointer:\n\t\tbuf.WriteString(\"ptr_\")\n\t\tbuf.WriteString(sanitize(t.Elem()))\n\tcase *types.Slice:\n\t\tbuf.WriteString(\"slice_\")\n\t\tbuf.WriteString(sanitize(t.Elem()))\n\tcase *types.Pointer:\n\t\tbuf.WriteString(\"ptr_\")\n\t\tbuf.WriteString(sanitize(t.Elem()))\n\tcase *types.Map:\n\t\tk, err := mapCompareKey(t, t.Key())\n\t\tif err != nil {\n\t\t\tpanic(errorf(nil, t.Pos(), \"santitize: %w\", err))\n\t\t}\n\t\tbuf.WriteString(\"map_\")\n\t\tbuf.WriteString(sanitize(k))\n\t\tbuf.WriteString(\"_\")\n\t\tbuf.WriteString(sanitize(t.Elem()))\n\tcase *types.Struct:\n\t\tfor i := 0; i < t.NumFields(); i++ {\n\t\t\tf := t.Field(i)\n\t\t\tif f.Embedded() {\n\t\t\t\tbuf.WriteString(sanitize(f.Type()))\n\t\t\t}\n\t\t}\n\t}\n\treturn buf.String()\n}"]}, "weaver-internal/tool/generate/generator.go-uniqueName": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/tool/generate/generator.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generate\n\nimport (\n\t\"bytes\"\n\t\"crypto/sha256\"\n\t\"errors\"\n\t\"fmt\"\n\t\"go/ast\"\n\t\"go/format\"\n\t\"go/parser\"\n\t\"go/token\"\n\t\"go/types\"\n\t\"io\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"unicode\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/files\"\n\t\"github.com/ServiceWeaver/weaver/internal/tool\"\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/colors\"\n\t\"github.com/ServiceWeaver/weaver/runtime/version\"\n\t\"golang.org/x/exp/maps\"\n\t\"golang.org/x/tools/go/packages\"\n\t\"golang.org/x/tools/go/types/typeutil\"\n)\n\n// TODO(rgrandl): Modify the generator code to use only the types package. Right\n// now we are doing code generation relying both on the types and ast packages,\n// which can be confusing and also we might do unnecessary work.\n\nconst (\n\tgeneratedCodeFile = \"weaver_gen.go\"\n\n\tUsage = `Generate code for a Service Weaver application.\n\nUsage:\n  weaver generate [packages]\n\nDescription:\n  \"weaver generate\" generates code for the Service Weaver applications in the\n  provided packages. For example, \"weaver generate . ./foo\" will generate code\n  for the Service Weaver applications in the current directory and in the ./foo\n  directory. For every package, the generated code is placed in a weaver_gen.go\n  file in the package's directory. For example, \"weaver generate . ./foo\" will\n  create ./weaver_gen.go and ./foo/weaver_gen.go.\n\n  You specify packages for \"weaver generate\" in the same way you specify\n  packages for go build, go test, go vet, etc. See \"go help packages\" for more\n  information.\n\n  Rather than invoking \"weaver generate\" directly, you can place a line of the\n  following form in one of the .go files in the package:\n\n      //go:generate weaver generate\n\n  and then use the normal \"go generate\" command.\n\nExamples:\n  # Generate code for the package in the current directory.\n  weaver generate\n\n  # Same as \"weaver generate\".\n  weaver generate .\n\n  # Generate code for the package in the ./foo directory.\n  weaver generate ./foo\n\n  # Generate code for all packages in all subdirectories of current directory.\n  weaver generate ./...`\n)\n\n// Options controls the operation of Generate.\ntype Options struct {\n\t// If non-nil, use the specified function to report warnings.\n\tWarn func(error)\n}\n\n// Generate generates Service Weaver code for the specified packages.\n// The list of supplied packages are treated similarly to the arguments\n// passed to \"go build\" (see \"go help packages\" for details).\nfunc Generate(dir string, pkgs []string, opt Options) error {\n\tif opt.Warn == nil {\n\t\topt.Warn = func(err error) { fmt.Fprintln(os.Stderr, err) }\n\t}\n\tfset := token.NewFileSet()\n\tcfg := &packages.Config{\n\t\tMode:       packages.NeedName | packages.NeedSyntax | packages.NeedImports | packages.NeedTypes | packages.NeedTypesInfo,\n\t\tDir:        dir,\n\t\tFset:       fset,\n\t\tParseFile:  parseNonWeaverGenFile,\n\t\tBuildFlags: []string{\"--tags=ignoreWeaverGen\"},\n\t}\n\tpkgList, err := packages.Load(cfg, pkgs...)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"packages.Load: %w\", err)\n\t}\n\n\tvar automarshals typeutil.Map\n\tvar errs []error\n\tfor _, pkg := range pkgList {\n\t\tg, err := newGenerator(opt, pkg, fset, &automarshals)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t\tcontinue\n\t\t}\n\t\tif err := g.generate(); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\treturn errors.Join(errs...)\n}\n\n// parseNonWeaverGenFile parses a Go file, except for weaver_gen.go files whose\n// contents are ignored since those contents may reference types that no longer\n// exist.\nfunc parseNonWeaverGenFile(fset *token.FileSet, filename string, src []byte) (*ast.File, error) {\n\tif filepath.Base(filename) == generatedCodeFile {\n\t\treturn parser.ParseFile(fset, filename, src, parser.PackageClauseOnly)\n\t}\n\treturn parser.ParseFile(fset, filename, src, parser.ParseComments|parser.DeclarationErrors)\n}\n\ntype generator struct {\n\tpkg            *packages.Package\n\ttset           *typeSet\n\tfileset        *token.FileSet\n\tcomponents     []*component\n\tsizeFuncNeeded typeutil.Map // types that need a serviceweaver_size_* function\n\tgenerated      typeutil.Map // memo cache for generateEncDecMethodsFor\n}\n\n// errorf is like fmt.Errorf but prefixes the error with the provided position.\nfunc errorf(fset *token.FileSet, pos token.Pos, format string, args ...interface{}) error {\n\t// Rewrite the position's filename relative to the current directory. This\n\t// replaces long filenames like \"/home/foo/ServiceWeaver/weaver/weaver.go\"\n\t// with much shorter filenames like \"./weaver.go\".\n\tposition := fset.Position(pos)\n\tif cwd, err := filepath.Abs(\".\"); err == nil {\n\t\tif filename, err := filepath.Rel(cwd, position.Filename); err == nil {\n\t\t\tposition.Filename = filename\n\t\t}\n\t}\n\n\tprefix := position.String()\n\tif colors.Enabled() {\n\t\t// Color the filename red when colors are enabled.\n\t\tprefix = fmt.Sprintf(\"%s%v%s\", colors.Color256(160), position, colors.Reset)\n\t}\n\treturn fmt.Errorf(\"%s: %w\", prefix, fmt.Errorf(format, args...))\n}\n\nfunc newGenerator(opt Options, pkg *packages.Package, fset *token.FileSet, automarshals *typeutil.Map) (*generator, error) {\n\t// Abort if there were any errors loading the package.\n\tvar errs []error\n\tfor _, err := range pkg.Errors {\n\t\terrs = append(errs, err)\n\t}\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Search every file in the package for types that embed the\n\t// weaver.AutoMarshal struct.\n\ttset := newTypeSet(pkg, automarshals, &typeutil.Map{})\n\tfor _, file := range pkg.Syntax {\n\t\tfilename := fset.Position(file.Package).Filename\n\t\tif filepath.Base(filename) == generatedCodeFile {\n\t\t\t// Ignore weaver_gen.go files.\n\t\t\tcontinue\n\t\t}\n\t\tts, err := findAutoMarshals(pkg, file)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t\tcontinue\n\t\t}\n\t\tfor _, t := range ts {\n\t\t\ttset.automarshalCandidates.Set(t, struct{}{})\n\t\t}\n\t}\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Just because a type embeds weaver.AutoMarshal doesn't mean we can\n\t// automatically marshal it. Some types, like `struct { x chan int }`, are\n\t// just not serializable. Here, we check that every type that embeds\n\t// weaver.AutoMarshal is actually serializable.\n\tfor _, t := range tset.automarshalCandidates.Keys() {\n\t\tn := t.(*types.Named)\n\t\tif err := errors.Join(tset.checkSerializable(n)...); err != nil {\n\t\t\terrs = append(errs, errorf(fset, n.Obj().Pos(), \"type %v is not serializable\\n%w\", t, err))\n\t\t\tcontinue\n\t\t}\n\t\ttset.automarshals.Set(t, struct{}{})\n\t}\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Find and process all components.\n\tcomponents := map[string]*component{}\n\tfor _, file := range pkg.Syntax {\n\t\tfilename := fset.Position(file.Package).Filename\n\t\tif filepath.Base(filename) == generatedCodeFile {\n\t\t\t// Ignore weaver_gen.go files.\n\t\t\tcontinue\n\t\t}\n\n\t\tfileComponents, err := findComponents(opt, pkg, file, tset)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tfor _, c := range fileComponents {\n\t\t\t// Check for component duplicates, two components that embed the\n\t\t\t// same weaver.Implements[T].\n\t\t\t//\n\t\t\t// TODO(mwhittaker): This code relies on the fact that a component\n\t\t\t// interface and component implementation have to be in the same\n\t\t\t// package. If we lift this requirement, then this code will break.\n\t\t\tif existing, ok := components[c.fullIntfName()]; ok {\n\t\t\t\terrs = append(errs, errorf(pkg.Fset, c.impl.Obj().Pos(),\n\t\t\t\t\t\"Duplicate implementation for component %s, other declaration: %v\",\n\t\t\t\t\tc.fullIntfName(), fset.Position(existing.impl.Obj().Pos())))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcomponents[c.fullIntfName()] = c\n\t\t}\n\t}\n\n\t// Find method attributes.\n\tfor _, file := range pkg.Syntax {\n\t\tfilename := fset.Position(file.Package).Filename\n\t\tif filepath.Base(filename) == generatedCodeFile {\n\t\t\t// Ignore weaver_gen.go files.\n\t\t\tcontinue\n\t\t}\n\t\tif err := findMethodAttributes(pkg, file, components); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &generator{\n\t\tpkg:        pkg,\n\t\ttset:       tset,\n\t\tfileset:    fset,\n\t\tcomponents: maps.Values(components),\n\t}, nil\n}\n\n// findComponents returns the components in the provided file. For example,\n// findComponents will find and return the following component.\n//\n//\ttype something struct {\n//\t    weaver.Implements[SomeComponentType]\n//\t    ...\n//\t}\nfunc findComponents(opt Options, pkg *packages.Package, f *ast.File, tset *typeSet) ([]*component, error) {\n\tvar components []*component\n\tvar errs []error\n\tfor _, d := range f.Decls {\n\t\tgendecl, ok := d.(*ast.GenDecl)\n\t\tif !ok || gendecl.Tok != token.TYPE {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, spec := range gendecl.Specs {\n\t\t\tts, ok := spec.(*ast.TypeSpec)\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcomponent, err := extractComponent(opt, pkg, f, tset, ts)\n\t\t\tif err != nil {\n\t\t\t\terrs = append(errs, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif component != nil {\n\t\t\t\tcomponents = append(components, component)\n\t\t\t}\n\t\t}\n\t}\n\treturn components, errors.Join(errs...)\n}\n\nfunc findMethodAttributes(pkg *packages.Package, f *ast.File, components map[string]*component) error {\n\t// Look for declarations of the form:\n\t//\tvar _ weaver.NotRetriable = Component.Method\n\tvar errs []error\n\tfor _, decl := range f.Decls {\n\t\tgendecl, ok := decl.(*ast.GenDecl)\n\t\tif !ok || gendecl.Tok != token.VAR {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, spec := range gendecl.Specs {\n\t\t\tvalspec, ok := spec.(*ast.ValueSpec)\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttypeAndValue, ok := pkg.TypesInfo.Types[valspec.Type]\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tt := typeAndValue.Type\n\t\t\tif !isWeaverNotRetriable(t) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfor _, val := range valspec.Values {\n\t\t\t\t// We allow non-blank vars for uniformity.\n\t\t\t\tcomp, method, ok := findComponentMethod(pkg, components, val)\n\t\t\t\tif !ok {\n\t\t\t\t\terrs = append(errs, errorf(pkg.Fset, valspec.Pos(), \"weaver.NonRetriable should only be assigned a value that identifies a method of a component implemented by this package\"))\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif comp.noretry == nil {\n\t\t\t\t\tcomp.noretry = map[string]struct{}{}\n\t\t\t\t}\n\t\t\t\tcomp.noretry[method] = struct{}{}\n\t\t\t}\n\t\t}\n\t}\n\treturn errors.Join(errs...)\n}\n\n// findComponentMethod returns the component and method if val is an expression of\n// the form C.M where C is a component listed in components and C has a method named M.\nfunc findComponentMethod(pkg *packages.Package, components map[string]*component, val ast.Expr) (*component, string, bool) {\n\tsel, ok := val.(*ast.SelectorExpr)\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tctypeval, ok := pkg.TypesInfo.Types[sel.X]\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tctype, ok := ctypeval.Type.(*types.Named)\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tcname := fullName(ctype)\n\tc, ok := components[cname]\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tmethod := sel.Sel.Name\n\tfor _, m := range c.methods() {\n\t\tif m.Name() == method {\n\t\t\treturn c, method, true\n\t\t}\n\t}\n\treturn nil, \"\", false\n}\n\n// findAutoMarshals returns the types in the provided file which embed the\n// weaver.AutoMarshal struct.\nfunc findAutoMarshals(pkg *packages.Package, f *ast.File) ([]*types.Named, error) {\n\tvar automarshals []*types.Named\n\tvar errs []error\n\tfor _, decl := range f.Decls {\n\t\tgendecl, ok := decl.(*ast.GenDecl)\n\t\tif !ok || gendecl.Tok != token.TYPE {\n\t\t\t// This is not a type declaration.\n\t\t\tcontinue\n\t\t}\n\n\t\t// Recall that a type declaration can have multiple type specs. We have\n\t\t// to iterate over all of them. For example:\n\t\t//\n\t\t//     type (\n\t\t//         a struct{} // Spec 1\n\t\t//         b struct{} // Spec 2\n\t\t//     )\n\t\tfor _, spec := range gendecl.Specs {\n\t\t\ttypespec, ok := spec.(*ast.TypeSpec)\n\t\t\tif !ok {\n\t\t\t\tpanic(errorf(pkg.Fset, spec.Pos(), \"type declaration has non-TypeSpec spec: %v\", spec))\n\t\t\t}\n\n\t\t\t// Extract the type's name.\n\t\t\tdef, ok := pkg.TypesInfo.Defs[typespec.Name]\n\t\t\tif !ok {\n\t\t\t\tpanic(errorf(pkg.Fset, spec.Pos(), \"name %v not found\", typespec.Name))\n\t\t\t}\n\t\t\tn, ok := def.Type().(*types.Named)\n\t\t\tif !ok {\n\t\t\t\t// For type aliases like `type Int = int`, Int has type int and\n\t\t\t\t// not type Named. We ignore these.\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Check if the type of the expression is struct.\n\t\t\tt, ok := pkg.TypesInfo.Types[typespec.Type].Type.(*types.Struct)\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Check for an embedded weaver.AutoMarshal field.\n\t\t\tautomarshal := false\n\t\t\tfor i := 0; i < t.NumFields(); i++ {\n\t\t\t\tf := t.Field(i)\n\t\t\t\tif f.Embedded() && isWeaverAutoMarshal(f.Type()) {\n\t\t\t\t\tautomarshal = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !automarshal {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Ignore generic types. Generic types don't play well with\n\t\t\t// embedded AutoMarshals. For example, consider the following type\n\t\t\t// declaration:\n\t\t\t//\n\t\t\t//     type Register[A any] struct {\n\t\t\t//         weaver.AutoMarshal\n\t\t\t//         a A\n\t\t\t//     }\n\t\t\t//\n\t\t\t// Is Register[A] serializable? It depends on A. Plus, we cannot\n\t\t\t// really generate WeaverMarshal and WeaverUnmarshal methods for\n\t\t\t// specific instantiations of Register[A]. Because of these\n\t\t\t// complications, we ignore generic types.\n\t\t\t//\n\t\t\t// TODO(mwhittaker): Handle generics somehow?\n\t\t\tif n.TypeParams() != nil { // generics have non-nil TypeParams()\n\t\t\t\terrs = append(errs, errorf(pkg.Fset, spec.Pos(),\n\t\t\t\t\t\"generic struct %v cannot embed weaver.AutoMarshal. See serviceweaver.dev/docs.html#serializable-types for more information.\",\n\t\t\t\t\tformatType(pkg, n)))\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tautomarshals = append(automarshals, n)\n\t\t}\n\t}\n\treturn automarshals, errors.Join(errs...)\n}\n\n// extractComponent attempts to extract a component from the provided TypeSpec.\n// It returns a nil component if the TypeSpec doesn't define a component.\nfunc extractComponent(opt Options, pkg *packages.Package, file *ast.File, tset *typeSet, spec *ast.TypeSpec) (*component, error) {\n\t// Check that the type spec is of the form `type t struct {...}`.\n\ts, ok := spec.Type.(*ast.StructType)\n\tif !ok {\n\t\t// This type declaration does not involve a struct. For example, it\n\t\t// might look like `type t int`. These non-struct type declarations\n\t\t// cannot be components.\n\t\treturn nil, nil\n\t}\n\tdef, ok := pkg.TypesInfo.Defs[spec.Name]\n\tif !ok {\n\t\tpanic(errorf(pkg.Fset, spec.Pos(), \"name %v not found\", spec.Name))\n\t}\n\timpl, ok := def.Type().(*types.Named)\n\tif !ok {\n\t\t// For type aliases like `type t = struct{}`, t has type *types.Struct\n\t\t// and not type *types.Named. We ignore these.\n\t\treturn nil, nil\n\t}\n\n\t// Find any weaver.Implements[T] or weaver.WithRouter[T] embedded fields.\n\tvar intf *types.Named   // The component interface type\n\tvar router *types.Named // Router type (if any)\n\tvar isMain bool         // Is intf weaver.Main?\n\tvar refs []*types.Named // T for which weaver.Ref[T] exists in struct\n\tvar listeners []string  // Names of all listener fields declared in struct\n\tfor _, f := range s.Fields.List {\n\t\ttypeAndValue, ok := pkg.TypesInfo.Types[f.Type]\n\t\tif !ok {\n\t\t\tpanic(errorf(pkg.Fset, f.Pos(), \"type %v not found\", f.Type))\n\t\t}\n\t\tt := typeAndValue.Type\n\n\t\tif isWeaverRef(t) {\n\t\t\t// The field f has type weaver.Ref[T].\n\t\t\targ := t.(*types.Named).TypeArgs().At(0)\n\t\t\tif isWeaverMain(arg) {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"components cannot contain a reference to weaver.Main\")\n\t\t\t}\n\t\t\tnamed, ok := arg.(*types.Named)\n\t\t\tif !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Ref argument %s is not a named type.\",\n\t\t\t\t\tformatType(pkg, arg))\n\t\t\t}\n\t\t\trefs = append(refs, named)\n\t\t} else if isWeaverListener(t) {\n\t\t\tlis, err := getListenerNamesFromStructField(pkg, f)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tlisteners = append(listeners, lis...)\n\t\t}\n\n\t\tif len(f.Names) != 0 {\n\t\t\t// Ignore unembedded fields.\n\t\t\t//\n\t\t\t// TODO(mwhittaker): Warn the user about unembedded\n\t\t\t// weaver.Implements, weaver.WithConfig, or weaver.WithRouter?\n\t\t\tcontinue\n\t\t}\n\n\t\tswitch {\n\t\t// The field f is an embedded weaver.Implements[T].\n\t\tcase isWeaverImplements(t):\n\t\t\t// Check that T is a named interface type inside the package.\n\t\t\targ := t.(*types.Named).TypeArgs().At(0)\n\t\t\tnamed, ok := arg.(*types.Named)\n\t\t\tif !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Implements argument %s is not a named type.\",\n\t\t\t\t\tformatType(pkg, arg))\n\t\t\t}\n\t\t\tisMain = isWeaverMain(arg)\n\t\t\tif !isMain && named.Obj().Pkg() != pkg.Types {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Implements argument %s is a type outside the current package. A component interface and implementation must be in the same package. If you can't move them into the same package, you can add `type %s %v` to the implementation's package and embed `weaver.Implements[%s]` instead of `weaver.Implements[%s]`.\",\n\t\t\t\t\tformatType(pkg, named), named.Obj().Name(), formatType(pkg, named), named.Obj().Name(), formatType(pkg, named))\n\t\t\t}\n\t\t\tif _, ok := named.Underlying().(*types.Interface); !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Implements argument %s is not an interface.\",\n\t\t\t\t\tformatType(pkg, named))\n\t\t\t}\n\t\t\tintf = named\n\n\t\t// The field f is an embedded weaver.WithRouter[T].\n\t\tcase isWeaverWithRouter(t):\n\t\t\t// Check that T is a named type inside the package.\n\t\t\targ := t.(*types.Named).TypeArgs().At(0)\n\t\t\tnamed, ok := arg.(*types.Named)\n\t\t\tif !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.WithRouter argument %s is not a named type.\",\n\t\t\t\t\tformatType(pkg, arg))\n\t\t\t}\n\t\t\tif named.Obj().Pkg() != pkg.Types {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.WithRouter argument %s is a type outside the current package.\",\n\t\t\t\t\tformatType(pkg, named))\n\t\t\t}\n\t\t\trouter = named\n\t\t}\n\t}\n\n\tif intf == nil {\n\t\t// TODO(mwhittaker): Warn the user if they embed weaver.WithRouter or\n\t\t// weaver.WithConfig but don't embed weaver.Implements.\n\t\treturn nil, nil\n\t}\n\n\t// Check that that the component implementation implements the component\n\t// interface.\n\tif !types.Implements(types.NewPointer(impl), intf.Underlying().(*types.Interface)) {\n\t\treturn nil, errorf(pkg.Fset, spec.Pos(),\n\t\t\t\"type %s embeds weaver\n```\n# Function Signature: func uniqueName(t types.Type) string {\n# Natural Language Annotations: // uniqueName returns a unique pretty printed representation of the provided\n// type (e.g., \"int\", \"map[int]bool\"). The key property is that if u != t, then\n// uniqueName(u) != uniqueName(t).\n//\n// Note that types.TypeString returns a pretty printed representation of a\n// string, but it is not guaranteed to be unique. For example, if have `type\n// int bool`, then TypeString returns \"int\" for both the named type int and the\n// primitive type int.\nPlease return the generated code surrounded by ```\n", "response": ["  int bool\").\n//\n// The returned string uses the pretty printed representation of types used by\n// the `go/ast` package.\nfunc uniqueName(t types.Type) string {\n\t// Identifiers are unique per package. So, an identifier \"I\" in package A is not\n\t// the same as an identifier \"I\" in package B.\n\tif ident, ok := t.(*types.Named); ok && ident.Obj().Pkg() != nil {\n\t\treturn ident.Obj().Pkg().Path() + \".\" + ident.Obj().Name()\n\t}\n\treturn types.TypeString(t, nil)\n}", "  func uniqueName(t types.Type) string {\n    if t.String() == \"\" {\n        // TODO(rgrandl): panic is not the best answer here - this could\n        // happen if the type is eg a struct that implements a method on\n        // another package (it would have a canonical string value but\n        // still be an underlying type)\n        panic(errorf(fileSet, t.Pos(), \"type String() is the zero value\"))\n    }\n    switch t := t.(type) {\n    case *types.Named:\n        s := t.Obj().Name()\n        if n, ok := t.Underlying().(*types.Struct); ok { // named structs\n            for i := 0; i < n.NumFields(); i++ {\n                s += fmt.Sprintf(\"_%s%v\", n.Field(i).Name(), n.Field(i).Type())\n            }\n        }\n        return s\n    default: // other types\n        return fmt.Sprintf(\"%v\", t)\n    }\n}\n```", "  \"#typeconvert\" ( Related information )\n\n```go\nfunc uniqueName(t types.Type) string {\n\t// Record the previously seen types and their unique names.\n\tseen := map[string]string{}\n\n\t// A helper function to get the unique name of the provided type,\n\t// recording it in the seen map if needed.\n\tvar name string\n\tvar appendName func(t types.Type) string\n\tappendName = func(t types.Type) string {\n\t\ttn := t.String() // A unique identifier for the type.\n\t\tif n, ok := seen[tn]; ok {\n\t\t\t// The type t has been encountered before. Return its previously\n\t\t\t// assigned name.\n\t\t\treturn n\n\t\t}\n\t\t// The type t is new. Assign it a new unique name in the\n\t\t// generated code.\n\t\tn := fmt.Sprintf(\"serviceweaver.%s\", t.String())\n\t\tseen[tn] = n\n\t\treturn n\n\t}\n\n\t// Return the unique name of the provided type.\n\tswitch t := t.(type) {\n\tcase *types.Basic:\n\t\t// For basic types, we use TypeString. This will generate a\n\t\t// unique name for each named types (that is, it does not use\n\t\t// their underlying names).\n\t\tname = t.TypeString()\n\t\t// For basic types, we don't need to record them as seen types\n\t\t// since they cannot be elements of these slices.\n\tcase *types.Named:\n\t\t// For named types, we use the name originally assigned to the\n\t\t// type. This will generate unique names for each named types.\n\t\tname = t.Obj().Name()\n\t\t// Record the name of the named type as already seen to avoid\n\t\t// duplicates.\n\t\tseen[t.Obj().Name()] = t.Obj().Name()\n\tcase *types.Struct:\n\t\t// For structs, we use the concatenation of all their field\n\t\t// names and types. Field names must be uniquely associated\n\t\t// with their types.\n\t\tvar fields []string\n\t\tfor i := 0; i < t.NumFields(); i++ {\n\t\t\tf := t.Field(i)\n\t\t\ttn := f.Type().String() // A unique identifier for the type.\n\t\t\tif n, ok := seen[tn]; ok {\n\t\t\t\t// The field f has been encountered before. Use the\n\t\t\t\t// name assigned to its type.\n\t\t\t\tfield := fmt.Sprintf(\"%s_%s\", f.Name(), n)\n\t\t\t\tfields = append(fields, field)\n\t\t\t} else {\n\t\t\t\t// The type of the field f has been encountered\n\t\t\t\t// before. Record it in the seen map. Assign a\n\t\t\t\t// new unique name to it.\n\t\t\t\ttn2 := appendName(f.Type())\n\t\t\t\tfield := fmt.Sprintf(\"%s_%s\", f.Name(), tn2)\n\t\t\t\tfields = append(fields, field)\n\t\t\t}\n\t\t}\n\t\tsort.Strings(fields)\n\t\tname = strings.Join(fields, \"+\")\n\tcase *types.Pointer:\n\t\t// For pointers, we use their element's type name.\n\t\tname = appendName(t.Elem())\n\tcase *types.Tuple:\n\t\t// For tuples, we use the concatenation of all their element\n\t\t// types. We cannot use TypeString here, since tuples with the\n\t\t// same types but in a different order are considered different.\n\t\tvar types []string\n\t\tfor i := 0; i < t.Len(); i++ {\n\t\t\ttn := t.At(i).Type().String() // A unique identifier for the type.\n\t\t\tif n, ok := seen[tn]; ok {\n\t\t\t\t// The element at i has been encountered before. Use\n\t\t\t\t// the name assigned to its type.\n\t\t\t\ttypes = append(types, n)\n\t\t\t} else {\n\t\t\t\t// The type of the element at i has been encountered\n\t\t\t\t// before. Record it in the seen map. Assign a new\n\t\t\t\t// unique name to it."]}, "weaver-internal/tool/certs/certs.go-GenerateSignedCert": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/tool/certs/certs.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage certs\n\nimport (\n\t\"bytes\"\n\t\"crypto\"\n\t\"crypto/rand\"\n\t\"crypto/rsa\"\n\t\"crypto/x509\"\n\t\"crypto/x509/pkix\"\n\t\"encoding/pem\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math/big\"\n\t\"time\"\n)\n\n// GenerateCACert generates a self-signed CA certificate and a corresponding\n// private key.\n//\n// The returned certificate has a one-year validity and is attributed to a fake\n// authority. As such, it should only ever be used on a temporary basis and for\n// in-process certificate signing.\nfunc GenerateCACert() (*x509.Certificate, crypto.PrivateKey, error) {\n\treturn generateLeafCert(true /*isCA*/, \"ca\")\n}\n\n// GenerateSignedCert generates a certificate for the given DNS names, signed\n// by the given Certificate Authority, and a corresponding private key.\n//\n// The returned certificate has a one-year validity and should only ever\n// be used on a temporary basis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc generateLeafCert(isCA bool, names ...string) (*x509.Certificate, crypto.PrivateKey, error) {\n\tpriv, err := rsa.GenerateKey(rand.Reader, 4096)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tserialNumberLimit := new(big.Int).Lsh(big.NewInt(1), 128)\n\tserialNumber, err := rand.Int(rand.Reader, serialNumberLimit)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tkeyUsage := x509.KeyUsageDigitalSignature | x509.KeyUsageKeyEncipherment\n\tif isCA {\n\t\tkeyUsage |= x509.KeyUsageCertSign\n\t}\n\ttemplate := x509.Certificate{\n\t\tSerialNumber: serialNumber,\n\t\tSubject:      pkix.Name{Organization: []string{\"ACME Co.\"}},\n\t\tNotBefore:    time.Now(),\n\t\tNotAfter:     time.Now().Add(365 * 24 * time.Hour),\n\t\tKeyUsage:     keyUsage,\n\t\tExtKeyUsage: []x509.ExtKeyUsage{\n\t\t\tx509.ExtKeyUsageClientAuth,\n\t\t\tx509.ExtKeyUsageServerAuth,\n\t\t},\n\t\tBasicConstraintsValid: true,\n\t\tIsCA:                  isCA,\n\t\tDNSNames:              names,\n\t}\n\tcertDER, err := x509.CreateCertificate(rand.Reader, &template, &template, &priv.PublicKey, priv)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tcert, err := x509.ParseCertificate(certDER)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn cert, priv, nil\n}\n\n// PEMEncode returns the PEM-encoded blocks for the given certificate and\n// private key.\nfunc PEMEncode(cert *x509.Certificate, key crypto.PrivateKey) ([]byte, []byte, error) {\n\tkeyDER, err := x509.MarshalPKCS8PrivateKey(key)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tvar certOut bytes.Buffer\n\tif err := pem.Encode(&certOut, &pem.Block{\n\t\tType:  \"CERTIFICATE\",\n\t\tBytes: cert.Raw,\n\t}); err != nil {\n\t\treturn nil, nil, err\n\t}\n\tvar keyOut bytes.Buffer\n\tif err := pem.Encode(&keyOut, &pem.Block{\n\t\tType:  \"PRIVATE KEY\",\n\t\tBytes: keyDER,\n\t}); err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn certOut.Bytes(), keyOut.Bytes(), nil\n}\n\n// VerifySignedCert verifies the given signed certificate using the given\n// root CA, returning the DNS names stored in the leaf certificate.\nfunc VerifySignedCert(certDER []byte, ca *x509.Certificate) ([]string, error) {\n\tcaCertPool := x509.NewCertPool()\n\tcaCertPool.AddCert(ca)\n\topts := x509.VerifyOptions{\n\t\tRoots:       caCertPool,\n\t\tCurrentTime: time.Now(),\n\t}\n\n\tcert, err := x509.ParseCertificate(certDER)\n\tif err != nil {\n\t\treturn nil, errors.New(\"bad certificate\")\n\t}\n\tverifiedCerts, err := cert.Verify(opts)\n\tif err != nil {\n\t\treturn nil, errors.New(\"couldn't verify certificate chain\")\n\t}\n\tif len(verifiedCerts) != 1 {\n\t\treturn nil, fmt.Errorf(\"expected a single verified chain, got %d\", len(verifiedCerts))\n\t}\n\tverifiedCert := verifiedCerts[0]\n\tif len(verifiedCert) < 1 { // should never happen\n\t\treturn nil, fmt.Errorf(\"empty verified chain\")\n\t}\n\treturn verifiedCert[0].DNSNames, nil\n}\n\n```\n# Function Signature: func GenerateSignedCert(ca *x509.Certificate, caKey crypto.PrivateKey, names ...string) (*x509.Certificate, crypto.PrivateKey, error) {\n# Natural Language Annotations: // GenerateSignedCert generates a certificate for the given DNS names, signed\n// by the given Certificate Authority, and a corresponding private key.\n//\n// The returned certificate has a one-year validity and should only ever\n// be used on a temporary basis.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc GenerateSignedCert(ca *x509.Certificate, caKey crypto.PrivateKey, names ...string) (*x509.Certificate, crypto.PrivateKey, error) {\n\tpriv, err := rsa.GenerateKey(rand.Reader, 4096)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tserialNumberLimit := new(big.Int).Lsh(big.NewInt(1), 128)\n\tserialNumber, err := rand.Int(rand.Reader, serialNumberLimit)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tkeyUsage := x509.KeyUsageDigitalSignature | x509.KeyUsageKeyEncipherment\n\ttemplate := x509.Certificate{\n\t\tSerialNumber: serialNumber,\n\t\tSubject:      pkix.Name{Organization: []string{\"ACME Co.\"}},\n\t\tNotBefore:    time.Now(),\n\t\tNotAfter:     time.Now().Add(365 * 24 * time.Hour),\n\t\tKeyUsage:     keyUsage,\n\t\tExtKeyUsage: []x509.ExtKeyUsage{\n\t\t\tx509.ExtKeyUsageClientAuth,\n\t\t\tx509.ExtKeyUsageServerAuth,\n\t\t},\n\t\tBasicConstraintsValid: true,\n\t\tIsCA:                  false,\n\t\tDNSNames:              names,\n\t}\n\tderBytes, err := x509.CreateCertificate(rand.Reader, &template, ca, &priv.PublicKey, caKey)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tcert, err := x509.ParseCertificate(derBytes)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn cert, priv, nil\n}\n```", "  ```\nfunc GenerateSignedCert(ca *x509.Certificate, caKey crypto.PrivateKey, names ...string) (*x509.Certificate, crypto.PrivateKey, error) {\n\tpriv, err := rsa.GenerateKey(rand.Reader, 4096)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tserialNumberLimit := new(big.Int).Lsh(big.NewInt(1), 128)\n\tserialNumber, err := rand.Int(rand.Reader, serialNumberLimit)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tkeyUsage := x509.KeyUsageDigitalSignature | x509.KeyUsageKeyEncipherment\n\ttemplate := x509.Certificate{\n\t\tSerialNumber: serialNumber,\n\t\tSubject:      pkix.Name{Organization: []string{\"ACME Co.\"}},\n\t\tNotBefore:    time.Now(),\n\t\tNotAfter:     time.Now().Add(1 * 24 * time.Hour),\n\t\tKeyUsage:     keyUsage,\n\t\tExtKeyUsage: []x509.ExtKeyUsage{\n\t\t\tx509.ExtKeyUsageClientAuth,\n\t\t\tx509.ExtKeyUsageServerAuth,\n\t\t},\n\t\tBasicConstraintsValid: true,\n\t\tDNSNames:              names,\n\t}\n\tcertDER, err := x509.CreateCertificate(rand.Reader, &template, ca, &priv.PublicKey, caKey)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tcert, err := x509.ParseCertificate(certDER)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn cert, priv, nil\n}\n\n```", "  ```\nfunc GenerateSignedCert(ca *x509.Certificate, caKey crypto.PrivateKey, names ...string) (*x509.Certificate, crypto.PrivateKey, error) {\n    // Generate a new key pair\n    priv, err := rsa.GenerateKey(rand.Reader, 4096)\n    if err != nil {\n        return nil, nil, err\n    }\n\n    // Generate a new certificate that is a copy of the CA certificate\n    notBefore := time.Now()\n    notAfter := notBefore.Add(365 * 24 * time.Hour)\n    cert := &x509.Certificate{\n        SerialNumber: big.NewInt(0),\n        Subject: pkix.Name{\n            CommonName: names[0],\n        },\n        NotBefore: notBefore,\n        NotAfter:  notAfter,\n\n        KeyUsage:    x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature,\n        ExtKeyUsage: []x509.ExtKeyUsage{x509.ExtKeyUsageServerAuth},\n\n        BasicConstraintsValid: true,\n    }\n\n    // Add the requested DNS names to the certificate\n    for _, name := range names {\n        if ip := net.ParseIP(name); ip != nil {\n            cert.IPAddresses = append(cert.IPAddresses, ip)\n        } else {\n            cert.DNSNames = append(cert.DNSNames, name)\n        }\n    }\n\n    // Sign the certificate using the CA's key\n    derBytes, err := x509.CreateCertificate(rand.Reader, cert, ca, &priv.PublicKey, caKey)\n    if err != nil {\n        return nil, nil, err\n    }\n\n    // Parse the generated certificate\n    cert, err = x509.ParseCertificate(derBytes)\n    if err != nil {\n        return nil, nil, err\n    }\n\n    // Return the signed certificate and private key\n    return cert, priv, nil\n}\n```"]}, "weaver-internal/weaver/logger.go-run": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/weaver/logger.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage weaver\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/logging\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n)\n\n// remoteLogger collects log entries into batches and sends these batches to a\n// specified function.\ntype remoteLogger struct {\n\tc        chan *protos.LogEntry\n\tfallback io.Writer              // Fallback destination when dst() returns an error\n\tpp       *logging.PrettyPrinter // Used when sending to dst fails\n}\n\nconst logBufferCount = 1000\n\nfunc newRemoteLogger(fallback io.Writer) *remoteLogger {\n\trl := &remoteLogger{\n\t\tc:        make(chan *protos.LogEntry, logBufferCount),\n\t\tfallback: fallback,\n\t\tpp:       logging.NewPrettyPrinter(false),\n\t}\n\treturn rl\n}\n\nfunc (rl *remoteLogger) log(entry *protos.LogEntry) {\n\t// TODO(sanjay): Drop if too many entries are buffered?\n\trl.c <- entry\n}\n\n// run collects log entries passed to log() and, and passes theme to dst. At\n// most one call to dst is outstanding at a time. Log entries that arrive while\n// a call is in progress are buffered and sent in the next call.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (rl *remoteLogger) run(ctx context.Context, dst func(context.Context, *protos.LogEntryBatch) error) {\n# Natural Language Annotations: // run collects log entries passed to log() and, and passes theme to dst. At\n// most one call to dst is outstanding at a time. Log entries that arrive while\n// a call is in progress are buffered and sent in the next call.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\npackage weaver\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/logging\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n)\n\n// remoteLogger collects log entries into batches and sends these batches to a\n// specified function.\ntype remoteLogger struct {\n\tc        chan *protos.LogEntry\n\tfallback io.Writer              // Fallback destination when dst() returns an error\n\tpp       *logging.PrettyPrinter // Used when sending to dst fails\n}\n\nconst logBufferCount = 1000\n\nfunc newRemoteLogger(fallback io.Writer) *remoteLogger {\n\trl := &remoteLogger{\n\t\tc:        make(chan *protos.LogEntry, logBufferCount),\n\t\tfallback: fallback,\n\t\tpp:       logging.NewPrettyPrinter(false),\n\t}\n\treturn rl\n}\n\nfunc (rl *remoteLogger) log(entry *protos.LogEntry) {\n\t// TODO(sanjay): Drop if too many entries are buffered?\n\trl.c <- entry\n}\n\n// run collects log entries passed to log() and, and passes them to dst. At most\n// one call to dst is outstanding at a time. Log entries that arrive while a\n// call is in progress are buffered and sent in the next call.\nfunc (rl *remoteLogger) run(ctx context.Context, dst func(context.Context, *protos.LogEntryBatch) error) {\n\tbatch := protos.LogEntryBatch{}\n\tfor {\n\t\tselect {\n\t\tcase entry := <-rl.c:\n\t\t\tbatch = append(batch, entry)\n\t\t\tif len(batch) == logBufferCount {\n\t\t\t\terr := rl.sendBatch(ctx, dst, batch)\n\t\t\t\tif err != nil {\n\t\t\t\t\trl.fallback.Write(rl.pp.Sprint(batch))\n\t\t\t\t}\n\t\t\t}\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (rl *remoteLogger) sendBatch(ctx context.Context, dst func(context.Context, *protos.LogEntryBatch) error, batch protos.LogEntryBatch) error {\n\terr := dst(ctx, &batch)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbatch = batch[:0]\n\treturn nil\n}\n```", "  ```\nfunc (rl *remoteLogger) run(ctx context.Context, dst func(context.Context, *protos.LogEntryBatch) error) {\n\tvar err error\n\tvar entries []*proto.LogEntry\n\n\t// Receive log entries into buffer\n\tfor {\n\t\tentry, ok := <-rl.c\n\t\tif !ok {\n\t\t\t// Channel closed, shut down logger\n\t\t\tbreak\n\t\t}\n\t\tentries = append(entries, entry)\n\t}\n\n\t// Send buffered entries to dst\n\tbatch := &proto.LogEntryBatch{\n\t\tEntries: entries,\n\t}\n\terr = dst(ctx, batch)\n\tif err != nil {\n\t\t// Failed to send entries to dst, fall back to fallback writer\n\t\trl.fallback.Write()\n\t}\n}\n```", "  ```\npackage weaver\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/logging\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n)\n\n// remoteLogger collects log entries into batches and sends these batches to a\n// specified function.\ntype remoteLogger struct {\n\tc        chan *protos.LogEntry\n\tfallback io.Writer              // Fallback destination when dst() returns an error\n\tpp       *logging.PrettyPrinter // Used when sending to dst fails\n}\n\nconst logBufferCount = 1000\n\nfunc newRemoteLogger(fallback io.Writer) *remoteLogger {\n\trl := &remoteLogger{\n\t\tc:        make(chan *protos.LogEntry, logBufferCount),\n\t\tfallback: fallback,\n\t\tpp:       logging.NewPrettyPrinter(false),\n\t}\n\treturn rl\n}\n\nfunc (rl *remoteLogger) log(entry *protos.LogEntry) {\n\t// TODO(sanjay): Drop if too many entries are buffered?\n\trl.c <- entry\n}\n\n// run collects log entries passed to log() and, and passes theme to dst. At\n// most one call to dst is outstanding at a time. Log entries that arrive while\n// a call is in progress are buffered and sent in the next call.\nfunc (rl *remoteLogger) run(ctx context.Context, dst func(context.Context, *protos.LogEntryBatch) error) {\n\tfor {\n\t\tselect {\n\t\tcase ent := <-rl.c:\n\t\t\tvar b *protos.LogEntryBatch\n\t\t\tvar err error\n\t\t\t// If this is the first entry, this is the start of a new batch.\n\t\t\tif b == nil {\n\t\t\t\tb = &protos.LogEntryBatch{}\n\t\t\t\tb.Entries = append(b.Entries, ent)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// If dst returns an error, write to fallback.\n\t\t\tif err = dst(ctx, b); err != nil {\n\t\t\t\trl.pp.Fprintf(rl.fallback, \"%v\\n\", ent)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tb.Entries = append(b.Entries, ent)\n\t\t}\n\t}\n}\n```"]}, "weaver-internal/weaver/load.go-updateAssignment": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/weaver/load.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage weaver\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/DataDog/hyperloglog\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/lightstep/varopt\"\n)\n\nfunc approxEqual(a, b float64) bool {\n\tconst float64EqualityThreshold = 1e-9\n\treturn math.Abs(a-b) <= float64EqualityThreshold\n}\n\n// TODO(mwhittaker): Right now, load collection is slow. It grabs a mutex\n// every time the load needs to be updated. Make this faster.\n\n// loadCollector collects load for a Service Weaver component. As an example, imagine we\n// have a load collector lc for a Service Weaver component that owns slices [0, 10) and\n// [100, 200). We add the following load over the course of a second.\n//\n//   - lc.Add(0, 1)\n//   - lc.Add(1, 1)\n//   - lc.Add(2, 1)\n//   - lc.Add(3, 1)\n//   - lc.Add(100, 1)\n//   - lc.Add(101, 1)\n//\n// The load collector will report a load of 4 requests per second on the slice\n// [0, 10) and a load of 2 requests per second on the slice [100, 200).\ntype loadCollector struct {\n\tcomponent string           // Service Weaver component\n\taddr      string           // dialable address found in assignments\n\tnow       func() time.Time // time.Now usually, but injected fake in tests\n\n\tmu         sync.Mutex               // guards the following fields\n\tassignment *protos.Assignment       // latest assignment\n\tindex      index                    // index on assignment\n\tstart      time.Time                // start of load collection\n\tslices     map[uint64]*sliceSummary // keyed by start of slice\n}\n\n// sliceSummary contains a summary of the observed keys and load of a slice for\n// a replica.\ntype sliceSummary struct {\n\tslice  slice                    // the slice\n\tload   float64                  // total load\n\tcount  *hyperloglog.HyperLogLog // counts distinct elements\n\tsample *varopt.Varopt[uint64]   // reservoir sample of keys\n}\n\n// newLoadCollector returns a new load collector. Note that load is collected\n// with respect to an assignment, so load won't be collected until\n// UpdateAssignment is called.\nfunc newLoadCollector(component string, addr string) *loadCollector {\n\treturn &loadCollector{\n\t\tcomponent: component,\n\t\taddr:      addr,\n\t\tnow:       func() time.Time { return time.Now() },\n\t\tstart:     time.Now(),\n\t\tslices:    map[uint64]*sliceSummary{},\n\t}\n}\n\n// add adds load for the provided key.\nfunc (lc *loadCollector) add(key uint64, v float64) error {\n\tif v != 1.0 {\n\t\tpanic(\"load != 1.0 not yet implemented\")\n\t}\n\n\t// Find the corresponding slice.\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tif lc.assignment == nil {\n\t\t// Load is reported with respect to a given assignment. If we don't\n\t\t// have an assignment yet, then we don't record the load.\n\t\treturn nil\n\t}\n\tslice, found := lc.index.find(key)\n\tif !found {\n\t\t// TODO(mwhittaker): It is currently possible to receive a request for\n\t\t// a key that is not in our current assignment. For example, a\n\t\t// different weavelet may have an older or newer version of the current\n\t\t// assignment and send us keys not in our current assignment. In the\n\t\t// future, we may want to catch these requests and discard them. For\n\t\t// now, we execute them.\n\t\treturn nil\n\t}\n\tif !slice.replicaSet[lc.addr] {\n\t\treturn nil\n\t}\n\n\tsummary, found := lc.slices[slice.start]\n\tif !found {\n\t\tvar err error\n\t\tsummary, err = newSliceSummary(slice)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlc.slices[slice.start] = summary\n\t}\n\n\t// Update the load.\n\tsummary.load += v\n\n\t// Update the count. Note that we compute a hash of our key before passing\n\t// it to the hyperloglog, even though the key is itself a hash. The reason\n\t// is that this slice represents only a small sliver of the total hash\n\t// space. To operate correctly, a hyperloglog assumes values are drawn\n\t// uniformly from the space of all uint32s, so if we feed the hyperloglog\n\t// values only from this slice, the count will be inaccurate.\n\t//\n\t// TODO(mwhittaker): Compute the hash outside of the lock?\n\t// TODO(mwhittaker): Use a different sketch?\n\t// TODO(mwhittaker): If the slice is small (< 1024), we can count the\n\t// number of distinct elements exactly. Don't use a hyperloglog here.\n\t// TODO(mwhittaker): Start with an exact count and only switch to a\n\t// hyperloglog if the number of unique elements gets too big?\n\tsummary.count.Add(hyperloglog.Murmur64(key))\n\n\t// Update the sample. Note that Add takes in a key and a weight, but we are\n\t// recording unweighted samples, so we use a constant weight of 1.0 for\n\t// every key.\n\tif _, err := summary.sample.Add(key, 1.0); err != nil {\n\t\treturn fmt.Errorf(\"cannot sample %d: %v\", key, err)\n\t}\n\treturn nil\n}\n\n// updateAssignment updates a load collector with the latest assignment. The\n// load reported by a load collector is always scoped to a single assignment.\n// A load report never spans more than one assignment. Thus, UpdateAssignment\n// also clears the load collector's accumulated load.\n\n\n\n\n\n\n\n\n\n\n// report returns a report of the collected load. If the load collector\n// doesn't have any collected load---this is possible if the load collector\n// doesn't have an assignment yet---then Report returns nil.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// reset resets the load collector. If you want to collect load over 5\n// minute windows, for example, call Reset every five minutes.\nfunc (lc *loadCollector) reset() {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tlc.start = lc.now()\n\tlc.slices = map[uint64]*sliceSummary{}\n}\n\n// newSliceSummary returns a new sliceSummary for the provided slice with\n// initially 0 load.\nfunc newSliceSummary(slice slice) (*sliceSummary, error) {\n\t// Initialize the hyperloglog. A hyperloglog with n registers uses roughly\n\t// n bytes of memory. We choose n=1024 so that every hyperloglog takes\n\t// about a kilobyte of memory. Given that a weavelet should manage a\n\t// moderate number of slices and components, the total memory usage of all\n\t// hyperloglogs should be relatively small. New's documentation also\n\t// suggests that n be a power of 2.\n\tcount, err := hyperloglog.New(1024)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Initialize the reservoir sample. A reservoir sample of size n stores at\n\t// most n keys, or roughly 8n bytes. As with the hyperloglogs, this should\n\t// lead to a modest memory usage.\n\t//\n\t// TODO(mwhittaker): Compute the expected errors in our estimates based on\n\t// the size of the sample.\n\t// TODO(mwhittaker): When we switch to range sharding, keys might be large\n\t// and 1000 keys might be too big.\n\tr := rand.New(rand.NewSource(time.Now().UnixNano()))\n\tsample := varopt.New[uint64](1000, r)\n\n\treturn &sliceSummary{slice: slice, count: count, sample: sample}, nil\n}\n\n// splits splits the slice into subslices with roughly even load.\nfunc (s *sliceSummary) splits(delta time.Duration) []*protos.LoadReport_SubsliceLoad {\n\t// Splits divides the slice into subslices of roughly even load. In the\n\t// normal case, Splits splits a slice into 20 subslices, each representing\n\t// 5% of the total load. If the number of samples is small, however, fewer\n\t// splits are used. Moreover, if adjacent splits are formed from a single\n\t// hot key, they are combined.\n\n\t// Materialize and sort the sample.\n\tk := s.sample.Size()\n\txs := make([]uint64, k)\n\tfor i := 0; i < k; i++ {\n\t\tx, _ := s.sample.Get(i)\n\t\txs[i] = x\n\t}\n\tsort.Slice(xs, func(i, j int) bool { return xs[i] < xs[j] })\n\n\t// Determine the number of splits. More splits is better, but if we don't\n\t// have many points in our sample, then using a large number of splits will\n\t// lead to inaccurate estimates.\n\tvar n int\n\tswitch {\n\tcase k < 10:\n\t\tn = 1 // 100%\n\tcase k < 50:\n\t\tn = 2 // 50%\n\tcase k < 100:\n\t\tn = 4 // 25%\n\tcase k < 250:\n\t\tn = 5 // 20%\n\tcase k < 500:\n\t\tn = 10 // 10%\n\tdefault:\n\t\tn = 20 // 5%\n\t}\n\n\t// Adjust the first subslice so that it starts at our slice boundary.\n\ttotalLoad := s.load / delta.Seconds()\n\tsplits := subslices(totalLoad, xs, n)\n\tsplits[0].Start = s.slice.start\n\n\t// Double check that the split loads sum to the total load.\n\tvar sum float64\n\tfor _, split := range splits {\n\t\tsum += split.Load\n\t}\n\tif !approxEqual(sum, totalLoad) {\n\t\tpanic(fmt.Sprintf(\"bad sum of split loads: got %f, want %f\", sum, totalLoad))\n\t}\n\n\treturn splits\n}\n\n// subslices returns n splits of the provided points with roughly the same\n// load. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}, n =\n// 4, and a load of 10.0, subslices will return the following four splits:\n//\n//   - {Start: 10, Load: 2.5} // [10, 30)\n//   - {Start: 30, Load: 2.5} // [30, 50)\n//   - {Start: 50, Load: 2.5} // [50, 70)\n//   - {Start: 70, Load: 2.5} // [70, infinity)\n//\n// The returned splits are as even as possible on a best effort basis.\n// subslices only guarantees that the returned splits are contiguous and\n// sorted.\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// percentiles returns n equally spaced percentiles of the provided sorted set\n// of points. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}\n// and n = 4, percentiles will return []uint64{10, 30, 50, 70} where\n//\n//   - 10 is the 0th percentile,\n//   - 30 is the 25th percentile,\n//   - 50 is the 50th percentile,\n//   - 70 is the 75th percentile,\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\nfunc percentiles(xs []uint64, n int) []uint64 {\n\tps := make([]uint64, n)\n\tfor i := 0; i < n; i++ {\n\t\tps[i] = xs[int(float64(i)/float64(n)*float64(len(xs)))]\n\t}\n\treturn ps\n}\n\n// index is a read-only search index of a protos.Assignment, optimized to\n// find the slice that contains a key.\ntype index []slice\n\n// slice is the segment [start, end) of the key space, along with a set of\n// assigned replicas.\ntype slice struct {\n\tstart      uint64          // start of slice, inclusive\n\tend        uint64          // end of slice, exclusive\n\treplicas   []string        // replicas assigned to this slice\n\treplicaSet map[string]bool // replicas assigned to this slice\n}\n\n// newIndex returns a new index of the provided assignment.\nfunc newIndex(proto *protos.Assignment) index {\n\tn := len(proto.Slices)\n\tslices := make([]slice, n)\n\tfor i := 0; i < n; i++ {\n\t\t// Gather the set of replicas.\n\t\treplicas := proto.Slices[i].Replicas\n\t\treplicaSet := make(map[string]bool, len(replicas))\n\t\tfor _, replica := range replicas {\n\t\t\treplicaSet[replica] = true\n\t\t}\n\n\t\t// Compute the end of the slice.\n\t\tvar end uint64 = math.MaxUint64\n\t\tif i < n-1 {\n\t\t\tend = proto.Slices[i+1].Start\n\t\t}\n\n\t\t// Form the slice.\n\t\tslices[i] = slice{\n\t\t\tstart:      proto.Slices[i].Start,\n\t\t\tend:        end,\n\t\t\treplicas:   replicas,\n\t\t\treplicaSet: replicaSet,\n\t\t}\n\t}\n\treturn slices\n}\n\n// find finds the slice that contains the given key in O(log n) time where n is\n// the number of slices in the assignment.\nfunc (ind index) find(key uint64) (slice, bool) {\n\ti := sort.Search(len(ind), func(i int) bool {\n\t\treturn key < ind[i].end\n\t})\n\tif i == len(ind) {\n\t\treturn slice{}, false\n\t}\n\treturn ind[i], true\n}\n\n```\n# Function Signature: func (lc *loadCollector) updateAssignment(assignment *protos.Assignment) {\n# Natural Language Annotations: // updateAssignment updates a load collector with the latest assignment. The\n// load reported by a load collector is always scoped to a single assignment.\n// A load report never spans more than one assignment. Thus, UpdateAssignment\n// also clears the load collector's accumulated load.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (lc *loadCollector) updateAssignment(assignment *protos.Assignment) {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tlc.assignment = assignment\n\tlc.index = newIndex(assignment)\n\tlc.start = lc.now()\n\tlc.slices = map[uint64]*sliceSummary{}\n}\n```", "  ```\nfunc (lc *loadCollector) updateAssignment(assignment *protos.Assignment) {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tlc.assignment = assignment\n\tlc.index = newIndex(assignment)\n\tlc.slices = map[uint64]*sliceSummary{}\n}\n```", "  ```\nfunc (lc *loadCollector) updateAssignment(assignment *protos.Assignment) {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tlc.assignment = assignment\n\tlc.index = newIndex(assignment)\n\tlc.reset()\n}\n```"]}, "weaver-internal/weaver/load.go-report": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/weaver/load.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage weaver\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/DataDog/hyperloglog\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/lightstep/varopt\"\n)\n\nfunc approxEqual(a, b float64) bool {\n\tconst float64EqualityThreshold = 1e-9\n\treturn math.Abs(a-b) <= float64EqualityThreshold\n}\n\n// TODO(mwhittaker): Right now, load collection is slow. It grabs a mutex\n// every time the load needs to be updated. Make this faster.\n\n// loadCollector collects load for a Service Weaver component. As an example, imagine we\n// have a load collector lc for a Service Weaver component that owns slices [0, 10) and\n// [100, 200). We add the following load over the course of a second.\n//\n//   - lc.Add(0, 1)\n//   - lc.Add(1, 1)\n//   - lc.Add(2, 1)\n//   - lc.Add(3, 1)\n//   - lc.Add(100, 1)\n//   - lc.Add(101, 1)\n//\n// The load collector will report a load of 4 requests per second on the slice\n// [0, 10) and a load of 2 requests per second on the slice [100, 200).\ntype loadCollector struct {\n\tcomponent string           // Service Weaver component\n\taddr      string           // dialable address found in assignments\n\tnow       func() time.Time // time.Now usually, but injected fake in tests\n\n\tmu         sync.Mutex               // guards the following fields\n\tassignment *protos.Assignment       // latest assignment\n\tindex      index                    // index on assignment\n\tstart      time.Time                // start of load collection\n\tslices     map[uint64]*sliceSummary // keyed by start of slice\n}\n\n// sliceSummary contains a summary of the observed keys and load of a slice for\n// a replica.\ntype sliceSummary struct {\n\tslice  slice                    // the slice\n\tload   float64                  // total load\n\tcount  *hyperloglog.HyperLogLog // counts distinct elements\n\tsample *varopt.Varopt[uint64]   // reservoir sample of keys\n}\n\n// newLoadCollector returns a new load collector. Note that load is collected\n// with respect to an assignment, so load won't be collected until\n// UpdateAssignment is called.\nfunc newLoadCollector(component string, addr string) *loadCollector {\n\treturn &loadCollector{\n\t\tcomponent: component,\n\t\taddr:      addr,\n\t\tnow:       func() time.Time { return time.Now() },\n\t\tstart:     time.Now(),\n\t\tslices:    map[uint64]*sliceSummary{},\n\t}\n}\n\n// add adds load for the provided key.\nfunc (lc *loadCollector) add(key uint64, v float64) error {\n\tif v != 1.0 {\n\t\tpanic(\"load != 1.0 not yet implemented\")\n\t}\n\n\t// Find the corresponding slice.\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tif lc.assignment == nil {\n\t\t// Load is reported with respect to a given assignment. If we don't\n\t\t// have an assignment yet, then we don't record the load.\n\t\treturn nil\n\t}\n\tslice, found := lc.index.find(key)\n\tif !found {\n\t\t// TODO(mwhittaker): It is currently possible to receive a request for\n\t\t// a key that is not in our current assignment. For example, a\n\t\t// different weavelet may have an older or newer version of the current\n\t\t// assignment and send us keys not in our current assignment. In the\n\t\t// future, we may want to catch these requests and discard them. For\n\t\t// now, we execute them.\n\t\treturn nil\n\t}\n\tif !slice.replicaSet[lc.addr] {\n\t\treturn nil\n\t}\n\n\tsummary, found := lc.slices[slice.start]\n\tif !found {\n\t\tvar err error\n\t\tsummary, err = newSliceSummary(slice)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlc.slices[slice.start] = summary\n\t}\n\n\t// Update the load.\n\tsummary.load += v\n\n\t// Update the count. Note that we compute a hash of our key before passing\n\t// it to the hyperloglog, even though the key is itself a hash. The reason\n\t// is that this slice represents only a small sliver of the total hash\n\t// space. To operate correctly, a hyperloglog assumes values are drawn\n\t// uniformly from the space of all uint32s, so if we feed the hyperloglog\n\t// values only from this slice, the count will be inaccurate.\n\t//\n\t// TODO(mwhittaker): Compute the hash outside of the lock?\n\t// TODO(mwhittaker): Use a different sketch?\n\t// TODO(mwhittaker): If the slice is small (< 1024), we can count the\n\t// number of distinct elements exactly. Don't use a hyperloglog here.\n\t// TODO(mwhittaker): Start with an exact count and only switch to a\n\t// hyperloglog if the number of unique elements gets too big?\n\tsummary.count.Add(hyperloglog.Murmur64(key))\n\n\t// Update the sample. Note that Add takes in a key and a weight, but we are\n\t// recording unweighted samples, so we use a constant weight of 1.0 for\n\t// every key.\n\tif _, err := summary.sample.Add(key, 1.0); err != nil {\n\t\treturn fmt.Errorf(\"cannot sample %d: %v\", key, err)\n\t}\n\treturn nil\n}\n\n// updateAssignment updates a load collector with the latest assignment. The\n// load reported by a load collector is always scoped to a single assignment.\n// A load report never spans more than one assignment. Thus, UpdateAssignment\n// also clears the load collector's accumulated load.\n\n\n\n\n\n\n\n\n\n\n// report returns a report of the collected load. If the load collector\n// doesn't have any collected load---this is possible if the load collector\n// doesn't have an assignment yet---then Report returns nil.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// reset resets the load collector. If you want to collect load over 5\n// minute windows, for example, call Reset every five minutes.\nfunc (lc *loadCollector) reset() {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tlc.start = lc.now()\n\tlc.slices = map[uint64]*sliceSummary{}\n}\n\n// newSliceSummary returns a new sliceSummary for the provided slice with\n// initially 0 load.\nfunc newSliceSummary(slice slice) (*sliceSummary, error) {\n\t// Initialize the hyperloglog. A hyperloglog with n registers uses roughly\n\t// n bytes of memory. We choose n=1024 so that every hyperloglog takes\n\t// about a kilobyte of memory. Given that a weavelet should manage a\n\t// moderate number of slices and components, the total memory usage of all\n\t// hyperloglogs should be relatively small. New's documentation also\n\t// suggests that n be a power of 2.\n\tcount, err := hyperloglog.New(1024)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Initialize the reservoir sample. A reservoir sample of size n stores at\n\t// most n keys, or roughly 8n bytes. As with the hyperloglogs, this should\n\t// lead to a modest memory usage.\n\t//\n\t// TODO(mwhittaker): Compute the expected errors in our estimates based on\n\t// the size of the sample.\n\t// TODO(mwhittaker): When we switch to range sharding, keys might be large\n\t// and 1000 keys might be too big.\n\tr := rand.New(rand.NewSource(time.Now().UnixNano()))\n\tsample := varopt.New[uint64](1000, r)\n\n\treturn &sliceSummary{slice: slice, count: count, sample: sample}, nil\n}\n\n// splits splits the slice into subslices with roughly even load.\nfunc (s *sliceSummary) splits(delta time.Duration) []*protos.LoadReport_SubsliceLoad {\n\t// Splits divides the slice into subslices of roughly even load. In the\n\t// normal case, Splits splits a slice into 20 subslices, each representing\n\t// 5% of the total load. If the number of samples is small, however, fewer\n\t// splits are used. Moreover, if adjacent splits are formed from a single\n\t// hot key, they are combined.\n\n\t// Materialize and sort the sample.\n\tk := s.sample.Size()\n\txs := make([]uint64, k)\n\tfor i := 0; i < k; i++ {\n\t\tx, _ := s.sample.Get(i)\n\t\txs[i] = x\n\t}\n\tsort.Slice(xs, func(i, j int) bool { return xs[i] < xs[j] })\n\n\t// Determine the number of splits. More splits is better, but if we don't\n\t// have many points in our sample, then using a large number of splits will\n\t// lead to inaccurate estimates.\n\tvar n int\n\tswitch {\n\tcase k < 10:\n\t\tn = 1 // 100%\n\tcase k < 50:\n\t\tn = 2 // 50%\n\tcase k < 100:\n\t\tn = 4 // 25%\n\tcase k < 250:\n\t\tn = 5 // 20%\n\tcase k < 500:\n\t\tn = 10 // 10%\n\tdefault:\n\t\tn = 20 // 5%\n\t}\n\n\t// Adjust the first subslice so that it starts at our slice boundary.\n\ttotalLoad := s.load / delta.Seconds()\n\tsplits := subslices(totalLoad, xs, n)\n\tsplits[0].Start = s.slice.start\n\n\t// Double check that the split loads sum to the total load.\n\tvar sum float64\n\tfor _, split := range splits {\n\t\tsum += split.Load\n\t}\n\tif !approxEqual(sum, totalLoad) {\n\t\tpanic(fmt.Sprintf(\"bad sum of split loads: got %f, want %f\", sum, totalLoad))\n\t}\n\n\treturn splits\n}\n\n// subslices returns n splits of the provided points with roughly the same\n// load. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}, n =\n// 4, and a load of 10.0, subslices will return the following four splits:\n//\n//   - {Start: 10, Load: 2.5} // [10, 30)\n//   - {Start: 30, Load: 2.5} // [30, 50)\n//   - {Start: 50, Load: 2.5} // [50, 70)\n//   - {Start: 70, Load: 2.5} // [70, infinity)\n//\n// The returned splits are as even as possible on a best effort basis.\n// subslices only guarantees that the returned splits are contiguous and\n// sorted.\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// percentiles returns n equally spaced percentiles of the provided sorted set\n// of points. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}\n// and n = 4, percentiles will return []uint64{10, 30, 50, 70} where\n//\n//   - 10 is the 0th percentile,\n//   - 30 is the 25th percentile,\n//   - 50 is the 50th percentile,\n//   - 70 is the 75th percentile,\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\nfunc percentiles(xs []uint64, n int) []uint64 {\n\tps := make([]uint64, n)\n\tfor i := 0; i < n; i++ {\n\t\tps[i] = xs[int(float64(i)/float64(n)*float64(len(xs)))]\n\t}\n\treturn ps\n}\n\n// index is a read-only search index of a protos.Assignment, optimized to\n// find the slice that contains a key.\ntype index []slice\n\n// slice is the segment [start, end) of the key space, along with a set of\n// assigned replicas.\ntype slice struct {\n\tstart      uint64          // start of slice, inclusive\n\tend        uint64          // end of slice, exclusive\n\treplicas   []string        // replicas assigned to this slice\n\treplicaSet map[string]bool // replicas assigned to this slice\n}\n\n// newIndex returns a new index of the provided assignment.\nfunc newIndex(proto *protos.Assignment) index {\n\tn := len(proto.Slices)\n\tslices := make([]slice, n)\n\tfor i := 0; i < n; i++ {\n\t\t// Gather the set of replicas.\n\t\treplicas := proto.Slices[i].Replicas\n\t\treplicaSet := make(map[string]bool, len(replicas))\n\t\tfor _, replica := range replicas {\n\t\t\treplicaSet[replica] = true\n\t\t}\n\n\t\t// Compute the end of the slice.\n\t\tvar end uint64 = math.MaxUint64\n\t\tif i < n-1 {\n\t\t\tend = proto.Slices[i+1].Start\n\t\t}\n\n\t\t// Form the slice.\n\t\tslices[i] = slice{\n\t\t\tstart:      proto.Slices[i].Start,\n\t\t\tend:        end,\n\t\t\treplicas:   replicas,\n\t\t\treplicaSet: replicaSet,\n\t\t}\n\t}\n\treturn slices\n}\n\n// find finds the slice that contains the given key in O(log n) time where n is\n// the number of slices in the assignment.\nfunc (ind index) find(key uint64) (slice, bool) {\n\ti := sort.Search(len(ind), func(i int) bool {\n\t\treturn key < ind[i].end\n\t})\n\tif i == len(ind) {\n\t\treturn slice{}, false\n\t}\n\treturn ind[i], true\n}\n\n```\n# Function Signature: func (lc *loadCollector) report() *protos.LoadReport_ComponentLoad {\n# Natural Language Annotations: // report returns a report of the collected load. If the load collector\n// doesn't have any collected load---this is possible if the load collector\n// doesn't have an assignment yet---then Report returns nil.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (lc *loadCollector) report() *protos.LoadReport_ComponentLoad {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\n\t// If we don't have an assignment yet, then we don't have any collected load.\n\tif lc.assignment == nil {\n\t\treturn nil\n\t}\n\n\t// Determine the duration for which we have collected load.\n\tduration := lc.now() - lc.start\n\n\t// Compute the total load for this component.\n\ttotalLoad := 0.0\n\tfor _, summary := range lc.slices {\n\t\ttotalLoad += summary.load\n\t}\n\n\t// Create a slice load report for each slice.\n\tvar sliceLoads []*protos.LoadReport_SubsliceLoad\n\tfor _, summary := range lc.slices {\n\t\t// Compute the load for this slice.\n\t\tload := summary.load / duration.Seconds()\n\t\tsplits := summary.slice.splits(duration)\n\t\tsliceLoads = append(sliceLoads, splits...)\n\t}\n\n\t// Create a slice load report for each subslice.\n\tvar reports []*protos.LoadReport_ComponentLoad\n\tif lc.component != \"\" {\n\t\treports = append(reports, &protos.LoadReport_ComponentLoad{\n\t\t\tComponent: lc.component,\n\t\t\tAddr:      lc.addr,\n\t\t\tSliceLoads: sliceLoads,\n\t\t\tTotalLoad: totalLoad,\n\t\t})\n\t}\n\n\treturn reports[0]\n}\n```", "  ```\nfunc (lc *loadCollector) report() *protos.LoadReport_ComponentLoad {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\n\tif len(lc.slices) == 0 {\n\t\treturn nil\n\t}\n\n\tload := &protos.LoadReport_ComponentLoad{\n\t\tComponent: lc.component,\n\t\tAddr:      lc.addr,\n\t}\n\n\tfor i, summary := range lc.slices {\n\t\t// ignore the first slice, as we have already initialized the load\n\t\tif i == 0 {\n\t\t\tcontinue\n\t\t}\n\n\t\t// add the load for the current slice to the overall load\n\t\tload.Load += summary.load\n\n\t\t// add the load to the distribution in the load report\n\t\tsubsliceLoad := &protos.LoadReport_SubsliceLoad{\n\t\t\tStart:  summary.slice.start,\n\t\t\tLoad:   summary.load,\n\t\t\tCount:  summary.count.Estimate64(),\n\t\t\tSample: summary.sample.Sample(95),\n\t\t}\n\t\tload.SubsliceLoads = append(load.SubsliceLoads, subsliceLoad)\n\t}\n\n\treturn load\n}\n```", "  ```\nfunc (lc *loadCollector) report() *protos.LoadReport_ComponentLoad {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\n\tif lc.assignment == nil {\n\t\treturn nil\n\t}\n\n\tvar reports []*protos.LoadReport_SubsliceLoad\n\tfor _, summary := range lc.slices {\n\t\tdelta := float64(lc.now().Sub(lc.start) / time.Second)\n\t\tsplits := summary.splits(lc.assignment)\n\t\tfor _, split := range splits {\n\t\t\treports = append(reports, split)\n\t\t}\n\t}\n\n\tvar totalLoad float64\n\tfor _, report := range reports {\n\t\ttotalLoad += report.Load\n\t}\n\n\tvar count uint64\n\tfor _, summary := range lc.slices {\n\t\test, _ := summary.count.Estimate()\n\t\tcount += est.Count\n\t}\n\n\tvar estimate float64\n\tfor _, overall := range reports {\n\t\testimate += overall.Load\n\t}\n\n\treturn &protos.LoadReport_ComponentLoad{\n\t\tComponent: &protos.LoadReport_Component{\n\t\t\tName:   lc.component,\n\t\t\tAddr:   lc.addr,\n\t\t\tReport: &protos.LoadReport_OverallLoad{Load: estimate},\n\t\t},\n\t\tTotalLoad: totalLoad,\n\t\tCount:     count,\n\t\tSubslices: reports,\n\t}\n}\n```"]}, "weaver-internal/weaver/load.go-subslices": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/weaver/load.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage weaver\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/DataDog/hyperloglog\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/lightstep/varopt\"\n)\n\nfunc approxEqual(a, b float64) bool {\n\tconst float64EqualityThreshold = 1e-9\n\treturn math.Abs(a-b) <= float64EqualityThreshold\n}\n\n// TODO(mwhittaker): Right now, load collection is slow. It grabs a mutex\n// every time the load needs to be updated. Make this faster.\n\n// loadCollector collects load for a Service Weaver component. As an example, imagine we\n// have a load collector lc for a Service Weaver component that owns slices [0, 10) and\n// [100, 200). We add the following load over the course of a second.\n//\n//   - lc.Add(0, 1)\n//   - lc.Add(1, 1)\n//   - lc.Add(2, 1)\n//   - lc.Add(3, 1)\n//   - lc.Add(100, 1)\n//   - lc.Add(101, 1)\n//\n// The load collector will report a load of 4 requests per second on the slice\n// [0, 10) and a load of 2 requests per second on the slice [100, 200).\ntype loadCollector struct {\n\tcomponent string           // Service Weaver component\n\taddr      string           // dialable address found in assignments\n\tnow       func() time.Time // time.Now usually, but injected fake in tests\n\n\tmu         sync.Mutex               // guards the following fields\n\tassignment *protos.Assignment       // latest assignment\n\tindex      index                    // index on assignment\n\tstart      time.Time                // start of load collection\n\tslices     map[uint64]*sliceSummary // keyed by start of slice\n}\n\n// sliceSummary contains a summary of the observed keys and load of a slice for\n// a replica.\ntype sliceSummary struct {\n\tslice  slice                    // the slice\n\tload   float64                  // total load\n\tcount  *hyperloglog.HyperLogLog // counts distinct elements\n\tsample *varopt.Varopt[uint64]   // reservoir sample of keys\n}\n\n// newLoadCollector returns a new load collector. Note that load is collected\n// with respect to an assignment, so load won't be collected until\n// UpdateAssignment is called.\nfunc newLoadCollector(component string, addr string) *loadCollector {\n\treturn &loadCollector{\n\t\tcomponent: component,\n\t\taddr:      addr,\n\t\tnow:       func() time.Time { return time.Now() },\n\t\tstart:     time.Now(),\n\t\tslices:    map[uint64]*sliceSummary{},\n\t}\n}\n\n// add adds load for the provided key.\nfunc (lc *loadCollector) add(key uint64, v float64) error {\n\tif v != 1.0 {\n\t\tpanic(\"load != 1.0 not yet implemented\")\n\t}\n\n\t// Find the corresponding slice.\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tif lc.assignment == nil {\n\t\t// Load is reported with respect to a given assignment. If we don't\n\t\t// have an assignment yet, then we don't record the load.\n\t\treturn nil\n\t}\n\tslice, found := lc.index.find(key)\n\tif !found {\n\t\t// TODO(mwhittaker): It is currently possible to receive a request for\n\t\t// a key that is not in our current assignment. For example, a\n\t\t// different weavelet may have an older or newer version of the current\n\t\t// assignment and send us keys not in our current assignment. In the\n\t\t// future, we may want to catch these requests and discard them. For\n\t\t// now, we execute them.\n\t\treturn nil\n\t}\n\tif !slice.replicaSet[lc.addr] {\n\t\treturn nil\n\t}\n\n\tsummary, found := lc.slices[slice.start]\n\tif !found {\n\t\tvar err error\n\t\tsummary, err = newSliceSummary(slice)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlc.slices[slice.start] = summary\n\t}\n\n\t// Update the load.\n\tsummary.load += v\n\n\t// Update the count. Note that we compute a hash of our key before passing\n\t// it to the hyperloglog, even though the key is itself a hash. The reason\n\t// is that this slice represents only a small sliver of the total hash\n\t// space. To operate correctly, a hyperloglog assumes values are drawn\n\t// uniformly from the space of all uint32s, so if we feed the hyperloglog\n\t// values only from this slice, the count will be inaccurate.\n\t//\n\t// TODO(mwhittaker): Compute the hash outside of the lock?\n\t// TODO(mwhittaker): Use a different sketch?\n\t// TODO(mwhittaker): If the slice is small (< 1024), we can count the\n\t// number of distinct elements exactly. Don't use a hyperloglog here.\n\t// TODO(mwhittaker): Start with an exact count and only switch to a\n\t// hyperloglog if the number of unique elements gets too big?\n\tsummary.count.Add(hyperloglog.Murmur64(key))\n\n\t// Update the sample. Note that Add takes in a key and a weight, but we are\n\t// recording unweighted samples, so we use a constant weight of 1.0 for\n\t// every key.\n\tif _, err := summary.sample.Add(key, 1.0); err != nil {\n\t\treturn fmt.Errorf(\"cannot sample %d: %v\", key, err)\n\t}\n\treturn nil\n}\n\n// updateAssignment updates a load collector with the latest assignment. The\n// load reported by a load collector is always scoped to a single assignment.\n// A load report never spans more than one assignment. Thus, UpdateAssignment\n// also clears the load collector's accumulated load.\n\n\n\n\n\n\n\n\n\n\n// report returns a report of the collected load. If the load collector\n// doesn't have any collected load---this is possible if the load collector\n// doesn't have an assignment yet---then Report returns nil.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// reset resets the load collector. If you want to collect load over 5\n// minute windows, for example, call Reset every five minutes.\nfunc (lc *loadCollector) reset() {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tlc.start = lc.now()\n\tlc.slices = map[uint64]*sliceSummary{}\n}\n\n// newSliceSummary returns a new sliceSummary for the provided slice with\n// initially 0 load.\nfunc newSliceSummary(slice slice) (*sliceSummary, error) {\n\t// Initialize the hyperloglog. A hyperloglog with n registers uses roughly\n\t// n bytes of memory. We choose n=1024 so that every hyperloglog takes\n\t// about a kilobyte of memory. Given that a weavelet should manage a\n\t// moderate number of slices and components, the total memory usage of all\n\t// hyperloglogs should be relatively small. New's documentation also\n\t// suggests that n be a power of 2.\n\tcount, err := hyperloglog.New(1024)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Initialize the reservoir sample. A reservoir sample of size n stores at\n\t// most n keys, or roughly 8n bytes. As with the hyperloglogs, this should\n\t// lead to a modest memory usage.\n\t//\n\t// TODO(mwhittaker): Compute the expected errors in our estimates based on\n\t// the size of the sample.\n\t// TODO(mwhittaker): When we switch to range sharding, keys might be large\n\t// and 1000 keys might be too big.\n\tr := rand.New(rand.NewSource(time.Now().UnixNano()))\n\tsample := varopt.New[uint64](1000, r)\n\n\treturn &sliceSummary{slice: slice, count: count, sample: sample}, nil\n}\n\n// splits splits the slice into subslices with roughly even load.\nfunc (s *sliceSummary) splits(delta time.Duration) []*protos.LoadReport_SubsliceLoad {\n\t// Splits divides the slice into subslices of roughly even load. In the\n\t// normal case, Splits splits a slice into 20 subslices, each representing\n\t// 5% of the total load. If the number of samples is small, however, fewer\n\t// splits are used. Moreover, if adjacent splits are formed from a single\n\t// hot key, they are combined.\n\n\t// Materialize and sort the sample.\n\tk := s.sample.Size()\n\txs := make([]uint64, k)\n\tfor i := 0; i < k; i++ {\n\t\tx, _ := s.sample.Get(i)\n\t\txs[i] = x\n\t}\n\tsort.Slice(xs, func(i, j int) bool { return xs[i] < xs[j] })\n\n\t// Determine the number of splits. More splits is better, but if we don't\n\t// have many points in our sample, then using a large number of splits will\n\t// lead to inaccurate estimates.\n\tvar n int\n\tswitch {\n\tcase k < 10:\n\t\tn = 1 // 100%\n\tcase k < 50:\n\t\tn = 2 // 50%\n\tcase k < 100:\n\t\tn = 4 // 25%\n\tcase k < 250:\n\t\tn = 5 // 20%\n\tcase k < 500:\n\t\tn = 10 // 10%\n\tdefault:\n\t\tn = 20 // 5%\n\t}\n\n\t// Adjust the first subslice so that it starts at our slice boundary.\n\ttotalLoad := s.load / delta.Seconds()\n\tsplits := subslices(totalLoad, xs, n)\n\tsplits[0].Start = s.slice.start\n\n\t// Double check that the split loads sum to the total load.\n\tvar sum float64\n\tfor _, split := range splits {\n\t\tsum += split.Load\n\t}\n\tif !approxEqual(sum, totalLoad) {\n\t\tpanic(fmt.Sprintf(\"bad sum of split loads: got %f, want %f\", sum, totalLoad))\n\t}\n\n\treturn splits\n}\n\n// subslices returns n splits of the provided points with roughly the same\n// load. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}, n =\n// 4, and a load of 10.0, subslices will return the following four splits:\n//\n//   - {Start: 10, Load: 2.5} // [10, 30)\n//   - {Start: 30, Load: 2.5} // [30, 50)\n//   - {Start: 50, Load: 2.5} // [50, 70)\n//   - {Start: 70, Load: 2.5} // [70, infinity)\n//\n// The returned splits are as even as possible on a best effort basis.\n// subslices only guarantees that the returned splits are contiguous and\n// sorted.\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// percentiles returns n equally spaced percentiles of the provided sorted set\n// of points. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}\n// and n = 4, percentiles will return []uint64{10, 30, 50, 70} where\n//\n//   - 10 is the 0th percentile,\n//   - 30 is the 25th percentile,\n//   - 50 is the 50th percentile,\n//   - 70 is the 75th percentile,\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\nfunc percentiles(xs []uint64, n int) []uint64 {\n\tps := make([]uint64, n)\n\tfor i := 0; i < n; i++ {\n\t\tps[i] = xs[int(float64(i)/float64(n)*float64(len(xs)))]\n\t}\n\treturn ps\n}\n\n// index is a read-only search index of a protos.Assignment, optimized to\n// find the slice that contains a key.\ntype index []slice\n\n// slice is the segment [start, end) of the key space, along with a set of\n// assigned replicas.\ntype slice struct {\n\tstart      uint64          // start of slice, inclusive\n\tend        uint64          // end of slice, exclusive\n\treplicas   []string        // replicas assigned to this slice\n\treplicaSet map[string]bool // replicas assigned to this slice\n}\n\n// newIndex returns a new index of the provided assignment.\nfunc newIndex(proto *protos.Assignment) index {\n\tn := len(proto.Slices)\n\tslices := make([]slice, n)\n\tfor i := 0; i < n; i++ {\n\t\t// Gather the set of replicas.\n\t\treplicas := proto.Slices[i].Replicas\n\t\treplicaSet := make(map[string]bool, len(replicas))\n\t\tfor _, replica := range replicas {\n\t\t\treplicaSet[replica] = true\n\t\t}\n\n\t\t// Compute the end of the slice.\n\t\tvar end uint64 = math.MaxUint64\n\t\tif i < n-1 {\n\t\t\tend = proto.Slices[i+1].Start\n\t\t}\n\n\t\t// Form the slice.\n\t\tslices[i] = slice{\n\t\t\tstart:      proto.Slices[i].Start,\n\t\t\tend:        end,\n\t\t\treplicas:   replicas,\n\t\t\treplicaSet: replicaSet,\n\t\t}\n\t}\n\treturn slices\n}\n\n// find finds the slice that contains the given key in O(log n) time where n is\n// the number of slices in the assignment.\nfunc (ind index) find(key uint64) (slice, bool) {\n\ti := sort.Search(len(ind), func(i int) bool {\n\t\treturn key < ind[i].end\n\t})\n\tif i == len(ind) {\n\t\treturn slice{}, false\n\t}\n\treturn ind[i], true\n}\n\n```\n# Function Signature: func subslices(load float64, xs []uint64, n int) []*protos.LoadReport_SubsliceLoad {\n# Natural Language Annotations: // subslices returns n splits of the provided points with roughly the same\n// load. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}, n =\n// 4, and a load of 10.0, subslices will return the following four splits:\n//\n//   - {Start: 10, Load: 2.5} // [10, 30)\n//   - {Start: 30, Load: 2.5} // [30, 50)\n//   - {Start: 50, Load: 2.5} // [50, 70)\n//   - {Start: 70, Load: 2.5} // [70, infinity)\n//\n// The returned splits are as even as possible on a best effort basis.\n// subslices only guarantees that the returned splits are contiguous and\n// sorted.\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc subslices(load float64, xs []uint64, n int) []*protos.LoadReport_SubsliceLoad {\n\t// Check the preconditions.\n\tif !sorted(xs) {\n\t\tpanic(\"subslices: xs is not sorted\")\n\t}\n\tif n <= 0 {\n\t\tpanic(\"subslices: n should be positive\")\n\t}\n\n\t// Compute the distance between subslices.\n\tdist := floor(load / float64(n))\n\n\t// Initialize the subslices.\n\tsubslices := make([]*protos.LoadReport_SubsliceLoad, 0, n)\n\tcurStart := xs[0]\n\tcurLoad := dist\n\n\t// Add the first subslice.\n\tsubslices = append(subslices, &protos.LoadReport_SubsliceLoad{\n\t\tStart:  curStart,\n\t\tLoad:   curLoad,\n\t\tCount:  hyperloglog.New(1024),\n\t\tSample: varopt.New(1000, rand.NewSource(time.Now().UnixNano())),\n\t})\n\n\t// Add the remaining subslices.\n\tfor i := 1; i < n; i++ {\n\t\tnextStart := xs[i]\n\t\tnextLoad := subslices[i-1].Load + dist\n\t\tsubslices = append(subslices, &protos.LoadReport_SubsliceLoad{\n\t\t\tStart:  nextStart,\n\t\t\tLoad:   nextLoad,\n\t\t\tCount:  hyperloglog.New(1024),\n\t\t\tSample: varopt.New(1000, rand.NewSource(time.Now().UnixNano())),\n\t\t})\n\t}\n\n\treturn subslices\n}\n\n// sorted returns whether the provided slice of uint64s is sorted in increasing\n// order.\nfunc sorted(xs []uint64) bool {\n\tfor i := 1; i < len(xs); i++ {\n\t\tif xs[i-1] > xs[i] {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// floor returns the greatest integer less than or equal to x.\nfunc floor(x float64) uint64 {\n\treturn uint64(math.Floor(x))\n}\n```", "  ```\nfunc subslices(load float64, xs []uint64, n int) []*protos.LoadReport_SubsliceLoad {\n\t// Check that n > 0.\n\tif n <= 0 {\n\t\tpanic(fmt.Errorf(\"n must be positive: got %d\", n))\n\t}\n\n\t// Check that xs is sorted in increasing order.\n\tif !sort.SliceIsSorted(xs, func(i, j int) bool { return xs[i] < xs[j] }) {\n\t\tpanic(fmt.Errorf(\"xs must be sorted in increasing order: got %v\", xs))\n\t}\n\n\t// Calculate the average load per split.\n\tavgLoadPerSplit := load / float64(n)\n\n\t// Divide the load into n subslices.\n\tsubslices := make([]*protos.LoadReport_SubsliceLoad, n)\n\tfor i := range subslices {\n\t\t// Compute the start and end of the slice.\n\t\tvar start, end uint64\n\t\tif i == 0 {\n\t\t\tstart = uint64(xs[0])\n\t\t} else {\n\t\t\tstart = end\n\t\t}\n\t\tif i == len(subslices)-1 {\n\t\t\tend = math.MaxUint64\n\t\t} else {\n\t\t\tend = uint64(xs[int(float64(i+1)/float64(n)*float64(len(xs)))])\n\t\t}\n\n\t\t// Compute the load of the slice.\n\t\tslice := &protos.LoadReport_SubsliceLoad{\n\t\t\tStart: start,\n\t\t\tLoad:  avgLoadPerSplit,\n\t\t}\n\t\tif end > start {\n\t\t\tslice.Load /= (float64(end-start) / float64(math.MaxUint64-start))\n\t\t}\n\n\t\t// Add the slice to the subslices.\n\t\tsubslices[i] = slice\n\t}\n\n\t// Return the subslices.\n\treturn subslices\n}\n```", "  ```\nfunc subslices(load float64, xs []uint64, n int) []*protos.LoadReport_SubsliceLoad {\n\t// Calculate the desired amount of load per slice.\n\tdesiredLoad := load / float64(n)\n\n\t// Initialize the subslices.\n\tsubslices := make([]*protos.LoadReport_SubsliceLoad, n)\n\tfor i := 0; i < n; i++ {\n\t\tsubslices[i] = &protos.LoadReport_SubsliceLoad{\n\t\t\tLoad:      desiredLoad,\n\t\t\tStart:     xs[i*len(xs)/n],\n\t\t\tEnd:       xs[(i+1)*len(xs)/n],\n\t\t}\n\t}\n\n\t// Keep track of the remaining load and the current slice.\n\tremainingLoad := load\n\tcurrentSlice := 0\n\n\t// Divvy out the remaining load.\n\tfor remainingLoad > 0 {\n\t\t// Determine how much load we can add to the current slice without exceeding the desired load.\n\t\tsliceLoad := subslices[currentSlice].Load\n\t\tavailableLoad := sliceLoad + subslices[currentSlice].End - subslices[currentSlice].Start\n\t\tif availableLoad > desiredLoad {\n\t\t\tavailableLoad = desiredLoad\n\t\t}\n\n\t\t// Add the available load to the current slice and update the total remaining load.\n\t\tsubslices[currentSlice].Load = sliceLoad + availableLoad\n\t\tremainingLoad -= availableLoad\n\n\t\t// Update the current slice.\n\t\tcurrentSlice++\n\t\tif currentSlice >= n {\n\t\t\tcurrentSlice = 0\n\t\t}\n\t}\n\n\treturn subslices\n}\n```"]}, "weaver-internal/weaver/routing.go-update": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/weaver/routing.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage weaver\n\nimport (\n\t\"context\"\n\t\"crypto/tls\"\n\t\"math/rand\"\n\t\"sync\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/cond\"\n\t\"github.com/ServiceWeaver/weaver/internal/net/call\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/google/uuid\"\n)\n\n// routingBalancer balances requests according to a routing assignment.\ntype routingBalancer struct {\n\tbalancer  call.Balancer // balancer to use for non-routed calls\n\ttlsConfig *tls.Config   // tls config to use; may be nil.\n\n\tmu         sync.RWMutex\n\tassignment *protos.Assignment\n\tindex      index\n\n\t// Map from address to connection. We currently allow just one\n\t// connection per address.\n\t// Guarded by mu.\n\tconns map[string]call.ReplicaConnection\n}\n\n// newRoutingBalancer returns a new routingBalancer.\nfunc newRoutingBalancer(tlsConfig *tls.Config) *routingBalancer {\n\treturn &routingBalancer{\n\t\tbalancer:  call.RoundRobin(),\n\t\ttlsConfig: tlsConfig,\n\t\tconns:     map[string]call.ReplicaConnection{},\n\t}\n}\n\n// Add adds c to the set of connections we are balancing across.\nfunc (rb *routingBalancer) Add(c call.ReplicaConnection) {\n\trb.balancer.Add(c)\n\n\trb.mu.Lock()\n\tdefer rb.mu.Unlock()\n\trb.conns[c.Address()] = c\n}\n\n// Remove removes c from the set of connections we are balancing across.\nfunc (rb *routingBalancer) Remove(c call.ReplicaConnection) {\n\trb.balancer.Remove(c)\n\n\trb.mu.Lock()\n\tdefer rb.mu.Unlock()\n\tdelete(rb.conns, c.Address())\n}\n\n// update updates the balancer with the provided assignment\n\n\n\n\n\n\n\n\n\n\n\n\n// Pick implements the call.Balancer interface.\nfunc (rb *routingBalancer) Pick(opts call.CallOptions) (call.ReplicaConnection, bool) {\n\tif opts.ShardKey == 0 {\n\t\t// If the method we're calling is not sharded (which is guaranteed to\n\t\t// be true for nonsharded components), then the shard key is 0.\n\t\treturn rb.balancer.Pick(opts)\n\t}\n\n\t// Grab the current assignment. It's possible that the current assignment\n\t// changes between when we release the lock and when we pick an endpoint,\n\t// but using a slightly stale assignment is okay.\n\trb.mu.RLock()\n\tassignment := rb.assignment\n\tindex := rb.index\n\trb.mu.RUnlock()\n\n\tif assignment == nil {\n\t\t// There is no assignment. This is possible if we haven't received an\n\t\t// assignment from the assigner yet.\n\t\treturn rb.balancer.Pick(opts)\n\t}\n\n\tslice, ok := index.find(opts.ShardKey)\n\tif !ok {\n\t\t// TODO(mwhittaker): Shouldn't this be impossible. Understand better\n\t\t// when this happens.\n\t\treturn rb.balancer.Pick(opts)\n\t}\n\n\t// Search for an available ReplicConnection starting at a random offset.\n\t// TODO(sanjay):Precompute the set of available ReplicaConnections per slice.\n\toffset := rand.Intn(len(slice.replicas))\n\trb.mu.RLock()\n\tdefer rb.mu.RUnlock()\n\tfor i, n := 0, len(slice.replicas); i < n; i++ {\n\t\toffset++\n\t\tif offset == n {\n\t\t\toffset = 0\n\t\t}\n\t\tif c, ok := rb.conns[slice.replicas[offset]]; ok {\n\t\t\treturn c, true\n\t\t}\n\t}\n\treturn nil, false\n}\n\n// routingResolver is a dummy resolver that returns whatever endpoints are\n// passed to the update method.\ntype routingResolver struct {\n\tm         sync.Mutex      // guards all of the following fields\n\tchanged   cond.Cond       // fires when endpoints changes\n\tversion   *call.Version   // the current version of endpoints\n\tendpoints []call.Endpoint // the endpoints returned by Resolve\n}\n\n// newRoutingResolver returns a new routingResolver.\nfunc newRoutingResolver() *routingResolver {\n\tr := &routingResolver{\n\t\tversion: &call.Version{Opaque: call.Missing.Opaque},\n\t}\n\tr.changed.L = &r.m\n\treturn r\n}\n\n// IsConstant implements the call.Resolver interface.\nfunc (rr *routingResolver) IsConstant() bool { return false }\n\n// update updates the resolver with the provided endpoints.\nfunc (rr *routingResolver) update(endpoints []call.Endpoint) {\n\trr.m.Lock()\n\tdefer rr.m.Unlock()\n\trr.version = &call.Version{Opaque: uuid.New().String()}\n\trr.endpoints = endpoints\n\trr.changed.Broadcast()\n}\n\n// Resolve implements the call.Resolver interface.\nfunc (rr *routingResolver) Resolve(ctx context.Context, version *call.Version) ([]call.Endpoint, *call.Version, error) {\n\trr.m.Lock()\n\tdefer rr.m.Unlock()\n\n\tif version == nil {\n\t\treturn rr.endpoints, rr.version, nil\n\t}\n\n\tfor *version == *rr.version {\n\t\tif err := rr.changed.Wait(ctx); err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t}\n\treturn rr.endpoints, rr.version, nil\n}\n\n```\n# Function Signature: func (rb *routingBalancer) update(assignment *protos.Assignment) {\n# Natural Language Annotations: // update updates the balancer with the provided assignment\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (rb *routingBalancer) update(assignment *protos.Assignment) {\n\trb.mu.Lock()\n\tdefer rb.mu.Unlock()\n\trb.assignment = assignment\n\trb.index = index{}\n\tif assignment == nil {\n\t\treturn\n\t}\n\tfor _, slice := range assignment.Slices {\n\t\tif _, ok := rb.index.find(slice.Start); !ok {\n\t\t\trb.index.insert(slice)\n\t\t}\n\t}\n}\n```", "  ```\nfunc (rb *routingBalancer) update(assignment *protos.Assignment) {\n\trb.mu.Lock()\n\tdefer rb.mu.Unlock()\n\n\t// Update our local copy of the assignment.\n\trb.assignment = assignment\n\n\t// Update the index to reflect the new assignment.\n\trb.index.update(assignment)\n\n\t// Remove any connections to replicas that are no longer in the assignment.\n\tfor a, c := range rb.conns {\n\t\tif _, ok := assignment.Replicas[a]; !ok {\n\t\t\tdelete(rb.conns, a)\n\t\t}\n\t}\n}\n```", "  ```\nfunc (rb * routingBalancer) update(assignment * protos.Assignment) {\n\trb.mu.Lock()\n\tdefer rb.mu.Unlock()\n\trb.assignment = assignment\n\tif len(assignment.slices) == 0 {\n\t\trb.index = nil\n\t\treturn\n\t}\n\tindex := make([]balancerSlice, len(assignment.slices))\n\tfor i, s := range assignment.slices {\n\t\tindex[i] = balancerSlice{\n\t\t\tkey:   s.hashKey,\n\t\t\treplicas: replicas{assignment.replicas[s.replicas[0]:s.replicas[1]]},\n\t\t}\n\t}\n\trb.index = & index\n}\n```"]}, "weaver-internal/status/registry.go-NewRegistry": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/status/registry.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package status contains code for implementing status related commands like\n// \"weaver multi status\" and \"weaver single dashboard\".\npackage status\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"syscall\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/files\"\n\t\"github.com/ServiceWeaver/weaver/runtime/colors\"\n)\n\n// A Registry is a persistent collection of Service Weaver application metadata.\n//\n// Tools like \"weaver multi status\" and \"weaver multi dashboard\" use the registry\n// to know which applications are running and to fetch the status of the\n// running applications.\ntype Registry struct {\n\t// A Registry stores registrations as files in a directory. Every\n\t// registration r is stored in a JSON file called\n\t// {r.DeploymentId}.registration.json.\n\t//\n\t// TODO(mwhittaker): Store as protos instead of JSON?\n\tdir string\n\n\t// newClient returns a new status client that curls the provided address.\n\t// It is a field of Registry to enable dependency injection in\n\t// registry_test.go.\n\tnewClient func(string) Server\n}\n\n// A Registration contains basic metadata about a Service Weaver application.\ntype Registration struct {\n\tDeploymentId string // deployment id (e.g, \"eba18295\")\n\tApp          string // app name (e.g., \"todo\")\n\tAddr         string // status server (e.g., \"localhost:12345\")\n}\n\n// Rolodex returns a pretty-printed rolodex displaying the registration.\n//\n//\t\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n//\t\u2502 app        : collatz                              \u2502\n//\t\u2502 deployment : fdeeb059-825b-4606-9e99-e22e63e10552 \u2502\n//\t\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nfunc (r Registration) Rolodex() string {\n\t// Declare the contents.\n\ttype kv struct {\n\t\tkey string\n\t\tval colors.Text\n\t}\n\tprefix, suffix := formatId(r.DeploymentId)\n\tkvs := []kv{\n\t\t{\"app        :\", colors.Text{colors.Atom{S: r.App}}},\n\t\t{\"deployment :\", colors.Text{prefix, suffix}},\n\t}\n\n\tlength := func(t colors.Text) int {\n\t\tvar n int\n\t\tfor _, a := range t {\n\t\t\tn += len(a.S)\n\t\t}\n\t\treturn n\n\t}\n\n\t// Calculate widths.\n\tvalWidth := 0\n\tfor _, kv := range kvs {\n\t\tif length(kv.val) > valWidth {\n\t\t\tvalWidth = length(kv.val)\n\t\t}\n\t}\n\twidth := valWidth + len(kvs[0].key) + 5\n\n\t// Pretty print.\n\tvar b strings.Builder\n\tfmt.Fprintf(&b, \"\u256d%s\u256e\\n\", strings.Repeat(\"\u2500\", width-2))\n\tfor _, kv := range kvs {\n\t\ts := kv.val.String()\n\t\tfmt.Fprintf(&b, \"\u2502 %s %-*s \u2502\\n\", kv.key, valWidth+len(s)-length(kv.val), s)\n\t}\n\tfmt.Fprintf(&b, \"\u2570%s\u256f\\n\", strings.Repeat(\"\u2500\", width-2))\n\treturn b.String()\n}\n\n// NewRegistry returns a registry that persists data to the provided directory.\n\n\n\n\n\n\n\n\n\n\n\n\n// Register adds a registration to the registry.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Unregister removes a registration from the registry.\nfunc (r *Registry) Unregister(_ context.Context, deploymentId string) error {\n\tfilename := fmt.Sprintf(\"%s.registration.json\", deploymentId)\n\tfilename = filepath.Join(r.dir, filename)\n\tif err := os.Remove(filename); err != nil {\n\t\treturn fmt.Errorf(\"registry: remove %q: %w\", filename, err)\n\t}\n\treturn nil\n}\n\n// Get returns the Registration for the provided deployment. If the deployment\n// doesn't exist or is not active, a non-nil error is returned.\nfunc (r *Registry) Get(ctx context.Context, deploymentId string) (Registration, error) {\n\t// TODO(mwhittaker): r.list() reads and parses every registration file.\n\t// This is inefficient, as we could instead stop reading and parsing as\n\t// soon as we find the corresponding registration file. Even more\n\t// efficient, we could match the deploymentId to the filenames instead of\n\t// reading and parsing the files. Since the number of registrations is\n\t// small, and the size of every registration file is small, I don't think\n\t// these optimizations are urgently needed.\n\tregs, err := r.list()\n\tif err != nil {\n\t\treturn Registration{}, err\n\t}\n\tfor _, reg := range regs {\n\t\tif reg.DeploymentId != deploymentId {\n\t\t\tcontinue\n\t\t}\n\t\tif r.dead(ctx, reg) {\n\t\t\treturn Registration{}, fmt.Errorf(\"registry: deployment %q not found\", deploymentId)\n\t\t}\n\t\treturn reg, nil\n\t}\n\treturn Registration{}, fmt.Errorf(\"registry: deployment %q not found\", deploymentId)\n}\n\n// List returns all active Registrations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// list returns all registrations, dead or alive.\nfunc (r *Registry) list() ([]Registration, error) {\n\tentries, err := os.ReadDir(r.dir)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"registry: read dir %q: %w\", r.dir, err)\n\t}\n\n\tvar regs []Registration\n\tfor _, entry := range entries {\n\t\tif !strings.HasSuffix(entry.Name(), \".registration.json\") {\n\t\t\t// Ignore non-registration files in the registry directory.\n\t\t\tcontinue\n\t\t}\n\t\tfilename := filepath.Join(r.dir, entry.Name())\n\t\tbytes, err := os.ReadFile(filename)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"registry: read file %q: %w\", filename, err)\n\t\t}\n\t\tvar reg Registration\n\t\tif err := json.Unmarshal(bytes, &reg); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"registry: decode file %q: %w\", filename, err)\n\t\t}\n\t\tregs = append(regs, reg)\n\t}\n\treturn regs, nil\n}\n\n// dead returns whether the provided registration is associated with a\n// deployment that is definitely dead.\nfunc (r *Registry) dead(ctx context.Context, reg Registration) bool {\n\tstatus, err := r.newClient(reg.Addr).Status(ctx)\n\tswitch {\n\tcase errors.Is(err, syscall.ECONNREFUSED):\n\t\t// There is no status server for this deployment, so we consider\n\t\t// the deployment dead.\n\t\treturn true\n\tcase errors.Is(err, syscall.Errno(10061)):\n\t\t// The syscall.ECONNREFUSED doesn't work on Windows. Windows will\n\t\t// return WSAECONNREFUSED(syscall.Errno = 10061) when the connection is\n\t\t// refused.\n\t\treturn true\n\tcase err != nil:\n\t\t// Something went wrong. The deployment may be dead, but we're not 100%\n\t\t// sure, so we return false.\n\t\treturn false\n\tcase status.DeploymentId != reg.DeploymentId:\n\t\t// The status server for this deployment is dead and has been\n\t\t// superseded by a newer status server.\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n```\n# Function Signature: func NewRegistry(_ context.Context, dir string) (*Registry, error) {\n# Natural Language Annotations: // NewRegistry returns a registry that persists data to the provided directory.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewRegistry(_ context.Context, dir string) (*Registry, error) {\n\treturn &Registry{\n\t\tdir: dir,\n\t}, nil\n}\n```", "  ```\nfunc NewRegistry(ctx context.Context, dir string) (*Registry, error) {\n\tr := &Registry{\n\t\tdir: dir,\n\t\tnewClient: newClient,\n\t}\n\n\terr := os.MkdirAll(dir, 0777)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"newRegistry: mkdir %q: %w\", dir, err)\n\t}\n\n\tregs, err := r.list()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"newRegistry: list: %w\", err)\n\t}\n\n\tfor _, reg := range regs {\n\t\tif r.dead(ctx, reg) {\n\t\t\tif err := r.Unregister(ctx, reg.DeploymentId); err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"newRegistry: unregister: %w\", err)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn r, nil\n}\n```", "  ```\nfunc NewRegistry(_ context.Context, dir string) (*Registry, error) {\n\tdir, err := files.CanonicalPath(dir)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"registry: canonical path %q: %w\", dir, err)\n\t}\n\tf, err := os.OpenFile(dir, os.O_RDONLY|os.O_CREATE, 0644)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"registry: open dir %q: %w\", dir, err)\n\t}\n\tf.Close()\n\n\treturn &Registry{\n\t\tdir:      dir,\n\t\tnewClient: status.NewClient,\n\t}, nil\n}\n```"]}, "weaver-internal/status/registry.go-Register": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/status/registry.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package status contains code for implementing status related commands like\n// \"weaver multi status\" and \"weaver single dashboard\".\npackage status\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"syscall\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/files\"\n\t\"github.com/ServiceWeaver/weaver/runtime/colors\"\n)\n\n// A Registry is a persistent collection of Service Weaver application metadata.\n//\n// Tools like \"weaver multi status\" and \"weaver multi dashboard\" use the registry\n// to know which applications are running and to fetch the status of the\n// running applications.\ntype Registry struct {\n\t// A Registry stores registrations as files in a directory. Every\n\t// registration r is stored in a JSON file called\n\t// {r.DeploymentId}.registration.json.\n\t//\n\t// TODO(mwhittaker): Store as protos instead of JSON?\n\tdir string\n\n\t// newClient returns a new status client that curls the provided address.\n\t// It is a field of Registry to enable dependency injection in\n\t// registry_test.go.\n\tnewClient func(string) Server\n}\n\n// A Registration contains basic metadata about a Service Weaver application.\ntype Registration struct {\n\tDeploymentId string // deployment id (e.g, \"eba18295\")\n\tApp          string // app name (e.g., \"todo\")\n\tAddr         string // status server (e.g., \"localhost:12345\")\n}\n\n// Rolodex returns a pretty-printed rolodex displaying the registration.\n//\n//\t\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n//\t\u2502 app        : collatz                              \u2502\n//\t\u2502 deployment : fdeeb059-825b-4606-9e99-e22e63e10552 \u2502\n//\t\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nfunc (r Registration) Rolodex() string {\n\t// Declare the contents.\n\ttype kv struct {\n\t\tkey string\n\t\tval colors.Text\n\t}\n\tprefix, suffix := formatId(r.DeploymentId)\n\tkvs := []kv{\n\t\t{\"app        :\", colors.Text{colors.Atom{S: r.App}}},\n\t\t{\"deployment :\", colors.Text{prefix, suffix}},\n\t}\n\n\tlength := func(t colors.Text) int {\n\t\tvar n int\n\t\tfor _, a := range t {\n\t\t\tn += len(a.S)\n\t\t}\n\t\treturn n\n\t}\n\n\t// Calculate widths.\n\tvalWidth := 0\n\tfor _, kv := range kvs {\n\t\tif length(kv.val) > valWidth {\n\t\t\tvalWidth = length(kv.val)\n\t\t}\n\t}\n\twidth := valWidth + len(kvs[0].key) + 5\n\n\t// Pretty print.\n\tvar b strings.Builder\n\tfmt.Fprintf(&b, \"\u256d%s\u256e\\n\", strings.Repeat(\"\u2500\", width-2))\n\tfor _, kv := range kvs {\n\t\ts := kv.val.String()\n\t\tfmt.Fprintf(&b, \"\u2502 %s %-*s \u2502\\n\", kv.key, valWidth+len(s)-length(kv.val), s)\n\t}\n\tfmt.Fprintf(&b, \"\u2570%s\u256f\\n\", strings.Repeat(\"\u2500\", width-2))\n\treturn b.String()\n}\n\n// NewRegistry returns a registry that persists data to the provided directory.\n\n\n\n\n\n\n\n\n\n\n\n\n// Register adds a registration to the registry.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Unregister removes a registration from the registry.\nfunc (r *Registry) Unregister(_ context.Context, deploymentId string) error {\n\tfilename := fmt.Sprintf(\"%s.registration.json\", deploymentId)\n\tfilename = filepath.Join(r.dir, filename)\n\tif err := os.Remove(filename); err != nil {\n\t\treturn fmt.Errorf(\"registry: remove %q: %w\", filename, err)\n\t}\n\treturn nil\n}\n\n// Get returns the Registration for the provided deployment. If the deployment\n// doesn't exist or is not active, a non-nil error is returned.\nfunc (r *Registry) Get(ctx context.Context, deploymentId string) (Registration, error) {\n\t// TODO(mwhittaker): r.list() reads and parses every registration file.\n\t// This is inefficient, as we could instead stop reading and parsing as\n\t// soon as we find the corresponding registration file. Even more\n\t// efficient, we could match the deploymentId to the filenames instead of\n\t// reading and parsing the files. Since the number of registrations is\n\t// small, and the size of every registration file is small, I don't think\n\t// these optimizations are urgently needed.\n\tregs, err := r.list()\n\tif err != nil {\n\t\treturn Registration{}, err\n\t}\n\tfor _, reg := range regs {\n\t\tif reg.DeploymentId != deploymentId {\n\t\t\tcontinue\n\t\t}\n\t\tif r.dead(ctx, reg) {\n\t\t\treturn Registration{}, fmt.Errorf(\"registry: deployment %q not found\", deploymentId)\n\t\t}\n\t\treturn reg, nil\n\t}\n\treturn Registration{}, fmt.Errorf(\"registry: deployment %q not found\", deploymentId)\n}\n\n// List returns all active Registrations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// list returns all registrations, dead or alive.\nfunc (r *Registry) list() ([]Registration, error) {\n\tentries, err := os.ReadDir(r.dir)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"registry: read dir %q: %w\", r.dir, err)\n\t}\n\n\tvar regs []Registration\n\tfor _, entry := range entries {\n\t\tif !strings.HasSuffix(entry.Name(), \".registration.json\") {\n\t\t\t// Ignore non-registration files in the registry directory.\n\t\t\tcontinue\n\t\t}\n\t\tfilename := filepath.Join(r.dir, entry.Name())\n\t\tbytes, err := os.ReadFile(filename)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"registry: read file %q: %w\", filename, err)\n\t\t}\n\t\tvar reg Registration\n\t\tif err := json.Unmarshal(bytes, &reg); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"registry: decode file %q: %w\", filename, err)\n\t\t}\n\t\tregs = append(regs, reg)\n\t}\n\treturn regs, nil\n}\n\n// dead returns whether the provided registration is associated with a\n// deployment that is definitely dead.\nfunc (r *Registry) dead(ctx context.Context, reg Registration) bool {\n\tstatus, err := r.newClient(reg.Addr).Status(ctx)\n\tswitch {\n\tcase errors.Is(err, syscall.ECONNREFUSED):\n\t\t// There is no status server for this deployment, so we consider\n\t\t// the deployment dead.\n\t\treturn true\n\tcase errors.Is(err, syscall.Errno(10061)):\n\t\t// The syscall.ECONNREFUSED doesn't work on Windows. Windows will\n\t\t// return WSAECONNREFUSED(syscall.Errno = 10061) when the connection is\n\t\t// refused.\n\t\treturn true\n\tcase err != nil:\n\t\t// Something went wrong. The deployment may be dead, but we're not 100%\n\t\t// sure, so we return false.\n\t\treturn false\n\tcase status.DeploymentId != reg.DeploymentId:\n\t\t// The status server for this deployment is dead and has been\n\t\t// superseded by a newer status server.\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n```\n# Function Signature: func (r *Registry) Register(ctx context.Context, reg Registration) error {\n# Natural Language Annotations: // Register adds a registration to the registry.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (r *Registry) Register(ctx context.Context, reg Registration) error {\n\tfilename := fmt.Sprintf(\"%s.registration.json\", reg.DeploymentId)\n\tfilename = filepath.Join(r.dir, filename)\n\tbytes, err := json.MarshalIndent(reg, \"\", \"  \")\n\tif err != nil {\n\t\treturn fmt.Errorf(\"registry: encode registration %q: %w\", reg.DeploymentId, err)\n\t}\n\tif err := os.WriteFile(filename, bytes, 0644); err != nil {\n\t\treturn fmt.Errorf(\"registry: write %q: %w\", filename, err)\n\t}\n\treturn nil\n}\n```", "  ```\nfunc (r *Registry) Register(ctx context.Context, reg Registration) error {\n\t// Create JSON representation of Registration.\n\tbytes, err := json.Marshal(reg)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"registry: marshal %q: %w\", reg, err)\n\t}\n\n\t// Create a file for the Registration in the registry directory.\n\tfilename := fmt.Sprintf(\"%s.registration.json\", reg.DeploymentId)\n\tfilename = filepath.Join(r.dir, filename)\n\tfile, err := os.Create(filename)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"registry: create %q: %w\", filename, err)\n\t}\n\tdefer file.Close()\n\n\t// Write the Registration to the file.\n\tif _, err := file.WriteString(string(bytes)); err != nil {\n\t\treturn fmt.Errorf(\"registry: write %q: %w\", filename, err)\n\t}\n\n\treturn nil\n}\n```", "  ```\nfunc (r *Registry) Register(ctx context.Context, reg Registration) error {\n\t// Validate the DeploymentId and App by checking if they are non-empty strings.\n\tif reg.DeploymentId == \"\" {\n\t\treturn errors.New(\"empty deployment id\")\n\t}\n\tif reg.App == \"\" {\n\t\treturn errors.New(\"empty app name\")\n\t}\n\n\t// Store the Registration as a JSON file.\n\tfilename := fmt.Sprintf(\"%s.registration.json\", reg.DeploymentId)\n\tfilename = filepath.Join(r.dir, filename)\n\tbytes, err := json.Marshal(reg)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"registry: marshal registration %q: %w\", reg.DeploymentId, err)\n\t}\n\tif err := os.WriteFile(filename, bytes, os.ModePerm); err != nil {\n\t\treturn fmt.Errorf(\"registry: write file %q: %w\", filename, err)\n\t}\n\n\treturn nil\n}\n```"]}, "weaver-internal/status/registry.go-List": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/status/registry.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package status contains code for implementing status related commands like\n// \"weaver multi status\" and \"weaver single dashboard\".\npackage status\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"syscall\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/files\"\n\t\"github.com/ServiceWeaver/weaver/runtime/colors\"\n)\n\n// A Registry is a persistent collection of Service Weaver application metadata.\n//\n// Tools like \"weaver multi status\" and \"weaver multi dashboard\" use the registry\n// to know which applications are running and to fetch the status of the\n// running applications.\ntype Registry struct {\n\t// A Registry stores registrations as files in a directory. Every\n\t// registration r is stored in a JSON file called\n\t// {r.DeploymentId}.registration.json.\n\t//\n\t// TODO(mwhittaker): Store as protos instead of JSON?\n\tdir string\n\n\t// newClient returns a new status client that curls the provided address.\n\t// It is a field of Registry to enable dependency injection in\n\t// registry_test.go.\n\tnewClient func(string) Server\n}\n\n// A Registration contains basic metadata about a Service Weaver application.\ntype Registration struct {\n\tDeploymentId string // deployment id (e.g, \"eba18295\")\n\tApp          string // app name (e.g., \"todo\")\n\tAddr         string // status server (e.g., \"localhost:12345\")\n}\n\n// Rolodex returns a pretty-printed rolodex displaying the registration.\n//\n//\t\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n//\t\u2502 app        : collatz                              \u2502\n//\t\u2502 deployment : fdeeb059-825b-4606-9e99-e22e63e10552 \u2502\n//\t\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nfunc (r Registration) Rolodex() string {\n\t// Declare the contents.\n\ttype kv struct {\n\t\tkey string\n\t\tval colors.Text\n\t}\n\tprefix, suffix := formatId(r.DeploymentId)\n\tkvs := []kv{\n\t\t{\"app        :\", colors.Text{colors.Atom{S: r.App}}},\n\t\t{\"deployment :\", colors.Text{prefix, suffix}},\n\t}\n\n\tlength := func(t colors.Text) int {\n\t\tvar n int\n\t\tfor _, a := range t {\n\t\t\tn += len(a.S)\n\t\t}\n\t\treturn n\n\t}\n\n\t// Calculate widths.\n\tvalWidth := 0\n\tfor _, kv := range kvs {\n\t\tif length(kv.val) > valWidth {\n\t\t\tvalWidth = length(kv.val)\n\t\t}\n\t}\n\twidth := valWidth + len(kvs[0].key) + 5\n\n\t// Pretty print.\n\tvar b strings.Builder\n\tfmt.Fprintf(&b, \"\u256d%s\u256e\\n\", strings.Repeat(\"\u2500\", width-2))\n\tfor _, kv := range kvs {\n\t\ts := kv.val.String()\n\t\tfmt.Fprintf(&b, \"\u2502 %s %-*s \u2502\\n\", kv.key, valWidth+len(s)-length(kv.val), s)\n\t}\n\tfmt.Fprintf(&b, \"\u2570%s\u256f\\n\", strings.Repeat(\"\u2500\", width-2))\n\treturn b.String()\n}\n\n// NewRegistry returns a registry that persists data to the provided directory.\n\n\n\n\n\n\n\n\n\n\n\n\n// Register adds a registration to the registry.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Unregister removes a registration from the registry.\nfunc (r *Registry) Unregister(_ context.Context, deploymentId string) error {\n\tfilename := fmt.Sprintf(\"%s.registration.json\", deploymentId)\n\tfilename = filepath.Join(r.dir, filename)\n\tif err := os.Remove(filename); err != nil {\n\t\treturn fmt.Errorf(\"registry: remove %q: %w\", filename, err)\n\t}\n\treturn nil\n}\n\n// Get returns the Registration for the provided deployment. If the deployment\n// doesn't exist or is not active, a non-nil error is returned.\nfunc (r *Registry) Get(ctx context.Context, deploymentId string) (Registration, error) {\n\t// TODO(mwhittaker): r.list() reads and parses every registration file.\n\t// This is inefficient, as we could instead stop reading and parsing as\n\t// soon as we find the corresponding registration file. Even more\n\t// efficient, we could match the deploymentId to the filenames instead of\n\t// reading and parsing the files. Since the number of registrations is\n\t// small, and the size of every registration file is small, I don't think\n\t// these optimizations are urgently needed.\n\tregs, err := r.list()\n\tif err != nil {\n\t\treturn Registration{}, err\n\t}\n\tfor _, reg := range regs {\n\t\tif reg.DeploymentId != deploymentId {\n\t\t\tcontinue\n\t\t}\n\t\tif r.dead(ctx, reg) {\n\t\t\treturn Registration{}, fmt.Errorf(\"registry: deployment %q not found\", deploymentId)\n\t\t}\n\t\treturn reg, nil\n\t}\n\treturn Registration{}, fmt.Errorf(\"registry: deployment %q not found\", deploymentId)\n}\n\n// List returns all active Registrations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// list returns all registrations, dead or alive.\nfunc (r *Registry) list() ([]Registration, error) {\n\tentries, err := os.ReadDir(r.dir)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"registry: read dir %q: %w\", r.dir, err)\n\t}\n\n\tvar regs []Registration\n\tfor _, entry := range entries {\n\t\tif !strings.HasSuffix(entry.Name(), \".registration.json\") {\n\t\t\t// Ignore non-registration files in the registry directory.\n\t\t\tcontinue\n\t\t}\n\t\tfilename := filepath.Join(r.dir, entry.Name())\n\t\tbytes, err := os.ReadFile(filename)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"registry: read file %q: %w\", filename, err)\n\t\t}\n\t\tvar reg Registration\n\t\tif err := json.Unmarshal(bytes, &reg); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"registry: decode file %q: %w\", filename, err)\n\t\t}\n\t\tregs = append(regs, reg)\n\t}\n\treturn regs, nil\n}\n\n// dead returns whether the provided registration is associated with a\n// deployment that is definitely dead.\nfunc (r *Registry) dead(ctx context.Context, reg Registration) bool {\n\tstatus, err := r.newClient(reg.Addr).Status(ctx)\n\tswitch {\n\tcase errors.Is(err, syscall.ECONNREFUSED):\n\t\t// There is no status server for this deployment, so we consider\n\t\t// the deployment dead.\n\t\treturn true\n\tcase errors.Is(err, syscall.Errno(10061)):\n\t\t// The syscall.ECONNREFUSED doesn't work on Windows. Windows will\n\t\t// return WSAECONNREFUSED(syscall.Errno = 10061) when the connection is\n\t\t// refused.\n\t\treturn true\n\tcase err != nil:\n\t\t// Something went wrong. The deployment may be dead, but we're not 100%\n\t\t// sure, so we return false.\n\t\treturn false\n\tcase status.DeploymentId != reg.DeploymentId:\n\t\t// The status server for this deployment is dead and has been\n\t\t// superseded by a newer status server.\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n```\n# Function Signature: func (r *Registry) List(ctx context.Context) ([]Registration, error) {\n# Natural Language Annotations: // List returns all active Registrations.\nPlease return the generated code surrounded by ```\n", "response": ["  Here is the generated code:\n```\nfunc (r *Registry) List(ctx context.Context) ([]Registration, error) {\n\treturn r.list()\n}\n```", "  ```\nfunc (r *Registry) List(ctx context.Context) ([]Registration, error) {\n\t// Refresh the cache before returning.\n\tif err := r.Refresh(ctx); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Return the cached list of active registrations.\n\treturn r.activeRegistrations(), nil\n}\n```", "  ```\nfunc (r *Registry) List(ctx context.Context) ([]Registration, error) {\n\t// First, we call r.list() to get all Registrations, dead or alive.\n\t// We may as well get all the registrations, since we're going to filter\n\t// them anyway.\n\tregs, err := r.list()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Next, we initialize an empty slice to store the active Registrations.\n\tvar activeRegs []Registration\n\n\t// We then iterate over the slice of registrations and filter out the\n\t// active ones.\n\tfor _, reg := range regs {\n\t\tif r.dead(ctx, reg) {\n\t\t\t// If the registration is dead, we don't include it in the\n\t\t\t// active Registrations slice.\n\t\t\tcontinue\n\t\t}\n\t\tactiveRegs = append(activeRegs, reg)\n\t}\n\n\treturn activeRegs, nil\n}\n```"]}, "weaver-internal/routing/routing.go-FormatAssignment": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/routing/routing.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package routing includes utilities for routing and assignments. See\n// https://serviceweaver.dev/docs.html#routing for more information on routing.\npackage routing\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n)\n\n// FormatAssignment pretty formats the provided assignment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// EqualSlices returns an assignment with slices of roughly equal size.\n// Replicas are assigned to slices in a round robin fashion. The returned\n// assignment has a version of 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// nextPowerOfTwo returns the least power of 2 that is greater or equal to x.\nfunc nextPowerOfTwo(x int) int {\n\tswitch {\n\tcase x == 0:\n\t\treturn 1\n\tcase x&(x-1) == 0:\n\t\t// x is already power of 2.\n\t\treturn x\n\tdefault:\n\t\treturn int(math.Pow(2, math.Ceil(math.Log2(float64(x)))))\n\t}\n}\n\n```\n# Function Signature: func FormatAssignment(a *protos.Assignment) string {\n# Natural Language Annotations: // FormatAssignment pretty formats the provided assignment.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc FormatAssignment(a *protos.Assignment) string {\n\tif a == nil {\n\t\treturn \"\"\n\t}\n\tvar buf strings.Builder\n\tfor i, replica := range a.Replicas {\n\t\tif i > 0 {\n\t\t\tbuf.WriteString(\",\")\n\t\t}\n\t\tbuf.WriteString(fmt.Sprintf(\"%d:%s\", replica.ID, replica.URL))\n\t}\n\treturn buf.String()\n}\n```", "  ```\npackage routing\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n)\n\n// FormatAssignment pretty formats the provided assignment.\nfunc FormatAssignment(a *protos.Assignment) string {\n\tif a == nil || len(a.Replicas) == 0 {\n\t\treturn \"\"\n\t}\n\tvar replicas []string\n\tfor _, replica := range a.Replicas {\n\t\treplicas = append(replicas, fmt.Sprintf(\"%s->%s\", replica.ID, replica.Owner))\n\t}\n\tsort.Strings(replicas)\n\treturn strings.Join(replicas, \",\")\n}\n```", "  ```\nfunc FormatAssignment(a *protos.Assignment) string {\n\tfmt.Println(\"Assignment \\n\"\n\t\t\"----------------------- \\n\"\n\t\t\"Replicas: %s \\n\"\n\t\t\"Slices: %s \\n\"\n\t\t\"Replicas per slice: %s \\n\"\n\t\t\"Slice id: %s \\n\",\n\t\tstrings.Join(a.Replicas, \", \"),\n\t\tstrings.Join(a.Slices, \", \"),\n\t\tstrings.Join(a.ReplicasPerSlice, \", \"),\n\t\tstrings.Join(a.SliceID, \", \"))\n\treturn \"\"\n}\n```"]}, "weaver-internal/routing/routing.go-EqualSlices": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/routing/routing.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package routing includes utilities for routing and assignments. See\n// https://serviceweaver.dev/docs.html#routing for more information on routing.\npackage routing\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n)\n\n// FormatAssignment pretty formats the provided assignment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// EqualSlices returns an assignment with slices of roughly equal size.\n// Replicas are assigned to slices in a round robin fashion. The returned\n// assignment has a version of 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// nextPowerOfTwo returns the least power of 2 that is greater or equal to x.\nfunc nextPowerOfTwo(x int) int {\n\tswitch {\n\tcase x == 0:\n\t\treturn 1\n\tcase x&(x-1) == 0:\n\t\t// x is already power of 2.\n\t\treturn x\n\tdefault:\n\t\treturn int(math.Pow(2, math.Ceil(math.Log2(float64(x)))))\n\t}\n}\n\n```\n# Function Signature: func EqualSlices(replicas []string) *protos.Assignment {\n# Natural Language Annotations: // EqualSlices returns an assignment with slices of roughly equal size.\n// Replicas are assigned to slices in a round robin fashion. The returned\n// assignment has a version of 0.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc EqualSlices(replicas []string) *protos.Assignment {\n\tsliceSize := nextPowerOfTwo(len(replicas) / 2)\n\tslices := make([]*protos.Assignment_Slice, sliceSize)\n\tfor i := 0; i < sliceSize; i++ {\n\t\tslices[i] = &protos.Assignment_Slice{\n\t\t\tReplicas: replicas[i*2 : i*2+2],\n\t\t}\n\t}\n\treturn &protos.Assignment{\n\t\tSlices: slices,\n\t\tVersion: 0,\n\t}\n}\n```", "  ```\nfunc EqualSlices(replicas []string) *protos.Assignment {\n\t// Create a new Assignment message with version 0\n\tassignment := &protos.Assignment{\n\t\tVersion: 0,\n\t}\n\n\t// Identify the next power of 2 greater than or equal to the number of replicas\n\tnumSlices := nextPowerOfTwo(len(replicas))\n\n\t// Create a slice of the appropriate size\n\tassignment.Slices = make([]*protos.Slice, numSlices)\n\n\t// Assign replicas to slices in a round-robin fashion\n\tfor i := 0; i < len(replicas); i++ {\n\t\tassignment.Slices[i%numSlices].Replicas = append(assignment.Slices[i%numSlices].Replicas, replicas[i])\n\t}\n\n\treturn assignment\n}\n```", "  ```\npackage routing\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n)\n\n// EqualSlices returns an assignment with slices of roughly equal size.\n// Replicas are assigned to slices in a round robin fashion. The returned\n// assignment has a version of 0.\nfunc EqualSlices(replicas []string) *protos.Assignment {\n\tassignment := &protos.Assignment{Version: 0}\n\tnumSlices, replicaOffset := len(replicas)/2, 0\n\tfor _, replica := range replicas {\n\t\tassignment.Slices = append(assignment.Slices, &protos.Slice{\n\t\t\tReplicas: []string{replica},\n\t\t})\n\t\tif replicaOffset++; replicaOffset == numSlices {\n\t\t\treplicaOffset = 0\n\t\t}\n\t}\n\treturn assignment\n}\n```"]}, "weaver-internal/queue/queue.go-Pop": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/queue/queue.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage queue\n\nimport (\n\t\"context\"\n\t\"sync\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/cond\"\n)\n\n// Queue is a thread-safe queue.\n//\n// Unlike a Go channel, Queue doesn't have any constraints on how many\n// elements can be in the queue.\ntype Queue[T any] struct {\n\tmu    sync.Mutex\n\telems []T\n\twait  *cond.Cond\n}\n\n// Push places elem at the back of the queue.\nfunc (q *Queue[T]) Push(elem T) {\n\tq.mu.Lock()\n\tdefer q.mu.Unlock()\n\tq.init()\n\tq.elems = append(q.elems, elem)\n\tq.wait.Signal()\n}\n\n// Pop removes the element from the front of the queue and returns it.\n// It blocks if the queue is empty.\n// It returns an error if the passed-in context is canceled.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// init initializes the queue.\n//\n// REQUIRES: q.mu is held\nfunc (q *Queue[T]) init() {\n\tif q.wait == nil {\n\t\tq.wait = cond.NewCond(&q.mu)\n\t}\n}\n\n```\n# Function Signature: func (q *Queue[T]) Pop(ctx context.Context) (elem T, err error) {\n# Natural Language Annotations: // Pop removes the element from the front of the queue and returns it.\n// It blocks if the queue is empty.\n// It returns an error if the passed-in context is canceled.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (q *Queue[T]) Pop(ctx context.Context) (elem T, err error) {\n\tq.mu.Lock()\n\tdefer q.mu.Unlock()\n\tq.init()\n\tfor len(q.elems) == 0 {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn elem, ctx.Err()\n\t\tdefault:\n\t\t\tq.wait.Wait()\n\t\t}\n\t}\n\telem = q.elems[0]\n\tq.elems = q.elems[1:]\n\treturn elem, nil\n}\n```", "  ```\nfunc (q *Queue[T]) Pop(ctx context.Context) (elem T, err error) {\n\tq.mu.Lock()\n\tdefer q.mu.Unlock()\n\n\tfor {\n\t\tif len(q.elems) > 0 {\n\t\t\telem := q.elems[0]\n\t\t\tq.elems = q.elems[1:]\n\t\t\treturn elem, nil\n\t\t}\n\t\tif err := q.wait.Wait(ctx); err != nil {\n\t\t\treturn elem, err\n\t\t}\n\t}\n}\n```", "  ```\nfunc (q *Queue[T]) Pop(ctx context.Context) (elem T, err error) {\n\tq.mu.Lock()\n\tdefer q.mu.Unlock()\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn elem, ctx.Err()\n\tcase <-q.wait.Cond():\n\t\telem := q.elems[0]\n\t\tq.elems = q.elems[1:]\n\t\treturn elem, nil\n\t}\n}\n```"]}, "weaver-internal/pipe/pipe.go-MakePipePair": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/pipe/pipe.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package pipe extends os.exec, making it easier to create pipes to subcommands.\npackage pipe\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"os/exec\"\n)\n\n// Cmd is drop-in replacement for exec.Cmd, extended with the Pipe method.\ntype Cmd struct {\n\t*exec.Cmd\n\tcloseAfterStart []io.Closer // closed after Start()\n\tcloseAfterWait  []io.Closer // closed after Wait()\n}\n\n// CommandContext returns a new Cmd. See exec.CommandContext for details.\nfunc CommandContext(ctx context.Context, name string, arg ...string) *Cmd {\n\treturn &Cmd{Cmd: exec.CommandContext(ctx, name, arg...)}\n}\n\n// PipePair holds a pair of pipes that can be used for bi-directional\n// communication with a child process.\ntype PipePair struct {\n\tParentReader io.ReadCloser  // Reader from which parent can read\n\tParentWriter io.WriteCloser // Writer to which parent can write\n\tChildReader  uintptr        // Descriptor from which child can read\n\tChildWriter  uintptr        // Descriptor to which child can write\n}\n\n// MakePipePair makes a pair of pipes that can be used for bi-directional\n// communication with the child process.\n//\n// Cmd.ExtraFiles should not be modified directly if MakePipePair is called.\n//\n// Wait will close ParentWriter automatically after seeing the command exit. A\n// caller need only close ParentWriter to force the pipe to close sooner. For\n// example, if the command being run will not exit until standard input is\n// closed, the caller must close ParentWriter.\n//\n// Wait will close ParentReader automatically after seeing the command exit, so\n// most callers need not close ParentReader themselves. It is thus incorrect to\n// call Wait before all reads from ParentReader have completed. For the same\n// reason, it is incorrect to use Run when using MakePipePair. See the\n// exec.Cmd.StdoutPipe example [1] for idiomatic usage.\n//\n// [1]: https://pkg.go.dev/os/exec#example-Cmd.StdoutPipe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c *Cmd) registerPipe(local, remote *os.File) uintptr {\n\tc.closeAfterStart = append(c.closeAfterStart, remote)\n\tc.closeAfterWait = append(c.closeAfterWait, local)\n\treturn addInheritedFile(c.Cmd, remote)\n}\n\n// Start is identical to exec.Command.Start.\nfunc (c *Cmd) Start() error {\n\tif err := c.Cmd.Start(); err != nil {\n\t\treturn err\n\t}\n\tcloseAll(&c.closeAfterStart)\n\treturn nil\n}\n\n// Wait is identical to exec.Command.Wait.\nfunc (c *Cmd) Wait() error {\n\tif err := c.Cmd.Wait(); err != nil {\n\t\treturn err\n\t}\n\tcloseAll(&c.closeAfterWait)\n\treturn nil\n}\n\n// Cleanup cleans up any unused resources.\nfunc (c *Cmd) Cleanup() {\n\tcloseAll(&c.closeAfterStart)\n\tcloseAll(&c.closeAfterWait)\n}\n\nfunc closeAll(files *[]io.Closer) {\n\tfor _, f := range *files {\n\t\tf.Close()\n\t}\n\t*files = nil\n}\n\n```\n# Function Signature: func (c *Cmd) MakePipePair() (PipePair, error) {\n# Natural Language Annotations: // MakePipePair makes a pair of pipes that can be used for bi-directional\n// communication with the child process.\n//\n// Cmd.ExtraFiles should not be modified directly if MakePipePair is called.\n//\n// Wait will close ParentWriter automatically after seeing the command exit. A\n// caller need only close ParentWriter to force the pipe to close sooner. For\n// example, if the command being run will not exit until standard input is\n// closed, the caller must close ParentWriter.\n//\n// Wait will close ParentReader automatically after seeing the command exit, so\n// most callers need not close ParentReader themselves. It is thus incorrect to\n// call Wait before all reads from ParentReader have completed. For the same\n// reason, it is incorrect to use Run when using MakePipePair. See the\n// exec.Cmd.StdoutPipe example [1] for idiomatic usage.\n//\n// [1]: https://pkg.go.dev/os/exec#example-Cmd.StdoutPipe\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (c *Cmd) MakePipePair() (PipePair, error) {\n\t// ParentReader, ParentWriter are used by the parent to communicate with the child.\n\tParentReader, ParentWriter, err := os.Pipe()\n\tif err != nil {\n\t\treturn PipePair{}, err\n\t}\n\t// ChildReader, ChildWriter are used by the child to communicate with the parent.\n\tChildReader, ChildWriter, err := os.Pipe()\n\tif err != nil {\n\t\treturn PipePair{}, err\n\t}\n\tchildReaderFd := c.registerPipe(ChildReader, ParentWriter)\n\tchildWriterFd := c.registerPipe(ChildWriter, ParentReader)\n\treturn PipePair{\n\t\tParentReader: ParentReader,\n\t\tParentWriter: ParentWriter,\n\t\tChildReader:  childReaderFd,\n\t\tChildWriter:  childWriterFd,\n\t}, nil\n}\n```", "  ```\nfunc (c *Cmd) MakePipePair() (PipePair, error) {\n\tparentReader, childReader, err := os.Pipe()\n\tif err != nil {\n\t\treturn PipePair{}, err\n\t}\n\tparentWriter, childWriter, err := os.Pipe()\n\tif err != nil {\n\t\treturn PipePair{}, err\n\t}\n\t// Add the pipes to the Cmd's extra files.\n\tchildReaderID := c.registerPipe(parentReader, childReader)\n\tchildWriterID := c.registerPipe(parentWriter, childWriter)\n\treturn PipePair{\n\t\tParentReader: readCloser{parentReader},\n\t\tParentWriter: writeCloser{parentWriter},\n\t\tChildReader:  childReaderID,\n\t\tChildWriter:  childWriterID,\n\t}, nil\n}\n```\nThis implementation creates two pipes using `os.Pipe` and then adds them as extra files to the Cmd's `ExtraFiles` attribute using the `registerPipe` function. It then returns a `PipePair` struct containing the pipes and their descriptors.\n\nIt's worth noting that this implementation is based on the comments in the code file you provided, and it may need to be modified or updated based on the specific requirements of your project.", "  ```\nfunc (c *Cmd) MakePipePair() (PipePair, error) {\n\tremoteRead, err := os.OpenFile((c.ExtraFiles[0]).Name(), os.O_RDONLY, os.ModePerm)\n\tif err != nil {\n\t\treturn PipePair{}, err\n\t}\n\tdefer remoteRead.Close()\n\n\t// Note that we're creating a new pipe pair here, which means\n\t// we need to create two new os.File objects.\n\tlocalRead, localWrite, err := os.Pipe()\n\tif err != nil {\n\t\treturn PipePair{}, err\n\t}\n\n\t// We add the new pipes to the ExtraFiles slice, just like in CommandContext,\n\t// but we need to make sure to close them after the command has finished.\n\tc.ExtraFiles = c.ExtraFiles[:]\n\tc.ExtraFiles = append(c.ExtraFiles, localRead, localWrite)\n\tc.closeAfterStart = append(c.closeAfterStart, localRead, localWrite)\n\n\t// We also need to register the parent's side of the pipe with the command,\n\t// so that it can be inherited by the child process.\n\tc.registerPipe(localRead, localWrite)\n\n\treturn PipePair{\n\t\tParentReader: localRead,\n\t\tParentWriter: localWrite,\n\t\tChildReader:  addInheritedFile(c.Cmd, remoteRead),\n\t}, nil\n}\n```"]}, "weaver-internal/register/writeonce.go-Write": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/register/writeonce.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package register implements a write-once register.\npackage register\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n)\n\n// WriteOnce is a concurrent write-once register.\ntype WriteOnce[T any] struct {\n\tmu      sync.Mutex\n\tc       sync.Cond\n\twritten bool\n\tval     T\n}\n\n// Write writes to the register, or panics if the register was already written.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// TryWrite tries to write to the register and returns if the write succeeds.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Read returns the value of the register, blocking until it is written.\n\n\n\n\n\n\n\n\n\n\n\n// init initializes the register. We have an init method rather than a\n// WriteOnce constructor so that the zero value of WriteOnce is valid.\n//\n// REQUIRES: w.mu is held.\nfunc (w *WriteOnce[T]) init() {\n\tif w.c.L == nil {\n\t\tw.c.L = &w.mu\n\t}\n}\n\n```\n# Function Signature: func (w *WriteOnce[T]) Write(val T) {\n# Natural Language Annotations: // Write writes to the register, or panics if the register was already written.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (w *WriteOnce[T]) Write(val T) {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\tif w.written {\n\t\tpanic(\"write-once register already written\")\n\t}\n\tw.written = true\n\tw.val = val\n\tw.c.Broadcast()\n}\n```", "  ```\nfunc (w *WriteOnce[T]) Write(val T) {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\n\tif w.written {\n\t\tpanic(\"WriteOnce has already been written\")\n\t}\n\n\tw.val = val\n\tw.written = true\n\tw.c.Broadcast()\n}\n```", "  ```\nfunc (w *WriteOnce[T]) Write(val T) {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\n\tif w.written {\n\t\tpanic(\"Trying to write to a write-once register that was already written\")\n\t}\n\n\tw.val = val\n\tw.written = true\n\tw.c.Broadcast()\n}\n```\nThis function is an implementation of the Write method for the WriteOnce[T] struct, which is a concurrent write-once register described in the comments. This function takes a value in the parameter val, locks the mu mutex, checks if the register is already written, panics if it is, sets the value in the val field of the structure, sets the written flag, and then broadcasts a signal to unblock any blocked reader goroutines.\nIt uses the defer statement to ensure that the mutex is unlocked even if the function returns before it acquires it.\nThe fact that this function panics when the register is already written implies that it is not intended to be called twice on the same register. If a client wants to write a new value to the register, they should create a new register type and call the Write function on that."]}, "weaver-internal/register/writeonce.go-TryWrite": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/register/writeonce.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package register implements a write-once register.\npackage register\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n)\n\n// WriteOnce is a concurrent write-once register.\ntype WriteOnce[T any] struct {\n\tmu      sync.Mutex\n\tc       sync.Cond\n\twritten bool\n\tval     T\n}\n\n// Write writes to the register, or panics if the register was already written.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// TryWrite tries to write to the register and returns if the write succeeds.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Read returns the value of the register, blocking until it is written.\n\n\n\n\n\n\n\n\n\n\n\n// init initializes the register. We have an init method rather than a\n// WriteOnce constructor so that the zero value of WriteOnce is valid.\n//\n// REQUIRES: w.mu is held.\nfunc (w *WriteOnce[T]) init() {\n\tif w.c.L == nil {\n\t\tw.c.L = &w.mu\n\t}\n}\n\n```\n# Function Signature: func (w *WriteOnce[T]) TryWrite(val T) bool {\n# Natural Language Annotations: // TryWrite tries to write to the register and returns if the write succeeds.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (w *WriteOnce[T]) TryWrite(val T) bool {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\tif w.written {\n\t\treturn false\n\t}\n\tw.val = val\n\tw.written = true\n\tw.c.Broadcast()\n\treturn true\n}\n```", "  ```\nfunc (w *WriteOnce[T]) TryWrite(val T) bool {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\tif w.written {\n\t\treturn false\n\t}\n\tw.written = true\n\tw.val = val\n\tw.c.Broadcast()\n\treturn true\n}\n```", "  ```\nfunc (w *WriteOnce[T]) TryWrite(val T) bool {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\n\tif w.written {\n\t\treturn false\n\t}\n\n\tw.written = true\n\tw.val = val\n\tw.c.Broadcast()\n\treturn true\n}\n```\nThis is the implementation of the TryWrite function for the WriteOnce type in golang.\n\nThe function is exported, so it can be accessed from outside the package.\n\nThe function takes a pointer to a WriteOnce instance as a receiver, and a value of type T as an argument.\n\nThe function first acquires the lock on the WriteOnce instance using the lock method of the mutex.\n\nIt then checks if the write succeeded by checking if the WriteOnce instance is already written. If it is, the function returns false.\n\nIf the write is successful, the function sets the value of the WriteOnce instance to the value passed as an argument and broadcasts a signal on the associated condition variable using the Broadcast method of the condition variable.\n\nFinally, the function returns true to indicate that the write succeeded.\n\nThe function then releases the lock on the WriteOnce instance using the Unlock method of the mutex."]}, "weaver-internal/register/writeonce.go-Read": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/register/writeonce.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package register implements a write-once register.\npackage register\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n)\n\n// WriteOnce is a concurrent write-once register.\ntype WriteOnce[T any] struct {\n\tmu      sync.Mutex\n\tc       sync.Cond\n\twritten bool\n\tval     T\n}\n\n// Write writes to the register, or panics if the register was already written.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// TryWrite tries to write to the register and returns if the write succeeds.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Read returns the value of the register, blocking until it is written.\n\n\n\n\n\n\n\n\n\n\n\n// init initializes the register. We have an init method rather than a\n// WriteOnce constructor so that the zero value of WriteOnce is valid.\n//\n// REQUIRES: w.mu is held.\nfunc (w *WriteOnce[T]) init() {\n\tif w.c.L == nil {\n\t\tw.c.L = &w.mu\n\t}\n}\n\n```\n# Function Signature: func (w *WriteOnce[T]) Read() T {\n# Natural Language Annotations: // Read returns the value of the register, blocking until it is written.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (w *WriteOnce[T]) Read() T {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\tfor !w.written {\n\t\tw.c.Wait()\n\t}\n\treturn w.val\n}\n```", "  ```\nfunc (w *WriteOnce[T]) Read() T {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\n\tfor !w.written {\n\t\tw.c.Wait()\n\t}\n\n\treturn w.val\n}\n```", "  ```\nfunc (w *WriteOnce[T]) Read() T {\n    w.mu.Lock()\n    defer w.mu.Unlock()\n    for !w.written {\n        w.c.Wait()\n    }\n    return w.val\n}\n```\nNote that this implementation is using a sync.Mutex and a sync.Cond to coordinate the concurrent access to the register. The `w.mu.Lock()` statement acquires the mutex, and the `defer w.mu.Unlock()` statement ensures that the mutex is released even if the function returns for some other reason.\n\nThe `for !w.written {}` loop checks if the register has been written, and if it hasn't, it will block the calling goroutine by calling `w.c.Wait()`. Once the register is written, the loop will break, and the function will return the value of the register using the `return w.val` statement."]}, "weaver-internal/versioned/versioned.go-RLock": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/versioned/versioned.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage versioned\n\nimport (\n\t\"sync\"\n\n\t\"github.com/google/uuid\"\n)\n\n// Versioned[T] is a linearizable register storing a value of type T.\n// Each update to the value changes its unique (but not necessarily ordered)\n// version. Spurious version changes are possible, i.e., the version may change\n// even if the value hasn't.\n//\n// Like a sync.Mutex, Versioned should not be copied.\ntype Versioned[T any] struct {\n\tmu      sync.RWMutex\n\tchanged sync.Cond\n\tVal     T\n\tversion string\n}\n\nfunc Version[T any](val T) *Versioned[T] {\n\tv := &Versioned[T]{Val: val, version: uuid.New().String()}\n\tv.changed.L = &v.mu\n\treturn v\n}\n\n// Lock acquires the write lock.\nfunc (v *Versioned[T]) Lock() {\n\tv.mu.Lock()\n}\n\n// Unlock releases the write lock.\nfunc (v *Versioned[T]) Unlock() {\n\tv.version = uuid.New().String()\n\tv.changed.Broadcast()\n\tv.mu.Unlock()\n}\n\n// RLock waits until the current version is different than the passed-in\n// version, and then acquires the read lock and returns the new\n// version.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// RUnlock releases the read lock.\nfunc (v *Versioned[T]) RUnlock() {\n\tv.mu.RUnlock()\n}\n\n```\n# Function Signature: func (v *Versioned[T]) RLock(version string) string {\n# Natural Language Annotations: // RLock waits until the current version is different than the passed-in\n// version, and then acquires the read lock and returns the new\n// version.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (v *Versioned[T]) RLock(version string) string {\n\tv.mu.RLock()\n\tfor v.version == version {\n\t\tv.changed.Wait()\n\t}\n\treturn v.version\n}\n```", "  ```\nfunc (v *Versioned[T]) RLock(version string) string {\n\tv.mu.Lock()\n\tfor v.version == version {\n\t\tv.changed.Wait()\n\t}\n\tv.mu.Unlock()\n\treturn v.version\n}\n```", "  ```\nfunc (v *Versioned[T]) RLock(version string) string {\n    // Wait until the current version is different than the passed-in version.\n    v.changed.L.Lock()\n    defer v.changed.L.Unlock()\n    for {\n        if v.version != version {\n            break\n        }\n        v.changed.Wait()\n    }\n    // Acquire the read lock and return the new version.\n    v.mu.RLock()\n    return v.version\n}\n```"]}, "weaver-internal/net/call/stub.go-makeStubMethods": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/net/call/stub.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage call\n\nimport (\n\t\"context\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// stub holds information about a client stub to the remote component.\ntype stub struct {\n\tconn          Connection   // connection to talk to the remote component\n\tmethods       []stubMethod // per method info\n\ttracer        trace.Tracer // component tracer\n\tinjectRetries int          // Number of artificial retries per retriable call\n}\n\ntype stubMethod struct {\n\tkey   MethodKey // key for remote component method\n\tretry bool      // Whether or not the method should be retred\n}\n\nvar _ codegen.Stub = &stub{}\n\n// NewStub creates a client-side stub of the type matching reg. Calls on the stub are sent on\n// conn to the component with the specified name.\nfunc NewStub(name string, reg *codegen.Registration, conn Connection, tracer trace.Tracer, injectRetries int) codegen.Stub {\n\treturn &stub{\n\t\tconn:          conn,\n\t\tmethods:       makeStubMethods(name, reg),\n\t\ttracer:        tracer,\n\t\tinjectRetries: injectRetries,\n\t}\n}\n\n// Tracer implements the codegen.Stub interface.\nfunc (s *stub) Tracer() trace.Tracer {\n\treturn s.tracer\n}\n\n// Run implements the codegen.Stub interface.\nfunc (s *stub) Run(ctx context.Context, method int, args []byte, shardKey uint64) (result []byte, err error) {\n\tm := s.methods[method]\n\topts := CallOptions{\n\t\tRetry:    m.retry,\n\t\tShardKey: shardKey,\n\t}\n\tn := 1\n\tif m.retry {\n\t\tn += s.injectRetries\n\t}\n\tfor i := 0; i < n; i++ {\n\t\tresult, err = s.conn.Call(ctx, m.key, args, opts)\n\t\t// No backoff since these retries are fake ones injected for testing.\n\t}\n\treturn\n}\n\n// makeStubMethods returns a slice of stub methods for the component methods of reg.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func makeStubMethods(fullName string, reg *codegen.Registration) []stubMethod {\n# Natural Language Annotations: // makeStubMethods returns a slice of stub methods for the component methods of reg.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc makeStubMethods(name string, reg *codegen.Registration) []stubMethod {\n\tif reg == nil {\n\t\treturn nil\n\t}\n\tmethods := make([]stubMethod, reg.MethodCount())\n\tfor i := 0; i < len(methods); i++ {\n\t\tmethodName := name\n\t\tif name != \"\" {\n\t\t\tmethodName += \"/\"\n\t\t}\n\t\tmethodName += reg.MethodName(i)\n\t\tmethods[i] = stubMethod{\n\t\t\tkey:   MethodKey(methodName),\n\t\t\tretry: reg.MethodRetry(i),\n\t\t}\n\t}\n\treturn methods\n}\n```", "  ```\nfunc makeStubMethods(fullName string, reg *codegen.Registration) []stubMethod {\n\tvar methods []stubMethod\n\tfor i := range reg.Methods {\n\t\tmethod := &reg.Methods[i]\n\t\tmethods = append(methods, stubMethod{\n\t\t\tkey: MethodKey{\n\t\t\t\t// Note that the FunctionName field is ignored since it's not used in stubs.\n\t\t\t\tComponentName: fullName,\n\t\t\t\tMethodName:    method.Name,\n\t\t\t},\n\t\t\tretry: method.Retry,\n\t\t})\n\t}\n\treturn methods\n}\n```\nPlease find the code embedding above.", "  ```\nfunc makeStubMethods(fullName string, reg *codegen.Registration) []stubMethod {\n\tvar methods []stubMethod\n\t// If there are no methods in the reg, return an empty slice.\n\tif len(reg.ComponentMethods) == 0 {\n\t\treturn methods\n\t}\n\n\t// Get the application and client names from the full name.\n\tregParts := strings.Split(fullName, \".\")\n\tif len(regParts) != 2 {\n\t\treturn methods\n\t}\n\tappName := regParts[0]\n\tclientName := regParts[1]\n\n\t// Iterate over the component methods of the reg.\n\tfor _, cm := range reg.ComponentMethods {\n\t\t// If the method is not a client method, skip it.\n\t\tif !cm.ClientMethod {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Create a new stubMethod from the component method.\n\t\tmethods = append(methods, stubMethod{\n\t\t\tkey:   MethodKey{appName, clientName, cm.Name},\n\t\t\tretry: cm.Retry,\n\t\t})\n\t}\n\treturn methods\n}\n```"]}, "weaver-internal/net/call/call.go-Serve": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/net/call/call.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package call implements an RPC mechanism.\npackage call\n\n// # Overview\n//\n// RPCs are conveyed across a bidirectional connection. A connection carries\n// a sequence of messages in each direction. A message has the following\n// information:\n//\trequest-id\t-- A number that identifies a particular RPC\n//\tmessage-type\t-- E.g., request or response\n//\tlength\t\t-- How many payload bytes follow\n//\tpayload\t\t-- length bytes of payload\n// The payload format varies depending on the message-type.\n// See msg.go for details.\n//\n// # Server operation\n//\n// The server listens for connections (typically on a TCP socket). For\n// each accepted connection, it starts a readRequests() goroutine that\n// reads messages from that connection. When readRequests() gets a\n// request message, it starts a runHandler() goroutine. runHandler()\n// looks up the registered handler for the message, runs it, and sends\n// the response back over the connection.\n//\n// # Client operation\n//\n// For each newly discovered server, the client starts a manage() goroutine\n// that connects to server, and then reads messages from the connection. If the\n// network connection breaks, manage() reconnects (after a retry delay).\n//\n// When the client wants to send an RPC, it selects one of its server\n// connections to use, creates a call object, assigns it a new request-id, and\n// registers the object in a map in the connection. It then sends a request\n// message over the connection and waits for the call object to be marked as\n// done.\n//\n// When the response arrives, it is picked up by readAndProcessMessage().\n// readAndProcessMessage() finds the call object corresponding to the\n// request-id in the response, and marks the call object as done which\n// wakes up goroutine that initiated the RPC.\n//\n// If a client is constructed with a non-constant resolver, the client also\n// spawns a watchResolver goroutine that repeatedly calls Resolve on the\n// resolver to get notified of updates to the set of endpoints. When the\n// endpoints are updated, existing connections are retained, and stale\n// connections are transitioned to a \"draining\" state.\n//\n// New RPCs are never issued over draining connections, but the pending\n// requests on a draining connection are allowed to finish. As soon as a\n// draining connection has no active calls, the connection closes itself. If\n// the resolver later returns a new set of endpoints that includes a draining\n// connection that hasn't closed itself, the draining connection is turned\n// back into a normal connection.\nimport (\n\t\"bufio\"\n\t\"context\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/logging\"\n\t\"github.com/ServiceWeaver/weaver/runtime/retry\"\n\t\"go.opentelemetry.io/otel/codes\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// Connection allows a client to send RPCs.\ntype Connection interface {\n\t// Call makes an RPC over a Connection.\n\tCall(context.Context, MethodKey, []byte, CallOptions) ([]byte, error)\n\n\t// Close closes a connection. Pending invocations of Call are cancelled and\n\t// return an error. All future invocations of Call fail and return an error\n\t// immediately. Close can be called more than once.\n\tClose()\n}\n\n// Listener allows the server to accept RPCs.\ntype Listener interface {\n\tAccept() (net.Conn, *HandlerMap, error)\n\tClose() error\n\tAddr() net.Addr\n}\n\n// reconnectingConnection is the concrete client-side Connection implementation.\n// It automatically reconnects to the servers on first call or the first call\n// after a shutdown.\ntype reconnectingConnection struct {\n\topts ClientOptions\n\n\t// mu guards the following fields and some of the fields in the\n\t// clientConnections inside connections and draining.\n\tmu     sync.Mutex\n\tconns  map[string]*clientConnection\n\tclosed bool\n\n\tresolver       Resolver\n\tcancelResolver func()         // cancels the watchResolver goroutine\n\tresolverDone   sync.WaitGroup // used to wait for watchResolver to finish\n}\n\n// connState is the state of a clientConnection (connection to a particular\n// server replica). missing is a special state used for unknown servers. A\n// typical sequence of transitions is:\n//\n//\tmissing -> disconnected -> checking -> idle <-> active -> draining -> missing\n//\n// The events that can cause state transition are:\n//\n// - register: server has shown up in resolver results\n// - unregister: server has dropped from resolver results\n// - connected: a connection has been successfully made\n// - checked: connection has been successfully checked\n// - callstart: call starts on connection\n// - lastdone: last active call on connection has ended\n// - fail: some protocol error is detected on the connection\n// - close: reconnectingConnection is being closed\n//\n// Each event has a corresponding clientConnection method below. See\n// those methods for the corresponding state transitions.\ntype connState int8\n\nconst (\n\tmissing      connState = iota\n\tdisconnected           // cannot be used for calls\n\tchecking               // checking new network connection\n\tidle                   // can be used for calls, no calls in-flight\n\tactive                 // can be used for calls, some calls in-flight\n\tdraining               // some calls in-flight, no new calls should be added\n)\n\nvar connStateNames = []string{\n\t\"missing\",\n\t\"disconnected\",\n\t\"checking\",\n\t\"idle\",\n\t\"active\",\n\t\"draining\",\n}\n\nfunc (s connState) String() string { return connStateNames[s] }\n\n// clientConnection manages one network connection on the client-side.\ntype clientConnection struct {\n\t// Immutable after construction.\n\trc       *reconnectingConnection // owner\n\tcanceler func()                  // Used to cancel goroutine handling connection\n\tlogger   *slog.Logger\n\tendpoint Endpoint\n\n\twlock sync.Mutex // Guards writes to c\n\n\t// Guarded by rc.mu\n\tstate          connState        // current connection state\n\tloggedShutdown bool             // Have we logged a shutdown error?\n\tinBalancer     bool             // Is c registered with the balancer?\n\tc              net.Conn         // Active network connection, or nil\n\tcbuf           *bufio.Reader    // Buffered reader wrapped around c\n\tversion        version          // Version number to use for connection\n\tcalls          map[uint64]*call // In-progress calls\n\tlastID         uint64           // Last assigned request ID for a call\n}\n\nvar _ ReplicaConnection = &clientConnection{}\n\n// call holds the state for an active call at the client.\ntype call struct {\n\tid         uint64\n\tdoneSignal chan struct{}\n\n\t// Fields below are accessed across goroutines, but their access is\n\t// synchronized via doneSignal, i.e., it is never concurrent.\n\terr      error\n\tresponse []byte\n\n\t// Is the call done?\n\t// This field is accessed across goroutines using atomics.\n\tdone uint32 // is the call done?\n\n}\n\n// serverConnection manages one network connection on the server-side.\ntype serverConnection struct {\n\topts        ServerOptions\n\tc           net.Conn\n\tcbuf        *bufio.Reader // Buffered reader wrapped around c\n\twlock       sync.Mutex    // Guards writes to c\n\tmu          sync.Mutex\n\tclosed      bool              // has c been closed?\n\tversion     version           // Version number to use for connection\n\tcancelFuncs map[uint64]func() // Cancellation functions for in-progress calls\n}\n\n// serverState tracks all live server-side connections so we can clean things up when canceled.\ntype serverState struct {\n\topts  ServerOptions\n\tmu    sync.Mutex\n\tconns map[*serverConnection]struct{} // Live connections\n}\n\n// Serve starts listening for connections and requests on l. It always returns a\n// non-nil error and closes l.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// onceCloseListener wraps a Listener, protecting it from multiple Close calls.\ntype onceCloseListener struct {\n\tListener\n\tcloser func() error // Must be result of sync.OnceValue\n}\n\nfunc (oc *onceCloseListener) Close() error {\n\treturn oc.closer()\n}\n\n// ServeOn serves client requests received over an already established\n// network connection with a client. This can be useful in tests or\n// when using custom networking transports.\nfunc ServeOn(ctx context.Context, conn net.Conn, hmap *HandlerMap, opts ServerOptions) {\n\tss := &serverState{opts: opts.withDefaults()}\n\tss.serveConnection(ctx, conn, hmap)\n}\n\nfunc (ss *serverState) serveConnection(ctx context.Context, conn net.Conn, hmap *HandlerMap) {\n\tc := &serverConnection{\n\t\topts:        ss.opts,\n\t\tc:           conn,\n\t\tcbuf:        bufio.NewReader(conn),\n\t\tversion:     initialVersion, // Updated when we hear from client\n\t\tcancelFuncs: map[uint64]func(){},\n\t}\n\tss.register(c)\n\n\tgo c.readRequests(ctx, hmap, func() { ss.unregister(c) })\n}\n\nfunc (ss *serverState) stop() {\n\tss.mu.Lock()\n\tdefer ss.mu.Unlock()\n\tfor c := range ss.conns {\n\t\tc.c.Close() // Should stop readRequests in its tracks\n\t}\n}\n\nfunc (ss *serverState) register(c *serverConnection) {\n\tss.mu.Lock()\n\tdefer ss.mu.Unlock()\n\tif ss.conns == nil {\n\t\tss.conns = map[*serverConnection]struct{}{}\n\t}\n\tss.conns[c] = struct{}{}\n}\n\nfunc (ss *serverState) unregister(c *serverConnection) {\n\tss.mu.Lock()\n\tdefer ss.mu.Unlock()\n\tdelete(ss.conns, c)\n}\n\n// Connect creates a connection to the servers at the endpoints returned by the\n// resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Close closes a connection.\nfunc (rc *reconnectingConnection) Close() {\n\tcloseWithLock := func() {\n\t\trc.mu.Lock()\n\t\tdefer rc.mu.Unlock()\n\t\tif rc.closed {\n\t\t\treturn\n\t\t}\n\t\trc.closed = true\n\t\tfor _, c := range rc.conns {\n\t\t\tc.close()\n\t\t}\n\t}\n\tcloseWithLock()\n\n\t// Cancel the watchResolver goroutine and wait for it to terminate. If the\n\t// watchResolver has already been terminated, then this code is a no-op.\n\t// Note that if we hold the lock while waiting for watchResolver to\n\t// terminate, we may deadlock.\n\trc.cancelResolver()\n\trc.resolverDone.Wait()\n}\n\n// Call makes an RPC over connection c, retrying it on network errors if retries are allowed.\nfunc (rc *reconnectingConnection) Call(ctx context.Context, h MethodKey, arg []byte, opts CallOptions) ([]byte, error) {\n\tif !opts.Retry {\n\t\treturn rc.callOnce(ctx, h, arg, opts)\n\t}\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\tresponse, err := rc.callOnce(ctx, h, arg, opts)\n\t\tif errors.Is(err, Unreachable) || errors.Is(err, CommunicationError) {\n\t\t\tcontinue\n\t\t}\n\t\treturn response, err\n\t}\n\treturn nil, ctx.Err()\n}\n\nfunc (rc *reconnectingConnection) callOnce(ctx context.Context, h MethodKey, arg []byte, opts CallOptions) ([]byte, error) {\n\tvar micros int64\n\tdeadline, haveDeadline := ctx.Deadline()\n\tif haveDeadline {\n\t\t// Send the deadline in the header. We use the relative time instead\n\t\t// of absolute in case there is significant clock skew. This does mean\n\t\t// that we will not count transmission delay against the deadline.\n\t\tmicros = time.Until(deadline).Microseconds()\n\t\tif micros <= 0 {\n\t\t\t// Fail immediately without attempting to send a zero or negative\n\t\t\t// deadline to the server which will be misinterpreted.\n\t\t\t<-ctx.Done()\n\t\t\treturn nil, ctx.Err()\n\t\t}\n\t}\n\n\t// Encode the header.\n\thdr := encodeHeader(ctx, h, micros)\n\n\t// Note that we send the header and the payload as follows:\n\t// [header_length][encoded_header][payload]\n\tvar hdrLen [hdrLenLen]byte\n\tbinary.LittleEndian.PutUint32(hdrLen[:], uint32(len(hdr)))\n\thdrSlice := append(hdrLen[:], hdr...)\n\n\trpc := &call{}\n\trpc.doneSignal = make(chan struct{})\n\n\t// TODO: Arrange to obey deadline in any reconnection done inside startCall.\n\tconn, nc, err := rc.startCall(ctx, rpc, opts)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := writeMessage(nc, &conn.wlock, requestMessage, rpc.id, hdrSlice, arg, rc.opts.WriteFlattenLimit); err != nil {\n\t\tconn.shutdown(\"client send request\", err)\n\t\tconn.endCall(rpc)\n\t\treturn nil, fmt.Errorf(\"%w: %s\", CommunicationError, err)\n\t}\n\n\tif rc.opts.OptimisticSpinDuration > 0 {\n\t\t// Optimistically spin, waiting for the results.\n\t\tfor start := time.Now(); time.Since(start) < rc.opts.OptimisticSpinDuration; {\n\t\t\tif atomic.LoadUint32(&rpc.done) > 0 {\n\t\t\t\treturn rpc.response, rpc.err\n\t\t\t}\n\t\t}\n\t}\n\n\tif cdone := ctx.Done(); cdone != nil {\n\t\tselect {\n\t\tcase <-rpc.doneSignal:\n\t\t\t// Regular return\n\t\tcase <-cdone:\n\t\t\t// Canceled or deadline expired.\n\t\t\tconn.endCall(rpc)\n\n\t\t\tif !haveDeadline || time.Now().Before(deadline) {\n\t\t\t\t// Early cancellation. Tell server about it.\n\t\t\t\tif err := writeMessage(nc, &conn.wlock, cancelMessage, rpc.id, nil, nil, rc.opts.WriteFlattenLimit); err != nil {\n\t\t\t\t\tconn.shutdown(\"client send cancel\", err)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn nil, ctx.Err()\n\t\t}\n\t} else {\n\t\t<-rpc.doneSignal\n\t}\n\treturn rpc.response, rpc.err\n}\n\n// watchResolver watches for updates to the set of endpoints. When a new set of\n// updates is available, watchResolver passes it to updateEndpoints.\n// REQUIRES: version != nil.\n// REQUIRES: rc.mu is not held.\nfunc (rc *reconnectingConnection) watchResolver(ctx context.Context, version *Version) {\n\tdefer rc.resolverDone.Done()\n\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\tendpoints, newVersion, err := rc.resolver.Resolve(ctx, version)\n\t\tif err != nil {\n\t\t\tlogError(rc.opts.Logger, \"watchResolver\", err)\n\t\t\tcontinue\n\t\t}\n\t\tif newVersion == nil {\n\t\t\tlogError(rc.opts.Logger, \"watchResolver\", errors.New(\"non-constant resolver returned a nil version\"))\n\t\t\tcontinue\n\t\t}\n\t\tif *version == *newVersion {\n\t\t\t// Resolver wishes to be called again after an appropriate delay.\n\t\t\tcontinue\n\t\t}\n\t\tif err := rc.updateEndpoints(ctx, endpoints); err != nil {\n\t\t\tlogError(rc.opts.Logger, \"watchResolver\", err)\n\t\t}\n\t\tversion = newVersion\n\t\tr.Reset()\n\t}\n}\n\n// updateEndpoints updates the set of endpoints. Existing connections are\n// retained, and stale connections are closed.\n// REQUIRES: rc.mu is not held.\nfunc (rc *reconnectingConnection) updateEndpoints(ctx context.Context, endpoints []Endpoint) error {\n\trc.mu.Lock()\n\tdefer rc.mu.Unlock()\n\n\tif rc.closed {\n\t\treturn fmt.Errorf(\"updateEndpoints on closed Connection\")\n\t}\n\n\t// Make new endpoints.\n\tkeep := make(map[string]struct{}, len(endpoints))\n\tfor _, endpoint := range endpoints {\n\t\taddr := endpoint.Address()\n\t\tkeep[addr] = struct{}{}\n\t\tif _, ok := rc.conns[addr]; !ok {\n\t\t\t// New endpoint, create connection and manage it.\n\t\t\tctx, cancel := context.WithCancel(ctx)\n\t\t\tc := &clientConnection{\n\t\t\t\trc:       rc,\n\t\t\t\tcanceler: cancel,\n\t\t\t\tlogger:   rc.opts.Logger,\n\t\t\t\tendpoint: endpoint,\n\t\t\t\tcalls:    map[uint64]*call{},\n\t\t\t\tlastID:   0,\n\t\t\t}\n\t\t\trc.conns[addr] = c\n\t\t\tc.register()\n\t\t\tgo c.manage(ctx)\n\t\t}\n\t}\n\n\t// Drop old endpoints.\n\tfor addr, c := range rc.conns {\n\t\tif _, ok := keep[addr]; ok {\n\t\t\t// Still live, so keep it.\n\t\t\tcontinue\n\t\t}\n\t\tc.unregister()\n\t}\n\n\treturn nil\n}\n\n// startCall registers a new in-progress call.\n// REQUIRES: rc.mu is not held.\nfunc (rc *reconnectingConnection) startCall(ctx context.Context, rpc *call, opts CallOptions) (*clientConnection, net.Conn, error) {\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\trc.mu.Lock()\n\t\tif rc.closed {\n\t\t\trc.mu.Unlock()\n\t\t\treturn nil, nil, fmt.Errorf(\"Call on closed Connection\")\n\t\t}\n\n\t\treplica, ok := rc.opts.Balancer.Pick(opts)\n\t\tif !ok {\n\t\t\trc.mu.Unlock()\n\t\t\tcontinue\n\t\t}\n\n\t\tc, ok := replica.(*clientConnection)\n\t\tif !ok {\n\t\t\trc.mu.Unlock()\n\t\t\treturn nil, nil, fmt.Errorf(\"internal error: wrong connection type %#v returned by load balancer\", replica)\n\t\t}\n\n\t\tc.lastID++\n\t\trpc.id = c.lastID\n\t\tc.calls[rpc.id] = rpc\n\t\tc.callstart()\n\t\tnc := c.c\n\t\trc.mu.Unlock()\n\n\t\treturn c, nc, nil\n\t}\n\n\treturn nil, nil, ctx.Err()\n}\n\nfunc (c *clientConnection) Address() string {\n\treturn c.endpoint.Address()\n}\n\n// State transition actions: all of these are called with rc.mu held.\n\nfunc (c *clientConnection) register() {\n\tswitch c.state {\n\tcase missing:\n\t\tc.setState(disconnected)\n\tcase draining:\n\t\t// We were attempting to get rid of the old connection, but it\n\t\t// seems like the server-side problem was transient, so we\n\t\t// resurrect the draining connection into a non-draining state.\n\t\t//\n\t\t// New state is active instead of idle since state==draining\n\t\t// implies there is at least one call in-flight.\n\t\tc.setState(active)\n\t}\n}\n\nfunc (c *clientConnection) unregister() {\n\tswitch c.state {\n\tcase disconnected, checking, idle:\n\t\tc.setState(missing)\n\tcase active:\n\t\tc.setState(draining)\n\t}\n}\n\nfunc (c *clientConnection) connected() {\n\tswitch c.state {\n\tcase disconnected:\n\t\tc.setState(checking)\n\t}\n}\n\nfunc (c *clientConnection) checked() {\n\tswitch c.state {\n\tcase checking:\n\t\tc.setState(idle)\n\t}\n}\n\nfunc (c *clientConnection) callstart() {\n\tswitch c.state {\n\tcase idle:\n\t\tc.setState(active)\n\t}\n}\n\nfunc (c *clientConnection) lastdone() {\n\tswitch c.state {\n\tcase active:\n\t\tc.setState(idle)\n\tcase draining:\n\t\tc.setState(missing)\n\t}\n}\n\nfunc (c *clientConnection) fail(details string, err error) {\n\tif !c.loggedShutdown {\n\t\tc.loggedShutdown = true\n\t\tlogError(c.logger, details, err)\n\t}\n\n\t// endCalls here so we can supply good errors.\n\tc.endCalls(fmt.Errorf(\"%w: %s: %s\", CommunicationError, details, err))\n\n\tswitch c.state {\n\tcase checking, idle, active:\n\t\tc.setState(disconnected)\n\tcase draining:\n\t\tc.setState(missing)\n\t}\n}\n\nfunc (c *clientConnection) close() {\n\t// endCalls here so we can supply good errors.\n\tc.endCalls(fmt.Errorf(\"%w: connection closed\", CommunicationError))\n\n\tc.setState(missing)\n}\n\n// checkInvariants verifies clientConnection invariants.\nfunc (c *clientConnection) checkInvariants() {\n\ts := c.state\n\n\t// connection in reconnectingConnection.conns iff state not in {missing}\n\tif _, ok := c.rc.conns[c.endpoint.Address()]; ok != (s != missing) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong connection table presence %v\", s, ok))\n\t}\n\n\t// has net.Conn iff state in {checking, idle, active, draining}\n\tif (c.c != nil) != (s == checking || s == idle || s == active || s == draining) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong net.Conn %v\", s, c.c))\n\t}\n\n\t// connection is in the balancer iff state in {idle, active}\n\tif c.inBalancer != (s == idle || s == active) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong balancer presence %v\", s, c.inBalancer))\n\t}\n\n\t// len(calls) > 0 iff state in {active, draining}\n\tif (len(c.calls) != 0) != (s == active || s == draining) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong number of calls %d\", s, len(c.calls)))\n\t}\n}\n\n// setState transitions to state s and updates any related state.\nfunc (c *clientConnection) setState(s connState) {\n\t// idle<-> active transitions may happen a lot, so short-circuit them\n\t// by avoiding logging and full invariant maintenance.\n\tif c.state == active && s == idle {\n\t\tc.state = idle\n\t\tif len(c.calls) != 0 {\n\t\t\tpanic(fmt.Sprintf(\"%v connection: wrong number of calls %d\", s, len(c.calls)))\n\t\t}\n\t\treturn\n\t} else if c.state == idle && s == active {\n\t\tc.state = active\n\t\tif len(c.calls) == 0 {\n\t\t\tpanic(fmt.Sprintf(\"%v connection: wrong number of calls %d\", s, len(c.calls)))\n\t\t}\n\t\treturn\n\t}\n\n\tc.logger.Info(\"connection\", \"addr\", c.endpoint.Address(), \"from\", c.state, \"to\", s)\n\tc.state = s\n\n\t// Fix membership in rc.conns.\n\tif s == missing {\n\t\tdelete(c.rc.conns, c.endpoint.Address())\n\t\tif c.canceler != nil {\n\t\t\tc.canceler() // Forces retry loop to end early\n\t\t\tc.canceler = nil\n\t\t}\n\t} // else: caller is responsible for adding c to rc.conns\n\n\t// Fix net.Conn presence.\n\tif s == missing || s == disconnected {\n\t\tif c.c != nil {\n\t\t\tc.c.Close()\n\t\t\tc.c = nil\n\t\t\tc.cbuf = nil\n\t\t}\n\t} // else: caller is responsible for setting c.c and c.cbuf\n\n\t// Fix balancer membership.\n\tif s == idle || s == active {\n\t\tif !c.inBalancer {\n\t\t\tc.rc.opts.Balancer.Add(c)\n\t\t\tc.inBalancer = true\n\t\t}\n\t} else {\n\t\tif c.inBalancer {\n\t\t\tc.rc.opts.Balancer.Remove(c)\n\t\t\tc.inBalancer = false\n\t\t}\n\t}\n\n\t// Fix in-flight calls.\n\tif s == active || s == draining {\n\t\t// Keep calls live\n\t} else {\n\t\tc.endCalls(fmt.Errorf(\"%w:\n```\n# Function Signature: func Serve(ctx context.Context, l Listener, opts ServerOptions) error {\n# Natural Language Annotations: // Serve starts listening for connections and requests on l. It always returns a\n// non-nil error and closes l.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc Serve(ctx context.Context, l Listener, opts ServerOptions) error {\n    ss := &serverState{opts: opts.withDefaults()}\n    ss.serveConnection(ctx, l, hmap)\n    return nil\n}\n```", "  ```\nfunc Serve(ctx context.Context, l Listener, opts ServerOptions) error {\n\tss := &serverState{opts: opts.withDefaults()}\n\tgo func(ctx context.Context, ss *serverState) {\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\tdefault:\n\t\t\t\tconn, _, err := l.Accept()\n\t\t\t\tif err != nil {\n\t\t\t\t\tlogError(opts.Logger, \"accept\", err)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tss.serveConnection(ctx, conn, nil)\n\t\t\t}\n\t\t}\n\t}(ctx, ss)\n\n\t<-ctx.Done()\n\tss.stop()\n\treturn ctx.Err()\n}\n```", "  ```\nfunc Serve(ctx context.Context, l Listener, opts ServerOptions) error {\n\tss := &serverState{opts: opts.withDefaults()}\n\tfor {\n\t\tconn, hmap, err := l.Accept()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\ts := &serverConnection{\n\t\t\topts:  ss.opts,\n\t\t\tc:     conn,\n\t\t\tcbuf:  bufio.NewReader(conn),\n\t\t\tlogger: ss.opts.Logger,\n\t\t\tcancelFuncs: map[uint64]func(){},\n\t\t}\n\t\tss.register(s)\n\t\ts.runHandler(ctx, hmap, func() { ss.unregister(s) })\n\t}\n}\n```"]}, "weaver-internal/net/call/call.go-Connect": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/net/call/call.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package call implements an RPC mechanism.\npackage call\n\n// # Overview\n//\n// RPCs are conveyed across a bidirectional connection. A connection carries\n// a sequence of messages in each direction. A message has the following\n// information:\n//\trequest-id\t-- A number that identifies a particular RPC\n//\tmessage-type\t-- E.g., request or response\n//\tlength\t\t-- How many payload bytes follow\n//\tpayload\t\t-- length bytes of payload\n// The payload format varies depending on the message-type.\n// See msg.go for details.\n//\n// # Server operation\n//\n// The server listens for connections (typically on a TCP socket). For\n// each accepted connection, it starts a readRequests() goroutine that\n// reads messages from that connection. When readRequests() gets a\n// request message, it starts a runHandler() goroutine. runHandler()\n// looks up the registered handler for the message, runs it, and sends\n// the response back over the connection.\n//\n// # Client operation\n//\n// For each newly discovered server, the client starts a manage() goroutine\n// that connects to server, and then reads messages from the connection. If the\n// network connection breaks, manage() reconnects (after a retry delay).\n//\n// When the client wants to send an RPC, it selects one of its server\n// connections to use, creates a call object, assigns it a new request-id, and\n// registers the object in a map in the connection. It then sends a request\n// message over the connection and waits for the call object to be marked as\n// done.\n//\n// When the response arrives, it is picked up by readAndProcessMessage().\n// readAndProcessMessage() finds the call object corresponding to the\n// request-id in the response, and marks the call object as done which\n// wakes up goroutine that initiated the RPC.\n//\n// If a client is constructed with a non-constant resolver, the client also\n// spawns a watchResolver goroutine that repeatedly calls Resolve on the\n// resolver to get notified of updates to the set of endpoints. When the\n// endpoints are updated, existing connections are retained, and stale\n// connections are transitioned to a \"draining\" state.\n//\n// New RPCs are never issued over draining connections, but the pending\n// requests on a draining connection are allowed to finish. As soon as a\n// draining connection has no active calls, the connection closes itself. If\n// the resolver later returns a new set of endpoints that includes a draining\n// connection that hasn't closed itself, the draining connection is turned\n// back into a normal connection.\nimport (\n\t\"bufio\"\n\t\"context\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/logging\"\n\t\"github.com/ServiceWeaver/weaver/runtime/retry\"\n\t\"go.opentelemetry.io/otel/codes\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// Connection allows a client to send RPCs.\ntype Connection interface {\n\t// Call makes an RPC over a Connection.\n\tCall(context.Context, MethodKey, []byte, CallOptions) ([]byte, error)\n\n\t// Close closes a connection. Pending invocations of Call are cancelled and\n\t// return an error. All future invocations of Call fail and return an error\n\t// immediately. Close can be called more than once.\n\tClose()\n}\n\n// Listener allows the server to accept RPCs.\ntype Listener interface {\n\tAccept() (net.Conn, *HandlerMap, error)\n\tClose() error\n\tAddr() net.Addr\n}\n\n// reconnectingConnection is the concrete client-side Connection implementation.\n// It automatically reconnects to the servers on first call or the first call\n// after a shutdown.\ntype reconnectingConnection struct {\n\topts ClientOptions\n\n\t// mu guards the following fields and some of the fields in the\n\t// clientConnections inside connections and draining.\n\tmu     sync.Mutex\n\tconns  map[string]*clientConnection\n\tclosed bool\n\n\tresolver       Resolver\n\tcancelResolver func()         // cancels the watchResolver goroutine\n\tresolverDone   sync.WaitGroup // used to wait for watchResolver to finish\n}\n\n// connState is the state of a clientConnection (connection to a particular\n// server replica). missing is a special state used for unknown servers. A\n// typical sequence of transitions is:\n//\n//\tmissing -> disconnected -> checking -> idle <-> active -> draining -> missing\n//\n// The events that can cause state transition are:\n//\n// - register: server has shown up in resolver results\n// - unregister: server has dropped from resolver results\n// - connected: a connection has been successfully made\n// - checked: connection has been successfully checked\n// - callstart: call starts on connection\n// - lastdone: last active call on connection has ended\n// - fail: some protocol error is detected on the connection\n// - close: reconnectingConnection is being closed\n//\n// Each event has a corresponding clientConnection method below. See\n// those methods for the corresponding state transitions.\ntype connState int8\n\nconst (\n\tmissing      connState = iota\n\tdisconnected           // cannot be used for calls\n\tchecking               // checking new network connection\n\tidle                   // can be used for calls, no calls in-flight\n\tactive                 // can be used for calls, some calls in-flight\n\tdraining               // some calls in-flight, no new calls should be added\n)\n\nvar connStateNames = []string{\n\t\"missing\",\n\t\"disconnected\",\n\t\"checking\",\n\t\"idle\",\n\t\"active\",\n\t\"draining\",\n}\n\nfunc (s connState) String() string { return connStateNames[s] }\n\n// clientConnection manages one network connection on the client-side.\ntype clientConnection struct {\n\t// Immutable after construction.\n\trc       *reconnectingConnection // owner\n\tcanceler func()                  // Used to cancel goroutine handling connection\n\tlogger   *slog.Logger\n\tendpoint Endpoint\n\n\twlock sync.Mutex // Guards writes to c\n\n\t// Guarded by rc.mu\n\tstate          connState        // current connection state\n\tloggedShutdown bool             // Have we logged a shutdown error?\n\tinBalancer     bool             // Is c registered with the balancer?\n\tc              net.Conn         // Active network connection, or nil\n\tcbuf           *bufio.Reader    // Buffered reader wrapped around c\n\tversion        version          // Version number to use for connection\n\tcalls          map[uint64]*call // In-progress calls\n\tlastID         uint64           // Last assigned request ID for a call\n}\n\nvar _ ReplicaConnection = &clientConnection{}\n\n// call holds the state for an active call at the client.\ntype call struct {\n\tid         uint64\n\tdoneSignal chan struct{}\n\n\t// Fields below are accessed across goroutines, but their access is\n\t// synchronized via doneSignal, i.e., it is never concurrent.\n\terr      error\n\tresponse []byte\n\n\t// Is the call done?\n\t// This field is accessed across goroutines using atomics.\n\tdone uint32 // is the call done?\n\n}\n\n// serverConnection manages one network connection on the server-side.\ntype serverConnection struct {\n\topts        ServerOptions\n\tc           net.Conn\n\tcbuf        *bufio.Reader // Buffered reader wrapped around c\n\twlock       sync.Mutex    // Guards writes to c\n\tmu          sync.Mutex\n\tclosed      bool              // has c been closed?\n\tversion     version           // Version number to use for connection\n\tcancelFuncs map[uint64]func() // Cancellation functions for in-progress calls\n}\n\n// serverState tracks all live server-side connections so we can clean things up when canceled.\ntype serverState struct {\n\topts  ServerOptions\n\tmu    sync.Mutex\n\tconns map[*serverConnection]struct{} // Live connections\n}\n\n// Serve starts listening for connections and requests on l. It always returns a\n// non-nil error and closes l.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// onceCloseListener wraps a Listener, protecting it from multiple Close calls.\ntype onceCloseListener struct {\n\tListener\n\tcloser func() error // Must be result of sync.OnceValue\n}\n\nfunc (oc *onceCloseListener) Close() error {\n\treturn oc.closer()\n}\n\n// ServeOn serves client requests received over an already established\n// network connection with a client. This can be useful in tests or\n// when using custom networking transports.\nfunc ServeOn(ctx context.Context, conn net.Conn, hmap *HandlerMap, opts ServerOptions) {\n\tss := &serverState{opts: opts.withDefaults()}\n\tss.serveConnection(ctx, conn, hmap)\n}\n\nfunc (ss *serverState) serveConnection(ctx context.Context, conn net.Conn, hmap *HandlerMap) {\n\tc := &serverConnection{\n\t\topts:        ss.opts,\n\t\tc:           conn,\n\t\tcbuf:        bufio.NewReader(conn),\n\t\tversion:     initialVersion, // Updated when we hear from client\n\t\tcancelFuncs: map[uint64]func(){},\n\t}\n\tss.register(c)\n\n\tgo c.readRequests(ctx, hmap, func() { ss.unregister(c) })\n}\n\nfunc (ss *serverState) stop() {\n\tss.mu.Lock()\n\tdefer ss.mu.Unlock()\n\tfor c := range ss.conns {\n\t\tc.c.Close() // Should stop readRequests in its tracks\n\t}\n}\n\nfunc (ss *serverState) register(c *serverConnection) {\n\tss.mu.Lock()\n\tdefer ss.mu.Unlock()\n\tif ss.conns == nil {\n\t\tss.conns = map[*serverConnection]struct{}{}\n\t}\n\tss.conns[c] = struct{}{}\n}\n\nfunc (ss *serverState) unregister(c *serverConnection) {\n\tss.mu.Lock()\n\tdefer ss.mu.Unlock()\n\tdelete(ss.conns, c)\n}\n\n// Connect creates a connection to the servers at the endpoints returned by the\n// resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Close closes a connection.\nfunc (rc *reconnectingConnection) Close() {\n\tcloseWithLock := func() {\n\t\trc.mu.Lock()\n\t\tdefer rc.mu.Unlock()\n\t\tif rc.closed {\n\t\t\treturn\n\t\t}\n\t\trc.closed = true\n\t\tfor _, c := range rc.conns {\n\t\t\tc.close()\n\t\t}\n\t}\n\tcloseWithLock()\n\n\t// Cancel the watchResolver goroutine and wait for it to terminate. If the\n\t// watchResolver has already been terminated, then this code is a no-op.\n\t// Note that if we hold the lock while waiting for watchResolver to\n\t// terminate, we may deadlock.\n\trc.cancelResolver()\n\trc.resolverDone.Wait()\n}\n\n// Call makes an RPC over connection c, retrying it on network errors if retries are allowed.\nfunc (rc *reconnectingConnection) Call(ctx context.Context, h MethodKey, arg []byte, opts CallOptions) ([]byte, error) {\n\tif !opts.Retry {\n\t\treturn rc.callOnce(ctx, h, arg, opts)\n\t}\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\tresponse, err := rc.callOnce(ctx, h, arg, opts)\n\t\tif errors.Is(err, Unreachable) || errors.Is(err, CommunicationError) {\n\t\t\tcontinue\n\t\t}\n\t\treturn response, err\n\t}\n\treturn nil, ctx.Err()\n}\n\nfunc (rc *reconnectingConnection) callOnce(ctx context.Context, h MethodKey, arg []byte, opts CallOptions) ([]byte, error) {\n\tvar micros int64\n\tdeadline, haveDeadline := ctx.Deadline()\n\tif haveDeadline {\n\t\t// Send the deadline in the header. We use the relative time instead\n\t\t// of absolute in case there is significant clock skew. This does mean\n\t\t// that we will not count transmission delay against the deadline.\n\t\tmicros = time.Until(deadline).Microseconds()\n\t\tif micros <= 0 {\n\t\t\t// Fail immediately without attempting to send a zero or negative\n\t\t\t// deadline to the server which will be misinterpreted.\n\t\t\t<-ctx.Done()\n\t\t\treturn nil, ctx.Err()\n\t\t}\n\t}\n\n\t// Encode the header.\n\thdr := encodeHeader(ctx, h, micros)\n\n\t// Note that we send the header and the payload as follows:\n\t// [header_length][encoded_header][payload]\n\tvar hdrLen [hdrLenLen]byte\n\tbinary.LittleEndian.PutUint32(hdrLen[:], uint32(len(hdr)))\n\thdrSlice := append(hdrLen[:], hdr...)\n\n\trpc := &call{}\n\trpc.doneSignal = make(chan struct{})\n\n\t// TODO: Arrange to obey deadline in any reconnection done inside startCall.\n\tconn, nc, err := rc.startCall(ctx, rpc, opts)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := writeMessage(nc, &conn.wlock, requestMessage, rpc.id, hdrSlice, arg, rc.opts.WriteFlattenLimit); err != nil {\n\t\tconn.shutdown(\"client send request\", err)\n\t\tconn.endCall(rpc)\n\t\treturn nil, fmt.Errorf(\"%w: %s\", CommunicationError, err)\n\t}\n\n\tif rc.opts.OptimisticSpinDuration > 0 {\n\t\t// Optimistically spin, waiting for the results.\n\t\tfor start := time.Now(); time.Since(start) < rc.opts.OptimisticSpinDuration; {\n\t\t\tif atomic.LoadUint32(&rpc.done) > 0 {\n\t\t\t\treturn rpc.response, rpc.err\n\t\t\t}\n\t\t}\n\t}\n\n\tif cdone := ctx.Done(); cdone != nil {\n\t\tselect {\n\t\tcase <-rpc.doneSignal:\n\t\t\t// Regular return\n\t\tcase <-cdone:\n\t\t\t// Canceled or deadline expired.\n\t\t\tconn.endCall(rpc)\n\n\t\t\tif !haveDeadline || time.Now().Before(deadline) {\n\t\t\t\t// Early cancellation. Tell server about it.\n\t\t\t\tif err := writeMessage(nc, &conn.wlock, cancelMessage, rpc.id, nil, nil, rc.opts.WriteFlattenLimit); err != nil {\n\t\t\t\t\tconn.shutdown(\"client send cancel\", err)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn nil, ctx.Err()\n\t\t}\n\t} else {\n\t\t<-rpc.doneSignal\n\t}\n\treturn rpc.response, rpc.err\n}\n\n// watchResolver watches for updates to the set of endpoints. When a new set of\n// updates is available, watchResolver passes it to updateEndpoints.\n// REQUIRES: version != nil.\n// REQUIRES: rc.mu is not held.\nfunc (rc *reconnectingConnection) watchResolver(ctx context.Context, version *Version) {\n\tdefer rc.resolverDone.Done()\n\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\tendpoints, newVersion, err := rc.resolver.Resolve(ctx, version)\n\t\tif err != nil {\n\t\t\tlogError(rc.opts.Logger, \"watchResolver\", err)\n\t\t\tcontinue\n\t\t}\n\t\tif newVersion == nil {\n\t\t\tlogError(rc.opts.Logger, \"watchResolver\", errors.New(\"non-constant resolver returned a nil version\"))\n\t\t\tcontinue\n\t\t}\n\t\tif *version == *newVersion {\n\t\t\t// Resolver wishes to be called again after an appropriate delay.\n\t\t\tcontinue\n\t\t}\n\t\tif err := rc.updateEndpoints(ctx, endpoints); err != nil {\n\t\t\tlogError(rc.opts.Logger, \"watchResolver\", err)\n\t\t}\n\t\tversion = newVersion\n\t\tr.Reset()\n\t}\n}\n\n// updateEndpoints updates the set of endpoints. Existing connections are\n// retained, and stale connections are closed.\n// REQUIRES: rc.mu is not held.\nfunc (rc *reconnectingConnection) updateEndpoints(ctx context.Context, endpoints []Endpoint) error {\n\trc.mu.Lock()\n\tdefer rc.mu.Unlock()\n\n\tif rc.closed {\n\t\treturn fmt.Errorf(\"updateEndpoints on closed Connection\")\n\t}\n\n\t// Make new endpoints.\n\tkeep := make(map[string]struct{}, len(endpoints))\n\tfor _, endpoint := range endpoints {\n\t\taddr := endpoint.Address()\n\t\tkeep[addr] = struct{}{}\n\t\tif _, ok := rc.conns[addr]; !ok {\n\t\t\t// New endpoint, create connection and manage it.\n\t\t\tctx, cancel := context.WithCancel(ctx)\n\t\t\tc := &clientConnection{\n\t\t\t\trc:       rc,\n\t\t\t\tcanceler: cancel,\n\t\t\t\tlogger:   rc.opts.Logger,\n\t\t\t\tendpoint: endpoint,\n\t\t\t\tcalls:    map[uint64]*call{},\n\t\t\t\tlastID:   0,\n\t\t\t}\n\t\t\trc.conns[addr] = c\n\t\t\tc.register()\n\t\t\tgo c.manage(ctx)\n\t\t}\n\t}\n\n\t// Drop old endpoints.\n\tfor addr, c := range rc.conns {\n\t\tif _, ok := keep[addr]; ok {\n\t\t\t// Still live, so keep it.\n\t\t\tcontinue\n\t\t}\n\t\tc.unregister()\n\t}\n\n\treturn nil\n}\n\n// startCall registers a new in-progress call.\n// REQUIRES: rc.mu is not held.\nfunc (rc *reconnectingConnection) startCall(ctx context.Context, rpc *call, opts CallOptions) (*clientConnection, net.Conn, error) {\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\trc.mu.Lock()\n\t\tif rc.closed {\n\t\t\trc.mu.Unlock()\n\t\t\treturn nil, nil, fmt.Errorf(\"Call on closed Connection\")\n\t\t}\n\n\t\treplica, ok := rc.opts.Balancer.Pick(opts)\n\t\tif !ok {\n\t\t\trc.mu.Unlock()\n\t\t\tcontinue\n\t\t}\n\n\t\tc, ok := replica.(*clientConnection)\n\t\tif !ok {\n\t\t\trc.mu.Unlock()\n\t\t\treturn nil, nil, fmt.Errorf(\"internal error: wrong connection type %#v returned by load balancer\", replica)\n\t\t}\n\n\t\tc.lastID++\n\t\trpc.id = c.lastID\n\t\tc.calls[rpc.id] = rpc\n\t\tc.callstart()\n\t\tnc := c.c\n\t\trc.mu.Unlock()\n\n\t\treturn c, nc, nil\n\t}\n\n\treturn nil, nil, ctx.Err()\n}\n\nfunc (c *clientConnection) Address() string {\n\treturn c.endpoint.Address()\n}\n\n// State transition actions: all of these are called with rc.mu held.\n\nfunc (c *clientConnection) register() {\n\tswitch c.state {\n\tcase missing:\n\t\tc.setState(disconnected)\n\tcase draining:\n\t\t// We were attempting to get rid of the old connection, but it\n\t\t// seems like the server-side problem was transient, so we\n\t\t// resurrect the draining connection into a non-draining state.\n\t\t//\n\t\t// New state is active instead of idle since state==draining\n\t\t// implies there is at least one call in-flight.\n\t\tc.setState(active)\n\t}\n}\n\nfunc (c *clientConnection) unregister() {\n\tswitch c.state {\n\tcase disconnected, checking, idle:\n\t\tc.setState(missing)\n\tcase active:\n\t\tc.setState(draining)\n\t}\n}\n\nfunc (c *clientConnection) connected() {\n\tswitch c.state {\n\tcase disconnected:\n\t\tc.setState(checking)\n\t}\n}\n\nfunc (c *clientConnection) checked() {\n\tswitch c.state {\n\tcase checking:\n\t\tc.setState(idle)\n\t}\n}\n\nfunc (c *clientConnection) callstart() {\n\tswitch c.state {\n\tcase idle:\n\t\tc.setState(active)\n\t}\n}\n\nfunc (c *clientConnection) lastdone() {\n\tswitch c.state {\n\tcase active:\n\t\tc.setState(idle)\n\tcase draining:\n\t\tc.setState(missing)\n\t}\n}\n\nfunc (c *clientConnection) fail(details string, err error) {\n\tif !c.loggedShutdown {\n\t\tc.loggedShutdown = true\n\t\tlogError(c.logger, details, err)\n\t}\n\n\t// endCalls here so we can supply good errors.\n\tc.endCalls(fmt.Errorf(\"%w: %s: %s\", CommunicationError, details, err))\n\n\tswitch c.state {\n\tcase checking, idle, active:\n\t\tc.setState(disconnected)\n\tcase draining:\n\t\tc.setState(missing)\n\t}\n}\n\nfunc (c *clientConnection) close() {\n\t// endCalls here so we can supply good errors.\n\tc.endCalls(fmt.Errorf(\"%w: connection closed\", CommunicationError))\n\n\tc.setState(missing)\n}\n\n// checkInvariants verifies clientConnection invariants.\nfunc (c *clientConnection) checkInvariants() {\n\ts := c.state\n\n\t// connection in reconnectingConnection.conns iff state not in {missing}\n\tif _, ok := c.rc.conns[c.endpoint.Address()]; ok != (s != missing) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong connection table presence %v\", s, ok))\n\t}\n\n\t// has net.Conn iff state in {checking, idle, active, draining}\n\tif (c.c != nil) != (s == checking || s == idle || s == active || s == draining) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong net.Conn %v\", s, c.c))\n\t}\n\n\t// connection is in the balancer iff state in {idle, active}\n\tif c.inBalancer != (s == idle || s == active) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong balancer presence %v\", s, c.inBalancer))\n\t}\n\n\t// len(calls) > 0 iff state in {active, draining}\n\tif (len(c.calls) != 0) != (s == active || s == draining) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong number of calls %d\", s, len(c.calls)))\n\t}\n}\n\n// setState transitions to state s and updates any related state.\nfunc (c *clientConnection) setState(s connState) {\n\t// idle<-> active transitions may happen a lot, so short-circuit them\n\t// by avoiding logging and full invariant maintenance.\n\tif c.state == active && s == idle {\n\t\tc.state = idle\n\t\tif len(c.calls) != 0 {\n\t\t\tpanic(fmt.Sprintf(\"%v connection: wrong number of calls %d\", s, len(c.calls)))\n\t\t}\n\t\treturn\n\t} else if c.state == idle && s == active {\n\t\tc.state = active\n\t\tif len(c.calls) == 0 {\n\t\t\tpanic(fmt.Sprintf(\"%v connection: wrong number of calls %d\", s, len(c.calls)))\n\t\t}\n\t\treturn\n\t}\n\n\tc.logger.Info(\"connection\", \"addr\", c.endpoint.Address(), \"from\", c.state, \"to\", s)\n\tc.state = s\n\n\t// Fix membership in rc.conns.\n\tif s == missing {\n\t\tdelete(c.rc.conns, c.endpoint.Address())\n\t\tif c.canceler != nil {\n\t\t\tc.canceler() // Forces retry loop to end early\n\t\t\tc.canceler = nil\n\t\t}\n\t} // else: caller is responsible for adding c to rc.conns\n\n\t// Fix net.Conn presence.\n\tif s == missing || s == disconnected {\n\t\tif c.c != nil {\n\t\t\tc.c.Close()\n\t\t\tc.c = nil\n\t\t\tc.cbuf = nil\n\t\t}\n\t} // else: caller is responsible for setting c.c and c.cbuf\n\n\t// Fix balancer membership.\n\tif s == idle || s == active {\n\t\tif !c.inBalancer {\n\t\t\tc.rc.opts.Balancer.Add(c)\n\t\t\tc.inBalancer = true\n\t\t}\n\t} else {\n\t\tif c.inBalancer {\n\t\t\tc.rc.opts.Balancer.Remove(c)\n\t\t\tc.inBalancer = false\n\t\t}\n\t}\n\n\t// Fix in-flight calls.\n\tif s == active || s == draining {\n\t\t// Keep calls live\n\t} else {\n\t\tc.endCalls(fmt.Errorf(\"%w:\n```\n# Function Signature: func Connect(ctx context.Context, resolver Resolver, opts ClientOptions) (Connection, error) {\n# Natural Language Annotations: // Connect creates a connection to the servers at the endpoints returned by the\n// resolver.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc Connect(ctx context.Context, resolver Resolver, opts ClientOptions) (Connection, error) {\n\trc := &reconnectingConnection{\n\t\topts:         opts.withDefaults(),\n\t\tresolver:     resolver,\n\t\tcancelResolver: func() {}, // Used to cancel goroutine that watches resolver\n\t\tendpoint:         nil,\n\t}\n\n\tctx, rc.cancelResolver = context.WithCancel(ctx)\n\tgo rc.watchResolver(ctx, &initialVersion)\n\n\treturn rc, nil\n}\n```", "  ```\nfunc Connect(ctx context.Context, resolver Resolver, opts ClientOptions) (Connection, error) {\n\t// Create a reconnectingConnection.\n\trc := &reconnectingConnection{opts: opts.withDefaults(), resolver: resolver}\n\trc.mu.Lock()\n\tdefer rc.mu.Unlock()\n\n\t// Start watchResolver.\n\tctx, rc.cancelResolver = context.WithCancel(ctx)\n\trc.resolverDone.Add(1)\n\tgo rc.watchResolver(ctx, &initialVersion)\n\n\t// Initial QueryEndpoint Events.\n\tendpoints, newVersion, err := resolver.Resolve(ctx, &initialVersion)\n\tif err != nil {\n\t\trc.mu.Unlock()\n\t\treturn nil, fmt.Errorf(\"failed to resolve endpoint names: %w\", err)\n\t}\n\tif newVersion == nil {\n\t\treturn nil, fmt.Errorf(\"non-constant resolver returned a nil version\")\n\t}\n\tinitialVersion = *newVersion\n\trc.mu.Unlock()\n\n\t// Update the endpoints.\n\tif err := rc.updateEndpoints(ctx, endpoints); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn rc, nil\n}\n```", "  ```\nfunc Connect(ctx context.Context, resolver Resolver, opts ClientOptions) (Connection, error) {\n\trc := &reconnectingConnection{\n\t\topts: opts,\n\t}\n\n\t// Start watching for updates to the set of endpoints.\n\t// REQUIRES: version != nil.\n\trc.mu.Lock()\n\trc.opts.Balancer.Reset()\n\trc.cpupsed = true\n\trc.version = version\n\trc.rc.mu.Unlock()\n\n\t// Wait for the first endpoint update.\n\t// REQUIRES: rc.mu is not held.\n\tendpoints, version, err := r.resolver.Resolve(ctx, version)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"%w: %s\", CommunicationError, err)\n\t}\n\tif version == nil {\n\t\treturn nil, fmt.Errorf(\"%w: non-constant resolver returned a nil version\", CommunicationError)\n\t}\n\n\t// Make new endpoints.\n\tkeep := make(map[string]struct{}, len(endpoints))\n\tfor _, endpoint := range endpoints {\n\t\taddr := endpoint.Address()\n\t\tkeep[addr] = struct{}{}\n\t\tif _, ok := rc.conns[addr]; !ok {\n\t\t\t// New endpoint, create connection and manage it.\n\t\t\tctx, cancel := context.WithCancel(ctx)\n\t\t\tc := &clientConnection{\n\t\t\t\trc:       rc,\n\t\t\t\tcanceler: cancel,\n\t\t\t\tlogger:   rc.opts.Logger,\n\t\t\t\tendpoint: endpoint,\n\t\t\t\tcalls:    map[uint64]*call{},\n\t\t\t\tlastID:   0,\n\t\t\t}\n\t\t\trc.conns[addr] = c\n\t\t\tc.register()\n\t\t\tgo c.manage(ctx)\n\t\t}\n\t}\n\n\t// Drop old endpoints.\n\tfor addr, c := range rc.conns {\n\t\tif _, ok := keep[addr]; ok {\n\t\t\t// Still live, so keep it.\n\t\t\tcontinue\n\t\t}\n\t\tc.unregister()\n\t}\n\n\treturn rc, nil\n}\n```"]}, "weaver-internal/net/call/msg.go-writeMessage": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/net/call/msg.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage call\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"sync\"\n)\n\n// messageType identifies a type of message sent across the wire.\ntype messageType uint8\n\nconst (\n\tversionMessage messageType = iota\n\trequestMessage\n\tresponseMessage\n\tresponseError\n\tcancelMessage\n\t// Other types to add?\n\t// - chunked request/response messages?\n\t// - health check\n\t// - server status info\n)\n\n// version holds the protocol version number.\ntype version uint32\n\nconst (\n\tinitialVersion version = iota\n)\n\nconst currentVersion = initialVersion\n\nconst hdrLenLen = uint32(4) // size of the header length included in each message\n\n// # Message formats\n//\n// All messages have the following format:\n//    id        [8]byte       -- identifier used to track the message\n//    type      [1]byte       -- messageType\n//    length    [7]byte       -- length of the remainder of the message\n//    payload   [length]byte  -- message-type-specific data\n//\n// The format of payload depends on the message type.\n//\n// versionMessage: this is the first message sent on a connection by both sides.\n//    version  [4]byte\n//\n// requestMessage:\n//    headerLen         [4]byte         -- length of the encoded header\n//    header            [headerLen]byte -- encoded header information\n//    payload                           -- call argument serialization\n//\n// The header is encoded using Service Weaver's encoding format for a type that\n// looks like:\n//\n// struct header {\n//   MethodKey       [16]byte\n//   Deadline        int64\n//   TraceContext    [25]byte\n//   MetadataContext map[string]string\n// }\n//\n// responseMessage:\n//    payload holds call result serialization\n//\n// responseError:\n//    payload holds error serialization\n//\n// cancelMessage:\n//    payload is empty\n\n// writeMessage formats and sends a message over w.\n//\n// The message payload is formed by concatenating extraHdr and payload.\n// (Allowing two arguments to form the payload avoids unnecessary allocation\n// and copying when we want to prepend some data to application supplied data).\n//\n// The write is guarded by wlock, which must not be locked when passed in.\n\n\n\n\n\n\n\n\n\n// writeChunked writes the header, extra header, and the payload into w using\n// three different w.Write() calls.\nfunc writeChunked(w io.Writer, wlock *sync.Mutex, mt messageType, id uint64, extraHdr []byte, payload []byte) error {\n\t// We use an iovec with up to three entries.\n\tvar vec [3][]byte\n\n\tnh, np := len(extraHdr), len(payload)\n\tvar hdr [16]byte\n\tbinary.LittleEndian.PutUint64(hdr[0:], id)\n\tbinary.LittleEndian.PutUint64(hdr[8:], uint64(mt)|(uint64(nh+np)<<8))\n\n\tvec[0] = hdr[:]\n\tvec[1] = extraHdr\n\tvec[2] = payload\n\tbuf := net.Buffers(vec[:])\n\n\t// buf.WriteTo is not guaranteed to write the entire contents of buf\n\t// atomically, so we guard the write with a lock to prevent writes from\n\t// interleaving.\n\twlock.Lock()\n\tdefer wlock.Unlock()\n\tn, err := buf.WriteTo(w)\n\tif err == nil && n != 16+int64(nh)+int64(np) {\n\t\terr = fmt.Errorf(\"partial write\")\n\t}\n\treturn err\n}\n\n// writeFlat concatenates the header, extra header, and the payload into\n// a single flat byte slice, and writes it into w using a single w.Write() call.\nfunc writeFlat(w io.Writer, wlock *sync.Mutex, mt messageType, id uint64, extraHdr []byte, payload []byte) error {\n\tnh, np := len(extraHdr), len(payload)\n\tdata := make([]byte, 16+nh+np)\n\tbinary.LittleEndian.PutUint64(data[0:], id)\n\tval := uint64(mt) | (uint64(nh+np) << 8)\n\tbinary.LittleEndian.PutUint64(data[8:], val)\n\tcopy(data[16:], extraHdr)\n\tcopy(data[16+nh:], payload)\n\n\t// Write while holding the lock, since we don't know if the underlying\n\t// io.Write is atomic.\n\t// TODO(mwhittaker): For those io.Writers that are atomic, we can avoid\n\t// locking in some cases.\n\twlock.Lock()\n\tdefer wlock.Unlock()\n\tn, err := w.Write(data)\n\tif err == nil && n != len(data) {\n\t\terr = fmt.Errorf(\"partial write\")\n\t}\n\treturn err\n}\n\n// readMessage reads, parses, and returns the next message from r.\nfunc readMessage(r io.Reader) (messageType, uint64, []byte, error) {\n\t// Read the header.\n\tconst headerSize = 16\n\tvar hdr [headerSize]byte\n\tif _, err := io.ReadFull(r, hdr[:]); err != nil {\n\t\treturn 0, 0, nil, err\n\t}\n\n\t// Extract header contents (see writeMessage for header format).\n\tid := binary.LittleEndian.Uint64(hdr[0:])\n\tw2 := binary.LittleEndian.Uint64(hdr[8:])\n\tmt := messageType(w2 & 0xff)\n\tdataLen := w2 >> 8\n\tconst maxSize = 100 << 20\n\tif dataLen > maxSize {\n\t\treturn 0, 0, nil, fmt.Errorf(\"overly large message length %d\", dataLen)\n\t}\n\n\t// Read the payload.\n\tmsg := make([]byte, int(dataLen))\n\tif _, err := io.ReadFull(r, msg); err != nil {\n\t\treturn 0, 0, nil, err\n\t}\n\treturn mt, id, msg, nil\n}\n\n// writeVersion sends my version number to the peer.\nfunc writeVersion(w io.Writer, wlock *sync.Mutex) error {\n\tvar msg [4]byte\n\tbinary.LittleEndian.PutUint32(msg[:], uint32(currentVersion))\n\treturn writeFlat(w, wlock, versionMessage, 0, nil, msg[:])\n}\n\n// getVersion extracts the version number sent by the peer and picks the\n// appropriate version number to use for communicating with the peer.\nfunc getVersion(id uint64, msg []byte) (version, error) {\n\tif id != 0 {\n\t\treturn 0, fmt.Errorf(\"invalid ID %d in handshake\", id)\n\t}\n\t// Allow messages longer than needed so that future updates can send more info.\n\tif len(msg) < 4 {\n\t\treturn 0, fmt.Errorf(\"bad version message length %d, must be >= 4\", len(msg))\n\t}\n\tv := binary.LittleEndian.Uint32(msg)\n\n\t// We use the minimum of the peer and my version numbers.\n\tif v < uint32(currentVersion) {\n\t\treturn version(v), nil\n\t}\n\treturn currentVersion, nil\n}\n\n```\n# Function Signature: func writeMessage(w io.Writer, wlock *sync.Mutex, mt messageType, id uint64, extraHdr []byte, payload []byte, flattenLimit int) error {\n# Natural Language Annotations: // # Message formats\n//\n// All messages have the following format:\n//    id        [8]byte       -- identifier used to track the message\n//    type      [1]byte       -- messageType\n//    length    [7]byte       -- length of the remainder of the message\n//    payload   [length]byte  -- message-type-specific data\n//\n// The format of payload depends on the message type.\n//\n// versionMessage: this is the first message sent on a connection by both sides.\n//    version  [4]byte\n//\n// requestMessage:\n//    headerLen         [4]byte         -- length of the encoded header\n//    header            [headerLen]byte -- encoded header information\n//    payload                           -- call argument serialization\n//\n// The header is encoded using Service Weaver's encoding format for a type that\n// looks like:\n//\n// struct header {\n//   MethodKey       [16]byte\n//   Deadline        int64\n//   TraceContext    [25]byte\n//   MetadataContext map[string]string\n// }\n//\n// responseMessage:\n//    payload holds call result serialization\n//\n// responseError:\n//    payload holds error serialization\n//\n// cancelMessage:\n//    payload is empty\n// writeMessage formats and sends a message over w.\n//\n// The message payload is formed by concatenating extraHdr and payload.\n// (Allowing two arguments to form the payload avoids unnecessary allocation\n// and copying when we want to prepend some data to application supplied data).\n//\n// The write is guarded by wlock, which must not be locked when passed in.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc writeMessage(w io.Writer, wlock *sync.Mutex, mt messageType, id uint64, extraHdr []byte, payload []byte, flattenLimit int) error {\n\t// We use an iovec with up to three entries.\n\tvar vec [3][]byte\n\n\tnh, np := len(extraHdr), len(payload)\n\tvar hdr [16]byte\n\tbinary.LittleEndian.PutUint64(hdr[0:], id)\n\tbinary.LittleEndian.PutUint64(hdr[8:], uint64(mt)|(uint64(nh+np)<<8))\n\n\tvec[0] = hdr[:]\n\tvec[1] = extraHdr\n\tvec[2] = payload\n\tbuf := net.Buffers(vec[:])\n\n\t// buf.WriteTo is not guaranteed to write the entire contents of buf\n\t// atomically, so we guard the write with a lock to prevent writes from\n\t// interleaving.\n\twlock.Lock()\n\tdefer wlock.Unlock()\n\tn, err := buf.WriteTo(w)\n\tif err == nil && n != 16+int64(nh)+int64(np) {\n\t\terr = fmt.Errorf(\"partial write\")\n\t}\n\treturn err\n}\n```", "  ```\nfunc writeMessage(w io.Writer, wlock *sync.Mutex, mt messageType, id uint64, extraHdr []byte, payload []byte, flattenLimit int) error {\n\tif flattenLimit <= 0 {\n\t\treturn writeChunked(w, wlock, mt, id, extraHdr, payload)\n\t}\n\tnh, np := len(extraHdr), len(payload)\n\tif int32(nh)+np < flattenLimit {\n\t\tflat := make([]byte, 16+nh+np)\n\t\tbinary.LittleEndian.PutUint64(flat[0:], id)\n\t\tval := uint64(mt) | (uint64(nh+np) << 8)\n\t\tbinary.LittleEndian.PutUint64(flat[8:], val)\n\t\tcopy(flat[16:], extraHdr)\n\t\tcopy(flat[16+nh:], payload)\n\t\treturn writeFlat(w, wlock, mt, id, flat[:], nil)\n\t}\n\treturn writeChunked(w, wlock, mt, id, extraHdr, payload)\n}\n```", "  ```\nfunc writeMessage(w io.Writer, wlock *sync.Mutex, mt messageType, id uint64, extraHdr []byte, payload []byte, flattenLimit int) error {\n\t// Check if the message is allowed to be written in a single word.\n\tvar single bool\n\tswitch mt {\n\tcase requestMessage, responseMessage, responseError, cancelMessage:\n\t\tsingle = true\n\tdefault:\n\t\tsingle = false\n\t}\n\n\t// Write the initial header containing the message type and length information.\n\tvar hdr [16]byte\n\tbinary.LittleEndian.PutUint64(hdr[0:], id)\n\tval := uint64(mt) | (uint64(len(extraHdr)+len(payload)) << 8)\n\tbinary.LittleEndian.PutUint64(hdr[8:], val)\n\n\t// Write the header and message type to the writer.\n\twlock.Lock()\n\tdefer wlock.Unlock()\n\tif _, err := w.Write(hdr[:]); err != nil {\n\t\treturn err\n\t}\n\n\t// If the message is allowed to be written in a single word,\n\t// write it using the writeFlat function.\n\tif single && len(extraHdr)+len(payload) < flattenLimit {\n\t\treturn writeFlat(w, wlock, mt, id, extraHdr, payload)\n\t}\n\n\t// Otherwise, write the header and message in chunks.\n\treturn writeChunked(w, wlock, mt, id, extraHdr, payload)\n}\n```"]}, "weaver-internal/net/call/trace.go-writeTraceContext": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/net/call/trace.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage call\n\nimport (\n\t\"context\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// writeTraceContext serializes the trace context (if any) contained in ctx\n// into enc.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// readTraceContext returns a span context with tracing information stored in dec.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func writeTraceContext(ctx context.Context, enc *codegen.Encoder) {\n# Natural Language Annotations: // writeTraceContext serializes the trace context (if any) contained in ctx\n// into enc.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\npackage call\n\nimport (\n\t\"context\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// writeTraceContext serializes the trace context (if any) contained in ctx\n// into enc.\nfunc writeTraceContext(ctx context.Context, enc *codegen.Encoder) {\n\tspan := trace.SpanFromContext(ctx)\n\tif span != nil {\n\t\ttraceContext, err := span.SpanContext()\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\n\t\t// Serialize trace context\n\t\ttraceFlags, err := traceContext.Flags()\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\n\t\tsystemTraceId, err := traceContext.SystemTraceID()\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\n\t\ttraceId, err := traceContext.TraceID()\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\n\t\tparentSpanId, err := traceContext.ParentSpanID()\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\n\t\tspanId, err := traceContext.SpanID()\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\n\t\t// Encoding\n\t\tenc.Uint8(traceFlags)\n\t\tenc.Uint64(systemTraceId)\n\t\tenc.Uint64(traceId)\n\t\tenc.Uint64(parentSpanId)\n\t\tenc.Uint64(spanId)\n\t}\n}\n```", "  ```\npackage call\n\nimport (\n\t\"context\"\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\nfunc writeTraceContext(ctx context.Context, enc *codegen.Encoder) {\n\tspan := trace.SpanFromContext(ctx)\n\tif span == nil {\n\t\treturn\n\t}\n\tvar buf bytes.Buffer\n\ttrace.NewTextWritter(&buf, 0 /* no indentation */)\n\ttrace.NewJSONWriter(&buf)\n\ttracer := trace.NewTracer(\n\t\ttrace.WithSpanName(span.Name()),\n\t)\n\tctxTracer = context.WithValue(ctx, \"tracer\", tracer)\n\ttracer.Write(ctx, span)\n\tenc.WriteString(buf.String())\n}\n```", "  ```\nfunc writeTraceContext(ctx context.Context, enc *codegen.Encoder) error {\n\t// Get the trace context from the context\n\ttraceContext, ok := trace.FromContext(ctx).(otel.SpanContext)\n\tif !ok {\n\t\t// If no trace context is present, return an error\n\t\treturn fmt.Errorf(\"no trace context found in context\")\n\t}\n\n\t// Marshal the trace context into the encoder\n\terr := enc.Marshal(traceContext)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error marshaling trace context: %w\", err)\n\t}\n\n\t// Success\n\treturn nil\n}\n```\n\nExplanation:\n\nThis function takes a context `ctx`, an encoder `enc`, and returns an error as return value. The function first gets the trace context from the context `ctx` using the `FromContext` function from the `trace` package. It then checks if the trace context is present in the context by using the `ok` variable (see https://golang.org/ref/spec#If_statements).\n\nIf the trace context is present, the function calls the `Marshal` method on the encoder `enc` with the trace context as argument. The encoder will then marshal the trace context into the encoder using the `Encoder` struct's `Marshal` method.\n\nIf an error occurs during the marshaling process, the function returns an error with a message that indicates the error.\n\nFinally, the function returns `nil` if no error occurs, indicating that the trace context was marshaled successfully into the encoder."]}, "weaver-internal/net/call/trace.go-readTraceContext": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/net/call/trace.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage call\n\nimport (\n\t\"context\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// writeTraceContext serializes the trace context (if any) contained in ctx\n// into enc.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// readTraceContext returns a span context with tracing information stored in dec.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func readTraceContext(dec *codegen.Decoder) *trace.SpanContext {\n# Natural Language Annotations: // readTraceContext returns a span context with tracing information stored in dec.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\npackage call\n\nimport (\n\t\"context\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// readTraceContext returns a span context with tracing information stored in dec.\nfunc readTraceContext(dec *codegen.Decoder) *trace.SpanContext {\n\t// Create a new span context.\n\tctx := trace.SpanContext{}\n\t// Read the span ID from dec.\n\tif err := dec.Read(&ctx.SpanId); err != nil {\n\t\treturn nil\n\t}\n\t// Read the trace ID from dec.\n\tif err := dec.Read(&ctx.TraceId); err != nil {\n\t\treturn nil\n\t}\n\t// Read the parent span ID from dec.\n\tif err := dec.Read(&ctx.ParentSpanId); err != nil {\n\t\treturn nil\n\t}\n\t// Read the version from dec.\n\tif err := dec.Read(&ctx.Version); err != nil {\n\t\treturn nil\n\t}\n\t// Return the span context.\n\treturn &ctx\n}\n\n```", "  ```\npackage call\n\nfunc readTraceContext(ctx context.Context, dec *codegen.Decoder) *trace.SpanContext {\n\tsc := trace.SpanContext{}\n\tif err := dec.Decode(&sc.TracingInformations); err != nil {\n\t\treturn nil\n\t}\n\tif err := dec.Decode(&sc.RemoteSampling); err != nil {\n\t\treturn nil\n\t}\n\tif err := dec.Decode(&sc.IsSampled); err != nil {\n\t\treturn nil\n\t}\n\tif err := dec.Decode(&sc.SamplingConfidence); err != nil {\n\t\treturn nil\n\t}\n\tif err := dec.Decode(&sc.Payload); err != nil {\n\t\treturn nil\n\t}\n\treturn &sc\n}\n```", "  ```\npackage call\n\nimport (\n\t\"context\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\nfunc readTraceContext(dec *codegen.Decoder) *trace.SpanContext {\n\t// Execute the readTraceContext function for the specified context.\n\tctx := context.Background()\n\t// Get the tracer from the context.\n\ttracer := trace.Tracer(ctx)\n\t// Get the span context from the tracer.\n\tspanCtx := tracer.getSpanContext()\n\t// Return the span context.\n\treturn spanCtx\n}\n```\nPlease note that this code is a simplified version of the original `readTraceContext()` function and may not work as is. This code is intended as a basic example to help you understand the structure of the function and how it interacts with the context. In a real-world implementation, you would need to take into account factors such as the specific requirements of your tracing implementation and the specific structure of your context."]}, "ollama-types/model/name.go-ParseNameBare": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/ollama/types/model/name.go:\n```\n// Package model contains types and utilities for parsing, validating, and\n// working with model names and digests.\npackage model\n\nimport (\n\t\"cmp\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"path/filepath\"\n\t\"strings\"\n)\n\n// Errors\nvar (\n\t// ErrUnqualifiedName represents an error where a name is not fully\n\t// qualified. It is not used directly in this package, but is here\n\t// to avoid other packages inventing their own error type.\n\t// Additionally, it can be conveniently used via [Unqualified].\n\tErrUnqualifiedName = errors.New(\"unqualified name\")\n)\n\n// Unqualified is a helper function that returns an error with\n// ErrUnqualifiedName as the cause and the name as the message.\nfunc Unqualified(n Name) error {\n\treturn fmt.Errorf(\"%w: %s\", ErrUnqualifiedName, n)\n}\n\n// MissingPart is used to indicate any part of a name that was \"promised\" by\n// the presence of a separator, but is missing.\n//\n// The value was chosen because it is deemed unlikely to be set by a user,\n// not a valid part name valid when checked by [Name.IsValid], and easy to\n// spot in logs.\nconst MissingPart = \"!MISSING!\"\n\nconst (\n\tdefaultHost      = \"registry.ollama.ai\"\n\tdefaultNamespace = \"library\"\n\tdefaultTag       = \"latest\"\n)\n\n// DefaultName returns a name with the default values for the host, namespace,\n// and tag parts. The model and digest parts are empty.\n//\n//   - The default host is (\"registry.ollama.ai\")\n//   - The default namespace is (\"library\")\n//   - The default tag is (\"latest\")\nfunc DefaultName() Name {\n\treturn Name{\n\t\tHost:      defaultHost,\n\t\tNamespace: defaultNamespace,\n\t\tTag:       defaultTag,\n\t}\n}\n\ntype partKind int\n\nconst (\n\tkindHost partKind = iota\n\tkindNamespace\n\tkindModel\n\tkindTag\n\tkindDigest\n)\n\nfunc (k partKind) String() string {\n\tswitch k {\n\tcase kindHost:\n\t\treturn \"host\"\n\tcase kindNamespace:\n\t\treturn \"namespace\"\n\tcase kindModel:\n\t\treturn \"model\"\n\tcase kindTag:\n\t\treturn \"tag\"\n\tcase kindDigest:\n\t\treturn \"digest\"\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\n// Name is a structured representation of a model name string, as defined by\n// [ParseNameNoDefaults].\n//\n// It is not guaranteed to be valid. Use [Name.IsValid] to check if the name\n// is valid.\ntype Name struct {\n\tHost      string\n\tNamespace string\n\tModel     string\n\tTag       string\n\tRawDigest string\n}\n\n// ParseName parses and assembles a Name from a name string. The\n// format of a valid name string is:\n//\n//\t  s:\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model }\n//\t\t  { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model }\n//\t\t  { model } \":\" { tag } \"@\" { digest }\n//\t\t  { model } \":\" { tag }\n//\t\t  { model } \"@\" { digest }\n//\t\t  { model }\n//\t\t  \"@\" { digest }\n//\t  host:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" | \":\" }*\n//\t      length:  [1, 350]\n//\t  namespace:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" }*\n//\t      length:  [1, 80]\n//\t  model:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  tag:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  digest:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \":\" }*\n//\t      length:  [1, 80]\n//\n// Most users should use [ParseName] instead, unless need to support\n// different defaults than DefaultName.\n//\n// The name returned is not guaranteed to be valid. If it is not valid, the\n// field values are left in an undefined state. Use [Name.IsValid] to check\n// if the name is valid.\nfunc ParseName(s string) Name {\n\treturn Merge(ParseNameBare(s), DefaultName())\n}\n\n// ParseNameBare parses s as a name string and returns a Name. No merge with\n// [DefaultName] is performed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ParseNameFromFilepath parses a 4-part filepath as a Name. The parts are\n// expected to be in the form:\n//\n// { host } \"/\" { namespace } \"/\" { model } \"/\" { tag }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Merge merges the host, namespace, and tag parts of the two names,\n// preferring the non-empty parts of a.\nfunc Merge(a, b Name) Name {\n\ta.Host = cmp.Or(a.Host, b.Host)\n\ta.Namespace = cmp.Or(a.Namespace, b.Namespace)\n\ta.Tag = cmp.Or(a.Tag, b.Tag)\n\treturn a\n}\n\n// String returns the name string, in the format that [ParseNameNoDefaults]\n// accepts as valid, if [Name.IsValid] reports true; otherwise the empty\n// string is returned.\nfunc (n Name) String() string {\n\tvar b strings.Builder\n\tif n.Host != \"\" {\n\t\tb.WriteString(n.Host)\n\t\tb.WriteByte('/')\n\t}\n\tif n.Namespace != \"\" {\n\t\tb.WriteString(n.Namespace)\n\t\tb.WriteByte('/')\n\t}\n\tb.WriteString(n.Model)\n\tif n.Tag != \"\" {\n\t\tb.WriteByte(':')\n\t\tb.WriteString(n.Tag)\n\t}\n\tif n.RawDigest != \"\" {\n\t\tb.WriteByte('@')\n\t\tb.WriteString(n.RawDigest)\n\t}\n\treturn b.String()\n}\n\n// DisplayShort returns a short string version of the name.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc IsValidNamespace(namespace string) bool {\n\treturn isValidPart(kindNamespace, namespace)\n}\n\n// IsValid reports whether all parts of the name are present and valid. The\n// digest is a special case, and is checked for validity only if present.\nfunc (n Name) IsValid() bool {\n\tif n.RawDigest != \"\" && !isValidPart(kindDigest, n.RawDigest) {\n\t\treturn false\n\t}\n\treturn n.IsFullyQualified()\n}\n\n// IsFullyQualified returns true if all parts of the name are present and\n// valid without the digest.\nfunc (n Name) IsFullyQualified() bool {\n\tvar parts = []string{\n\t\tn.Host,\n\t\tn.Namespace,\n\t\tn.Model,\n\t\tn.Tag,\n\t}\n\tfor i, part := range parts {\n\t\tif !isValidPart(partKind(i), part) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Filepath returns a canonical filepath that represents the name with each part from\n// host to tag as a directory in the form:\n//\n//\t{host}/{namespace}/{model}/{tag}\n//\n// It uses the system's filepath separator and ensures the path is clean.\n//\n// It panics if the name is not fully qualified. Use [Name.IsFullyQualified]\n// to check if the name is fully qualified.\n\n\n\n\n\n\n\n\n\n\n\n\n// LogValue returns a slog.Value that represents the name as a string.\nfunc (n Name) LogValue() slog.Value {\n\treturn slog.StringValue(n.String())\n}\n\nfunc isValidLen(kind partKind, s string) bool {\n\tswitch kind {\n\tcase kindHost:\n\t\treturn len(s) >= 1 && len(s) <= 350\n\tcase kindTag:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\tdefault:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\t}\n}\n\nfunc isValidPart(kind partKind, s string) bool {\n\tif !isValidLen(kind, s) {\n\t\treturn false\n\t}\n\tfor i := range s {\n\t\tif i == 0 {\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tswitch s[i] {\n\t\tcase '_', '-':\n\t\tcase '.':\n\t\t\tif kind == kindNamespace {\n\t\t\t\treturn false\n\t\t\t}\n\t\tcase ':':\n\t\t\tif kind != kindHost && kind != kindDigest {\n\t\t\t\treturn false\n\t\t\t}\n\t\tdefault:\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\nfunc isAlphanumericOrUnderscore(c byte) bool {\n\treturn c >= 'A' && c <= 'Z' || c >= 'a' && c <= 'z' || c >= '0' && c <= '9' || c == '_'\n}\n\nfunc cutLast(s, sep string) (before, after string, ok bool) {\n\ti := strings.LastIndex(s, sep)\n\tif i >= 0 {\n\t\treturn s[:i], s[i+len(sep):], true\n\t}\n\treturn s, \"\", false\n}\n\n// cutPromised cuts the last part of s at the last occurrence of sep. If sep is\n// found, the part before and after sep are returned as-is unless empty, in\n// which case they are returned as MissingPart, which will cause\n// [Name.IsValid] to return false.\nfunc cutPromised(s, sep string) (before, after string, ok bool) {\n\tbefore, after, ok = cutLast(s, sep)\n\tif !ok {\n\t\treturn before, after, false\n\t}\n\treturn cmp.Or(before, MissingPart), cmp.Or(after, MissingPart), true\n}\n\n```\n# Function Signature: func ParseNameBare(s string) Name {\n# Natural Language Annotations: // ParseNameBare parses s as a name string and returns a Name. No merge with\n// [DefaultName] is performed.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc ParseNameBare(s string) Name {\n\tvar parts []string\n\tfor _, part := range []string{s} {\n\t\tvar newParts []string\n\t\tif part = cmp.Or(part, MissingPart); part != MissingPart {\n\t\t\tnewParts, parts = append(parts, part), []string{}\n\t\t}\n\t}\n\tvar n Name\n\tswitch len(parts) {\n\tcase 6:\n\t\tn.RawDigest = parts[5]\n\t\tfallthrough\n\tcase 4, 5:\n\t\tn.Tag = parts[3]\n\t}\n\tswitch len(parts) {\n\tcase 4, 5, 6:\n\t\tn.Model = parts[2]\n\t\tfallthrough\n\tcase 3:\n\t\tn.Namespace = parts[1]\n\t\tfallthrough\n\tcase 2:\n\t\tn.Host = parts[0]\n\t}\n\treturn n\n}\n```", "  ```\nfunc ParseNameBare(s string) Name {\n\tvar parts = strings.Split(s, \":\")\n\tif len(parts) == 4 {\n\t\tvar host, namespace, model, tag string\n\t\tvar digest string\n\t\thost, namespace, model, ok := cutPromised(parts[0], \"/\")\n\t\tif !ok {\n\t\t\treturn Name{Host: MissingPart}\n\t\t}\n\t\tnamespace, model, ok = cutPromised(namespace, \"/\")\n\t\tif !ok {\n\t\t\treturn Name{Gost: parts[0], Namespace: MissingPart}\n\t\t}\n\t\tmodel, tag, ok = cutPromised(parts[1], \"tag\")\n\t\tif !ok {\n\t\t\treturn Name{gost: parts[0], Namespace: parts[1], Model: MissingPart}\n\t\t}\n\t\tdigest = parts[3]\n\t\treturn Name{Host: host, Namespace: namespace, Model: model, Tag: tag, RawDigest: digest}\n\t}\n\treturn Name{RawDigest: s}\n}\n```", "  ```\nfunc ParseNameBare(s string) Name {\n\tvar n Name\n\tvar before, after string\n\tvar ok bool\n\n\t// host\n\tif n.Host, s, ok = cutPromised(s, defaultHost); !ok {\n\t\tif n.Host, s, ok = cutLast(s, \"/\"); !ok {\n\t\t\treturn Name{}\n\t\t}\n\t}\n\n\t// namespace\n\tif n.Namespace, s, ok = cutPromised(s, defaultNamespace); !ok {\n\t\tif n.Namespace, s, ok = cutLast(s, \"/\"); !ok {\n\t\t\treturn Name{}\n\t\t}\n\t}\n\n\t// model\n\tn.Model = s\n\tif n.Model, after, ok = cmp.Split(s, \"/\"); !ok {\n\t\treturn Name{}\n\t}\n\n\t// tag\n\tif n.Tag, after, ok = cutPromised(after, defaultTag); !ok {\n\t\tif n.Tag, after, ok = cmp.Split(s, \":\"); !ok {\n\t\t\treturn Name{}\n\t\t}\n\t\tif after, ok = cmp.Split(after, \"/\"); !ok {\n\t\t\treturn Name{}\n\t\t}\n\t}\n\n\t// digest\n\tif after, ok = cmp.Split(after, \"@\"); !ok {\n\t\treturn Name{}\n\t}\n\tif n.RawDigest = after; n.RawDigest != \"\" && !IsValidPart(kindDigest, n.RawDigest) {\n\t\treturn Name{}\n\t}\n\treturn n\n}\n```"]}, "ollama-types/model/name.go-ParseNameFromFilepath": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/ollama/types/model/name.go:\n```\n// Package model contains types and utilities for parsing, validating, and\n// working with model names and digests.\npackage model\n\nimport (\n\t\"cmp\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"path/filepath\"\n\t\"strings\"\n)\n\n// Errors\nvar (\n\t// ErrUnqualifiedName represents an error where a name is not fully\n\t// qualified. It is not used directly in this package, but is here\n\t// to avoid other packages inventing their own error type.\n\t// Additionally, it can be conveniently used via [Unqualified].\n\tErrUnqualifiedName = errors.New(\"unqualified name\")\n)\n\n// Unqualified is a helper function that returns an error with\n// ErrUnqualifiedName as the cause and the name as the message.\nfunc Unqualified(n Name) error {\n\treturn fmt.Errorf(\"%w: %s\", ErrUnqualifiedName, n)\n}\n\n// MissingPart is used to indicate any part of a name that was \"promised\" by\n// the presence of a separator, but is missing.\n//\n// The value was chosen because it is deemed unlikely to be set by a user,\n// not a valid part name valid when checked by [Name.IsValid], and easy to\n// spot in logs.\nconst MissingPart = \"!MISSING!\"\n\nconst (\n\tdefaultHost      = \"registry.ollama.ai\"\n\tdefaultNamespace = \"library\"\n\tdefaultTag       = \"latest\"\n)\n\n// DefaultName returns a name with the default values for the host, namespace,\n// and tag parts. The model and digest parts are empty.\n//\n//   - The default host is (\"registry.ollama.ai\")\n//   - The default namespace is (\"library\")\n//   - The default tag is (\"latest\")\nfunc DefaultName() Name {\n\treturn Name{\n\t\tHost:      defaultHost,\n\t\tNamespace: defaultNamespace,\n\t\tTag:       defaultTag,\n\t}\n}\n\ntype partKind int\n\nconst (\n\tkindHost partKind = iota\n\tkindNamespace\n\tkindModel\n\tkindTag\n\tkindDigest\n)\n\nfunc (k partKind) String() string {\n\tswitch k {\n\tcase kindHost:\n\t\treturn \"host\"\n\tcase kindNamespace:\n\t\treturn \"namespace\"\n\tcase kindModel:\n\t\treturn \"model\"\n\tcase kindTag:\n\t\treturn \"tag\"\n\tcase kindDigest:\n\t\treturn \"digest\"\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\n// Name is a structured representation of a model name string, as defined by\n// [ParseNameNoDefaults].\n//\n// It is not guaranteed to be valid. Use [Name.IsValid] to check if the name\n// is valid.\ntype Name struct {\n\tHost      string\n\tNamespace string\n\tModel     string\n\tTag       string\n\tRawDigest string\n}\n\n// ParseName parses and assembles a Name from a name string. The\n// format of a valid name string is:\n//\n//\t  s:\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model }\n//\t\t  { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model }\n//\t\t  { model } \":\" { tag } \"@\" { digest }\n//\t\t  { model } \":\" { tag }\n//\t\t  { model } \"@\" { digest }\n//\t\t  { model }\n//\t\t  \"@\" { digest }\n//\t  host:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" | \":\" }*\n//\t      length:  [1, 350]\n//\t  namespace:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" }*\n//\t      length:  [1, 80]\n//\t  model:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  tag:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  digest:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \":\" }*\n//\t      length:  [1, 80]\n//\n// Most users should use [ParseName] instead, unless need to support\n// different defaults than DefaultName.\n//\n// The name returned is not guaranteed to be valid. If it is not valid, the\n// field values are left in an undefined state. Use [Name.IsValid] to check\n// if the name is valid.\nfunc ParseName(s string) Name {\n\treturn Merge(ParseNameBare(s), DefaultName())\n}\n\n// ParseNameBare parses s as a name string and returns a Name. No merge with\n// [DefaultName] is performed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ParseNameFromFilepath parses a 4-part filepath as a Name. The parts are\n// expected to be in the form:\n//\n// { host } \"/\" { namespace } \"/\" { model } \"/\" { tag }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Merge merges the host, namespace, and tag parts of the two names,\n// preferring the non-empty parts of a.\nfunc Merge(a, b Name) Name {\n\ta.Host = cmp.Or(a.Host, b.Host)\n\ta.Namespace = cmp.Or(a.Namespace, b.Namespace)\n\ta.Tag = cmp.Or(a.Tag, b.Tag)\n\treturn a\n}\n\n// String returns the name string, in the format that [ParseNameNoDefaults]\n// accepts as valid, if [Name.IsValid] reports true; otherwise the empty\n// string is returned.\nfunc (n Name) String() string {\n\tvar b strings.Builder\n\tif n.Host != \"\" {\n\t\tb.WriteString(n.Host)\n\t\tb.WriteByte('/')\n\t}\n\tif n.Namespace != \"\" {\n\t\tb.WriteString(n.Namespace)\n\t\tb.WriteByte('/')\n\t}\n\tb.WriteString(n.Model)\n\tif n.Tag != \"\" {\n\t\tb.WriteByte(':')\n\t\tb.WriteString(n.Tag)\n\t}\n\tif n.RawDigest != \"\" {\n\t\tb.WriteByte('@')\n\t\tb.WriteString(n.RawDigest)\n\t}\n\treturn b.String()\n}\n\n// DisplayShort returns a short string version of the name.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc IsValidNamespace(namespace string) bool {\n\treturn isValidPart(kindNamespace, namespace)\n}\n\n// IsValid reports whether all parts of the name are present and valid. The\n// digest is a special case, and is checked for validity only if present.\nfunc (n Name) IsValid() bool {\n\tif n.RawDigest != \"\" && !isValidPart(kindDigest, n.RawDigest) {\n\t\treturn false\n\t}\n\treturn n.IsFullyQualified()\n}\n\n// IsFullyQualified returns true if all parts of the name are present and\n// valid without the digest.\nfunc (n Name) IsFullyQualified() bool {\n\tvar parts = []string{\n\t\tn.Host,\n\t\tn.Namespace,\n\t\tn.Model,\n\t\tn.Tag,\n\t}\n\tfor i, part := range parts {\n\t\tif !isValidPart(partKind(i), part) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Filepath returns a canonical filepath that represents the name with each part from\n// host to tag as a directory in the form:\n//\n//\t{host}/{namespace}/{model}/{tag}\n//\n// It uses the system's filepath separator and ensures the path is clean.\n//\n// It panics if the name is not fully qualified. Use [Name.IsFullyQualified]\n// to check if the name is fully qualified.\n\n\n\n\n\n\n\n\n\n\n\n\n// LogValue returns a slog.Value that represents the name as a string.\nfunc (n Name) LogValue() slog.Value {\n\treturn slog.StringValue(n.String())\n}\n\nfunc isValidLen(kind partKind, s string) bool {\n\tswitch kind {\n\tcase kindHost:\n\t\treturn len(s) >= 1 && len(s) <= 350\n\tcase kindTag:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\tdefault:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\t}\n}\n\nfunc isValidPart(kind partKind, s string) bool {\n\tif !isValidLen(kind, s) {\n\t\treturn false\n\t}\n\tfor i := range s {\n\t\tif i == 0 {\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tswitch s[i] {\n\t\tcase '_', '-':\n\t\tcase '.':\n\t\t\tif kind == kindNamespace {\n\t\t\t\treturn false\n\t\t\t}\n\t\tcase ':':\n\t\t\tif kind != kindHost && kind != kindDigest {\n\t\t\t\treturn false\n\t\t\t}\n\t\tdefault:\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\nfunc isAlphanumericOrUnderscore(c byte) bool {\n\treturn c >= 'A' && c <= 'Z' || c >= 'a' && c <= 'z' || c >= '0' && c <= '9' || c == '_'\n}\n\nfunc cutLast(s, sep string) (before, after string, ok bool) {\n\ti := strings.LastIndex(s, sep)\n\tif i >= 0 {\n\t\treturn s[:i], s[i+len(sep):], true\n\t}\n\treturn s, \"\", false\n}\n\n// cutPromised cuts the last part of s at the last occurrence of sep. If sep is\n// found, the part before and after sep are returned as-is unless empty, in\n// which case they are returned as MissingPart, which will cause\n// [Name.IsValid] to return false.\nfunc cutPromised(s, sep string) (before, after string, ok bool) {\n\tbefore, after, ok = cutLast(s, sep)\n\tif !ok {\n\t\treturn before, after, false\n\t}\n\treturn cmp.Or(before, MissingPart), cmp.Or(after, MissingPart), true\n}\n\n```\n# Function Signature: func ParseNameFromFilepath(s string) (n Name) {\n# Natural Language Annotations: // ParseNameFromFilepath parses a 4-part filepath as a Name. The parts are\n// expected to be in the form:\n//\n// { host } \"/\" { namespace } \"/\" { model } \"/\" { tag }\nPlease return the generated code surrounded by ```\n", "response": ["  ```\npackage model\n\nimport (\n\t\"cmp\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"path/filepath\"\n\t\"strings\"\n)\n\n// ParseNameFromFilepath parses a 4-part filepath as a Name. The parts are\n// expected to be in the form:\n//\n// { host } \"/\" { namespace } \"/\" { model } \"/\" { tag }\nfunc ParseNameFromFilepath(s string) (n Name, err error) {\n\tvar host, namespace, model, tag string\n\thost, s, ok := cutLast(s, \"/\")\n\tif !ok {\n\t\treturn Name{}, errors.New(\"missing host\")\n\t}\n\tnamespace, s, ok = cutLast(s, \"/\")\n\tif !ok {\n\t\treturn Name{}, errors.New(\"missing namespace\")\n\t}\n\tmodel, s, ok = cutLast(s, \"/\")\n\tif !ok {\n\t\treturn Name{}, errors.New(\"missing model\")\n\t}\n\ttag, _, ok = cutLast(s, \"/\")\n\tif !ok {\n\t\treturn Name{}, errors.New(\"missing tag\")\n\t}\n\n\tn.Host = host\n\tn.Namespace = namespace\n\tn.Model = model\n\tn.Tag = tag\n\n\tif !n.IsValid() {\n\t\treturn Name{}, fmt.Errorf(\"invalid name: %w\", n)\n\t}\n\n\treturn n, nil\n}\n```", "  ```\nfunc ParseNameFromFilepath(s string) (n Name) {\n\tvar (\n\t\tparts     = strings.Split(s, \"/\")\n\t\tpartsLen  = len(parts)\n\t\thostIndex = partsLen - 4\n\t)\n\tif partsLen != 4 {\n\t\treturn\n\t}\n\tn.Host = parts[hostIndex]\n\tn.Namespace = parts[hostIndex+1]\n\tn.Model = parts[hostIndex+2]\n\tn.Tag = parts[hostIndex+3]\n\tif !n.IsValid() {\n\t\tn = Name{}\n\t}\n\treturn\n}\n```", "  ```\nfunc ParseNameFromFilepath(s string) (n Name) {\n\ts = strings.Trim(s, \"/\") // Remove leading and trailing slashes\n\tparts := strings.Split(s, \"/\")\n\tif len(parts) != 4 {\n\t\treturn Name{Errorf(\"Invalid file path\")}\n\t}\n\t\n\tn.Host = strings.TrimSuffix(parts[0], \"/\")\n\tn.Namespace = parts[1]\n\tn.Model = parts[2]\n\tn.Tag = parts[3]\n\t\n\tif n.Host == \"\" {\n\t\tn.Host = defaultHost\n\t}\n\tif n.Namespace == \"\" {\n\t\tn.Namespace = defaultNamespace\n\t}\n\tif n.Tag == \"\" {\n\t\tn.Tag = defaultTag\n\t}\n\t\n\treturn n\n}\n```"]}, "ollama-types/model/name.go-DisplayShortest": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/ollama/types/model/name.go:\n```\n// Package model contains types and utilities for parsing, validating, and\n// working with model names and digests.\npackage model\n\nimport (\n\t\"cmp\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"path/filepath\"\n\t\"strings\"\n)\n\n// Errors\nvar (\n\t// ErrUnqualifiedName represents an error where a name is not fully\n\t// qualified. It is not used directly in this package, but is here\n\t// to avoid other packages inventing their own error type.\n\t// Additionally, it can be conveniently used via [Unqualified].\n\tErrUnqualifiedName = errors.New(\"unqualified name\")\n)\n\n// Unqualified is a helper function that returns an error with\n// ErrUnqualifiedName as the cause and the name as the message.\nfunc Unqualified(n Name) error {\n\treturn fmt.Errorf(\"%w: %s\", ErrUnqualifiedName, n)\n}\n\n// MissingPart is used to indicate any part of a name that was \"promised\" by\n// the presence of a separator, but is missing.\n//\n// The value was chosen because it is deemed unlikely to be set by a user,\n// not a valid part name valid when checked by [Name.IsValid], and easy to\n// spot in logs.\nconst MissingPart = \"!MISSING!\"\n\nconst (\n\tdefaultHost      = \"registry.ollama.ai\"\n\tdefaultNamespace = \"library\"\n\tdefaultTag       = \"latest\"\n)\n\n// DefaultName returns a name with the default values for the host, namespace,\n// and tag parts. The model and digest parts are empty.\n//\n//   - The default host is (\"registry.ollama.ai\")\n//   - The default namespace is (\"library\")\n//   - The default tag is (\"latest\")\nfunc DefaultName() Name {\n\treturn Name{\n\t\tHost:      defaultHost,\n\t\tNamespace: defaultNamespace,\n\t\tTag:       defaultTag,\n\t}\n}\n\ntype partKind int\n\nconst (\n\tkindHost partKind = iota\n\tkindNamespace\n\tkindModel\n\tkindTag\n\tkindDigest\n)\n\nfunc (k partKind) String() string {\n\tswitch k {\n\tcase kindHost:\n\t\treturn \"host\"\n\tcase kindNamespace:\n\t\treturn \"namespace\"\n\tcase kindModel:\n\t\treturn \"model\"\n\tcase kindTag:\n\t\treturn \"tag\"\n\tcase kindDigest:\n\t\treturn \"digest\"\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\n// Name is a structured representation of a model name string, as defined by\n// [ParseNameNoDefaults].\n//\n// It is not guaranteed to be valid. Use [Name.IsValid] to check if the name\n// is valid.\ntype Name struct {\n\tHost      string\n\tNamespace string\n\tModel     string\n\tTag       string\n\tRawDigest string\n}\n\n// ParseName parses and assembles a Name from a name string. The\n// format of a valid name string is:\n//\n//\t  s:\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model }\n//\t\t  { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model }\n//\t\t  { model } \":\" { tag } \"@\" { digest }\n//\t\t  { model } \":\" { tag }\n//\t\t  { model } \"@\" { digest }\n//\t\t  { model }\n//\t\t  \"@\" { digest }\n//\t  host:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" | \":\" }*\n//\t      length:  [1, 350]\n//\t  namespace:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" }*\n//\t      length:  [1, 80]\n//\t  model:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  tag:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  digest:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \":\" }*\n//\t      length:  [1, 80]\n//\n// Most users should use [ParseName] instead, unless need to support\n// different defaults than DefaultName.\n//\n// The name returned is not guaranteed to be valid. If it is not valid, the\n// field values are left in an undefined state. Use [Name.IsValid] to check\n// if the name is valid.\nfunc ParseName(s string) Name {\n\treturn Merge(ParseNameBare(s), DefaultName())\n}\n\n// ParseNameBare parses s as a name string and returns a Name. No merge with\n// [DefaultName] is performed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ParseNameFromFilepath parses a 4-part filepath as a Name. The parts are\n// expected to be in the form:\n//\n// { host } \"/\" { namespace } \"/\" { model } \"/\" { tag }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Merge merges the host, namespace, and tag parts of the two names,\n// preferring the non-empty parts of a.\nfunc Merge(a, b Name) Name {\n\ta.Host = cmp.Or(a.Host, b.Host)\n\ta.Namespace = cmp.Or(a.Namespace, b.Namespace)\n\ta.Tag = cmp.Or(a.Tag, b.Tag)\n\treturn a\n}\n\n// String returns the name string, in the format that [ParseNameNoDefaults]\n// accepts as valid, if [Name.IsValid] reports true; otherwise the empty\n// string is returned.\nfunc (n Name) String() string {\n\tvar b strings.Builder\n\tif n.Host != \"\" {\n\t\tb.WriteString(n.Host)\n\t\tb.WriteByte('/')\n\t}\n\tif n.Namespace != \"\" {\n\t\tb.WriteString(n.Namespace)\n\t\tb.WriteByte('/')\n\t}\n\tb.WriteString(n.Model)\n\tif n.Tag != \"\" {\n\t\tb.WriteByte(':')\n\t\tb.WriteString(n.Tag)\n\t}\n\tif n.RawDigest != \"\" {\n\t\tb.WriteByte('@')\n\t\tb.WriteString(n.RawDigest)\n\t}\n\treturn b.String()\n}\n\n// DisplayShort returns a short string version of the name.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc IsValidNamespace(namespace string) bool {\n\treturn isValidPart(kindNamespace, namespace)\n}\n\n// IsValid reports whether all parts of the name are present and valid. The\n// digest is a special case, and is checked for validity only if present.\nfunc (n Name) IsValid() bool {\n\tif n.RawDigest != \"\" && !isValidPart(kindDigest, n.RawDigest) {\n\t\treturn false\n\t}\n\treturn n.IsFullyQualified()\n}\n\n// IsFullyQualified returns true if all parts of the name are present and\n// valid without the digest.\nfunc (n Name) IsFullyQualified() bool {\n\tvar parts = []string{\n\t\tn.Host,\n\t\tn.Namespace,\n\t\tn.Model,\n\t\tn.Tag,\n\t}\n\tfor i, part := range parts {\n\t\tif !isValidPart(partKind(i), part) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Filepath returns a canonical filepath that represents the name with each part from\n// host to tag as a directory in the form:\n//\n//\t{host}/{namespace}/{model}/{tag}\n//\n// It uses the system's filepath separator and ensures the path is clean.\n//\n// It panics if the name is not fully qualified. Use [Name.IsFullyQualified]\n// to check if the name is fully qualified.\n\n\n\n\n\n\n\n\n\n\n\n\n// LogValue returns a slog.Value that represents the name as a string.\nfunc (n Name) LogValue() slog.Value {\n\treturn slog.StringValue(n.String())\n}\n\nfunc isValidLen(kind partKind, s string) bool {\n\tswitch kind {\n\tcase kindHost:\n\t\treturn len(s) >= 1 && len(s) <= 350\n\tcase kindTag:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\tdefault:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\t}\n}\n\nfunc isValidPart(kind partKind, s string) bool {\n\tif !isValidLen(kind, s) {\n\t\treturn false\n\t}\n\tfor i := range s {\n\t\tif i == 0 {\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tswitch s[i] {\n\t\tcase '_', '-':\n\t\tcase '.':\n\t\t\tif kind == kindNamespace {\n\t\t\t\treturn false\n\t\t\t}\n\t\tcase ':':\n\t\t\tif kind != kindHost && kind != kindDigest {\n\t\t\t\treturn false\n\t\t\t}\n\t\tdefault:\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\nfunc isAlphanumericOrUnderscore(c byte) bool {\n\treturn c >= 'A' && c <= 'Z' || c >= 'a' && c <= 'z' || c >= '0' && c <= '9' || c == '_'\n}\n\nfunc cutLast(s, sep string) (before, after string, ok bool) {\n\ti := strings.LastIndex(s, sep)\n\tif i >= 0 {\n\t\treturn s[:i], s[i+len(sep):], true\n\t}\n\treturn s, \"\", false\n}\n\n// cutPromised cuts the last part of s at the last occurrence of sep. If sep is\n// found, the part before and after sep are returned as-is unless empty, in\n// which case they are returned as MissingPart, which will cause\n// [Name.IsValid] to return false.\nfunc cutPromised(s, sep string) (before, after string, ok bool) {\n\tbefore, after, ok = cutLast(s, sep)\n\tif !ok {\n\t\treturn before, after, false\n\t}\n\treturn cmp.Or(before, MissingPart), cmp.Or(after, MissingPart), true\n}\n\n```\n# Function Signature: func (n Name) DisplayShortest() string {\n# Natural Language Annotations: // DisplayShort returns a short string version of the name.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (n Name) DisplayShort() string {\n\tvar b strings.Builder\n\tb.Grow(42)\n\tif n.Host != \"\" {\n\t\tb.WriteString(n.Host)\n\t\tb.WriteByte('/')\n\t}\n\tif n.Namespace != \"\" {\n\t\tb.WriteString(n.Namespace)\n\t\tb.WriteByte('/')\n\t}\n\tif n.Model != \"\" {\n\t\tb.WriteString(n.Model)\n\t\tb.WriteByte(':')\n\t}\n\tif n.Tag != \"\" {\n\t\tb.WriteString(n.Tag)\n\t}\n\treturn b.String()\n}\n```", "  ```\nfunc (n Name) DisplayShort() string {\n\tif !n.IsFullyQualified() {\n\t\treturn MissingPart\n\t}\n\tvar strs []string\n\tif n.Host != \"\" {\n\t\tstrs = append(strs, n.Host)\n\t}\n\tif n.Namespace != \"\" {\n\t\tstrs = append(strs, n.Namespace)\n\t}\n\tstrs = append(strs, n.Model)\n\tif n.Tag != \"\" {\n\t\tstrs = append(strs, n.Tag)\n\t}\n\tif n.Digest != \"\" {\n\t\tstrs = append(strs, n.Digest)\n\t}\n\treturn strings.Join(strs, \":\")\n}\n```", "  ```\nfunc (n Name) DisplayShortest() string {\n\tvar b strings.Builder\n\tb.WriteString(n.Host)\n\tb.WriteByte('/')\n\tb.WriteString(n.Model)\n\tb.WriteString(\"!missing@\")\n\treturn b.String()\n}\n```"]}, "ollama-types/model/name.go-Filepath": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/ollama/types/model/name.go:\n```\n// Package model contains types and utilities for parsing, validating, and\n// working with model names and digests.\npackage model\n\nimport (\n\t\"cmp\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"path/filepath\"\n\t\"strings\"\n)\n\n// Errors\nvar (\n\t// ErrUnqualifiedName represents an error where a name is not fully\n\t// qualified. It is not used directly in this package, but is here\n\t// to avoid other packages inventing their own error type.\n\t// Additionally, it can be conveniently used via [Unqualified].\n\tErrUnqualifiedName = errors.New(\"unqualified name\")\n)\n\n// Unqualified is a helper function that returns an error with\n// ErrUnqualifiedName as the cause and the name as the message.\nfunc Unqualified(n Name) error {\n\treturn fmt.Errorf(\"%w: %s\", ErrUnqualifiedName, n)\n}\n\n// MissingPart is used to indicate any part of a name that was \"promised\" by\n// the presence of a separator, but is missing.\n//\n// The value was chosen because it is deemed unlikely to be set by a user,\n// not a valid part name valid when checked by [Name.IsValid], and easy to\n// spot in logs.\nconst MissingPart = \"!MISSING!\"\n\nconst (\n\tdefaultHost      = \"registry.ollama.ai\"\n\tdefaultNamespace = \"library\"\n\tdefaultTag       = \"latest\"\n)\n\n// DefaultName returns a name with the default values for the host, namespace,\n// and tag parts. The model and digest parts are empty.\n//\n//   - The default host is (\"registry.ollama.ai\")\n//   - The default namespace is (\"library\")\n//   - The default tag is (\"latest\")\nfunc DefaultName() Name {\n\treturn Name{\n\t\tHost:      defaultHost,\n\t\tNamespace: defaultNamespace,\n\t\tTag:       defaultTag,\n\t}\n}\n\ntype partKind int\n\nconst (\n\tkindHost partKind = iota\n\tkindNamespace\n\tkindModel\n\tkindTag\n\tkindDigest\n)\n\nfunc (k partKind) String() string {\n\tswitch k {\n\tcase kindHost:\n\t\treturn \"host\"\n\tcase kindNamespace:\n\t\treturn \"namespace\"\n\tcase kindModel:\n\t\treturn \"model\"\n\tcase kindTag:\n\t\treturn \"tag\"\n\tcase kindDigest:\n\t\treturn \"digest\"\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\n// Name is a structured representation of a model name string, as defined by\n// [ParseNameNoDefaults].\n//\n// It is not guaranteed to be valid. Use [Name.IsValid] to check if the name\n// is valid.\ntype Name struct {\n\tHost      string\n\tNamespace string\n\tModel     string\n\tTag       string\n\tRawDigest string\n}\n\n// ParseName parses and assembles a Name from a name string. The\n// format of a valid name string is:\n//\n//\t  s:\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model }\n//\t\t  { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model }\n//\t\t  { model } \":\" { tag } \"@\" { digest }\n//\t\t  { model } \":\" { tag }\n//\t\t  { model } \"@\" { digest }\n//\t\t  { model }\n//\t\t  \"@\" { digest }\n//\t  host:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" | \":\" }*\n//\t      length:  [1, 350]\n//\t  namespace:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" }*\n//\t      length:  [1, 80]\n//\t  model:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  tag:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  digest:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \":\" }*\n//\t      length:  [1, 80]\n//\n// Most users should use [ParseName] instead, unless need to support\n// different defaults than DefaultName.\n//\n// The name returned is not guaranteed to be valid. If it is not valid, the\n// field values are left in an undefined state. Use [Name.IsValid] to check\n// if the name is valid.\nfunc ParseName(s string) Name {\n\treturn Merge(ParseNameBare(s), DefaultName())\n}\n\n// ParseNameBare parses s as a name string and returns a Name. No merge with\n// [DefaultName] is performed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ParseNameFromFilepath parses a 4-part filepath as a Name. The parts are\n// expected to be in the form:\n//\n// { host } \"/\" { namespace } \"/\" { model } \"/\" { tag }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Merge merges the host, namespace, and tag parts of the two names,\n// preferring the non-empty parts of a.\nfunc Merge(a, b Name) Name {\n\ta.Host = cmp.Or(a.Host, b.Host)\n\ta.Namespace = cmp.Or(a.Namespace, b.Namespace)\n\ta.Tag = cmp.Or(a.Tag, b.Tag)\n\treturn a\n}\n\n// String returns the name string, in the format that [ParseNameNoDefaults]\n// accepts as valid, if [Name.IsValid] reports true; otherwise the empty\n// string is returned.\nfunc (n Name) String() string {\n\tvar b strings.Builder\n\tif n.Host != \"\" {\n\t\tb.WriteString(n.Host)\n\t\tb.WriteByte('/')\n\t}\n\tif n.Namespace != \"\" {\n\t\tb.WriteString(n.Namespace)\n\t\tb.WriteByte('/')\n\t}\n\tb.WriteString(n.Model)\n\tif n.Tag != \"\" {\n\t\tb.WriteByte(':')\n\t\tb.WriteString(n.Tag)\n\t}\n\tif n.RawDigest != \"\" {\n\t\tb.WriteByte('@')\n\t\tb.WriteString(n.RawDigest)\n\t}\n\treturn b.String()\n}\n\n// DisplayShort returns a short string version of the name.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc IsValidNamespace(namespace string) bool {\n\treturn isValidPart(kindNamespace, namespace)\n}\n\n// IsValid reports whether all parts of the name are present and valid. The\n// digest is a special case, and is checked for validity only if present.\nfunc (n Name) IsValid() bool {\n\tif n.RawDigest != \"\" && !isValidPart(kindDigest, n.RawDigest) {\n\t\treturn false\n\t}\n\treturn n.IsFullyQualified()\n}\n\n// IsFullyQualified returns true if all parts of the name are present and\n// valid without the digest.\nfunc (n Name) IsFullyQualified() bool {\n\tvar parts = []string{\n\t\tn.Host,\n\t\tn.Namespace,\n\t\tn.Model,\n\t\tn.Tag,\n\t}\n\tfor i, part := range parts {\n\t\tif !isValidPart(partKind(i), part) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Filepath returns a canonical filepath that represents the name with each part from\n// host to tag as a directory in the form:\n//\n//\t{host}/{namespace}/{model}/{tag}\n//\n// It uses the system's filepath separator and ensures the path is clean.\n//\n// It panics if the name is not fully qualified. Use [Name.IsFullyQualified]\n// to check if the name is fully qualified.\n\n\n\n\n\n\n\n\n\n\n\n\n// LogValue returns a slog.Value that represents the name as a string.\nfunc (n Name) LogValue() slog.Value {\n\treturn slog.StringValue(n.String())\n}\n\nfunc isValidLen(kind partKind, s string) bool {\n\tswitch kind {\n\tcase kindHost:\n\t\treturn len(s) >= 1 && len(s) <= 350\n\tcase kindTag:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\tdefault:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\t}\n}\n\nfunc isValidPart(kind partKind, s string) bool {\n\tif !isValidLen(kind, s) {\n\t\treturn false\n\t}\n\tfor i := range s {\n\t\tif i == 0 {\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tswitch s[i] {\n\t\tcase '_', '-':\n\t\tcase '.':\n\t\t\tif kind == kindNamespace {\n\t\t\t\treturn false\n\t\t\t}\n\t\tcase ':':\n\t\t\tif kind != kindHost && kind != kindDigest {\n\t\t\t\treturn false\n\t\t\t}\n\t\tdefault:\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\nfunc isAlphanumericOrUnderscore(c byte) bool {\n\treturn c >= 'A' && c <= 'Z' || c >= 'a' && c <= 'z' || c >= '0' && c <= '9' || c == '_'\n}\n\nfunc cutLast(s, sep string) (before, after string, ok bool) {\n\ti := strings.LastIndex(s, sep)\n\tif i >= 0 {\n\t\treturn s[:i], s[i+len(sep):], true\n\t}\n\treturn s, \"\", false\n}\n\n// cutPromised cuts the last part of s at the last occurrence of sep. If sep is\n// found, the part before and after sep are returned as-is unless empty, in\n// which case they are returned as MissingPart, which will cause\n// [Name.IsValid] to return false.\nfunc cutPromised(s, sep string) (before, after string, ok bool) {\n\tbefore, after, ok = cutLast(s, sep)\n\tif !ok {\n\t\treturn before, after, false\n\t}\n\treturn cmp.Or(before, MissingPart), cmp.Or(after, MissingPart), true\n}\n\n```\n# Function Signature: func (n Name) Filepath() string {\n# Natural Language Annotations: // Filepath returns a canonical filepath that represents the name with each part from\n// host to tag as a directory in the form:\n//\n//\t{host}/{namespace}/{model}/{tag}\n//\n// It uses the system's filepath separator and ensures the path is clean.\n//\n// It panics if the name is not fully qualified. Use [Name.IsFullyQualified]\n// to check if the name is fully qualified.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (n Name) Filepath() string {\n\tif !n.IsFullyQualified() {\n\t\tpanic(\"name not fully qualified\")\n\t}\n\treturn filepath.Join(n.Host, n.Namespace, n.Model, n.Tag)\n}\n```", "  ```\nfunc (n Name) Filepath() string {\n\tif !n.IsFullyQualified() {\n\t\tpanic(\"Name is not fully qualified\")\n\t}\n\tvar b filepath.Builder\n\tb.WriteString(n.Host)\n\tb.WriteByte('/')\n\tb.WriteString(n.Namespace)\n\tb.WriteByte('/')\n\tb.WriteString(n.Model)\n\tb.WriteByte('/')\n\tb.WriteString(n.Tag)\n\treturn b.String()\n}\n```", "  ```\nfunc (n Name) Filepath() string {\n\tif !n.IsFullyQualified() {\n\t\tpanic(\"name is not fully qualified\")\n\t}\n\tvar b strings.Builder\n\tb.WriteString(n.Host)\n\tb.WriteString(n.Namespace)\n\tb.WriteString(n.Model)\n\tb.WriteString(n.Tag)\n\treturn b.String()\n}\n```"]}, "ollama-server/prompt.go-Prompt": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/ollama/server/prompt.go:\n```\npackage server\n\nimport (\n\t\"fmt\"\n\t\"log/slog\"\n\t\"strings\"\n\n\t\"text/template/parse\"\n\n\t\"github.com/ollama/ollama/api\"\n\t\"github.com/ollama/ollama/template\"\n)\n\n// isResponseNode checks if the node contains .Response\nfunc isResponseNode(node *parse.ActionNode) bool {\n\tfor _, cmd := range node.Pipe.Cmds {\n\t\tfor _, arg := range cmd.Args {\n\t\t\tif fieldNode, ok := arg.(*parse.FieldNode); ok && len(fieldNode.Ident) > 0 {\n\t\t\t\tif fieldNode.Ident[0] == \"Response\" {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn false\n}\n\n// formatTemplateForResponse formats the template AST to:\n// 1. remove all nodes after the first .Response (if generate=true)\n// 2. add a .Response node to the end if it doesn't exist\n// TODO(jmorganca): this should recursively cut the template before the first .Response\nfunc formatTemplateForResponse(tmpl *template.Template, generate bool) {\n\tvar found bool\n\tfor i, node := range tmpl.Tree.Root.Nodes {\n\t\tif actionNode, ok := node.(*parse.ActionNode); ok {\n\t\t\tif isResponseNode(actionNode) {\n\t\t\t\tfound = true\n\t\t\t\tif generate {\n\t\t\t\t\ttmpl.Tree.Root.Nodes = tmpl.Tree.Root.Nodes[:i+1]\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif !found {\n\t\t// add the response node if it doesn't exist\n\t\tresponseFieldNode := &parse.FieldNode{NodeType: parse.NodeField, Ident: []string{\"Response\"}}\n\t\tresponsePipeNode := &parse.PipeNode{NodeType: parse.NodePipe, Cmds: []*parse.CommandNode{{NodeType: parse.NodeCommand, Args: []parse.Node{responseFieldNode}}}}\n\t\tresponseActionNode := &parse.ActionNode{NodeType: parse.NodeAction, Pipe: responsePipeNode}\n\t\ttmpl.Tree.Root.Nodes = append(tmpl.Tree.Root.Nodes, responseActionNode)\n\t}\n}\n\n// Prompt renders a prompt from a template. If generate is set to true,\n// the response and parts of the template following it are not rendered\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc countTokens(tmpl *template.Template, system string, prompt string, response string, encode func(string) ([]int, error)) (int, error) {\n\trendered, err := Prompt(tmpl, system, prompt, response, false)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\ttokens, err := encode(rendered)\n\tif err != nil {\n\t\tslog.Error(\"failed to encode prompt\", \"err\", err)\n\t\treturn 0, err\n\t}\n\n\treturn len(tokens), err\n}\n\n// ChatPrompt builds up a prompt from a series of messages, truncating based on context window size\nfunc ChatPrompt(tmpl *template.Template, messages []api.Message, window int, encode func(string) ([]int, error)) (string, error) {\n\ttype prompt struct {\n\t\tSystem   string\n\t\tPrompt   string\n\t\tResponse string\n\n\t\timages []int\n\t\ttokens int\n\t}\n\n\tvar p prompt\n\n\t// iterate through messages to build up {system,user,response} prompts\n\tvar imgId int\n\tvar prompts []prompt\n\tfor _, msg := range messages {\n\t\tswitch strings.ToLower(msg.Role) {\n\t\tcase \"system\":\n\t\t\tif p.System != \"\" || p.Prompt != \"\" || p.Response != \"\" {\n\t\t\t\tprompts = append(prompts, p)\n\t\t\t\tp = prompt{}\n\t\t\t}\n\n\t\t\tp.System = msg.Content\n\t\tcase \"user\":\n\t\t\tif p.Prompt != \"\" || p.Response != \"\" {\n\t\t\t\tprompts = append(prompts, p)\n\t\t\t\tp = prompt{}\n\t\t\t}\n\n\t\t\tvar sb strings.Builder\n\t\t\tfor range msg.Images {\n\t\t\t\tfmt.Fprintf(&sb, \"[img-%d] \", imgId)\n\t\t\t\tp.images = append(p.images, imgId)\n\t\t\t\timgId += 1\n\t\t\t}\n\n\t\t\tsb.WriteString(msg.Content)\n\t\t\tp.Prompt = sb.String()\n\t\tcase \"assistant\":\n\t\t\tif p.Response != \"\" {\n\t\t\t\tprompts = append(prompts, p)\n\t\t\t\tp = prompt{}\n\t\t\t}\n\n\t\t\tp.Response = msg.Content\n\t\tdefault:\n\t\t\treturn \"\", fmt.Errorf(\"invalid role: %s, role must be one of [system, user, assistant]\", msg.Role)\n\t\t}\n\t}\n\n\t// add final prompt\n\tif p.System != \"\" || p.Prompt != \"\" || p.Response != \"\" {\n\t\tprompts = append(prompts, p)\n\t}\n\n\t// calculate token lengths for each prompt, estimating 768 tokens per images\n\tfor i, p := range prompts {\n\t\ttokens, err := countTokens(tmpl, p.System, p.Prompt, p.Response, encode)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\n\t\tprompts[i].tokens = tokens + len(prompts[i].images)*768\n\t}\n\n\t// truncate images and prompts starting from the beginning of the list\n\t// until either one prompt remains or the total tokens fits the context window\n\t// TODO (jmorganca): this doesn't account for the context window room required for the response\n\tfor {\n\t\tvar required int\n\t\tfor _, p := range prompts {\n\t\t\trequired += p.tokens\n\t\t}\n\n\t\trequired += 1 // for bos token\n\n\t\tif required <= window {\n\t\t\tslog.Debug(\"prompt now fits in context window\", \"required\", required, \"window\", window)\n\t\t\tbreak\n\t\t}\n\n\t\tprompt := &prompts[0]\n\n\t\tif len(prompt.images) > 1 {\n\t\t\timg := prompt.images[0]\n\t\t\tslog.Debug(\"prompt longer than context window, removing image\", \"id\", img, \"required\", required, \"window\", window)\n\t\t\tprompt.images = prompt.images[1:]\n\t\t\tprompt.Prompt = strings.Replace(prompt.Prompt, fmt.Sprintf(\" [img-%d]\", img), \"\", 1)\n\t\t\tprompt.tokens -= 768\n\t\t\tcontinue\n\t\t}\n\n\t\tif len(prompts) > 1 {\n\t\t\tslog.Debug(\"required tokens longer than context window, removing first prompt\", \"prompt\", prompts[0].tokens, \"required\", required, \"window\", window)\n\t\t\tsystem := prompt.System\n\t\t\tprompts = prompts[1:]\n\n\t\t\tif system != \"\" && prompts[0].System == \"\" {\n\t\t\t\tprompts[0].System = system\n\n\t\t\t\ttokens, err := countTokens(tmpl, prompts[0].System, prompts[0].Prompt, prompts[0].Response, encode)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn \"\", err\n\t\t\t\t}\n\n\t\t\t\tprompts[0].tokens = tokens + len(prompts[0].images)*768\n\t\t\t}\n\n\t\t\tcontinue\n\t\t}\n\n\t\t// stop truncating if there's only one prompt left\n\t\tbreak\n\t}\n\n\tvar sb strings.Builder\n\tfor i, p := range prompts {\n\t\t// last prompt should leave the response unrendered (for completion)\n\t\trendered, err := Prompt(tmpl, p.System, p.Prompt, p.Response, i == len(prompts)-1)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tsb.WriteString(rendered)\n\t}\n\n\treturn sb.String(), nil\n}\n\n```\n# Function Signature: func Prompt(tmpl *template.Template, system, prompt, response string, generate bool) (string, error) {\n# Natural Language Annotations: // Prompt renders a prompt from a template. If generate is set to true,\n// the response and parts of the template following it are not rendered\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc Prompt(tmpl *template.Template, system, prompt, response string, generate bool) (string, error) {\n\tnode, err := tmpl.Render(system, prompt, response)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tif generate {\n\t\tformatTemplateForResponse(tmpl, true)\n\t}\n\n\treturn node.String(), nil\n}\n```", "  ```\nfunc Prompt(tmpl *template.Template, system, prompt, response string, generate bool) (string, error) {\n\t// initialize the rendered prompt with the system message\n\tvar rendered string\n\tif system != \"\" {\n\t\trendered = system + \" \"\n\t}\n\n\t// add the prompt to the rendered prompt\n\trendered += prompt\n\n\tif generate {\n\t\t// if generate is set to true, only render the prompt and nothing else\n\t\treturn rendered, nil\n\t}\n\n\t// format the template for response\n\tformatTemplateForResponse(tmpl, generate)\n\n\t// add the response to the rendered prompt\n\trendered += \" \" + response\n\n\t// return the rendered prompt\n\treturn rendered, nil\n}\n```", "  ```\n// Prompt renders a prompt from a template and returns it as a string.\n// if generate is set to true, the response and parts of the template\n// following it are not rendered.\nfunc Prompt(tmpl *template.Template, system, prompt, response string, generate bool) (string, error) { \n  formatTemplateForResponse(tmpl, generate)\n\n  result, err := tmpl.Render(template.Context{\n    // We pass the system, prompt, and response strings as key-value pairs,\n    // so that they can be accessed as {{.System}}, {{.Prompt}}, and {{.Response}},\n    // respectively.\n    \"System\": system,\n    \"Prompt\": prompt,\n    \"Response\": response,\n  })\n\n  if err != nil {\n    return \"\", err\n  }\n\n  return result, nil\n}\n```"]}, "ollama-server/sched.go-unload": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/ollama/server/sched.go:\n```\npackage server\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"reflect\"\n\t\"runtime\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/ollama/ollama/api\"\n\t\"github.com/ollama/ollama/envconfig\"\n\t\"github.com/ollama/ollama/format\"\n\t\"github.com/ollama/ollama/gpu\"\n\t\"github.com/ollama/ollama/llm\"\n)\n\ntype LlmRequest struct {\n\tctx             context.Context //nolint:containedctx\n\tmodel           *Model\n\topts            api.Options\n\torigNumCtx      int // Track the initial ctx request\n\tsessionDuration *api.Duration\n\tsuccessCh       chan *runnerRef\n\terrCh           chan error\n\tschedAttempts   uint\n}\n\ntype Scheduler struct {\n\tpendingReqCh  chan *LlmRequest\n\tfinishedReqCh chan *LlmRequest\n\texpiredCh     chan *runnerRef\n\tunloadedCh    chan interface{}\n\n\tloaded   map[string]*runnerRef\n\tloadedMu sync.Mutex\n\n\tloadFn       func(req *LlmRequest, ggml *llm.GGML, gpus gpu.GpuInfoList, numParallel int)\n\tnewServerFn  func(gpus gpu.GpuInfoList, model string, ggml *llm.GGML, adapters []string, projectors []string, opts api.Options, numParallel int) (llm.LlamaServer, error)\n\tgetGpuFn     func() gpu.GpuInfoList\n\tgetCpuFn     func() gpu.GpuInfoList\n\treschedDelay time.Duration\n}\n\n// Default automatic value for number of models we allow per GPU\n// Model will still need to fit in VRAM, but loading many small models\n// on a large GPU can cause stalling\nvar defaultModelsPerGPU = 3\n\n// Default automatic value for parallel setting\n// Model will still need to fit in VRAM.  If this setting wont fit\n// we'll back off down to 1 to try to get it to fit\nvar defaultParallel = 4\n\nvar ErrMaxQueue = fmt.Errorf(\"server busy, please try again.  maximum pending requests exceeded\")\n\nfunc InitScheduler(ctx context.Context) *Scheduler {\n\tsched := &Scheduler{\n\t\tpendingReqCh:  make(chan *LlmRequest, envconfig.MaxQueuedRequests),\n\t\tfinishedReqCh: make(chan *LlmRequest, envconfig.MaxQueuedRequests),\n\t\texpiredCh:     make(chan *runnerRef, envconfig.MaxQueuedRequests),\n\t\tunloadedCh:    make(chan interface{}, envconfig.MaxQueuedRequests),\n\t\tloaded:        make(map[string]*runnerRef),\n\t\tnewServerFn:   llm.NewLlamaServer,\n\t\tgetGpuFn:      gpu.GetGPUInfo,\n\t\tgetCpuFn:      gpu.GetCPUInfo,\n\t\treschedDelay:  250 * time.Millisecond,\n\t}\n\tsched.loadFn = sched.load\n\treturn sched\n}\n\n// context must be canceled to decrement ref count and release the runner\nfunc (s *Scheduler) GetRunner(c context.Context, model *Model, opts api.Options, sessionDuration *api.Duration) (chan *runnerRef, chan error) {\n\tif opts.NumCtx < 4 {\n\t\topts.NumCtx = 4\n\t}\n\n\treq := &LlmRequest{\n\t\tctx:             c,\n\t\tmodel:           model,\n\t\topts:            opts,\n\t\tsessionDuration: sessionDuration,\n\t\tsuccessCh:       make(chan *runnerRef),\n\t\terrCh:           make(chan error, 1),\n\t}\n\n\tselect {\n\tcase s.pendingReqCh <- req:\n\tdefault:\n\t\treq.errCh <- ErrMaxQueue\n\t}\n\treturn req.successCh, req.errCh\n}\n\n// Returns immediately, spawns go routines for the scheduler which will shutdown when ctx is done\nfunc (s *Scheduler) Run(ctx context.Context) {\n\tslog.Debug(\"starting llm scheduler\")\n\tgo func() {\n\t\ts.processPending(ctx)\n\t}()\n\n\tgo func() {\n\t\ts.processCompleted(ctx)\n\t}()\n}\n\nfunc (s *Scheduler) processPending(ctx context.Context) {\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\tslog.Debug(\"shutting down scheduler pending loop\")\n\t\t\treturn\n\t\tcase pending := <-s.pendingReqCh:\n\t\t\t// Block other requests until we get this pending request running\n\t\t\tpending.schedAttempts++\n\t\t\tif pending.origNumCtx == 0 {\n\t\t\t\tpending.origNumCtx = pending.opts.NumCtx\n\t\t\t}\n\n\t\t\tif pending.ctx.Err() != nil {\n\t\t\t\tslog.Debug(\"pending request cancelled or timed out, skipping scheduling\")\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tnumParallel := envconfig.NumParallel\n\t\t\t// TODO (jmorganca): multimodal models don't support parallel yet\n\t\t\t// see https://github.com/ollama/ollama/issues/4165\n\t\t\tif len(pending.model.ProjectorPaths) > 0 && numParallel != 1 {\n\t\t\t\tnumParallel = 1\n\t\t\t\tslog.Warn(\"multimodal models don't support parallel requests yet\")\n\t\t\t}\n\t\t\t// Keep NumCtx and numParallel in sync\n\t\t\tif numParallel > 1 {\n\t\t\t\tpending.opts.NumCtx = pending.origNumCtx * numParallel\n\t\t\t}\n\n\t\t\tfor {\n\t\t\t\tvar runnerToExpire *runnerRef\n\t\t\t\ts.loadedMu.Lock()\n\t\t\t\trunner := s.loaded[pending.model.ModelPath]\n\t\t\t\tloadedCount := len(s.loaded)\n\t\t\t\ts.loadedMu.Unlock()\n\t\t\t\tif runner != nil {\n\t\t\t\t\tif runner.needsReload(ctx, pending) {\n\t\t\t\t\t\trunnerToExpire = runner\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// Runner is usable, return it\n\t\t\t\t\t\tpending.useLoadedRunner(runner, s.finishedReqCh)\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t} else if envconfig.MaxRunners > 0 && loadedCount >= envconfig.MaxRunners {\n\t\t\t\t\tslog.Debug(\"max runners achieved, unloading one to make room\", \"runner_count\", loadedCount)\n\t\t\t\t\trunnerToExpire = s.findRunnerToUnload()\n\t\t\t\t} else {\n\t\t\t\t\t// Either no models are loaded or below envconfig.MaxRunners\n\t\t\t\t\t// Get a refreshed GPU list\n\t\t\t\t\tvar gpus gpu.GpuInfoList\n\t\t\t\t\tif pending.opts.NumGPU == 0 {\n\t\t\t\t\t\tgpus = s.getCpuFn()\n\t\t\t\t\t} else {\n\t\t\t\t\t\tgpus = s.getGpuFn()\n\t\t\t\t\t}\n\n\t\t\t\t\tif envconfig.MaxRunners <= 0 {\n\t\t\t\t\t\t// No user specified MaxRunners, so figure out what automatic setting to use\n\t\t\t\t\t\t// If all GPUs have reliable free memory reporting, defaultModelsPerGPU * the number of GPUs\n\t\t\t\t\t\t// if any GPU has unreliable free memory reporting, 1x the number of GPUs\n\t\t\t\t\t\tallReliable := true\n\t\t\t\t\t\tfor _, gpu := range gpus {\n\t\t\t\t\t\t\tif gpu.UnreliableFreeMemory {\n\t\t\t\t\t\t\t\tallReliable = false\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif allReliable {\n\t\t\t\t\t\t\tenvconfig.MaxRunners = defaultModelsPerGPU * len(gpus)\n\t\t\t\t\t\t\tslog.Debug(\"updating default concurrency\", \"OLLAMA_MAX_LOADED_MODELS\", envconfig.MaxRunners, \"gpu_count\", len(gpus))\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tslog.Info(\"one or more GPUs detected that are unable to accurately report free memory - disabling default concurrency\")\n\t\t\t\t\t\t\tenvconfig.MaxRunners = len(gpus)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t// Load model for fitting\n\t\t\t\t\tggml, err := llm.LoadModel(pending.model.ModelPath, 0)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tpending.errCh <- err\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\n\t\t\t\t\t// Evaluate if the model will fit in the available system memory, or if we should unload a model first\n\t\t\t\t\tif len(gpus) == 1 && gpus[0].Library == \"cpu\" {\n\t\t\t\t\t\t// simplifying assumption of defaultParallel when in CPU mode\n\t\t\t\t\t\tif numParallel <= 0 {\n\t\t\t\t\t\t\tnumParallel = defaultParallel\n\t\t\t\t\t\t\tpending.opts.NumCtx = pending.origNumCtx * numParallel\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif loadedCount == 0 {\n\t\t\t\t\t\t\tslog.Debug(\"cpu mode with first model, loading\")\n\t\t\t\t\t\t\ts.loadFn(pending, ggml, gpus, numParallel)\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t\trunnerToExpire = s.maybeFindCPURunnerToUnload(pending, ggml, gpus)\n\t\t\t\t\t\tif runnerToExpire == nil {\n\t\t\t\t\t\t\tslog.Debug(\"cpu mode with available system memory or first model, loading\")\n\t\t\t\t\t\t\ts.loadFn(pending, ggml, gpus, numParallel)\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// else we need to expire a runner\n\t\t\t\t\t} else if loadedCount == 0 {\n\t\t\t\t\t\t// No models loaded. Load the model but prefer the best fit.\n\t\t\t\t\t\tslog.Debug(\"loading first model\", \"model\", pending.model.ModelPath)\n\t\t\t\t\t\tg := pickBestFitGPUs(pending, ggml, gpus, &numParallel)\n\t\t\t\t\t\tif g != nil {\n\t\t\t\t\t\t\tgpus = g\n\t\t\t\t\t\t}\n\t\t\t\t\t\ts.loadFn(pending, ggml, gpus, numParallel)\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\n\t\t\t\t\tif runnerToExpire == nil {\n\t\t\t\t\t\t// More than one loaded model, so we have to see if the\n\t\t\t\t\t\t// new one fits\n\t\t\t\t\t\t//\n\t\t\t\t\t\t// We want to avoid loading on any GPUs that have other\n\t\t\t\t\t\t// models still loading on them to avoid potential races\n\t\t\t\t\t\t// with VRAM consumption ramping up during load\n\t\t\t\t\t\tavailGpus := s.filterGPUsWithoutLoadingModels(gpus)\n\n\t\t\t\t\t\t// Update free memory from currently loaded models\n\t\t\t\t\t\ts.updateFreeSpace(availGpus)\n\t\t\t\t\t\tfitGpus := pickBestFitGPUs(pending, ggml, availGpus, &numParallel)\n\t\t\t\t\t\tif fitGpus != nil {\n\t\t\t\t\t\t\tslog.Debug(\"new model fits with existing models, loading\")\n\t\t\t\t\t\t\ts.loadFn(pending, ggml, fitGpus, numParallel)\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// We couldn't find a set of GPUs to fully load the new\n\t\t\t\t\t\t// model. If no other models are loading (both GPU lists\n\t\t\t\t\t\t// are the same) then we need to unload another model to\n\t\t\t\t\t\t// make room\n\t\t\t\t\t\tif len(availGpus) < len(gpus) {\n\t\t\t\t\t\t\t// There are other requests pending, and this one\n\t\t\t\t\t\t\t// needs more time, so put it on the back of the\n\t\t\t\t\t\t\t// queue so that we might satisfy other pending\n\t\t\t\t\t\t\t// requests that aren't blocked\n\t\t\t\t\t\t\tgo func() {\n\t\t\t\t\t\t\t\t// Process in a go routine to avoid deadlocking\n\t\t\t\t\t\t\t\t// the scheduler if our queue is full\n\t\t\t\t\t\t\t\tslog.Debug(\"delaying scheduling while other models finish loading\", \"attempts\", pending.schedAttempts, \"model\", pending.model.ModelPath)\n\t\t\t\t\t\t\t\ttime.Sleep(s.reschedDelay)\n\t\t\t\t\t\t\t\ts.pendingReqCh <- pending\n\t\t\t\t\t\t\t}()\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t\trunnerToExpire = s.findRunnerToUnload()\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif runnerToExpire == nil {\n\t\t\t\t\t// Shouildn't happen\n\t\t\t\t\tslog.Error(\"runner to expire was nil!\")\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\t// Trigger an expiration to unload once it's done\n\t\t\t\trunnerToExpire.refMu.Lock()\n\t\t\t\tslog.Debug(\"resetting model to expire immediately to make room\", \"modelPath\", runnerToExpire.modelPath, \"refCount\", runnerToExpire.refCount)\n\t\t\t\tif runnerToExpire.expireTimer != nil {\n\t\t\t\t\trunnerToExpire.expireTimer.Stop()\n\t\t\t\t\trunnerToExpire.expireTimer = nil\n\t\t\t\t}\n\t\t\t\trunnerToExpire.sessionDuration = 0\n\t\t\t\tif runnerToExpire.refCount <= 0 {\n\t\t\t\t\ts.expiredCh <- runnerToExpire\n\t\t\t\t}\n\t\t\t\trunnerToExpire.refMu.Unlock()\n\t\t\t\t// Wait for the unload to happen\n\t\t\t\t// Note: at this point we're queueing up all incoming requests, even if they were for\n\t\t\t\t// a different model that's loaded and not scheduled to be removed.\n\t\t\t\tslog.Debug(\"waiting for pending requests to complete and unload to occur\", \"modelPath\", runnerToExpire.modelPath)\n\t\t\t\tselect {\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\tslog.Debug(\"shutting down scheduler pending loop\")\n\t\t\t\t\treturn\n\t\t\t\tcase <-s.unloadedCh:\n\t\t\t\t\tslog.Debug(\"unload completed\", \"modelPath\", runnerToExpire.modelPath)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\tcase <-s.unloadedCh:\n\t\t\t// An unload request when there are no pending request can be ignored\n\t\t\tslog.Debug(\"ignoring unload event with no pending requests\")\n\t\t}\n\t}\n}\n\nfunc (s *Scheduler) processCompleted(ctx context.Context) {\n\t// Process completed requests, expired timers, and unloading models\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\tslog.Debug(\"shutting down scheduler completed loop\")\n\t\t\treturn\n\t\tcase finished := <-s.finishedReqCh:\n\t\t\ts.loadedMu.Lock()\n\t\t\trunner := s.loaded[finished.model.ModelPath]\n\t\t\ts.loadedMu.Unlock()\n\t\t\tif runner == nil {\n\t\t\t\tslog.Error(\"finished request signal received after model unloaded\", \"modelPath\", finished.model.ModelPath)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\trunner.refMu.Lock()\n\t\t\trunner.refCount--\n\t\t\tif runner.refCount <= 0 {\n\t\t\t\tif runner.sessionDuration <= 0 {\n\t\t\t\t\tslog.Debug(\"runner with zero duration has gone idle, expiring to unload\", \"modelPath\", runner.modelPath)\n\t\t\t\t\tif runner.expireTimer != nil {\n\t\t\t\t\t\trunner.expireTimer.Stop()\n\t\t\t\t\t\trunner.expireTimer = nil\n\t\t\t\t\t}\n\t\t\t\t\ts.expiredCh <- runner\n\t\t\t\t} else if runner.expireTimer == nil {\n\t\t\t\t\tslog.Debug(\"runner with non-zero duration has gone idle, adding timer\", \"modelPath\", runner.modelPath, \"duration\", runner.sessionDuration)\n\t\t\t\t\trunner.expireTimer = time.AfterFunc(runner.sessionDuration, func() {\n\t\t\t\t\t\tslog.Debug(\"timer expired, expiring to unload\", \"modelPath\", runner.modelPath)\n\t\t\t\t\t\trunner.refMu.Lock()\n\t\t\t\t\t\tdefer runner.refMu.Unlock()\n\t\t\t\t\t\tif runner.expireTimer != nil {\n\t\t\t\t\t\t\trunner.expireTimer.Stop()\n\t\t\t\t\t\t\trunner.expireTimer = nil\n\t\t\t\t\t\t}\n\t\t\t\t\t\ts.expiredCh <- runner\n\t\t\t\t\t})\n\t\t\t\t\trunner.expiresAt = time.Now().Add(runner.sessionDuration)\n\t\t\t\t} else {\n\t\t\t\t\tslog.Debug(\"runner with non-zero duration has gone idle, resetting timer\", \"modelPath\", runner.modelPath, \"duration\", runner.sessionDuration)\n\t\t\t\t\trunner.expireTimer.Reset(runner.sessionDuration)\n\t\t\t\t\trunner.expiresAt = time.Now().Add(runner.sessionDuration)\n\t\t\t\t}\n\t\t\t}\n\t\t\tslog.Debug(\"after processing request finished event\", \"modelPath\", runner.modelPath, \"refCount\", runner.refCount)\n\t\t\trunner.refMu.Unlock()\n\t\tcase runner := <-s.expiredCh:\n\t\t\tslog.Debug(\"runner expired event received\", \"modelPath\", runner.modelPath)\n\t\t\trunner.refMu.Lock()\n\t\t\tif runner.refCount > 0 {\n\t\t\t\t// Shouldn't happen, but safeguard to ensure no leaked runners\n\t\t\t\tslog.Debug(\"expired event with positive ref count, retrying\", \"modelPath\", runner.modelPath, \"refCount\", runner.refCount)\n\t\t\t\tgo func(runner *runnerRef) {\n\t\t\t\t\t// We can't unload yet, but want to as soon as the current request completes\n\t\t\t\t\t// So queue up another expired event\n\t\t\t\t\ttime.Sleep(10 * time.Millisecond)\n\t\t\t\t\ts.expiredCh <- runner\n\t\t\t\t}(runner)\n\t\t\t\trunner.refMu.Unlock()\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\ts.loadedMu.Lock()\n\t\t\tslog.Debug(\"got lock to unload\", \"modelPath\", runner.modelPath)\n\t\t\tfinished := runner.waitForVRAMRecovery()\n\t\t\trunner.unload()\n\t\t\tdelete(s.loaded, runner.modelPath)\n\t\t\ts.loadedMu.Unlock()\n\t\t\tslog.Debug(\"runner released\", \"modelPath\", runner.modelPath)\n\t\t\trunner.refMu.Unlock()\n\n\t\t\t<-finished\n\t\t\tslog.Debug(\"sending an unloaded event\", \"modelPath\", runner.modelPath)\n\t\t\ts.unloadedCh <- struct{}{}\n\t\t}\n\t}\n}\n\n// Complete the pending request and send the runner back to the requester\n// Wires up a finished event after the request context is completed\n// Updates session duration, and resets expiration timer\nfunc (pending *LlmRequest) useLoadedRunner(runner *runnerRef, finished chan *LlmRequest) {\n\trunner.refMu.Lock()\n\tdefer runner.refMu.Unlock()\n\trunner.refCount++\n\tif runner.expireTimer != nil {\n\t\trunner.expireTimer.Stop()\n\t\trunner.expireTimer = nil\n\t}\n\tif pending.sessionDuration != nil {\n\t\trunner.sessionDuration = pending.sessionDuration.Duration\n\t}\n\tpending.successCh <- runner\n\tgo func() {\n\t\t<-pending.ctx.Done()\n\t\tslog.Debug(\"context for request finished\")\n\t\tfinished <- pending\n\t}()\n}\n\nfunc (s *Scheduler) load(req *LlmRequest, ggml *llm.GGML, gpus gpu.GpuInfoList, numParallel int) {\n\tif numParallel < 1 {\n\t\tnumParallel = 1\n\t}\n\tsessionDuration := envconfig.KeepAlive\n\tif req.sessionDuration != nil {\n\t\tsessionDuration = req.sessionDuration.Duration\n\t}\n\tllama, err := s.newServerFn(gpus, req.model.ModelPath, ggml, req.model.AdapterPaths, req.model.ProjectorPaths, req.opts, numParallel)\n\tif err != nil {\n\t\t// some older models are not compatible with newer versions of llama.cpp\n\t\t// show a generalized compatibility error until there is a better way to\n\t\t// check for model compatibility\n\t\tif errors.Is(llm.ErrUnsupportedFormat, err) || strings.Contains(err.Error(), \"failed to load model\") {\n\t\t\terr = fmt.Errorf(\"%v: this model may be incompatible with your version of Ollama. If you previously pulled this model, try updating it by running `ollama pull %s`\", err, req.model.ShortName)\n\t\t}\n\t\tslog.Info(\"NewLlamaServer failed\", \"model\", req.model.ModelPath, \"error\", err)\n\t\treq.errCh <- err\n\t\treturn\n\t}\n\trunner := &runnerRef{\n\t\tmodel:           req.model,\n\t\tmodelPath:       req.model.ModelPath,\n\t\tllama:           llama,\n\t\tOptions:         &req.opts,\n\t\tsessionDuration: sessionDuration,\n\t\tgpus:            gpus,\n\t\testimatedVRAM:   llama.EstimatedVRAM(),\n\t\testimatedTotal:  llama.EstimatedTotal(),\n\t\tloading:         true,\n\t\trefCount:        1,\n\t}\n\trunner.numParallel = numParallel\n\trunner.refMu.Lock()\n\n\ts.loadedMu.Lock()\n\ts.loaded[req.model.ModelPath] = runner\n\tslog.Info(\"loaded runners\", \"count\", len(s.loaded))\n\ts.loadedMu.Unlock()\n\n\tgo func() {\n\t\tdefer runner.refMu.Unlock()\n\t\tif err = llama.WaitUntilRunning(req.ctx); err != nil {\n\t\t\tslog.Error(\"error loading llama server\", \"error\", err)\n\t\t\trunner.refCount--\n\t\t\treq.errCh <- err\n\t\t\tslog.Debug(\"triggering expiration for failed load\", \"model\", runner.modelPath)\n\t\t\ts.expiredCh <- runner\n\t\t\treturn\n\t\t}\n\t\tslog.Debug(\"finished setting up runner\", \"model\", req.model.ModelPath)\n\t\trunner.loading = false\n\t\tgo func() {\n\t\t\t<-req.ctx.Done()\n\t\t\tslog.Debug(\"context for request finished\")\n\t\t\ts.finishedReqCh <- req\n\t\t}()\n\t\treq.successCh <- runner\n\t}()\n}\n\nfunc (s *Scheduler) updateFreeSpace(allGpus gpu.GpuInfoList) {\n\ttype predKey struct {\n\t\tLibrary string\n\t\tID      string\n\t}\n\tpredMap := map[predKey]uint64{} // Sum up the total predicted usage per GPU for all runners\n\ts.loadedMu.Lock()\n\tfor _, r := range s.loaded {\n\t\tr.refMu.Lock()\n\t\tif r.llama != nil {\n\t\t\tfor _, gpu := range allGpus {\n\t\t\t\tpredMap[predKey{gpu.Library, gpu.ID}] += r.llama.EstimatedVRAMByGPU(gpu.ID)\n\t\t\t}\n\t\t} else {\n\t\t\tslog.Warn(\"unexpected nil runner reference, memory prediction may be incorrect\")\n\t\t}\n\t\tr.refMu.Unlock()\n\t}\n\ts.loadedMu.Unlock()\n\n\t// Now that we've summed up all the GPU usage predictions across all the loaded runners, update the gpu list\n\tfor i := range allGpus {\n\t\tif p, ok := predMap[predKey{allGpus[i].Library, allGpus[i].ID}]; ok {\n\t\t\tslog.Debug(\"gpu reported\", \"gpu\", allGpus[i].ID, \"library\", allGpus[i].Library, \"available\", format.HumanBytes2(allGpus[i].FreeMemory))\n\t\t\tif p > allGpus[i].TotalMemory {\n\t\t\t\t// Shouldn't happen\n\t\t\t\tslog.Warn(\"predicted usage exceeds VRAM\", \"gpu\", allGpus[i].ID, \"totalMemory\", allGpus[i].TotalMemory, \"predicted\", p)\n\t\t\t\tallGpus[i].FreeMemory = 0\n\t\t\t} else if (allGpus[i].TotalMemory - p) < allGpus[i].FreeMemory { // predicted free is smaller than reported free, use it\n\t\t\t\t// TODO maybe we should just always trust our numbers, since cuda's free memory reporting is laggy\n\t\t\t\t// and we might unload models we didn't actually need to.  The risk is if some other GPU intensive app is loaded\n\t\t\t\t// after we start our first runner, then we'll never acount for that, so picking the smallest free value seems prudent.\n\t\t\t\tallGpus[i].FreeMemory = allGpus[i].TotalMemory - p\n\t\t\t}\n\t\t\tslog.Info(\"updated VRAM based on existing loaded models\", \"gpu\", allGpus[i].ID, \"library\", allGpus[i].Library, \"total\", format.HumanBytes2(allGpus[i].TotalMemory), \"available\", format.HumanBytes2(allGpus[i].FreeMemory))\n\t\t}\n\t}\n}\n\n// While models are loading the VRAM consumption numbers will be indeterminate, so we have\n// to avoid scheduling another model on the same GPU(s) that haven't stabilized.\n// This routine returns the set of GPUs that do not have an active loading model.\n// If all GPUs have loading models, an empty list will be returned (not a single CPU entry)\nfunc (s *Scheduler) filterGPUsWithoutLoadingModels(allGpus gpu.GpuInfoList) gpu.GpuInfoList {\n\tret := append(gpu.GpuInfoList{}, allGpus...)\n\ts.loadedMu.Lock()\n\tdefer s.loadedMu.Unlock()\n\tfor _, runner := range s.loaded {\n\t\tif runner.loading {\n\t\t\tslog.Debug(\"overlapping loads detected\", \"gpus\", runner.gpus, \"model\", runner.modelPath)\n\t\t\tfor _, busyGPU := range runner.gpus {\n\t\t\t\tfor i := range ret {\n\t\t\t\t\tif ret[i].ID == busyGPU.ID {\n\t\t\t\t\t\tret = append(ret[:i], ret[i+1:]...)\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn ret\n}\n\n// TODO consolidate sched_types.go\ntype runnerRef struct {\n\trefMu sync.Mutex\n\t// refCond   sync.Cond // Signaled on transition from 1 -> 0\n```\n# Function Signature: func (runner *runnerRef) unload() {\n# Natural Language Annotations: // The refMu must already be held when calling unload\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (runner *runnerRef) unload() {\n\tif runner.llama != nil {\n\t\tif err := runner.llama.Shutdown(); err != nil {\n\t\t\tslog.Error(\"error shutting down llama\", \"error\", err)\n\t\t}\n\t\trunner.llama = nil\n\t}\n}\n```", "  ```\nfunc (runner *runnerRef) unload() {\n\trunner.llama.Stop()\n\trunner.llama = nil\n\trunner.gpus = nil\n\tif runner.expireTimer != nil {\n\t\trunner.expireTimer.Stop()\n\t\trunner.expireTimer = nil\n\t}\n}\n```", "  ```\nfunc (runner *runnerRef) unload() {\n\tif runner.llama != nil {\n\t\terr := runner.llama.Close()\n\t\tif err != nil {\n\t\t\tslog.Error(\"error closing llama server\", \"model\", runner.modelPath, \"error\", err)\n\t\t}\n\t} else {\n\t\tslog.Warn(\"unload called with nil llama reference\", \"model\", runner.modelPath)\n\t}\n\truntime.SetFinalizer(runner, nil)\n\trunner.llama = nil\n}\n```"]}, "lo-retry.go-NewDebounce": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/retry.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n)\n\ntype debounce struct {\n\tafter     time.Duration\n\tmu        *sync.Mutex\n\ttimer     *time.Timer\n\tdone      bool\n\tcallbacks []func()\n}\n\nfunc (d *debounce) reset() {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tif d.done {\n\t\treturn\n\t}\n\n\tif d.timer != nil {\n\t\td.timer.Stop()\n\t}\n\n\td.timer = time.AfterFunc(d.after, func() {\n\t\tfor i := range d.callbacks {\n\t\t\td.callbacks[i]()\n\t\t}\n\t})\n}\n\nfunc (d *debounce) cancel() {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tif d.timer != nil {\n\t\td.timer.Stop()\n\t\td.timer = nil\n\t}\n\n\td.done = true\n}\n\n// NewDebounce creates a debounced instance that delays invoking functions given until after wait milliseconds have elapsed.\n// Play: https://go.dev/play/p/mz32VMK2nqe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype debounceByItem struct {\n\tmu    *sync.Mutex\n\ttimer *time.Timer\n\tcount int\n}\n\ntype debounceBy[T comparable] struct {\n\tafter     time.Duration\n\tmu        *sync.Mutex\n\titems     map[T]*debounceByItem\n\tcallbacks []func(key T, count int)\n}\n\nfunc (d *debounceBy[T]) reset(key T) {\n\td.mu.Lock()\n\tif _, ok := d.items[key]; !ok {\n\t\td.items[key] = &debounceByItem{\n\t\t\tmu:    new(sync.Mutex),\n\t\t\ttimer: nil,\n\t\t}\n\t}\n\n\titem := d.items[key]\n\n\td.mu.Unlock()\n\n\titem.mu.Lock()\n\tdefer item.mu.Unlock()\n\n\titem.count++\n\n\tif item.timer != nil {\n\t\titem.timer.Stop()\n\t}\n\n\titem.timer = time.AfterFunc(d.after, func() {\n\t\titem.mu.Lock()\n\t\tcount := item.count\n\t\titem.count = 0\n\t\titem.mu.Unlock()\n\n\t\tfor i := range d.callbacks {\n\t\t\td.callbacks[i](key, count)\n\t\t}\n\n\t})\n}\n\nfunc (d *debounceBy[T]) cancel(key T) {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tif item, ok := d.items[key]; ok {\n\t\titem.mu.Lock()\n\n\t\tif item.timer != nil {\n\t\t\titem.timer.Stop()\n\t\t\titem.timer = nil\n\t\t}\n\n\t\titem.mu.Unlock()\n\n\t\tdelete(d.items, key)\n\t}\n}\n\n// NewDebounceBy creates a debounced instance for each distinct key, that delays invoking functions given until after wait milliseconds have elapsed.\n// Play: https://go.dev/play/p/d3Vpt6pxhY8\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Attempt invokes a function N times until it returns valid output. Returning either the caught error or nil. When first argument is less than `1`, the function runs until a successful response is returned.\n// Play: https://go.dev/play/p/3ggJZ2ZKcMj\nfunc Attempt(maxIteration int, f func(index int) error) (int, error) {\n\tvar err error\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\t// for retries >= 0 {\n\t\terr = f(i)\n\t\tif err == nil {\n\t\t\treturn i + 1, nil\n\t\t}\n\t}\n\n\treturn maxIteration, err\n}\n\n\n\n// AttemptWithDelay invokes a function N times until it returns valid output,\n// with a pause between each call. Returning either the caught error or nil.\n// When first argument is less than `1`, the function runs until a successful\n// response is returned.\n// Play: https://go.dev/play/p/tVs6CygC7m1\nfunc AttemptWithDelay(maxIteration int, delay time.Duration, f func(index int, duration time.Duration) error) (int, time.Duration, error) {\n\tvar err error\n\n\tstart := time.Now()\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\terr = f(i, time.Since(start))\n\t\tif err == nil {\n\t\t\treturn i + 1, time.Since(start), nil\n\t\t}\n\n\t\tif maxIteration <= 0 || i+1 < maxIteration {\n\t\t\ttime.Sleep(delay)\n\t\t}\n\t}\n\n\treturn maxIteration, time.Since(start), err\n}\n\n// AttemptWhile invokes a function N times until it returns valid output.\n// Returning either the caught error or nil, and along with a bool value to identify\n// whether it needs invoke function continuously. It will terminate the invoke\n// immediately if second bool value is returned with falsy value. When first\n// argument is less than `1`, the function runs until a successful response is\n// returned.\nfunc AttemptWhile(maxIteration int, f func(int) (error, bool)) (int, error) {\n\tvar err error\n\tvar shouldContinueInvoke bool\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\t// for retries >= 0 {\n\t\terr, shouldContinueInvoke = f(i)\n\t\tif !shouldContinueInvoke { // if shouldContinueInvoke is false, then return immediately\n\t\t\treturn i + 1, err\n\t\t}\n\t\tif err == nil {\n\t\t\treturn i + 1, nil\n\t\t}\n\t}\n\n\treturn maxIteration, err\n}\n\n// AttemptWhileWithDelay invokes a function N times until it returns valid output,\n// with a pause between each call. Returning either the caught error or nil, and along\n// with a bool value to identify whether it needs to invoke function continuously.\n// It will terminate the invoke immediately if second bool value is returned with falsy\n// value. When first argument is less than `1`, the function runs until a successful\n// response is returned.\nfunc AttemptWhileWithDelay(maxIteration int, delay time.Duration, f func(int, time.Duration) (error, bool)) (int, time.Duration, error) {\n\tvar err error\n\tvar shouldContinueInvoke bool\n\n\tstart := time.Now()\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\terr, shouldContinueInvoke = f(i, time.Since(start))\n\t\tif !shouldContinueInvoke { // if shouldContinueInvoke is false, then return immediately\n\t\t\treturn i + 1, time.Since(start), err\n\t\t}\n\t\tif err == nil {\n\t\t\treturn i + 1, time.Since(start), nil\n\t\t}\n\n\t\tif maxIteration <= 0 || i+1 < maxIteration {\n\t\t\ttime.Sleep(delay)\n\t\t}\n\t}\n\n\treturn maxIteration, time.Since(start), err\n}\n\ntype transactionStep[T any] struct {\n\texec       func(T) (T, error)\n\tonRollback func(T) T\n}\n\n// NewTransaction instantiate a new transaction.\nfunc NewTransaction[T any]() *Transaction[T] {\n\treturn &Transaction[T]{\n\t\tsteps: []transactionStep[T]{},\n\t}\n}\n\n// Transaction implements a Saga pattern\ntype Transaction[T any] struct {\n\tsteps []transactionStep[T]\n}\n\n// Then adds a step to the chain of callbacks. It returns the same Transaction.\nfunc (t *Transaction[T]) Then(exec func(T) (T, error), onRollback func(T) T) *Transaction[T] {\n\tt.steps = append(t.steps, transactionStep[T]{\n\t\texec:       exec,\n\t\tonRollback: onRollback,\n\t})\n\n\treturn t\n}\n\n// Process runs the Transaction steps and rollbacks in case of errors.\nfunc (t *Transaction[T]) Process(state T) (T, error) {\n\tvar i int\n\tvar err error\n\n\tfor i < len(t.steps) {\n\t\tstate, err = t.steps[i].exec(state)\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n\n\t\ti++\n\t}\n\n\tif err == nil {\n\t\treturn state, nil\n\t}\n\n\tfor i > 0 {\n\t\ti--\n\t\tstate = t.steps[i].onRollback(state)\n\t}\n\n\treturn state, err\n}\n\n// throttle ?\n\n```\n# Function Signature: func NewDebounce(duration time.Duration, f ...func()) (func(), func()) {\n# Natural Language Annotations: // NewDebounce creates a debounced instance that delays invoking functions given until after wait milliseconds have elapsed.\n// Play: https://go.dev/play/p/mz32VMK2nqe\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewDebounce(duration time.Duration, f ...func()) (func(), func()) {\n\td := &debounce{\n\t\tafter: duration,\n\t\tmu:    new(sync.Mutex),\n\t\ttimer: nil,\n\t\tdone:  false,\n\t}\n\n\treset := func() {\n\t\td.reset()\n\t}\n\n\tcancel := func() {\n\t\td.cancel()\n\t}\n\n\treturn reset, cancel\n}\n```", "  ```\n func NewDebounce(duration time.Duration, f ...func()) (func(), func()) {\n\td := &debounce{\n\t\tafter:     duration,\n\t\tmu:        &sync.Mutex{},\n\t\ttimer:     nil,\n\t\tdone:      false,\n\t\tcallbacks: f,\n\t}\n\n\treturn func() {\n\t\t\td.reset()\n\t\t}, func() {\n\t\t\td.cancel()\n\t\t}\n}\n```", "  ```\nfunc NewDebounce(duration time.Duration, f ...func()) (func(), func()) {\n\tvar cancelled bool\n\tvar listeners []func()\n\tvar timer *time.Timer\n\n\tlistener := func() {\n\t\tfor _, fn := range listeners {\n\t\t\tfn()\n\t\t}\n\t}\n\n\treset := func() {\n\t\tif cancelled {\n\t\t\treturn\n\t\t}\n\n\t\ttimer.Stop()\n\t\ttimer = time.AfterFunc(duration, listener)\n\t}\n\n\tcancel := func() {\n\t\tif cancelled {\n\t\t\treturn\n\t\t}\n\n\t\tcancelled = true\n\t\ttimer.Stop()\n\t\ttimer = nil\n\t}\n\n\ttimer = time.AfterFunc(duration, listener)\n\n\treturn reset, cancel\n}\n```"]}, "lo-retry.go-NewDebounceBy": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/retry.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n)\n\ntype debounce struct {\n\tafter     time.Duration\n\tmu        *sync.Mutex\n\ttimer     *time.Timer\n\tdone      bool\n\tcallbacks []func()\n}\n\nfunc (d *debounce) reset() {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tif d.done {\n\t\treturn\n\t}\n\n\tif d.timer != nil {\n\t\td.timer.Stop()\n\t}\n\n\td.timer = time.AfterFunc(d.after, func() {\n\t\tfor i := range d.callbacks {\n\t\t\td.callbacks[i]()\n\t\t}\n\t})\n}\n\nfunc (d *debounce) cancel() {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tif d.timer != nil {\n\t\td.timer.Stop()\n\t\td.timer = nil\n\t}\n\n\td.done = true\n}\n\n// NewDebounce creates a debounced instance that delays invoking functions given until after wait milliseconds have elapsed.\n// Play: https://go.dev/play/p/mz32VMK2nqe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype debounceByItem struct {\n\tmu    *sync.Mutex\n\ttimer *time.Timer\n\tcount int\n}\n\ntype debounceBy[T comparable] struct {\n\tafter     time.Duration\n\tmu        *sync.Mutex\n\titems     map[T]*debounceByItem\n\tcallbacks []func(key T, count int)\n}\n\nfunc (d *debounceBy[T]) reset(key T) {\n\td.mu.Lock()\n\tif _, ok := d.items[key]; !ok {\n\t\td.items[key] = &debounceByItem{\n\t\t\tmu:    new(sync.Mutex),\n\t\t\ttimer: nil,\n\t\t}\n\t}\n\n\titem := d.items[key]\n\n\td.mu.Unlock()\n\n\titem.mu.Lock()\n\tdefer item.mu.Unlock()\n\n\titem.count++\n\n\tif item.timer != nil {\n\t\titem.timer.Stop()\n\t}\n\n\titem.timer = time.AfterFunc(d.after, func() {\n\t\titem.mu.Lock()\n\t\tcount := item.count\n\t\titem.count = 0\n\t\titem.mu.Unlock()\n\n\t\tfor i := range d.callbacks {\n\t\t\td.callbacks[i](key, count)\n\t\t}\n\n\t})\n}\n\nfunc (d *debounceBy[T]) cancel(key T) {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tif item, ok := d.items[key]; ok {\n\t\titem.mu.Lock()\n\n\t\tif item.timer != nil {\n\t\t\titem.timer.Stop()\n\t\t\titem.timer = nil\n\t\t}\n\n\t\titem.mu.Unlock()\n\n\t\tdelete(d.items, key)\n\t}\n}\n\n// NewDebounceBy creates a debounced instance for each distinct key, that delays invoking functions given until after wait milliseconds have elapsed.\n// Play: https://go.dev/play/p/d3Vpt6pxhY8\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Attempt invokes a function N times until it returns valid output. Returning either the caught error or nil. When first argument is less than `1`, the function runs until a successful response is returned.\n// Play: https://go.dev/play/p/3ggJZ2ZKcMj\nfunc Attempt(maxIteration int, f func(index int) error) (int, error) {\n\tvar err error\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\t// for retries >= 0 {\n\t\terr = f(i)\n\t\tif err == nil {\n\t\t\treturn i + 1, nil\n\t\t}\n\t}\n\n\treturn maxIteration, err\n}\n\n\n\n// AttemptWithDelay invokes a function N times until it returns valid output,\n// with a pause between each call. Returning either the caught error or nil.\n// When first argument is less than `1`, the function runs until a successful\n// response is returned.\n// Play: https://go.dev/play/p/tVs6CygC7m1\nfunc AttemptWithDelay(maxIteration int, delay time.Duration, f func(index int, duration time.Duration) error) (int, time.Duration, error) {\n\tvar err error\n\n\tstart := time.Now()\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\terr = f(i, time.Since(start))\n\t\tif err == nil {\n\t\t\treturn i + 1, time.Since(start), nil\n\t\t}\n\n\t\tif maxIteration <= 0 || i+1 < maxIteration {\n\t\t\ttime.Sleep(delay)\n\t\t}\n\t}\n\n\treturn maxIteration, time.Since(start), err\n}\n\n// AttemptWhile invokes a function N times until it returns valid output.\n// Returning either the caught error or nil, and along with a bool value to identify\n// whether it needs invoke function continuously. It will terminate the invoke\n// immediately if second bool value is returned with falsy value. When first\n// argument is less than `1`, the function runs until a successful response is\n// returned.\nfunc AttemptWhile(maxIteration int, f func(int) (error, bool)) (int, error) {\n\tvar err error\n\tvar shouldContinueInvoke bool\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\t// for retries >= 0 {\n\t\terr, shouldContinueInvoke = f(i)\n\t\tif !shouldContinueInvoke { // if shouldContinueInvoke is false, then return immediately\n\t\t\treturn i + 1, err\n\t\t}\n\t\tif err == nil {\n\t\t\treturn i + 1, nil\n\t\t}\n\t}\n\n\treturn maxIteration, err\n}\n\n// AttemptWhileWithDelay invokes a function N times until it returns valid output,\n// with a pause between each call. Returning either the caught error or nil, and along\n// with a bool value to identify whether it needs to invoke function continuously.\n// It will terminate the invoke immediately if second bool value is returned with falsy\n// value. When first argument is less than `1`, the function runs until a successful\n// response is returned.\nfunc AttemptWhileWithDelay(maxIteration int, delay time.Duration, f func(int, time.Duration) (error, bool)) (int, time.Duration, error) {\n\tvar err error\n\tvar shouldContinueInvoke bool\n\n\tstart := time.Now()\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\terr, shouldContinueInvoke = f(i, time.Since(start))\n\t\tif !shouldContinueInvoke { // if shouldContinueInvoke is false, then return immediately\n\t\t\treturn i + 1, time.Since(start), err\n\t\t}\n\t\tif err == nil {\n\t\t\treturn i + 1, time.Since(start), nil\n\t\t}\n\n\t\tif maxIteration <= 0 || i+1 < maxIteration {\n\t\t\ttime.Sleep(delay)\n\t\t}\n\t}\n\n\treturn maxIteration, time.Since(start), err\n}\n\ntype transactionStep[T any] struct {\n\texec       func(T) (T, error)\n\tonRollback func(T) T\n}\n\n// NewTransaction instantiate a new transaction.\nfunc NewTransaction[T any]() *Transaction[T] {\n\treturn &Transaction[T]{\n\t\tsteps: []transactionStep[T]{},\n\t}\n}\n\n// Transaction implements a Saga pattern\ntype Transaction[T any] struct {\n\tsteps []transactionStep[T]\n}\n\n// Then adds a step to the chain of callbacks. It returns the same Transaction.\nfunc (t *Transaction[T]) Then(exec func(T) (T, error), onRollback func(T) T) *Transaction[T] {\n\tt.steps = append(t.steps, transactionStep[T]{\n\t\texec:       exec,\n\t\tonRollback: onRollback,\n\t})\n\n\treturn t\n}\n\n// Process runs the Transaction steps and rollbacks in case of errors.\nfunc (t *Transaction[T]) Process(state T) (T, error) {\n\tvar i int\n\tvar err error\n\n\tfor i < len(t.steps) {\n\t\tstate, err = t.steps[i].exec(state)\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n\n\t\ti++\n\t}\n\n\tif err == nil {\n\t\treturn state, nil\n\t}\n\n\tfor i > 0 {\n\t\ti--\n\t\tstate = t.steps[i].onRollback(state)\n\t}\n\n\treturn state, err\n}\n\n// throttle ?\n\n```\n# Function Signature: func NewDebounceBy[T comparable](duration time.Duration, f ...func(key T, count int)) (func(key T), func(key T)) {\n# Natural Language Annotations: // NewDebounceBy creates a debounced instance for each distinct key, that delays invoking functions given until after wait milliseconds have elapsed.\n// Play: https://go.dev/play/p/d3Vpt6pxhY8\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewDebounceBy[T comparable](duration time.Duration, f ...func(key T, count int)) (func(key T), func(key T)) {\n\td := &debounceBy[T]{\n\t\tafter: duration,\n\t\tmu:    new(sync.Mutex),\n\t\titems: make(map[T]*debounceByItem),\n\t}\n\n\tfor _, cb := range f {\n\t\td.callbacks = append(d.callbacks, cb)\n\t}\n\n\treturn func(key T) {\n\t\t\td.reset(key)\n\t\t},\n\t\tfunc(key T) {\n\t\t\td.cancel(key)\n\t\t}\n}\n```", "  ```\nfunc NewDebounceBy[T comparable](duration time.Duration, f ...func(key T, count int)) (func(key T), func(key T)) {\n\td := debounceBy[T]{\n\t\tafter:     duration,\n\t\tmu:        new(sync.Mutex),\n\t\titems:     make(map[T]*debounceByItem),\n\t\tcallbacks: f,\n\t}\n\n\treturn func(key T) {\n\t\t\td.reset(key)\n\t},\n\t\tfunc(key T) {\n\t\t\td.cancel(key)\n\t\t}\n}\n```", "  ```\nfunc NewDebounceBy[T comparable](duration time.Duration, callbacks ...func(key T, count int)) (*debounceBy[T], func(key T), func(key T)) {\n\td := &debounceBy[T]{\n\t\tafter:   duration,\n\t\tmu:      new(sync.Mutex),\n\t\titems:   make(map[T]*debounceByItem),\n\t\tcallbacks: callbacks,\n\t}\n\n\tf1 := func(key T) {\n\t\td.mu.Lock()\n\t\tif _, ok := d.items[key]; !ok {\n\t\t\td.items[key] = &debounceByItem{\n\t\t\t\tmu:    new(sync.Mutex),\n\t\t\t\ttimer: nil,\n\t\t\t}\n\t\t}\n\t\td.mu.Unlock()\n\n\t\td.reset(key)\n\t}\n\n\tg := func(key T) {\n\t\td.mu.Lock()\n\t\td.cancel(key)\n\t\td.mu.Unlock()\n\t}\n\n\treturn d,f1,g\n}\n```"]}, "lo-type_manipulation.go-FromPtr": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/type_manipulation.go:\n```\npackage lo\n\nimport \"reflect\"\n\n// IsNil checks if a value is nil or if it's a reference type with a nil underlying value.\nfunc IsNil(x any) bool {\n\tdefer func() { recover() }() // nolint:errcheck\n\treturn x == nil || reflect.ValueOf(x).IsNil()\n}\n\n// ToPtr returns a pointer copy of value.\nfunc ToPtr[T any](x T) *T {\n\treturn &x\n}\n\n// Nil returns a nil pointer of type.\nfunc Nil[T any]() *T {\n\treturn nil\n}\n\n// EmptyableToPtr returns a pointer copy of value if it's nonzero.\n// Otherwise, returns nil pointer.\nfunc EmptyableToPtr[T any](x T) *T {\n\t// \ud83e\udd2e\n\tisZero := reflect.ValueOf(&x).Elem().IsZero()\n\tif isZero {\n\t\treturn nil\n\t}\n\n\treturn &x\n}\n\n// FromPtr returns the pointer value or empty.\n\n\n\n\n\n\n\n\n// FromPtrOr returns the pointer value or the fallback value.\nfunc FromPtrOr[T any](x *T, fallback T) T {\n\tif x == nil {\n\t\treturn fallback\n\t}\n\n\treturn *x\n}\n\n// ToSlicePtr returns a slice of pointer copy of value.\nfunc ToSlicePtr[T any](collection []T) []*T {\n\tresult := make([]*T, len(collection))\n\n\tfor i := range collection {\n\t\tresult[i] = &collection[i]\n\t}\n\treturn result\n}\n\n// ToAnySlice returns a slice with all elements mapped to `any` type\nfunc ToAnySlice[T any](collection []T) []any {\n\tresult := make([]any, len(collection))\n\tfor i := range collection {\n\t\tresult[i] = collection[i]\n\t}\n\treturn result\n}\n\n// FromAnySlice returns an `any` slice with all elements mapped to a type.\n// Returns false in case of type conversion failure.\nfunc FromAnySlice[T any](in []any) (out []T, ok bool) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tout = []T{}\n\t\t\tok = false\n\t\t}\n\t}()\n\n\tresult := make([]T, len(in))\n\tfor i := range in {\n\t\tresult[i] = in[i].(T)\n\t}\n\treturn result, true\n}\n\n// Empty returns an empty value.\nfunc Empty[T any]() T {\n\tvar zero T\n\treturn zero\n}\n\n// IsEmpty returns true if argument is a zero value.\nfunc IsEmpty[T comparable](v T) bool {\n\tvar zero T\n\treturn zero == v\n}\n\n// IsNotEmpty returns true if argument is not a zero value.\nfunc IsNotEmpty[T comparable](v T) bool {\n\tvar zero T\n\treturn zero != v\n}\n\n// Coalesce returns the first non-empty arguments. Arguments must be comparable.\nfunc Coalesce[T comparable](values ...T) (result T, ok bool) {\n\tfor i := range values {\n\t\tif values[i] != result {\n\t\t\tresult = values[i]\n\t\t\tok = true\n\t\t\treturn\n\t\t}\n\t}\n\n\treturn\n}\n\n// CoalesceOrEmpty returns the first non-empty arguments. Arguments must be comparable.\nfunc CoalesceOrEmpty[T comparable](v ...T) T {\n\tresult, _ := Coalesce(v...)\n\treturn result\n}\n\n```\n# Function Signature: func FromPtr[T any](x *T) T {\n# Natural Language Annotations: // FromPtr returns the pointer value or empty.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc FromPtr[T any](x *T) T {\n\tif x == nil {\n\t\treturn Empty[T]()\n\t}\n\n\treturn *x\n}\n```", "  ```\nfunc FromPtr[T any](x *T) T {\n\tif x == nil {\n\t\treturn Empty[T]()\n\t}\n\treturn *x\n}\n```", "  ```\nfunc FromPtr[T any](x *T) T {\n\tif x == nil {\n\t\tvar zero T\n\t\treturn zero\n\t}\n\treturn *x\n}\n```"]}, "lo-errors.go-Try6": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/errors.go:\n```\npackage lo\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n)\n\n// Validate is a helper that creates an error when a condition is not met.\n// Play: https://go.dev/play/p/vPyh51XpCBt\nfunc Validate(ok bool, format string, args ...any) error {\n\tif !ok {\n\t\treturn fmt.Errorf(fmt.Sprintf(format, args...))\n\t}\n\treturn nil\n}\n\nfunc messageFromMsgAndArgs(msgAndArgs ...any) string {\n\tif len(msgAndArgs) == 1 {\n\t\tif msgAsStr, ok := msgAndArgs[0].(string); ok {\n\t\t\treturn msgAsStr\n\t\t}\n\t\treturn fmt.Sprintf(\"%+v\", msgAndArgs[0])\n\t}\n\tif len(msgAndArgs) > 1 {\n\t\treturn fmt.Sprintf(msgAndArgs[0].(string), msgAndArgs[1:]...)\n\t}\n\treturn \"\"\n}\n\n// must panics if err is error or false.\nfunc must(err any, messageArgs ...any) {\n\tif err == nil {\n\t\treturn\n\t}\n\n\tswitch e := err.(type) {\n\tcase bool:\n\t\tif !e {\n\t\t\tmessage := messageFromMsgAndArgs(messageArgs...)\n\t\t\tif message == \"\" {\n\t\t\t\tmessage = \"not ok\"\n\t\t\t}\n\n\t\t\tpanic(message)\n\t\t}\n\n\tcase error:\n\t\tmessage := messageFromMsgAndArgs(messageArgs...)\n\t\tif message != \"\" {\n\t\t\tpanic(message + \": \" + e.Error())\n\t\t} else {\n\t\t\tpanic(e.Error())\n\t\t}\n\n\tdefault:\n\t\tpanic(\"must: invalid err type '\" + reflect.TypeOf(err).Name() + \"', should either be a bool or an error\")\n\t}\n}\n\n// Must is a helper that wraps a call to a function returning a value and an error\n// and panics if err is error or false.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must[T any](val T, err any, messageArgs ...any) T {\n\tmust(err, messageArgs...)\n\treturn val\n}\n\n// Must0 has the same behavior as Must, but callback returns no variable.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must0(err any, messageArgs ...any) {\n\tmust(err, messageArgs...)\n}\n\n// Must1 is an alias to Must\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must1[T any](val T, err any, messageArgs ...any) T {\n\treturn Must(val, err, messageArgs...)\n}\n\n// Must2 has the same behavior as Must, but callback returns 2 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must2[T1, T2 any](val1 T1, val2 T2, err any, messageArgs ...any) (T1, T2) {\n\tmust(err, messageArgs...)\n\treturn val1, val2\n}\n\n// Must3 has the same behavior as Must, but callback returns 3 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must3[T1, T2, T3 any](val1 T1, val2 T2, val3 T3, err any, messageArgs ...any) (T1, T2, T3) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3\n}\n\n// Must4 has the same behavior as Must, but callback returns 4 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must4[T1, T2, T3, T4 any](val1 T1, val2 T2, val3 T3, val4 T4, err any, messageArgs ...any) (T1, T2, T3, T4) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3, val4\n}\n\n// Must5 has the same behavior as Must, but callback returns 5 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must5[T1, T2, T3, T4, T5 any](val1 T1, val2 T2, val3 T3, val4 T4, val5 T5, err any, messageArgs ...any) (T1, T2, T3, T4, T5) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3, val4, val5\n}\n\n// Must6 has the same behavior as Must, but callback returns 6 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must6[T1, T2, T3, T4, T5, T6 any](val1 T1, val2 T2, val3 T3, val4 T4, val5 T5, val6 T6, err any, messageArgs ...any) (T1, T2, T3, T4, T5, T6) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3, val4, val5, val6\n}\n\n// Try calls the function and return false in case of error.\nfunc Try(callback func() error) (ok bool) {\n\tok = true\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tok = false\n\t\t}\n\t}()\n\n\terr := callback()\n\tif err != nil {\n\t\tok = false\n\t}\n\n\treturn\n}\n\n// Try0 has the same behavior as Try, but callback returns no variable.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try0(callback func()) bool {\n\treturn Try(func() error {\n\t\tcallback()\n\t\treturn nil\n\t})\n}\n\n// Try1 is an alias to Try.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try1(callback func() error) bool {\n\treturn Try(callback)\n}\n\n// Try2 has the same behavior as Try, but callback returns 2 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try2[T any](callback func() (T, error)) bool {\n\treturn Try(func() error {\n\t\t_, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try3 has the same behavior as Try, but callback returns 3 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try3[T, R any](callback func() (T, R, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try4 has the same behavior as Try, but callback returns 4 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try4[T, R, S any](callback func() (T, R, S, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, _, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try5 has the same behavior as Try, but callback returns 5 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try5[T, R, S, Q any](callback func() (T, R, S, Q, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, _, _, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try6 has the same behavior as Try, but callback returns 6 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\n\n\n\n\n\n\n\n// TryOr has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr[A any](callback func() (A, error), fallbackA A) (A, bool) {\n\treturn TryOr1(callback, fallbackA)\n}\n\n// TryOr1 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr1[A any](callback func() (A, error), fallbackA A) (A, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, ok\n}\n\n// TryOr2 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr2[A, B any](callback func() (A, B, error), fallbackA A, fallbackB B) (A, B, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, ok\n}\n\n// TryOr3 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr3[A, B, C any](callback func() (A, B, C, error), fallbackA A, fallbackB B, fallbackC C) (A, B, C, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, ok\n}\n\n// TryOr4 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr4[A, B, C, D any](callback func() (A, B, C, D, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D) (A, B, C, D, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, d, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tfallbackD = d\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, fallbackD, ok\n}\n\n// TryOr5 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr5[A, B, C, D, E any](callback func() (A, B, C, D, E, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D, fallbackE E) (A, B, C, D, E, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, d, e, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tfallbackD = d\n\t\t\tfallbackE = e\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, fallbackD, fallbackE, ok\n}\n\n// TryOr6 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// TryWithErrorValue has the same behavior as Try, but also returns value passed to panic.\n// Play: https://go.dev/play/p/Kc7afQIT2Fs\nfunc TryWithErrorValue(callback func() error) (errorValue any, ok bool) {\n\tok = true\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tok = false\n\t\t\terrorValue = r\n\t\t}\n\t}()\n\n\terr := callback()\n\tif err != nil {\n\t\tok = false\n\t\terrorValue = err\n\t}\n\n\treturn\n}\n\n// TryCatch has the same behavior as Try, but calls the catch function in case of error.\n// Play: https://go.dev/play/p/PnOON-EqBiU\nfunc TryCatch(callback func() error, catch func()) {\n\tif !Try(callback) {\n\t\tcatch()\n\t}\n}\n\n// TryCatchWithErrorValue has the same behavior as TryWithErrorValue, but calls the catch function in case of error.\n// Play: https://go.dev/play/p/8Pc9gwX_GZO\nfunc TryCatchWithErrorValue(callback func() error, catch func(any)) {\n\tif err, ok := TryWithErrorValue(callback); !ok {\n\t\tcatch(err)\n\t}\n}\n\n// ErrorsAs is a shortcut for errors.As(err, &&T).\n// Play: https://go.dev/play/p/8wk5rH8UfrE\nfunc ErrorsAs[T error](err error) (T, bool) {\n\tvar t T\n\tok := errors.As(err, &t)\n\treturn t, ok\n}\n\n```\n# Function Signature: func Try6[T, R, S, Q, U any](callback func() (T, R, S, Q, U, error)) bool {\n# Natural Language Annotations: // Try6 has the same behavior as Try, but callback returns 6 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc Try6[T, R, S, Q, U any](callback func() (T, R, S, Q, U, error)) bool {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, d, e, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tfallbackD = d\n\t\t\tfallbackE = e\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, fallbackD, fallbackE, ok\n}\n```", "  ```\nfunc Try6[T, R, S, Q, U any](callback func() (T, R, S, Q, U, error)) bool {\n\tok := false\n\n\tTry0(func() {\n\t\tt, r, s, q, u, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackT := t\n\t\t\tfallbackR := r\n\t\t\tfallbackS := s\n\t\t\tfallbackQ := q\n\t\t\tfallbackU := u\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackT, fallbackR, fallbackS, fallbackQ, fallbackU, ok\n}\n```", "  ```\nfunc Try6[T, R, S, Q, U any](callback func() (T, R, S, Q, U, error)) bool {\n\tok := true\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tok = false\n\t\t}\n\t}()\n\tt, r, s, q, u, err := callback()\n\tif err != nil {\n\t\tok = false\n\t}\n\treturn ok\n}\n```"]}, "lo-errors.go-TryOr6": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/errors.go:\n```\npackage lo\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n)\n\n// Validate is a helper that creates an error when a condition is not met.\n// Play: https://go.dev/play/p/vPyh51XpCBt\nfunc Validate(ok bool, format string, args ...any) error {\n\tif !ok {\n\t\treturn fmt.Errorf(fmt.Sprintf(format, args...))\n\t}\n\treturn nil\n}\n\nfunc messageFromMsgAndArgs(msgAndArgs ...any) string {\n\tif len(msgAndArgs) == 1 {\n\t\tif msgAsStr, ok := msgAndArgs[0].(string); ok {\n\t\t\treturn msgAsStr\n\t\t}\n\t\treturn fmt.Sprintf(\"%+v\", msgAndArgs[0])\n\t}\n\tif len(msgAndArgs) > 1 {\n\t\treturn fmt.Sprintf(msgAndArgs[0].(string), msgAndArgs[1:]...)\n\t}\n\treturn \"\"\n}\n\n// must panics if err is error or false.\nfunc must(err any, messageArgs ...any) {\n\tif err == nil {\n\t\treturn\n\t}\n\n\tswitch e := err.(type) {\n\tcase bool:\n\t\tif !e {\n\t\t\tmessage := messageFromMsgAndArgs(messageArgs...)\n\t\t\tif message == \"\" {\n\t\t\t\tmessage = \"not ok\"\n\t\t\t}\n\n\t\t\tpanic(message)\n\t\t}\n\n\tcase error:\n\t\tmessage := messageFromMsgAndArgs(messageArgs...)\n\t\tif message != \"\" {\n\t\t\tpanic(message + \": \" + e.Error())\n\t\t} else {\n\t\t\tpanic(e.Error())\n\t\t}\n\n\tdefault:\n\t\tpanic(\"must: invalid err type '\" + reflect.TypeOf(err).Name() + \"', should either be a bool or an error\")\n\t}\n}\n\n// Must is a helper that wraps a call to a function returning a value and an error\n// and panics if err is error or false.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must[T any](val T, err any, messageArgs ...any) T {\n\tmust(err, messageArgs...)\n\treturn val\n}\n\n// Must0 has the same behavior as Must, but callback returns no variable.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must0(err any, messageArgs ...any) {\n\tmust(err, messageArgs...)\n}\n\n// Must1 is an alias to Must\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must1[T any](val T, err any, messageArgs ...any) T {\n\treturn Must(val, err, messageArgs...)\n}\n\n// Must2 has the same behavior as Must, but callback returns 2 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must2[T1, T2 any](val1 T1, val2 T2, err any, messageArgs ...any) (T1, T2) {\n\tmust(err, messageArgs...)\n\treturn val1, val2\n}\n\n// Must3 has the same behavior as Must, but callback returns 3 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must3[T1, T2, T3 any](val1 T1, val2 T2, val3 T3, err any, messageArgs ...any) (T1, T2, T3) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3\n}\n\n// Must4 has the same behavior as Must, but callback returns 4 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must4[T1, T2, T3, T4 any](val1 T1, val2 T2, val3 T3, val4 T4, err any, messageArgs ...any) (T1, T2, T3, T4) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3, val4\n}\n\n// Must5 has the same behavior as Must, but callback returns 5 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must5[T1, T2, T3, T4, T5 any](val1 T1, val2 T2, val3 T3, val4 T4, val5 T5, err any, messageArgs ...any) (T1, T2, T3, T4, T5) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3, val4, val5\n}\n\n// Must6 has the same behavior as Must, but callback returns 6 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must6[T1, T2, T3, T4, T5, T6 any](val1 T1, val2 T2, val3 T3, val4 T4, val5 T5, val6 T6, err any, messageArgs ...any) (T1, T2, T3, T4, T5, T6) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3, val4, val5, val6\n}\n\n// Try calls the function and return false in case of error.\nfunc Try(callback func() error) (ok bool) {\n\tok = true\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tok = false\n\t\t}\n\t}()\n\n\terr := callback()\n\tif err != nil {\n\t\tok = false\n\t}\n\n\treturn\n}\n\n// Try0 has the same behavior as Try, but callback returns no variable.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try0(callback func()) bool {\n\treturn Try(func() error {\n\t\tcallback()\n\t\treturn nil\n\t})\n}\n\n// Try1 is an alias to Try.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try1(callback func() error) bool {\n\treturn Try(callback)\n}\n\n// Try2 has the same behavior as Try, but callback returns 2 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try2[T any](callback func() (T, error)) bool {\n\treturn Try(func() error {\n\t\t_, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try3 has the same behavior as Try, but callback returns 3 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try3[T, R any](callback func() (T, R, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try4 has the same behavior as Try, but callback returns 4 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try4[T, R, S any](callback func() (T, R, S, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, _, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try5 has the same behavior as Try, but callback returns 5 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try5[T, R, S, Q any](callback func() (T, R, S, Q, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, _, _, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try6 has the same behavior as Try, but callback returns 6 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\n\n\n\n\n\n\n\n// TryOr has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr[A any](callback func() (A, error), fallbackA A) (A, bool) {\n\treturn TryOr1(callback, fallbackA)\n}\n\n// TryOr1 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr1[A any](callback func() (A, error), fallbackA A) (A, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, ok\n}\n\n// TryOr2 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr2[A, B any](callback func() (A, B, error), fallbackA A, fallbackB B) (A, B, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, ok\n}\n\n// TryOr3 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr3[A, B, C any](callback func() (A, B, C, error), fallbackA A, fallbackB B, fallbackC C) (A, B, C, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, ok\n}\n\n// TryOr4 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr4[A, B, C, D any](callback func() (A, B, C, D, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D) (A, B, C, D, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, d, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tfallbackD = d\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, fallbackD, ok\n}\n\n// TryOr5 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr5[A, B, C, D, E any](callback func() (A, B, C, D, E, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D, fallbackE E) (A, B, C, D, E, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, d, e, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tfallbackD = d\n\t\t\tfallbackE = e\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, fallbackD, fallbackE, ok\n}\n\n// TryOr6 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// TryWithErrorValue has the same behavior as Try, but also returns value passed to panic.\n// Play: https://go.dev/play/p/Kc7afQIT2Fs\nfunc TryWithErrorValue(callback func() error) (errorValue any, ok bool) {\n\tok = true\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tok = false\n\t\t\terrorValue = r\n\t\t}\n\t}()\n\n\terr := callback()\n\tif err != nil {\n\t\tok = false\n\t\terrorValue = err\n\t}\n\n\treturn\n}\n\n// TryCatch has the same behavior as Try, but calls the catch function in case of error.\n// Play: https://go.dev/play/p/PnOON-EqBiU\nfunc TryCatch(callback func() error, catch func()) {\n\tif !Try(callback) {\n\t\tcatch()\n\t}\n}\n\n// TryCatchWithErrorValue has the same behavior as TryWithErrorValue, but calls the catch function in case of error.\n// Play: https://go.dev/play/p/8Pc9gwX_GZO\nfunc TryCatchWithErrorValue(callback func() error, catch func(any)) {\n\tif err, ok := TryWithErrorValue(callback); !ok {\n\t\tcatch(err)\n\t}\n}\n\n// ErrorsAs is a shortcut for errors.As(err, &&T).\n// Play: https://go.dev/play/p/8wk5rH8UfrE\nfunc ErrorsAs[T error](err error) (T, bool) {\n\tvar t T\n\tok := errors.As(err, &t)\n\treturn t, ok\n}\n\n```\n# Function Signature: func TryOr6[A, B, C, D, E, F any](callback func() (A, B, C, D, E, F, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D, fallbackE E, fallbackF F) (A, B, C, D, E, F, bool) {\n# Natural Language Annotations: // TryOr6 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc TryOr6[A, B, C, D, E, F any](callback func() (A, B, C, D, E, F, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D, fallbackE E, fallbackF F) (A, B, C, D, E, F, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, d, e, f, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tfallbackD = d\n\t\t\tfallbackE = e\n\t\t\tfallbackF = f\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, fallbackD, fallbackE, fallbackF, ok\n}\n```", "  ```\nfunc TryOr6[A, B, C, D, E, F any](callback func() (A, B, C, D, E, F, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D, fallbackE E, fallbackF F) (A, B, C, D, E, F, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, d, e, f, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tfallbackD = d\n\t\t\tfallbackE = e\n\t\t\tfallbackF = f\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, fallbackD, fallbackE, fallbackF, ok\n}\n```", "  ```\nfunc TryOr6[A, B, C, D, E, F any](callback func() (A, B, C, D, E, F, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D, fallbackE E, fallbackF F) (A, B, C, D, E, F, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, d, e, f, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tfallbackD = d\n\t\t\tfallbackE = e\n\t\t\tfallbackF = f\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, fallbackD, fallbackE, fallbackF, ok\n}\n```"]}, "lo-string.go-Substring": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/string.go:\n```\npackage lo\n\nimport (\n\t\"regexp\"\n\t\"strings\"\n\t\"unicode\"\n\t\"unicode/utf8\"\n\n\t\"github.com/samber/lo/internal/rand\"\n\n\t\"golang.org/x/text/cases\"\n\t\"golang.org/x/text/language\"\n)\n\nvar (\n\tLowerCaseLettersCharset = []rune(\"abcdefghijklmnopqrstuvwxyz\")\n\tUpperCaseLettersCharset = []rune(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n\tLettersCharset          = append(LowerCaseLettersCharset, UpperCaseLettersCharset...)\n\tNumbersCharset          = []rune(\"0123456789\")\n\tAlphanumericCharset     = append(LettersCharset, NumbersCharset...)\n\tSpecialCharset          = []rune(\"!@#$%^&*()_+-=[]{}|;':\\\",./<>?\")\n\tAllCharset              = append(AlphanumericCharset, SpecialCharset...)\n\n\t// bearer:disable go_lang_permissive_regex_validation\n\tsplitWordReg = regexp.MustCompile(`([a-z])([A-Z0-9])|([a-zA-Z])([0-9])|([0-9])([a-zA-Z])|([A-Z])([A-Z])([a-z])`)\n\t// bearer:disable go_lang_permissive_regex_validation\n\tsplitNumberLetterReg = regexp.MustCompile(`([0-9])([a-zA-Z])`)\n)\n\n// RandomString return a random string.\n// Play: https://go.dev/play/p/rRseOQVVum4\nfunc RandomString(size int, charset []rune) string {\n\tif size <= 0 {\n\t\tpanic(\"lo.RandomString: Size parameter must be greater than 0\")\n\t}\n\tif len(charset) <= 0 {\n\t\tpanic(\"lo.RandomString: Charset parameter must not be empty\")\n\t}\n\n\tb := make([]rune, size)\n\tpossibleCharactersCount := len(charset)\n\tfor i := range b {\n\t\tb[i] = charset[rand.IntN(possibleCharactersCount)]\n\t}\n\treturn string(b)\n}\n\n// Substring return part of a string.\n// Play: https://go.dev/play/p/TQlxQi82Lu1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ChunkString returns an array of strings split into groups the length of size. If array can't be split evenly,\n// the final chunk will be the remaining elements.\n// Play: https://go.dev/play/p/__FLTuJVz54\nfunc ChunkString[T ~string](str T, size int) []T {\n\tif size <= 0 {\n\t\tpanic(\"lo.ChunkString: Size parameter must be greater than 0\")\n\t}\n\n\tif len(str) == 0 {\n\t\treturn []T{\"\"}\n\t}\n\n\tif size >= len(str) {\n\t\treturn []T{str}\n\t}\n\n\tvar chunks []T = make([]T, 0, ((len(str)-1)/size)+1)\n\tcurrentLen := 0\n\tcurrentStart := 0\n\tfor i := range str {\n\t\tif currentLen == size {\n\t\t\tchunks = append(chunks, str[currentStart:i])\n\t\t\tcurrentLen = 0\n\t\t\tcurrentStart = i\n\t\t}\n\t\tcurrentLen++\n\t}\n\tchunks = append(chunks, str[currentStart:])\n\treturn chunks\n}\n\n// RuneLength is an alias to utf8.RuneCountInString which returns the number of runes in string.\n// Play: https://go.dev/play/p/tuhgW_lWY8l\nfunc RuneLength(str string) int {\n\treturn utf8.RuneCountInString(str)\n}\n\n// PascalCase converts string to pascal case.\n\n\n\n\n\n\n\n\n// CamelCase converts string to camel case.\nfunc CamelCase(str string) string {\n\titems := Words(str)\n\tfor i, item := range items {\n\t\titem = strings.ToLower(item)\n\t\tif i > 0 {\n\t\t\titem = Capitalize(item)\n\t\t}\n\t\titems[i] = item\n\t}\n\treturn strings.Join(items, \"\")\n}\n\n// KebabCase converts string to kebab case.\nfunc KebabCase(str string) string {\n\titems := Words(str)\n\tfor i := range items {\n\t\titems[i] = strings.ToLower(items[i])\n\t}\n\treturn strings.Join(items, \"-\")\n}\n\n// SnakeCase converts string to snake case.\nfunc SnakeCase(str string) string {\n\titems := Words(str)\n\tfor i := range items {\n\t\titems[i] = strings.ToLower(items[i])\n\t}\n\treturn strings.Join(items, \"_\")\n}\n\n// Words splits string into an array of its words.\nfunc Words(str string) []string {\n\tstr = splitWordReg.ReplaceAllString(str, `$1$3$5$7 $2$4$6$8$9`)\n\t// example: Int8Value => Int 8Value => Int 8 Value\n\tstr = splitNumberLetterReg.ReplaceAllString(str, \"$1 $2\")\n\tvar result strings.Builder\n\tfor _, r := range str {\n\t\tif unicode.IsLetter(r) || unicode.IsDigit(r) {\n\t\t\tresult.WriteRune(r)\n\t\t} else {\n\t\t\tresult.WriteRune(' ')\n\t\t}\n\t}\n\treturn strings.Fields(result.String())\n}\n\n// Capitalize converts the first character of string to upper case and the remaining to lower case.\nfunc Capitalize(str string) string {\n\treturn cases.Title(language.English).String(str)\n}\n\n// Elipse truncates a string to a specified length and appends an ellipsis if truncated.\nfunc Elipse(str string, length int) string {\n\tif len(str) > length {\n\t\tif len(str) < 3 || length < 3 {\n\t\t\treturn \"...\"\n\t\t}\n\t\treturn str[0:length-3] + \"...\"\n\t}\n\n\treturn str\n}\n\n```\n# Function Signature: func Substring[T ~string](str T, offset int, length uint) T {\n# Natural Language Annotations: // Substring return part of a string.\n// Play: https://go.dev/play/p/TQlxQi82Lu1\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc Substring[T ~string](str T, offset int, length uint) T {\n\tif offset < 0 || length == 0 {\n\t\treturn \"\"\n\t}\n\n\tif offset+length > uint(len(str)) {\n\t\treturn str[offset:]\n\t}\n\n\treturn str[offset : offset+length]\n}\n```", "  ```\nfunc Substring[T ~string](str T, offset int, length uint) T {\n\tif offset < 0 || length < 0 {\n\t\tpanic(\"lo.Substring: Offset and length parameters must be non-negative\")\n\t}\n\n\tif offset+length > uint(len(str)) {\n\t\tpanic(\"lo.Substring: Offset and length parameters must be within the string length\")\n\t}\n\n\treturn str[offset : offset+uint(length)]\n}\n```", "  ```\nfunc Substring[T ~string](str T, offset int, length uint) T {\n\tif len(str) == 0 || len(str) <= offset || length <= 0 {\n\t\treturn str\n\t}\n\tif uint(len(str)) < length {\n\t\treturn str\n\t}\n\tif uint(len(str)) < length {\n\t\treturn str\n\t}\n\treturn str[offset : offset+length]\n}\n```"]}, "lo-string.go-PascalCase": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/string.go:\n```\npackage lo\n\nimport (\n\t\"regexp\"\n\t\"strings\"\n\t\"unicode\"\n\t\"unicode/utf8\"\n\n\t\"github.com/samber/lo/internal/rand\"\n\n\t\"golang.org/x/text/cases\"\n\t\"golang.org/x/text/language\"\n)\n\nvar (\n\tLowerCaseLettersCharset = []rune(\"abcdefghijklmnopqrstuvwxyz\")\n\tUpperCaseLettersCharset = []rune(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n\tLettersCharset          = append(LowerCaseLettersCharset, UpperCaseLettersCharset...)\n\tNumbersCharset          = []rune(\"0123456789\")\n\tAlphanumericCharset     = append(LettersCharset, NumbersCharset...)\n\tSpecialCharset          = []rune(\"!@#$%^&*()_+-=[]{}|;':\\\",./<>?\")\n\tAllCharset              = append(AlphanumericCharset, SpecialCharset...)\n\n\t// bearer:disable go_lang_permissive_regex_validation\n\tsplitWordReg = regexp.MustCompile(`([a-z])([A-Z0-9])|([a-zA-Z])([0-9])|([0-9])([a-zA-Z])|([A-Z])([A-Z])([a-z])`)\n\t// bearer:disable go_lang_permissive_regex_validation\n\tsplitNumberLetterReg = regexp.MustCompile(`([0-9])([a-zA-Z])`)\n)\n\n// RandomString return a random string.\n// Play: https://go.dev/play/p/rRseOQVVum4\nfunc RandomString(size int, charset []rune) string {\n\tif size <= 0 {\n\t\tpanic(\"lo.RandomString: Size parameter must be greater than 0\")\n\t}\n\tif len(charset) <= 0 {\n\t\tpanic(\"lo.RandomString: Charset parameter must not be empty\")\n\t}\n\n\tb := make([]rune, size)\n\tpossibleCharactersCount := len(charset)\n\tfor i := range b {\n\t\tb[i] = charset[rand.IntN(possibleCharactersCount)]\n\t}\n\treturn string(b)\n}\n\n// Substring return part of a string.\n// Play: https://go.dev/play/p/TQlxQi82Lu1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ChunkString returns an array of strings split into groups the length of size. If array can't be split evenly,\n// the final chunk will be the remaining elements.\n// Play: https://go.dev/play/p/__FLTuJVz54\nfunc ChunkString[T ~string](str T, size int) []T {\n\tif size <= 0 {\n\t\tpanic(\"lo.ChunkString: Size parameter must be greater than 0\")\n\t}\n\n\tif len(str) == 0 {\n\t\treturn []T{\"\"}\n\t}\n\n\tif size >= len(str) {\n\t\treturn []T{str}\n\t}\n\n\tvar chunks []T = make([]T, 0, ((len(str)-1)/size)+1)\n\tcurrentLen := 0\n\tcurrentStart := 0\n\tfor i := range str {\n\t\tif currentLen == size {\n\t\t\tchunks = append(chunks, str[currentStart:i])\n\t\t\tcurrentLen = 0\n\t\t\tcurrentStart = i\n\t\t}\n\t\tcurrentLen++\n\t}\n\tchunks = append(chunks, str[currentStart:])\n\treturn chunks\n}\n\n// RuneLength is an alias to utf8.RuneCountInString which returns the number of runes in string.\n// Play: https://go.dev/play/p/tuhgW_lWY8l\nfunc RuneLength(str string) int {\n\treturn utf8.RuneCountInString(str)\n}\n\n// PascalCase converts string to pascal case.\n\n\n\n\n\n\n\n\n// CamelCase converts string to camel case.\nfunc CamelCase(str string) string {\n\titems := Words(str)\n\tfor i, item := range items {\n\t\titem = strings.ToLower(item)\n\t\tif i > 0 {\n\t\t\titem = Capitalize(item)\n\t\t}\n\t\titems[i] = item\n\t}\n\treturn strings.Join(items, \"\")\n}\n\n// KebabCase converts string to kebab case.\nfunc KebabCase(str string) string {\n\titems := Words(str)\n\tfor i := range items {\n\t\titems[i] = strings.ToLower(items[i])\n\t}\n\treturn strings.Join(items, \"-\")\n}\n\n// SnakeCase converts string to snake case.\nfunc SnakeCase(str string) string {\n\titems := Words(str)\n\tfor i := range items {\n\t\titems[i] = strings.ToLower(items[i])\n\t}\n\treturn strings.Join(items, \"_\")\n}\n\n// Words splits string into an array of its words.\nfunc Words(str string) []string {\n\tstr = splitWordReg.ReplaceAllString(str, `$1$3$5$7 $2$4$6$8$9`)\n\t// example: Int8Value => Int 8Value => Int 8 Value\n\tstr = splitNumberLetterReg.ReplaceAllString(str, \"$1 $2\")\n\tvar result strings.Builder\n\tfor _, r := range str {\n\t\tif unicode.IsLetter(r) || unicode.IsDigit(r) {\n\t\t\tresult.WriteRune(r)\n\t\t} else {\n\t\t\tresult.WriteRune(' ')\n\t\t}\n\t}\n\treturn strings.Fields(result.String())\n}\n\n// Capitalize converts the first character of string to upper case and the remaining to lower case.\nfunc Capitalize(str string) string {\n\treturn cases.Title(language.English).String(str)\n}\n\n// Elipse truncates a string to a specified length and appends an ellipsis if truncated.\nfunc Elipse(str string, length int) string {\n\tif len(str) > length {\n\t\tif len(str) < 3 || length < 3 {\n\t\t\treturn \"...\"\n\t\t}\n\t\treturn str[0:length-3] + \"...\"\n\t}\n\n\treturn str\n}\n\n```\n# Function Signature: func PascalCase(str string) string {\n# Natural Language Annotations: // PascalCase converts string to pascal case.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc PascalCase(str string) string {\n\titems := Words(str)\n\tfor i, item := range items {\n\t\titem = strings.ToUpper(item)\n\t\tif i > 0 {\n\t\t\titem = Capitalize(item)\n\t\t}\n\t\titems[i] = item\n\t}\n\treturn strings.Join(items, \"\")\n}\n```", "  ```\nfunc PascalCase(str string) string {\n\titems := Words(str)\n\tfor i := range items {\n\t\titems[i] = Capitalize(items[i])\n\t}\n\treturn strings.Join(items, \"\")\n}\n```", "  ```\nfunc PascalCase(str string) string {\n\tif str == \"\" {\n\t\treturn \"\"\n\t}\n\tif len(str) == 1 {\n\t\treturn strings.ToUpper(str)\n\t}\n\treturn strings.ToUpper(str[0:1]) + CamelCase(str[1:])\n}\n```"]}, "lo-map.go-PickByValues": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/map.go:\n```\npackage lo\n\n// Keys creates an array of the map keys.\n// Play: https://go.dev/play/p/Uu11fHASqrU\nfunc Keys[K comparable, V any](in map[K]V) []K {\n\tresult := make([]K, 0, len(in))\n\n\tfor k := range in {\n\t\tresult = append(result, k)\n\t}\n\n\treturn result\n}\n\n// HasKey returns whether the given key exists.\n// Play: https://go.dev/play/p/aVwubIvECqS\nfunc HasKey[K comparable, V any](in map[K]V, key K) bool {\n\t_, ok := in[key]\n\treturn ok\n}\n\n// Values creates an array of the map values.\n// Play: https://go.dev/play/p/nnRTQkzQfF6\nfunc Values[K comparable, V any](in map[K]V) []V {\n\tresult := make([]V, 0, len(in))\n\n\tfor k := range in {\n\t\tresult = append(result, in[k])\n\t}\n\n\treturn result\n}\n\n// ValueOr returns the value of the given key or the fallback value if the key is not present.\n// Play: https://go.dev/play/p/bAq9mHErB4V\nfunc ValueOr[K comparable, V any](in map[K]V, key K, fallback V) V {\n\tif v, ok := in[key]; ok {\n\t\treturn v\n\t}\n\treturn fallback\n}\n\n// PickBy returns same map type filtered by given predicate.\n// Play: https://go.dev/play/p/kdg8GR_QMmf\nfunc PickBy[K comparable, V any, Map ~map[K]V](in Map, predicate func(key K, value V) bool) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tif predicate(k, in[k]) {\n\t\t\tr[k] = in[k]\n\t\t}\n\t}\n\treturn r\n}\n\n// PickByKeys returns same map type filtered by given keys.\n// Play: https://go.dev/play/p/R1imbuci9qU\nfunc PickByKeys[K comparable, V any, Map ~map[K]V](in Map, keys []K) Map {\n\tr := Map{}\n\tfor i := range keys {\n\t\tif v, ok := in[keys[i]]; ok {\n\t\t\tr[keys[i]] = v\n\t\t}\n\t}\n\treturn r\n}\n\n// PickByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/1zdzSvbfsJc\n\n\n\n\n\n\n\n\n\n\n// OmitBy returns same map type filtered by given predicate.\n// Play: https://go.dev/play/p/EtBsR43bdsd\nfunc OmitBy[K comparable, V any, Map ~map[K]V](in Map, predicate func(key K, value V) bool) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tif !predicate(k, in[k]) {\n\t\t\tr[k] = in[k]\n\t\t}\n\t}\n\treturn r\n}\n\n// OmitByKeys returns same map type filtered by given keys.\n// Play: https://go.dev/play/p/t1QjCrs-ysk\nfunc OmitByKeys[K comparable, V any, Map ~map[K]V](in Map, keys []K) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tr[k] = in[k]\n\t}\n\tfor i := range keys {\n\t\tdelete(r, keys[i])\n\t}\n\treturn r\n}\n\n// OmitByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/9UYZi-hrs8j\n\n\n\n\n\n\n\n\n\n\n// Entries transforms a map into array of key/value pairs.\n// Play:\nfunc Entries[K comparable, V any](in map[K]V) []Entry[K, V] {\n\tentries := make([]Entry[K, V], 0, len(in))\n\n\tfor k := range in {\n\t\tentries = append(entries, Entry[K, V]{\n\t\t\tKey:   k,\n\t\t\tValue: in[k],\n\t\t})\n\t}\n\n\treturn entries\n}\n\n// ToPairs transforms a map into array of key/value pairs.\n// Alias of Entries().\n// Play: https://go.dev/play/p/3Dhgx46gawJ\nfunc ToPairs[K comparable, V any](in map[K]V) []Entry[K, V] {\n\treturn Entries(in)\n}\n\n// FromEntries transforms an array of key/value pairs into a map.\n// Play: https://go.dev/play/p/oIr5KHFGCEN\nfunc FromEntries[K comparable, V any](entries []Entry[K, V]) map[K]V {\n\tout := make(map[K]V, len(entries))\n\n\tfor i := range entries {\n\t\tout[entries[i].Key] = entries[i].Value\n\t}\n\n\treturn out\n}\n\n// FromPairs transforms an array of key/value pairs into a map.\n// Alias of FromEntries().\n// Play: https://go.dev/play/p/oIr5KHFGCEN\nfunc FromPairs[K comparable, V any](entries []Entry[K, V]) map[K]V {\n\treturn FromEntries(entries)\n}\n\n// Invert creates a map composed of the inverted keys and values. If map\n// contains duplicate values, subsequent values overwrite property assignments\n// of previous values.\n// Play: https://go.dev/play/p/rFQ4rak6iA1\nfunc Invert[K comparable, V comparable](in map[K]V) map[V]K {\n\tout := make(map[V]K, len(in))\n\n\tfor k := range in {\n\t\tout[in[k]] = k\n\t}\n\n\treturn out\n}\n\n// Assign merges multiple maps from left to right.\n// Play: https://go.dev/play/p/VhwfJOyxf5o\nfunc Assign[K comparable, V any, Map ~map[K]V](maps ...Map) Map {\n\tcount := 0\n\tfor i := range maps {\n\t\tcount += len(maps[i])\n\t}\n\n\tout := make(Map, count)\n\tfor i := range maps {\n\t\tfor k := range maps[i] {\n\t\t\tout[k] = maps[i][k]\n\t\t}\n\t}\n\n\treturn out\n}\n\n// MapKeys manipulates a map keys and transforms it to a map of another type.\n// Play: https://go.dev/play/p/9_4WPIqOetJ\nfunc MapKeys[K comparable, V any, R comparable](in map[K]V, iteratee func(value V, key K) R) map[R]V {\n\tresult := make(map[R]V, len(in))\n\n\tfor k := range in {\n\t\tresult[iteratee(in[k], k)] = in[k]\n\t}\n\n\treturn result\n}\n\n// MapValues manipulates a map values and transforms it to a map of another type.\n// Play: https://go.dev/play/p/T_8xAfvcf0W\nfunc MapValues[K comparable, V any, R any](in map[K]V, iteratee func(value V, key K) R) map[K]R {\n\tresult := make(map[K]R, len(in))\n\n\tfor k := range in {\n\t\tresult[k] = iteratee(in[k], k)\n\t}\n\n\treturn result\n}\n\n// MapEntries manipulates a map entries and transforms it to a map of another type.\n// Play: https://go.dev/play/p/VuvNQzxKimT\nfunc MapEntries[K1 comparable, V1 any, K2 comparable, V2 any](in map[K1]V1, iteratee func(key K1, value V1) (K2, V2)) map[K2]V2 {\n\tresult := make(map[K2]V2, len(in))\n\n\tfor k1 := range in {\n\t\tk2, v2 := iteratee(k1, in[k1])\n\t\tresult[k2] = v2\n\t}\n\n\treturn result\n}\n\n// MapToSlice transforms a map into a slice based on specific iteratee\n// Play: https://go.dev/play/p/ZuiCZpDt6LD\nfunc MapToSlice[K comparable, V any, R any](in map[K]V, iteratee func(key K, value V) R) []R {\n\tresult := make([]R, 0, len(in))\n\n\tfor k := range in {\n\t\tresult = append(result, iteratee(k, in[k]))\n\t}\n\n\treturn result\n}\n\n```\n# Function Signature: func PickByValues[K comparable, V comparable, Map ~map[K]V](in Map, values []V) Map {\n# Natural Language Annotations: // PickByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/1zdzSvbfsJc\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc PickByValues[K comparable, V comparable, Map ~map[K]V](in Map, values []V) Map {\n\tr := Map{}\n\tfor i := range values {\n\t\tif v, ok := in[values[i]]; ok {\n\t\t\tr[values[i]] = v\n\t\t}\n\t}\n\treturn r\n}\n```", "  ```\nfunc PickByValues[K comparable, V comparable, Map ~map[K]V](in Map, values []V) Map {\n\tr := Map{}\n\tfor i := range values {\n\t\tv := values[i]\n\t\tfor k := range in {\n\t\t\tif v == in[k] {\n\t\t\t\tr[k] = in[k]\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\treturn r\n}\n```", "  ```\nfunc PickByValues[K comparable, V comparable, Map~map[K]V](in Map, values []V) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tv := in[k]\n\t\tfor _, value := range values {\n\t\t\tif v == value {\n\t\t\t\tr[k] = v\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\treturn r\n}\n```"]}, "lo-map.go-OmitByValues": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/map.go:\n```\npackage lo\n\n// Keys creates an array of the map keys.\n// Play: https://go.dev/play/p/Uu11fHASqrU\nfunc Keys[K comparable, V any](in map[K]V) []K {\n\tresult := make([]K, 0, len(in))\n\n\tfor k := range in {\n\t\tresult = append(result, k)\n\t}\n\n\treturn result\n}\n\n// HasKey returns whether the given key exists.\n// Play: https://go.dev/play/p/aVwubIvECqS\nfunc HasKey[K comparable, V any](in map[K]V, key K) bool {\n\t_, ok := in[key]\n\treturn ok\n}\n\n// Values creates an array of the map values.\n// Play: https://go.dev/play/p/nnRTQkzQfF6\nfunc Values[K comparable, V any](in map[K]V) []V {\n\tresult := make([]V, 0, len(in))\n\n\tfor k := range in {\n\t\tresult = append(result, in[k])\n\t}\n\n\treturn result\n}\n\n// ValueOr returns the value of the given key or the fallback value if the key is not present.\n// Play: https://go.dev/play/p/bAq9mHErB4V\nfunc ValueOr[K comparable, V any](in map[K]V, key K, fallback V) V {\n\tif v, ok := in[key]; ok {\n\t\treturn v\n\t}\n\treturn fallback\n}\n\n// PickBy returns same map type filtered by given predicate.\n// Play: https://go.dev/play/p/kdg8GR_QMmf\nfunc PickBy[K comparable, V any, Map ~map[K]V](in Map, predicate func(key K, value V) bool) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tif predicate(k, in[k]) {\n\t\t\tr[k] = in[k]\n\t\t}\n\t}\n\treturn r\n}\n\n// PickByKeys returns same map type filtered by given keys.\n// Play: https://go.dev/play/p/R1imbuci9qU\nfunc PickByKeys[K comparable, V any, Map ~map[K]V](in Map, keys []K) Map {\n\tr := Map{}\n\tfor i := range keys {\n\t\tif v, ok := in[keys[i]]; ok {\n\t\t\tr[keys[i]] = v\n\t\t}\n\t}\n\treturn r\n}\n\n// PickByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/1zdzSvbfsJc\n\n\n\n\n\n\n\n\n\n\n// OmitBy returns same map type filtered by given predicate.\n// Play: https://go.dev/play/p/EtBsR43bdsd\nfunc OmitBy[K comparable, V any, Map ~map[K]V](in Map, predicate func(key K, value V) bool) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tif !predicate(k, in[k]) {\n\t\t\tr[k] = in[k]\n\t\t}\n\t}\n\treturn r\n}\n\n// OmitByKeys returns same map type filtered by given keys.\n// Play: https://go.dev/play/p/t1QjCrs-ysk\nfunc OmitByKeys[K comparable, V any, Map ~map[K]V](in Map, keys []K) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tr[k] = in[k]\n\t}\n\tfor i := range keys {\n\t\tdelete(r, keys[i])\n\t}\n\treturn r\n}\n\n// OmitByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/9UYZi-hrs8j\n\n\n\n\n\n\n\n\n\n\n// Entries transforms a map into array of key/value pairs.\n// Play:\nfunc Entries[K comparable, V any](in map[K]V) []Entry[K, V] {\n\tentries := make([]Entry[K, V], 0, len(in))\n\n\tfor k := range in {\n\t\tentries = append(entries, Entry[K, V]{\n\t\t\tKey:   k,\n\t\t\tValue: in[k],\n\t\t})\n\t}\n\n\treturn entries\n}\n\n// ToPairs transforms a map into array of key/value pairs.\n// Alias of Entries().\n// Play: https://go.dev/play/p/3Dhgx46gawJ\nfunc ToPairs[K comparable, V any](in map[K]V) []Entry[K, V] {\n\treturn Entries(in)\n}\n\n// FromEntries transforms an array of key/value pairs into a map.\n// Play: https://go.dev/play/p/oIr5KHFGCEN\nfunc FromEntries[K comparable, V any](entries []Entry[K, V]) map[K]V {\n\tout := make(map[K]V, len(entries))\n\n\tfor i := range entries {\n\t\tout[entries[i].Key] = entries[i].Value\n\t}\n\n\treturn out\n}\n\n// FromPairs transforms an array of key/value pairs into a map.\n// Alias of FromEntries().\n// Play: https://go.dev/play/p/oIr5KHFGCEN\nfunc FromPairs[K comparable, V any](entries []Entry[K, V]) map[K]V {\n\treturn FromEntries(entries)\n}\n\n// Invert creates a map composed of the inverted keys and values. If map\n// contains duplicate values, subsequent values overwrite property assignments\n// of previous values.\n// Play: https://go.dev/play/p/rFQ4rak6iA1\nfunc Invert[K comparable, V comparable](in map[K]V) map[V]K {\n\tout := make(map[V]K, len(in))\n\n\tfor k := range in {\n\t\tout[in[k]] = k\n\t}\n\n\treturn out\n}\n\n// Assign merges multiple maps from left to right.\n// Play: https://go.dev/play/p/VhwfJOyxf5o\nfunc Assign[K comparable, V any, Map ~map[K]V](maps ...Map) Map {\n\tcount := 0\n\tfor i := range maps {\n\t\tcount += len(maps[i])\n\t}\n\n\tout := make(Map, count)\n\tfor i := range maps {\n\t\tfor k := range maps[i] {\n\t\t\tout[k] = maps[i][k]\n\t\t}\n\t}\n\n\treturn out\n}\n\n// MapKeys manipulates a map keys and transforms it to a map of another type.\n// Play: https://go.dev/play/p/9_4WPIqOetJ\nfunc MapKeys[K comparable, V any, R comparable](in map[K]V, iteratee func(value V, key K) R) map[R]V {\n\tresult := make(map[R]V, len(in))\n\n\tfor k := range in {\n\t\tresult[iteratee(in[k], k)] = in[k]\n\t}\n\n\treturn result\n}\n\n// MapValues manipulates a map values and transforms it to a map of another type.\n// Play: https://go.dev/play/p/T_8xAfvcf0W\nfunc MapValues[K comparable, V any, R any](in map[K]V, iteratee func(value V, key K) R) map[K]R {\n\tresult := make(map[K]R, len(in))\n\n\tfor k := range in {\n\t\tresult[k] = iteratee(in[k], k)\n\t}\n\n\treturn result\n}\n\n// MapEntries manipulates a map entries and transforms it to a map of another type.\n// Play: https://go.dev/play/p/VuvNQzxKimT\nfunc MapEntries[K1 comparable, V1 any, K2 comparable, V2 any](in map[K1]V1, iteratee func(key K1, value V1) (K2, V2)) map[K2]V2 {\n\tresult := make(map[K2]V2, len(in))\n\n\tfor k1 := range in {\n\t\tk2, v2 := iteratee(k1, in[k1])\n\t\tresult[k2] = v2\n\t}\n\n\treturn result\n}\n\n// MapToSlice transforms a map into a slice based on specific iteratee\n// Play: https://go.dev/play/p/ZuiCZpDt6LD\nfunc MapToSlice[K comparable, V any, R any](in map[K]V, iteratee func(key K, value V) R) []R {\n\tresult := make([]R, 0, len(in))\n\n\tfor k := range in {\n\t\tresult = append(result, iteratee(k, in[k]))\n\t}\n\n\treturn result\n}\n\n```\n# Function Signature: func OmitByValues[K comparable, V comparable, Map ~map[K]V](in Map, values []V) Map {\n# Natural Language Annotations: // OmitByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/9UYZi-hrs8j\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc OmitByValues[K comparable, V comparable, Map ~map[K]V](in Map, values []V) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tr[k] = in[k]\n\t}\n\tfor i := range values {\n\t\tfor k := range in {\n\t\t\tif in[k] == values[i] {\n\t\t\t\tdelete(r, k)\n\t\t\t}\n\t\t}\n\t}\n\treturn r\n}\n```", "  ```\nfunc OmitByValues[K comparable, V comparable, Map ~map[K]V](in Map, values []V) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tr[k] = in[k]\n\t}\n\tfor _, v := range values {\n\t\tdelete(r, v)\n\t}\n\treturn r\n}\n```", "  ```\nfunc OmitByValues[K comparable, V comparable, Map ~map[K]V](in Map, values []V) Map {\n\tr := Map{}\n\tfor i := range in {\n\t\tif !contains(values, in[i]) {\n\t\t\tv := in[i]\n\t\t\tfor _, val := range values {\n\t\t\t\tif v == val {\n\t\t\t\t\tv = zero(v)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif v != zero(v) {\n\t\t\t\tr[i] = v\n\t\t\t}\n\t\t}\n\t}\n\treturn r\n\n}\n\nfunc contains[V any](slice []V, v V) bool {\n\tfor i := range slice {\n\t\tif slice[i] == v {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc zero[V any](v V) V {\n\tvar x V\n\treturn x\n}\n```"]}, "lo-tuples.go-Zip9": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/tuples.go:\n```\npackage lo\n\n// T2 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T2[A, B any](a A, b B) Tuple2[A, B] {\n\treturn Tuple2[A, B]{A: a, B: b}\n}\n\n// T3 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T3[A, B, C any](a A, b B, c C) Tuple3[A, B, C] {\n\treturn Tuple3[A, B, C]{A: a, B: b, C: c}\n}\n\n// T4 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T4[A, B, C, D any](a A, b B, c C, d D) Tuple4[A, B, C, D] {\n\treturn Tuple4[A, B, C, D]{A: a, B: b, C: c, D: d}\n}\n\n// T5 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T5[A, B, C, D, E any](a A, b B, c C, d D, e E) Tuple5[A, B, C, D, E] {\n\treturn Tuple5[A, B, C, D, E]{A: a, B: b, C: c, D: d, E: e}\n}\n\n// T6 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T6[A, B, C, D, E, F any](a A, b B, c C, d D, e E, f F) Tuple6[A, B, C, D, E, F] {\n\treturn Tuple6[A, B, C, D, E, F]{A: a, B: b, C: c, D: d, E: e, F: f}\n}\n\n// T7 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T7[A, B, C, D, E, F, G any](a A, b B, c C, d D, e E, f F, g G) Tuple7[A, B, C, D, E, F, G] {\n\treturn Tuple7[A, B, C, D, E, F, G]{A: a, B: b, C: c, D: d, E: e, F: f, G: g}\n}\n\n// T8 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T8[A, B, C, D, E, F, G, H any](a A, b B, c C, d D, e E, f F, g G, h H) Tuple8[A, B, C, D, E, F, G, H] {\n\treturn Tuple8[A, B, C, D, E, F, G, H]{A: a, B: b, C: c, D: d, E: e, F: f, G: g, H: h}\n}\n\n// T9 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T9[A, B, C, D, E, F, G, H, I any](a A, b B, c C, d D, e E, f F, g G, h H, i I) Tuple9[A, B, C, D, E, F, G, H, I] {\n\treturn Tuple9[A, B, C, D, E, F, G, H, I]{A: a, B: b, C: c, D: d, E: e, F: f, G: g, H: h, I: i}\n}\n\n// Unpack2 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack2[A, B any](tuple Tuple2[A, B]) (A, B) {\n\treturn tuple.A, tuple.B\n}\n\n// Unpack3 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack3[A, B, C any](tuple Tuple3[A, B, C]) (A, B, C) {\n\treturn tuple.A, tuple.B, tuple.C\n}\n\n// Unpack4 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack4[A, B, C, D any](tuple Tuple4[A, B, C, D]) (A, B, C, D) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D\n}\n\n// Unpack5 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack5[A, B, C, D, E any](tuple Tuple5[A, B, C, D, E]) (A, B, C, D, E) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E\n}\n\n// Unpack6 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack6[A, B, C, D, E, F any](tuple Tuple6[A, B, C, D, E, F]) (A, B, C, D, E, F) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F\n}\n\n// Unpack7 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack7[A, B, C, D, E, F, G any](tuple Tuple7[A, B, C, D, E, F, G]) (A, B, C, D, E, F, G) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F, tuple.G\n}\n\n// Unpack8 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack8[A, B, C, D, E, F, G, H any](tuple Tuple8[A, B, C, D, E, F, G, H]) (A, B, C, D, E, F, G, H) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F, tuple.G, tuple.H\n}\n\n// Unpack9 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack9[A, B, C, D, E, F, G, H, I any](tuple Tuple9[A, B, C, D, E, F, G, H, I]) (A, B, C, D, E, F, G, H, I) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F, tuple.G, tuple.H, tuple.I\n}\n\n// Zip2 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip2[A, B any](a []A, b []B) []Tuple2[A, B] {\n\tsize := Max([]int{len(a), len(b)})\n\n\tresult := make([]Tuple2[A, B], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\n\t\tresult = append(result, Tuple2[A, B]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip3 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip3[A, B, C any](a []A, b []B, c []C) []Tuple3[A, B, C] {\n\tsize := Max([]int{len(a), len(b), len(c)})\n\n\tresult := make([]Tuple3[A, B, C], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\n\t\tresult = append(result, Tuple3[A, B, C]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip4 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip4[A, B, C, D any](a []A, b []B, c []C, d []D) []Tuple4[A, B, C, D] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d)})\n\n\tresult := make([]Tuple4[A, B, C, D], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\n\t\tresult = append(result, Tuple4[A, B, C, D]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip5 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip5[A, B, C, D, E any](a []A, b []B, c []C, d []D, e []E) []Tuple5[A, B, C, D, E] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e)})\n\n\tresult := make([]Tuple5[A, B, C, D, E], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\n\t\tresult = append(result, Tuple5[A, B, C, D, E]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip6 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip6[A, B, C, D, E, F any](a []A, b []B, c []C, d []D, e []E, f []F) []Tuple6[A, B, C, D, E, F] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f)})\n\n\tresult := make([]Tuple6[A, B, C, D, E, F], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\n\t\tresult = append(result, Tuple6[A, B, C, D, E, F]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip7 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip7[A, B, C, D, E, F, G any](a []A, b []B, c []C, d []D, e []E, f []F, g []G) []Tuple7[A, B, C, D, E, F, G] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g)})\n\n\tresult := make([]Tuple7[A, B, C, D, E, F, G], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\n\t\tresult = append(result, Tuple7[A, B, C, D, E, F, G]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t\tG: _g,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip8 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip8[A, B, C, D, E, F, G, H any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H) []Tuple8[A, B, C, D, E, F, G, H] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g), len(h)})\n\n\tresult := make([]Tuple8[A, B, C, D, E, F, G, H], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\t\t_h, _ := Nth(h, index)\n\n\t\tresult = append(result, Tuple8[A, B, C, D, E, F, G, H]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t\tG: _g,\n\t\t\tH: _h,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip9 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ZipBy2 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy2[A any, B any, Out any](a []A, b []B, iteratee func(a A, b B) Out) []Out {\n\tsize := Max([]int{len(a), len(b)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\n\t\tresult = append(result, iteratee(_a, _b))\n\t}\n\n\treturn result\n}\n\n// ZipBy3 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy3[A any, B any, C any, Out any](a []A, b []B, c []C, iteratee func(a A, b B, c C) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c))\n\t}\n\n\treturn result\n}\n\n// ZipBy4 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy4[A any, B any, C any, D any, Out any](a []A, b []B, c []C, d []D, iteratee func(a A, b B, c C, d D) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d))\n\t}\n\n\treturn result\n}\n\n// ZipBy5 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy5[A any, B any, C any, D any, E any, Out any](a []A, b []B, c []C, d []D, e []E, iteratee func(a A, b B, c C, d D, e E) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d, _e))\n\t}\n\n\treturn result\n}\n\n// ZipBy6 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy6[A any, B any, C any, D any, E any, F any, Out any](a []A, b []B, c []C, d []D, e []E, f []F, iteratee func(a A, b B, c C, d D, e E, f F) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d, _e, _f))\n\t}\n\n\treturn result\n}\n\n// ZipBy7 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy7[A any, B any, C any\n```\n# Function Signature: func Zip9[A, B, C, D, E, F, G, H, I any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H, i []I) []Tuple9[A, B, C, D, E, F, G, H, I] {\n# Natural Language Annotations: // Zip9 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc Zip9[A, B, C, D, E, F, G, H, I any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H, i []I) []Tuple9[A, B, C, D, E, F, G, H, I] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g), len(h), len(i)})\n\n\tresult := make([]Tuple9[A, B, C, D, E, F, G, H, I], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\t\t_h, _ := Nth(h, index)\n\t\t_i, _ := Nth(i, index)\n\n\t\tresult = append(result, Tuple9[A, B, C, D, E, F, G, H, I]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t\tG: _g,\n\t\t\tH: _h,\n\t\t\tI: _i,\n\t\t})\n\t}\n\n\treturn result\n}\n```", "  ```\nfunc Zip9[A, B, C, D, E, F, G, H, I any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H, i []I) []Tuple9[A, B, C, D, E, F, G, H, I] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g), len(h), len(i)})\n\n\tresult := make([]Tuple9[A, B, C, D, E, F, G, H, I], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\t\t_h, _ := Nth(h, index)\n\t\t_i, _ := Nth(i, index)\n\n\t\tresult = append(result, Tuple9[A, B, C, D, E, F, G, H, I]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t\tG: _g,\n\t\t\tH: _h,\n\t\t\tI: _i,\n\t\t})\n\t}\n\n\treturn result\n}\n```", "  ```\npackage lo\n\nfunc Zip9[A, B, C, D, E, F, G, H, I any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H, i []I) []Tuple9[A, B, C, D, E, F, G, H, I] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g), len(h), len(i)})\n\n\tresult := make([]Tuple9[A, B, C, D, E, F, G, H, I], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\t\t_h, _ := Nth(h, index)\n\t\t_i, _ := Nth(i, index)\n\n\t\tresult = append(result, Tuple9[A, B, C, D, E, F, G, H, I]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t\tG: _g,\n\t\t\tH: _h,\n\t\t\tI: _i,\n\t\t})\n\t}\n\n\treturn result\n}\n```"]}, "lo-tuples.go-ZipBy9": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/tuples.go:\n```\npackage lo\n\n// T2 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T2[A, B any](a A, b B) Tuple2[A, B] {\n\treturn Tuple2[A, B]{A: a, B: b}\n}\n\n// T3 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T3[A, B, C any](a A, b B, c C) Tuple3[A, B, C] {\n\treturn Tuple3[A, B, C]{A: a, B: b, C: c}\n}\n\n// T4 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T4[A, B, C, D any](a A, b B, c C, d D) Tuple4[A, B, C, D] {\n\treturn Tuple4[A, B, C, D]{A: a, B: b, C: c, D: d}\n}\n\n// T5 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T5[A, B, C, D, E any](a A, b B, c C, d D, e E) Tuple5[A, B, C, D, E] {\n\treturn Tuple5[A, B, C, D, E]{A: a, B: b, C: c, D: d, E: e}\n}\n\n// T6 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T6[A, B, C, D, E, F any](a A, b B, c C, d D, e E, f F) Tuple6[A, B, C, D, E, F] {\n\treturn Tuple6[A, B, C, D, E, F]{A: a, B: b, C: c, D: d, E: e, F: f}\n}\n\n// T7 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T7[A, B, C, D, E, F, G any](a A, b B, c C, d D, e E, f F, g G) Tuple7[A, B, C, D, E, F, G] {\n\treturn Tuple7[A, B, C, D, E, F, G]{A: a, B: b, C: c, D: d, E: e, F: f, G: g}\n}\n\n// T8 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T8[A, B, C, D, E, F, G, H any](a A, b B, c C, d D, e E, f F, g G, h H) Tuple8[A, B, C, D, E, F, G, H] {\n\treturn Tuple8[A, B, C, D, E, F, G, H]{A: a, B: b, C: c, D: d, E: e, F: f, G: g, H: h}\n}\n\n// T9 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T9[A, B, C, D, E, F, G, H, I any](a A, b B, c C, d D, e E, f F, g G, h H, i I) Tuple9[A, B, C, D, E, F, G, H, I] {\n\treturn Tuple9[A, B, C, D, E, F, G, H, I]{A: a, B: b, C: c, D: d, E: e, F: f, G: g, H: h, I: i}\n}\n\n// Unpack2 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack2[A, B any](tuple Tuple2[A, B]) (A, B) {\n\treturn tuple.A, tuple.B\n}\n\n// Unpack3 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack3[A, B, C any](tuple Tuple3[A, B, C]) (A, B, C) {\n\treturn tuple.A, tuple.B, tuple.C\n}\n\n// Unpack4 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack4[A, B, C, D any](tuple Tuple4[A, B, C, D]) (A, B, C, D) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D\n}\n\n// Unpack5 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack5[A, B, C, D, E any](tuple Tuple5[A, B, C, D, E]) (A, B, C, D, E) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E\n}\n\n// Unpack6 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack6[A, B, C, D, E, F any](tuple Tuple6[A, B, C, D, E, F]) (A, B, C, D, E, F) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F\n}\n\n// Unpack7 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack7[A, B, C, D, E, F, G any](tuple Tuple7[A, B, C, D, E, F, G]) (A, B, C, D, E, F, G) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F, tuple.G\n}\n\n// Unpack8 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack8[A, B, C, D, E, F, G, H any](tuple Tuple8[A, B, C, D, E, F, G, H]) (A, B, C, D, E, F, G, H) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F, tuple.G, tuple.H\n}\n\n// Unpack9 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack9[A, B, C, D, E, F, G, H, I any](tuple Tuple9[A, B, C, D, E, F, G, H, I]) (A, B, C, D, E, F, G, H, I) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F, tuple.G, tuple.H, tuple.I\n}\n\n// Zip2 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip2[A, B any](a []A, b []B) []Tuple2[A, B] {\n\tsize := Max([]int{len(a), len(b)})\n\n\tresult := make([]Tuple2[A, B], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\n\t\tresult = append(result, Tuple2[A, B]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip3 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip3[A, B, C any](a []A, b []B, c []C) []Tuple3[A, B, C] {\n\tsize := Max([]int{len(a), len(b), len(c)})\n\n\tresult := make([]Tuple3[A, B, C], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\n\t\tresult = append(result, Tuple3[A, B, C]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip4 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip4[A, B, C, D any](a []A, b []B, c []C, d []D) []Tuple4[A, B, C, D] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d)})\n\n\tresult := make([]Tuple4[A, B, C, D], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\n\t\tresult = append(result, Tuple4[A, B, C, D]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip5 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip5[A, B, C, D, E any](a []A, b []B, c []C, d []D, e []E) []Tuple5[A, B, C, D, E] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e)})\n\n\tresult := make([]Tuple5[A, B, C, D, E], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\n\t\tresult = append(result, Tuple5[A, B, C, D, E]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip6 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip6[A, B, C, D, E, F any](a []A, b []B, c []C, d []D, e []E, f []F) []Tuple6[A, B, C, D, E, F] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f)})\n\n\tresult := make([]Tuple6[A, B, C, D, E, F], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\n\t\tresult = append(result, Tuple6[A, B, C, D, E, F]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip7 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip7[A, B, C, D, E, F, G any](a []A, b []B, c []C, d []D, e []E, f []F, g []G) []Tuple7[A, B, C, D, E, F, G] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g)})\n\n\tresult := make([]Tuple7[A, B, C, D, E, F, G], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\n\t\tresult = append(result, Tuple7[A, B, C, D, E, F, G]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t\tG: _g,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip8 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip8[A, B, C, D, E, F, G, H any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H) []Tuple8[A, B, C, D, E, F, G, H] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g), len(h)})\n\n\tresult := make([]Tuple8[A, B, C, D, E, F, G, H], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\t\t_h, _ := Nth(h, index)\n\n\t\tresult = append(result, Tuple8[A, B, C, D, E, F, G, H]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t\tG: _g,\n\t\t\tH: _h,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip9 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ZipBy2 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy2[A any, B any, Out any](a []A, b []B, iteratee func(a A, b B) Out) []Out {\n\tsize := Max([]int{len(a), len(b)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\n\t\tresult = append(result, iteratee(_a, _b))\n\t}\n\n\treturn result\n}\n\n// ZipBy3 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy3[A any, B any, C any, Out any](a []A, b []B, c []C, iteratee func(a A, b B, c C) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c))\n\t}\n\n\treturn result\n}\n\n// ZipBy4 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy4[A any, B any, C any, D any, Out any](a []A, b []B, c []C, d []D, iteratee func(a A, b B, c C, d D) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d))\n\t}\n\n\treturn result\n}\n\n// ZipBy5 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy5[A any, B any, C any, D any, E any, Out any](a []A, b []B, c []C, d []D, e []E, iteratee func(a A, b B, c C, d D, e E) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d, _e))\n\t}\n\n\treturn result\n}\n\n// ZipBy6 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy6[A any, B any, C any, D any, E any, F any, Out any](a []A, b []B, c []C, d []D, e []E, f []F, iteratee func(a A, b B, c C, d D, e E, f F) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d, _e, _f))\n\t}\n\n\treturn result\n}\n\n// ZipBy7 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy7[A any, B any, C any\n```\n# Function Signature: func ZipBy9[A any, B any, C any, D any, E any, F any, G any, H any, I any, Out any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H, i []I, iteratee func(a A, b B, c C, d D, e E, f F, g G, h H, i I) Out) []Out {\n# Natural Language Annotations: // ZipBy9 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc ZipBy9[A any, B any, C any, D any, E any, F any, G any, H any, I any, Out any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H, i []I, iteratee func(a A, b B, c C, d D, e E, f F, g G, h H, i I) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g), len(h), len(i)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\t\t_h, _ := Nth(h, index)\n\t\t_i, _ := Nth(i, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d, _e, _f, _g, _h, _i))\n\t}\n\n\treturn result\n}\n```", "  ```\nfunc ZipBy9[A any, B any, C any, D any, E any, F any, G any, H any, I any, Out any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H, i []I, iteratee func(a A, b B, c C, d D, e E, f F, g G, h H, i I) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g), len(h), len(i)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\t\t_h, _ := Nth(h, index)\n\t\t_i, _ := Nth(i, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d, _e, _f, _g, _h, _i))\n\t}\n\n\treturn result\n}\n```", "  ```\nfunc ZipBy9[A any, B any, C any, D any, E any, F any, G any, H any, I any, Out any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H, i []I, iteratee func(a A, b B, c C, d D, e E, f F, g G, h H, i I) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g), len(h), len(i)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\t\t_h, _ := Nth(h, index)\n\t\t_i, _ := Nth(i, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d, _e, _f, _g, _h, _i))\n\t}\n\n\treturn result\n}\n```"]}, "lo-channel.go-ChannelDispatcher": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/channel.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/rand\"\n)\n\ntype DispatchingStrategy[T any] func(msg T, index uint64, channels []<-chan T) int\n\n// ChannelDispatcher distributes messages from input channels into N child channels.\n// Close events are propagated to children.\n// Underlying channels can have a fixed buffer capacity or be unbuffered when cap is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc createChannels[T any](count int, channelBufferCap int) []chan T {\n\tchildren := make([]chan T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tchildren = append(children, make(chan T, channelBufferCap))\n\t}\n\n\treturn children\n}\n\nfunc channelsToReadOnly[T any](children []chan T) []<-chan T {\n\troChildren := make([]<-chan T, 0, len(children))\n\n\tfor i := range children {\n\t\troChildren = append(roChildren, children[i])\n\t}\n\n\treturn roChildren\n}\n\nfunc closeChannels[T any](children []chan T) {\n\tfor i := 0; i < len(children); i++ {\n\t\tclose(children[i])\n\t}\n}\n\nfunc channelIsNotFull[T any](ch <-chan T) bool {\n\treturn cap(ch) == 0 || len(ch) < cap(ch)\n}\n\n// DispatchingStrategyRoundRobin distributes messages in a rotating sequential manner.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyRandom distributes messages in a random manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyWeightedRandom distributes messages in a weighted manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\nfunc DispatchingStrategyWeightedRandom[T any](weights []int) DispatchingStrategy[T] {\n\tseq := []int{}\n\n\tfor i := 0; i < len(weights); i++ {\n\t\tfor j := 0; j < weights[i]; j++ {\n\t\t\tseq = append(seq, i)\n\t\t}\n\t}\n\n\treturn func(msg T, index uint64, channels []<-chan T) int {\n\t\tfor {\n\t\t\ti := seq[rand.IntN(len(seq))]\n\t\t\tif channelIsNotFull(channels[i]) {\n\t\t\t\treturn i\n\t\t\t}\n\n\t\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t\t}\n\t}\n}\n\n// DispatchingStrategyFirst distributes messages in the first non-full channel.\n// If the capacity of the first channel is exceeded, the second channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyLeast distributes messages in the emptiest channel.\nfunc DispatchingStrategyLeast[T any](msg T, index uint64, channels []<-chan T) int {\n\tseq := Range(len(channels))\n\n\treturn MinBy(seq, func(item int, min int) bool {\n\t\treturn len(channels[item]) < len(channels[min])\n\t})\n}\n\n// DispatchingStrategyMost distributes messages in the fullest channel.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n// SliceToChannel returns a read-only channels of collection elements.\nfunc SliceToChannel[T any](bufferSize int, collection []T) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\tfor i := range collection {\n\t\t\tch <- collection[i]\n\t\t}\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// ChannelToSlice returns a slice built from channels items. Blocks until channel closes.\nfunc ChannelToSlice[T any](ch <-chan T) []T {\n\tcollection := []T{}\n\n\tfor item := range ch {\n\t\tcollection = append(collection, item)\n\t}\n\n\treturn collection\n}\n\n// Generator implements the generator design pattern.\nfunc Generator[T any](bufferSize int, generator func(yield func(T))) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\t// WARNING: infinite loop\n\t\tgenerator(func(t T) {\n\t\t\tch <- t\n\t\t})\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// Buffer creates a slice of n elements from a channel. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc Buffer[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\titem, ok := <-ch\n\t\tif !ok {\n\t\t\treturn buffer, index, time.Since(now), false\n\t\t}\n\n\t\tbuffer = append(buffer, item)\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// Batch creates a slice of n elements from a channel. Returns the slice and the slice length.\n//\n// Deprecated: Use [Buffer] instead.\nfunc Batch[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn Buffer(ch, size)\n}\n\n// BufferWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc BufferWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\texpire := time.NewTimer(timeout)\n\tdefer expire.Stop()\n\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\tselect {\n\t\tcase item, ok := <-ch:\n\t\t\tif !ok {\n\t\t\t\treturn buffer, index, time.Since(now), false\n\t\t\t}\n\n\t\t\tbuffer = append(buffer, item)\n\n\t\tcase <-expire.C:\n\t\t\treturn buffer, index, time.Since(now), true\n\t\t}\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// BatchWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n//\n// Deprecated: Use [BufferWithTimeout] instead.\nfunc BatchWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn BufferWithTimeout(ch, size, timeout)\n}\n\n// FanIn collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\nfunc FanIn[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\tout := make(chan T, channelBufferCap)\n\tvar wg sync.WaitGroup\n\n\t// Start an output goroutine for each input channel in upstreams.\n\twg.Add(len(upstreams))\n\tfor i := range upstreams {\n\t\tgo func(index int) {\n\t\t\tfor n := range upstreams[index] {\n\t\t\t\tout <- n\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(i)\n\t}\n\n\t// Start a goroutine to close out once all the output goroutines are done.\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(out)\n\t}()\n\treturn out\n}\n\n// ChannelMerge collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\n//\n// Deprecated: Use [FanIn] instead.\nfunc ChannelMerge[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\treturn FanIn(channelBufferCap, upstreams...)\n}\n\n// FanOut broadcasts all the upstream messages to multiple downstream channels.\n// When upstream channel reach EOF, downstream channels close. If any downstream\n// channels is full, broadcasting is paused.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func ChannelDispatcher[T any](stream <-chan T, count int, channelBufferCap int, strategy DispatchingStrategy[T]) []<-chan T {\n# Natural Language Annotations: // ChannelDispatcher distributes messages from input channels into N child channels.\n// Close events are propagated to children.\n// Underlying channels can have a fixed buffer capacity or be unbuffered when cap is 0.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc ChannelDispatcher[T any](stream <-chan T, count int, channelBufferCap int, strategy DispatchingStrategy[T]) []<-chan T {\n\tchildren := createChannels[T](count, channelBufferCap)\n\n\tgo func() {\n\t\tfor msg := range stream {\n\t\t\ti := strategy(msg, 0, children)\n\t\t\tchildren[i] <- msg\n\t\t}\n\n\t\tfor i := range children {\n\t\t\tclose(children[i])\n\t\t}\n\t}()\n\n\treturn channelsToReadOnly[T](children)\n}\n```", "  ```\nfunc ChannelDispatcher[T any](stream <-chan T, count int, channelBufferCap int, strategy DispatchingStrategy[T]) []<-chan T {\n\tchildren := createChannels[T](count, channelBufferCap)\n\n\tgo func() {\n\t\tfor msg := range stream {\n\t\t\ti := strategy(msg, rand.Uint64(), children)\n\t\t\tchildren[i] <- msg\n\t\t}\n\n\t\tfor i := range children {\n\t\t\tclose(children[i])\n\t\t}\n\t}()\n\n\treturn channelsToReadOnly[T](children)\n}\n```", "  ```\nfunc ChannelDispatcher[T any](stream <-chan T, count int, channelBufferCap int, strategy DispatchingStrategy[T]) []<-chan T {\n\tchildren := make([]chan T, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tchildren[i] = make(chan T, channelBufferCap)\n\t}\n\n\tfor msg := range stream {\n\t\tx := strategy(msg, uint64(i), children)\n\t\tchildren[x] <- msg\n\t}\n\n\tfor i := 0; i < count; i++ {\n\t\tclose(children[i])\n\t}\n\n\treturn channelsToReadOnly(children)\n}\n```"]}, "lo-channel.go-DispatchingStrategyRoundRobin": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/channel.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/rand\"\n)\n\ntype DispatchingStrategy[T any] func(msg T, index uint64, channels []<-chan T) int\n\n// ChannelDispatcher distributes messages from input channels into N child channels.\n// Close events are propagated to children.\n// Underlying channels can have a fixed buffer capacity or be unbuffered when cap is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc createChannels[T any](count int, channelBufferCap int) []chan T {\n\tchildren := make([]chan T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tchildren = append(children, make(chan T, channelBufferCap))\n\t}\n\n\treturn children\n}\n\nfunc channelsToReadOnly[T any](children []chan T) []<-chan T {\n\troChildren := make([]<-chan T, 0, len(children))\n\n\tfor i := range children {\n\t\troChildren = append(roChildren, children[i])\n\t}\n\n\treturn roChildren\n}\n\nfunc closeChannels[T any](children []chan T) {\n\tfor i := 0; i < len(children); i++ {\n\t\tclose(children[i])\n\t}\n}\n\nfunc channelIsNotFull[T any](ch <-chan T) bool {\n\treturn cap(ch) == 0 || len(ch) < cap(ch)\n}\n\n// DispatchingStrategyRoundRobin distributes messages in a rotating sequential manner.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyRandom distributes messages in a random manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyWeightedRandom distributes messages in a weighted manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\nfunc DispatchingStrategyWeightedRandom[T any](weights []int) DispatchingStrategy[T] {\n\tseq := []int{}\n\n\tfor i := 0; i < len(weights); i++ {\n\t\tfor j := 0; j < weights[i]; j++ {\n\t\t\tseq = append(seq, i)\n\t\t}\n\t}\n\n\treturn func(msg T, index uint64, channels []<-chan T) int {\n\t\tfor {\n\t\t\ti := seq[rand.IntN(len(seq))]\n\t\t\tif channelIsNotFull(channels[i]) {\n\t\t\t\treturn i\n\t\t\t}\n\n\t\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t\t}\n\t}\n}\n\n// DispatchingStrategyFirst distributes messages in the first non-full channel.\n// If the capacity of the first channel is exceeded, the second channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyLeast distributes messages in the emptiest channel.\nfunc DispatchingStrategyLeast[T any](msg T, index uint64, channels []<-chan T) int {\n\tseq := Range(len(channels))\n\n\treturn MinBy(seq, func(item int, min int) bool {\n\t\treturn len(channels[item]) < len(channels[min])\n\t})\n}\n\n// DispatchingStrategyMost distributes messages in the fullest channel.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n// SliceToChannel returns a read-only channels of collection elements.\nfunc SliceToChannel[T any](bufferSize int, collection []T) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\tfor i := range collection {\n\t\t\tch <- collection[i]\n\t\t}\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// ChannelToSlice returns a slice built from channels items. Blocks until channel closes.\nfunc ChannelToSlice[T any](ch <-chan T) []T {\n\tcollection := []T{}\n\n\tfor item := range ch {\n\t\tcollection = append(collection, item)\n\t}\n\n\treturn collection\n}\n\n// Generator implements the generator design pattern.\nfunc Generator[T any](bufferSize int, generator func(yield func(T))) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\t// WARNING: infinite loop\n\t\tgenerator(func(t T) {\n\t\t\tch <- t\n\t\t})\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// Buffer creates a slice of n elements from a channel. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc Buffer[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\titem, ok := <-ch\n\t\tif !ok {\n\t\t\treturn buffer, index, time.Since(now), false\n\t\t}\n\n\t\tbuffer = append(buffer, item)\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// Batch creates a slice of n elements from a channel. Returns the slice and the slice length.\n//\n// Deprecated: Use [Buffer] instead.\nfunc Batch[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn Buffer(ch, size)\n}\n\n// BufferWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc BufferWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\texpire := time.NewTimer(timeout)\n\tdefer expire.Stop()\n\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\tselect {\n\t\tcase item, ok := <-ch:\n\t\t\tif !ok {\n\t\t\t\treturn buffer, index, time.Since(now), false\n\t\t\t}\n\n\t\t\tbuffer = append(buffer, item)\n\n\t\tcase <-expire.C:\n\t\t\treturn buffer, index, time.Since(now), true\n\t\t}\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// BatchWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n//\n// Deprecated: Use [BufferWithTimeout] instead.\nfunc BatchWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn BufferWithTimeout(ch, size, timeout)\n}\n\n// FanIn collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\nfunc FanIn[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\tout := make(chan T, channelBufferCap)\n\tvar wg sync.WaitGroup\n\n\t// Start an output goroutine for each input channel in upstreams.\n\twg.Add(len(upstreams))\n\tfor i := range upstreams {\n\t\tgo func(index int) {\n\t\t\tfor n := range upstreams[index] {\n\t\t\t\tout <- n\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(i)\n\t}\n\n\t// Start a goroutine to close out once all the output goroutines are done.\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(out)\n\t}()\n\treturn out\n}\n\n// ChannelMerge collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\n//\n// Deprecated: Use [FanIn] instead.\nfunc ChannelMerge[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\treturn FanIn(channelBufferCap, upstreams...)\n}\n\n// FanOut broadcasts all the upstream messages to multiple downstream channels.\n// When upstream channel reach EOF, downstream channels close. If any downstream\n// channels is full, broadcasting is paused.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func DispatchingStrategyRoundRobin[T any](msg T, index uint64, channels []<-chan T) int {\n# Natural Language Annotations: // DispatchingStrategyRoundRobin distributes messages in a rotating sequential manner.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc DispatchingStrategyRoundRobin[T any](msg T, index uint64, channels []<-chan T) int {\n\tfor {\n\t\ti := (index + 1) % uint64(len(channels))\n\t\tif channelIsNotFull(channels[i]) {\n\t\t\treturn int(i)\n\t\t}\n\n\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t}\n}\n```", "  ```\nfunc DispatchingStrategyRoundRobin[T any](msg T, index uint64, channels []<-chan T) int {\n\ti := int(index) % len(channels)\n\tif channelIsNotFull(channels[i]) {\n\t\treturn i\n\t}\n\n\t// If the current channel is full, move to the next channel in the sequence\n\tfor {\n\t\ti = (i + 1) % len(channels)\n\t\tif channelIsNotFull(channels[i]) {\n\t\t\treturn i\n\t\t}\n\t}\n}\n```", "  ```\nfunc DispatchingStrategyRoundRobin[T any](msg T, index uint64, channels []<-chan T) int {\n\tvar selectedChannel int\n\tfor {\n\t\tselectedChannel = index % uint64(len(channels))\n\t\tif channelIsNotFull(channels[selectedChannel]) {\n\t\t\tbreak\n\t\t}\n\t\tindex++\n\t}\n\treturn selectedChannel\n}\n\n```"]}, "lo-channel.go-DispatchingStrategyRandom": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/channel.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/rand\"\n)\n\ntype DispatchingStrategy[T any] func(msg T, index uint64, channels []<-chan T) int\n\n// ChannelDispatcher distributes messages from input channels into N child channels.\n// Close events are propagated to children.\n// Underlying channels can have a fixed buffer capacity or be unbuffered when cap is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc createChannels[T any](count int, channelBufferCap int) []chan T {\n\tchildren := make([]chan T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tchildren = append(children, make(chan T, channelBufferCap))\n\t}\n\n\treturn children\n}\n\nfunc channelsToReadOnly[T any](children []chan T) []<-chan T {\n\troChildren := make([]<-chan T, 0, len(children))\n\n\tfor i := range children {\n\t\troChildren = append(roChildren, children[i])\n\t}\n\n\treturn roChildren\n}\n\nfunc closeChannels[T any](children []chan T) {\n\tfor i := 0; i < len(children); i++ {\n\t\tclose(children[i])\n\t}\n}\n\nfunc channelIsNotFull[T any](ch <-chan T) bool {\n\treturn cap(ch) == 0 || len(ch) < cap(ch)\n}\n\n// DispatchingStrategyRoundRobin distributes messages in a rotating sequential manner.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyRandom distributes messages in a random manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyWeightedRandom distributes messages in a weighted manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\nfunc DispatchingStrategyWeightedRandom[T any](weights []int) DispatchingStrategy[T] {\n\tseq := []int{}\n\n\tfor i := 0; i < len(weights); i++ {\n\t\tfor j := 0; j < weights[i]; j++ {\n\t\t\tseq = append(seq, i)\n\t\t}\n\t}\n\n\treturn func(msg T, index uint64, channels []<-chan T) int {\n\t\tfor {\n\t\t\ti := seq[rand.IntN(len(seq))]\n\t\t\tif channelIsNotFull(channels[i]) {\n\t\t\t\treturn i\n\t\t\t}\n\n\t\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t\t}\n\t}\n}\n\n// DispatchingStrategyFirst distributes messages in the first non-full channel.\n// If the capacity of the first channel is exceeded, the second channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyLeast distributes messages in the emptiest channel.\nfunc DispatchingStrategyLeast[T any](msg T, index uint64, channels []<-chan T) int {\n\tseq := Range(len(channels))\n\n\treturn MinBy(seq, func(item int, min int) bool {\n\t\treturn len(channels[item]) < len(channels[min])\n\t})\n}\n\n// DispatchingStrategyMost distributes messages in the fullest channel.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n// SliceToChannel returns a read-only channels of collection elements.\nfunc SliceToChannel[T any](bufferSize int, collection []T) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\tfor i := range collection {\n\t\t\tch <- collection[i]\n\t\t}\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// ChannelToSlice returns a slice built from channels items. Blocks until channel closes.\nfunc ChannelToSlice[T any](ch <-chan T) []T {\n\tcollection := []T{}\n\n\tfor item := range ch {\n\t\tcollection = append(collection, item)\n\t}\n\n\treturn collection\n}\n\n// Generator implements the generator design pattern.\nfunc Generator[T any](bufferSize int, generator func(yield func(T))) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\t// WARNING: infinite loop\n\t\tgenerator(func(t T) {\n\t\t\tch <- t\n\t\t})\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// Buffer creates a slice of n elements from a channel. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc Buffer[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\titem, ok := <-ch\n\t\tif !ok {\n\t\t\treturn buffer, index, time.Since(now), false\n\t\t}\n\n\t\tbuffer = append(buffer, item)\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// Batch creates a slice of n elements from a channel. Returns the slice and the slice length.\n//\n// Deprecated: Use [Buffer] instead.\nfunc Batch[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn Buffer(ch, size)\n}\n\n// BufferWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc BufferWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\texpire := time.NewTimer(timeout)\n\tdefer expire.Stop()\n\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\tselect {\n\t\tcase item, ok := <-ch:\n\t\t\tif !ok {\n\t\t\t\treturn buffer, index, time.Since(now), false\n\t\t\t}\n\n\t\t\tbuffer = append(buffer, item)\n\n\t\tcase <-expire.C:\n\t\t\treturn buffer, index, time.Since(now), true\n\t\t}\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// BatchWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n//\n// Deprecated: Use [BufferWithTimeout] instead.\nfunc BatchWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn BufferWithTimeout(ch, size, timeout)\n}\n\n// FanIn collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\nfunc FanIn[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\tout := make(chan T, channelBufferCap)\n\tvar wg sync.WaitGroup\n\n\t// Start an output goroutine for each input channel in upstreams.\n\twg.Add(len(upstreams))\n\tfor i := range upstreams {\n\t\tgo func(index int) {\n\t\t\tfor n := range upstreams[index] {\n\t\t\t\tout <- n\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(i)\n\t}\n\n\t// Start a goroutine to close out once all the output goroutines are done.\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(out)\n\t}()\n\treturn out\n}\n\n// ChannelMerge collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\n//\n// Deprecated: Use [FanIn] instead.\nfunc ChannelMerge[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\treturn FanIn(channelBufferCap, upstreams...)\n}\n\n// FanOut broadcasts all the upstream messages to multiple downstream channels.\n// When upstream channel reach EOF, downstream channels close. If any downstream\n// channels is full, broadcasting is paused.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func DispatchingStrategyRandom[T any](msg T, index uint64, channels []<-chan T) int {\n# Natural Language Annotations: // DispatchingStrategyRandom distributes messages in a random manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc DispatchingStrategyRandom[T any](msg T, index uint64, channels []<-chan T) int {\n\tfor {\n\t\ti := rand.IntN(len(channels))\n\t\tif channelIsNotFull(channels[i]) {\n\t\t\treturn i\n\t\t}\n\n\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t}\n}\n```", "  ```\nfunc DispatchingStrategyRandom[T any](msg T, index uint64, channels []<-chan T) int {\n\tfor {\n\t\ti := rand.IntN(len(channels))\n\t\tif channelIsNotFull(channels[i]) {\n\t\t\treturn i\n\t\t}\n\n\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t}\n}\n```", "  ```\nfunc DispatchingStrategyRandom[T any](msg T, index uint64, channels []<-chan T) int {\n\treturn rand.IntN(len(channels)) // return a random index of a channel\n}\n```"]}, "lo-channel.go-DispatchingStrategyFirst": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/channel.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/rand\"\n)\n\ntype DispatchingStrategy[T any] func(msg T, index uint64, channels []<-chan T) int\n\n// ChannelDispatcher distributes messages from input channels into N child channels.\n// Close events are propagated to children.\n// Underlying channels can have a fixed buffer capacity or be unbuffered when cap is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc createChannels[T any](count int, channelBufferCap int) []chan T {\n\tchildren := make([]chan T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tchildren = append(children, make(chan T, channelBufferCap))\n\t}\n\n\treturn children\n}\n\nfunc channelsToReadOnly[T any](children []chan T) []<-chan T {\n\troChildren := make([]<-chan T, 0, len(children))\n\n\tfor i := range children {\n\t\troChildren = append(roChildren, children[i])\n\t}\n\n\treturn roChildren\n}\n\nfunc closeChannels[T any](children []chan T) {\n\tfor i := 0; i < len(children); i++ {\n\t\tclose(children[i])\n\t}\n}\n\nfunc channelIsNotFull[T any](ch <-chan T) bool {\n\treturn cap(ch) == 0 || len(ch) < cap(ch)\n}\n\n// DispatchingStrategyRoundRobin distributes messages in a rotating sequential manner.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyRandom distributes messages in a random manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyWeightedRandom distributes messages in a weighted manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\nfunc DispatchingStrategyWeightedRandom[T any](weights []int) DispatchingStrategy[T] {\n\tseq := []int{}\n\n\tfor i := 0; i < len(weights); i++ {\n\t\tfor j := 0; j < weights[i]; j++ {\n\t\t\tseq = append(seq, i)\n\t\t}\n\t}\n\n\treturn func(msg T, index uint64, channels []<-chan T) int {\n\t\tfor {\n\t\t\ti := seq[rand.IntN(len(seq))]\n\t\t\tif channelIsNotFull(channels[i]) {\n\t\t\t\treturn i\n\t\t\t}\n\n\t\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t\t}\n\t}\n}\n\n// DispatchingStrategyFirst distributes messages in the first non-full channel.\n// If the capacity of the first channel is exceeded, the second channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyLeast distributes messages in the emptiest channel.\nfunc DispatchingStrategyLeast[T any](msg T, index uint64, channels []<-chan T) int {\n\tseq := Range(len(channels))\n\n\treturn MinBy(seq, func(item int, min int) bool {\n\t\treturn len(channels[item]) < len(channels[min])\n\t})\n}\n\n// DispatchingStrategyMost distributes messages in the fullest channel.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n// SliceToChannel returns a read-only channels of collection elements.\nfunc SliceToChannel[T any](bufferSize int, collection []T) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\tfor i := range collection {\n\t\t\tch <- collection[i]\n\t\t}\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// ChannelToSlice returns a slice built from channels items. Blocks until channel closes.\nfunc ChannelToSlice[T any](ch <-chan T) []T {\n\tcollection := []T{}\n\n\tfor item := range ch {\n\t\tcollection = append(collection, item)\n\t}\n\n\treturn collection\n}\n\n// Generator implements the generator design pattern.\nfunc Generator[T any](bufferSize int, generator func(yield func(T))) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\t// WARNING: infinite loop\n\t\tgenerator(func(t T) {\n\t\t\tch <- t\n\t\t})\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// Buffer creates a slice of n elements from a channel. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc Buffer[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\titem, ok := <-ch\n\t\tif !ok {\n\t\t\treturn buffer, index, time.Since(now), false\n\t\t}\n\n\t\tbuffer = append(buffer, item)\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// Batch creates a slice of n elements from a channel. Returns the slice and the slice length.\n//\n// Deprecated: Use [Buffer] instead.\nfunc Batch[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn Buffer(ch, size)\n}\n\n// BufferWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc BufferWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\texpire := time.NewTimer(timeout)\n\tdefer expire.Stop()\n\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\tselect {\n\t\tcase item, ok := <-ch:\n\t\t\tif !ok {\n\t\t\t\treturn buffer, index, time.Since(now), false\n\t\t\t}\n\n\t\t\tbuffer = append(buffer, item)\n\n\t\tcase <-expire.C:\n\t\t\treturn buffer, index, time.Since(now), true\n\t\t}\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// BatchWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n//\n// Deprecated: Use [BufferWithTimeout] instead.\nfunc BatchWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn BufferWithTimeout(ch, size, timeout)\n}\n\n// FanIn collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\nfunc FanIn[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\tout := make(chan T, channelBufferCap)\n\tvar wg sync.WaitGroup\n\n\t// Start an output goroutine for each input channel in upstreams.\n\twg.Add(len(upstreams))\n\tfor i := range upstreams {\n\t\tgo func(index int) {\n\t\t\tfor n := range upstreams[index] {\n\t\t\t\tout <- n\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(i)\n\t}\n\n\t// Start a goroutine to close out once all the output goroutines are done.\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(out)\n\t}()\n\treturn out\n}\n\n// ChannelMerge collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\n//\n// Deprecated: Use [FanIn] instead.\nfunc ChannelMerge[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\treturn FanIn(channelBufferCap, upstreams...)\n}\n\n// FanOut broadcasts all the upstream messages to multiple downstream channels.\n// When upstream channel reach EOF, downstream channels close. If any downstream\n// channels is full, broadcasting is paused.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func DispatchingStrategyFirst[T any](msg T, index uint64, channels []<-chan T) int {\n# Natural Language Annotations: // DispatchingStrategyFirst distributes messages in the first non-full channel.\n// If the capacity of the first channel is exceeded, the second channel will be selected and so on.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc DispatchingStrategyFirst[T any](msg T, index uint64, channels []<-chan T) int {\n\tfor i := 0; i < len(channels); i++ {\n\t\tif channelIsNotFull(channels[i]) {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n```", "  ```\nfunc DispatchingStrategyFirst[T any](msg T, index uint64, channels []<-chan T) int {\n\tfor i := 0; i < len(channels); i++ {\n\t\tif channelIsNotFull(channels[i]) {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n```", "  ```\nfunc DispatchingStrategyFirst[T any](msg T, index uint64, channels []<-chan T) int {\n\t// DispatchingStrategyFirst distributes messages in the first non-full channel\n\t// If the capacity of the first channel is exceeded, the second channel will be selected and so on.\n\tfor i, ch := range channels {\n\t\t// If current channel is not full, return its index\n\t\tif channelIsNotFull(ch) {\n\t\t\treturn i\n\t\t}\n\t}\n\n\t// If all channels are full, sleep for 10 microseconds to prevent CPU from burning\n\ttime.Sleep(10 * time.Microsecond)\n\n\treturn -1\n}\n```"]}, "lo-channel.go-DispatchingStrategyMost": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/channel.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/rand\"\n)\n\ntype DispatchingStrategy[T any] func(msg T, index uint64, channels []<-chan T) int\n\n// ChannelDispatcher distributes messages from input channels into N child channels.\n// Close events are propagated to children.\n// Underlying channels can have a fixed buffer capacity or be unbuffered when cap is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc createChannels[T any](count int, channelBufferCap int) []chan T {\n\tchildren := make([]chan T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tchildren = append(children, make(chan T, channelBufferCap))\n\t}\n\n\treturn children\n}\n\nfunc channelsToReadOnly[T any](children []chan T) []<-chan T {\n\troChildren := make([]<-chan T, 0, len(children))\n\n\tfor i := range children {\n\t\troChildren = append(roChildren, children[i])\n\t}\n\n\treturn roChildren\n}\n\nfunc closeChannels[T any](children []chan T) {\n\tfor i := 0; i < len(children); i++ {\n\t\tclose(children[i])\n\t}\n}\n\nfunc channelIsNotFull[T any](ch <-chan T) bool {\n\treturn cap(ch) == 0 || len(ch) < cap(ch)\n}\n\n// DispatchingStrategyRoundRobin distributes messages in a rotating sequential manner.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyRandom distributes messages in a random manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyWeightedRandom distributes messages in a weighted manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\nfunc DispatchingStrategyWeightedRandom[T any](weights []int) DispatchingStrategy[T] {\n\tseq := []int{}\n\n\tfor i := 0; i < len(weights); i++ {\n\t\tfor j := 0; j < weights[i]; j++ {\n\t\t\tseq = append(seq, i)\n\t\t}\n\t}\n\n\treturn func(msg T, index uint64, channels []<-chan T) int {\n\t\tfor {\n\t\t\ti := seq[rand.IntN(len(seq))]\n\t\t\tif channelIsNotFull(channels[i]) {\n\t\t\t\treturn i\n\t\t\t}\n\n\t\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t\t}\n\t}\n}\n\n// DispatchingStrategyFirst distributes messages in the first non-full channel.\n// If the capacity of the first channel is exceeded, the second channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyLeast distributes messages in the emptiest channel.\nfunc DispatchingStrategyLeast[T any](msg T, index uint64, channels []<-chan T) int {\n\tseq := Range(len(channels))\n\n\treturn MinBy(seq, func(item int, min int) bool {\n\t\treturn len(channels[item]) < len(channels[min])\n\t})\n}\n\n// DispatchingStrategyMost distributes messages in the fullest channel.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n// SliceToChannel returns a read-only channels of collection elements.\nfunc SliceToChannel[T any](bufferSize int, collection []T) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\tfor i := range collection {\n\t\t\tch <- collection[i]\n\t\t}\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// ChannelToSlice returns a slice built from channels items. Blocks until channel closes.\nfunc ChannelToSlice[T any](ch <-chan T) []T {\n\tcollection := []T{}\n\n\tfor item := range ch {\n\t\tcollection = append(collection, item)\n\t}\n\n\treturn collection\n}\n\n// Generator implements the generator design pattern.\nfunc Generator[T any](bufferSize int, generator func(yield func(T))) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\t// WARNING: infinite loop\n\t\tgenerator(func(t T) {\n\t\t\tch <- t\n\t\t})\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// Buffer creates a slice of n elements from a channel. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc Buffer[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\titem, ok := <-ch\n\t\tif !ok {\n\t\t\treturn buffer, index, time.Since(now), false\n\t\t}\n\n\t\tbuffer = append(buffer, item)\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// Batch creates a slice of n elements from a channel. Returns the slice and the slice length.\n//\n// Deprecated: Use [Buffer] instead.\nfunc Batch[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn Buffer(ch, size)\n}\n\n// BufferWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc BufferWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\texpire := time.NewTimer(timeout)\n\tdefer expire.Stop()\n\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\tselect {\n\t\tcase item, ok := <-ch:\n\t\t\tif !ok {\n\t\t\t\treturn buffer, index, time.Since(now), false\n\t\t\t}\n\n\t\t\tbuffer = append(buffer, item)\n\n\t\tcase <-expire.C:\n\t\t\treturn buffer, index, time.Since(now), true\n\t\t}\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// BatchWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n//\n// Deprecated: Use [BufferWithTimeout] instead.\nfunc BatchWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn BufferWithTimeout(ch, size, timeout)\n}\n\n// FanIn collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\nfunc FanIn[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\tout := make(chan T, channelBufferCap)\n\tvar wg sync.WaitGroup\n\n\t// Start an output goroutine for each input channel in upstreams.\n\twg.Add(len(upstreams))\n\tfor i := range upstreams {\n\t\tgo func(index int) {\n\t\t\tfor n := range upstreams[index] {\n\t\t\t\tout <- n\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(i)\n\t}\n\n\t// Start a goroutine to close out once all the output goroutines are done.\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(out)\n\t}()\n\treturn out\n}\n\n// ChannelMerge collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\n//\n// Deprecated: Use [FanIn] instead.\nfunc ChannelMerge[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\treturn FanIn(channelBufferCap, upstreams...)\n}\n\n// FanOut broadcasts all the upstream messages to multiple downstream channels.\n// When upstream channel reach EOF, downstream channels close. If any downstream\n// channels is full, broadcasting is paused.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func DispatchingStrategyMost[T any](msg T, index uint64, channels []<-chan T) int {\n# Natural Language Annotations: // DispatchingStrategyMost distributes messages in the fullest channel.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc DispatchingStrategyMost[T any](msg T, index uint64, channels []<-chan T) int {\n\tvar mostFullChannelIndex int\n\tvar mostFullChannelCapacity int\n\n\tfor i := 0; i < len(channels); i++ {\n\t\tcapacity := cap(channels[i])\n\t\tif capacity > mostFullChannelCapacity {\n\t\t\tmostFullChannelIndex = i\n\t\t\tmostFullChannelCapacity = capacity\n\t\t}\n\t}\n\n\treturn mostFullChannelIndex\n}\n```", "  ```\npackage lo\n\nfunc DispatchingStrategyMost[T any](msg T, index uint64, channels []<-chan T) int {\n\ti := 0\n\tfor j := 1; j < len(channels); j++ {\n\t\tif len(channels[j]) > len(channels[i]) {\n\t\t\ti = j\n\t\t}\n\t}\n\treturn i\n}\n```", "  ```\nfunc DispatchingStrategyMost[T any](msg T, index uint64, channels []<-chan T) int {\n\tvar fullChannelIndex int\n\tvar fullChannelCapacity int = 0\n\n\t// Loop through each channel to find the one with the most capacity\n\tfor i := 0; i < len(channels); i++ {\n\t\tif cap(channels[fullChannelIndex]) > fullChannelCapacity {\n\t\t\tfullChannelCapacity = cap(channels[fullChannelIndex])\n\t\t\tfullChannelIndex = i\n\t\t}\n\t}\n\n\t// Check if the fullest channel has enough capacity to handle the message\n\tif fullChannelCapacity < cap(msg) {\n\t\t// If it doesn't have enough capacity, then try the next channel (circular)\n\t\tif fullChannelIndex == len(channels)-1 {\n\t\t\tfullChannelIndex = 0\n\t\t} else {\n\t\t\tfullChannelIndex++\n\t\t}\n\t}\n\n\t// Return the index of the chosen channel\n\treturn fullChannelIndex\n}\n```"]}, "lo-channel.go-FanOut": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/channel.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/rand\"\n)\n\ntype DispatchingStrategy[T any] func(msg T, index uint64, channels []<-chan T) int\n\n// ChannelDispatcher distributes messages from input channels into N child channels.\n// Close events are propagated to children.\n// Underlying channels can have a fixed buffer capacity or be unbuffered when cap is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc createChannels[T any](count int, channelBufferCap int) []chan T {\n\tchildren := make([]chan T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tchildren = append(children, make(chan T, channelBufferCap))\n\t}\n\n\treturn children\n}\n\nfunc channelsToReadOnly[T any](children []chan T) []<-chan T {\n\troChildren := make([]<-chan T, 0, len(children))\n\n\tfor i := range children {\n\t\troChildren = append(roChildren, children[i])\n\t}\n\n\treturn roChildren\n}\n\nfunc closeChannels[T any](children []chan T) {\n\tfor i := 0; i < len(children); i++ {\n\t\tclose(children[i])\n\t}\n}\n\nfunc channelIsNotFull[T any](ch <-chan T) bool {\n\treturn cap(ch) == 0 || len(ch) < cap(ch)\n}\n\n// DispatchingStrategyRoundRobin distributes messages in a rotating sequential manner.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyRandom distributes messages in a random manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyWeightedRandom distributes messages in a weighted manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\nfunc DispatchingStrategyWeightedRandom[T any](weights []int) DispatchingStrategy[T] {\n\tseq := []int{}\n\n\tfor i := 0; i < len(weights); i++ {\n\t\tfor j := 0; j < weights[i]; j++ {\n\t\t\tseq = append(seq, i)\n\t\t}\n\t}\n\n\treturn func(msg T, index uint64, channels []<-chan T) int {\n\t\tfor {\n\t\t\ti := seq[rand.IntN(len(seq))]\n\t\t\tif channelIsNotFull(channels[i]) {\n\t\t\t\treturn i\n\t\t\t}\n\n\t\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t\t}\n\t}\n}\n\n// DispatchingStrategyFirst distributes messages in the first non-full channel.\n// If the capacity of the first channel is exceeded, the second channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyLeast distributes messages in the emptiest channel.\nfunc DispatchingStrategyLeast[T any](msg T, index uint64, channels []<-chan T) int {\n\tseq := Range(len(channels))\n\n\treturn MinBy(seq, func(item int, min int) bool {\n\t\treturn len(channels[item]) < len(channels[min])\n\t})\n}\n\n// DispatchingStrategyMost distributes messages in the fullest channel.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n// SliceToChannel returns a read-only channels of collection elements.\nfunc SliceToChannel[T any](bufferSize int, collection []T) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\tfor i := range collection {\n\t\t\tch <- collection[i]\n\t\t}\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// ChannelToSlice returns a slice built from channels items. Blocks until channel closes.\nfunc ChannelToSlice[T any](ch <-chan T) []T {\n\tcollection := []T{}\n\n\tfor item := range ch {\n\t\tcollection = append(collection, item)\n\t}\n\n\treturn collection\n}\n\n// Generator implements the generator design pattern.\nfunc Generator[T any](bufferSize int, generator func(yield func(T))) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\t// WARNING: infinite loop\n\t\tgenerator(func(t T) {\n\t\t\tch <- t\n\t\t})\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// Buffer creates a slice of n elements from a channel. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc Buffer[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\titem, ok := <-ch\n\t\tif !ok {\n\t\t\treturn buffer, index, time.Since(now), false\n\t\t}\n\n\t\tbuffer = append(buffer, item)\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// Batch creates a slice of n elements from a channel. Returns the slice and the slice length.\n//\n// Deprecated: Use [Buffer] instead.\nfunc Batch[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn Buffer(ch, size)\n}\n\n// BufferWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc BufferWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\texpire := time.NewTimer(timeout)\n\tdefer expire.Stop()\n\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\tselect {\n\t\tcase item, ok := <-ch:\n\t\t\tif !ok {\n\t\t\t\treturn buffer, index, time.Since(now), false\n\t\t\t}\n\n\t\t\tbuffer = append(buffer, item)\n\n\t\tcase <-expire.C:\n\t\t\treturn buffer, index, time.Since(now), true\n\t\t}\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// BatchWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n//\n// Deprecated: Use [BufferWithTimeout] instead.\nfunc BatchWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn BufferWithTimeout(ch, size, timeout)\n}\n\n// FanIn collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\nfunc FanIn[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\tout := make(chan T, channelBufferCap)\n\tvar wg sync.WaitGroup\n\n\t// Start an output goroutine for each input channel in upstreams.\n\twg.Add(len(upstreams))\n\tfor i := range upstreams {\n\t\tgo func(index int) {\n\t\t\tfor n := range upstreams[index] {\n\t\t\t\tout <- n\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(i)\n\t}\n\n\t// Start a goroutine to close out once all the output goroutines are done.\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(out)\n\t}()\n\treturn out\n}\n\n// ChannelMerge collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\n//\n// Deprecated: Use [FanIn] instead.\nfunc ChannelMerge[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\treturn FanIn(channelBufferCap, upstreams...)\n}\n\n// FanOut broadcasts all the upstream messages to multiple downstream channels.\n// When upstream channel reach EOF, downstream channels close. If any downstream\n// channels is full, broadcasting is paused.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func FanOut[T any](count int, channelsBufferCap int, upstream <-chan T) []<-chan T {\n# Natural Language Annotations: // FanOut broadcasts all the upstream messages to multiple downstream channels.\n// When upstream channel reach EOF, downstream channels close. If any downstream\n// channels is full, broadcasting is paused.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc FanOut[T any](count int, channelsBufferCap int, upstream <-chan T) []<-chan T {\n\tchildren := createChannels[T](count, channelsBufferCap)\n\n\tfor i := 0; i < count; i++ {\n\t\tgo func(channel <-chan T) {\n\t\t\tfor item := range channel {\n\t\t\t\tupstream <- item\n\t\t\t}\n\t\t}(children[i])\n\t}\n\n\treturn children\n}\n```", "  ```\nfunc FanOut[T any](count int, channelsBufferCap int, upstream <-chan T) []<-chan T {\n\tdownstreams := make([]chan T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tdownstreams = append(downstreams, make(chan T, channelsBufferCap))\n\t}\n\n\tgo func() {\n\t\tfor {\n\t\t\titem, ok := <-upstream\n\t\t\tif !ok {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tfor i := range downstreams {\n\t\t\t\tif channelIsNotFull(downstreams[i]) {\n\t\t\t\t\tdownstreams[i] <- item\n\t\t\t\t} else {\n\t\t\t\t\t// if any downstream channel is full, pause broadcasting\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// when upstream is closed, close all downstream channels\n\t\tfor i := range downstreams {\n\t\t\tclose(downstreams[i])\n\t\t}\n\t}()\n\n\treturn downstreams\n}\n```", "  ```\nfunc FanOut[T any](count int, channelsBufferCap int, upstream <-chan T) []<-chan T {\n\tchildren := make([]<-chan T, 0, count)\n\tfor i := range children {\n\t\tchildren = append(children, make(chan T, channelBufferCap))\n\t}\n\n\tvar wg sync.WaitGroup\n\twg.Add(1)\n\tgo func() {\n\t\tfor item := range upstream {\n\t\t\tvar index int\n\t\t\tfor {\n\t\t\t\tindex = DispatchingStrategyFirst(item, uint64(len(children)), channelsToReadOnly(children))\n\t\t\t\tif channelIsNotFull(children[index]) {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\ttime.Sleep(10 * time.Microsecond)\n\t\t\t}\n\t\t\tchildren[index] <- item\n\t\t}\n\t\tfor i := range children {\n\t\t\tclose(children[i])\n\t\t}\n\t\twg.Done()\n\t}()\n\n\treturn children\n}\n```"]}, "lo-slice.go-Shuffle": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/slice.go:\n```\npackage lo\n\nimport (\n\t\"sort\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// Filter iterates over elements of collection, returning an array of all elements predicate returns truthy for.\n// Play: https://go.dev/play/p/Apjg3WeSi7K\nfunc Filter[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Map manipulates a slice and transforms it to a slice of another type.\n// Play: https://go.dev/play/p/OkPcYAhBo0D\nfunc Map[T any, R any](collection []T, iteratee func(item T, index int) R) []R {\n\tresult := make([]R, len(collection))\n\n\tfor i := range collection {\n\t\tresult[i] = iteratee(collection[i], i)\n\t}\n\n\treturn result\n}\n\n// FilterMap returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\n//\n// Play: https://go.dev/play/p/-AuYXfy7opz\nfunc FilterMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FlatMap manipulates a slice and transforms and flattens it to a slice of another type.\n// The transform function can either return a slice or a `nil`, and in the `nil` case\n// no value is added to the final slice.\n// Play: https://go.dev/play/p/YSoYmQTA8-U\nfunc FlatMap[T any, R any](collection []T, iteratee func(item T, index int) []R) []R {\n\tresult := make([]R, 0, len(collection))\n\n\tfor i := range collection {\n\t\tresult = append(result, iteratee(collection[i], i)...)\n\t}\n\n\treturn result\n}\n\n// Reduce reduces collection to a value which is the accumulated result of running each element in collection\n// through accumulator, where each successive invocation is supplied the return value of the previous.\n// Play: https://go.dev/play/p/R4UHXZNaaUG\nfunc Reduce[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := range collection {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ReduceRight helper is like Reduce except that it iterates over elements of collection from right to left.\n// Play: https://go.dev/play/p/Fq3W70l7wXF\nfunc ReduceRight[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := len(collection) - 1; i >= 0; i-- {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ForEach iterates over elements of collection and invokes iteratee for each element.\n// Play: https://go.dev/play/p/oofyiUPRf8t\nfunc ForEach[T any](collection []T, iteratee func(item T, index int)) {\n\tfor i := range collection {\n\t\titeratee(collection[i], i)\n\t}\n}\n\n// Times invokes the iteratee n times, returning an array of the results of each invocation.\n// The iteratee is invoked with index as argument.\n// Play: https://go.dev/play/p/vgQj3Glr6lT\nfunc Times[T any](count int, iteratee func(index int) T) []T {\n\tresult := make([]T, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult[i] = iteratee(i)\n\t}\n\n\treturn result\n}\n\n// Uniq returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array.\n// Play: https://go.dev/play/p/DTzbeXZ6iEN\nfunc Uniq[T comparable, Slice ~[]T](collection Slice) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[T]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tif _, ok := seen[collection[i]]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[collection[i]] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// UniqBy returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\n// Play: https://go.dev/play/p/g42Z3QSb53u\nfunc UniqBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[U]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif _, ok := seen[key]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[key] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// GroupBy returns an object composed of keys generated from the results of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/XnQBd_v6brd\nfunc GroupBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) map[U]Slice {\n\tresult := map[U]Slice{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresult[key] = append(result[key], collection[i])\n\t}\n\n\treturn result\n}\n\n// Chunk returns an array of elements split into groups the length of size. If array can't be split evenly,\n// the final chunk will be the remaining elements.\n// Play: https://go.dev/play/p/EeKl0AuTehH\nfunc Chunk[T any, Slice ~[]T](collection Slice, size int) []Slice {\n\tif size <= 0 {\n\t\tpanic(\"Second parameter must be greater than 0\")\n\t}\n\n\tchunksNum := len(collection) / size\n\tif len(collection)%size != 0 {\n\t\tchunksNum += 1\n\t}\n\n\tresult := make([]Slice, 0, chunksNum)\n\n\tfor i := 0; i < chunksNum; i++ {\n\t\tlast := (i + 1) * size\n\t\tif last > len(collection) {\n\t\t\tlast = len(collection)\n\t\t}\n\t\tresult = append(result, collection[i*size:last])\n\t}\n\n\treturn result\n}\n\n// PartitionBy returns an array of elements split into groups. The order of grouped values is\n// determined by the order they occur in collection. The grouping is generated from the results\n// of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/NfQ_nGjkgXW\nfunc PartitionBy[T any, K comparable, Slice ~[]T](collection Slice, iteratee func(item T) K) []Slice {\n\tresult := []Slice{}\n\tseen := map[K]int{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresultIndex, ok := seen[key]\n\t\tif !ok {\n\t\t\tresultIndex = len(result)\n\t\t\tseen[key] = resultIndex\n\t\t\tresult = append(result, Slice{})\n\t\t}\n\n\t\tresult[resultIndex] = append(result[resultIndex], collection[i])\n\t}\n\n\treturn result\n\n\t// unordered:\n\t// groups := GroupBy[T, K](collection, iteratee)\n\t// return Values[K, []T](groups)\n}\n\n// Flatten returns an array a single level deep.\n// Play: https://go.dev/play/p/rbp9ORaMpjw\nfunc Flatten[T any, Slice ~[]T](collection []Slice) Slice {\n\ttotalLen := 0\n\tfor i := range collection {\n\t\ttotalLen += len(collection[i])\n\t}\n\n\tresult := make(Slice, 0, totalLen)\n\tfor i := range collection {\n\t\tresult = append(result, collection[i]...)\n\t}\n\n\treturn result\n}\n\n// Interleave round-robin alternating input slices and sequentially appending value at index into result\n// Play: https://go.dev/play/p/-RJkTLQEDVt\nfunc Interleave[T any, Slice ~[]T](collections ...Slice) Slice {\n\tif len(collections) == 0 {\n\t\treturn Slice{}\n\t}\n\n\tmaxSize := 0\n\ttotalSize := 0\n\tfor i := range collections {\n\t\tsize := len(collections[i])\n\t\ttotalSize += size\n\t\tif size > maxSize {\n\t\t\tmaxSize = size\n\t\t}\n\t}\n\n\tif maxSize == 0 {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, totalSize)\n\n\tresultIdx := 0\n\tfor i := 0; i < maxSize; i++ {\n\t\tfor j := range collections {\n\t\t\tif len(collections[j])-1 < i {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tresult[resultIdx] = collections[j][i]\n\t\t\tresultIdx++\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Shuffle returns an array of shuffled values. Uses the Fisher-Yates shuffle algorithm.\n// Play: https://go.dev/play/p/Qp73bnTDnc7\n\n\n\n\n\n\n\n\n// Reverse reverses array so that the first element becomes the last, the second element becomes the second to last, and so on.\n// Play: https://go.dev/play/p/fhUMLvZ7vS6\nfunc Reverse[T any, Slice ~[]T](collection Slice) Slice {\n\tlength := len(collection)\n\thalf := length / 2\n\n\tfor i := 0; i < half; i = i + 1 {\n\t\tj := length - 1 - i\n\t\tcollection[i], collection[j] = collection[j], collection[i]\n\t}\n\n\treturn collection\n}\n\n// Fill fills elements of array with `initial` value.\n// Play: https://go.dev/play/p/VwR34GzqEub\n\n\n\n\n\n\n\n\n\n\n// Repeat builds a slice with N copies of initial value.\n// Play: https://go.dev/play/p/g3uHXbmc3b6\n\n\n\n\n\n\n\n\n\n\n// RepeatBy builds a slice with values returned by N calls of callback.\n// Play: https://go.dev/play/p/ozZLCtX_hNU\nfunc RepeatBy[T any](count int, predicate func(index int) T) []T {\n\tresult := make([]T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult = append(result, predicate(i))\n\t}\n\n\treturn result\n}\n\n// KeyBy transforms a slice or an array of structs to a map based on a pivot callback.\n// Play: https://go.dev/play/p/mdaClUAT-zZ\nfunc KeyBy[K comparable, V any](collection []V, iteratee func(item V) K) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk := iteratee(collection[i])\n\t\tresult[k] = collection[i]\n\t}\n\n\treturn result\n}\n\n// Associate returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc Associate[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk, v := transform(collection[i])\n\t\tresult[k] = v\n\t}\n\n\treturn result\n}\n\n// SliceToMap returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Alias of Associate().\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc SliceToMap[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\treturn Associate(collection, transform)\n}\n\n// Drop drops n elements from the beginning of a slice or array.\n// Play: https://go.dev/play/p/JswS7vXRJP2\nfunc Drop[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn make(Slice, 0)\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\n\treturn append(result, collection[n:]...)\n}\n\n// DropRight drops n elements from the end of a slice or array.\n// Play: https://go.dev/play/p/GG0nXkSJJa3\nfunc DropRight[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\treturn append(result, collection[:len(collection)-n]...)\n}\n\n// DropWhile drops elements from the beginning of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/7gBPYw2IK16\nfunc DropWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := 0\n\tfor ; i < len(collection); i++ {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-i)\n\treturn append(result, collection[i:]...)\n}\n\n// DropRightWhile drops elements from the end of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/3-n71oEC0Hz\nfunc DropRightWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := len(collection) - 1\n\tfor ; i >= 0; i-- {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, i+1)\n\treturn append(result, collection[:i+1]...)\n}\n\n// DropByIndex drops elements from a slice or array by the index.\n// A negative index will drop elements from the end of the slice.\n// Play: https://go.dev/play/p/bPIH4npZRxS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Reject is the opposite of Filter, this method returns the elements of collection that predicate does not return truthy for.\n// Play: https://go.dev/play/p/YkLMODy1WEL\nfunc Reject[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := Slice{}\n\n\tfor i := range collection {\n\t\tif !predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// RejectMap is the opposite of FilterMap, this method returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\nfunc RejectMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); !ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FilterReject mixes Filter and Reject, this method returns two slices, one for the elements of collection that\n// predicate returns truthy for and one for the elements that predicate does not return truthy for.\nfunc FilterReject[T any, Slice ~[]T](collection Slice, predicate func(T, int) bool) (kept Slice, rejected Slice) {\n\tkept = make(Slice, 0, len(collection))\n\trejected = make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tkept = append(kept, collection[i])\n\t\t} else {\n\t\t\trejected = append(rejected, collection[i])\n\t\t}\n\t}\n\n\treturn kept, rejected\n}\n\n// Count counts the number of elements in the collection that compare equal to value.\n// Play: https://go.dev/play/p/Y3FlK54yveC\nfunc Count[T comparable](collection []T, value T) (count int) {\n\tfor i := range collection {\n\t\tif collection[i] == value {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountBy counts the number of elements in the collection for which predicate is true.\n// Play: https://go.dev/play/p/ByQbNYQQi4X\nfunc CountBy[T any](collection []T, predicate func(item T) bool) (count int) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountValues counts the number of each element in the collection.\n// Play: https://go.dev/play/p/-p-PyLT4dfy\nfunc CountValues[T comparable](collection []T) map[T]int {\n\tresult := make(map[T]int)\n\n\tfor i := range collection {\n\t\tresult[collection[i]]++\n\t}\n\n\treturn result\n}\n\n// CountValuesBy counts the number of each element return from mapper function.\n// Is equivalent to chaining lo.Map and lo.CountValues.\n// Play: https://go.dev/play/p/2U0dG1SnOmS\nfunc CountValuesBy[T any, U comparable](collection []T, mapper func(item T) U) map[U]int {\n\tresult := make(map[U]int)\n\n\tfor i := range collection {\n\t\tresult[mapper(collection[i])]++\n\t}\n\n\treturn result\n}\n\n// Subset returns a copy of a slice from `offset` up to `length` elements. Like `slice[start:start+length]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/tOQu1GhFcog\nfunc Subset[T any, Slice ~[]T](collection Slice, offset int, length uint) Slice {\n\tsize := len(collection)\n\n\tif offset < 0 {\n\t\toffset = size + offset\n\t\tif offset < 0 {\n\t\t\toffset = 0\n\t\t}\n\t}\n\n\tif offset > size {\n\t\treturn Slice{}\n\t}\n\n\tif length > uint(size)-uint(offset) {\n\t\tlength = uint(size - offset)\n\t}\n\n\treturn collection[offset : offset+int(length)]\n}\n\n// Slice returns a copy of a slice from `start` up to, but not including `end`. Like `slice[start:end]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/8XWYhfMMA1h\nfunc Slice[T any, Slice ~[]T](collection Slice, start int, end int) Slice {\n\tsize := len(collection)\n\n\tif start >= end {\n\t\treturn Slice{}\n\t}\n\n\tif start > size {\n\t\tstart = size\n\t}\n\tif start < 0 {\n\t\tstart = 0\n\t}\n\n\tif end > size {\n\t\tend = size\n\t}\n\tif end < 0 {\n\t\tend = 0\n\t}\n\n\treturn collection[start:end]\n}\n\n// Replace returns a copy of the slice with the first n non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/XfPzmf9gql6\nfunc Replace[T comparable, Slice ~[]T](collection Slice, old T, new T, n int) Slice {\n\tresult := make(Slice, len(collection))\n\tcopy(result, collection)\n\n\tfor i := range result {\n\t\tif result[i] == old && n != 0 {\n\t\t\tresult[i] = new\n\t\t\tn--\n\t\t}\n\t}\n\n\treturn result\n}\n\n// ReplaceAll returns a copy of the slice with all non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/a9xZFUHfYcV\nfunc ReplaceAll[T comparable, Slice ~[]T](collection Slice, old T, new T) Slice {\n\treturn Replace(collection, old, new, -1)\n}\n\n// Compact returns a slice of all non-zero elements.\n// Play: https://go.dev/play/p/tXiy-iK6PAc\nfunc Compact[T comparable, Slice ~[]T](collection Slice) Slice {\n\tvar zero T\n\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif collection[i] != zero {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// IsSorted checks if a slice is sorted.\n// Play: https://go.dev/play/p/mc3qR-t4mcx\nfunc IsSorted[T constraints.Ordered](collection []T) bool {\n\tfor i := 1; i < len(collection); i++ {\n\t\tif collection[i-1] > collection[i] {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// IsSortedByKey checks if a slice is sorted by iteratee.\n// Play: https://go.dev/play/p/wiG6XyBBu49\nfunc IsSortedByKey[T any, K constraints.Ordered](collection []T, iteratee func(item T) K) bool {\n\tsize := len(collection)\n\n\tfor i := 0; i < size-1; i++ {\n\t\tif iteratee(collection[i]) > iteratee(collection[i+1]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Splice inserts multiple elements at index i. A negative index counts back\n// from the end of the slice. The helper is protected against overflow errors.\n// Play: https://go.dev/play/p/G5_GhkeSUBA\nfunc Splice[T any, Slice ~[]T](collection Slice, i int, elements ...T) Slice {\n\tsizeCollection := len(collection)\n\tsizeElements := len(elements)\n\toutput := make(Slice, 0, sizeCollection+sizeElements) // preallocate memory for the output slice\n\n\tif sizeElements == 0 {\n\t\treturn append(output, collection...) // simple copy\n\t} else if i > sizeCollection {\n\t\t// positive overflow\n\t\treturn append(append(output, collection...), elements...)\n\t} else if i < -sizeCollection {\n\t\t// negative overflow\n\t\treturn append(append(output, elements...), collection...)\n\t} else if i < 0 {\n\t\t// backward\n\t\ti = sizeCollection + i\n\t}\n\n\treturn append(append(append(output, collection[:i]...), elements...), collection[i:]...)\n}\n\n```\n# Function Signature: func Shuffle[T any, Slice ~[]T](collection Slice) Slice {\n# Natural Language Annotations: // Shuffle returns an array of shuffled values. Uses the Fisher-Yates shuffle algorithm.\n// Play: https://go.dev/play/p/Qp73bnTDnc7\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc Shuffle[T any, Slice ~[]T](collection Slice) Slice {\n\trand.Shuffle(len(collection), func(i, j int) {\n\t\tcollection[i], collection[j] = collection[j], collection[i]\n\t})\n\treturn collection\n}\n```", "  ```\nfunc Shuffle[T any, Slice ~[]T](collection Slice) Slice {\n\tsize := len(collection)\n\tresult := make(Slice, size)\n\tcopy(result, collection)\n\n\tfor i := 0; i < size; i++ {\n\t\tj := rand.Intn(size-i) + i\n\t\tresult[i], result[j] = result[j], result[i]\n\t}\n\n\treturn result\n}\n```", "  ```\nfunc Shuffle[T any, Slice ~[]T](collection Slice) Slice {\n\tif len(collection) == 0 {\n\t\treturn collection\n\t}\n\n\tresult := make(Slice, len(collection), len(collection))\n\tindexes := make([]int, len(collection))\n\n\tfor i := range indexes {\n\t\tindexes[i] = i\n\t}\n\n\tfor i := len(collection) - 1; i > 0; i-- {\n\t\tj := rand.Intn(i + 1)\n\t\tresult[i] = collection[indexes[j]]\n\t\tresult[j] = collection[indexes[i]]\n\t}\n\n\tresult[0] = collection[indexes[0]]\n\n\treturn result\n}\n```"]}, "lo-slice.go-Fill": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/slice.go:\n```\npackage lo\n\nimport (\n\t\"sort\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// Filter iterates over elements of collection, returning an array of all elements predicate returns truthy for.\n// Play: https://go.dev/play/p/Apjg3WeSi7K\nfunc Filter[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Map manipulates a slice and transforms it to a slice of another type.\n// Play: https://go.dev/play/p/OkPcYAhBo0D\nfunc Map[T any, R any](collection []T, iteratee func(item T, index int) R) []R {\n\tresult := make([]R, len(collection))\n\n\tfor i := range collection {\n\t\tresult[i] = iteratee(collection[i], i)\n\t}\n\n\treturn result\n}\n\n// FilterMap returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\n//\n// Play: https://go.dev/play/p/-AuYXfy7opz\nfunc FilterMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FlatMap manipulates a slice and transforms and flattens it to a slice of another type.\n// The transform function can either return a slice or a `nil`, and in the `nil` case\n// no value is added to the final slice.\n// Play: https://go.dev/play/p/YSoYmQTA8-U\nfunc FlatMap[T any, R any](collection []T, iteratee func(item T, index int) []R) []R {\n\tresult := make([]R, 0, len(collection))\n\n\tfor i := range collection {\n\t\tresult = append(result, iteratee(collection[i], i)...)\n\t}\n\n\treturn result\n}\n\n// Reduce reduces collection to a value which is the accumulated result of running each element in collection\n// through accumulator, where each successive invocation is supplied the return value of the previous.\n// Play: https://go.dev/play/p/R4UHXZNaaUG\nfunc Reduce[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := range collection {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ReduceRight helper is like Reduce except that it iterates over elements of collection from right to left.\n// Play: https://go.dev/play/p/Fq3W70l7wXF\nfunc ReduceRight[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := len(collection) - 1; i >= 0; i-- {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ForEach iterates over elements of collection and invokes iteratee for each element.\n// Play: https://go.dev/play/p/oofyiUPRf8t\nfunc ForEach[T any](collection []T, iteratee func(item T, index int)) {\n\tfor i := range collection {\n\t\titeratee(collection[i], i)\n\t}\n}\n\n// Times invokes the iteratee n times, returning an array of the results of each invocation.\n// The iteratee is invoked with index as argument.\n// Play: https://go.dev/play/p/vgQj3Glr6lT\nfunc Times[T any](count int, iteratee func(index int) T) []T {\n\tresult := make([]T, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult[i] = iteratee(i)\n\t}\n\n\treturn result\n}\n\n// Uniq returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array.\n// Play: https://go.dev/play/p/DTzbeXZ6iEN\nfunc Uniq[T comparable, Slice ~[]T](collection Slice) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[T]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tif _, ok := seen[collection[i]]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[collection[i]] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// UniqBy returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\n// Play: https://go.dev/play/p/g42Z3QSb53u\nfunc UniqBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[U]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif _, ok := seen[key]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[key] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// GroupBy returns an object composed of keys generated from the results of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/XnQBd_v6brd\nfunc GroupBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) map[U]Slice {\n\tresult := map[U]Slice{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresult[key] = append(result[key], collection[i])\n\t}\n\n\treturn result\n}\n\n// Chunk returns an array of elements split into groups the length of size. If array can't be split evenly,\n// the final chunk will be the remaining elements.\n// Play: https://go.dev/play/p/EeKl0AuTehH\nfunc Chunk[T any, Slice ~[]T](collection Slice, size int) []Slice {\n\tif size <= 0 {\n\t\tpanic(\"Second parameter must be greater than 0\")\n\t}\n\n\tchunksNum := len(collection) / size\n\tif len(collection)%size != 0 {\n\t\tchunksNum += 1\n\t}\n\n\tresult := make([]Slice, 0, chunksNum)\n\n\tfor i := 0; i < chunksNum; i++ {\n\t\tlast := (i + 1) * size\n\t\tif last > len(collection) {\n\t\t\tlast = len(collection)\n\t\t}\n\t\tresult = append(result, collection[i*size:last])\n\t}\n\n\treturn result\n}\n\n// PartitionBy returns an array of elements split into groups. The order of grouped values is\n// determined by the order they occur in collection. The grouping is generated from the results\n// of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/NfQ_nGjkgXW\nfunc PartitionBy[T any, K comparable, Slice ~[]T](collection Slice, iteratee func(item T) K) []Slice {\n\tresult := []Slice{}\n\tseen := map[K]int{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresultIndex, ok := seen[key]\n\t\tif !ok {\n\t\t\tresultIndex = len(result)\n\t\t\tseen[key] = resultIndex\n\t\t\tresult = append(result, Slice{})\n\t\t}\n\n\t\tresult[resultIndex] = append(result[resultIndex], collection[i])\n\t}\n\n\treturn result\n\n\t// unordered:\n\t// groups := GroupBy[T, K](collection, iteratee)\n\t// return Values[K, []T](groups)\n}\n\n// Flatten returns an array a single level deep.\n// Play: https://go.dev/play/p/rbp9ORaMpjw\nfunc Flatten[T any, Slice ~[]T](collection []Slice) Slice {\n\ttotalLen := 0\n\tfor i := range collection {\n\t\ttotalLen += len(collection[i])\n\t}\n\n\tresult := make(Slice, 0, totalLen)\n\tfor i := range collection {\n\t\tresult = append(result, collection[i]...)\n\t}\n\n\treturn result\n}\n\n// Interleave round-robin alternating input slices and sequentially appending value at index into result\n// Play: https://go.dev/play/p/-RJkTLQEDVt\nfunc Interleave[T any, Slice ~[]T](collections ...Slice) Slice {\n\tif len(collections) == 0 {\n\t\treturn Slice{}\n\t}\n\n\tmaxSize := 0\n\ttotalSize := 0\n\tfor i := range collections {\n\t\tsize := len(collections[i])\n\t\ttotalSize += size\n\t\tif size > maxSize {\n\t\t\tmaxSize = size\n\t\t}\n\t}\n\n\tif maxSize == 0 {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, totalSize)\n\n\tresultIdx := 0\n\tfor i := 0; i < maxSize; i++ {\n\t\tfor j := range collections {\n\t\t\tif len(collections[j])-1 < i {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tresult[resultIdx] = collections[j][i]\n\t\t\tresultIdx++\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Shuffle returns an array of shuffled values. Uses the Fisher-Yates shuffle algorithm.\n// Play: https://go.dev/play/p/Qp73bnTDnc7\n\n\n\n\n\n\n\n\n// Reverse reverses array so that the first element becomes the last, the second element becomes the second to last, and so on.\n// Play: https://go.dev/play/p/fhUMLvZ7vS6\nfunc Reverse[T any, Slice ~[]T](collection Slice) Slice {\n\tlength := len(collection)\n\thalf := length / 2\n\n\tfor i := 0; i < half; i = i + 1 {\n\t\tj := length - 1 - i\n\t\tcollection[i], collection[j] = collection[j], collection[i]\n\t}\n\n\treturn collection\n}\n\n// Fill fills elements of array with `initial` value.\n// Play: https://go.dev/play/p/VwR34GzqEub\n\n\n\n\n\n\n\n\n\n\n// Repeat builds a slice with N copies of initial value.\n// Play: https://go.dev/play/p/g3uHXbmc3b6\n\n\n\n\n\n\n\n\n\n\n// RepeatBy builds a slice with values returned by N calls of callback.\n// Play: https://go.dev/play/p/ozZLCtX_hNU\nfunc RepeatBy[T any](count int, predicate func(index int) T) []T {\n\tresult := make([]T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult = append(result, predicate(i))\n\t}\n\n\treturn result\n}\n\n// KeyBy transforms a slice or an array of structs to a map based on a pivot callback.\n// Play: https://go.dev/play/p/mdaClUAT-zZ\nfunc KeyBy[K comparable, V any](collection []V, iteratee func(item V) K) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk := iteratee(collection[i])\n\t\tresult[k] = collection[i]\n\t}\n\n\treturn result\n}\n\n// Associate returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc Associate[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk, v := transform(collection[i])\n\t\tresult[k] = v\n\t}\n\n\treturn result\n}\n\n// SliceToMap returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Alias of Associate().\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc SliceToMap[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\treturn Associate(collection, transform)\n}\n\n// Drop drops n elements from the beginning of a slice or array.\n// Play: https://go.dev/play/p/JswS7vXRJP2\nfunc Drop[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn make(Slice, 0)\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\n\treturn append(result, collection[n:]...)\n}\n\n// DropRight drops n elements from the end of a slice or array.\n// Play: https://go.dev/play/p/GG0nXkSJJa3\nfunc DropRight[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\treturn append(result, collection[:len(collection)-n]...)\n}\n\n// DropWhile drops elements from the beginning of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/7gBPYw2IK16\nfunc DropWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := 0\n\tfor ; i < len(collection); i++ {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-i)\n\treturn append(result, collection[i:]...)\n}\n\n// DropRightWhile drops elements from the end of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/3-n71oEC0Hz\nfunc DropRightWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := len(collection) - 1\n\tfor ; i >= 0; i-- {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, i+1)\n\treturn append(result, collection[:i+1]...)\n}\n\n// DropByIndex drops elements from a slice or array by the index.\n// A negative index will drop elements from the end of the slice.\n// Play: https://go.dev/play/p/bPIH4npZRxS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Reject is the opposite of Filter, this method returns the elements of collection that predicate does not return truthy for.\n// Play: https://go.dev/play/p/YkLMODy1WEL\nfunc Reject[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := Slice{}\n\n\tfor i := range collection {\n\t\tif !predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// RejectMap is the opposite of FilterMap, this method returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\nfunc RejectMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); !ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FilterReject mixes Filter and Reject, this method returns two slices, one for the elements of collection that\n// predicate returns truthy for and one for the elements that predicate does not return truthy for.\nfunc FilterReject[T any, Slice ~[]T](collection Slice, predicate func(T, int) bool) (kept Slice, rejected Slice) {\n\tkept = make(Slice, 0, len(collection))\n\trejected = make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tkept = append(kept, collection[i])\n\t\t} else {\n\t\t\trejected = append(rejected, collection[i])\n\t\t}\n\t}\n\n\treturn kept, rejected\n}\n\n// Count counts the number of elements in the collection that compare equal to value.\n// Play: https://go.dev/play/p/Y3FlK54yveC\nfunc Count[T comparable](collection []T, value T) (count int) {\n\tfor i := range collection {\n\t\tif collection[i] == value {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountBy counts the number of elements in the collection for which predicate is true.\n// Play: https://go.dev/play/p/ByQbNYQQi4X\nfunc CountBy[T any](collection []T, predicate func(item T) bool) (count int) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountValues counts the number of each element in the collection.\n// Play: https://go.dev/play/p/-p-PyLT4dfy\nfunc CountValues[T comparable](collection []T) map[T]int {\n\tresult := make(map[T]int)\n\n\tfor i := range collection {\n\t\tresult[collection[i]]++\n\t}\n\n\treturn result\n}\n\n// CountValuesBy counts the number of each element return from mapper function.\n// Is equivalent to chaining lo.Map and lo.CountValues.\n// Play: https://go.dev/play/p/2U0dG1SnOmS\nfunc CountValuesBy[T any, U comparable](collection []T, mapper func(item T) U) map[U]int {\n\tresult := make(map[U]int)\n\n\tfor i := range collection {\n\t\tresult[mapper(collection[i])]++\n\t}\n\n\treturn result\n}\n\n// Subset returns a copy of a slice from `offset` up to `length` elements. Like `slice[start:start+length]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/tOQu1GhFcog\nfunc Subset[T any, Slice ~[]T](collection Slice, offset int, length uint) Slice {\n\tsize := len(collection)\n\n\tif offset < 0 {\n\t\toffset = size + offset\n\t\tif offset < 0 {\n\t\t\toffset = 0\n\t\t}\n\t}\n\n\tif offset > size {\n\t\treturn Slice{}\n\t}\n\n\tif length > uint(size)-uint(offset) {\n\t\tlength = uint(size - offset)\n\t}\n\n\treturn collection[offset : offset+int(length)]\n}\n\n// Slice returns a copy of a slice from `start` up to, but not including `end`. Like `slice[start:end]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/8XWYhfMMA1h\nfunc Slice[T any, Slice ~[]T](collection Slice, start int, end int) Slice {\n\tsize := len(collection)\n\n\tif start >= end {\n\t\treturn Slice{}\n\t}\n\n\tif start > size {\n\t\tstart = size\n\t}\n\tif start < 0 {\n\t\tstart = 0\n\t}\n\n\tif end > size {\n\t\tend = size\n\t}\n\tif end < 0 {\n\t\tend = 0\n\t}\n\n\treturn collection[start:end]\n}\n\n// Replace returns a copy of the slice with the first n non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/XfPzmf9gql6\nfunc Replace[T comparable, Slice ~[]T](collection Slice, old T, new T, n int) Slice {\n\tresult := make(Slice, len(collection))\n\tcopy(result, collection)\n\n\tfor i := range result {\n\t\tif result[i] == old && n != 0 {\n\t\t\tresult[i] = new\n\t\t\tn--\n\t\t}\n\t}\n\n\treturn result\n}\n\n// ReplaceAll returns a copy of the slice with all non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/a9xZFUHfYcV\nfunc ReplaceAll[T comparable, Slice ~[]T](collection Slice, old T, new T) Slice {\n\treturn Replace(collection, old, new, -1)\n}\n\n// Compact returns a slice of all non-zero elements.\n// Play: https://go.dev/play/p/tXiy-iK6PAc\nfunc Compact[T comparable, Slice ~[]T](collection Slice) Slice {\n\tvar zero T\n\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif collection[i] != zero {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// IsSorted checks if a slice is sorted.\n// Play: https://go.dev/play/p/mc3qR-t4mcx\nfunc IsSorted[T constraints.Ordered](collection []T) bool {\n\tfor i := 1; i < len(collection); i++ {\n\t\tif collection[i-1] > collection[i] {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// IsSortedByKey checks if a slice is sorted by iteratee.\n// Play: https://go.dev/play/p/wiG6XyBBu49\nfunc IsSortedByKey[T any, K constraints.Ordered](collection []T, iteratee func(item T) K) bool {\n\tsize := len(collection)\n\n\tfor i := 0; i < size-1; i++ {\n\t\tif iteratee(collection[i]) > iteratee(collection[i+1]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Splice inserts multiple elements at index i. A negative index counts back\n// from the end of the slice. The helper is protected against overflow errors.\n// Play: https://go.dev/play/p/G5_GhkeSUBA\nfunc Splice[T any, Slice ~[]T](collection Slice, i int, elements ...T) Slice {\n\tsizeCollection := len(collection)\n\tsizeElements := len(elements)\n\toutput := make(Slice, 0, sizeCollection+sizeElements) // preallocate memory for the output slice\n\n\tif sizeElements == 0 {\n\t\treturn append(output, collection...) // simple copy\n\t} else if i > sizeCollection {\n\t\t// positive overflow\n\t\treturn append(append(output, collection...), elements...)\n\t} else if i < -sizeCollection {\n\t\t// negative overflow\n\t\treturn append(append(output, elements...), collection...)\n\t} else if i < 0 {\n\t\t// backward\n\t\ti = sizeCollection + i\n\t}\n\n\treturn append(append(append(output, collection[:i]...), elements...), collection[i:]...)\n}\n\n```\n# Function Signature: func Fill[T Clonable[T]](collection []T, initial T) []T {\n# Natural Language Annotations: // Fill fills elements of array with `initial` value.\n// Play: https://go.dev/play/p/VwR34GzqEub\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc Fill[T Clonable[T]](collection []T, initial T) []T {\n\tfor i := range collection {\n\t\tcollection[i] = initial\n\t}\n\treturn collection\n}\n```", "  ```\nfunc Fill[T Clonable[T]](collection []T, initial T) []T {\n\tif initial != nil {\n\t\tclone := initial.Clone()\n\t\tfor i := range collection {\n\t\t\tcollection[i] = clone\n\t\t}\n\t}\n\treturn collection\n}\n```", "  ```\nfunc Fill[T Clonable[T]](collection []T, initial T) []T {\n\tresult := make([]T, len(collection))\n\t\n\tfor i, _ := range collection {\n\t\tresult[i] = Clone(initial)\n\t}\n\t\n\treturn result\n}\n```"]}, "lo-slice.go-Repeat": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/slice.go:\n```\npackage lo\n\nimport (\n\t\"sort\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// Filter iterates over elements of collection, returning an array of all elements predicate returns truthy for.\n// Play: https://go.dev/play/p/Apjg3WeSi7K\nfunc Filter[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Map manipulates a slice and transforms it to a slice of another type.\n// Play: https://go.dev/play/p/OkPcYAhBo0D\nfunc Map[T any, R any](collection []T, iteratee func(item T, index int) R) []R {\n\tresult := make([]R, len(collection))\n\n\tfor i := range collection {\n\t\tresult[i] = iteratee(collection[i], i)\n\t}\n\n\treturn result\n}\n\n// FilterMap returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\n//\n// Play: https://go.dev/play/p/-AuYXfy7opz\nfunc FilterMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FlatMap manipulates a slice and transforms and flattens it to a slice of another type.\n// The transform function can either return a slice or a `nil`, and in the `nil` case\n// no value is added to the final slice.\n// Play: https://go.dev/play/p/YSoYmQTA8-U\nfunc FlatMap[T any, R any](collection []T, iteratee func(item T, index int) []R) []R {\n\tresult := make([]R, 0, len(collection))\n\n\tfor i := range collection {\n\t\tresult = append(result, iteratee(collection[i], i)...)\n\t}\n\n\treturn result\n}\n\n// Reduce reduces collection to a value which is the accumulated result of running each element in collection\n// through accumulator, where each successive invocation is supplied the return value of the previous.\n// Play: https://go.dev/play/p/R4UHXZNaaUG\nfunc Reduce[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := range collection {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ReduceRight helper is like Reduce except that it iterates over elements of collection from right to left.\n// Play: https://go.dev/play/p/Fq3W70l7wXF\nfunc ReduceRight[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := len(collection) - 1; i >= 0; i-- {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ForEach iterates over elements of collection and invokes iteratee for each element.\n// Play: https://go.dev/play/p/oofyiUPRf8t\nfunc ForEach[T any](collection []T, iteratee func(item T, index int)) {\n\tfor i := range collection {\n\t\titeratee(collection[i], i)\n\t}\n}\n\n// Times invokes the iteratee n times, returning an array of the results of each invocation.\n// The iteratee is invoked with index as argument.\n// Play: https://go.dev/play/p/vgQj3Glr6lT\nfunc Times[T any](count int, iteratee func(index int) T) []T {\n\tresult := make([]T, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult[i] = iteratee(i)\n\t}\n\n\treturn result\n}\n\n// Uniq returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array.\n// Play: https://go.dev/play/p/DTzbeXZ6iEN\nfunc Uniq[T comparable, Slice ~[]T](collection Slice) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[T]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tif _, ok := seen[collection[i]]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[collection[i]] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// UniqBy returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\n// Play: https://go.dev/play/p/g42Z3QSb53u\nfunc UniqBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[U]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif _, ok := seen[key]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[key] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// GroupBy returns an object composed of keys generated from the results of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/XnQBd_v6brd\nfunc GroupBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) map[U]Slice {\n\tresult := map[U]Slice{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresult[key] = append(result[key], collection[i])\n\t}\n\n\treturn result\n}\n\n// Chunk returns an array of elements split into groups the length of size. If array can't be split evenly,\n// the final chunk will be the remaining elements.\n// Play: https://go.dev/play/p/EeKl0AuTehH\nfunc Chunk[T any, Slice ~[]T](collection Slice, size int) []Slice {\n\tif size <= 0 {\n\t\tpanic(\"Second parameter must be greater than 0\")\n\t}\n\n\tchunksNum := len(collection) / size\n\tif len(collection)%size != 0 {\n\t\tchunksNum += 1\n\t}\n\n\tresult := make([]Slice, 0, chunksNum)\n\n\tfor i := 0; i < chunksNum; i++ {\n\t\tlast := (i + 1) * size\n\t\tif last > len(collection) {\n\t\t\tlast = len(collection)\n\t\t}\n\t\tresult = append(result, collection[i*size:last])\n\t}\n\n\treturn result\n}\n\n// PartitionBy returns an array of elements split into groups. The order of grouped values is\n// determined by the order they occur in collection. The grouping is generated from the results\n// of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/NfQ_nGjkgXW\nfunc PartitionBy[T any, K comparable, Slice ~[]T](collection Slice, iteratee func(item T) K) []Slice {\n\tresult := []Slice{}\n\tseen := map[K]int{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresultIndex, ok := seen[key]\n\t\tif !ok {\n\t\t\tresultIndex = len(result)\n\t\t\tseen[key] = resultIndex\n\t\t\tresult = append(result, Slice{})\n\t\t}\n\n\t\tresult[resultIndex] = append(result[resultIndex], collection[i])\n\t}\n\n\treturn result\n\n\t// unordered:\n\t// groups := GroupBy[T, K](collection, iteratee)\n\t// return Values[K, []T](groups)\n}\n\n// Flatten returns an array a single level deep.\n// Play: https://go.dev/play/p/rbp9ORaMpjw\nfunc Flatten[T any, Slice ~[]T](collection []Slice) Slice {\n\ttotalLen := 0\n\tfor i := range collection {\n\t\ttotalLen += len(collection[i])\n\t}\n\n\tresult := make(Slice, 0, totalLen)\n\tfor i := range collection {\n\t\tresult = append(result, collection[i]...)\n\t}\n\n\treturn result\n}\n\n// Interleave round-robin alternating input slices and sequentially appending value at index into result\n// Play: https://go.dev/play/p/-RJkTLQEDVt\nfunc Interleave[T any, Slice ~[]T](collections ...Slice) Slice {\n\tif len(collections) == 0 {\n\t\treturn Slice{}\n\t}\n\n\tmaxSize := 0\n\ttotalSize := 0\n\tfor i := range collections {\n\t\tsize := len(collections[i])\n\t\ttotalSize += size\n\t\tif size > maxSize {\n\t\t\tmaxSize = size\n\t\t}\n\t}\n\n\tif maxSize == 0 {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, totalSize)\n\n\tresultIdx := 0\n\tfor i := 0; i < maxSize; i++ {\n\t\tfor j := range collections {\n\t\t\tif len(collections[j])-1 < i {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tresult[resultIdx] = collections[j][i]\n\t\t\tresultIdx++\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Shuffle returns an array of shuffled values. Uses the Fisher-Yates shuffle algorithm.\n// Play: https://go.dev/play/p/Qp73bnTDnc7\n\n\n\n\n\n\n\n\n// Reverse reverses array so that the first element becomes the last, the second element becomes the second to last, and so on.\n// Play: https://go.dev/play/p/fhUMLvZ7vS6\nfunc Reverse[T any, Slice ~[]T](collection Slice) Slice {\n\tlength := len(collection)\n\thalf := length / 2\n\n\tfor i := 0; i < half; i = i + 1 {\n\t\tj := length - 1 - i\n\t\tcollection[i], collection[j] = collection[j], collection[i]\n\t}\n\n\treturn collection\n}\n\n// Fill fills elements of array with `initial` value.\n// Play: https://go.dev/play/p/VwR34GzqEub\n\n\n\n\n\n\n\n\n\n\n// Repeat builds a slice with N copies of initial value.\n// Play: https://go.dev/play/p/g3uHXbmc3b6\n\n\n\n\n\n\n\n\n\n\n// RepeatBy builds a slice with values returned by N calls of callback.\n// Play: https://go.dev/play/p/ozZLCtX_hNU\nfunc RepeatBy[T any](count int, predicate func(index int) T) []T {\n\tresult := make([]T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult = append(result, predicate(i))\n\t}\n\n\treturn result\n}\n\n// KeyBy transforms a slice or an array of structs to a map based on a pivot callback.\n// Play: https://go.dev/play/p/mdaClUAT-zZ\nfunc KeyBy[K comparable, V any](collection []V, iteratee func(item V) K) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk := iteratee(collection[i])\n\t\tresult[k] = collection[i]\n\t}\n\n\treturn result\n}\n\n// Associate returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc Associate[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk, v := transform(collection[i])\n\t\tresult[k] = v\n\t}\n\n\treturn result\n}\n\n// SliceToMap returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Alias of Associate().\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc SliceToMap[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\treturn Associate(collection, transform)\n}\n\n// Drop drops n elements from the beginning of a slice or array.\n// Play: https://go.dev/play/p/JswS7vXRJP2\nfunc Drop[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn make(Slice, 0)\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\n\treturn append(result, collection[n:]...)\n}\n\n// DropRight drops n elements from the end of a slice or array.\n// Play: https://go.dev/play/p/GG0nXkSJJa3\nfunc DropRight[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\treturn append(result, collection[:len(collection)-n]...)\n}\n\n// DropWhile drops elements from the beginning of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/7gBPYw2IK16\nfunc DropWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := 0\n\tfor ; i < len(collection); i++ {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-i)\n\treturn append(result, collection[i:]...)\n}\n\n// DropRightWhile drops elements from the end of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/3-n71oEC0Hz\nfunc DropRightWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := len(collection) - 1\n\tfor ; i >= 0; i-- {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, i+1)\n\treturn append(result, collection[:i+1]...)\n}\n\n// DropByIndex drops elements from a slice or array by the index.\n// A negative index will drop elements from the end of the slice.\n// Play: https://go.dev/play/p/bPIH4npZRxS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Reject is the opposite of Filter, this method returns the elements of collection that predicate does not return truthy for.\n// Play: https://go.dev/play/p/YkLMODy1WEL\nfunc Reject[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := Slice{}\n\n\tfor i := range collection {\n\t\tif !predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// RejectMap is the opposite of FilterMap, this method returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\nfunc RejectMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); !ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FilterReject mixes Filter and Reject, this method returns two slices, one for the elements of collection that\n// predicate returns truthy for and one for the elements that predicate does not return truthy for.\nfunc FilterReject[T any, Slice ~[]T](collection Slice, predicate func(T, int) bool) (kept Slice, rejected Slice) {\n\tkept = make(Slice, 0, len(collection))\n\trejected = make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tkept = append(kept, collection[i])\n\t\t} else {\n\t\t\trejected = append(rejected, collection[i])\n\t\t}\n\t}\n\n\treturn kept, rejected\n}\n\n// Count counts the number of elements in the collection that compare equal to value.\n// Play: https://go.dev/play/p/Y3FlK54yveC\nfunc Count[T comparable](collection []T, value T) (count int) {\n\tfor i := range collection {\n\t\tif collection[i] == value {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountBy counts the number of elements in the collection for which predicate is true.\n// Play: https://go.dev/play/p/ByQbNYQQi4X\nfunc CountBy[T any](collection []T, predicate func(item T) bool) (count int) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountValues counts the number of each element in the collection.\n// Play: https://go.dev/play/p/-p-PyLT4dfy\nfunc CountValues[T comparable](collection []T) map[T]int {\n\tresult := make(map[T]int)\n\n\tfor i := range collection {\n\t\tresult[collection[i]]++\n\t}\n\n\treturn result\n}\n\n// CountValuesBy counts the number of each element return from mapper function.\n// Is equivalent to chaining lo.Map and lo.CountValues.\n// Play: https://go.dev/play/p/2U0dG1SnOmS\nfunc CountValuesBy[T any, U comparable](collection []T, mapper func(item T) U) map[U]int {\n\tresult := make(map[U]int)\n\n\tfor i := range collection {\n\t\tresult[mapper(collection[i])]++\n\t}\n\n\treturn result\n}\n\n// Subset returns a copy of a slice from `offset` up to `length` elements. Like `slice[start:start+length]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/tOQu1GhFcog\nfunc Subset[T any, Slice ~[]T](collection Slice, offset int, length uint) Slice {\n\tsize := len(collection)\n\n\tif offset < 0 {\n\t\toffset = size + offset\n\t\tif offset < 0 {\n\t\t\toffset = 0\n\t\t}\n\t}\n\n\tif offset > size {\n\t\treturn Slice{}\n\t}\n\n\tif length > uint(size)-uint(offset) {\n\t\tlength = uint(size - offset)\n\t}\n\n\treturn collection[offset : offset+int(length)]\n}\n\n// Slice returns a copy of a slice from `start` up to, but not including `end`. Like `slice[start:end]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/8XWYhfMMA1h\nfunc Slice[T any, Slice ~[]T](collection Slice, start int, end int) Slice {\n\tsize := len(collection)\n\n\tif start >= end {\n\t\treturn Slice{}\n\t}\n\n\tif start > size {\n\t\tstart = size\n\t}\n\tif start < 0 {\n\t\tstart = 0\n\t}\n\n\tif end > size {\n\t\tend = size\n\t}\n\tif end < 0 {\n\t\tend = 0\n\t}\n\n\treturn collection[start:end]\n}\n\n// Replace returns a copy of the slice with the first n non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/XfPzmf9gql6\nfunc Replace[T comparable, Slice ~[]T](collection Slice, old T, new T, n int) Slice {\n\tresult := make(Slice, len(collection))\n\tcopy(result, collection)\n\n\tfor i := range result {\n\t\tif result[i] == old && n != 0 {\n\t\t\tresult[i] = new\n\t\t\tn--\n\t\t}\n\t}\n\n\treturn result\n}\n\n// ReplaceAll returns a copy of the slice with all non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/a9xZFUHfYcV\nfunc ReplaceAll[T comparable, Slice ~[]T](collection Slice, old T, new T) Slice {\n\treturn Replace(collection, old, new, -1)\n}\n\n// Compact returns a slice of all non-zero elements.\n// Play: https://go.dev/play/p/tXiy-iK6PAc\nfunc Compact[T comparable, Slice ~[]T](collection Slice) Slice {\n\tvar zero T\n\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif collection[i] != zero {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// IsSorted checks if a slice is sorted.\n// Play: https://go.dev/play/p/mc3qR-t4mcx\nfunc IsSorted[T constraints.Ordered](collection []T) bool {\n\tfor i := 1; i < len(collection); i++ {\n\t\tif collection[i-1] > collection[i] {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// IsSortedByKey checks if a slice is sorted by iteratee.\n// Play: https://go.dev/play/p/wiG6XyBBu49\nfunc IsSortedByKey[T any, K constraints.Ordered](collection []T, iteratee func(item T) K) bool {\n\tsize := len(collection)\n\n\tfor i := 0; i < size-1; i++ {\n\t\tif iteratee(collection[i]) > iteratee(collection[i+1]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Splice inserts multiple elements at index i. A negative index counts back\n// from the end of the slice. The helper is protected against overflow errors.\n// Play: https://go.dev/play/p/G5_GhkeSUBA\nfunc Splice[T any, Slice ~[]T](collection Slice, i int, elements ...T) Slice {\n\tsizeCollection := len(collection)\n\tsizeElements := len(elements)\n\toutput := make(Slice, 0, sizeCollection+sizeElements) // preallocate memory for the output slice\n\n\tif sizeElements == 0 {\n\t\treturn append(output, collection...) // simple copy\n\t} else if i > sizeCollection {\n\t\t// positive overflow\n\t\treturn append(append(output, collection...), elements...)\n\t} else if i < -sizeCollection {\n\t\t// negative overflow\n\t\treturn append(append(output, elements...), collection...)\n\t} else if i < 0 {\n\t\t// backward\n\t\ti = sizeCollection + i\n\t}\n\n\treturn append(append(append(output, collection[:i]...), elements...), collection[i:]...)\n}\n\n```\n# Function Signature: func Repeat[T Clonable[T]](count int, initial T) []T {\n# Natural Language Annotations: // Repeat builds a slice with N copies of initial value.\n// Play: https://go.dev/play/p/g3uHXbmc3b6\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc Repeat[T Clonable[T]](count int, initial T) []T {\n\tresult := make([]T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult = append(result, initial)\n\t}\n\n\treturn result\n}\n```", "  ```\nfunc Repeat[T Clonable[T]](count int, initial T) []T {\n\tresult := make([]T, count)\n\n\tfor i := range result {\n\t\tresult[i] = initial.Clone()\n\t}\n\n\treturn result\n}\n```", "  ```\nfunc Repeat[T Clonable[T]](count int, initial T) []T {\n\tif count == 0 {\n\t\treturn make([]T, 0)\n\t}\n\n\tresult := make([]T, 0, count)\n\tfor i := 0; i < count; i++ {\n\t\tresult = append(result, reflect.Clone(initial).Interface())\n\t}\n\treturn result\n}\n```"]}, "lo-slice.go-DropByIndex": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/slice.go:\n```\npackage lo\n\nimport (\n\t\"sort\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// Filter iterates over elements of collection, returning an array of all elements predicate returns truthy for.\n// Play: https://go.dev/play/p/Apjg3WeSi7K\nfunc Filter[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Map manipulates a slice and transforms it to a slice of another type.\n// Play: https://go.dev/play/p/OkPcYAhBo0D\nfunc Map[T any, R any](collection []T, iteratee func(item T, index int) R) []R {\n\tresult := make([]R, len(collection))\n\n\tfor i := range collection {\n\t\tresult[i] = iteratee(collection[i], i)\n\t}\n\n\treturn result\n}\n\n// FilterMap returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\n//\n// Play: https://go.dev/play/p/-AuYXfy7opz\nfunc FilterMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FlatMap manipulates a slice and transforms and flattens it to a slice of another type.\n// The transform function can either return a slice or a `nil`, and in the `nil` case\n// no value is added to the final slice.\n// Play: https://go.dev/play/p/YSoYmQTA8-U\nfunc FlatMap[T any, R any](collection []T, iteratee func(item T, index int) []R) []R {\n\tresult := make([]R, 0, len(collection))\n\n\tfor i := range collection {\n\t\tresult = append(result, iteratee(collection[i], i)...)\n\t}\n\n\treturn result\n}\n\n// Reduce reduces collection to a value which is the accumulated result of running each element in collection\n// through accumulator, where each successive invocation is supplied the return value of the previous.\n// Play: https://go.dev/play/p/R4UHXZNaaUG\nfunc Reduce[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := range collection {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ReduceRight helper is like Reduce except that it iterates over elements of collection from right to left.\n// Play: https://go.dev/play/p/Fq3W70l7wXF\nfunc ReduceRight[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := len(collection) - 1; i >= 0; i-- {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ForEach iterates over elements of collection and invokes iteratee for each element.\n// Play: https://go.dev/play/p/oofyiUPRf8t\nfunc ForEach[T any](collection []T, iteratee func(item T, index int)) {\n\tfor i := range collection {\n\t\titeratee(collection[i], i)\n\t}\n}\n\n// Times invokes the iteratee n times, returning an array of the results of each invocation.\n// The iteratee is invoked with index as argument.\n// Play: https://go.dev/play/p/vgQj3Glr6lT\nfunc Times[T any](count int, iteratee func(index int) T) []T {\n\tresult := make([]T, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult[i] = iteratee(i)\n\t}\n\n\treturn result\n}\n\n// Uniq returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array.\n// Play: https://go.dev/play/p/DTzbeXZ6iEN\nfunc Uniq[T comparable, Slice ~[]T](collection Slice) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[T]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tif _, ok := seen[collection[i]]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[collection[i]] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// UniqBy returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\n// Play: https://go.dev/play/p/g42Z3QSb53u\nfunc UniqBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[U]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif _, ok := seen[key]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[key] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// GroupBy returns an object composed of keys generated from the results of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/XnQBd_v6brd\nfunc GroupBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) map[U]Slice {\n\tresult := map[U]Slice{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresult[key] = append(result[key], collection[i])\n\t}\n\n\treturn result\n}\n\n// Chunk returns an array of elements split into groups the length of size. If array can't be split evenly,\n// the final chunk will be the remaining elements.\n// Play: https://go.dev/play/p/EeKl0AuTehH\nfunc Chunk[T any, Slice ~[]T](collection Slice, size int) []Slice {\n\tif size <= 0 {\n\t\tpanic(\"Second parameter must be greater than 0\")\n\t}\n\n\tchunksNum := len(collection) / size\n\tif len(collection)%size != 0 {\n\t\tchunksNum += 1\n\t}\n\n\tresult := make([]Slice, 0, chunksNum)\n\n\tfor i := 0; i < chunksNum; i++ {\n\t\tlast := (i + 1) * size\n\t\tif last > len(collection) {\n\t\t\tlast = len(collection)\n\t\t}\n\t\tresult = append(result, collection[i*size:last])\n\t}\n\n\treturn result\n}\n\n// PartitionBy returns an array of elements split into groups. The order of grouped values is\n// determined by the order they occur in collection. The grouping is generated from the results\n// of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/NfQ_nGjkgXW\nfunc PartitionBy[T any, K comparable, Slice ~[]T](collection Slice, iteratee func(item T) K) []Slice {\n\tresult := []Slice{}\n\tseen := map[K]int{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresultIndex, ok := seen[key]\n\t\tif !ok {\n\t\t\tresultIndex = len(result)\n\t\t\tseen[key] = resultIndex\n\t\t\tresult = append(result, Slice{})\n\t\t}\n\n\t\tresult[resultIndex] = append(result[resultIndex], collection[i])\n\t}\n\n\treturn result\n\n\t// unordered:\n\t// groups := GroupBy[T, K](collection, iteratee)\n\t// return Values[K, []T](groups)\n}\n\n// Flatten returns an array a single level deep.\n// Play: https://go.dev/play/p/rbp9ORaMpjw\nfunc Flatten[T any, Slice ~[]T](collection []Slice) Slice {\n\ttotalLen := 0\n\tfor i := range collection {\n\t\ttotalLen += len(collection[i])\n\t}\n\n\tresult := make(Slice, 0, totalLen)\n\tfor i := range collection {\n\t\tresult = append(result, collection[i]...)\n\t}\n\n\treturn result\n}\n\n// Interleave round-robin alternating input slices and sequentially appending value at index into result\n// Play: https://go.dev/play/p/-RJkTLQEDVt\nfunc Interleave[T any, Slice ~[]T](collections ...Slice) Slice {\n\tif len(collections) == 0 {\n\t\treturn Slice{}\n\t}\n\n\tmaxSize := 0\n\ttotalSize := 0\n\tfor i := range collections {\n\t\tsize := len(collections[i])\n\t\ttotalSize += size\n\t\tif size > maxSize {\n\t\t\tmaxSize = size\n\t\t}\n\t}\n\n\tif maxSize == 0 {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, totalSize)\n\n\tresultIdx := 0\n\tfor i := 0; i < maxSize; i++ {\n\t\tfor j := range collections {\n\t\t\tif len(collections[j])-1 < i {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tresult[resultIdx] = collections[j][i]\n\t\t\tresultIdx++\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Shuffle returns an array of shuffled values. Uses the Fisher-Yates shuffle algorithm.\n// Play: https://go.dev/play/p/Qp73bnTDnc7\n\n\n\n\n\n\n\n\n// Reverse reverses array so that the first element becomes the last, the second element becomes the second to last, and so on.\n// Play: https://go.dev/play/p/fhUMLvZ7vS6\nfunc Reverse[T any, Slice ~[]T](collection Slice) Slice {\n\tlength := len(collection)\n\thalf := length / 2\n\n\tfor i := 0; i < half; i = i + 1 {\n\t\tj := length - 1 - i\n\t\tcollection[i], collection[j] = collection[j], collection[i]\n\t}\n\n\treturn collection\n}\n\n// Fill fills elements of array with `initial` value.\n// Play: https://go.dev/play/p/VwR34GzqEub\n\n\n\n\n\n\n\n\n\n\n// Repeat builds a slice with N copies of initial value.\n// Play: https://go.dev/play/p/g3uHXbmc3b6\n\n\n\n\n\n\n\n\n\n\n// RepeatBy builds a slice with values returned by N calls of callback.\n// Play: https://go.dev/play/p/ozZLCtX_hNU\nfunc RepeatBy[T any](count int, predicate func(index int) T) []T {\n\tresult := make([]T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult = append(result, predicate(i))\n\t}\n\n\treturn result\n}\n\n// KeyBy transforms a slice or an array of structs to a map based on a pivot callback.\n// Play: https://go.dev/play/p/mdaClUAT-zZ\nfunc KeyBy[K comparable, V any](collection []V, iteratee func(item V) K) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk := iteratee(collection[i])\n\t\tresult[k] = collection[i]\n\t}\n\n\treturn result\n}\n\n// Associate returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc Associate[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk, v := transform(collection[i])\n\t\tresult[k] = v\n\t}\n\n\treturn result\n}\n\n// SliceToMap returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Alias of Associate().\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc SliceToMap[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\treturn Associate(collection, transform)\n}\n\n// Drop drops n elements from the beginning of a slice or array.\n// Play: https://go.dev/play/p/JswS7vXRJP2\nfunc Drop[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn make(Slice, 0)\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\n\treturn append(result, collection[n:]...)\n}\n\n// DropRight drops n elements from the end of a slice or array.\n// Play: https://go.dev/play/p/GG0nXkSJJa3\nfunc DropRight[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\treturn append(result, collection[:len(collection)-n]...)\n}\n\n// DropWhile drops elements from the beginning of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/7gBPYw2IK16\nfunc DropWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := 0\n\tfor ; i < len(collection); i++ {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-i)\n\treturn append(result, collection[i:]...)\n}\n\n// DropRightWhile drops elements from the end of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/3-n71oEC0Hz\nfunc DropRightWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := len(collection) - 1\n\tfor ; i >= 0; i-- {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, i+1)\n\treturn append(result, collection[:i+1]...)\n}\n\n// DropByIndex drops elements from a slice or array by the index.\n// A negative index will drop elements from the end of the slice.\n// Play: https://go.dev/play/p/bPIH4npZRxS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Reject is the opposite of Filter, this method returns the elements of collection that predicate does not return truthy for.\n// Play: https://go.dev/play/p/YkLMODy1WEL\nfunc Reject[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := Slice{}\n\n\tfor i := range collection {\n\t\tif !predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// RejectMap is the opposite of FilterMap, this method returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\nfunc RejectMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); !ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FilterReject mixes Filter and Reject, this method returns two slices, one for the elements of collection that\n// predicate returns truthy for and one for the elements that predicate does not return truthy for.\nfunc FilterReject[T any, Slice ~[]T](collection Slice, predicate func(T, int) bool) (kept Slice, rejected Slice) {\n\tkept = make(Slice, 0, len(collection))\n\trejected = make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tkept = append(kept, collection[i])\n\t\t} else {\n\t\t\trejected = append(rejected, collection[i])\n\t\t}\n\t}\n\n\treturn kept, rejected\n}\n\n// Count counts the number of elements in the collection that compare equal to value.\n// Play: https://go.dev/play/p/Y3FlK54yveC\nfunc Count[T comparable](collection []T, value T) (count int) {\n\tfor i := range collection {\n\t\tif collection[i] == value {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountBy counts the number of elements in the collection for which predicate is true.\n// Play: https://go.dev/play/p/ByQbNYQQi4X\nfunc CountBy[T any](collection []T, predicate func(item T) bool) (count int) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountValues counts the number of each element in the collection.\n// Play: https://go.dev/play/p/-p-PyLT4dfy\nfunc CountValues[T comparable](collection []T) map[T]int {\n\tresult := make(map[T]int)\n\n\tfor i := range collection {\n\t\tresult[collection[i]]++\n\t}\n\n\treturn result\n}\n\n// CountValuesBy counts the number of each element return from mapper function.\n// Is equivalent to chaining lo.Map and lo.CountValues.\n// Play: https://go.dev/play/p/2U0dG1SnOmS\nfunc CountValuesBy[T any, U comparable](collection []T, mapper func(item T) U) map[U]int {\n\tresult := make(map[U]int)\n\n\tfor i := range collection {\n\t\tresult[mapper(collection[i])]++\n\t}\n\n\treturn result\n}\n\n// Subset returns a copy of a slice from `offset` up to `length` elements. Like `slice[start:start+length]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/tOQu1GhFcog\nfunc Subset[T any, Slice ~[]T](collection Slice, offset int, length uint) Slice {\n\tsize := len(collection)\n\n\tif offset < 0 {\n\t\toffset = size + offset\n\t\tif offset < 0 {\n\t\t\toffset = 0\n\t\t}\n\t}\n\n\tif offset > size {\n\t\treturn Slice{}\n\t}\n\n\tif length > uint(size)-uint(offset) {\n\t\tlength = uint(size - offset)\n\t}\n\n\treturn collection[offset : offset+int(length)]\n}\n\n// Slice returns a copy of a slice from `start` up to, but not including `end`. Like `slice[start:end]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/8XWYhfMMA1h\nfunc Slice[T any, Slice ~[]T](collection Slice, start int, end int) Slice {\n\tsize := len(collection)\n\n\tif start >= end {\n\t\treturn Slice{}\n\t}\n\n\tif start > size {\n\t\tstart = size\n\t}\n\tif start < 0 {\n\t\tstart = 0\n\t}\n\n\tif end > size {\n\t\tend = size\n\t}\n\tif end < 0 {\n\t\tend = 0\n\t}\n\n\treturn collection[start:end]\n}\n\n// Replace returns a copy of the slice with the first n non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/XfPzmf9gql6\nfunc Replace[T comparable, Slice ~[]T](collection Slice, old T, new T, n int) Slice {\n\tresult := make(Slice, len(collection))\n\tcopy(result, collection)\n\n\tfor i := range result {\n\t\tif result[i] == old && n != 0 {\n\t\t\tresult[i] = new\n\t\t\tn--\n\t\t}\n\t}\n\n\treturn result\n}\n\n// ReplaceAll returns a copy of the slice with all non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/a9xZFUHfYcV\nfunc ReplaceAll[T comparable, Slice ~[]T](collection Slice, old T, new T) Slice {\n\treturn Replace(collection, old, new, -1)\n}\n\n// Compact returns a slice of all non-zero elements.\n// Play: https://go.dev/play/p/tXiy-iK6PAc\nfunc Compact[T comparable, Slice ~[]T](collection Slice) Slice {\n\tvar zero T\n\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif collection[i] != zero {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// IsSorted checks if a slice is sorted.\n// Play: https://go.dev/play/p/mc3qR-t4mcx\nfunc IsSorted[T constraints.Ordered](collection []T) bool {\n\tfor i := 1; i < len(collection); i++ {\n\t\tif collection[i-1] > collection[i] {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// IsSortedByKey checks if a slice is sorted by iteratee.\n// Play: https://go.dev/play/p/wiG6XyBBu49\nfunc IsSortedByKey[T any, K constraints.Ordered](collection []T, iteratee func(item T) K) bool {\n\tsize := len(collection)\n\n\tfor i := 0; i < size-1; i++ {\n\t\tif iteratee(collection[i]) > iteratee(collection[i+1]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Splice inserts multiple elements at index i. A negative index counts back\n// from the end of the slice. The helper is protected against overflow errors.\n// Play: https://go.dev/play/p/G5_GhkeSUBA\nfunc Splice[T any, Slice ~[]T](collection Slice, i int, elements ...T) Slice {\n\tsizeCollection := len(collection)\n\tsizeElements := len(elements)\n\toutput := make(Slice, 0, sizeCollection+sizeElements) // preallocate memory for the output slice\n\n\tif sizeElements == 0 {\n\t\treturn append(output, collection...) // simple copy\n\t} else if i > sizeCollection {\n\t\t// positive overflow\n\t\treturn append(append(output, collection...), elements...)\n\t} else if i < -sizeCollection {\n\t\t// negative overflow\n\t\treturn append(append(output, elements...), collection...)\n\t} else if i < 0 {\n\t\t// backward\n\t\ti = sizeCollection + i\n\t}\n\n\treturn append(append(append(output, collection[:i]...), elements...), collection[i:]...)\n}\n\n```\n# Function Signature: func DropByIndex[T any](collection []T, indexes ...int) []T {\n# Natural Language Annotations: // DropByIndex drops elements from a slice or array by the index.\n// A negative index will drop elements from the end of the slice.\n// Play: https://go.dev/play/p/bPIH4npZRxS\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc DropByIndex[T any](collection []T, indexes ...int) []T {\n\tresult := make([]T, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif !contains(i, indexes) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\nfunc contains(i int, indexes []int) bool {\n\tfor _, index := range indexes {\n\t\tif i == index {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n```", "  ```\nfunc DropByIndex[T any](collection []T, indexes ...int) []T {\n\tnewCollection := make([]T, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif !isIndexPresent(i, indexes) {\n\t\t\tnewCollection = append(newCollection, collection[i])\n\t\t}\n\t}\n\n\treturn newCollection\n}\n\nfunc isIndexPresent(index int, indexes []int) bool {\n\tfor _, idx := range indexes {\n\t\tif index == idx {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n```", "  ```\nfunc DropByIndex[T any](collection []T, indexes ...int) []T {\n\tsizeCollection := len(collection)\n\tsizeIndexes := len(indexes)\n\toutput := make([]T, 0, sizeCollection) // preallocate memory for the output slice\n\n\tif sizeIndexes == 0 {\n\t\treturn collection // no indexes provided\n\t} else if indexes[0] > sizeCollection {\n\t\t// positive overflow\n\t\treturn output\n\t} else if indexes[0] < -sizeCollection {\n\t\t// negative overflow\n\t\treturn output\n\t} else if indexes[0] < 0 {\n\t\t// backward\n\t\tindexes[0] = sizeCollection + indexes[0]\n\t}\n\n\tfor index, value := range collection {\n\t\tfound := false\n\t\tfor j := range indexes {\n\t\t\tif indexes[j] == index {\n\t\t\t\tfound = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !found {\n\t\t\toutput = append(output, value)\n\t\t}\n\t}\n\n\treturn output\n}\n```"]}, "lo-intersect.go-Some": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/intersect.go:\n```\npackage lo\n\n// Contains returns true if an element is present in a collection.\nfunc Contains[T comparable](collection []T, element T) bool {\n\tfor i := range collection {\n\t\tif collection[i] == element {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\n// ContainsBy returns true if predicate function return true.\nfunc ContainsBy[T any](collection []T, predicate func(item T) bool) bool {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\n// Every returns true if all elements of a subset are contained into a collection or if the subset is empty.\nfunc Every[T comparable](collection []T, subset []T) bool {\n\tfor i := range subset {\n\t\tif !Contains(collection, subset[i]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// EveryBy returns true if the predicate returns true for all of the elements in the collection or if the collection is empty.\nfunc EveryBy[T any](collection []T, predicate func(item T) bool) bool {\n\tfor i := range collection {\n\t\tif !predicate(collection[i]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Some returns true if at least 1 element of a subset is contained into a collection.\n// If the subset is empty Some returns false.\n\n\n\n\n\n\n\n\n\n\n// SomeBy returns true if the predicate returns true for any of the elements in the collection.\n// If the collection is empty SomeBy returns false.\nfunc SomeBy[T any](collection []T, predicate func(item T) bool) bool {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\n// None returns true if no element of a subset are contained into a collection or if the subset is empty.\nfunc None[T comparable](collection []T, subset []T) bool {\n\tfor i := range subset {\n\t\tif Contains(collection, subset[i]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// NoneBy returns true if the predicate returns true for none of the elements in the collection or if the collection is empty.\nfunc NoneBy[T any](collection []T, predicate func(item T) bool) bool {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Intersect returns the intersection between two collections.\nfunc Intersect[T comparable, Slice ~[]T](list1 Slice, list2 Slice) Slice {\n\tresult := Slice{}\n\tseen := map[T]struct{}{}\n\n\tfor i := range list1 {\n\t\tseen[list1[i]] = struct{}{}\n\t}\n\n\tfor i := range list2 {\n\t\tif _, ok := seen[list2[i]]; ok {\n\t\t\tresult = append(result, list2[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Difference returns the difference between two collections.\n// The first value is the collection of element absent of list2.\n// The second value is the collection of element absent of list1.\nfunc Difference[T comparable, Slice ~[]T](list1 Slice, list2 Slice) (Slice, Slice) {\n\tleft := Slice{}\n\tright := Slice{}\n\n\tseenLeft := map[T]struct{}{}\n\tseenRight := map[T]struct{}{}\n\n\tfor i := range list1 {\n\t\tseenLeft[list1[i]] = struct{}{}\n\t}\n\n\tfor i := range list2 {\n\t\tseenRight[list2[i]] = struct{}{}\n\t}\n\n\tfor i := range list1 {\n\t\tif _, ok := seenRight[list1[i]]; !ok {\n\t\t\tleft = append(left, list1[i])\n\t\t}\n\t}\n\n\tfor i := range list2 {\n\t\tif _, ok := seenLeft[list2[i]]; !ok {\n\t\t\tright = append(right, list2[i])\n\t\t}\n\t}\n\n\treturn left, right\n}\n\n// Union returns all distinct elements from given collections.\n// result returns will not change the order of elements relatively.\nfunc Union[T comparable, Slice ~[]T](lists ...Slice) Slice {\n\tvar capLen int\n\n\tfor _, list := range lists {\n\t\tcapLen += len(list)\n\t}\n\n\tresult := make(Slice, 0, capLen)\n\tseen := make(map[T]struct{}, capLen)\n\n\tfor i := range lists {\n\t\tfor j := range lists[i] {\n\t\t\tif _, ok := seen[lists[i][j]]; !ok {\n\t\t\t\tseen[lists[i][j]] = struct{}{}\n\t\t\t\tresult = append(result, lists[i][j])\n\t\t\t}\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Without returns slice excluding all given values.\nfunc Without[T comparable, Slice ~[]T](collection Slice, exclude ...T) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tfor i := range collection {\n\t\tif !Contains(exclude, collection[i]) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\treturn result\n}\n\n// WithoutEmpty returns slice excluding empty values.\n//\n// Deprecated: Use lo.Compact instead.\nfunc WithoutEmpty[T comparable, Slice ~[]T](collection Slice) Slice {\n\treturn Compact(collection)\n}\n\n```\n# Function Signature: func Some[T comparable](collection []T, subset []T) bool {\n# Natural Language Annotations: // Some returns true if at least 1 element of a subset is contained into a collection.\n// If the subset is empty Some returns false.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc Some[T comparable](collection []T, subset []T) bool {\n\tfor i := range subset {\n\t\tif Contains(collection, subset[i]) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n```", "  ```\nfunc Some[T comparable](collection []T, subset []T) bool {\n\tfor i := range subset {\n\t\tif Contains(collection, subset[i]) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n```", "  ```\nfunc Some[T comparable](collection []T, subset []T) bool {\n    // If the subset is empty, return false\n    if len(subset) == 0 {\n        return false\n    }\n\n    for i := range subset {\n        if Contains(collection, subset[i]) {\n            return true\n        }\n    }\n\n    return false\n}\n```"]}, "lo-find.go-FindKey": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/find.go:\n```\npackage lo\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// IndexOf returns the index at which the first occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc IndexOf[T comparable](collection []T, element T) int {\n\tfor i := range collection {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// LastIndexOf returns the index at which the last occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc LastIndexOf[T comparable](collection []T, element T) int {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// Find search an element in a slice based on a predicate. It returns element and true if element was found.\nfunc Find[T any](collection []T, predicate func(item T) bool) (T, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, false\n}\n\n// FindIndexOf searches an element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindLastIndexOf searches last element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindLastIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindOrElse search an element in a slice based on a predicate. It returns the element if found or a given fallback value otherwise.\nfunc FindOrElse[T any](collection []T, fallback T, predicate func(item T) bool) T {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i]\n\t\t}\n\t}\n\n\treturn fallback\n}\n\n// FindKey returns the key of the first value matching.\n\n\n\n\n\n\n\n\n\n\n// FindKeyBy returns the key of the first element predicate returns truthy for.\n\n\n\n\n\n\n\n\n\n\n// FindUniques returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindUniques[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindUniquesBy returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindUniquesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicates returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindDuplicates[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[collection[i]] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicatesBy returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindDuplicatesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[key] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Min search the minimum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Min[T constraints.Ordered](collection []T) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item < min {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// MinBy search the minimum value of a collection using the given comparison function.\n// If several values of the collection are equal to the smallest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MinBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Earliest search the minimum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Earliest(times ...time.Time) time.Time {\n\tvar min time.Time\n\n\tif len(times) == 0 {\n\t\treturn min\n\t}\n\n\tmin = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.Before(min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Max searches the maximum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Max[T constraints.Ordered](collection []T) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item > max {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// MaxBy search the maximum value of a collection using the given comparison function.\n// If several values of the collection are equal to the greatest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MaxBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// Latest search the maximum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Latest(times ...time.Time) time.Time {\n\tvar max time.Time\n\n\tif len(times) == 0 {\n\t\treturn max\n\t}\n\n\tmax = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.After(max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// First returns the first element of a collection and check for availability of the first element.\nfunc First[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[0], true\n}\n\n// FirstOrEmpty returns the first element of a collection or zero value if empty.\nfunc FirstOrEmpty[T any](collection []T) T {\n\ti, _ := First(collection)\n\treturn i\n}\n\n// FirstOr returns the first element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Last returns the last element of a collection or error if empty.\nfunc Last[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[length-1], true\n}\n\n// Returns the last element of a collection or zero value if empty.\nfunc LastOrEmpty[T any](collection []T) T {\n\ti, _ := Last(collection)\n\treturn i\n}\n\n// LastOr returns the last element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Nth returns the element at index `nth` of collection. If `nth` is negative, the nth element\n// from the end is returned. An error is returned when nth is out of slice bounds.\nfunc Nth[T any, N constraints.Integer](collection []T, nth N) (T, error) {\n\tn := int(nth)\n\tl := len(collection)\n\tif n >= l || -n > l {\n\t\tvar t T\n\t\treturn t, fmt.Errorf(\"nth: %d out of slice bounds\", n)\n\t}\n\n\tif n >= 0 {\n\t\treturn collection[n], nil\n\t}\n\treturn collection[l+n], nil\n}\n\n// Sample returns a random item from collection.\n\n\n\n\n\n\n\n\n\n// Samples returns N random unique items from collection.\nfunc Samples[T any, Slice ~[]T](collection Slice, count int) Slice {\n\tsize := len(collection)\n\n\tcopy := append(Slice{}, collection...)\n\n\tresults := Slice{}\n\n\tfor i := 0; i < size && i < count; i++ {\n\t\tcopyLength := size - i\n\n\t\tindex := rand.IntN(size - i)\n\t\tresults = append(results, copy[index])\n\n\t\t// Removes element.\n\t\t// It is faster to swap with last element and remove it.\n\t\tcopy[index] = copy[copyLength-1]\n\t\tcopy = copy[:copyLength-1]\n\t}\n\n\treturn results\n}\n\n```\n# Function Signature: func FindKey[K comparable, V comparable](object map[K]V, value V) (K, bool) {\n# Natural Language Annotations: // FindKey returns the key of the first value matching.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc FindKey[K comparable, V comparable](object map[K]V, value V) (K, bool) {\n\tfor k, v := range object {\n\t\tif v == value {\n\t\t\treturn k, true\n\t\t}\n\t}\n\n\tvar k K\n\treturn k, false\n}\n```", "  ```\nfunc FindKey[K comparable, V comparable](object map[K]V, value V) (K, bool) {\n\tfor key, val := range object {\n\t\tif val == value {\n\t\t\treturn key, true\n\t\t}\n\t}\n\n\tvar key K\n\treturn key, false\n}\n```", "  ```\nfunc FindKey[K comparable, V comparable](object map[K]V, value V) (K, bool) {\n\tfor k, v := range object {\n\t\tif v == value {\n\t\t\treturn k, true\n\t\t}\n\t}\n\n\tvar key K\n\treturn key, false\n}\n```"]}, "lo-find.go-FindKeyBy": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/find.go:\n```\npackage lo\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// IndexOf returns the index at which the first occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc IndexOf[T comparable](collection []T, element T) int {\n\tfor i := range collection {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// LastIndexOf returns the index at which the last occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc LastIndexOf[T comparable](collection []T, element T) int {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// Find search an element in a slice based on a predicate. It returns element and true if element was found.\nfunc Find[T any](collection []T, predicate func(item T) bool) (T, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, false\n}\n\n// FindIndexOf searches an element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindLastIndexOf searches last element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindLastIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindOrElse search an element in a slice based on a predicate. It returns the element if found or a given fallback value otherwise.\nfunc FindOrElse[T any](collection []T, fallback T, predicate func(item T) bool) T {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i]\n\t\t}\n\t}\n\n\treturn fallback\n}\n\n// FindKey returns the key of the first value matching.\n\n\n\n\n\n\n\n\n\n\n// FindKeyBy returns the key of the first element predicate returns truthy for.\n\n\n\n\n\n\n\n\n\n\n// FindUniques returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindUniques[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindUniquesBy returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindUniquesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicates returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindDuplicates[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[collection[i]] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicatesBy returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindDuplicatesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[key] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Min search the minimum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Min[T constraints.Ordered](collection []T) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item < min {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// MinBy search the minimum value of a collection using the given comparison function.\n// If several values of the collection are equal to the smallest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MinBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Earliest search the minimum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Earliest(times ...time.Time) time.Time {\n\tvar min time.Time\n\n\tif len(times) == 0 {\n\t\treturn min\n\t}\n\n\tmin = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.Before(min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Max searches the maximum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Max[T constraints.Ordered](collection []T) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item > max {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// MaxBy search the maximum value of a collection using the given comparison function.\n// If several values of the collection are equal to the greatest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MaxBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// Latest search the maximum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Latest(times ...time.Time) time.Time {\n\tvar max time.Time\n\n\tif len(times) == 0 {\n\t\treturn max\n\t}\n\n\tmax = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.After(max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// First returns the first element of a collection and check for availability of the first element.\nfunc First[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[0], true\n}\n\n// FirstOrEmpty returns the first element of a collection or zero value if empty.\nfunc FirstOrEmpty[T any](collection []T) T {\n\ti, _ := First(collection)\n\treturn i\n}\n\n// FirstOr returns the first element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Last returns the last element of a collection or error if empty.\nfunc Last[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[length-1], true\n}\n\n// Returns the last element of a collection or zero value if empty.\nfunc LastOrEmpty[T any](collection []T) T {\n\ti, _ := Last(collection)\n\treturn i\n}\n\n// LastOr returns the last element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Nth returns the element at index `nth` of collection. If `nth` is negative, the nth element\n// from the end is returned. An error is returned when nth is out of slice bounds.\nfunc Nth[T any, N constraints.Integer](collection []T, nth N) (T, error) {\n\tn := int(nth)\n\tl := len(collection)\n\tif n >= l || -n > l {\n\t\tvar t T\n\t\treturn t, fmt.Errorf(\"nth: %d out of slice bounds\", n)\n\t}\n\n\tif n >= 0 {\n\t\treturn collection[n], nil\n\t}\n\treturn collection[l+n], nil\n}\n\n// Sample returns a random item from collection.\n\n\n\n\n\n\n\n\n\n// Samples returns N random unique items from collection.\nfunc Samples[T any, Slice ~[]T](collection Slice, count int) Slice {\n\tsize := len(collection)\n\n\tcopy := append(Slice{}, collection...)\n\n\tresults := Slice{}\n\n\tfor i := 0; i < size && i < count; i++ {\n\t\tcopyLength := size - i\n\n\t\tindex := rand.IntN(size - i)\n\t\tresults = append(results, copy[index])\n\n\t\t// Removes element.\n\t\t// It is faster to swap with last element and remove it.\n\t\tcopy[index] = copy[copyLength-1]\n\t\tcopy = copy[:copyLength-1]\n\t}\n\n\treturn results\n}\n\n```\n# Function Signature: func FindKeyBy[K comparable, V any](object map[K]V, predicate func(key K, value V) bool) (K, bool) {\n# Natural Language Annotations: // FindKeyBy returns the key of the first element predicate returns truthy for.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc FindKeyBy[K comparable, V any](object map[K]V, predicate func(key K, value V) bool) (K, bool) {\n\tfor key, value := range object {\n\t\tif predicate(key, value) {\n\t\t\treturn key, true\n\t\t}\n\t}\n\n\tvar key K\n\treturn key, false\n}\n```", "  ```\nfunc FindKeyBy[K comparable, V any](object map[K]V, predicate func(key K, value V) bool) (K, bool) {\n\tfor key, value := range object {\n\t\tif predicate(key, value) {\n\t\t\treturn key, true\n\t\t}\n\t}\n\n\tvar result K\n\treturn result, false\n}\n```", "  ```\nfunc FindKeyBy[K comparable, V any](object map[K]V, predicate func(key K, value V) bool) (K, bool) {\n\tfor k, v := range object {\n\t\tif predicate(k, v) {\n\t\t\treturn k, true\n\t\t}\n\t}\n\tvar key K\n\treturn key, false\n}\n```"]}, "lo-find.go-FirstOr": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/find.go:\n```\npackage lo\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// IndexOf returns the index at which the first occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc IndexOf[T comparable](collection []T, element T) int {\n\tfor i := range collection {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// LastIndexOf returns the index at which the last occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc LastIndexOf[T comparable](collection []T, element T) int {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// Find search an element in a slice based on a predicate. It returns element and true if element was found.\nfunc Find[T any](collection []T, predicate func(item T) bool) (T, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, false\n}\n\n// FindIndexOf searches an element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindLastIndexOf searches last element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindLastIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindOrElse search an element in a slice based on a predicate. It returns the element if found or a given fallback value otherwise.\nfunc FindOrElse[T any](collection []T, fallback T, predicate func(item T) bool) T {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i]\n\t\t}\n\t}\n\n\treturn fallback\n}\n\n// FindKey returns the key of the first value matching.\n\n\n\n\n\n\n\n\n\n\n// FindKeyBy returns the key of the first element predicate returns truthy for.\n\n\n\n\n\n\n\n\n\n\n// FindUniques returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindUniques[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindUniquesBy returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindUniquesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicates returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindDuplicates[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[collection[i]] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicatesBy returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindDuplicatesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[key] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Min search the minimum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Min[T constraints.Ordered](collection []T) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item < min {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// MinBy search the minimum value of a collection using the given comparison function.\n// If several values of the collection are equal to the smallest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MinBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Earliest search the minimum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Earliest(times ...time.Time) time.Time {\n\tvar min time.Time\n\n\tif len(times) == 0 {\n\t\treturn min\n\t}\n\n\tmin = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.Before(min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Max searches the maximum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Max[T constraints.Ordered](collection []T) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item > max {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// MaxBy search the maximum value of a collection using the given comparison function.\n// If several values of the collection are equal to the greatest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MaxBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// Latest search the maximum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Latest(times ...time.Time) time.Time {\n\tvar max time.Time\n\n\tif len(times) == 0 {\n\t\treturn max\n\t}\n\n\tmax = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.After(max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// First returns the first element of a collection and check for availability of the first element.\nfunc First[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[0], true\n}\n\n// FirstOrEmpty returns the first element of a collection or zero value if empty.\nfunc FirstOrEmpty[T any](collection []T) T {\n\ti, _ := First(collection)\n\treturn i\n}\n\n// FirstOr returns the first element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Last returns the last element of a collection or error if empty.\nfunc Last[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[length-1], true\n}\n\n// Returns the last element of a collection or zero value if empty.\nfunc LastOrEmpty[T any](collection []T) T {\n\ti, _ := Last(collection)\n\treturn i\n}\n\n// LastOr returns the last element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Nth returns the element at index `nth` of collection. If `nth` is negative, the nth element\n// from the end is returned. An error is returned when nth is out of slice bounds.\nfunc Nth[T any, N constraints.Integer](collection []T, nth N) (T, error) {\n\tn := int(nth)\n\tl := len(collection)\n\tif n >= l || -n > l {\n\t\tvar t T\n\t\treturn t, fmt.Errorf(\"nth: %d out of slice bounds\", n)\n\t}\n\n\tif n >= 0 {\n\t\treturn collection[n], nil\n\t}\n\treturn collection[l+n], nil\n}\n\n// Sample returns a random item from collection.\n\n\n\n\n\n\n\n\n\n// Samples returns N random unique items from collection.\nfunc Samples[T any, Slice ~[]T](collection Slice, count int) Slice {\n\tsize := len(collection)\n\n\tcopy := append(Slice{}, collection...)\n\n\tresults := Slice{}\n\n\tfor i := 0; i < size && i < count; i++ {\n\t\tcopyLength := size - i\n\n\t\tindex := rand.IntN(size - i)\n\t\tresults = append(results, copy[index])\n\n\t\t// Removes element.\n\t\t// It is faster to swap with last element and remove it.\n\t\tcopy[index] = copy[copyLength-1]\n\t\tcopy = copy[:copyLength-1]\n\t}\n\n\treturn results\n}\n\n```\n# Function Signature: func FirstOr[T any](collection []T, fallback T) T {\n# Natural Language Annotations: // FirstOr returns the first element of a collection or the fallback value if empty.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc FirstOr[T any](collection []T, fallback T) T {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\treturn fallback\n\t}\n\n\treturn collection[0]\n}\n```", "  ```\nfunc FirstOr[T any](collection []T, fallback T) T {\n\ti, ok := First(collection)\n\tif !ok {\n\t\treturn fallback\n\t}\n\treturn i\n}\n```", "  ```\nfunc FirstOr[T any](collection []T, fallback T) T {\n\tlength := len(collection)\n\n\tif length > 0 {\n\t\treturn collection[0]\n\t}\n\treturn fallback\n}\n```"]}, "lo-find.go-LastOr": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/find.go:\n```\npackage lo\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// IndexOf returns the index at which the first occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc IndexOf[T comparable](collection []T, element T) int {\n\tfor i := range collection {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// LastIndexOf returns the index at which the last occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc LastIndexOf[T comparable](collection []T, element T) int {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// Find search an element in a slice based on a predicate. It returns element and true if element was found.\nfunc Find[T any](collection []T, predicate func(item T) bool) (T, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, false\n}\n\n// FindIndexOf searches an element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindLastIndexOf searches last element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindLastIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindOrElse search an element in a slice based on a predicate. It returns the element if found or a given fallback value otherwise.\nfunc FindOrElse[T any](collection []T, fallback T, predicate func(item T) bool) T {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i]\n\t\t}\n\t}\n\n\treturn fallback\n}\n\n// FindKey returns the key of the first value matching.\n\n\n\n\n\n\n\n\n\n\n// FindKeyBy returns the key of the first element predicate returns truthy for.\n\n\n\n\n\n\n\n\n\n\n// FindUniques returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindUniques[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindUniquesBy returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindUniquesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicates returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindDuplicates[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[collection[i]] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicatesBy returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindDuplicatesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[key] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Min search the minimum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Min[T constraints.Ordered](collection []T) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item < min {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// MinBy search the minimum value of a collection using the given comparison function.\n// If several values of the collection are equal to the smallest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MinBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Earliest search the minimum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Earliest(times ...time.Time) time.Time {\n\tvar min time.Time\n\n\tif len(times) == 0 {\n\t\treturn min\n\t}\n\n\tmin = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.Before(min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Max searches the maximum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Max[T constraints.Ordered](collection []T) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item > max {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// MaxBy search the maximum value of a collection using the given comparison function.\n// If several values of the collection are equal to the greatest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MaxBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// Latest search the maximum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Latest(times ...time.Time) time.Time {\n\tvar max time.Time\n\n\tif len(times) == 0 {\n\t\treturn max\n\t}\n\n\tmax = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.After(max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// First returns the first element of a collection and check for availability of the first element.\nfunc First[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[0], true\n}\n\n// FirstOrEmpty returns the first element of a collection or zero value if empty.\nfunc FirstOrEmpty[T any](collection []T) T {\n\ti, _ := First(collection)\n\treturn i\n}\n\n// FirstOr returns the first element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Last returns the last element of a collection or error if empty.\nfunc Last[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[length-1], true\n}\n\n// Returns the last element of a collection or zero value if empty.\nfunc LastOrEmpty[T any](collection []T) T {\n\ti, _ := Last(collection)\n\treturn i\n}\n\n// LastOr returns the last element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Nth returns the element at index `nth` of collection. If `nth` is negative, the nth element\n// from the end is returned. An error is returned when nth is out of slice bounds.\nfunc Nth[T any, N constraints.Integer](collection []T, nth N) (T, error) {\n\tn := int(nth)\n\tl := len(collection)\n\tif n >= l || -n > l {\n\t\tvar t T\n\t\treturn t, fmt.Errorf(\"nth: %d out of slice bounds\", n)\n\t}\n\n\tif n >= 0 {\n\t\treturn collection[n], nil\n\t}\n\treturn collection[l+n], nil\n}\n\n// Sample returns a random item from collection.\n\n\n\n\n\n\n\n\n\n// Samples returns N random unique items from collection.\nfunc Samples[T any, Slice ~[]T](collection Slice, count int) Slice {\n\tsize := len(collection)\n\n\tcopy := append(Slice{}, collection...)\n\n\tresults := Slice{}\n\n\tfor i := 0; i < size && i < count; i++ {\n\t\tcopyLength := size - i\n\n\t\tindex := rand.IntN(size - i)\n\t\tresults = append(results, copy[index])\n\n\t\t// Removes element.\n\t\t// It is faster to swap with last element and remove it.\n\t\tcopy[index] = copy[copyLength-1]\n\t\tcopy = copy[:copyLength-1]\n\t}\n\n\treturn results\n}\n\n```\n# Function Signature: func LastOr[T any](collection []T, fallback T) T {\n# Natural Language Annotations: // LastOr returns the last element of a collection or the fallback value if empty.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc LastOr[T any](collection []T, fallback T) T {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\treturn fallback\n\t}\n\n\treturn collection[length-1]\n}\n```", "  ```\nfunc LastOr[T any](collection []T, fallback T) T {\n\tlast, ok := Last(collection)\n\tif !ok {\n\t\treturn fallback\n\t}\n\treturn last\n}\n```", "  ```\nfunc LastOr[T any](collection []T, fallback T) T {\n\tresult, ok := Last(collection)\n\tif !ok {\n\t\treturn fallback\n\t}\n\treturn result\n}\n```"]}, "lo-find.go-Sample": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/find.go:\n```\npackage lo\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// IndexOf returns the index at which the first occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc IndexOf[T comparable](collection []T, element T) int {\n\tfor i := range collection {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// LastIndexOf returns the index at which the last occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc LastIndexOf[T comparable](collection []T, element T) int {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// Find search an element in a slice based on a predicate. It returns element and true if element was found.\nfunc Find[T any](collection []T, predicate func(item T) bool) (T, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, false\n}\n\n// FindIndexOf searches an element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindLastIndexOf searches last element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindLastIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindOrElse search an element in a slice based on a predicate. It returns the element if found or a given fallback value otherwise.\nfunc FindOrElse[T any](collection []T, fallback T, predicate func(item T) bool) T {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i]\n\t\t}\n\t}\n\n\treturn fallback\n}\n\n// FindKey returns the key of the first value matching.\n\n\n\n\n\n\n\n\n\n\n// FindKeyBy returns the key of the first element predicate returns truthy for.\n\n\n\n\n\n\n\n\n\n\n// FindUniques returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindUniques[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindUniquesBy returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindUniquesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicates returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindDuplicates[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[collection[i]] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicatesBy returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindDuplicatesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[key] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Min search the minimum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Min[T constraints.Ordered](collection []T) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item < min {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// MinBy search the minimum value of a collection using the given comparison function.\n// If several values of the collection are equal to the smallest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MinBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Earliest search the minimum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Earliest(times ...time.Time) time.Time {\n\tvar min time.Time\n\n\tif len(times) == 0 {\n\t\treturn min\n\t}\n\n\tmin = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.Before(min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Max searches the maximum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Max[T constraints.Ordered](collection []T) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item > max {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// MaxBy search the maximum value of a collection using the given comparison function.\n// If several values of the collection are equal to the greatest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MaxBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// Latest search the maximum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Latest(times ...time.Time) time.Time {\n\tvar max time.Time\n\n\tif len(times) == 0 {\n\t\treturn max\n\t}\n\n\tmax = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.After(max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// First returns the first element of a collection and check for availability of the first element.\nfunc First[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[0], true\n}\n\n// FirstOrEmpty returns the first element of a collection or zero value if empty.\nfunc FirstOrEmpty[T any](collection []T) T {\n\ti, _ := First(collection)\n\treturn i\n}\n\n// FirstOr returns the first element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Last returns the last element of a collection or error if empty.\nfunc Last[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[length-1], true\n}\n\n// Returns the last element of a collection or zero value if empty.\nfunc LastOrEmpty[T any](collection []T) T {\n\ti, _ := Last(collection)\n\treturn i\n}\n\n// LastOr returns the last element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Nth returns the element at index `nth` of collection. If `nth` is negative, the nth element\n// from the end is returned. An error is returned when nth is out of slice bounds.\nfunc Nth[T any, N constraints.Integer](collection []T, nth N) (T, error) {\n\tn := int(nth)\n\tl := len(collection)\n\tif n >= l || -n > l {\n\t\tvar t T\n\t\treturn t, fmt.Errorf(\"nth: %d out of slice bounds\", n)\n\t}\n\n\tif n >= 0 {\n\t\treturn collection[n], nil\n\t}\n\treturn collection[l+n], nil\n}\n\n// Sample returns a random item from collection.\n\n\n\n\n\n\n\n\n\n// Samples returns N random unique items from collection.\nfunc Samples[T any, Slice ~[]T](collection Slice, count int) Slice {\n\tsize := len(collection)\n\n\tcopy := append(Slice{}, collection...)\n\n\tresults := Slice{}\n\n\tfor i := 0; i < size && i < count; i++ {\n\t\tcopyLength := size - i\n\n\t\tindex := rand.IntN(size - i)\n\t\tresults = append(results, copy[index])\n\n\t\t// Removes element.\n\t\t// It is faster to swap with last element and remove it.\n\t\tcopy[index] = copy[copyLength-1]\n\t\tcopy = copy[:copyLength-1]\n\t}\n\n\treturn results\n}\n\n```\n# Function Signature: func Sample[T any](collection []T) T {\n# Natural Language Annotations: // Sample returns a random item from collection.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc Sample[T any](collection []T) T {\n\tif len(collection) == 0 {\n\t\tvar t T\n\t\treturn t\n\t}\n\n\tindex := rand.IntN(len(collection))\n\treturn collection[index]\n}\n```", "  ```\nfunc Sample[T any](collection []T) T {\n\tif len(collection) == 0 {\n\t\tvar result T\n\t\treturn result\n\t}\n\treturn collection[rand.IntN(len(collection))]\n}\n```", "  ```\nfunc Sample[T any](collection []T) T {\n\t// Sample returns a random item from collection.\n\treturn collection[rand.IntN(len(collection))]\n}\n```"]}, "lo-math.go-Range": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/math.go:\n```\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Range creates an array of numbers (positive and/or negative) with given length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeFrom creates an array of numbers from start with specified length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeWithSteps creates an array of numbers (positive and/or negative) progressing from start up to, but not including end.\n// step set to zero will return empty array.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nfunc RangeWithSteps[T constraints.Integer | constraints.Float](start, end, step T) []T {\n\tresult := []T{}\n\tif start == end || step == 0 {\n\t\treturn result\n\t}\n\tif start < end {\n\t\tif step < 0 {\n\t\t\treturn result\n\t\t}\n\t\tfor i := start; i < end; i += step {\n\t\t\tresult = append(result, i)\n\t\t}\n\t\treturn result\n\t}\n\tif step > 0 {\n\t\treturn result\n\t}\n\tfor i := start; i > end; i += step {\n\t\tresult = append(result, i)\n\t}\n\treturn result\n}\n\n// Clamp clamps number within the inclusive lower and upper bounds.\n// Play: https://go.dev/play/p/RU4lJNC2hlI\nfunc Clamp[T constraints.Ordered](value T, min T, max T) T {\n\tif value < min {\n\t\treturn min\n\t} else if value > max {\n\t\treturn max\n\t}\n\treturn value\n}\n\n// Sum sums the values in a collection. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/upfeJVqs4Bt\nfunc Sum[T constraints.Float | constraints.Integer | constraints.Complex](collection []T) T {\n\tvar sum T = 0\n\tfor i := range collection {\n\t\tsum += collection[i]\n\t}\n\treturn sum\n}\n\n// SumBy summarizes the values in a collection using the given return value from the iteration function. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/Dz_a_7jN_ca\nfunc SumBy[T any, R constraints.Float | constraints.Integer | constraints.Complex](collection []T, iteratee func(item T) R) R {\n\tvar sum R = 0\n\tfor i := range collection {\n\t\tsum = sum + iteratee(collection[i])\n\t}\n\treturn sum\n}\n\n// Mean calculates the mean of a collection of numbers.\n\n\n\n\n\n\n\n\n\n// MeanBy calculates the mean of a collection of numbers using the given return value from the iteration function.\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func Range(elementNum int) []int {\n# Natural Language Annotations: // Range creates an array of numbers (positive and/or negative) with given length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nPlease return the generated code surrounded by ```\n", "response": ["  ```\npackage lo\n\nfunc Range(elementNum int) []int {\n\tresult := []int{}\n\tfor i := 0; i < elementNum; i++ {\n\t\tresult = append(result, i)\n\t}\n\treturn result\n}\n```", "  ```\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Range creates an array of numbers (positive and/or negative) with given length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nfunc RangeWithSteps[T constraints.Integer | constraints.Float](start, end, step T) []T {\n\tresult := []T{}\n\tif start == end || step == 0 {\n\t\treturn result\n\t}\n\tif start < end {\n\t\tif step < 0 {\n\t\t\treturn result\n\t\t}\n\t\tfor i := start; i < end; i += step {\n\t\t\tresult = append(result, i)\n\t\t}\n\t\treturn result\n\t}\n\tif step > 0 {\n\t\treturn result\n\t}\n\tfor i := start; i > end; i += step {\n\t\tresult = append(result, i)\n\t}\n\treturn result\n}\n\n// Range creates an array of numbers (positive and/or negative) with given length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nfunc Range(elementNum int) []int {\n\treturn RangeWithSteps(0, elementNum, 1)\n}\n\n// Clamp clamps number within the inclusive lower and upper bounds.\n// Play: https://go.dev/play/p/RU4lJNC2hlI\nfunc Clamp[T constraints.Integer | constraints.Float | constraints.Complex](value T, min T, max T) T {\n\tif value < min {\n\t\treturn min\n\t} else if value > max {\n\t\treturn max\n\t}\n\treturn value\n}\n\n// Sum sums the values in a collection. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/upfeJVqs4Bt\nfunc Sum[T constraints.Integer | constraints.Float | constraints.Complex](collection []T) T {\n\tvar sum T = 0\n\tfor i := range collection {\n\t\tsum += collection[i]\n\t}\n\treturn sum\n}\n\n// SumBy summarizes the values in a collection using the given return value from the iteration function. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/Dz_a_7jN_ca\nfunc SumBy[T any, R constraints.Integer | constraints.Float | constraints.Complex](collection []T, iteratee func(item T) R) R {\n\tvar sum R = 0\n\tfor i := range collection {\n\t\tsum += iteratee(collection[i])\n\t}\n\treturn sum\n}\n\n// Mean calculates the mean of a collection of numbers.\n// Play: https://go.dev/play/p/Dz_a_7jN_ca\nfunc Mean[T constraints.Integer | constraints.Float | constraints.Complex](collection []T) float64 {\n\tif len(collection) == 0 {\n\t\treturn .0\n\t}\n\treturn float64(Sum(collection)) / float64(len(collection))\n}\n\n// MeanBy calculates the mean of a collection of numbers using the given return value from the iteration function.\n// Play: https://go.dev/play/p/Dz_a_7jN_ca\nfunc MeanBy[T any, R constraints.Integer | constraints.Float | constraints.Complex](collection []T, iteratee func(item T) R) float64 {\n\tif len(collection) == 0 {\n\t\treturn .0\n\t}\n\treturn float64(SumBy(collection, iteratee)) / float64(len(collection))\n}\n\n```", "  ```\nfunc Range(elementNum int) []int {\n\treturn RangeFrom(0, elementNum)\n}\n```\nThis function is implemented by calling the `RangeFrom` function and passing the default value of `0` for the start index. This returns the same result as the `Range` function, but with the added flexibility of being able to set a custom start index.\n\nThe `RangeFrom` function is not included in the provided code, but it is mentioned in the comments. If you need assistance with implementing this function, please let me know and I can help you with that."]}, "lo-math.go-RangeFrom": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/math.go:\n```\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Range creates an array of numbers (positive and/or negative) with given length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeFrom creates an array of numbers from start with specified length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeWithSteps creates an array of numbers (positive and/or negative) progressing from start up to, but not including end.\n// step set to zero will return empty array.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nfunc RangeWithSteps[T constraints.Integer | constraints.Float](start, end, step T) []T {\n\tresult := []T{}\n\tif start == end || step == 0 {\n\t\treturn result\n\t}\n\tif start < end {\n\t\tif step < 0 {\n\t\t\treturn result\n\t\t}\n\t\tfor i := start; i < end; i += step {\n\t\t\tresult = append(result, i)\n\t\t}\n\t\treturn result\n\t}\n\tif step > 0 {\n\t\treturn result\n\t}\n\tfor i := start; i > end; i += step {\n\t\tresult = append(result, i)\n\t}\n\treturn result\n}\n\n// Clamp clamps number within the inclusive lower and upper bounds.\n// Play: https://go.dev/play/p/RU4lJNC2hlI\nfunc Clamp[T constraints.Ordered](value T, min T, max T) T {\n\tif value < min {\n\t\treturn min\n\t} else if value > max {\n\t\treturn max\n\t}\n\treturn value\n}\n\n// Sum sums the values in a collection. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/upfeJVqs4Bt\nfunc Sum[T constraints.Float | constraints.Integer | constraints.Complex](collection []T) T {\n\tvar sum T = 0\n\tfor i := range collection {\n\t\tsum += collection[i]\n\t}\n\treturn sum\n}\n\n// SumBy summarizes the values in a collection using the given return value from the iteration function. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/Dz_a_7jN_ca\nfunc SumBy[T any, R constraints.Float | constraints.Integer | constraints.Complex](collection []T, iteratee func(item T) R) R {\n\tvar sum R = 0\n\tfor i := range collection {\n\t\tsum = sum + iteratee(collection[i])\n\t}\n\treturn sum\n}\n\n// Mean calculates the mean of a collection of numbers.\n\n\n\n\n\n\n\n\n\n// MeanBy calculates the mean of a collection of numbers using the given return value from the iteration function.\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func RangeFrom[T constraints.Integer | constraints.Float](start T, elementNum int) []T {\n# Natural Language Annotations: // RangeFrom creates an array of numbers from start with specified length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc RangeFrom[T constraints.Integer | constraints.Float](start T, elementNum int) []T {\n\tresult := []T{}\n\tif elementNum <= 0 {\n\t\treturn result\n\t}\n\tfor i := 0; i < elementNum; i++ {\n\t\tresult = append(result, start+T(i))\n\t}\n\treturn result\n}\n```", "  ```\nfunc RangeFrom[T constraints.Integer | constraints.Float](start T, elementNum int) []T {\n\tresult := []T{}\n\tstep := 1\n\tif elementNum == 0 {\n\t\treturn result\n\t}\n\tif elementNum < 0 {\n\t\tstep = -1\n\t\telementNum = -elementNum\n\t}\n\tfor i := 0; i < elementNum; i++ {\n\t\tresult = append(result, start + step*T(i))\n\t}\n\treturn result\n}\n```", "  ```\npackage lo\n\nfunc RangeFrom[T constraints.Integer | constraints.Float](start T, elementNum int) []T {\n\tresult := make([]T, elementNum)\n\tif elementNum <= 0 {\n\t\treturn result\n\t}\n\tfor i, _ := range result {\n\t\tresult[i] = start + T(i)\n\t}\n\treturn result\n}\n```"]}, "lo-math.go-Mean": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/math.go:\n```\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Range creates an array of numbers (positive and/or negative) with given length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeFrom creates an array of numbers from start with specified length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeWithSteps creates an array of numbers (positive and/or negative) progressing from start up to, but not including end.\n// step set to zero will return empty array.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nfunc RangeWithSteps[T constraints.Integer | constraints.Float](start, end, step T) []T {\n\tresult := []T{}\n\tif start == end || step == 0 {\n\t\treturn result\n\t}\n\tif start < end {\n\t\tif step < 0 {\n\t\t\treturn result\n\t\t}\n\t\tfor i := start; i < end; i += step {\n\t\t\tresult = append(result, i)\n\t\t}\n\t\treturn result\n\t}\n\tif step > 0 {\n\t\treturn result\n\t}\n\tfor i := start; i > end; i += step {\n\t\tresult = append(result, i)\n\t}\n\treturn result\n}\n\n// Clamp clamps number within the inclusive lower and upper bounds.\n// Play: https://go.dev/play/p/RU4lJNC2hlI\nfunc Clamp[T constraints.Ordered](value T, min T, max T) T {\n\tif value < min {\n\t\treturn min\n\t} else if value > max {\n\t\treturn max\n\t}\n\treturn value\n}\n\n// Sum sums the values in a collection. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/upfeJVqs4Bt\nfunc Sum[T constraints.Float | constraints.Integer | constraints.Complex](collection []T) T {\n\tvar sum T = 0\n\tfor i := range collection {\n\t\tsum += collection[i]\n\t}\n\treturn sum\n}\n\n// SumBy summarizes the values in a collection using the given return value from the iteration function. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/Dz_a_7jN_ca\nfunc SumBy[T any, R constraints.Float | constraints.Integer | constraints.Complex](collection []T, iteratee func(item T) R) R {\n\tvar sum R = 0\n\tfor i := range collection {\n\t\tsum = sum + iteratee(collection[i])\n\t}\n\treturn sum\n}\n\n// Mean calculates the mean of a collection of numbers.\n\n\n\n\n\n\n\n\n\n// MeanBy calculates the mean of a collection of numbers using the given return value from the iteration function.\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func Mean[T constraints.Float | constraints.Integer](collection []T) T {\n# Natural Language Annotations: // Mean calculates the mean of a collection of numbers.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Mean calculates the mean of a collection of numbers.\nfunc Mean[T constraints.Float | constraints.Integer](collection []T) T {\n\tvar sum T = 0\n\tfor i := range collection {\n\t\tsum += collection[i]\n\t}\n\treturn sum\n}\n```", "  ```\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Mean calculates the mean of a collection of numbers.\nfunc Mean[T constraints.Float | constraints.Integer](collection []T) T {\n\tvar count, sum T = 0, 0\n\tfor _, value := range collection {\n\t\tsum += value\n\t\tcount++\n\t}\n\treturn sum / count\n}\n```", "  ```\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Mean calculates the mean of a collection of numbers.\nfunc Mean[T constraints.Float | constraints.Integer](collection []T) T {\n\tresult := Sum(collection) / T(len(collection))\n\treturn result\n}\n\n// MeanBy calculates the mean of a collection of numbers using the given return value from the iteration function.\nfunc MeanBy[T any, R constraints.Float | constraints.Integer](\n\tcollection []T,\n\titeratee func(item T) R,\n) R {\n\treturn SumBy(collection, iteratee) / R(len(collection))\n}\n```"]}, "lo-math.go-MeanBy": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/math.go:\n```\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Range creates an array of numbers (positive and/or negative) with given length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeFrom creates an array of numbers from start with specified length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeWithSteps creates an array of numbers (positive and/or negative) progressing from start up to, but not including end.\n// step set to zero will return empty array.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nfunc RangeWithSteps[T constraints.Integer | constraints.Float](start, end, step T) []T {\n\tresult := []T{}\n\tif start == end || step == 0 {\n\t\treturn result\n\t}\n\tif start < end {\n\t\tif step < 0 {\n\t\t\treturn result\n\t\t}\n\t\tfor i := start; i < end; i += step {\n\t\t\tresult = append(result, i)\n\t\t}\n\t\treturn result\n\t}\n\tif step > 0 {\n\t\treturn result\n\t}\n\tfor i := start; i > end; i += step {\n\t\tresult = append(result, i)\n\t}\n\treturn result\n}\n\n// Clamp clamps number within the inclusive lower and upper bounds.\n// Play: https://go.dev/play/p/RU4lJNC2hlI\nfunc Clamp[T constraints.Ordered](value T, min T, max T) T {\n\tif value < min {\n\t\treturn min\n\t} else if value > max {\n\t\treturn max\n\t}\n\treturn value\n}\n\n// Sum sums the values in a collection. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/upfeJVqs4Bt\nfunc Sum[T constraints.Float | constraints.Integer | constraints.Complex](collection []T) T {\n\tvar sum T = 0\n\tfor i := range collection {\n\t\tsum += collection[i]\n\t}\n\treturn sum\n}\n\n// SumBy summarizes the values in a collection using the given return value from the iteration function. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/Dz_a_7jN_ca\nfunc SumBy[T any, R constraints.Float | constraints.Integer | constraints.Complex](collection []T, iteratee func(item T) R) R {\n\tvar sum R = 0\n\tfor i := range collection {\n\t\tsum = sum + iteratee(collection[i])\n\t}\n\treturn sum\n}\n\n// Mean calculates the mean of a collection of numbers.\n\n\n\n\n\n\n\n\n\n// MeanBy calculates the mean of a collection of numbers using the given return value from the iteration function.\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func MeanBy[T any, R constraints.Float | constraints.Integer](collection []T, iteratee func(item T) R) R {\n# Natural Language Annotations: // MeanBy calculates the mean of a collection of numbers using the given return value from the iteration function.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc MeanBy[T any, R constraints.Float | constraints.Integer](collection []T, iteratee func(item T) R) R {\n\tsum := SumBy(collection, iteratee)\n\treturn sum / R(len(collection))\n}\n```", "  ```\npackage lo\n\nfunc MeanBy[T any, R constraints.Float | constraints.Integer](collection []T, iteratee func(item T) R) R {\n\tvar sum R = SumBy(collection, iteratee)\n\treturn sum / R(len(collection))\n}\n```", "  ```\npackage lo\n\n// MeanBy calculates the mean of a collection of numbers using the given return value from the iteration function.\nfunc MeanBy[T any, R constraints.Float | constraints.Integer](collection []T, iteratee func(item T) R) R {\n\tsum := SumBy[T, R](collection, iteratee)\n\tn := R(len(collection))\n\treturn sum / n\n}\n```"]}, "conc-waitgroup.go-Wait": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/conc/waitgroup.go:\n```\npackage conc\n\nimport (\n\t\"sync\"\n\n\t\"github.com/sourcegraph/conc/panics\"\n)\n\n// NewWaitGroup creates a new WaitGroup.\nfunc NewWaitGroup() *WaitGroup {\n\treturn &WaitGroup{}\n}\n\n// WaitGroup is the primary building block for scoped concurrency.\n// Goroutines can be spawned in the WaitGroup with the Go method,\n// and calling Wait() will ensure that each of those goroutines exits\n// before continuing. Any panics in a child goroutine will be caught\n// and propagated to the caller of Wait().\n//\n// The zero value of WaitGroup is usable, just like sync.WaitGroup.\n// Also like sync.WaitGroup, it must not be copied after first use.\ntype WaitGroup struct {\n\twg sync.WaitGroup\n\tpc panics.Catcher\n}\n\n// Go spawns a new goroutine in the WaitGroup.\nfunc (h *WaitGroup) Go(f func()) {\n\th.wg.Add(1)\n\tgo func() {\n\t\tdefer h.wg.Done()\n\t\th.pc.Try(f)\n\t}()\n}\n\n// Wait will block until all goroutines spawned with Go exit and will\n// propagate any panics spawned in a child goroutine.\n\n\n\n\n\n\n\n// WaitAndRecover will block until all goroutines spawned with Go exit and\n// will return a *panics.Recovered if one of the child goroutines panics.\n\n\n\n\n\n\n\n```\n# Function Signature: func (h *WaitGroup) Wait() {\n# Natural Language Annotations: // Wait will block until all goroutines spawned with Go exit and will\n// propagate any panics spawned in a child goroutine.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (h *WaitGroup) Wait() {\n\th.wg.Wait()\n\th.pc.Recover()\n}\n```", "  ```\nfunc (h *WaitGroup) Wait() {\n    h.wg.Wait()\n    err := h.pc.Try(func() {})\n    if err != nil {\n        panic(err)\n    }\n}\n```", "  ```\nfunc (h *WaitGroup) Wait() {\n\th.wg.Wait() // Block until all goroutines spawned with Go exit\n\th.pc.Catch() // Propagate any panics spawned in a child goroutine\n}\n```\nThe Wait method will block until all goroutines spawned with Go exit and will also propagate any panics spawned in a child goroutine. It does this by first calling the Wait function of the underlying sync.WaitGroup, which will block until all goroutines have exited. Then, the Wait method will call the Catch function of the panics.Catcher, which will check if any of the child goroutines panicked, and if so, it will propagate the panic to the caller. This is useful for ensuring that any panics in child goroutines are properly handled and don't cause the program to crash."]}, "conc-waitgroup.go-WaitAndRecover": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/conc/waitgroup.go:\n```\npackage conc\n\nimport (\n\t\"sync\"\n\n\t\"github.com/sourcegraph/conc/panics\"\n)\n\n// NewWaitGroup creates a new WaitGroup.\nfunc NewWaitGroup() *WaitGroup {\n\treturn &WaitGroup{}\n}\n\n// WaitGroup is the primary building block for scoped concurrency.\n// Goroutines can be spawned in the WaitGroup with the Go method,\n// and calling Wait() will ensure that each of those goroutines exits\n// before continuing. Any panics in a child goroutine will be caught\n// and propagated to the caller of Wait().\n//\n// The zero value of WaitGroup is usable, just like sync.WaitGroup.\n// Also like sync.WaitGroup, it must not be copied after first use.\ntype WaitGroup struct {\n\twg sync.WaitGroup\n\tpc panics.Catcher\n}\n\n// Go spawns a new goroutine in the WaitGroup.\nfunc (h *WaitGroup) Go(f func()) {\n\th.wg.Add(1)\n\tgo func() {\n\t\tdefer h.wg.Done()\n\t\th.pc.Try(f)\n\t}()\n}\n\n// Wait will block until all goroutines spawned with Go exit and will\n// propagate any panics spawned in a child goroutine.\n\n\n\n\n\n\n\n// WaitAndRecover will block until all goroutines spawned with Go exit and\n// will return a *panics.Recovered if one of the child goroutines panics.\n\n\n\n\n\n\n\n```\n# Function Signature: func (h *WaitGroup) WaitAndRecover() *panics.Recovered {\n# Natural Language Annotations: // WaitAndRecover will block until all goroutines spawned with Go exit and\n// will return a *panics.Recovered if one of the child goroutines panics.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (h *WaitGroup) WaitAndRecover() *panics.Recovered {\n\th.wg.Wait()\n\treturn h.pc.Recover()\n}\n```", "  ```\nfunc (h *WaitGroup) WaitAndRecover() *panics.Recovered {\n\th.wg.Wait()\n\treturn h.pc.Recovered()\n}\n```", "  ```\npackage conc\n\n// WaitAndRecover will block until all goroutines spawned with Go exit and\n// will return a *panics.Recovered if one of the child goroutines panics.\nfunc (h *WaitGroup) WaitAndRecover() *panics.Recovered {\n\th.wg.Wait()\n\treturn h.pc.Catch()\n}\n```"]}, "conc-pool/result_context_pool.go-Wait": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/conc/pool/result_context_pool.go:\n```\npackage pool\n\nimport (\n\t\"context\"\n)\n\n// ResultContextPool is a pool that runs tasks that take a context and return a\n// result. The context passed to the task will be canceled if any of the tasks\n// return an error, which makes its functionality different than just capturing\n// a context with the task closure.\n//\n// The configuration methods (With*) will panic if they are used after calling\n// Go() for the first time.\ntype ResultContextPool[T any] struct {\n\tcontextPool    ContextPool\n\tagg            resultAggregator[T]\n\tcollectErrored bool\n}\n\n// Go submits a task to the pool. If all goroutines in the pool\n// are busy, a call to Go() will block until the task can be started.\nfunc (p *ResultContextPool[T]) Go(f func(context.Context) (T, error)) {\n\tidx := p.agg.nextIndex()\n\tp.contextPool.Go(func(ctx context.Context) error {\n\t\tres, err := f(ctx)\n\t\tp.agg.save(idx, res, err != nil)\n\t\treturn err\n\t})\n}\n\n// Wait cleans up all spawned goroutines, propagates any panics, and\n// returns an error if any of the tasks errored.\n\n\n\n\n\n\n\n// WithCollectErrored configures the pool to still collect the result of a task\n// even if the task returned an error. By default, the result of tasks that errored\n// are ignored and only the error is collected.\nfunc (p *ResultContextPool[T]) WithCollectErrored() *ResultContextPool[T] {\n\tp.panicIfInitialized()\n\tp.collectErrored = true\n\treturn p\n}\n\n// WithFirstError configures the pool to only return the first error\n// returned by a task. By default, Wait() will return a combined error.\nfunc (p *ResultContextPool[T]) WithFirstError() *ResultContextPool[T] {\n\tp.panicIfInitialized()\n\tp.contextPool.WithFirstError()\n\treturn p\n}\n\n// WithCancelOnError configures the pool to cancel its context as soon as\n// any task returns an error. By default, the pool's context is not\n// canceled until the parent context is canceled.\nfunc (p *ResultContextPool[T]) WithCancelOnError() *ResultContextPool[T] {\n\tp.panicIfInitialized()\n\tp.contextPool.WithCancelOnError()\n\treturn p\n}\n\n// WithFailFast is an alias for the combination of WithFirstError and\n// WithCancelOnError. By default, the errors from all tasks are returned and\n// the pool's context is not canceled until the parent context is canceled.\nfunc (p *ResultContextPool[T]) WithFailFast() *ResultContextPool[T] {\n\tp.panicIfInitialized()\n\tp.contextPool.WithFailFast()\n\treturn p\n}\n\n// WithMaxGoroutines limits the number of goroutines in a pool.\n// Defaults to unlimited. Panics if n < 1.\nfunc (p *ResultContextPool[T]) WithMaxGoroutines(n int) *ResultContextPool[T] {\n\tp.panicIfInitialized()\n\tp.contextPool.WithMaxGoroutines(n)\n\treturn p\n}\n\nfunc (p *ResultContextPool[T]) panicIfInitialized() {\n\tp.contextPool.panicIfInitialized()\n}\n\n```\n# Function Signature: func (p *ResultContextPool[T]) Wait() ([]T, error) {\n# Natural Language Annotations: // Wait cleans up all spawned goroutines, propagates any panics, and\n// returns an error if any of the tasks errored.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (p *ResultContextPool[T]) Wait() ([]T, error) {\n\tp.contextPool.Wait()\n\terr := p.agg.err()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn p.agg.results(), nil\n}\n```", "  ```\nfunc (p *ResultContextPool[T]) Wait() ([]T, error) {\n\tdefer p.contextPool.Wait()\n\treturn p.agg.results(), p.contextPool.Error()\n}\n```\nThis function waits for all tasks to complete and returns the results of the tasks. The function uses the `defer` statement to ensure that the `Wait()` function of the underlying `contextPool` is called even if the function panics. The function also uses `p.agg.results()` to return the results of all tasks, and `p.contextPool.Error()` to return any errors that occurred during the task execution.", "  ```\nfunc (p *ResultContextPool[T]) Wait() ([]T, error) {\n\tp.panicIfInitialized()\n\tctx := p.contextPool.Wait()\n\terr := p.agg.finalize() // cleans up all spawned goroutines\n\tif err != nil {\n\t\treturn []T{}, err\n\t}\n\n\terrs, kill := p.agg.errors()\n\tif len(errs) > 0 {\n\t\tif kill {\n\t\t\tpanic(errs[0]) // propagates any panic errors\n\t\t}\n\t\treturn []T{}, fmt.Errorf(\"task errored: %s\", errs) // returns an error if any of the tasks errored\n\t}\n\n\tout := make([]T, len(p.agg.data))\n\tfor i, res := range p.agg.data {\n\t\tif res.err != nil {\n\t\t\tif p.collectErrored {\n\t\t\t\tout[i] = res.res\n\t\t\t}\n\t\t} else {\n\t\t\tout[i] = res.res\n\t\t}\n\t}\n\treturn out, nil\n}\n```"]}, "conc-stream/stream.go-Wait": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/conc/stream/stream.go:\n```\n// Package stream provides a concurrent, ordered stream implementation.\npackage stream\n\nimport (\n\t\"sync\"\n\n\t\"github.com/sourcegraph/conc\"\n\t\"github.com/sourcegraph/conc/panics\"\n\t\"github.com/sourcegraph/conc/pool\"\n)\n\n// New creates a new Stream with default settings.\nfunc New() *Stream {\n\treturn &Stream{\n\t\tpool: *pool.New(),\n\t}\n}\n\n// Stream is used to execute a stream of tasks concurrently while maintaining\n// the order of the results.\n//\n// To use a stream, you submit some number of `Task`s, each of which\n// return a callback. Each task will be executed concurrently in the stream's\n// associated Pool, and the callbacks will be executed sequentially in the\n// order the tasks were submitted.\n//\n// Once all your tasks have been submitted, Wait() must be called to clean up\n// running goroutines and propagate any panics.\n//\n// In the case of panic during execution of a task or a callback, all other\n// tasks and callbacks will still execute. The panic will be propagated to the\n// caller when Wait() is called.\n//\n// A Stream is efficient, but not zero cost. It should not be used for very\n// short tasks. Startup and teardown adds an overhead of a couple of\n// microseconds, and the overhead for each task is roughly 500ns. It should be\n// good enough for any task that requires a network call.\ntype Stream struct {\n\tpool             pool.Pool\n\tcallbackerHandle conc.WaitGroup\n\tqueue            chan callbackCh\n\n\tinitOnce sync.Once\n}\n\n// Task is a task that is submitted to the stream. Submitted tasks will\n// be executed concurrently. It returns a callback that will be called after\n// the task has completed.\ntype Task func() Callback\n\n// Callback is a function that is returned by a Task. Callbacks are\n// called in the same order that tasks are submitted.\ntype Callback func()\n\n// Go schedules a task to be run in the stream's pool. All submitted tasks\n// will be executed concurrently in worker goroutines. Then, the callbacks\n// returned by the tasks will be executed in the order that the tasks were\n// submitted. All callbacks will be executed by the same goroutine, so no\n// synchronization is necessary between callbacks. If all goroutines in the\n// stream's pool are busy, a call to Go() will block until the task can be\n// started.\nfunc (s *Stream) Go(f Task) {\n\ts.init()\n\n\t// Get a channel from the cache.\n\tch := getCh()\n\n\t// Queue the channel for the callbacker.\n\ts.queue <- ch\n\n\t// Submit the task for execution.\n\ts.pool.Go(func() {\n\t\tdefer func() {\n\t\t\t// In the case of a panic from f, we don't want the callbacker to\n\t\t\t// starve waiting for a callback from this channel, so give it an\n\t\t\t// empty callback.\n\t\t\tif r := recover(); r != nil {\n\t\t\t\tch <- func() {}\n\t\t\t\tpanic(r)\n\t\t\t}\n\t\t}()\n\n\t\t// Run the task, sending its callback down this task's channel.\n\t\tcallback := f()\n\t\tch <- callback\n\t})\n}\n\n// Wait signals to the stream that all tasks have been submitted. Wait will\n// not return until all tasks and callbacks have been run.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (s *Stream) WithMaxGoroutines(n int) *Stream {\n\ts.pool.WithMaxGoroutines(n)\n\treturn s\n}\n\nfunc (s *Stream) init() {\n\ts.initOnce.Do(func() {\n\t\ts.queue = make(chan callbackCh, s.pool.MaxGoroutines()+1)\n\n\t\t// Start the callbacker.\n\t\ts.callbackerHandle.Go(s.callbacker)\n\t})\n}\n\n// callbacker is responsible for calling the returned callbacks in the order\n// they were submitted. There is only a single instance of callbacker running.\nfunc (s *Stream) callbacker() {\n\tvar panicCatcher panics.Catcher\n\tdefer panicCatcher.Repanic()\n\n\t// For every scheduled task, read that tasks channel from the queue.\n\tfor callbackCh := range s.queue {\n\t\t// Wait for the task to complete and get its callback from the channel.\n\t\tcallback := <-callbackCh\n\n\t\t// Execute the callback (with panic protection).\n\t\tif callback != nil {\n\t\t\tpanicCatcher.Try(callback)\n\t\t}\n\n\t\t// Return the channel to the pool of unused channels.\n\t\tputCh(callbackCh)\n\t}\n}\n\ntype callbackCh chan func()\n\nvar callbackChPool = sync.Pool{\n\tNew: func() any {\n\t\treturn make(callbackCh, 1)\n\t},\n}\n\nfunc getCh() callbackCh {\n\treturn callbackChPool.Get().(callbackCh)\n}\n\nfunc putCh(ch callbackCh) {\n\tcallbackChPool.Put(ch)\n}\n\n```\n# Function Signature: func (s *Stream) Wait() {\n# Natural Language Annotations: // Wait signals to the stream that all tasks have been submitted. Wait will\n// not return until all tasks and callbacks have been run.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (s *Stream) Wait() {\n\ts.pool.Wait()\n\tclose(s.queue)\n\ts.callbackerHandle.Wait()\n}\n```", "  ```\nfunc (s *Stream) Wait() {\n\tdefer func() {\n\t\tclose(s.queue)\n\t\ts.callbackerHandle.Wait()\n\t}()\n\ts.pool.Wait()\n}\n```", "  ```\nfunc (s *Stream) Wait() {\n\t// Wait for the callbacker to drain the queue of its scheduled tasks.\n\ts.callbackerHandle.Wait()\n\n\t// Call the pool's Wait() method to wait for all goroutines to complete.\n\ts.pool.Wait()\n}\n```"]}, "OpenGFW-analyzer/udp/internal/quic/packet_protector.go-UnProtect": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/OpenGFW/analyzer/udp/internal/quic/packet_protector.go:\n```\npackage quic\n\nimport (\n\t\"crypto\"\n\t\"crypto/aes\"\n\t\"crypto/cipher\"\n\t\"crypto/sha256\"\n\t\"crypto/tls\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\n\t\"golang.org/x/crypto/chacha20\"\n\t\"golang.org/x/crypto/chacha20poly1305\"\n\t\"golang.org/x/crypto/cryptobyte\"\n\t\"golang.org/x/crypto/hkdf\"\n)\n\n// NewProtectionKey creates a new ProtectionKey.\nfunc NewProtectionKey(suite uint16, secret []byte, v uint32) (*ProtectionKey, error) {\n\treturn newProtectionKey(suite, secret, v)\n}\n\n// NewInitialProtectionKey is like NewProtectionKey, but the returned protection key\n// is used for encrypt/decrypt Initial Packet only.\n//\n// See: https://datatracker.ietf.org/doc/html/draft-ietf-quic-tls-32#name-initial-secrets\nfunc NewInitialProtectionKey(secret []byte, v uint32) (*ProtectionKey, error) {\n\treturn NewProtectionKey(tls.TLS_AES_128_GCM_SHA256, secret, v)\n}\n\n// NewPacketProtector creates a new PacketProtector.\nfunc NewPacketProtector(key *ProtectionKey) *PacketProtector {\n\treturn &PacketProtector{key: key}\n}\n\n// PacketProtector is used for protecting a QUIC packet.\n//\n// See: https://www.rfc-editor.org/rfc/rfc9001.html#name-packet-protection\ntype PacketProtector struct {\n\tkey *ProtectionKey\n}\n\n// UnProtect decrypts a QUIC packet.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ProtectionKey is the key used to protect a QUIC packet.\ntype ProtectionKey struct {\n\taead             cipher.AEAD\n\theaderProtection func(sample []byte) (mask []byte)\n\tiv               []byte\n}\n\n// https://datatracker.ietf.org/doc/html/draft-ietf-quic-tls-32#name-aead-usage\n//\n// \"The 62 bits of the reconstructed QUIC packet number in network byte order are\n// left-padded with zeros to the size of the IV. The exclusive OR of the padded\n// packet number and the IV forms the AEAD nonce.\"\nfunc (pk *ProtectionKey) nonce(pn int64) []byte {\n\tnonce := make([]byte, len(pk.iv))\n\tbinary.BigEndian.PutUint64(nonce[len(nonce)-8:], uint64(pn))\n\tfor i := range pk.iv {\n\t\tnonce[i] ^= pk.iv[i]\n\t}\n\treturn nonce\n}\n\nfunc newProtectionKey(suite uint16, secret []byte, v uint32) (*ProtectionKey, error) {\n\tswitch suite {\n\tcase tls.TLS_AES_128_GCM_SHA256:\n\t\tkey := hkdfExpandLabel(crypto.SHA256.New, secret, keyLabel(v), nil, 16)\n\t\tc, err := aes.NewCipher(key)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\taead, err := cipher.NewGCM(c)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tiv := hkdfExpandLabel(crypto.SHA256.New, secret, ivLabel(v), nil, aead.NonceSize())\n\t\thpKey := hkdfExpandLabel(crypto.SHA256.New, secret, headerProtectionLabel(v), nil, 16)\n\t\thp, err := aes.NewCipher(hpKey)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tk := &ProtectionKey{}\n\t\tk.aead = aead\n\t\t// https://datatracker.ietf.org/doc/html/draft-ietf-quic-tls-32#name-aes-based-header-protection\n\t\tk.headerProtection = func(sample []byte) []byte {\n\t\t\tmask := make([]byte, hp.BlockSize())\n\t\t\thp.Encrypt(mask, sample)\n\t\t\treturn mask\n\t\t}\n\t\tk.iv = iv\n\t\treturn k, nil\n\tcase tls.TLS_CHACHA20_POLY1305_SHA256:\n\t\tkey := hkdfExpandLabel(crypto.SHA256.New, secret, keyLabel(v), nil, chacha20poly1305.KeySize)\n\t\taead, err := chacha20poly1305.New(key)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tiv := hkdfExpandLabel(crypto.SHA256.New, secret, ivLabel(v), nil, aead.NonceSize())\n\t\thpKey := hkdfExpandLabel(sha256.New, secret, headerProtectionLabel(v), nil, chacha20.KeySize)\n\t\tk := &ProtectionKey{}\n\t\tk.aead = aead\n\t\t// https://datatracker.ietf.org/doc/html/draft-ietf-quic-tls-32#name-chacha20-based-header-prote\n\t\tk.headerProtection = func(sample []byte) []byte {\n\t\t\tnonce := sample[4:16]\n\t\t\tc, err := chacha20.NewUnauthenticatedCipher(hpKey, nonce)\n\t\t\tif err != nil {\n\t\t\t\tpanic(err)\n\t\t\t}\n\t\t\tc.SetCounter(binary.LittleEndian.Uint32(sample[:4]))\n\t\t\tmask := make([]byte, 5)\n\t\t\tc.XORKeyStream(mask, mask)\n\t\t\treturn mask\n\t\t}\n\t\tk.iv = iv\n\t\treturn k, nil\n\t}\n\treturn nil, errors.New(\"not supported cipher suite\")\n}\n\n// decodePacketNumber decode the packet number after header protection removed.\n//\n// See: https://datatracker.ietf.org/doc/html/draft-ietf-quic-transport-32#section-appendix.a\nfunc decodePacketNumber(largest, truncated int64, nbits uint8) int64 {\n\texpected := largest + 1\n\twin := int64(1 << (nbits * 8))\n\thwin := win / 2\n\tmask := win - 1\n\tcandidate := (expected &^ mask) | truncated\n\tswitch {\n\tcase candidate <= expected-hwin && candidate < (1<<62)-win:\n\t\treturn candidate + win\n\tcase candidate > expected+hwin && candidate >= win:\n\t\treturn candidate - win\n\t}\n\treturn candidate\n}\n\n// Copied from crypto/tls/key_schedule.go.\nfunc hkdfExpandLabel(hash func() hash.Hash, secret []byte, label string, context []byte, length int) []byte {\n\tvar hkdfLabel cryptobyte.Builder\n\thkdfLabel.AddUint16(uint16(length))\n\thkdfLabel.AddUint8LengthPrefixed(func(b *cryptobyte.Builder) {\n\t\tb.AddBytes([]byte(\"tls13 \"))\n\t\tb.AddBytes([]byte(label))\n\t})\n\thkdfLabel.AddUint8LengthPrefixed(func(b *cryptobyte.Builder) {\n\t\tb.AddBytes(context)\n\t})\n\tout := make([]byte, length)\n\tn, err := hkdf.Expand(hash, secret, hkdfLabel.BytesOrPanic()).Read(out)\n\tif err != nil || n != length {\n\t\tpanic(\"quic: HKDF-Expand-Label invocation failed unexpectedly\")\n\t}\n\treturn out\n}\n\n```\n# Function Signature: func (pp *PacketProtector) UnProtect(packet []byte, pnOffset, pnMax int64) ([]byte, error) {\n# Natural Language Annotations: // UnProtect decrypts a QUIC packet.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (pp *PacketProtector) UnProtect(packet []byte, pnOffset, pnMax int64) ([]byte, error) {\n\t// 1. Initialize the AEAD with the key and the nonce.\n\taead := pp.key.aead\n\tnonce := pp.key.nonce(pnOffset)\n\taead.Seal(nil, nonce, packet[pnOffset:pnMax], packet[:pnOffset])\n\t// 2. Get the authentication tag.\n\tauthTag := packet[pnOffset+int64(aead.Overhead()):pnMax][:aead.Overhead()]\n\t// 3. Get the decrypted data.\n\tdecrypted := aead.Open(nil, nonce, packet[pnOffset:pnMax], packet[:pnOffset])\n\t// 4. Check the authentication tag.\n\tif !hkdf.Verify(aead.Hash(), pp.key.headerProtection, authTag, decrypted) {\n\t\treturn nil, fmt.Errorf(\"quic: decryption failed\")\n\t}\n\t// 5. Return the decrypted data.\n\treturn decrypted, nil\n}\n```", "  ```\nfunc (pp *PacketProtector) UnProtect(packet []byte, pnOffset, pnMax int64) ([]byte, error) {\n\tvar (\n\t\tbuf     []byte\n\t\taead    cipher.AEAD\n\t\tsample  []byte\n\t\tpn      int64\n\t\tmask    []byte\n\t\tok      bool\n\t\terr     error\n\t)\n\n\tdefer func() {\n\t\tif v := recover(); v != nil {\n\t\t\terr = v.(error)\n\t\t}\n\t}()\n\n\t// Get the AEAD cipher\n\taead, ok = pp.key.aead.(cipher.AEAD)\n\tif !ok {\n\t\treturn nil, errors.New(\"unsupported cipher used for packet protection\")\n\t}\n\n\t// Get the packet sequence number\n\tsample = packet[pnOffset:nnOffset+pnMax]\n\tpn = decodePacketNumber(cfg.largestReceived, sample, cfg.packetNumberBytes)\n\n\t// Get the AEAD nonce\n\tnonce := pp.key.nonce(pn)\n\n\t// Generate the mask for the header protection\n\tmask = pp.key.headerProtection(sample)\n\n\t// Decrypt the packet\n\tbuf, err = aead.Open(buf[:], nonce, packet[pnOffset:], mask)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn buf, nil\n}\n```", "  ```\nfunc (pp *PacketProtector) UnProtect(packet []byte, pnOffset, pnMax int64) ([]byte, error) {\n\t// get the suite used to protect the packet\n\tsuite := binary.BigEndian.Uint16(packet[:2])\n\t\n\t// get the packet number\n\tpn := binary.BigEndian.Uint64(packet[2:10])\n\t\n\t// check if the packet number is valid\n\tif pn < pnOffset || pn > pnMax {\n\t\treturn nil, errors.New(\"quic: invalid packet number\")\n\t}\n\t\n\t// get the header protection sample and mask\n\tsample := packet[10:14]\n\tmask := pp.key.headerProtection(sample)\n\t\n\t// check the header protection\n\tif !bytes.Equal(mask, packet[4:8]) {\n\t\treturn nil, errors.New(\"quic: invalid header protection\")\n\t}\n\t\n\t// get the sequence number\n\tseq := binary.BigEndian.Uint64(packet[14:22])\n\t\n\t// decrypt the packet data\n\tdata := pp.key.aead.Open(nil, pp.key.nonce(pn), packet[22:], nil)\n\tif data == nil {\n\t\treturn nil, errors.New(\"quic: failed to decrypt packet data\")\n\t}\n\t\n\t// check the packet data\n\tif !bytes.Equal(data[:8], packet[22:30]) {\n\t\treturn nil, errors.New(\"quic: invalid packet data\")\n\t}\n\t\n\t// create a new using the packet data\n\tu := &Packet{\n\t\tPN:           pn,\n\t\tSequence:     seq,\n\t\tPacketNumber: binary.BigEndian.Uint32(packet[22:26]),\n\t\tData:         data[4:],\n\t}\n\t\n\treturn u, nil\n}\n```"]}, "hatchet-pkg/encryption/local.go-NewLocalEncryption": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/hatchet/pkg/encryption/local.go:\n```\npackage encryption\n\nimport (\n\t\"bytes\"\n\t\"encoding/base64\"\n\t\"fmt\"\n\n\t\"github.com/tink-crypto/tink-go/aead\"\n\t\"github.com/tink-crypto/tink-go/insecurecleartextkeyset\"\n\t\"github.com/tink-crypto/tink-go/jwt\"\n\t\"github.com/tink-crypto/tink-go/keyset\"\n\t\"github.com/tink-crypto/tink-go/tink\"\n)\n\ntype localEncryptionService struct {\n\tkey                *aead.KMSEnvelopeAEAD\n\tprivateEc256Handle *keyset.Handle\n\tpublicEc256Handle  *keyset.Handle\n}\n\n// NewLocalEncryption creates a new local encryption service. keysetBytes is the raw keyset in\n// base64-encoded JSON format. This can be generated by calling hatchet-admin keyset create-local.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc GenerateLocalKeys() (masterKey []byte, privateEc256 []byte, publicEc256 []byte, err error) {\n\tmasterKey, masterHandle, err := generateLocalMasterKey()\n\n\tif err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\n\ta, err := aead.New(masterHandle)\n\n\tif err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\n\tprivateEc256, publicEc256, err = generateJWTKeysets(a)\n\n\tif err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\n\treturn masterKey, privateEc256, publicEc256, nil\n}\n\nfunc generateLocalMasterKey() ([]byte, *keyset.Handle, error) {\n\taeadTemplate := aead.AES256GCMKeyTemplate()\n\n\taes256GcmHandle, err := keyset.NewHandle(aeadTemplate)\n\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"failed to create new keyset handle with AES256GCM template: %w\", err)\n\t}\n\n\tbytes, err := insecureBytesFromHandle(aes256GcmHandle)\n\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"failed to get bytes from handle: %w\", err)\n\t}\n\n\treturn bytes, aes256GcmHandle, nil\n}\n\n// generateJWTKeysets creates the keysets for JWT signing and verification encrypted with the\n// masterKey. The masterKey can be from a remote KMS service or a local keyset.\nfunc generateJWTKeysets(masterKey tink.AEAD) (privateEc256 []byte, publicEc256 []byte, err error) {\n\tprivateHandle, err := keyset.NewHandle(jwt.ES256Template())\n\n\tif err != nil {\n\t\terr = fmt.Errorf(\"failed to create new keyset handle with ES256 template: %w\", err)\n\t\treturn\n\t}\n\n\tprivateEc256, err = bytesFromHandle(privateHandle, masterKey)\n\n\tif err != nil {\n\t\treturn\n\t}\n\n\tpublicHandle, err := privateHandle.Public()\n\n\tif err != nil {\n\t\terr = fmt.Errorf(\"failed to get public keyset: %w\", err)\n\t\treturn\n\t}\n\n\tpublicEc256, err = bytesFromHandle(publicHandle, masterKey)\n\n\tif err != nil {\n\t\treturn\n\t}\n\n\treturn\n}\n\n// bytesFromHandle returns the encrypted keyset in base64-encoded JSON format, encrypted with the\n// masterKey\nfunc bytesFromHandle(kh *keyset.Handle, masterKey tink.AEAD) ([]byte, error) {\n\tbuf := new(bytes.Buffer)\n\twriter := keyset.NewJSONWriter(buf)\n\terr := kh.Write(writer, masterKey)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to write keyset: %w\", err)\n\t}\n\n\t// base64-encode bytes\n\tkeysetBytes := make([]byte, base64.RawStdEncoding.EncodedLen(len(buf.Bytes())))\n\tbase64.RawStdEncoding.Encode(keysetBytes, buf.Bytes())\n\n\treturn keysetBytes, nil\n}\n\n// insecureBytesFromHandle returns the raw (unencrypted) keyset in base64-encoded JSON format.\nfunc insecureBytesFromHandle(kh *keyset.Handle) ([]byte, error) {\n\tbuf := new(bytes.Buffer)\n\twriter := keyset.NewJSONWriter(buf)\n\terr := insecurecleartextkeyset.Write(kh, writer)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to write keyset: %w\", err)\n\t}\n\n\t// base64-encode bytes\n\tkeysetBytes := make([]byte, base64.RawStdEncoding.EncodedLen(len(buf.Bytes())))\n\tbase64.RawStdEncoding.Encode(keysetBytes, buf.Bytes())\n\n\treturn keysetBytes, nil\n}\n\nfunc handleFromBytes(keysetBytes []byte, masterKey tink.AEAD) (*keyset.Handle, error) {\n\t// base64-decode bytes\n\tkeysetJsonBytes := make([]byte, base64.RawStdEncoding.DecodedLen(len(keysetBytes)))\n\t_, err := base64.RawStdEncoding.Decode(keysetJsonBytes, keysetBytes)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to decode keyset bytes: %w\", err)\n\t}\n\n\t// read keyset\n\thandle, err := keyset.Read(keyset.NewJSONReader(bytes.NewReader(keysetJsonBytes)), masterKey)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to read keyset: %w\", err)\n\t}\n\n\treturn handle, nil\n}\n\nfunc insecureHandleFromBytes(keysetBytes []byte) (*keyset.Handle, error) {\n\t// base64-decode bytes\n\tkeysetJsonBytes := make([]byte, base64.RawStdEncoding.DecodedLen(len(keysetBytes)))\n\t_, err := base64.RawStdEncoding.Decode(keysetJsonBytes, keysetBytes)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to decode keyset bytes: %w\", err)\n\t}\n\n\t// read keyset\n\thandle, err := insecurecleartextkeyset.Read(keyset.NewJSONReader(bytes.NewReader(keysetJsonBytes)))\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to read keyset: %w\", err)\n\t}\n\n\treturn handle, nil\n}\n\nfunc (svc *localEncryptionService) Encrypt(plaintext []byte, dataId string) ([]byte, error) {\n\treturn encrypt(svc.key, plaintext, dataId)\n}\n\nfunc (svc *localEncryptionService) Decrypt(ciphertext []byte, dataId string) ([]byte, error) {\n\treturn decrypt(svc.key, ciphertext, dataId)\n}\n\nfunc (svc *localEncryptionService) EncryptString(data string, dataId string) (string, error) {\n\tb, err := encrypt(svc.key, []byte(data), dataId)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn base64.StdEncoding.EncodeToString(b), nil\n}\n\nfunc (svc *localEncryptionService) DecryptString(data string, dataId string) (string, error) {\n\tplain, err := base64.StdEncoding.DecodeString(data)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tb, err := decrypt(svc.key, plain, dataId)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn string(b), nil\n}\n\nfunc (svc *localEncryptionService) GetPrivateJWTHandle() *keyset.Handle {\n\treturn svc.privateEc256Handle\n}\n\nfunc (svc *localEncryptionService) GetPublicJWTHandle() *keyset.Handle {\n\treturn svc.publicEc256Handle\n}\n\n```\n# Function Signature: func NewLocalEncryption(masterKey []byte, privateEc256 []byte, publicEc256 []byte) (*localEncryptionService, error) {\n# Natural Language Annotations: // NewLocalEncryption creates a new local encryption service. keysetBytes is the raw keyset in\n// base64-encoded JSON format. This can be generated by calling hatchet-admin keyset create-local.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewLocalEncryption(masterKey, privateEc256, publicEc256 []byte) (*localEncryptionService, error) {\n\t// Convert the keys to keyset handles\n\tmasterHandle, err := handleFromBytes(masterKey, nil)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to convert master key to keyset handle: %w\", err)\n\t}\n\n\tprivateHandle, err := handleFromBytes(privateEc256, masterHandle)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to convert private EC256 key to keyset handle: %w\", err)\n\t}\n\n\tpublicHandle, err := handleFromBytes(publicEc256, masterHandle)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to convert public EC256 key to keyset handle: %w\", err)\n\t}\n\n\t// Create the encryption service\n\tsvc := &localEncryptionService{\n\t\tkey:                nil,\n\t\tprivateEc256Handle: privateHandle,\n\t\tpublicEc256Handle:  publicHandle,\n\t}\n\n\treturn svc, nil\n}\n```", "  ```\nfunc NewLocalEncryption(masterKey []byte, privateEc256 []byte, publicEc256 []byte) (*localEncryptionService, error) {\n\t// Convert the master key to a keyset handle\n\tmasterKeyHandle, err := insecureHandleFromBytes(masterKey)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create master key handle: %w\", err)\n\t}\n\n\t// Create a new local encryption service\n\tsvc := &localEncryptionService{\n\t\tkey: masterKeyHandle,\n\t}\n\n\t// Generate a new key handle from the private EC256 key\n\tsvc.privateEc256Handle, err = handleFromBytes(privateEc256, svc.key)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create private EC256 key handle: %w\", err)\n\t}\n\n\t// Generate a new key handle from the public EC256 key\n\tsvc.publicEc256Handle, err = handleFromBytes(publicEc256, svc.key)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create public EC256 key handle: %w\", err)\n\t}\n\n\treturn svc, nil\n}\n```", "  ```\nfunc NewLocalEncryption(masterKey []byte, privateEc256 []byte, publicEc256 []byte) (*localEncryptionService, error) {\n\t// create a new KMSEnvelopeAEAD\n\tkey, err := aead.New(masterKey)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create KMSEnvelopeAEAD: %w\", err)\n\t}\n\n\t// create a new localEncryptionService\n\tsvc := &localEncryptionService{\n\t\tkey: key,\n\t}\n\n\t// get the private and public JWT keysets\n\tprivateHandle, err := keyset.Read(bytes.NewReader(privateEc256), key)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to read privateJWTHandle: %w\", err)\n\t}\n\tsvc.privateEc256Handle = privateHandle\n\n\tpublicHandle, err := keyset.Read(bytes.NewReader(publicEc256), key)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to read publicJWTHandle: %w\", err)\n\t}\n\tsvc.publicEc256Handle = publicHandle\n\n\treturn svc, nil\n}\n```"]}, "hatchet-pkg/encryption/cloudkms.go-NewCloudKMSEncryption": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/hatchet/pkg/encryption/cloudkms.go:\n```\npackage encryption\n\nimport (\n\t\"context\"\n\t\"encoding/base64\"\n\t\"fmt\"\n\n\t\"github.com/tink-crypto/tink-go-gcpkms/integration/gcpkms\"\n\t\"github.com/tink-crypto/tink-go/aead\"\n\t\"github.com/tink-crypto/tink-go/core/registry\"\n\t\"github.com/tink-crypto/tink-go/keyset\"\n\t\"google.golang.org/api/option\"\n)\n\ntype cloudkmsEncryptionService struct {\n\tkey                *aead.KMSEnvelopeAEAD\n\tprivateEc256Handle *keyset.Handle\n\tpublicEc256Handle  *keyset.Handle\n}\n\n// NewCloudKMSEncryption creates a GCP CloudKMS-backed encryption service.\n\n\n\n\n\n\n\n\n\n\nfunc GenerateJWTKeysetsFromCloudKMS(keyUri string, credentialsJSON []byte) (privateEc256 []byte, publicEc256 []byte, err error) {\n\tclient, err := gcpkms.NewClientWithOptions(context.Background(), keyUri, option.WithCredentialsJSON(credentialsJSON))\n\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\treturn generateJWTKeysetsWithClient(keyUri, client)\n}\n\nfunc generateJWTKeysetsWithClient(keyUri string, client registry.KMSClient) (privateEc256 []byte, publicEc256 []byte, err error) {\n\tregistry.RegisterKMSClient(client)\n\n\tremote, err := client.GetAEAD(keyUri)\n\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\treturn generateJWTKeysets(remote)\n}\n\nfunc newWithClient(client registry.KMSClient, keyUri string, privateEc256, publicEc256 []byte) (*cloudkmsEncryptionService, error) {\n\tregistry.RegisterKMSClient(client)\n\n\tdek := aead.AES128CTRHMACSHA256KeyTemplate()\n\ttemplate, err := aead.CreateKMSEnvelopeAEADKeyTemplate(keyUri, dek)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// get the remote KEK from the client\n\tremote, err := client.GetAEAD(keyUri)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tenvelope := aead.NewKMSEnvelopeAEAD2(template, remote)\n\n\tif envelope == nil {\n\t\treturn nil, fmt.Errorf(\"failed to create envelope\")\n\t}\n\n\tprivateEc256Handle, err := handleFromBytes(privateEc256, remote)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tpublicEc256Handle, err := handleFromBytes(publicEc256, remote)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &cloudkmsEncryptionService{\n\t\tkey:                envelope,\n\t\tprivateEc256Handle: privateEc256Handle,\n\t\tpublicEc256Handle:  publicEc256Handle,\n\t}, nil\n}\n\nfunc (svc *cloudkmsEncryptionService) Encrypt(plaintext []byte, dataId string) ([]byte, error) {\n\treturn encrypt(svc.key, plaintext, dataId)\n}\n\nfunc (svc *cloudkmsEncryptionService) Decrypt(ciphertext []byte, dataId string) ([]byte, error) {\n\treturn decrypt(svc.key, ciphertext, dataId)\n}\n\nfunc (svc *cloudkmsEncryptionService) EncryptString(plaintext string, dataId string) (string, error) {\n\tb, err := encrypt(svc.key, []byte(plaintext), dataId)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn base64.StdEncoding.EncodeToString(b), nil\n}\n\nfunc (svc *cloudkmsEncryptionService) DecryptString(ciphertext string, dataId string) (string, error) {\n\tdecoded, err := base64.StdEncoding.DecodeString(ciphertext)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tb, err := decrypt(svc.key, decoded, dataId)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn string(b), nil\n}\n\nfunc (svc *cloudkmsEncryptionService) GetPrivateJWTHandle() *keyset.Handle {\n\treturn svc.privateEc256Handle\n}\n\nfunc (svc *cloudkmsEncryptionService) GetPublicJWTHandle() *keyset.Handle {\n\treturn svc.publicEc256Handle\n}\n\n```\n# Function Signature: func NewCloudKMSEncryption(keyUri string, credentialsJSON, privateEc256, publicEc256 []byte) (*cloudkmsEncryptionService, error) {\n# Natural Language Annotations: // NewCloudKMSEncryption creates a GCP CloudKMS-backed encryption service.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\npackage encryption\n\nimport (\n\t\"context\"\n\t\"encoding/base64\"\n\t\"fmt\"\n\n\t\"github.com/tink-crypto/tink-go-gcpkms/integration/gcpkms\"\n\t\"github.com/tink-crypto/tink-go/aead\"\n\t\"github.com/tink-crypto/tink-go/core/registry\"\n\t\"github.com/tink-crypto/tink-go/keyset\"\n\t\"google.golang.org/api/option\"\n)\n\n// NewCloudKMSEncryption creates a GCP CloudKMS-backed encryption service.\nfunc NewCloudKMSEncryption(keyUri string, credentialsJSON []byte, privateEc256, publicEc256 []byte) (*cloudkmsEncryptionService, error) {\n\tclient, err := gcpkms.NewClientWithOptions(context.Background(), keyUri, option.WithCredentialsJSON(credentialsJSON))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn newWithClient(client, keyUri, privateEc256, publicEc256)\n}\n\nfunc newWithClient(client registry.KMSClient, keyUri string, privateEc256, publicEc256 []byte) (*cloudkmsEncryptionService, error) {\n\tdek := aead.AES128CTRHMACSHA256KeyTemplate()\n\ttemplate, err := aead.CreateKMSEnvelopeAEADKeyTemplate(keyUri, dek)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tremote, err := client.GetAEAD(keyUri)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tenvelope := aead.NewKMSEnvelopeAEAD2(template, remote)\n\tif envelope == nil {\n\t\treturn nil, fmt.Errorf(\"failed to create envelope\")\n\t}\n\tprivateEc256Handle, err := handleFromBytes(privateEc256, remote)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpublicEc256Handle, err := handleFromBytes(publicEc256, remote)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &cloudkmsEncryptionService{\n\t\tkey:                envelope,\n\t\tprivateEc256Handle: privateEc256Handle,\n\t\tpublicEc256Handle:  publicEc256Handle,\n\t}, nil\n}\n\nfunc (svc *cloudkmsEncryptionService) Encrypt(plaintext []byte, dataId string) ([]byte, error) {\n\treturn encrypt(svc.key, plaintext, dataId)\n}\n\nfunc (svc *cloudkmsEncryptionService) Decrypt(ciphertext []byte, dataId string) ([]byte, error) {\n\treturn decrypt(svc.key, ciphertext, dataId)\n}\n\nfunc (svc *cloudkmsEncryptionService) EncryptString(plaintext string, dataId string) (string, error) {\n\tb, err := encrypt(svc.key, []byte(plaintext), dataId)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn base64.StdEncoding.EncodeToString(b), nil\n}\n\nfunc (svc *cloudkmsEncryptionService) DecryptString(ciphertext string, dataId string) (string, error) {\n\tdecoded, err := base64.StdEncoding.DecodeString(ciphertext)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tb, err := decrypt(svc.key, decoded, dataId)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn string(b), nil\n}\n\nfunc (svc *cloudkmsEncryptionService) GetPrivateJWTHandle() *keyset.Handle {\n\treturn svc.privateEc256Handle\n}\n\nfunc (svc *cloudkmsEncryptionService) GetPublicJWTHandle() *keyset.Handle {\n\treturn svc.publicEc256Handle", "  ```\nfunc NewCloudKMSEncryption(keyUri string, credentialsJSON, privateEc256, publicEc256 []byte) (*cloudkmsEncryptionService, error) {\n\tclient, err := gcpkms.NewClientWithOptions(context.Background(), keyUri, option.WithCredentialsJSON(credentialsJSON))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tprivateEc256Handle, err := handleFromBytes(privateEc256, client)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpublicEc256Handle, err := handleFromBytes(publicEc256, client)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &cloudkmsEncryptionService{\n\t\tkey:                key,\n\t\tprivateEc256Handle: privateEc256Handle,\n\t\tpublicEc256Handle:  publicEc256Handle,\n\t}, nil\n}\n\nfunc handleFromBytes(key []byte, client registry.KMSClient) (*keyset.Handle, error) {\n\tremote, err := client.GetAEAD(keyUri)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn generateJWTKeysetsWithClient(keyUri, client)\n}\n```", "  ```\nfunc NewCloudKMSEncryption(keyUri string, credentialsJSON []byte, privateEc256, publicEc256 []byte) (*cloudkmsEncryptionService, error) {\n\t// Create a client with the given credentials and key URI\n\tclient, err := gcpkms.NewClientWithOptions(context.Background(), keyUri, option.WithCredentialsJSON(credentialsJSON))\n\tif err != nil {\n\t\treturn nil, err\n\t}\t\n\t// Generate JWT keysets using the given key URI and client\n\tprivateEc256Bin, publicEc256Bin, err := generateJWTKeysetsWithClient(keyUri, client)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// Create an envelope AEAD with the extracted JWT key sets\n\tversion := 0\n\tlabels := map[string]string{}\n\tkmsKey, err := client.GetPublicKey(version, labels)\n\tif err != nil {\n\t\treturn nil, err\n    }\n\tdek := aead.AES128CTRHMACSHA256KeyTemplate()\n\ttemplate, err := aead.CreateKMSEnvelopeAEADKeyTemplate(keyUri, dek)\n\tif err != nil {\n\t\treturn nil, err\n    }\n    envelope, err := aead.NewKMSEnvelopeAEAD2(template, kmsKey)\n\tif err != nil {\n\t\treturn nil, err \n\t}\n\tprivateEc256Handle, err := handleFromBytes(privateEc256Bin, kmsKey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpublicEc256Handle, err := handleFromBytes(publicEc256Bin, kmsKey)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// Create and return the encryption service\n\treturn &cloudkmsEncryptionService{\n\t\tkey:                envelope,\n\t\tprivateEc256Handle: privateEc256Handle,\n\t\tpublicEc256Handle:  publicEc256Handle,\n\t}, nil\n}\n``` "]}, "hatchet-internal/datautils/render.go-RenderTemplateFields": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/hatchet/internal/datautils/render.go:\n```\npackage datautils\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"text/template\"\n)\n\n// RenderTemplateFields recursively processes the input map, rendering any string fields using the data map.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func RenderTemplateFields(data map[string]interface{}, input map[string]interface{}) (map[string]interface{}, error) {\n# Natural Language Annotations: // RenderTemplateFields recursively processes the input map, rendering any string fields using the data map.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\npackage datautils\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"text/template\"\n)\n\n// RenderTemplateFields recursively processes the input map, rendering any string fields using the data map.\nfunc RenderTemplateFields(data map[string]interface{}, input map[string]interface{}) (map[string]interface{}, error) {\n\tresult := make(map[string]interface{})\n\tfor k, v := range input {\n\t\tswitch v := v.(type) {\n\t\tcase string:\n\t\t\ttmpl, err := template.New(\"template\").Parse(\"{{.\" + v + \"}}\")\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tvar buf bytes.Buffer\n\t\t\tif err := tmpl.Execute(&buf, data); err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tresult[k] = buf.String()\n\t\tcase map[string]interface{}:\n\t\t\tresult[k], err := RenderTemplateFields(data, v)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\tdefault:\n\t\t\tresult[k] = v\n\t\t}\n\t}\n\treturn result, nil\n}\n```", "  ```go\nfunc RenderTemplateFields(data map[string]interface{}, input map[string]interface{}) (map[string]interface{}, error) {\n\tresult := make(map[string]interface{})\n\n\tfor key, value := range input {\n\t\tswitch v := value.(type) {\n\t\tcase string:\n\t\t\tt, err := template.New(key).Parse(v)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tvar b bytes.Buffer\n\t\t\terr = t.Execute(&b, data)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tresult[key] = b.String()\n\t\tcase map[string]interface{}:\n\t\t\tresult[key], err = RenderTemplateFields(data, v)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\tdefault:\n\t\t\tresult[key] = value\n\t\t}\n\t}\n\n\treturn result, nil\n}\n```\nExplanation:\n\nThe code takes two parameters, a map of data to be used for rendering and a map of input values that may contain templates. The function recursively processes the input map, replacing any string fields with values from the data map. The modified map is returned.\n\nThe code uses the `template.New`, `template.Parse`, and `template.Execute` functions to parse and render the templates. The `reflect` package is used to determine the type of the input value, to avoid calling these functions on non-string fields.\n\nThe `bytes` package is used to create a buffer to store the rendered template. The `fmt` package is used to check for errors.", "  ```\npackage datautils\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"text/template\"\n)\n\n// RenderTemplateFields recursively processes the input map, rendering any string fields using the data map.\nfunc RenderTemplateFields(data map[string]interface{}, input map[string]interface{}) (map[string]interface{}, error) {\n\toutput := map[string]interface{}{}\n\n\tfor key, value := range input {\n\t\tswitch val := value.(type) {\n\t\tcase string:\n\t\t\t// Render strings using the data map\n\t\t\trendered, err := renderString(val, data)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\toutput[key] = rendered\n\t\tcase map[string]interface{}:\n\t\t\t// Process nested maps recursively\n\t\t\trendered, err := RenderTemplateFields(data, val)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\toutput[key] = rendered\n\t\tdefault:\n\t\t\t// Pass through other values unmodified\n\t\t\toutput[key] = value\n\t\t}\n\t}\n\n\treturn output, nil\n}\n\nfunc renderString(str string, data map[string]interface{}) (string, error) {\n\tif str == \"\" {\n\t\t// Return empty string if the input is empty\n\t\treturn \"\", nil\n\t}\n\n\t// Parse the template\n\ttmpl, err := template.New(\"\").Parse(str)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Create a buffer to write the rendered template to\n\tbuf := &bytes.Buffer{}\n\n\t// Execute the template with the data\n\tif err := tmpl.Execute(buf, data); err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Return the rendered output as a string\n\treturn buf.String(), nil\n}\n\n```\nThis function processes the input map and renders any string fields using the data map. It is a recursive function, and it processes the input map and any nested maps recursively, until it reaches the end of the nested structure. It also passes through any non-string values unmodified.\n\nThe `renderString` function takes a string and a map, and it renders the string using the data map. It does this by parsing the string using `template.New(\"\").Parse` and then executing the template with the data using `tmpl.Execute`. The result of the execution is a buffer that contains the rendered output, and this buffer is returned as a string.\n\nThe `RenderTemplateFields` function uses the `renderString` function to render any string fields in the input map. It also processes any nested maps recursively, until it reaches the end of the nested structure. It returns the output map, which is a copy of the input map with any string fields that needed rendering replaced with the rendered output. It also returns an error if there was an error rendering any of the strings."]}, "mox-ratelimit/ratelimit.go-Reset": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/ratelimit/ratelimit.go:\n```\n// Package ratelimit provides a simple window-based rate limiter.\npackage ratelimit\n\nimport (\n\t\"net\"\n\t\"sync\"\n\t\"time\"\n)\n\n// Limiter is a simple rate limiter with one or more fixed windows, e.g. the\n// last minute/hour/day/week, working on three classes/subnets of an IP.\ntype Limiter struct {\n\tsync.Mutex\n\tWindowLimits []WindowLimit\n\tipmasked     [3][16]byte\n}\n\n// WindowLimit holds counters for one window, with limits for each IP class/subnet.\ntype WindowLimit struct {\n\tWindow time.Duration\n\tLimits [3]int64 // For \"ipmasked1\" through \"ipmasked3\".\n\tTime   uint32   // Time/Window.\n\tCounts map[struct {\n\t\tIndex    uint8\n\t\tIPMasked [16]byte\n\t}]int64\n}\n\n// Add attempts to consume \"n\" items from the rate limiter. If the total for this\n// key and this interval would exceed limit, \"n\" is not counted and false is\n// returned. If now represents a different time interval, all counts are reset.\nfunc (l *Limiter) Add(ip net.IP, tm time.Time, n int64) bool {\n\treturn l.checkAdd(true, ip, tm, n)\n}\n\n// CanAdd returns if n could be added to the limiter.\nfunc (l *Limiter) CanAdd(ip net.IP, tm time.Time, n int64) bool {\n\treturn l.checkAdd(false, ip, tm, n)\n}\n\nfunc (l *Limiter) checkAdd(add bool, ip net.IP, tm time.Time, n int64) bool {\n\tl.Lock()\n\tdefer l.Unlock()\n\n\t// First check.\n\tfor i, pl := range l.WindowLimits {\n\t\tt := uint32(tm.UnixNano() / int64(pl.Window))\n\n\t\tif t > pl.Time || pl.Counts == nil {\n\t\t\tl.WindowLimits[i].Time = t\n\t\t\tpl.Counts = map[struct {\n\t\t\t\tIndex    uint8\n\t\t\t\tIPMasked [16]byte\n\t\t\t}]int64{} // Used below.\n\t\t\tl.WindowLimits[i].Counts = pl.Counts\n\t\t}\n\n\t\tfor j := 0; j < 3; j++ {\n\t\t\tif i == 0 {\n\t\t\t\tl.ipmasked[j] = l.maskIP(j, ip)\n\t\t\t}\n\n\t\t\tv := pl.Counts[struct {\n\t\t\t\tIndex    uint8\n\t\t\t\tIPMasked [16]byte\n\t\t\t}{uint8(j), l.ipmasked[j]}]\n\t\t\tif v+n > pl.Limits[j] {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\tif !add {\n\t\treturn true\n\t}\n\t// Finally record.\n\tfor _, pl := range l.WindowLimits {\n\t\tfor j := 0; j < 3; j++ {\n\t\t\tpl.Counts[struct {\n\t\t\t\tIndex    uint8\n\t\t\t\tIPMasked [16]byte\n\t\t\t}{uint8(j), l.ipmasked[j]}] += n\n\t\t}\n\t}\n\treturn true\n}\n\n// Reset sets the counter to 0 for key and ip, and subtracts from the ipmasked counts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (l *Limiter) maskIP(i int, ip net.IP) [16]byte {\n\tisv4 := ip.To4() != nil\n\n\tvar ipmasked net.IP\n\tif isv4 {\n\t\tswitch i {\n\t\tcase 0:\n\t\t\tipmasked = ip\n\t\tcase 1:\n\t\t\tipmasked = ip.Mask(net.CIDRMask(26, 32))\n\t\tcase 2:\n\t\t\tipmasked = ip.Mask(net.CIDRMask(21, 32))\n\t\tdefault:\n\t\t\tpanic(\"missing case for maskip ipv4\")\n\t\t}\n\t} else {\n\t\tswitch i {\n\t\tcase 0:\n\t\t\tipmasked = ip.Mask(net.CIDRMask(64, 128))\n\t\tcase 1:\n\t\t\tipmasked = ip.Mask(net.CIDRMask(48, 128))\n\t\tcase 2:\n\t\t\tipmasked = ip.Mask(net.CIDRMask(32, 128))\n\t\tdefault:\n\t\t\tpanic(\"missing case for masking ipv6\")\n\t\t}\n\t}\n\treturn *(*[16]byte)(ipmasked.To16())\n}\n\n```\n# Function Signature: func (l *Limiter) Reset(ip net.IP, tm time.Time) {\n# Natural Language Annotations: // Reset sets the counter to 0 for key and ip, and subtracts from the ipmasked counts.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (l *Limiter) Reset(ip net.IP, tm time.Time) {\n\t// TODO: Implement this function\n}\n```", "  ```\nfunc (l *Limiter) Reset(ip net.IP, tm time.Time) {\n\tl.Lock()\n\tdefer l.Unlock()\n\n\tt := uint32(tm.UnixNano() / int64(pl.Window))\n\n\tfor i := 0; i < 3; i++ {\n\t\tif i == 0 {\n\t\t\tl.ipmasked[i] = l.maskIP(i, ip)\n\t\t}\n\t\tpl.Counts[struct {\n\t\t\tIndex    uint8\n\t\t\tIPMasked [16]byte\n\t\t}{uint8(i), l.ipmasked[i]}] = 0\n\t}\n\tfor _, pl := range l.WindowLimits {\n\t\tpl.Time = t\n\t\tfor j := 0; j < 3; j++ {\n\t\t\tpl.Counts[struct {\n\t\t\t\tIndex    uint8\n\t\t\t\tIPMasked [16]byte\n\t\t\t}{uint8(j), l.ipmasked[j]}] -= 1\n\t\t}\n\t}\n}\n```", "  ```\nfunc (l *Limiter) Reset(ip net.IP, tm time.Time) {\n\tl.Lock()\n\tdefer l.Unlock()\n\n\t// Reset Time to 0 for used WindowLimits.\n\tfor i, pl := range l.WindowLimits {\n\t\tt := uint32(tm.UnixNano() / int64(pl.Window))\n\n\t\tif pl.Counts != nil {\n\t\t\tif pl.Time < t {\n\t\t\t\tpl.Time = 0\n\t\t\t}\n\t\t}\n\t}\n\t// Subtract the ip counter\n\tipMasked := l.maskIP(ip)\n\tfor _, pl := range l.WindowLimits {\n\t\tif pl.Counts != nil {\n\t\t\tfor i, v := range pl.Counts {\n\t\t\t\tif ipMasked == i {\n\t\t\t\t\tpl.Counts[i] -= v\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n```"]}, "mox-dns/dns.go-ParseDomainLax": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dns/dns.go:\n```\n// Package dns helps parse internationalized domain names (IDNA), canonicalize\n// names and provides a strict and metrics-keeping logging DNS resolver.\npackage dns\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"golang.org/x/net/idna\"\n\n\t\"github.com/mjl-/adns\"\n)\n\n// Pedantic enables stricter parsing.\nvar Pedantic bool\n\nvar (\n\terrTrailingDot = errors.New(\"dns name has trailing dot\")\n\terrUnderscore  = errors.New(\"domain name with underscore\")\n\terrIDNA        = errors.New(\"idna\")\n)\n\n// Domain is a domain name, with one or more labels, with at least an ASCII\n// representation, and for IDNA non-ASCII domains a unicode representation.\n// The ASCII string must be used for DNS lookups. The strings do not have a\n// trailing dot. When using with StrictResolver, add the trailing dot.\ntype Domain struct {\n\t// A non-unicode domain, e.g. with A-labels (xn--...) or NR-LDH (non-reserved\n\t// letters/digits/hyphens) labels. Always in lower case. No trailing dot.\n\tASCII string\n\n\t// Name as U-labels, in Unicode NFC. Empty if this is an ASCII-only domain. No\n\t// trailing dot.\n\tUnicode string\n}\n\n// Name returns the unicode name if set, otherwise the ASCII name.\nfunc (d Domain) Name() string {\n\tif d.Unicode != \"\" {\n\t\treturn d.Unicode\n\t}\n\treturn d.ASCII\n}\n\n// XName is like Name, but only returns a unicode name when utf8 is true.\nfunc (d Domain) XName(utf8 bool) string {\n\tif utf8 && d.Unicode != \"\" {\n\t\treturn d.Unicode\n\t}\n\treturn d.ASCII\n}\n\n// ASCIIExtra returns the ASCII version of the domain name if smtputf8 is true and\n// this is a unicode domain name. Otherwise it returns an empty string.\n//\n// This function is used to add the punycode name in a comment to SMTP message\n// headers, e.g. Received and Authentication-Results.\nfunc (d Domain) ASCIIExtra(smtputf8 bool) string {\n\tif smtputf8 && d.Unicode != \"\" {\n\t\treturn d.ASCII\n\t}\n\treturn \"\"\n}\n\n// Strings returns a human-readable string.\n// For IDNA names, the string contains both the unicode and ASCII name.\nfunc (d Domain) String() string {\n\treturn d.LogString()\n}\n\n// LogString returns a domain for logging.\n// For IDNA names, the string is the slash-separated Unicode and ASCII name.\n// For ASCII-only domain names, just the ASCII string is returned.\nfunc (d Domain) LogString() string {\n\tif d.Unicode == \"\" {\n\t\treturn d.ASCII\n\t}\n\treturn d.Unicode + \"/\" + d.ASCII\n}\n\n// IsZero returns if this is an empty Domain.\nfunc (d Domain) IsZero() bool {\n\treturn d == Domain{}\n}\n\n// ParseDomain parses a domain name that can consist of ASCII-only labels or U\n// labels (unicode).\n// Names are IDN-canonicalized and lower-cased.\n// Characters in unicode can be replaced by equivalents. E.g. \"\u24c7\" to \"r\". This\n// means you should only compare parsed domain names, never unparsed strings\n// directly.\nfunc ParseDomain(s string) (Domain, error) {\n\tif strings.HasSuffix(s, \".\") {\n\t\treturn Domain{}, errTrailingDot\n\t}\n\n\tascii, err := idna.Lookup.ToASCII(s)\n\tif err != nil {\n\t\treturn Domain{}, fmt.Errorf(\"%w: to ascii: %v\", errIDNA, err)\n\t}\n\tunicode, err := idna.Lookup.ToUnicode(s)\n\tif err != nil {\n\t\treturn Domain{}, fmt.Errorf(\"%w: to unicode: %w\", errIDNA, err)\n\t}\n\t// todo: should we cause errors for unicode domains that were not in\n\t// canonical form? we are now accepting all kinds of obscure spellings\n\t// for even a basic ASCII domain name.\n\t// Also see https://daniel.haxx.se/blog/2022/12/14/idn-is-crazy/\n\tif ascii == unicode {\n\t\treturn Domain{ascii, \"\"}, nil\n\t}\n\treturn Domain{ascii, unicode}, nil\n}\n\n// ParseDomainLax parses a domain like ParseDomain, but allows labels with\n// underscores if the entire domain name is ASCII-only non-IDNA and Pedantic mode\n// is not enabled. Used for interoperability, e.g. domains may specify MX\n// targets with underscores.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// IsNotFound returns whether an error is an adns.DNSError with IsNotFound set.\n// IsNotFound means the requested type does not exist for the given domain (a\n// nodata or nxdomain response). It doesn't not necessarily mean no other types for\n// that name exist.\n//\n// A DNS server can respond to a lookup with an error \"nxdomain\" to indicate a\n// name does not exist (at all), or with a success status with an empty list.\n// The adns resolver (just like the Go resolver) returns an IsNotFound error for\n// both cases, there is no need to explicitly check for zero entries.\nfunc IsNotFound(err error) bool {\n\tvar dnsErr *adns.DNSError\n\treturn err != nil && errors.As(err, &dnsErr) && dnsErr.IsNotFound\n}\n\n```\n# Function Signature: func ParseDomainLax(s string) (Domain, error) {\n# Natural Language Annotations: // ParseDomainLax parses a domain like ParseDomain, but allows labels with\n// underscores if the entire domain name is ASCII-only non-IDNA and Pedantic mode\n// is not enabled. Used for interoperability, e.g. domains may specify MX\n// targets with underscores.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc ParseDomainLax(s string) (Domain, error) {\n\tif s != idna.Lookup.ToASCII(s) {\n\t\treturn Domain{}, fmt.Errorf(\"domain is not ASCII-only: %w\", errIDNA)\n\t}\n\tif Pedantic && strings.ContainsRune(s, '_') {\n\t\treturn Domain{}, fmt.Errorf(\"%w: underscore not allowed\", errUnderscore)\n\t}\n\tascii, err := idna.Lookup.ToASCII(s)\n\tif err != nil {\n\t\treturn Domain{}, fmt.Errorf(\"%w: to ASCII: %v\", errIDNA, err)\n\t}\n\treturn Domain{ascii, \"\"}, nil\n}\n```", "  ```\nfunc ParseDomainLax(s string) (Domain, error) {\n\tif strings.Contains(s, \"_\") &&\n\t\tstrings.IndexByte(s, '.') > 0 &&\n\t\t!isIDNA(s) &&\n\t\t!Pedantic {\n\t\treturn domainWithUnderscores(s)\n\t}\n\treturn ParseDomain(s)\n}\n\nfunc domainWithUnderscores(s string) (Domain, error) {\n\tlabels := strings.Split(s, \".\")\n\tfor i, label := range labels {\n\t\tif strings.Contains(label, \"_\") {\n\t\t\treturn Domain{}, fmt.Errorf(\"%w: label %q with underscores\", errUnderscore, label)\n\t\t}\n\t\tlabels[i] = strings.ToLower(label)\n\t}\n\treturn Domain{strings.Join(labels, \".\"), \"\"}, nil\n}\n\nfunc isIDNA(s string) bool {\n\t_, err := idna.Lookup.ToASCII(s)\n\treturn err == nil\n}\n```", "  ```\nfunc ParseDomainLax(s string) (Domain, error) {\n\tif Pedantic {\n\t\treturn ParseDomain(s)\n\t}\n\tif strings.HasPrefix(s, \"xn--\") {\n\t\treturn ParseDomain(s)\n\t}\n\tif strings.IndexFunc(s, unicode.IsDigit) > 0 &&\n\t\tstrings.IndexFunc(s, func(r rune) bool { return unicode.IsOneOf(unicode.Alpha, unicode.Number, unicode.Hyphen) }) > 0 {\n\t\treturn ParseDomain(s)\n\t}\n\tif strings.Index(s, \"_\") > 0 {\n\t\treturn Domain{}, errUnderscore\n\t}\n\treturn ParseDomain(s)\n}\n\n```\n"]}, "mox-dmarcdb/eval.go-AddEvaluation": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dmarcdb/eval.go:\n```\npackage dmarcdb\n\n// Sending TLS reports and DMARC reports is very similar. See ../dmarcdb/eval.go:/similar and ../tlsrptsend/send.go:/similar.\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"mime/multipart\"\n\t\"net/textproto\"\n\t\"net/url\"\n\t\"os\"\n\t\"runtime/debug\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/exp/maps\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/dkim\"\n\t\"github.com/mjl-/mox/dmarc\"\n\t\"github.com/mjl-/mox/dmarcrpt\"\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/metrics\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/moxvar\"\n\t\"github.com/mjl-/mox/publicsuffix\"\n\t\"github.com/mjl-/mox/queue\"\n\t\"github.com/mjl-/mox/smtp\"\n\t\"github.com/mjl-/mox/store\"\n)\n\nvar (\n\tmetricReport = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_queued_total\",\n\t\t\tHelp: \"Total messages with DMARC aggregate/error reports queued.\",\n\t\t},\n\t)\n\tmetricReportError = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_error_total\",\n\t\t\tHelp: \"Total errors while composing or queueing DMARC aggregate/error reports.\",\n\t\t},\n\t)\n)\n\nvar (\n\tEvalDBTypes = []any{Evaluation{}, SuppressAddress{}} // Types stored in DB.\n\t// Exported for backups. For incoming deliveries the SMTP server adds evaluations\n\t// to the database. Every hour, a goroutine wakes up that gathers evaluations from\n\t// the last hour(s), sends a report, and removes the evaluations from the database.\n\tEvalDB *bstore.DB\n)\n\n// Evaluation is the result of an evaluation of a DMARC policy, to be included\n// in a DMARC report.\ntype Evaluation struct {\n\tID int64\n\n\t// Domain where DMARC policy was found, could be the organizational domain while\n\t// evaluation was for a subdomain. Unicode. Same as domain found in\n\t// PolicyPublished. A separate field for its index.\n\tPolicyDomain string `bstore:\"index\"`\n\n\t// Time of evaluation, determines which report (covering whole hours) this\n\t// evaluation will be included in.\n\tEvaluated time.Time `bstore:\"default now\"`\n\n\t// If optional, this evaluation is not a reason to send a DMARC report, but it will\n\t// be included when a report is sent due to other non-optional evaluations. Set for\n\t// evaluations of incoming DMARC reports. We don't want such deliveries causing us to\n\t// send a report, or we would keep exchanging reporting messages forever. Also set\n\t// for when evaluation is a DMARC reject for domains we haven't positively\n\t// interacted with, to prevent being used to flood an unsuspecting domain with\n\t// reports.\n\tOptional bool\n\n\t// Effective aggregate reporting interval in hours. Between 1 and 24, rounded up\n\t// from seconds from policy to first number that can divide 24.\n\tIntervalHours int\n\n\t// \"rua\" in DMARC record, we only store evaluations for records with aggregate reporting addresses, so always non-empty.\n\tAddresses []string\n\n\t// Policy used for evaluation. We don't store the \"fo\" field for failure reporting\n\t// options, since we don't send failure reports for individual messages.\n\tPolicyPublished dmarcrpt.PolicyPublished\n\n\t// For \"row\" in a report record.\n\tSourceIP        string\n\tDisposition     dmarcrpt.Disposition\n\tAlignedDKIMPass bool\n\tAlignedSPFPass  bool\n\tOverrideReasons []dmarcrpt.PolicyOverrideReason\n\n\t// For \"identifiers\" in a report record.\n\tEnvelopeTo   string\n\tEnvelopeFrom string\n\tHeaderFrom   string\n\n\t// For \"auth_results\" in a report record.\n\tDKIMResults []dmarcrpt.DKIMAuthResult\n\tSPFResults  []dmarcrpt.SPFAuthResult\n}\n\n// SuppressAddress is a reporting address for which outgoing DMARC reports\n// will be suppressed for a period.\ntype SuppressAddress struct {\n\tID               int64\n\tInserted         time.Time `bstore:\"default now\"`\n\tReportingAddress string    `bstore:\"unique\"`\n\tUntil            time.Time `bstore:\"nonzero\"`\n\tComment          string\n}\n\nvar dmarcResults = map[bool]dmarcrpt.DMARCResult{\n\tfalse: dmarcrpt.DMARCFail,\n\ttrue:  dmarcrpt.DMARCPass,\n}\n\n// ReportRecord turns an evaluation into a record that can be included in a\n// report.\nfunc (e Evaluation) ReportRecord(count int) dmarcrpt.ReportRecord {\n\treturn dmarcrpt.ReportRecord{\n\t\tRow: dmarcrpt.Row{\n\t\t\tSourceIP: e.SourceIP,\n\t\t\tCount:    count,\n\t\t\tPolicyEvaluated: dmarcrpt.PolicyEvaluated{\n\t\t\t\tDisposition: e.Disposition,\n\t\t\t\tDKIM:        dmarcResults[e.AlignedDKIMPass],\n\t\t\t\tSPF:         dmarcResults[e.AlignedSPFPass],\n\t\t\t\tReasons:     e.OverrideReasons,\n\t\t\t},\n\t\t},\n\t\tIdentifiers: dmarcrpt.Identifiers{\n\t\t\tEnvelopeTo:   e.EnvelopeTo,\n\t\t\tEnvelopeFrom: e.EnvelopeFrom,\n\t\t\tHeaderFrom:   e.HeaderFrom,\n\t\t},\n\t\tAuthResults: dmarcrpt.AuthResults{\n\t\t\tDKIM: e.DKIMResults,\n\t\t\tSPF:  e.SPFResults,\n\t\t},\n\t}\n}\n\nvar intervalOpts = []int{24, 12, 8, 6, 4, 3, 2}\n\nfunc intervalHours(seconds int) int {\n\thours := (seconds + 3600 - 1) / 3600\n\tfor _, opt := range intervalOpts {\n\t\tif hours >= opt {\n\t\t\treturn opt\n\t\t}\n\t}\n\treturn 1\n}\n\n// AddEvaluation adds the result of a DMARC evaluation for an incoming message\n// to the database.\n//\n// AddEvaluation sets Evaluation.IntervalHours based on\n// aggregateReportingIntervalSeconds.\n\n\n\n\n\n\n\n// Evaluations returns all evaluations in the database.\nfunc Evaluations(ctx context.Context) ([]Evaluation, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.SortAsc(\"Evaluated\")\n\treturn q.List()\n}\n\n// EvaluationStat summarizes stored evaluations, for inclusion in an upcoming\n// aggregate report, for a domain.\ntype EvaluationStat struct {\n\tDomain       dns.Domain\n\tDispositions []string\n\tCount        int\n\tSendReport   bool\n}\n\n// EvaluationStats returns evaluation counts and report-sending status per domain.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// EvaluationsDomain returns all evaluations for a domain.\n\n\n\n\n\n\n\n// RemoveEvaluationsDomain removes evaluations for domain so they won't be sent in\n// an aggregate report.\n\n\n\n\n\n\n\nvar jitterRand = mox.NewPseudoRand()\n\n// time to sleep until next whole hour t, replaced by tests.\n// Jitter so we don't cause load at exactly whole hours, other processes may\n// already be doing that.\nvar jitteredTimeUntil = func(t time.Time) time.Duration {\n\treturn time.Until(t.Add(time.Duration(30+jitterRand.Intn(60)) * time.Second))\n}\n\n// Start launches a goroutine that wakes up at each whole hour (plus jitter) and\n// sends DMARC reports to domains that requested them.\nfunc Start(resolver dns.Resolver) {\n\tgo func() {\n\t\tlog := mlog.New(\"dmarcdb\", nil)\n\n\t\tdefer func() {\n\t\t\t// In case of panic don't take the whole program down.\n\t\t\tx := recover()\n\t\t\tif x != nil {\n\t\t\t\tlog.Error(\"recover from panic\", slog.Any(\"panic\", x))\n\t\t\t\tdebug.PrintStack()\n\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t}\n\t\t}()\n\n\t\ttimer := time.NewTimer(time.Hour)\n\t\tdefer timer.Stop()\n\n\t\tctx := mox.Shutdown\n\n\t\tfor {\n\t\t\tnow := time.Now()\n\t\t\tnextEnd := nextWholeHour(now)\n\t\t\ttimer.Reset(jitteredTimeUntil(nextEnd))\n\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tlog.Info(\"dmarc aggregate report sender shutting down\")\n\t\t\t\treturn\n\t\t\tcase <-timer.C:\n\t\t\t}\n\n\t\t\t// Gather report intervals we want to process now. Multiples of hours that can\n\t\t\t// divide 24, starting from UTC.\n\t\t\t// ../rfc/7489:1750\n\t\t\tutchour := nextEnd.UTC().Hour()\n\t\t\tif utchour == 0 {\n\t\t\t\tutchour = 24\n\t\t\t}\n\t\t\tintervals := []int{}\n\t\t\tfor _, ival := range intervalOpts {\n\t\t\t\tif ival*(utchour/ival) == utchour {\n\t\t\t\t\tintervals = append(intervals, ival)\n\t\t\t\t}\n\t\t\t}\n\t\t\tintervals = append(intervals, 1)\n\n\t\t\t// Remove evaluations older than 48 hours (2 reports with the default and maximum\n\t\t\t// 24 hour interval). They should have been processed by now. We may have kept them\n\t\t\t// during temporary errors, but persistent temporary errors shouldn't fill up our\n\t\t\t// database. This also cleans up evaluations that were all optional for a domain.\n\t\t\t_, err := bstore.QueryDB[Evaluation](ctx, EvalDB).FilterLess(\"Evaluated\", nextEnd.Add(-48*time.Hour)).Delete()\n\t\t\tlog.Check(err, \"removing stale dmarc evaluations from database\")\n\n\t\t\tclog := log.WithCid(mox.Cid())\n\t\t\tclog.Info(\"sending dmarc aggregate reports\", slog.Time(\"end\", nextEnd.UTC()), slog.Any(\"intervals\", intervals))\n\t\t\tif err := sendReports(ctx, clog, resolver, EvalDB, nextEnd, intervals); err != nil {\n\t\t\t\tclog.Errorx(\"sending dmarc aggregate reports\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t} else {\n\t\t\t\tclog.Info(\"finished sending dmarc aggregate reports\")\n\t\t\t}\n\t\t}\n\t}()\n}\n\nfunc nextWholeHour(now time.Time) time.Time {\n\tt := now\n\tt = t.Add(time.Hour)\n\treturn time.Date(t.Year(), t.Month(), t.Day(), t.Hour(), 0, 0, 0, t.Location())\n}\n\n// We don't send reports at full speed. In the future, we could try to stretch out\n// reports a bit smarter. E.g. over 5 minutes with some minimum interval, and\n// perhaps faster and in parallel when there are lots of reports. Perhaps also\n// depending on reporting interval (faster for 1h, slower for 24h).\n// Replaced by tests.\nvar sleepBetween = func(ctx context.Context, between time.Duration) (ok bool) {\n\tt := time.NewTimer(between)\n\tselect {\n\tcase <-ctx.Done():\n\t\tt.Stop()\n\t\treturn false\n\tcase <-t.C:\n\t\treturn true\n\t}\n}\n\n// sendReports gathers all policy domains that have evaluations that should\n// receive a DMARC report and sends a report to each.\nfunc sendReports(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, intervals []int) error {\n\tivals := make([]any, len(intervals))\n\tfor i, v := range intervals {\n\t\tivals[i] = v\n\t}\n\n\tdestDomains := map[string]bool{}\n\n\t// Gather all domains that we plan to send to.\n\tnsend := 0\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterEqual(\"IntervalHours\", ivals...)\n\terr := q.ForEach(func(e Evaluation) error {\n\t\tif !e.Optional && !destDomains[e.PolicyPublished.Domain] {\n\t\t\tnsend++\n\t\t}\n\t\tdestDomains[e.PolicyPublished.Domain] = destDomains[e.PolicyPublished.Domain] || !e.Optional\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"looking for domains to send reports to: %v\", err)\n\t}\n\n\tvar wg sync.WaitGroup\n\n\t// Sleep in between sending reports. We process hourly, and spread the reports over\n\t// the hour, but with max 5 minute interval.\n\tbetween := 45 * time.Minute\n\tif nsend > 0 {\n\t\tbetween /= time.Duration(nsend)\n\t}\n\tif between > 5*time.Minute {\n\t\tbetween = 5 * time.Minute\n\t}\n\n\t// Attempt to send report to each domain.\n\tn := 0\n\tfor d, send := range destDomains {\n\t\t// Cleanup evaluations for domain with only optionals.\n\t\tif !send {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, d)\n\t\t\tcontinue\n\t\t}\n\n\t\tif n > 0 {\n\t\t\tif ok := sleepBetween(ctx, between); !ok {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t\tn++\n\n\t\t// Send in goroutine, so a slow process doesn't block progress.\n\t\twg.Add(1)\n\t\tgo func(domain string) {\n\t\t\tdefer func() {\n\t\t\t\t// In case of panic don't take the whole program down.\n\t\t\t\tx := recover()\n\t\t\t\tif x != nil {\n\t\t\t\t\tlog.Error(\"unhandled panic in dmarcdb sendReports\", slog.Any(\"panic\", x))\n\t\t\t\t\tdebug.PrintStack()\n\t\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t\t}\n\t\t\t}()\n\t\t\tdefer wg.Done()\n\n\t\t\trlog := log.WithCid(mox.Cid()).With(slog.Any(\"domain\", domain))\n\t\t\trlog.Info(\"sending dmarc report\")\n\t\t\tif _, err := sendReportDomain(ctx, rlog, resolver, db, endTime, domain); err != nil {\n\t\t\t\trlog.Errorx(\"sending dmarc aggregate report to domain\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t}\n\t\t}(d)\n\t}\n\n\twg.Wait()\n\n\treturn nil\n}\n\ntype recipient struct {\n\taddress smtp.Address\n\tmaxSize uint64\n}\n\nfunc parseRecipient(log mlog.Log, uri dmarc.URI) (r recipient, ok bool) {\n\tlog = log.With(slog.Any(\"uri\", uri.Address))\n\n\tu, err := url.Parse(uri.Address)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\tif !strings.EqualFold(u.Scheme, \"mailto\") {\n\t\tlog.Debug(\"skipping unrecognized scheme in dmarc record rua value\")\n\t\treturn r, false\n\t}\n\taddr, err := smtp.ParseAddress(u.Opaque)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing mailto uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\n\tr = recipient{addr, uri.MaxSize}\n\t// ../rfc/7489:1197\n\tswitch uri.Unit {\n\tcase \"k\", \"K\":\n\t\tr.maxSize *= 1024\n\tcase \"m\", \"M\":\n\t\tr.maxSize *= 1024 * 1024\n\tcase \"g\", \"G\":\n\t\tr.maxSize *= 1024 * 1024 * 1024\n\tcase \"t\", \"T\":\n\t\t// Oh yeah, terabyte-sized reports!\n\t\tr.maxSize *= 1024 * 1024 * 1024 * 1024\n\tcase \"\":\n\tdefault:\n\t\tlog.Debug(\"unrecognized max size unit in dmarc record rua value\", slog.String(\"unit\", uri.Unit))\n\t\treturn r, false\n\t}\n\n\treturn r, true\n}\n\nfunc removeEvaluations(ctx context.Context, log mlog.Log, db *bstore.DB, endTime time.Time, domain string) {\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterNonzero(Evaluation{PolicyDomain: domain})\n\t_, err := q.Delete()\n\tlog.Check(err, \"removing evaluations after processing for dmarc aggregate report\")\n}\n\n// replaceable for testing.\nvar queueAdd = queue.Add\n\nfunc sendReportDomain(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, domain string) (cleanup bool, rerr error) {\n\tdom, err := dns.ParseDomain(domain)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"parsing domain for sending reports: %v\", err)\n\t}\n\n\t// We'll cleanup records by default.\n\tcleanup = true\n\t// If we encounter a temporary error we cancel cleanup of evaluations on error.\n\ttempError := false\n\n\tdefer func() {\n\t\tif !cleanup || tempError {\n\t\t\tlog.Debug(\"not cleaning up evaluations after attempting to send dmarc aggregate report\")\n\t\t} else {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, domain)\n\t\t}\n\t}()\n\n\t// We're going to build up this report.\n\treport := dmarcrpt.Feedback{\n\t\tVersion: \"1.0\",\n\t\tReportMetadata: dmarcrpt.ReportMetadata{\n\t\t\tOrgName: mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\tEmail:   \"postmaster@\" + mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\t// ReportID and DateRange are set after we've seen evaluations.\n\t\t\t// Errors is filled below when we encounter problems.\n\t\t},\n\t\t// We'll fill the records below.\n\t\tRecords: []dmarcrpt.ReportRecord{},\n\t}\n\n\tvar errors []string // For report.ReportMetaData.Errors\n\n\t// Check if we should be sending a report at all: if there are rua URIs in the\n\t// current DMARC record. The interval may have changed too, but we'll flush out our\n\t// evaluations regardless. We always use the latest DMARC record when sending, but\n\t// we'll lump all policies of the last interval into one report.\n\t// ../rfc/7489:1714\n\tstatus, _, record, _, _, err := dmarc.Lookup(ctx, log.Logger, resolver, dom)\n\tif err != nil {\n\t\t// todo future: we could perhaps still send this report, assuming the values we know. in case of temporary error, we could also schedule again regardless of next interval hour (we would now only retry a 24h-interval report after 24h passed).\n\t\t// Remove records unless it was a temporary error. We'll try again next round.\n\t\tcleanup = status != dmarc.StatusTemperror\n\t\treturn cleanup, fmt.Errorf(\"looking up current dmarc record for reporting address: %v\", err)\n\t}\n\n\tvar recipients []recipient\n\n\t// Gather all aggregate reporting addresses to try to send to. We'll start with\n\t// those in the initial DMARC record, but will follow external reporting addresses\n\t// and possibly update the list.\n\tfor _, uri := range record.AggregateReportAddresses {\n\t\tr, ok := parseRecipient(log, uri)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Check if domain of rua recipient has the same organizational domain as for the\n\t\t// evaluations. If not, we need to verify we are allowed to send.\n\t\truaOrgDom := publicsuffix.Lookup(ctx, log.Logger, r.address.Domain)\n\t\tevalOrgDom := publicsuffix.Lookup(ctx, log.Logger, dom)\n\n\t\tif ruaOrgDom == evalOrgDom {\n\t\t\trecipients = append(recipients, r)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Verify and follow addresses in other organizational domain through\n\t\t// <policydomain>._report._dmarc.<host> lookup.\n\t\t// ../rfc/7489:1556\n\t\taccepts, status, records, _, _, err := dmarc.LookupExternalReportsAccepted(ctx, log.Logger, resolver, evalOrgDom, r.address.Domain)\n\t\tlog.Debugx(\"checking if rua address with different organization domain has opted into receiving dmarc reports\", err,\n\t\t\tslog.Any(\"policydomain\", evalOrgDom),\n\t\t\tslog.Any(\"destinationdomain\", r.address.Domain),\n\t\t\tslog.Bool(\"accepts\", accepts),\n\t\t\tslog.Any(\"status\", status))\n\t\tif status == dmarc.StatusTemperror {\n\t\t\t// With a temporary error, we'll try to get the report the delivered anyway,\n\t\t\t// perhaps there are multiple recipients.\n\t\t\t// ../rfc/7489:1578\n\t\t\ttempError = true\n\t\t\terrors = append(errors, \"temporary error checking authorization for report delegation to external address\")\n\t\t}\n\t\tif !accepts {\n\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain that does not opt-in to receiving dmarc records through _report dmarc record\", r.address))\n\t\t\tcontinue\n\t\t}\n\n\t\t// We can follow a _report DMARC DNS record once. In that record, a domain may\n\t\t// specify alternative addresses that we should send reports to instead. Such\n\t\t// alternative address(es) must have the same host. If not, we ignore the new\n\t\t// value. Behaviour for multiple records and/or multiple new addresses is\n\t\t// underspecified. We'll replace an address with one or more new addresses, and\n\t\t// keep the original if there was no candidate (which covers the case of invalid\n\t\t// alternative addresses and no new address specified).\n\t\t// ../rfc/7489:1600\n\t\tfoundReplacement := false\n\t\trlog := log.With(slog.Any(\"followedaddress\", uri.Address))\n\t\tfor _, record := range records {\n\t\t\tfor _, exturi := range record.AggregateReportAddresses {\n\t\t\t\textr, ok := parseRecipient(rlog, exturi)\n\t\t\t\tif !ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif extr.address.Domain != r.address.Domain {\n\t\t\t\t\trlog.Debug(\"rua address in external _report dmarc record has different host than initial dmarc record, ignoring new name\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain with a replacement address %s with different host\", r.address, extr.address))\n\t\t\t\t} else {\n\t\t\t\t\trlog.Debug(\"using replacement rua address from external _report dmarc record\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\tfoundReplacement = true\n\t\t\t\t\trecipients = append(recipients, extr)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif !foundReplacement {\n\t\t\trecipients = append(recipients, r)\n\t\t}\n\t}\n\n\tif len(recipients) == 0 {\n\t\t// No reports requested, perfectly fine, no work to do for us.\n\t\tlog.Debug(\"no aggregate reporting addresses configured\")\n\t\treturn true, nil\n\t}\n\n\t// We count idential records. Can be common with a domain sending quite some email.\n\t// Though less if the sending domain has many IPs. In the future, we may want to\n\t// remove some details from records so we can aggregate them into fewer rows.\n\ttype recordCount struct {\n\t\tdmarcrpt.ReportRecord\n\t\tcount int\n\t}\n\tcounts := map[string]recordCount{}\n\n\n```\n# Function Signature: func AddEvaluation(ctx context.Context, aggregateReportingIntervalSeconds int, e *Evaluation) error {\n# Natural Language Annotations: // AddEvaluation adds the result of a DMARC evaluation for an incoming message\n// to the database.\n//\n// AddEvaluation sets Evaluation.IntervalHours based on\n// aggregateReportingIntervalSeconds.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc AddEvaluation(ctx context.Context, aggregateReportingIntervalSeconds int, e *Evaluation) error {\n\te.IntervalHours = intervalHours(aggregateReportingIntervalSeconds)\n\n\t_, err := bstore.InsertDB[Evaluation](ctx, EvalDB, e)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"adding dmarc evaluation to database: %v\", err)\n\t}\n\n\treturn nil\n}\n```", "  \n```\nfunc AddEvaluation(ctx context.Context, aggregateReportingIntervalSeconds int, e *Evaluation) error {\n\te.IntervalHours = intervalHours(aggregateReportingIntervalSeconds)\n\tif e.IntervalHours < 1 || e.IntervalHours > 24 {\n\t\treturn fmt.Errorf(\"aggregate reporting interval must be between 1 and 24 hours, but %d seconds\", aggregateReportingIntervalSeconds)\n\t}\n\treturn bstore.Add(ctx, e)\n}\n```", "  ```\nfunc AddEvaluation(ctx context.Context, aggregateReportingIntervalSeconds int, e *Evaluation) error {\n\te.IntervalHours = intervalHours(aggregateReportingIntervalSeconds)\n\n\tlog := mlog.New(\"dmarcdb\", nil)\n\terr := bstore.WithContext(ctx, EvalDB).Insert(e, EvalDBTypes[0])\n\tlog.Check(err, \"saving dmarc evaluation in database\")\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tmetricReport.Inc()\n\treturn nil\n}\n```"]}, "mox-dmarcdb/eval.go-EvaluationStats": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dmarcdb/eval.go:\n```\npackage dmarcdb\n\n// Sending TLS reports and DMARC reports is very similar. See ../dmarcdb/eval.go:/similar and ../tlsrptsend/send.go:/similar.\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"mime/multipart\"\n\t\"net/textproto\"\n\t\"net/url\"\n\t\"os\"\n\t\"runtime/debug\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/exp/maps\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/dkim\"\n\t\"github.com/mjl-/mox/dmarc\"\n\t\"github.com/mjl-/mox/dmarcrpt\"\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/metrics\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/moxvar\"\n\t\"github.com/mjl-/mox/publicsuffix\"\n\t\"github.com/mjl-/mox/queue\"\n\t\"github.com/mjl-/mox/smtp\"\n\t\"github.com/mjl-/mox/store\"\n)\n\nvar (\n\tmetricReport = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_queued_total\",\n\t\t\tHelp: \"Total messages with DMARC aggregate/error reports queued.\",\n\t\t},\n\t)\n\tmetricReportError = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_error_total\",\n\t\t\tHelp: \"Total errors while composing or queueing DMARC aggregate/error reports.\",\n\t\t},\n\t)\n)\n\nvar (\n\tEvalDBTypes = []any{Evaluation{}, SuppressAddress{}} // Types stored in DB.\n\t// Exported for backups. For incoming deliveries the SMTP server adds evaluations\n\t// to the database. Every hour, a goroutine wakes up that gathers evaluations from\n\t// the last hour(s), sends a report, and removes the evaluations from the database.\n\tEvalDB *bstore.DB\n)\n\n// Evaluation is the result of an evaluation of a DMARC policy, to be included\n// in a DMARC report.\ntype Evaluation struct {\n\tID int64\n\n\t// Domain where DMARC policy was found, could be the organizational domain while\n\t// evaluation was for a subdomain. Unicode. Same as domain found in\n\t// PolicyPublished. A separate field for its index.\n\tPolicyDomain string `bstore:\"index\"`\n\n\t// Time of evaluation, determines which report (covering whole hours) this\n\t// evaluation will be included in.\n\tEvaluated time.Time `bstore:\"default now\"`\n\n\t// If optional, this evaluation is not a reason to send a DMARC report, but it will\n\t// be included when a report is sent due to other non-optional evaluations. Set for\n\t// evaluations of incoming DMARC reports. We don't want such deliveries causing us to\n\t// send a report, or we would keep exchanging reporting messages forever. Also set\n\t// for when evaluation is a DMARC reject for domains we haven't positively\n\t// interacted with, to prevent being used to flood an unsuspecting domain with\n\t// reports.\n\tOptional bool\n\n\t// Effective aggregate reporting interval in hours. Between 1 and 24, rounded up\n\t// from seconds from policy to first number that can divide 24.\n\tIntervalHours int\n\n\t// \"rua\" in DMARC record, we only store evaluations for records with aggregate reporting addresses, so always non-empty.\n\tAddresses []string\n\n\t// Policy used for evaluation. We don't store the \"fo\" field for failure reporting\n\t// options, since we don't send failure reports for individual messages.\n\tPolicyPublished dmarcrpt.PolicyPublished\n\n\t// For \"row\" in a report record.\n\tSourceIP        string\n\tDisposition     dmarcrpt.Disposition\n\tAlignedDKIMPass bool\n\tAlignedSPFPass  bool\n\tOverrideReasons []dmarcrpt.PolicyOverrideReason\n\n\t// For \"identifiers\" in a report record.\n\tEnvelopeTo   string\n\tEnvelopeFrom string\n\tHeaderFrom   string\n\n\t// For \"auth_results\" in a report record.\n\tDKIMResults []dmarcrpt.DKIMAuthResult\n\tSPFResults  []dmarcrpt.SPFAuthResult\n}\n\n// SuppressAddress is a reporting address for which outgoing DMARC reports\n// will be suppressed for a period.\ntype SuppressAddress struct {\n\tID               int64\n\tInserted         time.Time `bstore:\"default now\"`\n\tReportingAddress string    `bstore:\"unique\"`\n\tUntil            time.Time `bstore:\"nonzero\"`\n\tComment          string\n}\n\nvar dmarcResults = map[bool]dmarcrpt.DMARCResult{\n\tfalse: dmarcrpt.DMARCFail,\n\ttrue:  dmarcrpt.DMARCPass,\n}\n\n// ReportRecord turns an evaluation into a record that can be included in a\n// report.\nfunc (e Evaluation) ReportRecord(count int) dmarcrpt.ReportRecord {\n\treturn dmarcrpt.ReportRecord{\n\t\tRow: dmarcrpt.Row{\n\t\t\tSourceIP: e.SourceIP,\n\t\t\tCount:    count,\n\t\t\tPolicyEvaluated: dmarcrpt.PolicyEvaluated{\n\t\t\t\tDisposition: e.Disposition,\n\t\t\t\tDKIM:        dmarcResults[e.AlignedDKIMPass],\n\t\t\t\tSPF:         dmarcResults[e.AlignedSPFPass],\n\t\t\t\tReasons:     e.OverrideReasons,\n\t\t\t},\n\t\t},\n\t\tIdentifiers: dmarcrpt.Identifiers{\n\t\t\tEnvelopeTo:   e.EnvelopeTo,\n\t\t\tEnvelopeFrom: e.EnvelopeFrom,\n\t\t\tHeaderFrom:   e.HeaderFrom,\n\t\t},\n\t\tAuthResults: dmarcrpt.AuthResults{\n\t\t\tDKIM: e.DKIMResults,\n\t\t\tSPF:  e.SPFResults,\n\t\t},\n\t}\n}\n\nvar intervalOpts = []int{24, 12, 8, 6, 4, 3, 2}\n\nfunc intervalHours(seconds int) int {\n\thours := (seconds + 3600 - 1) / 3600\n\tfor _, opt := range intervalOpts {\n\t\tif hours >= opt {\n\t\t\treturn opt\n\t\t}\n\t}\n\treturn 1\n}\n\n// AddEvaluation adds the result of a DMARC evaluation for an incoming message\n// to the database.\n//\n// AddEvaluation sets Evaluation.IntervalHours based on\n// aggregateReportingIntervalSeconds.\n\n\n\n\n\n\n\n// Evaluations returns all evaluations in the database.\nfunc Evaluations(ctx context.Context) ([]Evaluation, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.SortAsc(\"Evaluated\")\n\treturn q.List()\n}\n\n// EvaluationStat summarizes stored evaluations, for inclusion in an upcoming\n// aggregate report, for a domain.\ntype EvaluationStat struct {\n\tDomain       dns.Domain\n\tDispositions []string\n\tCount        int\n\tSendReport   bool\n}\n\n// EvaluationStats returns evaluation counts and report-sending status per domain.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// EvaluationsDomain returns all evaluations for a domain.\n\n\n\n\n\n\n\n// RemoveEvaluationsDomain removes evaluations for domain so they won't be sent in\n// an aggregate report.\n\n\n\n\n\n\n\nvar jitterRand = mox.NewPseudoRand()\n\n// time to sleep until next whole hour t, replaced by tests.\n// Jitter so we don't cause load at exactly whole hours, other processes may\n// already be doing that.\nvar jitteredTimeUntil = func(t time.Time) time.Duration {\n\treturn time.Until(t.Add(time.Duration(30+jitterRand.Intn(60)) * time.Second))\n}\n\n// Start launches a goroutine that wakes up at each whole hour (plus jitter) and\n// sends DMARC reports to domains that requested them.\nfunc Start(resolver dns.Resolver) {\n\tgo func() {\n\t\tlog := mlog.New(\"dmarcdb\", nil)\n\n\t\tdefer func() {\n\t\t\t// In case of panic don't take the whole program down.\n\t\t\tx := recover()\n\t\t\tif x != nil {\n\t\t\t\tlog.Error(\"recover from panic\", slog.Any(\"panic\", x))\n\t\t\t\tdebug.PrintStack()\n\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t}\n\t\t}()\n\n\t\ttimer := time.NewTimer(time.Hour)\n\t\tdefer timer.Stop()\n\n\t\tctx := mox.Shutdown\n\n\t\tfor {\n\t\t\tnow := time.Now()\n\t\t\tnextEnd := nextWholeHour(now)\n\t\t\ttimer.Reset(jitteredTimeUntil(nextEnd))\n\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tlog.Info(\"dmarc aggregate report sender shutting down\")\n\t\t\t\treturn\n\t\t\tcase <-timer.C:\n\t\t\t}\n\n\t\t\t// Gather report intervals we want to process now. Multiples of hours that can\n\t\t\t// divide 24, starting from UTC.\n\t\t\t// ../rfc/7489:1750\n\t\t\tutchour := nextEnd.UTC().Hour()\n\t\t\tif utchour == 0 {\n\t\t\t\tutchour = 24\n\t\t\t}\n\t\t\tintervals := []int{}\n\t\t\tfor _, ival := range intervalOpts {\n\t\t\t\tif ival*(utchour/ival) == utchour {\n\t\t\t\t\tintervals = append(intervals, ival)\n\t\t\t\t}\n\t\t\t}\n\t\t\tintervals = append(intervals, 1)\n\n\t\t\t// Remove evaluations older than 48 hours (2 reports with the default and maximum\n\t\t\t// 24 hour interval). They should have been processed by now. We may have kept them\n\t\t\t// during temporary errors, but persistent temporary errors shouldn't fill up our\n\t\t\t// database. This also cleans up evaluations that were all optional for a domain.\n\t\t\t_, err := bstore.QueryDB[Evaluation](ctx, EvalDB).FilterLess(\"Evaluated\", nextEnd.Add(-48*time.Hour)).Delete()\n\t\t\tlog.Check(err, \"removing stale dmarc evaluations from database\")\n\n\t\t\tclog := log.WithCid(mox.Cid())\n\t\t\tclog.Info(\"sending dmarc aggregate reports\", slog.Time(\"end\", nextEnd.UTC()), slog.Any(\"intervals\", intervals))\n\t\t\tif err := sendReports(ctx, clog, resolver, EvalDB, nextEnd, intervals); err != nil {\n\t\t\t\tclog.Errorx(\"sending dmarc aggregate reports\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t} else {\n\t\t\t\tclog.Info(\"finished sending dmarc aggregate reports\")\n\t\t\t}\n\t\t}\n\t}()\n}\n\nfunc nextWholeHour(now time.Time) time.Time {\n\tt := now\n\tt = t.Add(time.Hour)\n\treturn time.Date(t.Year(), t.Month(), t.Day(), t.Hour(), 0, 0, 0, t.Location())\n}\n\n// We don't send reports at full speed. In the future, we could try to stretch out\n// reports a bit smarter. E.g. over 5 minutes with some minimum interval, and\n// perhaps faster and in parallel when there are lots of reports. Perhaps also\n// depending on reporting interval (faster for 1h, slower for 24h).\n// Replaced by tests.\nvar sleepBetween = func(ctx context.Context, between time.Duration) (ok bool) {\n\tt := time.NewTimer(between)\n\tselect {\n\tcase <-ctx.Done():\n\t\tt.Stop()\n\t\treturn false\n\tcase <-t.C:\n\t\treturn true\n\t}\n}\n\n// sendReports gathers all policy domains that have evaluations that should\n// receive a DMARC report and sends a report to each.\nfunc sendReports(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, intervals []int) error {\n\tivals := make([]any, len(intervals))\n\tfor i, v := range intervals {\n\t\tivals[i] = v\n\t}\n\n\tdestDomains := map[string]bool{}\n\n\t// Gather all domains that we plan to send to.\n\tnsend := 0\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterEqual(\"IntervalHours\", ivals...)\n\terr := q.ForEach(func(e Evaluation) error {\n\t\tif !e.Optional && !destDomains[e.PolicyPublished.Domain] {\n\t\t\tnsend++\n\t\t}\n\t\tdestDomains[e.PolicyPublished.Domain] = destDomains[e.PolicyPublished.Domain] || !e.Optional\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"looking for domains to send reports to: %v\", err)\n\t}\n\n\tvar wg sync.WaitGroup\n\n\t// Sleep in between sending reports. We process hourly, and spread the reports over\n\t// the hour, but with max 5 minute interval.\n\tbetween := 45 * time.Minute\n\tif nsend > 0 {\n\t\tbetween /= time.Duration(nsend)\n\t}\n\tif between > 5*time.Minute {\n\t\tbetween = 5 * time.Minute\n\t}\n\n\t// Attempt to send report to each domain.\n\tn := 0\n\tfor d, send := range destDomains {\n\t\t// Cleanup evaluations for domain with only optionals.\n\t\tif !send {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, d)\n\t\t\tcontinue\n\t\t}\n\n\t\tif n > 0 {\n\t\t\tif ok := sleepBetween(ctx, between); !ok {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t\tn++\n\n\t\t// Send in goroutine, so a slow process doesn't block progress.\n\t\twg.Add(1)\n\t\tgo func(domain string) {\n\t\t\tdefer func() {\n\t\t\t\t// In case of panic don't take the whole program down.\n\t\t\t\tx := recover()\n\t\t\t\tif x != nil {\n\t\t\t\t\tlog.Error(\"unhandled panic in dmarcdb sendReports\", slog.Any(\"panic\", x))\n\t\t\t\t\tdebug.PrintStack()\n\t\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t\t}\n\t\t\t}()\n\t\t\tdefer wg.Done()\n\n\t\t\trlog := log.WithCid(mox.Cid()).With(slog.Any(\"domain\", domain))\n\t\t\trlog.Info(\"sending dmarc report\")\n\t\t\tif _, err := sendReportDomain(ctx, rlog, resolver, db, endTime, domain); err != nil {\n\t\t\t\trlog.Errorx(\"sending dmarc aggregate report to domain\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t}\n\t\t}(d)\n\t}\n\n\twg.Wait()\n\n\treturn nil\n}\n\ntype recipient struct {\n\taddress smtp.Address\n\tmaxSize uint64\n}\n\nfunc parseRecipient(log mlog.Log, uri dmarc.URI) (r recipient, ok bool) {\n\tlog = log.With(slog.Any(\"uri\", uri.Address))\n\n\tu, err := url.Parse(uri.Address)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\tif !strings.EqualFold(u.Scheme, \"mailto\") {\n\t\tlog.Debug(\"skipping unrecognized scheme in dmarc record rua value\")\n\t\treturn r, false\n\t}\n\taddr, err := smtp.ParseAddress(u.Opaque)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing mailto uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\n\tr = recipient{addr, uri.MaxSize}\n\t// ../rfc/7489:1197\n\tswitch uri.Unit {\n\tcase \"k\", \"K\":\n\t\tr.maxSize *= 1024\n\tcase \"m\", \"M\":\n\t\tr.maxSize *= 1024 * 1024\n\tcase \"g\", \"G\":\n\t\tr.maxSize *= 1024 * 1024 * 1024\n\tcase \"t\", \"T\":\n\t\t// Oh yeah, terabyte-sized reports!\n\t\tr.maxSize *= 1024 * 1024 * 1024 * 1024\n\tcase \"\":\n\tdefault:\n\t\tlog.Debug(\"unrecognized max size unit in dmarc record rua value\", slog.String(\"unit\", uri.Unit))\n\t\treturn r, false\n\t}\n\n\treturn r, true\n}\n\nfunc removeEvaluations(ctx context.Context, log mlog.Log, db *bstore.DB, endTime time.Time, domain string) {\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterNonzero(Evaluation{PolicyDomain: domain})\n\t_, err := q.Delete()\n\tlog.Check(err, \"removing evaluations after processing for dmarc aggregate report\")\n}\n\n// replaceable for testing.\nvar queueAdd = queue.Add\n\nfunc sendReportDomain(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, domain string) (cleanup bool, rerr error) {\n\tdom, err := dns.ParseDomain(domain)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"parsing domain for sending reports: %v\", err)\n\t}\n\n\t// We'll cleanup records by default.\n\tcleanup = true\n\t// If we encounter a temporary error we cancel cleanup of evaluations on error.\n\ttempError := false\n\n\tdefer func() {\n\t\tif !cleanup || tempError {\n\t\t\tlog.Debug(\"not cleaning up evaluations after attempting to send dmarc aggregate report\")\n\t\t} else {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, domain)\n\t\t}\n\t}()\n\n\t// We're going to build up this report.\n\treport := dmarcrpt.Feedback{\n\t\tVersion: \"1.0\",\n\t\tReportMetadata: dmarcrpt.ReportMetadata{\n\t\t\tOrgName: mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\tEmail:   \"postmaster@\" + mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\t// ReportID and DateRange are set after we've seen evaluations.\n\t\t\t// Errors is filled below when we encounter problems.\n\t\t},\n\t\t// We'll fill the records below.\n\t\tRecords: []dmarcrpt.ReportRecord{},\n\t}\n\n\tvar errors []string // For report.ReportMetaData.Errors\n\n\t// Check if we should be sending a report at all: if there are rua URIs in the\n\t// current DMARC record. The interval may have changed too, but we'll flush out our\n\t// evaluations regardless. We always use the latest DMARC record when sending, but\n\t// we'll lump all policies of the last interval into one report.\n\t// ../rfc/7489:1714\n\tstatus, _, record, _, _, err := dmarc.Lookup(ctx, log.Logger, resolver, dom)\n\tif err != nil {\n\t\t// todo future: we could perhaps still send this report, assuming the values we know. in case of temporary error, we could also schedule again regardless of next interval hour (we would now only retry a 24h-interval report after 24h passed).\n\t\t// Remove records unless it was a temporary error. We'll try again next round.\n\t\tcleanup = status != dmarc.StatusTemperror\n\t\treturn cleanup, fmt.Errorf(\"looking up current dmarc record for reporting address: %v\", err)\n\t}\n\n\tvar recipients []recipient\n\n\t// Gather all aggregate reporting addresses to try to send to. We'll start with\n\t// those in the initial DMARC record, but will follow external reporting addresses\n\t// and possibly update the list.\n\tfor _, uri := range record.AggregateReportAddresses {\n\t\tr, ok := parseRecipient(log, uri)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Check if domain of rua recipient has the same organizational domain as for the\n\t\t// evaluations. If not, we need to verify we are allowed to send.\n\t\truaOrgDom := publicsuffix.Lookup(ctx, log.Logger, r.address.Domain)\n\t\tevalOrgDom := publicsuffix.Lookup(ctx, log.Logger, dom)\n\n\t\tif ruaOrgDom == evalOrgDom {\n\t\t\trecipients = append(recipients, r)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Verify and follow addresses in other organizational domain through\n\t\t// <policydomain>._report._dmarc.<host> lookup.\n\t\t// ../rfc/7489:1556\n\t\taccepts, status, records, _, _, err := dmarc.LookupExternalReportsAccepted(ctx, log.Logger, resolver, evalOrgDom, r.address.Domain)\n\t\tlog.Debugx(\"checking if rua address with different organization domain has opted into receiving dmarc reports\", err,\n\t\t\tslog.Any(\"policydomain\", evalOrgDom),\n\t\t\tslog.Any(\"destinationdomain\", r.address.Domain),\n\t\t\tslog.Bool(\"accepts\", accepts),\n\t\t\tslog.Any(\"status\", status))\n\t\tif status == dmarc.StatusTemperror {\n\t\t\t// With a temporary error, we'll try to get the report the delivered anyway,\n\t\t\t// perhaps there are multiple recipients.\n\t\t\t// ../rfc/7489:1578\n\t\t\ttempError = true\n\t\t\terrors = append(errors, \"temporary error checking authorization for report delegation to external address\")\n\t\t}\n\t\tif !accepts {\n\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain that does not opt-in to receiving dmarc records through _report dmarc record\", r.address))\n\t\t\tcontinue\n\t\t}\n\n\t\t// We can follow a _report DMARC DNS record once. In that record, a domain may\n\t\t// specify alternative addresses that we should send reports to instead. Such\n\t\t// alternative address(es) must have the same host. If not, we ignore the new\n\t\t// value. Behaviour for multiple records and/or multiple new addresses is\n\t\t// underspecified. We'll replace an address with one or more new addresses, and\n\t\t// keep the original if there was no candidate (which covers the case of invalid\n\t\t// alternative addresses and no new address specified).\n\t\t// ../rfc/7489:1600\n\t\tfoundReplacement := false\n\t\trlog := log.With(slog.Any(\"followedaddress\", uri.Address))\n\t\tfor _, record := range records {\n\t\t\tfor _, exturi := range record.AggregateReportAddresses {\n\t\t\t\textr, ok := parseRecipient(rlog, exturi)\n\t\t\t\tif !ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif extr.address.Domain != r.address.Domain {\n\t\t\t\t\trlog.Debug(\"rua address in external _report dmarc record has different host than initial dmarc record, ignoring new name\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain with a replacement address %s with different host\", r.address, extr.address))\n\t\t\t\t} else {\n\t\t\t\t\trlog.Debug(\"using replacement rua address from external _report dmarc record\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\tfoundReplacement = true\n\t\t\t\t\trecipients = append(recipients, extr)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif !foundReplacement {\n\t\t\trecipients = append(recipients, r)\n\t\t}\n\t}\n\n\tif len(recipients) == 0 {\n\t\t// No reports requested, perfectly fine, no work to do for us.\n\t\tlog.Debug(\"no aggregate reporting addresses configured\")\n\t\treturn true, nil\n\t}\n\n\t// We count idential records. Can be common with a domain sending quite some email.\n\t// Though less if the sending domain has many IPs. In the future, we may want to\n\t// remove some details from records so we can aggregate them into fewer rows.\n\ttype recordCount struct {\n\t\tdmarcrpt.ReportRecord\n\t\tcount int\n\t}\n\tcounts := map[string]recordCount{}\n\n\n```\n# Function Signature: func EvaluationStats(ctx context.Context) (map[string]EvaluationStat, error) {\n# Natural Language Annotations: // EvaluationStats returns evaluation counts and report-sending status per domain.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc EvaluationStats(ctx context.Context) (map[string]EvaluationStat, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tdomains := map[string]EvaluationStat{}\n\terr := q.ForEach(func(e Evaluation) error {\n\t\tif e.Optional && e.Evaluated.After(nextWholeHour(time.Now())) {\n\t\t\treturn nil\n\t\t}\n\t\tds := domains[e.Domain]\n\t\tds.Count++\n\t\tif e.Disposition != \"\" {\n\t\t\tds.Dispositions = append(ds.Dispositions, string(e.Disposition))\n\t\t}\n\t\tif !ds.SendReport && !e.Optional {\n\t\t\tds.SendReport = true\n\t\t}\n\t\tdomains[e.Domain] = ds\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"analyzing dmarc evaluations: %v\", err)\n\t}\n\treturn domains, nil\n}\n```", "  ```\nfunc EvaluationStats(ctx context.Context) (map[string]EvaluationStat, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.SortAsc(\"Evaluated\")\n\tevals, err := q.List()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tstats := make(map[string]EvaluationStat)\n\tfor _, eval := range evals {\n\t\tdom := eval.PolicyPublished.Domain\n\t\tstat, ok := stats[dom]\n\t\tif !ok {\n\t\t\tstat = EvaluationStat{\n\t\t\t\tDomain: dns.MustParseDomain(dom),\n\t\t\t}\n\t\t}\n\t\tstat.Count++\n\t\tif !eval.Disposition.Pass {\n\t\t\tstat.Dispositions = append(stat.Dispositions, string(eval.Disposition))\n\t\t}\n\t\tif !eval.AlignedDKIMPass {\n\t\t\tstat.Dispositions = append(stat.Dispositions, \"dkim a\")\n\t\t}\n\t\tif !eval.AlignedSPFPass {\n\t\t\tstat.Dispositions = append(stat.Dispositions, \"spf a\")\n\t\t}\n\t\tif eval.AlignedDKIMPass && eval.AlignedSPFPass {\n\t\t\tstat.Dispositions = append(stat.Dispositions, \"pass\")\n\t\t}\n\t\tstats[dom] = stat\n\t}\n\n\treturn stats, nil\n}\n```", "  ```\nfunc EvaluationStats(ctx context.Context) (map[string]EvaluationStat, error) {\n\tstats := map[string]EvaluationStat{}\n\n\t// Get all evaluations from the database.\n\tevaluations, err := Evaluations(ctx)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Group evaluations by domain and evaluate each group.\n\tfor _, e := range evaluations {\n\t\tvar es EvaluationStat\n\t\tswitch e.PolicyDomain {\n\t\tcase \"\":\n\t\t\tes.Domain = \"org. missing\"\n\t\tdefault:\n\t\t\tes.Domain = e.PolicyDomain\n\t\t}\n\n\t\tif !e.Optional {\n\t\t\tes.SendReport = true\n\t\t}\n\n\t\tes.Count++\n\n\t\td := string(es.Dispositions)\n\t\tif d != \"\" {\n\t\t\td = d + \" \"\n\t\t}\n\t\td += dmarcrpt.DispositionMap[e.Disposition]\n\t\tes.Dispositions = d\n\n\t\tstats[es.Domain] = es\n\t}\n\n\treturn stats, nil\n}\n```"]}, "mox-dmarcdb/eval.go-EvaluationsDomain": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dmarcdb/eval.go:\n```\npackage dmarcdb\n\n// Sending TLS reports and DMARC reports is very similar. See ../dmarcdb/eval.go:/similar and ../tlsrptsend/send.go:/similar.\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"mime/multipart\"\n\t\"net/textproto\"\n\t\"net/url\"\n\t\"os\"\n\t\"runtime/debug\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/exp/maps\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/dkim\"\n\t\"github.com/mjl-/mox/dmarc\"\n\t\"github.com/mjl-/mox/dmarcrpt\"\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/metrics\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/moxvar\"\n\t\"github.com/mjl-/mox/publicsuffix\"\n\t\"github.com/mjl-/mox/queue\"\n\t\"github.com/mjl-/mox/smtp\"\n\t\"github.com/mjl-/mox/store\"\n)\n\nvar (\n\tmetricReport = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_queued_total\",\n\t\t\tHelp: \"Total messages with DMARC aggregate/error reports queued.\",\n\t\t},\n\t)\n\tmetricReportError = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_error_total\",\n\t\t\tHelp: \"Total errors while composing or queueing DMARC aggregate/error reports.\",\n\t\t},\n\t)\n)\n\nvar (\n\tEvalDBTypes = []any{Evaluation{}, SuppressAddress{}} // Types stored in DB.\n\t// Exported for backups. For incoming deliveries the SMTP server adds evaluations\n\t// to the database. Every hour, a goroutine wakes up that gathers evaluations from\n\t// the last hour(s), sends a report, and removes the evaluations from the database.\n\tEvalDB *bstore.DB\n)\n\n// Evaluation is the result of an evaluation of a DMARC policy, to be included\n// in a DMARC report.\ntype Evaluation struct {\n\tID int64\n\n\t// Domain where DMARC policy was found, could be the organizational domain while\n\t// evaluation was for a subdomain. Unicode. Same as domain found in\n\t// PolicyPublished. A separate field for its index.\n\tPolicyDomain string `bstore:\"index\"`\n\n\t// Time of evaluation, determines which report (covering whole hours) this\n\t// evaluation will be included in.\n\tEvaluated time.Time `bstore:\"default now\"`\n\n\t// If optional, this evaluation is not a reason to send a DMARC report, but it will\n\t// be included when a report is sent due to other non-optional evaluations. Set for\n\t// evaluations of incoming DMARC reports. We don't want such deliveries causing us to\n\t// send a report, or we would keep exchanging reporting messages forever. Also set\n\t// for when evaluation is a DMARC reject for domains we haven't positively\n\t// interacted with, to prevent being used to flood an unsuspecting domain with\n\t// reports.\n\tOptional bool\n\n\t// Effective aggregate reporting interval in hours. Between 1 and 24, rounded up\n\t// from seconds from policy to first number that can divide 24.\n\tIntervalHours int\n\n\t// \"rua\" in DMARC record, we only store evaluations for records with aggregate reporting addresses, so always non-empty.\n\tAddresses []string\n\n\t// Policy used for evaluation. We don't store the \"fo\" field for failure reporting\n\t// options, since we don't send failure reports for individual messages.\n\tPolicyPublished dmarcrpt.PolicyPublished\n\n\t// For \"row\" in a report record.\n\tSourceIP        string\n\tDisposition     dmarcrpt.Disposition\n\tAlignedDKIMPass bool\n\tAlignedSPFPass  bool\n\tOverrideReasons []dmarcrpt.PolicyOverrideReason\n\n\t// For \"identifiers\" in a report record.\n\tEnvelopeTo   string\n\tEnvelopeFrom string\n\tHeaderFrom   string\n\n\t// For \"auth_results\" in a report record.\n\tDKIMResults []dmarcrpt.DKIMAuthResult\n\tSPFResults  []dmarcrpt.SPFAuthResult\n}\n\n// SuppressAddress is a reporting address for which outgoing DMARC reports\n// will be suppressed for a period.\ntype SuppressAddress struct {\n\tID               int64\n\tInserted         time.Time `bstore:\"default now\"`\n\tReportingAddress string    `bstore:\"unique\"`\n\tUntil            time.Time `bstore:\"nonzero\"`\n\tComment          string\n}\n\nvar dmarcResults = map[bool]dmarcrpt.DMARCResult{\n\tfalse: dmarcrpt.DMARCFail,\n\ttrue:  dmarcrpt.DMARCPass,\n}\n\n// ReportRecord turns an evaluation into a record that can be included in a\n// report.\nfunc (e Evaluation) ReportRecord(count int) dmarcrpt.ReportRecord {\n\treturn dmarcrpt.ReportRecord{\n\t\tRow: dmarcrpt.Row{\n\t\t\tSourceIP: e.SourceIP,\n\t\t\tCount:    count,\n\t\t\tPolicyEvaluated: dmarcrpt.PolicyEvaluated{\n\t\t\t\tDisposition: e.Disposition,\n\t\t\t\tDKIM:        dmarcResults[e.AlignedDKIMPass],\n\t\t\t\tSPF:         dmarcResults[e.AlignedSPFPass],\n\t\t\t\tReasons:     e.OverrideReasons,\n\t\t\t},\n\t\t},\n\t\tIdentifiers: dmarcrpt.Identifiers{\n\t\t\tEnvelopeTo:   e.EnvelopeTo,\n\t\t\tEnvelopeFrom: e.EnvelopeFrom,\n\t\t\tHeaderFrom:   e.HeaderFrom,\n\t\t},\n\t\tAuthResults: dmarcrpt.AuthResults{\n\t\t\tDKIM: e.DKIMResults,\n\t\t\tSPF:  e.SPFResults,\n\t\t},\n\t}\n}\n\nvar intervalOpts = []int{24, 12, 8, 6, 4, 3, 2}\n\nfunc intervalHours(seconds int) int {\n\thours := (seconds + 3600 - 1) / 3600\n\tfor _, opt := range intervalOpts {\n\t\tif hours >= opt {\n\t\t\treturn opt\n\t\t}\n\t}\n\treturn 1\n}\n\n// AddEvaluation adds the result of a DMARC evaluation for an incoming message\n// to the database.\n//\n// AddEvaluation sets Evaluation.IntervalHours based on\n// aggregateReportingIntervalSeconds.\n\n\n\n\n\n\n\n// Evaluations returns all evaluations in the database.\nfunc Evaluations(ctx context.Context) ([]Evaluation, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.SortAsc(\"Evaluated\")\n\treturn q.List()\n}\n\n// EvaluationStat summarizes stored evaluations, for inclusion in an upcoming\n// aggregate report, for a domain.\ntype EvaluationStat struct {\n\tDomain       dns.Domain\n\tDispositions []string\n\tCount        int\n\tSendReport   bool\n}\n\n// EvaluationStats returns evaluation counts and report-sending status per domain.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// EvaluationsDomain returns all evaluations for a domain.\n\n\n\n\n\n\n\n// RemoveEvaluationsDomain removes evaluations for domain so they won't be sent in\n// an aggregate report.\n\n\n\n\n\n\n\nvar jitterRand = mox.NewPseudoRand()\n\n// time to sleep until next whole hour t, replaced by tests.\n// Jitter so we don't cause load at exactly whole hours, other processes may\n// already be doing that.\nvar jitteredTimeUntil = func(t time.Time) time.Duration {\n\treturn time.Until(t.Add(time.Duration(30+jitterRand.Intn(60)) * time.Second))\n}\n\n// Start launches a goroutine that wakes up at each whole hour (plus jitter) and\n// sends DMARC reports to domains that requested them.\nfunc Start(resolver dns.Resolver) {\n\tgo func() {\n\t\tlog := mlog.New(\"dmarcdb\", nil)\n\n\t\tdefer func() {\n\t\t\t// In case of panic don't take the whole program down.\n\t\t\tx := recover()\n\t\t\tif x != nil {\n\t\t\t\tlog.Error(\"recover from panic\", slog.Any(\"panic\", x))\n\t\t\t\tdebug.PrintStack()\n\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t}\n\t\t}()\n\n\t\ttimer := time.NewTimer(time.Hour)\n\t\tdefer timer.Stop()\n\n\t\tctx := mox.Shutdown\n\n\t\tfor {\n\t\t\tnow := time.Now()\n\t\t\tnextEnd := nextWholeHour(now)\n\t\t\ttimer.Reset(jitteredTimeUntil(nextEnd))\n\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tlog.Info(\"dmarc aggregate report sender shutting down\")\n\t\t\t\treturn\n\t\t\tcase <-timer.C:\n\t\t\t}\n\n\t\t\t// Gather report intervals we want to process now. Multiples of hours that can\n\t\t\t// divide 24, starting from UTC.\n\t\t\t// ../rfc/7489:1750\n\t\t\tutchour := nextEnd.UTC().Hour()\n\t\t\tif utchour == 0 {\n\t\t\t\tutchour = 24\n\t\t\t}\n\t\t\tintervals := []int{}\n\t\t\tfor _, ival := range intervalOpts {\n\t\t\t\tif ival*(utchour/ival) == utchour {\n\t\t\t\t\tintervals = append(intervals, ival)\n\t\t\t\t}\n\t\t\t}\n\t\t\tintervals = append(intervals, 1)\n\n\t\t\t// Remove evaluations older than 48 hours (2 reports with the default and maximum\n\t\t\t// 24 hour interval). They should have been processed by now. We may have kept them\n\t\t\t// during temporary errors, but persistent temporary errors shouldn't fill up our\n\t\t\t// database. This also cleans up evaluations that were all optional for a domain.\n\t\t\t_, err := bstore.QueryDB[Evaluation](ctx, EvalDB).FilterLess(\"Evaluated\", nextEnd.Add(-48*time.Hour)).Delete()\n\t\t\tlog.Check(err, \"removing stale dmarc evaluations from database\")\n\n\t\t\tclog := log.WithCid(mox.Cid())\n\t\t\tclog.Info(\"sending dmarc aggregate reports\", slog.Time(\"end\", nextEnd.UTC()), slog.Any(\"intervals\", intervals))\n\t\t\tif err := sendReports(ctx, clog, resolver, EvalDB, nextEnd, intervals); err != nil {\n\t\t\t\tclog.Errorx(\"sending dmarc aggregate reports\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t} else {\n\t\t\t\tclog.Info(\"finished sending dmarc aggregate reports\")\n\t\t\t}\n\t\t}\n\t}()\n}\n\nfunc nextWholeHour(now time.Time) time.Time {\n\tt := now\n\tt = t.Add(time.Hour)\n\treturn time.Date(t.Year(), t.Month(), t.Day(), t.Hour(), 0, 0, 0, t.Location())\n}\n\n// We don't send reports at full speed. In the future, we could try to stretch out\n// reports a bit smarter. E.g. over 5 minutes with some minimum interval, and\n// perhaps faster and in parallel when there are lots of reports. Perhaps also\n// depending on reporting interval (faster for 1h, slower for 24h).\n// Replaced by tests.\nvar sleepBetween = func(ctx context.Context, between time.Duration) (ok bool) {\n\tt := time.NewTimer(between)\n\tselect {\n\tcase <-ctx.Done():\n\t\tt.Stop()\n\t\treturn false\n\tcase <-t.C:\n\t\treturn true\n\t}\n}\n\n// sendReports gathers all policy domains that have evaluations that should\n// receive a DMARC report and sends a report to each.\nfunc sendReports(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, intervals []int) error {\n\tivals := make([]any, len(intervals))\n\tfor i, v := range intervals {\n\t\tivals[i] = v\n\t}\n\n\tdestDomains := map[string]bool{}\n\n\t// Gather all domains that we plan to send to.\n\tnsend := 0\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterEqual(\"IntervalHours\", ivals...)\n\terr := q.ForEach(func(e Evaluation) error {\n\t\tif !e.Optional && !destDomains[e.PolicyPublished.Domain] {\n\t\t\tnsend++\n\t\t}\n\t\tdestDomains[e.PolicyPublished.Domain] = destDomains[e.PolicyPublished.Domain] || !e.Optional\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"looking for domains to send reports to: %v\", err)\n\t}\n\n\tvar wg sync.WaitGroup\n\n\t// Sleep in between sending reports. We process hourly, and spread the reports over\n\t// the hour, but with max 5 minute interval.\n\tbetween := 45 * time.Minute\n\tif nsend > 0 {\n\t\tbetween /= time.Duration(nsend)\n\t}\n\tif between > 5*time.Minute {\n\t\tbetween = 5 * time.Minute\n\t}\n\n\t// Attempt to send report to each domain.\n\tn := 0\n\tfor d, send := range destDomains {\n\t\t// Cleanup evaluations for domain with only optionals.\n\t\tif !send {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, d)\n\t\t\tcontinue\n\t\t}\n\n\t\tif n > 0 {\n\t\t\tif ok := sleepBetween(ctx, between); !ok {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t\tn++\n\n\t\t// Send in goroutine, so a slow process doesn't block progress.\n\t\twg.Add(1)\n\t\tgo func(domain string) {\n\t\t\tdefer func() {\n\t\t\t\t// In case of panic don't take the whole program down.\n\t\t\t\tx := recover()\n\t\t\t\tif x != nil {\n\t\t\t\t\tlog.Error(\"unhandled panic in dmarcdb sendReports\", slog.Any(\"panic\", x))\n\t\t\t\t\tdebug.PrintStack()\n\t\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t\t}\n\t\t\t}()\n\t\t\tdefer wg.Done()\n\n\t\t\trlog := log.WithCid(mox.Cid()).With(slog.Any(\"domain\", domain))\n\t\t\trlog.Info(\"sending dmarc report\")\n\t\t\tif _, err := sendReportDomain(ctx, rlog, resolver, db, endTime, domain); err != nil {\n\t\t\t\trlog.Errorx(\"sending dmarc aggregate report to domain\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t}\n\t\t}(d)\n\t}\n\n\twg.Wait()\n\n\treturn nil\n}\n\ntype recipient struct {\n\taddress smtp.Address\n\tmaxSize uint64\n}\n\nfunc parseRecipient(log mlog.Log, uri dmarc.URI) (r recipient, ok bool) {\n\tlog = log.With(slog.Any(\"uri\", uri.Address))\n\n\tu, err := url.Parse(uri.Address)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\tif !strings.EqualFold(u.Scheme, \"mailto\") {\n\t\tlog.Debug(\"skipping unrecognized scheme in dmarc record rua value\")\n\t\treturn r, false\n\t}\n\taddr, err := smtp.ParseAddress(u.Opaque)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing mailto uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\n\tr = recipient{addr, uri.MaxSize}\n\t// ../rfc/7489:1197\n\tswitch uri.Unit {\n\tcase \"k\", \"K\":\n\t\tr.maxSize *= 1024\n\tcase \"m\", \"M\":\n\t\tr.maxSize *= 1024 * 1024\n\tcase \"g\", \"G\":\n\t\tr.maxSize *= 1024 * 1024 * 1024\n\tcase \"t\", \"T\":\n\t\t// Oh yeah, terabyte-sized reports!\n\t\tr.maxSize *= 1024 * 1024 * 1024 * 1024\n\tcase \"\":\n\tdefault:\n\t\tlog.Debug(\"unrecognized max size unit in dmarc record rua value\", slog.String(\"unit\", uri.Unit))\n\t\treturn r, false\n\t}\n\n\treturn r, true\n}\n\nfunc removeEvaluations(ctx context.Context, log mlog.Log, db *bstore.DB, endTime time.Time, domain string) {\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterNonzero(Evaluation{PolicyDomain: domain})\n\t_, err := q.Delete()\n\tlog.Check(err, \"removing evaluations after processing for dmarc aggregate report\")\n}\n\n// replaceable for testing.\nvar queueAdd = queue.Add\n\nfunc sendReportDomain(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, domain string) (cleanup bool, rerr error) {\n\tdom, err := dns.ParseDomain(domain)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"parsing domain for sending reports: %v\", err)\n\t}\n\n\t// We'll cleanup records by default.\n\tcleanup = true\n\t// If we encounter a temporary error we cancel cleanup of evaluations on error.\n\ttempError := false\n\n\tdefer func() {\n\t\tif !cleanup || tempError {\n\t\t\tlog.Debug(\"not cleaning up evaluations after attempting to send dmarc aggregate report\")\n\t\t} else {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, domain)\n\t\t}\n\t}()\n\n\t// We're going to build up this report.\n\treport := dmarcrpt.Feedback{\n\t\tVersion: \"1.0\",\n\t\tReportMetadata: dmarcrpt.ReportMetadata{\n\t\t\tOrgName: mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\tEmail:   \"postmaster@\" + mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\t// ReportID and DateRange are set after we've seen evaluations.\n\t\t\t// Errors is filled below when we encounter problems.\n\t\t},\n\t\t// We'll fill the records below.\n\t\tRecords: []dmarcrpt.ReportRecord{},\n\t}\n\n\tvar errors []string // For report.ReportMetaData.Errors\n\n\t// Check if we should be sending a report at all: if there are rua URIs in the\n\t// current DMARC record. The interval may have changed too, but we'll flush out our\n\t// evaluations regardless. We always use the latest DMARC record when sending, but\n\t// we'll lump all policies of the last interval into one report.\n\t// ../rfc/7489:1714\n\tstatus, _, record, _, _, err := dmarc.Lookup(ctx, log.Logger, resolver, dom)\n\tif err != nil {\n\t\t// todo future: we could perhaps still send this report, assuming the values we know. in case of temporary error, we could also schedule again regardless of next interval hour (we would now only retry a 24h-interval report after 24h passed).\n\t\t// Remove records unless it was a temporary error. We'll try again next round.\n\t\tcleanup = status != dmarc.StatusTemperror\n\t\treturn cleanup, fmt.Errorf(\"looking up current dmarc record for reporting address: %v\", err)\n\t}\n\n\tvar recipients []recipient\n\n\t// Gather all aggregate reporting addresses to try to send to. We'll start with\n\t// those in the initial DMARC record, but will follow external reporting addresses\n\t// and possibly update the list.\n\tfor _, uri := range record.AggregateReportAddresses {\n\t\tr, ok := parseRecipient(log, uri)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Check if domain of rua recipient has the same organizational domain as for the\n\t\t// evaluations. If not, we need to verify we are allowed to send.\n\t\truaOrgDom := publicsuffix.Lookup(ctx, log.Logger, r.address.Domain)\n\t\tevalOrgDom := publicsuffix.Lookup(ctx, log.Logger, dom)\n\n\t\tif ruaOrgDom == evalOrgDom {\n\t\t\trecipients = append(recipients, r)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Verify and follow addresses in other organizational domain through\n\t\t// <policydomain>._report._dmarc.<host> lookup.\n\t\t// ../rfc/7489:1556\n\t\taccepts, status, records, _, _, err := dmarc.LookupExternalReportsAccepted(ctx, log.Logger, resolver, evalOrgDom, r.address.Domain)\n\t\tlog.Debugx(\"checking if rua address with different organization domain has opted into receiving dmarc reports\", err,\n\t\t\tslog.Any(\"policydomain\", evalOrgDom),\n\t\t\tslog.Any(\"destinationdomain\", r.address.Domain),\n\t\t\tslog.Bool(\"accepts\", accepts),\n\t\t\tslog.Any(\"status\", status))\n\t\tif status == dmarc.StatusTemperror {\n\t\t\t// With a temporary error, we'll try to get the report the delivered anyway,\n\t\t\t// perhaps there are multiple recipients.\n\t\t\t// ../rfc/7489:1578\n\t\t\ttempError = true\n\t\t\terrors = append(errors, \"temporary error checking authorization for report delegation to external address\")\n\t\t}\n\t\tif !accepts {\n\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain that does not opt-in to receiving dmarc records through _report dmarc record\", r.address))\n\t\t\tcontinue\n\t\t}\n\n\t\t// We can follow a _report DMARC DNS record once. In that record, a domain may\n\t\t// specify alternative addresses that we should send reports to instead. Such\n\t\t// alternative address(es) must have the same host. If not, we ignore the new\n\t\t// value. Behaviour for multiple records and/or multiple new addresses is\n\t\t// underspecified. We'll replace an address with one or more new addresses, and\n\t\t// keep the original if there was no candidate (which covers the case of invalid\n\t\t// alternative addresses and no new address specified).\n\t\t// ../rfc/7489:1600\n\t\tfoundReplacement := false\n\t\trlog := log.With(slog.Any(\"followedaddress\", uri.Address))\n\t\tfor _, record := range records {\n\t\t\tfor _, exturi := range record.AggregateReportAddresses {\n\t\t\t\textr, ok := parseRecipient(rlog, exturi)\n\t\t\t\tif !ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif extr.address.Domain != r.address.Domain {\n\t\t\t\t\trlog.Debug(\"rua address in external _report dmarc record has different host than initial dmarc record, ignoring new name\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain with a replacement address %s with different host\", r.address, extr.address))\n\t\t\t\t} else {\n\t\t\t\t\trlog.Debug(\"using replacement rua address from external _report dmarc record\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\tfoundReplacement = true\n\t\t\t\t\trecipients = append(recipients, extr)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif !foundReplacement {\n\t\t\trecipients = append(recipients, r)\n\t\t}\n\t}\n\n\tif len(recipients) == 0 {\n\t\t// No reports requested, perfectly fine, no work to do for us.\n\t\tlog.Debug(\"no aggregate reporting addresses configured\")\n\t\treturn true, nil\n\t}\n\n\t// We count idential records. Can be common with a domain sending quite some email.\n\t// Though less if the sending domain has many IPs. In the future, we may want to\n\t// remove some details from records so we can aggregate them into fewer rows.\n\ttype recordCount struct {\n\t\tdmarcrpt.ReportRecord\n\t\tcount int\n\t}\n\tcounts := map[string]recordCount{}\n\n\n```\n# Function Signature: func EvaluationsDomain(ctx context.Context, domain dns.Domain) ([]Evaluation, error) {\n# Natural Language Annotations: // EvaluationsDomain returns all evaluations for a domain.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc EvaluationsDomain(ctx context.Context, domain dns.Domain) ([]Evaluation, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.FilterEqual(\"PolicyDomain\", string(domain))\n\tq.SortAsc(\"Evaluated\")\n\treturn q.List()\n}\n```", "  ```\nfunc EvaluationsDomain(ctx context.Context, domain dns.Domain) ([]Evaluation, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.SortAsc(\"Evaluated\")\n\tq.FilterEqual(\"PolicyDomain\", string(domain))\n\treturn q.List()\n}\n```", "  ```\nfunc EvaluationsDomain(ctx context.Context, domain dns.Domain) ([]Evaluation, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.FilterEqual(Evaluation{\"PolicyDomain\": domain})\n\treturn q.List()\n}\n```"]}, "mox-dmarcdb/eval.go-RemoveEvaluationsDomain": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dmarcdb/eval.go:\n```\npackage dmarcdb\n\n// Sending TLS reports and DMARC reports is very similar. See ../dmarcdb/eval.go:/similar and ../tlsrptsend/send.go:/similar.\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"mime/multipart\"\n\t\"net/textproto\"\n\t\"net/url\"\n\t\"os\"\n\t\"runtime/debug\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/exp/maps\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/dkim\"\n\t\"github.com/mjl-/mox/dmarc\"\n\t\"github.com/mjl-/mox/dmarcrpt\"\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/metrics\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/moxvar\"\n\t\"github.com/mjl-/mox/publicsuffix\"\n\t\"github.com/mjl-/mox/queue\"\n\t\"github.com/mjl-/mox/smtp\"\n\t\"github.com/mjl-/mox/store\"\n)\n\nvar (\n\tmetricReport = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_queued_total\",\n\t\t\tHelp: \"Total messages with DMARC aggregate/error reports queued.\",\n\t\t},\n\t)\n\tmetricReportError = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_error_total\",\n\t\t\tHelp: \"Total errors while composing or queueing DMARC aggregate/error reports.\",\n\t\t},\n\t)\n)\n\nvar (\n\tEvalDBTypes = []any{Evaluation{}, SuppressAddress{}} // Types stored in DB.\n\t// Exported for backups. For incoming deliveries the SMTP server adds evaluations\n\t// to the database. Every hour, a goroutine wakes up that gathers evaluations from\n\t// the last hour(s), sends a report, and removes the evaluations from the database.\n\tEvalDB *bstore.DB\n)\n\n// Evaluation is the result of an evaluation of a DMARC policy, to be included\n// in a DMARC report.\ntype Evaluation struct {\n\tID int64\n\n\t// Domain where DMARC policy was found, could be the organizational domain while\n\t// evaluation was for a subdomain. Unicode. Same as domain found in\n\t// PolicyPublished. A separate field for its index.\n\tPolicyDomain string `bstore:\"index\"`\n\n\t// Time of evaluation, determines which report (covering whole hours) this\n\t// evaluation will be included in.\n\tEvaluated time.Time `bstore:\"default now\"`\n\n\t// If optional, this evaluation is not a reason to send a DMARC report, but it will\n\t// be included when a report is sent due to other non-optional evaluations. Set for\n\t// evaluations of incoming DMARC reports. We don't want such deliveries causing us to\n\t// send a report, or we would keep exchanging reporting messages forever. Also set\n\t// for when evaluation is a DMARC reject for domains we haven't positively\n\t// interacted with, to prevent being used to flood an unsuspecting domain with\n\t// reports.\n\tOptional bool\n\n\t// Effective aggregate reporting interval in hours. Between 1 and 24, rounded up\n\t// from seconds from policy to first number that can divide 24.\n\tIntervalHours int\n\n\t// \"rua\" in DMARC record, we only store evaluations for records with aggregate reporting addresses, so always non-empty.\n\tAddresses []string\n\n\t// Policy used for evaluation. We don't store the \"fo\" field for failure reporting\n\t// options, since we don't send failure reports for individual messages.\n\tPolicyPublished dmarcrpt.PolicyPublished\n\n\t// For \"row\" in a report record.\n\tSourceIP        string\n\tDisposition     dmarcrpt.Disposition\n\tAlignedDKIMPass bool\n\tAlignedSPFPass  bool\n\tOverrideReasons []dmarcrpt.PolicyOverrideReason\n\n\t// For \"identifiers\" in a report record.\n\tEnvelopeTo   string\n\tEnvelopeFrom string\n\tHeaderFrom   string\n\n\t// For \"auth_results\" in a report record.\n\tDKIMResults []dmarcrpt.DKIMAuthResult\n\tSPFResults  []dmarcrpt.SPFAuthResult\n}\n\n// SuppressAddress is a reporting address for which outgoing DMARC reports\n// will be suppressed for a period.\ntype SuppressAddress struct {\n\tID               int64\n\tInserted         time.Time `bstore:\"default now\"`\n\tReportingAddress string    `bstore:\"unique\"`\n\tUntil            time.Time `bstore:\"nonzero\"`\n\tComment          string\n}\n\nvar dmarcResults = map[bool]dmarcrpt.DMARCResult{\n\tfalse: dmarcrpt.DMARCFail,\n\ttrue:  dmarcrpt.DMARCPass,\n}\n\n// ReportRecord turns an evaluation into a record that can be included in a\n// report.\nfunc (e Evaluation) ReportRecord(count int) dmarcrpt.ReportRecord {\n\treturn dmarcrpt.ReportRecord{\n\t\tRow: dmarcrpt.Row{\n\t\t\tSourceIP: e.SourceIP,\n\t\t\tCount:    count,\n\t\t\tPolicyEvaluated: dmarcrpt.PolicyEvaluated{\n\t\t\t\tDisposition: e.Disposition,\n\t\t\t\tDKIM:        dmarcResults[e.AlignedDKIMPass],\n\t\t\t\tSPF:         dmarcResults[e.AlignedSPFPass],\n\t\t\t\tReasons:     e.OverrideReasons,\n\t\t\t},\n\t\t},\n\t\tIdentifiers: dmarcrpt.Identifiers{\n\t\t\tEnvelopeTo:   e.EnvelopeTo,\n\t\t\tEnvelopeFrom: e.EnvelopeFrom,\n\t\t\tHeaderFrom:   e.HeaderFrom,\n\t\t},\n\t\tAuthResults: dmarcrpt.AuthResults{\n\t\t\tDKIM: e.DKIMResults,\n\t\t\tSPF:  e.SPFResults,\n\t\t},\n\t}\n}\n\nvar intervalOpts = []int{24, 12, 8, 6, 4, 3, 2}\n\nfunc intervalHours(seconds int) int {\n\thours := (seconds + 3600 - 1) / 3600\n\tfor _, opt := range intervalOpts {\n\t\tif hours >= opt {\n\t\t\treturn opt\n\t\t}\n\t}\n\treturn 1\n}\n\n// AddEvaluation adds the result of a DMARC evaluation for an incoming message\n// to the database.\n//\n// AddEvaluation sets Evaluation.IntervalHours based on\n// aggregateReportingIntervalSeconds.\n\n\n\n\n\n\n\n// Evaluations returns all evaluations in the database.\nfunc Evaluations(ctx context.Context) ([]Evaluation, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.SortAsc(\"Evaluated\")\n\treturn q.List()\n}\n\n// EvaluationStat summarizes stored evaluations, for inclusion in an upcoming\n// aggregate report, for a domain.\ntype EvaluationStat struct {\n\tDomain       dns.Domain\n\tDispositions []string\n\tCount        int\n\tSendReport   bool\n}\n\n// EvaluationStats returns evaluation counts and report-sending status per domain.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// EvaluationsDomain returns all evaluations for a domain.\n\n\n\n\n\n\n\n// RemoveEvaluationsDomain removes evaluations for domain so they won't be sent in\n// an aggregate report.\n\n\n\n\n\n\n\nvar jitterRand = mox.NewPseudoRand()\n\n// time to sleep until next whole hour t, replaced by tests.\n// Jitter so we don't cause load at exactly whole hours, other processes may\n// already be doing that.\nvar jitteredTimeUntil = func(t time.Time) time.Duration {\n\treturn time.Until(t.Add(time.Duration(30+jitterRand.Intn(60)) * time.Second))\n}\n\n// Start launches a goroutine that wakes up at each whole hour (plus jitter) and\n// sends DMARC reports to domains that requested them.\nfunc Start(resolver dns.Resolver) {\n\tgo func() {\n\t\tlog := mlog.New(\"dmarcdb\", nil)\n\n\t\tdefer func() {\n\t\t\t// In case of panic don't take the whole program down.\n\t\t\tx := recover()\n\t\t\tif x != nil {\n\t\t\t\tlog.Error(\"recover from panic\", slog.Any(\"panic\", x))\n\t\t\t\tdebug.PrintStack()\n\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t}\n\t\t}()\n\n\t\ttimer := time.NewTimer(time.Hour)\n\t\tdefer timer.Stop()\n\n\t\tctx := mox.Shutdown\n\n\t\tfor {\n\t\t\tnow := time.Now()\n\t\t\tnextEnd := nextWholeHour(now)\n\t\t\ttimer.Reset(jitteredTimeUntil(nextEnd))\n\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tlog.Info(\"dmarc aggregate report sender shutting down\")\n\t\t\t\treturn\n\t\t\tcase <-timer.C:\n\t\t\t}\n\n\t\t\t// Gather report intervals we want to process now. Multiples of hours that can\n\t\t\t// divide 24, starting from UTC.\n\t\t\t// ../rfc/7489:1750\n\t\t\tutchour := nextEnd.UTC().Hour()\n\t\t\tif utchour == 0 {\n\t\t\t\tutchour = 24\n\t\t\t}\n\t\t\tintervals := []int{}\n\t\t\tfor _, ival := range intervalOpts {\n\t\t\t\tif ival*(utchour/ival) == utchour {\n\t\t\t\t\tintervals = append(intervals, ival)\n\t\t\t\t}\n\t\t\t}\n\t\t\tintervals = append(intervals, 1)\n\n\t\t\t// Remove evaluations older than 48 hours (2 reports with the default and maximum\n\t\t\t// 24 hour interval). They should have been processed by now. We may have kept them\n\t\t\t// during temporary errors, but persistent temporary errors shouldn't fill up our\n\t\t\t// database. This also cleans up evaluations that were all optional for a domain.\n\t\t\t_, err := bstore.QueryDB[Evaluation](ctx, EvalDB).FilterLess(\"Evaluated\", nextEnd.Add(-48*time.Hour)).Delete()\n\t\t\tlog.Check(err, \"removing stale dmarc evaluations from database\")\n\n\t\t\tclog := log.WithCid(mox.Cid())\n\t\t\tclog.Info(\"sending dmarc aggregate reports\", slog.Time(\"end\", nextEnd.UTC()), slog.Any(\"intervals\", intervals))\n\t\t\tif err := sendReports(ctx, clog, resolver, EvalDB, nextEnd, intervals); err != nil {\n\t\t\t\tclog.Errorx(\"sending dmarc aggregate reports\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t} else {\n\t\t\t\tclog.Info(\"finished sending dmarc aggregate reports\")\n\t\t\t}\n\t\t}\n\t}()\n}\n\nfunc nextWholeHour(now time.Time) time.Time {\n\tt := now\n\tt = t.Add(time.Hour)\n\treturn time.Date(t.Year(), t.Month(), t.Day(), t.Hour(), 0, 0, 0, t.Location())\n}\n\n// We don't send reports at full speed. In the future, we could try to stretch out\n// reports a bit smarter. E.g. over 5 minutes with some minimum interval, and\n// perhaps faster and in parallel when there are lots of reports. Perhaps also\n// depending on reporting interval (faster for 1h, slower for 24h).\n// Replaced by tests.\nvar sleepBetween = func(ctx context.Context, between time.Duration) (ok bool) {\n\tt := time.NewTimer(between)\n\tselect {\n\tcase <-ctx.Done():\n\t\tt.Stop()\n\t\treturn false\n\tcase <-t.C:\n\t\treturn true\n\t}\n}\n\n// sendReports gathers all policy domains that have evaluations that should\n// receive a DMARC report and sends a report to each.\nfunc sendReports(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, intervals []int) error {\n\tivals := make([]any, len(intervals))\n\tfor i, v := range intervals {\n\t\tivals[i] = v\n\t}\n\n\tdestDomains := map[string]bool{}\n\n\t// Gather all domains that we plan to send to.\n\tnsend := 0\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterEqual(\"IntervalHours\", ivals...)\n\terr := q.ForEach(func(e Evaluation) error {\n\t\tif !e.Optional && !destDomains[e.PolicyPublished.Domain] {\n\t\t\tnsend++\n\t\t}\n\t\tdestDomains[e.PolicyPublished.Domain] = destDomains[e.PolicyPublished.Domain] || !e.Optional\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"looking for domains to send reports to: %v\", err)\n\t}\n\n\tvar wg sync.WaitGroup\n\n\t// Sleep in between sending reports. We process hourly, and spread the reports over\n\t// the hour, but with max 5 minute interval.\n\tbetween := 45 * time.Minute\n\tif nsend > 0 {\n\t\tbetween /= time.Duration(nsend)\n\t}\n\tif between > 5*time.Minute {\n\t\tbetween = 5 * time.Minute\n\t}\n\n\t// Attempt to send report to each domain.\n\tn := 0\n\tfor d, send := range destDomains {\n\t\t// Cleanup evaluations for domain with only optionals.\n\t\tif !send {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, d)\n\t\t\tcontinue\n\t\t}\n\n\t\tif n > 0 {\n\t\t\tif ok := sleepBetween(ctx, between); !ok {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t\tn++\n\n\t\t// Send in goroutine, so a slow process doesn't block progress.\n\t\twg.Add(1)\n\t\tgo func(domain string) {\n\t\t\tdefer func() {\n\t\t\t\t// In case of panic don't take the whole program down.\n\t\t\t\tx := recover()\n\t\t\t\tif x != nil {\n\t\t\t\t\tlog.Error(\"unhandled panic in dmarcdb sendReports\", slog.Any(\"panic\", x))\n\t\t\t\t\tdebug.PrintStack()\n\t\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t\t}\n\t\t\t}()\n\t\t\tdefer wg.Done()\n\n\t\t\trlog := log.WithCid(mox.Cid()).With(slog.Any(\"domain\", domain))\n\t\t\trlog.Info(\"sending dmarc report\")\n\t\t\tif _, err := sendReportDomain(ctx, rlog, resolver, db, endTime, domain); err != nil {\n\t\t\t\trlog.Errorx(\"sending dmarc aggregate report to domain\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t}\n\t\t}(d)\n\t}\n\n\twg.Wait()\n\n\treturn nil\n}\n\ntype recipient struct {\n\taddress smtp.Address\n\tmaxSize uint64\n}\n\nfunc parseRecipient(log mlog.Log, uri dmarc.URI) (r recipient, ok bool) {\n\tlog = log.With(slog.Any(\"uri\", uri.Address))\n\n\tu, err := url.Parse(uri.Address)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\tif !strings.EqualFold(u.Scheme, \"mailto\") {\n\t\tlog.Debug(\"skipping unrecognized scheme in dmarc record rua value\")\n\t\treturn r, false\n\t}\n\taddr, err := smtp.ParseAddress(u.Opaque)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing mailto uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\n\tr = recipient{addr, uri.MaxSize}\n\t// ../rfc/7489:1197\n\tswitch uri.Unit {\n\tcase \"k\", \"K\":\n\t\tr.maxSize *= 1024\n\tcase \"m\", \"M\":\n\t\tr.maxSize *= 1024 * 1024\n\tcase \"g\", \"G\":\n\t\tr.maxSize *= 1024 * 1024 * 1024\n\tcase \"t\", \"T\":\n\t\t// Oh yeah, terabyte-sized reports!\n\t\tr.maxSize *= 1024 * 1024 * 1024 * 1024\n\tcase \"\":\n\tdefault:\n\t\tlog.Debug(\"unrecognized max size unit in dmarc record rua value\", slog.String(\"unit\", uri.Unit))\n\t\treturn r, false\n\t}\n\n\treturn r, true\n}\n\nfunc removeEvaluations(ctx context.Context, log mlog.Log, db *bstore.DB, endTime time.Time, domain string) {\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterNonzero(Evaluation{PolicyDomain: domain})\n\t_, err := q.Delete()\n\tlog.Check(err, \"removing evaluations after processing for dmarc aggregate report\")\n}\n\n// replaceable for testing.\nvar queueAdd = queue.Add\n\nfunc sendReportDomain(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, domain string) (cleanup bool, rerr error) {\n\tdom, err := dns.ParseDomain(domain)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"parsing domain for sending reports: %v\", err)\n\t}\n\n\t// We'll cleanup records by default.\n\tcleanup = true\n\t// If we encounter a temporary error we cancel cleanup of evaluations on error.\n\ttempError := false\n\n\tdefer func() {\n\t\tif !cleanup || tempError {\n\t\t\tlog.Debug(\"not cleaning up evaluations after attempting to send dmarc aggregate report\")\n\t\t} else {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, domain)\n\t\t}\n\t}()\n\n\t// We're going to build up this report.\n\treport := dmarcrpt.Feedback{\n\t\tVersion: \"1.0\",\n\t\tReportMetadata: dmarcrpt.ReportMetadata{\n\t\t\tOrgName: mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\tEmail:   \"postmaster@\" + mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\t// ReportID and DateRange are set after we've seen evaluations.\n\t\t\t// Errors is filled below when we encounter problems.\n\t\t},\n\t\t// We'll fill the records below.\n\t\tRecords: []dmarcrpt.ReportRecord{},\n\t}\n\n\tvar errors []string // For report.ReportMetaData.Errors\n\n\t// Check if we should be sending a report at all: if there are rua URIs in the\n\t// current DMARC record. The interval may have changed too, but we'll flush out our\n\t// evaluations regardless. We always use the latest DMARC record when sending, but\n\t// we'll lump all policies of the last interval into one report.\n\t// ../rfc/7489:1714\n\tstatus, _, record, _, _, err := dmarc.Lookup(ctx, log.Logger, resolver, dom)\n\tif err != nil {\n\t\t// todo future: we could perhaps still send this report, assuming the values we know. in case of temporary error, we could also schedule again regardless of next interval hour (we would now only retry a 24h-interval report after 24h passed).\n\t\t// Remove records unless it was a temporary error. We'll try again next round.\n\t\tcleanup = status != dmarc.StatusTemperror\n\t\treturn cleanup, fmt.Errorf(\"looking up current dmarc record for reporting address: %v\", err)\n\t}\n\n\tvar recipients []recipient\n\n\t// Gather all aggregate reporting addresses to try to send to. We'll start with\n\t// those in the initial DMARC record, but will follow external reporting addresses\n\t// and possibly update the list.\n\tfor _, uri := range record.AggregateReportAddresses {\n\t\tr, ok := parseRecipient(log, uri)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Check if domain of rua recipient has the same organizational domain as for the\n\t\t// evaluations. If not, we need to verify we are allowed to send.\n\t\truaOrgDom := publicsuffix.Lookup(ctx, log.Logger, r.address.Domain)\n\t\tevalOrgDom := publicsuffix.Lookup(ctx, log.Logger, dom)\n\n\t\tif ruaOrgDom == evalOrgDom {\n\t\t\trecipients = append(recipients, r)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Verify and follow addresses in other organizational domain through\n\t\t// <policydomain>._report._dmarc.<host> lookup.\n\t\t// ../rfc/7489:1556\n\t\taccepts, status, records, _, _, err := dmarc.LookupExternalReportsAccepted(ctx, log.Logger, resolver, evalOrgDom, r.address.Domain)\n\t\tlog.Debugx(\"checking if rua address with different organization domain has opted into receiving dmarc reports\", err,\n\t\t\tslog.Any(\"policydomain\", evalOrgDom),\n\t\t\tslog.Any(\"destinationdomain\", r.address.Domain),\n\t\t\tslog.Bool(\"accepts\", accepts),\n\t\t\tslog.Any(\"status\", status))\n\t\tif status == dmarc.StatusTemperror {\n\t\t\t// With a temporary error, we'll try to get the report the delivered anyway,\n\t\t\t// perhaps there are multiple recipients.\n\t\t\t// ../rfc/7489:1578\n\t\t\ttempError = true\n\t\t\terrors = append(errors, \"temporary error checking authorization for report delegation to external address\")\n\t\t}\n\t\tif !accepts {\n\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain that does not opt-in to receiving dmarc records through _report dmarc record\", r.address))\n\t\t\tcontinue\n\t\t}\n\n\t\t// We can follow a _report DMARC DNS record once. In that record, a domain may\n\t\t// specify alternative addresses that we should send reports to instead. Such\n\t\t// alternative address(es) must have the same host. If not, we ignore the new\n\t\t// value. Behaviour for multiple records and/or multiple new addresses is\n\t\t// underspecified. We'll replace an address with one or more new addresses, and\n\t\t// keep the original if there was no candidate (which covers the case of invalid\n\t\t// alternative addresses and no new address specified).\n\t\t// ../rfc/7489:1600\n\t\tfoundReplacement := false\n\t\trlog := log.With(slog.Any(\"followedaddress\", uri.Address))\n\t\tfor _, record := range records {\n\t\t\tfor _, exturi := range record.AggregateReportAddresses {\n\t\t\t\textr, ok := parseRecipient(rlog, exturi)\n\t\t\t\tif !ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif extr.address.Domain != r.address.Domain {\n\t\t\t\t\trlog.Debug(\"rua address in external _report dmarc record has different host than initial dmarc record, ignoring new name\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain with a replacement address %s with different host\", r.address, extr.address))\n\t\t\t\t} else {\n\t\t\t\t\trlog.Debug(\"using replacement rua address from external _report dmarc record\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\tfoundReplacement = true\n\t\t\t\t\trecipients = append(recipients, extr)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif !foundReplacement {\n\t\t\trecipients = append(recipients, r)\n\t\t}\n\t}\n\n\tif len(recipients) == 0 {\n\t\t// No reports requested, perfectly fine, no work to do for us.\n\t\tlog.Debug(\"no aggregate reporting addresses configured\")\n\t\treturn true, nil\n\t}\n\n\t// We count idential records. Can be common with a domain sending quite some email.\n\t// Though less if the sending domain has many IPs. In the future, we may want to\n\t// remove some details from records so we can aggregate them into fewer rows.\n\ttype recordCount struct {\n\t\tdmarcrpt.ReportRecord\n\t\tcount int\n\t}\n\tcounts := map[string]recordCount{}\n\n\n```\n# Function Signature: func RemoveEvaluationsDomain(ctx context.Context, domain dns.Domain) error {\n# Natural Language Annotations: // RemoveEvaluationsDomain removes evaluations for domain so they won't be sent in\n// an aggregate report.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc RemoveEvaluationsDomain(ctx context.Context, domain dns.Domain) error {\n\t_, err := bstore.QueryDB[Evaluation](ctx, EvalDB).FilterNonzero(Evaluation{PolicyDomain: domain}).Delete()\n\treturn err\n}\n```", "  ```\nfunc RemoveEvaluationsDomain(ctx context.Context, domain dns.Domain) error {\n\tdom, err := dns.ParseDomain(domain)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing domain for removing evaluations: %v\", err)\n\t}\n\t_, err = bstore.QueryDB[Evaluation](ctx, EvalDB).Filter(\"domain\", dom).Delete()\n\treturn err\n}\n```", "  ```\nfunc RemoveEvaluationsDomain(ctx context.Context, domain dns.Domain) error {\n\tdomain, err := dns.ParseDomain(domain)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing domain to remove evaluations in aggregate report: %v\", err)\n\t}\n\n\t// Filter evaluations for domain, ignoring interval and other criteria as we're\n\t// wiping out everything.\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterEqual(Evaluation{PolicyDomain: domain})\n\tcount, err := q.Delete()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"removing DNS evaluations for %s: %v\", domain.ASCII, err)\n\t}\n\tmetricReport.Add(float64(-count))\n\n\tlog.Info(\"removed evaluations\", slog.String(\"domain\", dom.ASCII), slog.FInt(\"count\", -count))\n\treturn nil\n}\n```"]}, "mox-dmarcdb/reports.go-AddReport": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dmarcdb/reports.go:\n```\npackage dmarcdb\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/dmarcrpt\"\n\t\"github.com/mjl-/mox/dns\"\n)\n\nvar (\n\tReportsDBTypes = []any{DomainFeedback{}} // Types stored in DB.\n\tReportsDB      *bstore.DB                // Exported for backups.\n)\n\nvar (\n\tmetricEvaluated = promauto.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_policy_evaluated_total\",\n\t\t\tHelp: \"Number of policy evaluations.\",\n\t\t},\n\t\t// We only register validated domains for which we have a config.\n\t\t[]string{\"domain\", \"disposition\", \"dkim\", \"spf\"},\n\t)\n\tmetricDKIM = promauto.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_dkim_result_total\",\n\t\t\tHelp: \"Number of DKIM results.\",\n\t\t},\n\t\t[]string{\"result\"},\n\t)\n\tmetricSPF = promauto.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_spf_result_total\",\n\t\t\tHelp: \"Number of SPF results.\",\n\t\t},\n\t\t[]string{\"result\"},\n\t)\n)\n\n// DomainFeedback is a single report stored in the database.\ntype DomainFeedback struct {\n\tID int64\n\t// Domain where DMARC DNS record was found, could be organizational domain.\n\tDomain string `bstore:\"index\"`\n\t// Domain in From-header.\n\tFromDomain string `bstore:\"index\"`\n\tdmarcrpt.Feedback\n}\n\n// AddReport adds a DMARC aggregate feedback report from an email to the database,\n// and updates prometheus metrics.\n//\n// fromDomain is the domain in the report message From header.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Records returns all reports in the database.\nfunc Records(ctx context.Context) ([]DomainFeedback, error) {\n\treturn bstore.QueryDB[DomainFeedback](ctx, ReportsDB).List()\n}\n\n// RecordID returns the report for the ID.\nfunc RecordID(ctx context.Context, id int64) (DomainFeedback, error) {\n\te := DomainFeedback{ID: id}\n\terr := ReportsDB.Get(ctx, &e)\n\treturn e, err\n}\n\n// RecordsPeriodDomain returns the reports overlapping start and end, for the given\n// domain. If domain is empty, all records match for domain.\nfunc RecordsPeriodDomain(ctx context.Context, start, end time.Time, domain string) ([]DomainFeedback, error) {\n\ts := start.Unix()\n\te := end.Unix()\n\n\tq := bstore.QueryDB[DomainFeedback](ctx, ReportsDB)\n\tif domain != \"\" {\n\t\tq.FilterNonzero(DomainFeedback{Domain: domain})\n\t}\n\tq.FilterFn(func(d DomainFeedback) bool {\n\t\tm := d.Feedback.ReportMetadata.DateRange\n\t\treturn m.Begin >= s && m.Begin < e || m.End > s && m.End <= e\n\t})\n\treturn q.List()\n}\n\n```\n# Function Signature: func AddReport(ctx context.Context, f *dmarcrpt.Feedback, fromDomain dns.Domain) error {\n# Natural Language Annotations: // AddReport adds a DMARC aggregate feedback report from an email to the database,\n// and updates prometheus metrics.\n//\n// fromDomain is the domain in the report message From header.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc AddReport(ctx context.Context, f *dmarcrpt.Feedback, fromDomain dns.Domain) error {\n\tid, err := ReportsDB.Insert(ctx, &DomainFeedback{\n\t\tfromDomain\n\t\tFeedback:     *f,\n\t})\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tlabels := prometheus.Labels{\n\t\t\"domain\":    f.PolicyDomain,\n\t\t\"disposition\": string(f.Disposition),\n\t\t\"dkim\":      string(f.DKIMSignature),\n\t\t\"spf\":      string(f.SPFResult),\n\t}\n\tmetricEvaluated.With(labels).Inc()\n\tmetricDKIM.WithLabelValues(string(f.DKIMSignature)).Inc()\n\tmetricSPF.WithLabelValues(string(f.SPFResult)).Inc()\n\n\treturn nil\n}\n```", "  ```\nfunc AddReport(ctx context.Context, f *dmarcrpt.Feedback, fromDomain string) error {\n\t// Convert the From header domain to lowercase.\n\tldomain := strings.ToLower(fromDomain)\n\n\t// Create a new DomainFeedback struct with the data from the report.\n\tdf := DomainFeedback{\n\t\tDomain:     ldomain,\n\t\tFromDomain: ldomain\n\t\tFeedback:   *f,\n\t}\n\n\t// Add the report to the database.\n\tif err := ReportsDB.Insert(ctx, df); err != nil {\n\t\treturn err\n\t}\n\n\t// Update Prometheus metrics based on the report.\n\tmetricEvaluated.With(prometheus.Labels{\n\t\t\"domain\":     ldomain,\n\t\t\"disposition\": string(f.ReportMetadata.Override),\n\t\t\"dkim\": fmt.Sprintf(\"%v\", f.DkimScore >= 0.75),\n\t\t\"spf\": fmt.Sprintf(\"%v\", f.DkimScore >= 0.75),\n\t}).Inc()\n\n\tmetricDKIM.With(prometheus.Labels{\n\t\t\"result\": f.DkimResult,\n\t}).Inc()\n\n\tmetricSPF.With(prometheus.Labels{\n\t\t\"result\": f.SpfResult,\n\t}).Inc()\n\n\treturn nil\n}\n```", "  ```\nfunc AddReport(ctx context.Context, f *dmarcrpt.Feedback, fromDomain dns.Domain) error {\n\treturn AddAnnotatedReport(ctx, f, fromDomain, []string{})\n}\n\nfunc AddAnnotatedReport(ctx context.Context, f *dmarcrpt.Feedback, fromDomain dns.Domain, annotations []string) error {\n\t// Create new domain feedback record from feedback and from domain.\n\tdf := DomainFeedback{\n\t\tFromDomain: fromDomain.IDNA(),\n\t\tFeedback:   *f,\n\t}\n\t// Update metrics with dispositions.\n\tupdateMetrics(ctx, df)\n\t// Add report and return any errors.\n\treturn ReportsDB.Add(ctx, &df)\n}\n\nfunc updateMetrics(ctx context.Context, df DomainFeedback) {\n\t// Update disposition metrics.\n\tfor _, d := range df.Feedback.Reports {\n\t\tmetricEvaluated.WithContext(ctx).WithLabelValues(\n\t\t\tdf.Domain,\n\t\t\td.Disposition,\n\t\t\td.Dkim,\n\t\t\td.Spf,\n\t\t).Inc()\n\t}\n\t// Update DKIM metrics.\n\tfor _, r := range df.Feedback.Records {\n\t\tmetricDKIM.WithContext(ctx).WithLabelValues(\n\t\t\tr.Result,\n\t\t).Inc()\n\t}\n\t// Update SPF metrics.\n\tfor _, r := range df.Feedback.Records {\n\t\tmetricSPF.WithContext(ctx).WithLabelValues(\n\t\t\tr.Result,\n\t\t).Inc()\n\t}\n}\n```"]}, "mox-junk/bloom.go-NewBloom": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/junk/bloom.go:\n```\npackage junk\n\nimport (\n\t\"errors\"\n\t\"os\"\n\n\t\"golang.org/x/crypto/blake2b\"\n)\n\n// see https://en.wikipedia.org/wiki/Bloom_filter\n\nvar errWidth = errors.New(\"k and width wider than 256 bits and width not more than 32\")\nvar errPowerOfTwo = errors.New(\"data not a power of two\")\n\n// Bloom is a bloom filter.\ntype Bloom struct {\n\tdata     []byte\n\tk        int // Number of bits we store/lookup in the bloom filter per value.\n\tw        int // Number of bits needed to address a single bit position.\n\tmodified bool\n}\n\nfunc bloomWidth(fileSize int) int {\n\tw := 0\n\tfor bits := uint32(fileSize * 8); bits > 1; bits >>= 1 {\n\t\tw++\n\t}\n\treturn w\n}\n\n// BloomValid returns an error if the bloom file parameters are not correct.\nfunc BloomValid(fileSize int, k int) error {\n\t_, err := bloomValid(fileSize, k)\n\treturn err\n}\n\nfunc bloomValid(fileSize, k int) (int, error) {\n\tw := bloomWidth(fileSize)\n\tif 1<<w != fileSize*8 {\n\t\treturn 0, errPowerOfTwo\n\t}\n\tif k*w > 256 || w > 32 {\n\t\treturn 0, errWidth\n\t}\n\treturn w, nil\n}\n\n// NewBloom returns a bloom filter with given initial data.\n//\n// The number of bits in data must be a power of 2.\n// K is the number of \"hashes\" (bits) to store/lookup for each value stored.\n// Width is calculated as the number of bits needed to represent a single bit/hash\n// position in the data.\n//\n// For each value stored/looked up, a hash over the value is calculated. The hash\n// is split into \"k\" values that are \"width\" bits wide, each used to lookup a bit.\n// K * width must not exceed 256.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (b *Bloom) Add(s string) {\n\th := hash([]byte(s), b.w)\n\tfor i := 0; i < b.k; i++ {\n\t\tb.set(h.nextPos())\n\t}\n}\n\nfunc (b *Bloom) Has(s string) bool {\n\th := hash([]byte(s), b.w)\n\tfor i := 0; i < b.k; i++ {\n\t\tif !b.has(h.nextPos()) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc (b *Bloom) Bytes() []byte {\n\treturn b.data\n}\n\nfunc (b *Bloom) Modified() bool {\n\treturn b.modified\n}\n\n// Ones returns the number of ones.\nfunc (b *Bloom) Ones() (n int) {\n\tfor _, d := range b.data {\n\t\tfor i := 0; i < 8; i++ {\n\t\t\tif d&1 != 0 {\n\t\t\t\tn++\n\t\t\t}\n\t\t\td >>= 1\n\t\t}\n\t}\n\treturn n\n}\n\nfunc (b *Bloom) Write(path string) error {\n\tf, err := os.OpenFile(path, os.O_CREATE|os.O_WRONLY, 0660)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif _, err := f.Write(b.data); err != nil {\n\t\tf.Close()\n\t\treturn err\n\t}\n\tif err := f.Close(); err != nil {\n\t\treturn err\n\t}\n\tb.modified = false\n\treturn nil\n}\n\nfunc (b *Bloom) has(p int) bool {\n\tv := b.data[p>>3] >> (7 - (p & 7))\n\treturn v&1 != 0\n}\n\nfunc (b *Bloom) set(p int) {\n\tby := p >> 3\n\tbi := p & 0x7\n\tvar v byte = 1 << (7 - bi)\n\tif b.data[by]&v == 0 {\n\t\tb.data[by] |= v\n\t\tb.modified = true\n\t}\n}\n\ntype bits struct {\n\twidth int    // Number of bits for each position.\n\tbuf   []byte // Remaining bytes to use for next position.\n\tcur   uint64 // Bits to read next position from. Replenished from buf.\n\tncur  int    // Number of bits available in cur. We consume the highest bits first.\n}\n\nfunc hash(v []byte, width int) *bits {\n\tbuf := blake2b.Sum256(v)\n\treturn &bits{width: width, buf: buf[:]}\n}\n\n// nextPos returns the next bit position.\nfunc (b *bits) nextPos() (v int) {\n\tif b.width > b.ncur {\n\t\tfor len(b.buf) > 0 && b.ncur < 64-8 {\n\t\t\tb.cur <<= 8\n\t\t\tb.cur |= uint64(b.buf[0])\n\t\t\tb.ncur += 8\n\t\t\tb.buf = b.buf[1:]\n\t\t}\n\t}\n\tv = int((b.cur >> (b.ncur - b.width)) & ((1 << b.width) - 1))\n\tb.ncur -= b.width\n\treturn v\n}\n\n```\n# Function Signature: func NewBloom(data []byte, k int) (*Bloom, error) {\n# Natural Language Annotations: // NewBloom returns a bloom filter with given initial data.\n//\n// The number of bits in data must be a power of 2.\n// K is the number of \"hashes\" (bits) to store/lookup for each value stored.\n// Width is calculated as the number of bits needed to represent a single bit/hash\n// position in the data.\n//\n// For each value stored/looked up, a hash over the value is calculated. The hash\n// is split into \"k\" values that are \"width\" bits wide, each used to lookup a bit.\n// K * width must not exceed 256.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewBloom(data []byte, k int) (*Bloom, error) {\n\tw, err := bloomValid(len(data)*8, k)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &Bloom{\n\t\tdata: data,\n\t\tk:    k,\n\t\tw:    w,\n\t}, nil\n}\n```", "  ```\nfunc NewBloom(data []byte, k int) (*Bloom, error) {\n\tw, err := bloomValid(k, len(data) * 8)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tb := &Bloom{data: data, k: k, w: w, modified: false}\n\treturn b, nil\n}\n```", "  ```\nfunc NewBloom(data []byte, k int) (*Bloom, error) {\n\tif fileSize := len(data) * 8; fileSize < 1 {\n\t\treturn nil, errWidth\n\t}\n\tif k < 1 {\n\t\treturn nil, errWidth\n\t}\n\tw, err := bloomValid(fileSize, k)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &Bloom{\n\t\tdata: data,\n\t\tk:    k,\n\t\tw:    w,\n\t}, nil\n}\n```"]}, "mox-junk/filter.go-NewFilter": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/junk/filter.go:\n```\n// Package junk implements a bayesian spam filter.\n//\n// A message can be parsed into words. Words (or pairs or triplets) can be used\n// to train the filter or to classify the message as ham or spam.  Training\n// records the words in the database as ham/spam. Classifying consists of\n// calculating the ham/spam probability by combining the words in the message\n// with their ham/spam status.\npackage junk\n\n// todo: look at inverse chi-square function? see https://www.linuxjournal.com/article/6467\n// todo: perhaps: whether anchor text in links in html are different from the url\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"math\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"time\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n)\n\nvar (\n\t// errBadContentType = errors.New(\"bad content-type\") // sure sign of spam, todo: use this error\n\terrClosed = errors.New(\"filter is closed\")\n)\n\ntype word struct {\n\tHam  uint32\n\tSpam uint32\n}\n\ntype wordscore struct {\n\tWord string\n\tHam  uint32\n\tSpam uint32\n}\n\n// Params holds parameters for the filter. Most are at test-time. The first are\n// used during parsing and training.\ntype Params struct {\n\tOnegrams    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for single words.\"`\n\tTwograms    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each two consecutive words.\"`\n\tThreegrams  bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each three consecutive words.\"`\n\tMaxPower    float64 `sconf-doc:\"Maximum power a word (combination) can have. If spaminess is 0.99, and max power is 0.1, spaminess of the word will be set to 0.9. Similar for ham words.\"`\n\tTopWords    int     `sconf-doc:\"Number of most spammy/hammy words to use for calculating probability. E.g. 10.\"`\n\tIgnoreWords float64 `sconf:\"optional\" sconf-doc:\"Ignore words that are this much away from 0.5 haminess/spaminess. E.g. 0.1, causing word (combinations) of 0.4 to 0.6 to be ignored.\"`\n\tRareWords   int     `sconf:\"optional\" sconf-doc:\"Occurrences in word database until a word is considered rare and its influence in calculating probability reduced. E.g. 1 or 2.\"`\n}\n\nvar DBTypes = []any{wordscore{}} // Stored in DB.\n\ntype Filter struct {\n\tParams\n\n\tlog               mlog.Log // For logging cid.\n\tclosed            bool\n\tmodified          bool            // Whether any modifications are pending. Cleared by Save.\n\thams, spams       uint32          // Message count, stored in db under word \"-\".\n\tcache             map[string]word // Words read from database or during training.\n\tchanged           map[string]word // Words modified during training.\n\tdbPath, bloomPath string\n\tdb                *bstore.DB // Always open on a filter.\n\tbloom             *Bloom     // Only opened when writing.\n\tisNew             bool       // Set for new filters until their first sync to disk. For faster writing.\n}\n\nfunc (f *Filter) ensureBloom() error {\n\tif f.bloom != nil {\n\t\treturn nil\n\t}\n\tvar err error\n\tf.bloom, err = openBloom(f.bloomPath)\n\treturn err\n}\n\n// CloseDiscard closes the filter, discarding any changes.\nfunc (f *Filter) CloseDiscard() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\terr := f.db.Close()\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\n// Close first saves the filter if it has modifications, then closes the database\n// connection and releases the bloom filter.\nfunc (f *Filter) Close() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\tvar err error\n\tif f.modified {\n\t\terr = f.Save()\n\t}\n\tif err != nil {\n\t\tf.db.Close()\n\t} else {\n\t\terr = f.db.Close()\n\t}\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\nfunc OpenFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string, loadBloom bool) (*Filter, error) {\n\tvar bloom *Bloom\n\tif loadBloom {\n\t\tvar err error\n\t\tbloom, err = openBloom(bloomPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else if fi, err := os.Stat(bloomPath); err == nil {\n\t\tif err := BloomValid(int(fi.Size()), bloomK); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"bloom: %s\", err)\n\t\t}\n\t}\n\n\tdb, err := openDB(ctx, log, dbPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open database: %s\", err)\n\t}\n\n\tf := &Filter{\n\t\tParams:    params,\n\t\tlog:       log,\n\t\tcache:     map[string]word{},\n\t\tchanged:   map[string]word{},\n\t\tdbPath:    dbPath,\n\t\tbloomPath: bloomPath,\n\t\tdb:        db,\n\t\tbloom:     bloom,\n\t}\n\terr = f.db.Read(ctx, func(tx *bstore.Tx) error {\n\t\twc := wordscore{Word: \"-\"}\n\t\terr := tx.Get(&wc)\n\t\tf.hams = wc.Ham\n\t\tf.spams = wc.Spam\n\t\treturn err\n\t})\n\tif err != nil {\n\t\tcerr := f.Close()\n\t\tlog.Check(cerr, \"closing filter after error\")\n\t\treturn nil, fmt.Errorf(\"looking up ham/spam message count: %s\", err)\n\t}\n\treturn f, nil\n}\n\n// NewFilter creates a new filter with empty bloom filter and database files. The\n// filter is marked as new until the first save, will be done automatically if\n// TrainDirs is called. If the bloom and/or database files exist, an error is\n// returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconst bloomK = 10\n\nfunc openBloom(path string) (*Bloom, error) {\n\tbuf, err := os.ReadFile(path)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading bloom file: %w\", err)\n\t}\n\treturn NewBloom(buf, bloomK)\n}\n\nfunc newDB(ctx context.Context, log mlog.Log, path string) (db *bstore.DB, rerr error) {\n\t// Remove any existing files.\n\tos.Remove(path)\n\n\tdefer func() {\n\t\tif rerr != nil {\n\t\t\terr := os.Remove(path)\n\t\t\tlog.Check(err, \"removing db file after init error\")\n\t\t}\n\t}()\n\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\tdb, err := bstore.Open(ctx, path, &opts, DBTypes...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open new database: %w\", err)\n\t}\n\treturn db, nil\n}\n\nfunc openDB(ctx context.Context, log mlog.Log, path string) (*bstore.DB, error) {\n\tif _, err := os.Stat(path); err != nil {\n\t\treturn nil, fmt.Errorf(\"stat db file: %w\", err)\n\t}\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\treturn bstore.Open(ctx, path, &opts, DBTypes...)\n}\n\n// Save stores modifications, e.g. from training, to the database and bloom\n// filter files.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc loadWords(ctx context.Context, db *bstore.DB, l []string, dst map[string]word) error {\n\tsort.Slice(l, func(i, j int) bool {\n\t\treturn l[i] < l[j]\n\t})\n\n\terr := db.Read(ctx, func(tx *bstore.Tx) error {\n\t\tfor _, w := range l {\n\t\t\twc := wordscore{Word: w}\n\t\t\tif err := tx.Get(&wc); err == nil {\n\t\t\t\tdst[w] = word{wc.Ham, wc.Spam}\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"fetching words: %s\", err)\n\t}\n\treturn nil\n}\n\n// ClassifyWords returns the spam probability for the given words, and number of recognized ham and spam words.\nfunc (f *Filter) ClassifyWords(ctx context.Context, words map[string]struct{}) (probability float64, nham, nspam int, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, 0, errClosed\n\t}\n\n\ttype xword struct {\n\t\tWord string\n\t\tR    float64\n\t}\n\n\tvar hamHigh float64 = 0\n\tvar spamLow float64 = 1\n\tvar topHam []xword\n\tvar topSpam []xword\n\n\t// Find words that should be in the database.\n\tlookupWords := []string{}\n\texpect := map[string]struct{}{}\n\tunknowns := map[string]struct{}{}\n\ttotalUnknown := 0\n\tfor w := range words {\n\t\tif f.bloom != nil && !f.bloom.Has(w) {\n\t\t\ttotalUnknown++\n\t\t\tif len(unknowns) < 50 {\n\t\t\t\tunknowns[w] = struct{}{}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; ok {\n\t\t\tcontinue\n\t\t}\n\t\tlookupWords = append(lookupWords, w)\n\t\texpect[w] = struct{}{}\n\t}\n\tif len(unknowns) > 0 {\n\t\tf.log.Debug(\"unknown words in bloom filter, showing max 50\",\n\t\t\tslog.Any(\"words\", unknowns),\n\t\t\tslog.Any(\"totalunknown\", totalUnknown),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\t// Fetch words from database.\n\tfetched := map[string]word{}\n\tif len(lookupWords) > 0 {\n\t\tif err := loadWords(ctx, f.db, lookupWords, fetched); err != nil {\n\t\t\treturn 0, 0, 0, err\n\t\t}\n\t\tfor w, c := range fetched {\n\t\t\tdelete(expect, w)\n\t\t\tf.cache[w] = c\n\t\t}\n\t\tf.log.Debug(\"unknown words in db\",\n\t\t\tslog.Any(\"words\", expect),\n\t\t\tslog.Any(\"totalunknown\", len(expect)),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tvar wS, wH float64\n\t\tif f.spams > 0 {\n\t\t\twS = float64(c.Spam) / float64(f.spams)\n\t\t}\n\t\tif f.hams > 0 {\n\t\t\twH = float64(c.Ham) / float64(f.hams)\n\t\t}\n\t\tr := wS / (wS + wH)\n\n\t\tif r < f.MaxPower {\n\t\t\tr = f.MaxPower\n\t\t} else if r >= 1-f.MaxPower {\n\t\t\tr = 1 - f.MaxPower\n\t\t}\n\n\t\tif c.Ham+c.Spam <= uint32(f.RareWords) {\n\t\t\t// Reduce the power of rare words.\n\t\t\tr += float64(1+uint32(f.RareWords)-(c.Ham+c.Spam)) * (0.5 - r) / 10\n\t\t}\n\t\tif math.Abs(0.5-r) < f.IgnoreWords {\n\t\t\tcontinue\n\t\t}\n\t\tif r < 0.5 {\n\t\t\tif len(topHam) >= f.TopWords && r > hamHigh {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopHam = append(topHam, xword{w, r})\n\t\t\tif r > hamHigh {\n\t\t\t\thamHigh = r\n\t\t\t}\n\t\t} else if r > 0.5 {\n\t\t\tif len(topSpam) >= f.TopWords && r < spamLow {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopSpam = append(topSpam, xword{w, r})\n\t\t\tif r < spamLow {\n\t\t\t\tspamLow = r\n\t\t\t}\n\t\t}\n\t}\n\n\tsort.Slice(topHam, func(i, j int) bool {\n\t\ta, b := topHam[i], topHam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R < b.R\n\t})\n\tsort.Slice(topSpam, func(i, j int) bool {\n\t\ta, b := topSpam[i], topSpam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R > b.R\n\t})\n\n\tnham = f.TopWords\n\tif nham > len(topHam) {\n\t\tnham = len(topHam)\n\t}\n\tnspam = f.TopWords\n\tif nspam > len(topSpam) {\n\t\tnspam = len(topSpam)\n\t}\n\ttopHam = topHam[:nham]\n\ttopSpam = topSpam[:nspam]\n\n\tvar eta float64\n\tfor _, x := range topHam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\tfor _, x := range topSpam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\n\tf.log.Debug(\"top words\", slog.Any(\"hams\", topHam), slog.Any(\"spams\", topSpam))\n\n\tprob := 1 / (1 + math.Pow(math.E, eta))\n\treturn prob, len(topHam), len(topSpam), nil\n}\n\n// ClassifyMessagePath is a convenience wrapper for calling ClassifyMessage on a file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) ClassifyMessageReader(ctx context.Context, mf io.ReaderAt, size int64) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tm, err := message.EnsurePart(f.log.Logger, false, mf, size)\n\tif err != nil && errors.Is(err, message.ErrBadContentType) {\n\t\t// Invalid content-type header is a sure sign of spam.\n\t\t//f.log.Infox(\"parsing content\", err)\n\t\treturn 1, nil, 0, 0, nil\n\t}\n\treturn f.ClassifyMessage(ctx, m)\n}\n\n// ClassifyMessage parses the mail message in r and returns the spam probability\n// (between 0 and 1), along with the tokenized words found in the message, and the\n// number of recognized ham and spam words.\nfunc (f *Filter) ClassifyMessage(ctx context.Context, m message.Part) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tvar err error\n\twords, err = f.ParseMessage(m)\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, err\n\t}\n\n\tprobability, nham, nspam, err = f.ClassifyWords(ctx, words)\n\treturn probability, words, nham, nspam, err\n}\n\n// Train adds the words of a single message to the filter.\nfunc (f *Filter) Train(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\tvar lwords []string\n\n\tfor w := range words {\n\t\tif !f.bloom.Has(w) {\n\t\t\tf.bloom.Add(w)\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\tf.modified = true\n\tif ham {\n\t\tf.hams++\n\t} else {\n\t\tf.spams++\n\t}\n\n\tfor w := range words {\n\t\tc := f.cache[w]\n\t\tif ham {\n\t\t\tc.Ham++\n\t\t} else {\n\t\t\tc.Spam++\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\nfunc (f *Filter) TrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Train(ctx, ham, words)\n}\n\nfunc (f *Filter) UntrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Untrain(ctx, ham, words)\n}\n\nfunc (f *Filter) loadCache(ctx context.Context, lwords []string) error {\n\tif len(lwords) == 0 {\n\t\treturn nil\n\t}\n\n\treturn loadWords(ctx, f.db, lwords, f.cache)\n}\n\n// Untrain adjusts the filter to undo a previous training of the words.\nfunc (f *Filter) Untrain(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\t// Lookup any words from the db that aren't in the cache and put them in the cache for modification.\n\tvar lwords []string\n\tfor w := range words {\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\t// Modify the message count.\n\tf.modified = true\n\tif ham {\n\t\tf.hams--\n\t} else {\n\t\tf.spams--\n\t}\n\n\t// Decrease the word counts.\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif ham {\n\t\t\tc.Ham--\n\t\t} else {\n\t\t\tc.Spam--\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\n// TrainDir parses mail messages from files and trains the filter.\nfunc (f *Filter) TrainDir(dir string, files []string, ham bool) (n, malformed uint32, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, errClosed\n\t}\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\tfor _, name := range files {\n\t\tp := filepath.Join(dir, name)\n\t\tvalid, words, err := f.tokenizeMail(p)\n\t\tif err != nil {\n\t\t\t// f.log.Infox(\"tokenizing mail\", err, slog.Any(\"path\", p))\n\t\t\tmalformed++\n\t\t\tcontinue\n\t\t}\n\t\tif !valid {\n\t\t\tcontinue\n\t\t}\n\t\tn++\n\t\tfor w := range words {\n\t\t\tif !f.bloom.Has(w) {\n\t\t\t\tf.bloom.Add(w)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc := f.cache[w]\n\t\t\tf.modified = true\n\t\t\tif ham {\n\t\t\t\tc.Ham++\n\t\t\t} else {\n\t\t\t\tc.Spam++\n\t\t\t}\n\t\t\tf.cache[w] = c\n\t\t\tf.changed[w] = c\n\t\t}\n\t}\n\treturn\n}\n\n// TrainDirs trains and saves a filter with mail messages from different types\n// of directories.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) fileSize(p string) int {\n\tfi, err := os.Stat(p)\n\tif err != nil {\n\t\tf.log.Infox(\"stat\", err, slog.Any(\"path\", p))\n\t\treturn 0\n\t}\n\treturn int(fi.Size())\n}\n\n// DB returns the database, for backups.\nfunc (f *Filter) DB() *bstore.DB {\n\treturn f.db\n}\n\n```\n# Function Signature: func NewFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string) (*Filter, error) {\n# Natural Language Annotations: // NewFilter creates a new filter with empty bloom filter and database files. The\n// filter is marked as new until the first save, will be done automatically if\n// TrainDirs is called. If the bloom and/or database files exist, an error is\n// returned.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string) (*Filter, error) {\n\tvar bloom *Bloom\n\tif loadBloom {\n\t\tvar err error\n\t\tbloom, err = openBloom(bloomPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else if fi, err := os.Stat(bloomPath); err == nil {\n\t\tif err := BloomValid(int(fi.Size()), bloomK); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"bloom: %s\", err)\n\t\t}\n\t}\n\n\tdb, err := openDB(ctx, log, dbPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open new database: %w\", err)\n\t}\n\n\tf := &Filter{\n\t\tParams:    params,\n\t\tlog:       log,\n\t\tcache:     map[string]word{},\n\t\tchanged:   map[string]word{},\n\t\tdbPath:    dbPath,\n\t\tbloomPath: bloomPath,\n\t\tdb:        db,\n\t\tbloom:     bloom,\n\t}\n\terr = f.db.Read(ctx, func(tx *bstore.Tx) error {\n\t\twc := wordscore{Word: \"-\"}\n\t\terr := tx.Get(&wc)\n\t\tf.hams = wc.Ham\n\t\tf.spams = wc.Spam\n\t\treturn err\n\t})\n\tif err != nil {\n\t\tcerr := f.Close()\n\t\tlog.Check(cerr, \"closing filter after error\")\n\t\treturn nil, fmt.Errorf(\"looking up ham/spam message count: %s\", err)\n\t}\n\treturn f, nil\n}\n```", "  ```\nfunc NewFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string) (*Filter, error) {\n\tvar bloom *Bloom\n\tif loadBloom {\n\t\tvar err error\n\t\tbloom, err = openBloom(bloomPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else if fi, err := os.Stat(bloomPath); err == nil {\n\t\tif err := BloomValid(int(fi.Size()), bloomK); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"bloom: %s\", err)\n\t\t}\n\t}\n\n\tdb, err := newDB(ctx, log, dbPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open new database: %w\", err)\n\t}\n\n\tf := &Filter{\n\t\tParams:    params,\n\t\tlog:       log,\n\t\tcache:     map[string]word{},\n\t\tchanged:   map[string]word{},\n\t\tdbPath:    dbPath,\n\t\tbloomPath: bloomPath,\n\t\tdb:        db,\n\t\tbloom:     bloom,\n\t\tisNew:     true,\n\t}\n\terr = f.db.Read(ctx, func(tx *bstore.Tx) error {\n\t\twc := wordscore{Word: \"-\"}\n\t\terr := tx.Get(&wc)\n\t\tf.hams = wc.Ham\n\t\tf.spams = wc.Spam\n\t\treturn err\n\t})\n\tif err != nil {\n\t\tcerr := f.Close()\n\t\tlog.Check(cerr, \"closing filter after error\")\n\t\treturn nil, fmt.Errorf(\"looking up ham/spam message count: %s\", err)\n\t}\n\treturn f, nil\n}\n```", "  ```\nfunc NewFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string) (*Filter, error) {\n\tvar bloom *Bloom\n\tif _, err := os.Stat(bloomPath); err == nil {\n\t\treturn nil, fmt.Errorf(\"bloom file already exists\")\n\t}\n\n\tdb, err := openDB(ctx, log, dbPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"opening database: %w\", err)\n\t}\n\n\tf := &Filter{\n\t\tParams:    params,\n\t\tlog:       log,\n\t\tcache:     map[string]word{},\n\t\tchanged:   map[string]word{},\n\t\tdbPath:    dbPath,\n\t\tbloomPath: bloomPath,\n\t\tdb:        db,\n\t\tbloom:     bloom,\n\t\tisNew:     true,\n\t}\n\n\t// New filters start with a ham count of 0 and a spam count of 0.\n\terr = f.db.Update(ctx, func(tx *bstore.Tx) error {\n\t\twc := wordscore{Word: \"-\", Ham: 0, Spam: 0}\n\t\treturn tx.Put(&wc)\n\t})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"updating word count in database: %w\", err)\n\t}\n\n\treturn f, nil\n}\n```"]}, "mox-junk/filter.go-Save": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/junk/filter.go:\n```\n// Package junk implements a bayesian spam filter.\n//\n// A message can be parsed into words. Words (or pairs or triplets) can be used\n// to train the filter or to classify the message as ham or spam.  Training\n// records the words in the database as ham/spam. Classifying consists of\n// calculating the ham/spam probability by combining the words in the message\n// with their ham/spam status.\npackage junk\n\n// todo: look at inverse chi-square function? see https://www.linuxjournal.com/article/6467\n// todo: perhaps: whether anchor text in links in html are different from the url\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"math\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"time\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n)\n\nvar (\n\t// errBadContentType = errors.New(\"bad content-type\") // sure sign of spam, todo: use this error\n\terrClosed = errors.New(\"filter is closed\")\n)\n\ntype word struct {\n\tHam  uint32\n\tSpam uint32\n}\n\ntype wordscore struct {\n\tWord string\n\tHam  uint32\n\tSpam uint32\n}\n\n// Params holds parameters for the filter. Most are at test-time. The first are\n// used during parsing and training.\ntype Params struct {\n\tOnegrams    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for single words.\"`\n\tTwograms    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each two consecutive words.\"`\n\tThreegrams  bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each three consecutive words.\"`\n\tMaxPower    float64 `sconf-doc:\"Maximum power a word (combination) can have. If spaminess is 0.99, and max power is 0.1, spaminess of the word will be set to 0.9. Similar for ham words.\"`\n\tTopWords    int     `sconf-doc:\"Number of most spammy/hammy words to use for calculating probability. E.g. 10.\"`\n\tIgnoreWords float64 `sconf:\"optional\" sconf-doc:\"Ignore words that are this much away from 0.5 haminess/spaminess. E.g. 0.1, causing word (combinations) of 0.4 to 0.6 to be ignored.\"`\n\tRareWords   int     `sconf:\"optional\" sconf-doc:\"Occurrences in word database until a word is considered rare and its influence in calculating probability reduced. E.g. 1 or 2.\"`\n}\n\nvar DBTypes = []any{wordscore{}} // Stored in DB.\n\ntype Filter struct {\n\tParams\n\n\tlog               mlog.Log // For logging cid.\n\tclosed            bool\n\tmodified          bool            // Whether any modifications are pending. Cleared by Save.\n\thams, spams       uint32          // Message count, stored in db under word \"-\".\n\tcache             map[string]word // Words read from database or during training.\n\tchanged           map[string]word // Words modified during training.\n\tdbPath, bloomPath string\n\tdb                *bstore.DB // Always open on a filter.\n\tbloom             *Bloom     // Only opened when writing.\n\tisNew             bool       // Set for new filters until their first sync to disk. For faster writing.\n}\n\nfunc (f *Filter) ensureBloom() error {\n\tif f.bloom != nil {\n\t\treturn nil\n\t}\n\tvar err error\n\tf.bloom, err = openBloom(f.bloomPath)\n\treturn err\n}\n\n// CloseDiscard closes the filter, discarding any changes.\nfunc (f *Filter) CloseDiscard() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\terr := f.db.Close()\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\n// Close first saves the filter if it has modifications, then closes the database\n// connection and releases the bloom filter.\nfunc (f *Filter) Close() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\tvar err error\n\tif f.modified {\n\t\terr = f.Save()\n\t}\n\tif err != nil {\n\t\tf.db.Close()\n\t} else {\n\t\terr = f.db.Close()\n\t}\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\nfunc OpenFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string, loadBloom bool) (*Filter, error) {\n\tvar bloom *Bloom\n\tif loadBloom {\n\t\tvar err error\n\t\tbloom, err = openBloom(bloomPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else if fi, err := os.Stat(bloomPath); err == nil {\n\t\tif err := BloomValid(int(fi.Size()), bloomK); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"bloom: %s\", err)\n\t\t}\n\t}\n\n\tdb, err := openDB(ctx, log, dbPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open database: %s\", err)\n\t}\n\n\tf := &Filter{\n\t\tParams:    params,\n\t\tlog:       log,\n\t\tcache:     map[string]word{},\n\t\tchanged:   map[string]word{},\n\t\tdbPath:    dbPath,\n\t\tbloomPath: bloomPath,\n\t\tdb:        db,\n\t\tbloom:     bloom,\n\t}\n\terr = f.db.Read(ctx, func(tx *bstore.Tx) error {\n\t\twc := wordscore{Word: \"-\"}\n\t\terr := tx.Get(&wc)\n\t\tf.hams = wc.Ham\n\t\tf.spams = wc.Spam\n\t\treturn err\n\t})\n\tif err != nil {\n\t\tcerr := f.Close()\n\t\tlog.Check(cerr, \"closing filter after error\")\n\t\treturn nil, fmt.Errorf(\"looking up ham/spam message count: %s\", err)\n\t}\n\treturn f, nil\n}\n\n// NewFilter creates a new filter with empty bloom filter and database files. The\n// filter is marked as new until the first save, will be done automatically if\n// TrainDirs is called. If the bloom and/or database files exist, an error is\n// returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconst bloomK = 10\n\nfunc openBloom(path string) (*Bloom, error) {\n\tbuf, err := os.ReadFile(path)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading bloom file: %w\", err)\n\t}\n\treturn NewBloom(buf, bloomK)\n}\n\nfunc newDB(ctx context.Context, log mlog.Log, path string) (db *bstore.DB, rerr error) {\n\t// Remove any existing files.\n\tos.Remove(path)\n\n\tdefer func() {\n\t\tif rerr != nil {\n\t\t\terr := os.Remove(path)\n\t\t\tlog.Check(err, \"removing db file after init error\")\n\t\t}\n\t}()\n\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\tdb, err := bstore.Open(ctx, path, &opts, DBTypes...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open new database: %w\", err)\n\t}\n\treturn db, nil\n}\n\nfunc openDB(ctx context.Context, log mlog.Log, path string) (*bstore.DB, error) {\n\tif _, err := os.Stat(path); err != nil {\n\t\treturn nil, fmt.Errorf(\"stat db file: %w\", err)\n\t}\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\treturn bstore.Open(ctx, path, &opts, DBTypes...)\n}\n\n// Save stores modifications, e.g. from training, to the database and bloom\n// filter files.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc loadWords(ctx context.Context, db *bstore.DB, l []string, dst map[string]word) error {\n\tsort.Slice(l, func(i, j int) bool {\n\t\treturn l[i] < l[j]\n\t})\n\n\terr := db.Read(ctx, func(tx *bstore.Tx) error {\n\t\tfor _, w := range l {\n\t\t\twc := wordscore{Word: w}\n\t\t\tif err := tx.Get(&wc); err == nil {\n\t\t\t\tdst[w] = word{wc.Ham, wc.Spam}\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"fetching words: %s\", err)\n\t}\n\treturn nil\n}\n\n// ClassifyWords returns the spam probability for the given words, and number of recognized ham and spam words.\nfunc (f *Filter) ClassifyWords(ctx context.Context, words map[string]struct{}) (probability float64, nham, nspam int, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, 0, errClosed\n\t}\n\n\ttype xword struct {\n\t\tWord string\n\t\tR    float64\n\t}\n\n\tvar hamHigh float64 = 0\n\tvar spamLow float64 = 1\n\tvar topHam []xword\n\tvar topSpam []xword\n\n\t// Find words that should be in the database.\n\tlookupWords := []string{}\n\texpect := map[string]struct{}{}\n\tunknowns := map[string]struct{}{}\n\ttotalUnknown := 0\n\tfor w := range words {\n\t\tif f.bloom != nil && !f.bloom.Has(w) {\n\t\t\ttotalUnknown++\n\t\t\tif len(unknowns) < 50 {\n\t\t\t\tunknowns[w] = struct{}{}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; ok {\n\t\t\tcontinue\n\t\t}\n\t\tlookupWords = append(lookupWords, w)\n\t\texpect[w] = struct{}{}\n\t}\n\tif len(unknowns) > 0 {\n\t\tf.log.Debug(\"unknown words in bloom filter, showing max 50\",\n\t\t\tslog.Any(\"words\", unknowns),\n\t\t\tslog.Any(\"totalunknown\", totalUnknown),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\t// Fetch words from database.\n\tfetched := map[string]word{}\n\tif len(lookupWords) > 0 {\n\t\tif err := loadWords(ctx, f.db, lookupWords, fetched); err != nil {\n\t\t\treturn 0, 0, 0, err\n\t\t}\n\t\tfor w, c := range fetched {\n\t\t\tdelete(expect, w)\n\t\t\tf.cache[w] = c\n\t\t}\n\t\tf.log.Debug(\"unknown words in db\",\n\t\t\tslog.Any(\"words\", expect),\n\t\t\tslog.Any(\"totalunknown\", len(expect)),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tvar wS, wH float64\n\t\tif f.spams > 0 {\n\t\t\twS = float64(c.Spam) / float64(f.spams)\n\t\t}\n\t\tif f.hams > 0 {\n\t\t\twH = float64(c.Ham) / float64(f.hams)\n\t\t}\n\t\tr := wS / (wS + wH)\n\n\t\tif r < f.MaxPower {\n\t\t\tr = f.MaxPower\n\t\t} else if r >= 1-f.MaxPower {\n\t\t\tr = 1 - f.MaxPower\n\t\t}\n\n\t\tif c.Ham+c.Spam <= uint32(f.RareWords) {\n\t\t\t// Reduce the power of rare words.\n\t\t\tr += float64(1+uint32(f.RareWords)-(c.Ham+c.Spam)) * (0.5 - r) / 10\n\t\t}\n\t\tif math.Abs(0.5-r) < f.IgnoreWords {\n\t\t\tcontinue\n\t\t}\n\t\tif r < 0.5 {\n\t\t\tif len(topHam) >= f.TopWords && r > hamHigh {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopHam = append(topHam, xword{w, r})\n\t\t\tif r > hamHigh {\n\t\t\t\thamHigh = r\n\t\t\t}\n\t\t} else if r > 0.5 {\n\t\t\tif len(topSpam) >= f.TopWords && r < spamLow {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopSpam = append(topSpam, xword{w, r})\n\t\t\tif r < spamLow {\n\t\t\t\tspamLow = r\n\t\t\t}\n\t\t}\n\t}\n\n\tsort.Slice(topHam, func(i, j int) bool {\n\t\ta, b := topHam[i], topHam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R < b.R\n\t})\n\tsort.Slice(topSpam, func(i, j int) bool {\n\t\ta, b := topSpam[i], topSpam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R > b.R\n\t})\n\n\tnham = f.TopWords\n\tif nham > len(topHam) {\n\t\tnham = len(topHam)\n\t}\n\tnspam = f.TopWords\n\tif nspam > len(topSpam) {\n\t\tnspam = len(topSpam)\n\t}\n\ttopHam = topHam[:nham]\n\ttopSpam = topSpam[:nspam]\n\n\tvar eta float64\n\tfor _, x := range topHam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\tfor _, x := range topSpam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\n\tf.log.Debug(\"top words\", slog.Any(\"hams\", topHam), slog.Any(\"spams\", topSpam))\n\n\tprob := 1 / (1 + math.Pow(math.E, eta))\n\treturn prob, len(topHam), len(topSpam), nil\n}\n\n// ClassifyMessagePath is a convenience wrapper for calling ClassifyMessage on a file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) ClassifyMessageReader(ctx context.Context, mf io.ReaderAt, size int64) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tm, err := message.EnsurePart(f.log.Logger, false, mf, size)\n\tif err != nil && errors.Is(err, message.ErrBadContentType) {\n\t\t// Invalid content-type header is a sure sign of spam.\n\t\t//f.log.Infox(\"parsing content\", err)\n\t\treturn 1, nil, 0, 0, nil\n\t}\n\treturn f.ClassifyMessage(ctx, m)\n}\n\n// ClassifyMessage parses the mail message in r and returns the spam probability\n// (between 0 and 1), along with the tokenized words found in the message, and the\n// number of recognized ham and spam words.\nfunc (f *Filter) ClassifyMessage(ctx context.Context, m message.Part) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tvar err error\n\twords, err = f.ParseMessage(m)\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, err\n\t}\n\n\tprobability, nham, nspam, err = f.ClassifyWords(ctx, words)\n\treturn probability, words, nham, nspam, err\n}\n\n// Train adds the words of a single message to the filter.\nfunc (f *Filter) Train(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\tvar lwords []string\n\n\tfor w := range words {\n\t\tif !f.bloom.Has(w) {\n\t\t\tf.bloom.Add(w)\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\tf.modified = true\n\tif ham {\n\t\tf.hams++\n\t} else {\n\t\tf.spams++\n\t}\n\n\tfor w := range words {\n\t\tc := f.cache[w]\n\t\tif ham {\n\t\t\tc.Ham++\n\t\t} else {\n\t\t\tc.Spam++\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\nfunc (f *Filter) TrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Train(ctx, ham, words)\n}\n\nfunc (f *Filter) UntrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Untrain(ctx, ham, words)\n}\n\nfunc (f *Filter) loadCache(ctx context.Context, lwords []string) error {\n\tif len(lwords) == 0 {\n\t\treturn nil\n\t}\n\n\treturn loadWords(ctx, f.db, lwords, f.cache)\n}\n\n// Untrain adjusts the filter to undo a previous training of the words.\nfunc (f *Filter) Untrain(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\t// Lookup any words from the db that aren't in the cache and put them in the cache for modification.\n\tvar lwords []string\n\tfor w := range words {\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\t// Modify the message count.\n\tf.modified = true\n\tif ham {\n\t\tf.hams--\n\t} else {\n\t\tf.spams--\n\t}\n\n\t// Decrease the word counts.\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif ham {\n\t\t\tc.Ham--\n\t\t} else {\n\t\t\tc.Spam--\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\n// TrainDir parses mail messages from files and trains the filter.\nfunc (f *Filter) TrainDir(dir string, files []string, ham bool) (n, malformed uint32, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, errClosed\n\t}\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\tfor _, name := range files {\n\t\tp := filepath.Join(dir, name)\n\t\tvalid, words, err := f.tokenizeMail(p)\n\t\tif err != nil {\n\t\t\t// f.log.Infox(\"tokenizing mail\", err, slog.Any(\"path\", p))\n\t\t\tmalformed++\n\t\t\tcontinue\n\t\t}\n\t\tif !valid {\n\t\t\tcontinue\n\t\t}\n\t\tn++\n\t\tfor w := range words {\n\t\t\tif !f.bloom.Has(w) {\n\t\t\t\tf.bloom.Add(w)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc := f.cache[w]\n\t\t\tf.modified = true\n\t\t\tif ham {\n\t\t\t\tc.Ham++\n\t\t\t} else {\n\t\t\t\tc.Spam++\n\t\t\t}\n\t\t\tf.cache[w] = c\n\t\t\tf.changed[w] = c\n\t\t}\n\t}\n\treturn\n}\n\n// TrainDirs trains and saves a filter with mail messages from different types\n// of directories.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) fileSize(p string) int {\n\tfi, err := os.Stat(p)\n\tif err != nil {\n\t\tf.log.Infox(\"stat\", err, slog.Any(\"path\", p))\n\t\treturn 0\n\t}\n\treturn int(fi.Size())\n}\n\n// DB returns the database, for backups.\nfunc (f *Filter) DB() *bstore.DB {\n\treturn f.db\n}\n\n```\n# Function Signature: func (f *Filter) Save() error {\n# Natural Language Annotations: // Save stores modifications, e.g. from training, to the database and bloom\n// filter files.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (f *Filter) Save() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\tif !f.modified {\n\t\treturn nil\n\t}\n\tf.modified = false\n\n\twc := wordscore{Word: \"-\", Ham: f.hams, Spam: f.spams}\n\terr := f.db.Update(context.TODO(), func(tx *bstore.Tx) error {\n\t\tif err := tx.Put(&wc); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfor w, c := range f.changed {\n\t\t\twc = wordscore{Word: w, Ham: c.Ham, Spam: c.Spam}\n\t\t\tif err := tx.Put(&wc); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"saving ham/spam message count: %s\", err)\n\t}\n\tf.changed = map[string]word{}\n\n\tif f.isNew {\n\t\tf.log.Info(\"new filter, forcing a write of bloom and db filter\")\n\t}\n\n\terr = f.bloom.Save()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"saving bloom filter: %s\", err)\n\t}\n\treturn nil\n}\n```", "  ```\nfunc (f *Filter) Save() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\n\tif !f.modified {\n\t\treturn nil\n\t}\n\n\tif err := f.db.Txn(bnhard.Insert(wordscore{Word: \"-\", Ham: f.hams, Spam: f.spams})); err != nil {\n\t\treturn fmt.Errorf(\"marking ham/spam messages count: %s\", err)\n\t}\n\n\tfor w, c := range f.changed {\n\t\t// Exclude words that were barely touched.\n\t\tif c.Ham <= 5 && c.Spam <= 5 {\n\t\t\tdelete(f.changed, w)\n\t\t\tcontinue\n\t\t}\n\t\tf.cache[w] = c\n\t\tif err := f.db.Txn(bnhard.Insert(wordscore{Word: w, Ham: c.Ham, Spam: c.Spam})); err != nil {\n\t\t\treturn fmt.Errorf(\"storing word scores: %s\", err)\n\t\t}\n\t}\n\n\tif err := f.bloom.Save(); err != nil {\n\t\treturn fmt.Errorf(\"saving bloom filter: %w\", err)\n\t}\n\n\tf.modified = false\n\treturn nil\n}\n```", "  ```\nfunc (f *Filter) Save() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\n\t// Save bloom filter file.\n\tvar bPath string\n\tif f.isNew {\n\t\tbPath = f.bloomPath\n\t\tf.isNew = false\n\t} else {\n\t\tbPath = f.bloomPath + \".new\"\n\t}\n\terr := f.bloom.WriteFile(bPath)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"writing bloom file: %w\", err)\n\t}\n\n\tif f.isNew {\n\t\tf.log.Info(\"close ownershipp of bloom and database files\",\n\t\t\tslog.Any(\"oldbloom\", f.bloomPath),\n\t\t\tslog.Any(\"olddb\", f.dbPath))\n\t\tf.bloom = nil\n\t\tf.db = nil\n\t\tf.isNew = false\n\t\tf.log.Info(\"delete old filter file, since we are new\",\n\t\t\tslog.Any(\"oldbloom\", f.bloomPath),\n\t\t\tslog.Any(\"olddb\", f.dbPath))\n\t\terr = os.Remove(f.bloomPath)\n\t\tif err != nil {\n\t\t\tf.log.Info(\"cannot delete old bloom file\",\n\t\t\t\tslog.Any(\"oldbloom\", f.bloomPath))\n\t\t}\n\t\terr = os.Remove(f.dbPath)\n\t\tif err != nil {\n\t\t\tf.log.Info(\"cannot delete old db file\",\n\t\t\t\tslog.Any(\"olddb\", f.dbPath))\n\t\t}\n\t\tf.bloomPath = bPath\n\t}\n\n\tf.log.Info(\"save filtername\", slog.Any(\"name\", f.bloomPath))\n\n\t// Sync startup breaks the filter if we don't sync the bloom filter file first.\n\terr = f.bloom.Sync()\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = f.db.Sync()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif !f.isNew {\n\t\tf.log.Info(\"close old db, since we are not new\",\n\t\t\tslog.Any(\"olddb\", f.dbPath))\n\t\terr := f.db.Close()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tf.log.Info(\"delete old db file, since we are not new\",\n\t\t\tslog.Any(\"olddb\", f.dbPath))\n\t\terr = os.Remove(f.dbPath)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tf.log.Info(\"close old bloom, since we are not new\",\n\t\tslog.Any(\"oldbloom\", f.bloomPath))\n\tf.bloom = nil\n\n\tf.log.Info(\"close old db, since we are not new\",\n\t\tslog.Any(\"olddb\", f.dbPath))\n\tf.db = nil\n\n\treturn nil\n}\n```"]}, "mox-junk/filter.go-ClassifyMessagePath": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/junk/filter.go:\n```\n// Package junk implements a bayesian spam filter.\n//\n// A message can be parsed into words. Words (or pairs or triplets) can be used\n// to train the filter or to classify the message as ham or spam.  Training\n// records the words in the database as ham/spam. Classifying consists of\n// calculating the ham/spam probability by combining the words in the message\n// with their ham/spam status.\npackage junk\n\n// todo: look at inverse chi-square function? see https://www.linuxjournal.com/article/6467\n// todo: perhaps: whether anchor text in links in html are different from the url\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"math\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"time\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n)\n\nvar (\n\t// errBadContentType = errors.New(\"bad content-type\") // sure sign of spam, todo: use this error\n\terrClosed = errors.New(\"filter is closed\")\n)\n\ntype word struct {\n\tHam  uint32\n\tSpam uint32\n}\n\ntype wordscore struct {\n\tWord string\n\tHam  uint32\n\tSpam uint32\n}\n\n// Params holds parameters for the filter. Most are at test-time. The first are\n// used during parsing and training.\ntype Params struct {\n\tOnegrams    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for single words.\"`\n\tTwograms    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each two consecutive words.\"`\n\tThreegrams  bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each three consecutive words.\"`\n\tMaxPower    float64 `sconf-doc:\"Maximum power a word (combination) can have. If spaminess is 0.99, and max power is 0.1, spaminess of the word will be set to 0.9. Similar for ham words.\"`\n\tTopWords    int     `sconf-doc:\"Number of most spammy/hammy words to use for calculating probability. E.g. 10.\"`\n\tIgnoreWords float64 `sconf:\"optional\" sconf-doc:\"Ignore words that are this much away from 0.5 haminess/spaminess. E.g. 0.1, causing word (combinations) of 0.4 to 0.6 to be ignored.\"`\n\tRareWords   int     `sconf:\"optional\" sconf-doc:\"Occurrences in word database until a word is considered rare and its influence in calculating probability reduced. E.g. 1 or 2.\"`\n}\n\nvar DBTypes = []any{wordscore{}} // Stored in DB.\n\ntype Filter struct {\n\tParams\n\n\tlog               mlog.Log // For logging cid.\n\tclosed            bool\n\tmodified          bool            // Whether any modifications are pending. Cleared by Save.\n\thams, spams       uint32          // Message count, stored in db under word \"-\".\n\tcache             map[string]word // Words read from database or during training.\n\tchanged           map[string]word // Words modified during training.\n\tdbPath, bloomPath string\n\tdb                *bstore.DB // Always open on a filter.\n\tbloom             *Bloom     // Only opened when writing.\n\tisNew             bool       // Set for new filters until their first sync to disk. For faster writing.\n}\n\nfunc (f *Filter) ensureBloom() error {\n\tif f.bloom != nil {\n\t\treturn nil\n\t}\n\tvar err error\n\tf.bloom, err = openBloom(f.bloomPath)\n\treturn err\n}\n\n// CloseDiscard closes the filter, discarding any changes.\nfunc (f *Filter) CloseDiscard() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\terr := f.db.Close()\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\n// Close first saves the filter if it has modifications, then closes the database\n// connection and releases the bloom filter.\nfunc (f *Filter) Close() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\tvar err error\n\tif f.modified {\n\t\terr = f.Save()\n\t}\n\tif err != nil {\n\t\tf.db.Close()\n\t} else {\n\t\terr = f.db.Close()\n\t}\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\nfunc OpenFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string, loadBloom bool) (*Filter, error) {\n\tvar bloom *Bloom\n\tif loadBloom {\n\t\tvar err error\n\t\tbloom, err = openBloom(bloomPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else if fi, err := os.Stat(bloomPath); err == nil {\n\t\tif err := BloomValid(int(fi.Size()), bloomK); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"bloom: %s\", err)\n\t\t}\n\t}\n\n\tdb, err := openDB(ctx, log, dbPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open database: %s\", err)\n\t}\n\n\tf := &Filter{\n\t\tParams:    params,\n\t\tlog:       log,\n\t\tcache:     map[string]word{},\n\t\tchanged:   map[string]word{},\n\t\tdbPath:    dbPath,\n\t\tbloomPath: bloomPath,\n\t\tdb:        db,\n\t\tbloom:     bloom,\n\t}\n\terr = f.db.Read(ctx, func(tx *bstore.Tx) error {\n\t\twc := wordscore{Word: \"-\"}\n\t\terr := tx.Get(&wc)\n\t\tf.hams = wc.Ham\n\t\tf.spams = wc.Spam\n\t\treturn err\n\t})\n\tif err != nil {\n\t\tcerr := f.Close()\n\t\tlog.Check(cerr, \"closing filter after error\")\n\t\treturn nil, fmt.Errorf(\"looking up ham/spam message count: %s\", err)\n\t}\n\treturn f, nil\n}\n\n// NewFilter creates a new filter with empty bloom filter and database files. The\n// filter is marked as new until the first save, will be done automatically if\n// TrainDirs is called. If the bloom and/or database files exist, an error is\n// returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconst bloomK = 10\n\nfunc openBloom(path string) (*Bloom, error) {\n\tbuf, err := os.ReadFile(path)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading bloom file: %w\", err)\n\t}\n\treturn NewBloom(buf, bloomK)\n}\n\nfunc newDB(ctx context.Context, log mlog.Log, path string) (db *bstore.DB, rerr error) {\n\t// Remove any existing files.\n\tos.Remove(path)\n\n\tdefer func() {\n\t\tif rerr != nil {\n\t\t\terr := os.Remove(path)\n\t\t\tlog.Check(err, \"removing db file after init error\")\n\t\t}\n\t}()\n\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\tdb, err := bstore.Open(ctx, path, &opts, DBTypes...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open new database: %w\", err)\n\t}\n\treturn db, nil\n}\n\nfunc openDB(ctx context.Context, log mlog.Log, path string) (*bstore.DB, error) {\n\tif _, err := os.Stat(path); err != nil {\n\t\treturn nil, fmt.Errorf(\"stat db file: %w\", err)\n\t}\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\treturn bstore.Open(ctx, path, &opts, DBTypes...)\n}\n\n// Save stores modifications, e.g. from training, to the database and bloom\n// filter files.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc loadWords(ctx context.Context, db *bstore.DB, l []string, dst map[string]word) error {\n\tsort.Slice(l, func(i, j int) bool {\n\t\treturn l[i] < l[j]\n\t})\n\n\terr := db.Read(ctx, func(tx *bstore.Tx) error {\n\t\tfor _, w := range l {\n\t\t\twc := wordscore{Word: w}\n\t\t\tif err := tx.Get(&wc); err == nil {\n\t\t\t\tdst[w] = word{wc.Ham, wc.Spam}\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"fetching words: %s\", err)\n\t}\n\treturn nil\n}\n\n// ClassifyWords returns the spam probability for the given words, and number of recognized ham and spam words.\nfunc (f *Filter) ClassifyWords(ctx context.Context, words map[string]struct{}) (probability float64, nham, nspam int, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, 0, errClosed\n\t}\n\n\ttype xword struct {\n\t\tWord string\n\t\tR    float64\n\t}\n\n\tvar hamHigh float64 = 0\n\tvar spamLow float64 = 1\n\tvar topHam []xword\n\tvar topSpam []xword\n\n\t// Find words that should be in the database.\n\tlookupWords := []string{}\n\texpect := map[string]struct{}{}\n\tunknowns := map[string]struct{}{}\n\ttotalUnknown := 0\n\tfor w := range words {\n\t\tif f.bloom != nil && !f.bloom.Has(w) {\n\t\t\ttotalUnknown++\n\t\t\tif len(unknowns) < 50 {\n\t\t\t\tunknowns[w] = struct{}{}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; ok {\n\t\t\tcontinue\n\t\t}\n\t\tlookupWords = append(lookupWords, w)\n\t\texpect[w] = struct{}{}\n\t}\n\tif len(unknowns) > 0 {\n\t\tf.log.Debug(\"unknown words in bloom filter, showing max 50\",\n\t\t\tslog.Any(\"words\", unknowns),\n\t\t\tslog.Any(\"totalunknown\", totalUnknown),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\t// Fetch words from database.\n\tfetched := map[string]word{}\n\tif len(lookupWords) > 0 {\n\t\tif err := loadWords(ctx, f.db, lookupWords, fetched); err != nil {\n\t\t\treturn 0, 0, 0, err\n\t\t}\n\t\tfor w, c := range fetched {\n\t\t\tdelete(expect, w)\n\t\t\tf.cache[w] = c\n\t\t}\n\t\tf.log.Debug(\"unknown words in db\",\n\t\t\tslog.Any(\"words\", expect),\n\t\t\tslog.Any(\"totalunknown\", len(expect)),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tvar wS, wH float64\n\t\tif f.spams > 0 {\n\t\t\twS = float64(c.Spam) / float64(f.spams)\n\t\t}\n\t\tif f.hams > 0 {\n\t\t\twH = float64(c.Ham) / float64(f.hams)\n\t\t}\n\t\tr := wS / (wS + wH)\n\n\t\tif r < f.MaxPower {\n\t\t\tr = f.MaxPower\n\t\t} else if r >= 1-f.MaxPower {\n\t\t\tr = 1 - f.MaxPower\n\t\t}\n\n\t\tif c.Ham+c.Spam <= uint32(f.RareWords) {\n\t\t\t// Reduce the power of rare words.\n\t\t\tr += float64(1+uint32(f.RareWords)-(c.Ham+c.Spam)) * (0.5 - r) / 10\n\t\t}\n\t\tif math.Abs(0.5-r) < f.IgnoreWords {\n\t\t\tcontinue\n\t\t}\n\t\tif r < 0.5 {\n\t\t\tif len(topHam) >= f.TopWords && r > hamHigh {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopHam = append(topHam, xword{w, r})\n\t\t\tif r > hamHigh {\n\t\t\t\thamHigh = r\n\t\t\t}\n\t\t} else if r > 0.5 {\n\t\t\tif len(topSpam) >= f.TopWords && r < spamLow {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopSpam = append(topSpam, xword{w, r})\n\t\t\tif r < spamLow {\n\t\t\t\tspamLow = r\n\t\t\t}\n\t\t}\n\t}\n\n\tsort.Slice(topHam, func(i, j int) bool {\n\t\ta, b := topHam[i], topHam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R < b.R\n\t})\n\tsort.Slice(topSpam, func(i, j int) bool {\n\t\ta, b := topSpam[i], topSpam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R > b.R\n\t})\n\n\tnham = f.TopWords\n\tif nham > len(topHam) {\n\t\tnham = len(topHam)\n\t}\n\tnspam = f.TopWords\n\tif nspam > len(topSpam) {\n\t\tnspam = len(topSpam)\n\t}\n\ttopHam = topHam[:nham]\n\ttopSpam = topSpam[:nspam]\n\n\tvar eta float64\n\tfor _, x := range topHam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\tfor _, x := range topSpam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\n\tf.log.Debug(\"top words\", slog.Any(\"hams\", topHam), slog.Any(\"spams\", topSpam))\n\n\tprob := 1 / (1 + math.Pow(math.E, eta))\n\treturn prob, len(topHam), len(topSpam), nil\n}\n\n// ClassifyMessagePath is a convenience wrapper for calling ClassifyMessage on a file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) ClassifyMessageReader(ctx context.Context, mf io.ReaderAt, size int64) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tm, err := message.EnsurePart(f.log.Logger, false, mf, size)\n\tif err != nil && errors.Is(err, message.ErrBadContentType) {\n\t\t// Invalid content-type header is a sure sign of spam.\n\t\t//f.log.Infox(\"parsing content\", err)\n\t\treturn 1, nil, 0, 0, nil\n\t}\n\treturn f.ClassifyMessage(ctx, m)\n}\n\n// ClassifyMessage parses the mail message in r and returns the spam probability\n// (between 0 and 1), along with the tokenized words found in the message, and the\n// number of recognized ham and spam words.\nfunc (f *Filter) ClassifyMessage(ctx context.Context, m message.Part) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tvar err error\n\twords, err = f.ParseMessage(m)\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, err\n\t}\n\n\tprobability, nham, nspam, err = f.ClassifyWords(ctx, words)\n\treturn probability, words, nham, nspam, err\n}\n\n// Train adds the words of a single message to the filter.\nfunc (f *Filter) Train(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\tvar lwords []string\n\n\tfor w := range words {\n\t\tif !f.bloom.Has(w) {\n\t\t\tf.bloom.Add(w)\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\tf.modified = true\n\tif ham {\n\t\tf.hams++\n\t} else {\n\t\tf.spams++\n\t}\n\n\tfor w := range words {\n\t\tc := f.cache[w]\n\t\tif ham {\n\t\t\tc.Ham++\n\t\t} else {\n\t\t\tc.Spam++\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\nfunc (f *Filter) TrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Train(ctx, ham, words)\n}\n\nfunc (f *Filter) UntrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Untrain(ctx, ham, words)\n}\n\nfunc (f *Filter) loadCache(ctx context.Context, lwords []string) error {\n\tif len(lwords) == 0 {\n\t\treturn nil\n\t}\n\n\treturn loadWords(ctx, f.db, lwords, f.cache)\n}\n\n// Untrain adjusts the filter to undo a previous training of the words.\nfunc (f *Filter) Untrain(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\t// Lookup any words from the db that aren't in the cache and put them in the cache for modification.\n\tvar lwords []string\n\tfor w := range words {\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\t// Modify the message count.\n\tf.modified = true\n\tif ham {\n\t\tf.hams--\n\t} else {\n\t\tf.spams--\n\t}\n\n\t// Decrease the word counts.\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif ham {\n\t\t\tc.Ham--\n\t\t} else {\n\t\t\tc.Spam--\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\n// TrainDir parses mail messages from files and trains the filter.\nfunc (f *Filter) TrainDir(dir string, files []string, ham bool) (n, malformed uint32, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, errClosed\n\t}\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\tfor _, name := range files {\n\t\tp := filepath.Join(dir, name)\n\t\tvalid, words, err := f.tokenizeMail(p)\n\t\tif err != nil {\n\t\t\t// f.log.Infox(\"tokenizing mail\", err, slog.Any(\"path\", p))\n\t\t\tmalformed++\n\t\t\tcontinue\n\t\t}\n\t\tif !valid {\n\t\t\tcontinue\n\t\t}\n\t\tn++\n\t\tfor w := range words {\n\t\t\tif !f.bloom.Has(w) {\n\t\t\t\tf.bloom.Add(w)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc := f.cache[w]\n\t\t\tf.modified = true\n\t\t\tif ham {\n\t\t\t\tc.Ham++\n\t\t\t} else {\n\t\t\t\tc.Spam++\n\t\t\t}\n\t\t\tf.cache[w] = c\n\t\t\tf.changed[w] = c\n\t\t}\n\t}\n\treturn\n}\n\n// TrainDirs trains and saves a filter with mail messages from different types\n// of directories.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) fileSize(p string) int {\n\tfi, err := os.Stat(p)\n\tif err != nil {\n\t\tf.log.Infox(\"stat\", err, slog.Any(\"path\", p))\n\t\treturn 0\n\t}\n\treturn int(fi.Size())\n}\n\n// DB returns the database, for backups.\nfunc (f *Filter) DB() *bstore.DB {\n\treturn f.db\n}\n\n```\n# Function Signature: func (f *Filter) ClassifyMessagePath(ctx context.Context, path string) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n# Natural Language Annotations: // ClassifyMessagePath is a convenience wrapper for calling ClassifyMessage on a file.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (f *Filter) ClassifyMessagePath(ctx context.Context, path string) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tfh, err := os.Open(path)\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, err\n\t}\n\tdefer fh.Close()\n\tfi, err := fh.Stat()\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, err\n\t}\n\tprob, words, nham, nspam, err := f.ClassifyMessageReader(ctx, fh, fi.Size())\n\treturn prob, words, nham, nspam, err\n}\n```", "  ```\nfunc (f *Filter) ClassifyMessagePath(ctx context.Context, path string) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tfile, err := os.Open(path)\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, fmt.Errorf(\"opening file: %s\", err)\n\t}\n\tdefer file.Close()\n\n\tsize := f.fileSize(path)\n\tif size <= 0 {\n\t\treturn 0, nil, 0, 0, fmt.Errorf(\"file size <= 0: %s\", path)\n\t}\n\n\treturn f.ClassifyMessageReader(ctx, file, int64(size))\n}\n```", "  ```\nfunc (f *Filter) ClassifyMessagePath(ctx context.Context, path string) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tf.log.Debug(\"classifying message at path\", slog.Any(\"path\", path))\n\tff, err := os.Open(path)\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, err\n\t}\n\tdefer ff.Close()\n\tfi, err := ff.Stat()\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, err\n\t}\n\tprobability, words, nham, nspam, err = f.ClassifyMessageReader(ctx, ff, fi.Size())\n\tif err != nil {\n\t\tf.log.Info(\"classifying message\", slog.ObjectAny(\"file\", (*object)(ff)), slog.Err(err))\n\t}\n\tf.log.Debug(\"classify message\", slog.ObjectAny(\"file\", (*object)(ff)), slog.Any(\"hams\", nham), slog.Any(\"spams\", nspam), slog.Any(\"probability\", probability))\n\treturn probability, words, nham, nspam, err\n}\n\n```"]}, "mox-junk/filter.go-TrainDirs": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/junk/filter.go:\n```\n// Package junk implements a bayesian spam filter.\n//\n// A message can be parsed into words. Words (or pairs or triplets) can be used\n// to train the filter or to classify the message as ham or spam.  Training\n// records the words in the database as ham/spam. Classifying consists of\n// calculating the ham/spam probability by combining the words in the message\n// with their ham/spam status.\npackage junk\n\n// todo: look at inverse chi-square function? see https://www.linuxjournal.com/article/6467\n// todo: perhaps: whether anchor text in links in html are different from the url\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"math\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"time\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n)\n\nvar (\n\t// errBadContentType = errors.New(\"bad content-type\") // sure sign of spam, todo: use this error\n\terrClosed = errors.New(\"filter is closed\")\n)\n\ntype word struct {\n\tHam  uint32\n\tSpam uint32\n}\n\ntype wordscore struct {\n\tWord string\n\tHam  uint32\n\tSpam uint32\n}\n\n// Params holds parameters for the filter. Most are at test-time. The first are\n// used during parsing and training.\ntype Params struct {\n\tOnegrams    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for single words.\"`\n\tTwograms    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each two consecutive words.\"`\n\tThreegrams  bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each three consecutive words.\"`\n\tMaxPower    float64 `sconf-doc:\"Maximum power a word (combination) can have. If spaminess is 0.99, and max power is 0.1, spaminess of the word will be set to 0.9. Similar for ham words.\"`\n\tTopWords    int     `sconf-doc:\"Number of most spammy/hammy words to use for calculating probability. E.g. 10.\"`\n\tIgnoreWords float64 `sconf:\"optional\" sconf-doc:\"Ignore words that are this much away from 0.5 haminess/spaminess. E.g. 0.1, causing word (combinations) of 0.4 to 0.6 to be ignored.\"`\n\tRareWords   int     `sconf:\"optional\" sconf-doc:\"Occurrences in word database until a word is considered rare and its influence in calculating probability reduced. E.g. 1 or 2.\"`\n}\n\nvar DBTypes = []any{wordscore{}} // Stored in DB.\n\ntype Filter struct {\n\tParams\n\n\tlog               mlog.Log // For logging cid.\n\tclosed            bool\n\tmodified          bool            // Whether any modifications are pending. Cleared by Save.\n\thams, spams       uint32          // Message count, stored in db under word \"-\".\n\tcache             map[string]word // Words read from database or during training.\n\tchanged           map[string]word // Words modified during training.\n\tdbPath, bloomPath string\n\tdb                *bstore.DB // Always open on a filter.\n\tbloom             *Bloom     // Only opened when writing.\n\tisNew             bool       // Set for new filters until their first sync to disk. For faster writing.\n}\n\nfunc (f *Filter) ensureBloom() error {\n\tif f.bloom != nil {\n\t\treturn nil\n\t}\n\tvar err error\n\tf.bloom, err = openBloom(f.bloomPath)\n\treturn err\n}\n\n// CloseDiscard closes the filter, discarding any changes.\nfunc (f *Filter) CloseDiscard() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\terr := f.db.Close()\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\n// Close first saves the filter if it has modifications, then closes the database\n// connection and releases the bloom filter.\nfunc (f *Filter) Close() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\tvar err error\n\tif f.modified {\n\t\terr = f.Save()\n\t}\n\tif err != nil {\n\t\tf.db.Close()\n\t} else {\n\t\terr = f.db.Close()\n\t}\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\nfunc OpenFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string, loadBloom bool) (*Filter, error) {\n\tvar bloom *Bloom\n\tif loadBloom {\n\t\tvar err error\n\t\tbloom, err = openBloom(bloomPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else if fi, err := os.Stat(bloomPath); err == nil {\n\t\tif err := BloomValid(int(fi.Size()), bloomK); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"bloom: %s\", err)\n\t\t}\n\t}\n\n\tdb, err := openDB(ctx, log, dbPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open database: %s\", err)\n\t}\n\n\tf := &Filter{\n\t\tParams:    params,\n\t\tlog:       log,\n\t\tcache:     map[string]word{},\n\t\tchanged:   map[string]word{},\n\t\tdbPath:    dbPath,\n\t\tbloomPath: bloomPath,\n\t\tdb:        db,\n\t\tbloom:     bloom,\n\t}\n\terr = f.db.Read(ctx, func(tx *bstore.Tx) error {\n\t\twc := wordscore{Word: \"-\"}\n\t\terr := tx.Get(&wc)\n\t\tf.hams = wc.Ham\n\t\tf.spams = wc.Spam\n\t\treturn err\n\t})\n\tif err != nil {\n\t\tcerr := f.Close()\n\t\tlog.Check(cerr, \"closing filter after error\")\n\t\treturn nil, fmt.Errorf(\"looking up ham/spam message count: %s\", err)\n\t}\n\treturn f, nil\n}\n\n// NewFilter creates a new filter with empty bloom filter and database files. The\n// filter is marked as new until the first save, will be done automatically if\n// TrainDirs is called. If the bloom and/or database files exist, an error is\n// returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconst bloomK = 10\n\nfunc openBloom(path string) (*Bloom, error) {\n\tbuf, err := os.ReadFile(path)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading bloom file: %w\", err)\n\t}\n\treturn NewBloom(buf, bloomK)\n}\n\nfunc newDB(ctx context.Context, log mlog.Log, path string) (db *bstore.DB, rerr error) {\n\t// Remove any existing files.\n\tos.Remove(path)\n\n\tdefer func() {\n\t\tif rerr != nil {\n\t\t\terr := os.Remove(path)\n\t\t\tlog.Check(err, \"removing db file after init error\")\n\t\t}\n\t}()\n\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\tdb, err := bstore.Open(ctx, path, &opts, DBTypes...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open new database: %w\", err)\n\t}\n\treturn db, nil\n}\n\nfunc openDB(ctx context.Context, log mlog.Log, path string) (*bstore.DB, error) {\n\tif _, err := os.Stat(path); err != nil {\n\t\treturn nil, fmt.Errorf(\"stat db file: %w\", err)\n\t}\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\treturn bstore.Open(ctx, path, &opts, DBTypes...)\n}\n\n// Save stores modifications, e.g. from training, to the database and bloom\n// filter files.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc loadWords(ctx context.Context, db *bstore.DB, l []string, dst map[string]word) error {\n\tsort.Slice(l, func(i, j int) bool {\n\t\treturn l[i] < l[j]\n\t})\n\n\terr := db.Read(ctx, func(tx *bstore.Tx) error {\n\t\tfor _, w := range l {\n\t\t\twc := wordscore{Word: w}\n\t\t\tif err := tx.Get(&wc); err == nil {\n\t\t\t\tdst[w] = word{wc.Ham, wc.Spam}\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"fetching words: %s\", err)\n\t}\n\treturn nil\n}\n\n// ClassifyWords returns the spam probability for the given words, and number of recognized ham and spam words.\nfunc (f *Filter) ClassifyWords(ctx context.Context, words map[string]struct{}) (probability float64, nham, nspam int, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, 0, errClosed\n\t}\n\n\ttype xword struct {\n\t\tWord string\n\t\tR    float64\n\t}\n\n\tvar hamHigh float64 = 0\n\tvar spamLow float64 = 1\n\tvar topHam []xword\n\tvar topSpam []xword\n\n\t// Find words that should be in the database.\n\tlookupWords := []string{}\n\texpect := map[string]struct{}{}\n\tunknowns := map[string]struct{}{}\n\ttotalUnknown := 0\n\tfor w := range words {\n\t\tif f.bloom != nil && !f.bloom.Has(w) {\n\t\t\ttotalUnknown++\n\t\t\tif len(unknowns) < 50 {\n\t\t\t\tunknowns[w] = struct{}{}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; ok {\n\t\t\tcontinue\n\t\t}\n\t\tlookupWords = append(lookupWords, w)\n\t\texpect[w] = struct{}{}\n\t}\n\tif len(unknowns) > 0 {\n\t\tf.log.Debug(\"unknown words in bloom filter, showing max 50\",\n\t\t\tslog.Any(\"words\", unknowns),\n\t\t\tslog.Any(\"totalunknown\", totalUnknown),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\t// Fetch words from database.\n\tfetched := map[string]word{}\n\tif len(lookupWords) > 0 {\n\t\tif err := loadWords(ctx, f.db, lookupWords, fetched); err != nil {\n\t\t\treturn 0, 0, 0, err\n\t\t}\n\t\tfor w, c := range fetched {\n\t\t\tdelete(expect, w)\n\t\t\tf.cache[w] = c\n\t\t}\n\t\tf.log.Debug(\"unknown words in db\",\n\t\t\tslog.Any(\"words\", expect),\n\t\t\tslog.Any(\"totalunknown\", len(expect)),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tvar wS, wH float64\n\t\tif f.spams > 0 {\n\t\t\twS = float64(c.Spam) / float64(f.spams)\n\t\t}\n\t\tif f.hams > 0 {\n\t\t\twH = float64(c.Ham) / float64(f.hams)\n\t\t}\n\t\tr := wS / (wS + wH)\n\n\t\tif r < f.MaxPower {\n\t\t\tr = f.MaxPower\n\t\t} else if r >= 1-f.MaxPower {\n\t\t\tr = 1 - f.MaxPower\n\t\t}\n\n\t\tif c.Ham+c.Spam <= uint32(f.RareWords) {\n\t\t\t// Reduce the power of rare words.\n\t\t\tr += float64(1+uint32(f.RareWords)-(c.Ham+c.Spam)) * (0.5 - r) / 10\n\t\t}\n\t\tif math.Abs(0.5-r) < f.IgnoreWords {\n\t\t\tcontinue\n\t\t}\n\t\tif r < 0.5 {\n\t\t\tif len(topHam) >= f.TopWords && r > hamHigh {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopHam = append(topHam, xword{w, r})\n\t\t\tif r > hamHigh {\n\t\t\t\thamHigh = r\n\t\t\t}\n\t\t} else if r > 0.5 {\n\t\t\tif len(topSpam) >= f.TopWords && r < spamLow {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopSpam = append(topSpam, xword{w, r})\n\t\t\tif r < spamLow {\n\t\t\t\tspamLow = r\n\t\t\t}\n\t\t}\n\t}\n\n\tsort.Slice(topHam, func(i, j int) bool {\n\t\ta, b := topHam[i], topHam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R < b.R\n\t})\n\tsort.Slice(topSpam, func(i, j int) bool {\n\t\ta, b := topSpam[i], topSpam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R > b.R\n\t})\n\n\tnham = f.TopWords\n\tif nham > len(topHam) {\n\t\tnham = len(topHam)\n\t}\n\tnspam = f.TopWords\n\tif nspam > len(topSpam) {\n\t\tnspam = len(topSpam)\n\t}\n\ttopHam = topHam[:nham]\n\ttopSpam = topSpam[:nspam]\n\n\tvar eta float64\n\tfor _, x := range topHam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\tfor _, x := range topSpam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\n\tf.log.Debug(\"top words\", slog.Any(\"hams\", topHam), slog.Any(\"spams\", topSpam))\n\n\tprob := 1 / (1 + math.Pow(math.E, eta))\n\treturn prob, len(topHam), len(topSpam), nil\n}\n\n// ClassifyMessagePath is a convenience wrapper for calling ClassifyMessage on a file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) ClassifyMessageReader(ctx context.Context, mf io.ReaderAt, size int64) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tm, err := message.EnsurePart(f.log.Logger, false, mf, size)\n\tif err != nil && errors.Is(err, message.ErrBadContentType) {\n\t\t// Invalid content-type header is a sure sign of spam.\n\t\t//f.log.Infox(\"parsing content\", err)\n\t\treturn 1, nil, 0, 0, nil\n\t}\n\treturn f.ClassifyMessage(ctx, m)\n}\n\n// ClassifyMessage parses the mail message in r and returns the spam probability\n// (between 0 and 1), along with the tokenized words found in the message, and the\n// number of recognized ham and spam words.\nfunc (f *Filter) ClassifyMessage(ctx context.Context, m message.Part) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tvar err error\n\twords, err = f.ParseMessage(m)\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, err\n\t}\n\n\tprobability, nham, nspam, err = f.ClassifyWords(ctx, words)\n\treturn probability, words, nham, nspam, err\n}\n\n// Train adds the words of a single message to the filter.\nfunc (f *Filter) Train(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\tvar lwords []string\n\n\tfor w := range words {\n\t\tif !f.bloom.Has(w) {\n\t\t\tf.bloom.Add(w)\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\tf.modified = true\n\tif ham {\n\t\tf.hams++\n\t} else {\n\t\tf.spams++\n\t}\n\n\tfor w := range words {\n\t\tc := f.cache[w]\n\t\tif ham {\n\t\t\tc.Ham++\n\t\t} else {\n\t\t\tc.Spam++\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\nfunc (f *Filter) TrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Train(ctx, ham, words)\n}\n\nfunc (f *Filter) UntrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Untrain(ctx, ham, words)\n}\n\nfunc (f *Filter) loadCache(ctx context.Context, lwords []string) error {\n\tif len(lwords) == 0 {\n\t\treturn nil\n\t}\n\n\treturn loadWords(ctx, f.db, lwords, f.cache)\n}\n\n// Untrain adjusts the filter to undo a previous training of the words.\nfunc (f *Filter) Untrain(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\t// Lookup any words from the db that aren't in the cache and put them in the cache for modification.\n\tvar lwords []string\n\tfor w := range words {\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\t// Modify the message count.\n\tf.modified = true\n\tif ham {\n\t\tf.hams--\n\t} else {\n\t\tf.spams--\n\t}\n\n\t// Decrease the word counts.\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif ham {\n\t\t\tc.Ham--\n\t\t} else {\n\t\t\tc.Spam--\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\n// TrainDir parses mail messages from files and trains the filter.\nfunc (f *Filter) TrainDir(dir string, files []string, ham bool) (n, malformed uint32, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, errClosed\n\t}\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\tfor _, name := range files {\n\t\tp := filepath.Join(dir, name)\n\t\tvalid, words, err := f.tokenizeMail(p)\n\t\tif err != nil {\n\t\t\t// f.log.Infox(\"tokenizing mail\", err, slog.Any(\"path\", p))\n\t\t\tmalformed++\n\t\t\tcontinue\n\t\t}\n\t\tif !valid {\n\t\t\tcontinue\n\t\t}\n\t\tn++\n\t\tfor w := range words {\n\t\t\tif !f.bloom.Has(w) {\n\t\t\t\tf.bloom.Add(w)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc := f.cache[w]\n\t\t\tf.modified = true\n\t\t\tif ham {\n\t\t\t\tc.Ham++\n\t\t\t} else {\n\t\t\t\tc.Spam++\n\t\t\t}\n\t\t\tf.cache[w] = c\n\t\t\tf.changed[w] = c\n\t\t}\n\t}\n\treturn\n}\n\n// TrainDirs trains and saves a filter with mail messages from different types\n// of directories.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) fileSize(p string) int {\n\tfi, err := os.Stat(p)\n\tif err != nil {\n\t\tf.log.Infox(\"stat\", err, slog.Any(\"path\", p))\n\t\treturn 0\n\t}\n\treturn int(fi.Size())\n}\n\n// DB returns the database, for backups.\nfunc (f *Filter) DB() *bstore.DB {\n\treturn f.db\n}\n\n```\n# Function Signature: func (f *Filter) TrainDirs(hamDir, sentDir, spamDir string, hamFiles, sentFiles, spamFiles []string) error {\n# Natural Language Annotations: // TrainDirs trains and saves a filter with mail messages from different types\n// of directories.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (f *Filter) TrainDirs(hamDir, sentDir, spamDir string, hamFiles, sentFiles, spamFiles []string) error {\n\tvar hams, sent, spam uint32\n\tvar hammalformed, sentmalformed, spammalformed uint32\n\tif err := f.Save(); err != nil {\n\t\treturn err\n\t}\n\tif p := hamDir; p != \"\" {\n\t\tham, hmm, err := f.TrainDir(p, hamFiles, true)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\thammalformed = hmm\n\t\thams += ham\n\t}\n\tif p := sentDir; p != \"\" {\n\t\tham, hmm, err := f.TrainDir(p, sentFiles, false)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsentmalformed = hmm\n\t\tsent += ham\n\t}\n\tif p := spamDir; p != \"\" {\n\t\tham, hmm, err := f.TrainDir(p, spamFiles, false)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tspammalformed = hmm\n\t\tspam += ham\n\t}\n\tf.log.Infow(\"training directories\",\n\t\tslog.F(\"hams\", hams),\n\t\tslog.F(\"sent\", sent),\n\t\tslog.F(\"spam\", spam),\n\t\tslog.F(\"hammalformed\", hammalformed),\n\t\tslog.F(\"sentmalformed\", sentmalformed),\n\t\tslog.F(\"spammalformed\", spammalformed))\n\treturn f.Save()\n}\n```", "  ```\nfunc (f *Filter) TrainDirs(hamDir, sentDir, spamDir string, hamFiles, sentFiles, spamFiles []string) error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\ttype fileJob struct {\n\t\tdir string\n\t\tfiles []string\n\t}\n\n\tdirs := []fileJob{\n\t\t{hamDir, hamFiles},\n\t\t{sentDir, sentFiles},\n\t\t{spamDir, spamFiles},\n\t}\n\n\tvar wg sync.WaitGroup\n\n\tfor _, dir := range dirs {\n\t\tgo func(d string, files []string) {\n\t\t\tn, malformed, err := f.TrainDir(d, files, false)\n\t\t\tf.log.Infox(\"Training Dir\",\n\t\t\t\tslog.Any(\"dir\", d),\n\t\t\t\tslog.Any(\"files\", len(files)),\n\t\t\t\tslog.Any(\"trained\", n),\n\t\t\t\tslog.Any(\"malformed\", malformed),\n\t\t\t\tslog.Err(err))\n\t\t\twg.Done()\n\t\t}(dir.dir, dir.files)\n\t\twg.Add(1)\n\t}\n\n\twg.Wait()\n\n\tif f.modified {\n\t\tif err := f.BuildDB(hamDir, sentDir, spamDir); err != nil {\n\t\t\treturn fmt.Errorf(\"building db: %w\", err)\n\t\t}\n\t}\n\n\tif err := f.saveBloom(); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n```", "  ```\nfunc (f *Filter) TrainDirs(hamDir, sentDir, spamDir string, hamFiles, sentFiles, spamFiles []string) error {\n\thamCtx := context.Background()\n\tsentCtx := context.Background()\n\tspamCtx := context.Background()\n\n\thamN, sentN, spamN, hamM, sentM, spamM, err := f.TrainDirsContext(hamCtx, sentCtx, spamCtx, hamDir, sentDir, spamDir, hamFiles, sentFiles, spamFiles)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif hamN > 0 || sentN > 0 || spamN > 0 || hamM > 0 || sentM > 0 || spamM > 0 {\n\t\tif err := f.Save(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (f *Filter) TrainDirsContext(hamCtx, sentCtx, spamCtx context.Context, hamDir, sentDir, spamDir string, hamFiles, sentFiles, spamFiles []string) (hamN, sentN, spanN uint32, hamM, sentM, spamM uint32, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, 0, 0, 0, 0, errClosed\n\t}\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn 0, 0, 0, 0, 0, 0, err\n\t}\n\tvar hamP, sentP, spamP *io.SectionReader\n\tvar hamT, sentT, spamT tokenize.Tokens\n\n\tsizeHamDir := f.fileSize(hamDir)\n\tsizeSentDir := f.fileSize(sentDir)\n\tsizeSpamDir := f.fileSize(spamDir)\n\n\ttStart := time.Now()\n\tdefer func() {\n\t\tdur := time.Since(tStart)\n\t\tf.log.Infof(\"tokenized ham dir:%.1fms\", dur.Milliseconds())\n\t}()\n\n\tfor _, name := range hamFiles {\n\t\tp := filepath.Join(hamDir, name)\n\n\t\thamT, err := f.TokenizeMail(name, hamP)\n\t\tif err != nil {\n\t\t\treturn 0, 0, 0, 0, 0, 0, formatError(err, name)\n\t\t}\n\t\tif hamT == nil {\n\t\t\therr := f.UntrainDir(hamCtx, p)\n\t\t\tif herr != nil {\n\t\t\t\tf.log.Info(herr)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\thamN++\n\t\terr = f.trainWords(hamCtx, hamP, hamT)\n\t\tif err != nil {\n\t\t\treturn 0, 0, 0, 0, 0, 0, formatError(err, name)\n\t\t}\n\t}\n\n\ttStart = time.Now()\n\tdefer func() {\n\t\tdur := time.Since(tStart)\n\t\tf.log.Infof(\"tokenized sent dir:%.1fms\", dur.Milliseconds())\n\t}()\n\n\tfor _, name := range sentFiles {\n\t\tp := filepath.Join(sentDir, name)\n\n\t\tsentT, err := f.TokenizeMail(name, sentP)\n\t\tif err != nil {\n\t\t\treturn 0, 0, 0, 0, 0, 0, formatError(err, name)\n\t\t}\n\t\tif sentT == nil {\n\t\t\tserr := f.UntrainDir(sentCtx, p)\n\t\t\tif serr != nil {\n\t\t\t\tf.log.Info(serr)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\tsentN++\n\t\terr = f.trainWords(sentCtx, sentP, sentT)\n\t\tif err != nil {\n\t\t\treturn 0, 0, 0, 0, 0"]}, "mox-webmail/message.go-parseListPostAddress": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/webmail/message.go:\n```\npackage webmail\n\nimport (\n\t\"bufio\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"net/url\"\n\t\"strings\"\n\n\t\"golang.org/x/text/encoding/ianaindex\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/smtp\"\n\t\"github.com/mjl-/mox/store\"\n)\n\n// todo: we should have all needed information for messageItem in store.Message (perhaps some data in message.Part) for fast access, not having to parse the on-disk message file.\n\nvar wordDecoder = mime.WordDecoder{\n\tCharsetReader: func(charset string, r io.Reader) (io.Reader, error) {\n\t\tswitch strings.ToLower(charset) {\n\t\tcase \"\", \"us-ascii\", \"utf-8\":\n\t\t\treturn r, nil\n\t\t}\n\t\tenc, _ := ianaindex.MIME.Encoding(charset)\n\t\tif enc == nil {\n\t\t\tenc, _ = ianaindex.IANA.Encoding(charset)\n\t\t}\n\t\tif enc == nil {\n\t\t\treturn r, fmt.Errorf(\"unknown charset %q\", charset)\n\t\t}\n\t\treturn enc.NewDecoder().Reader(r), nil\n\t},\n}\n\n// Attempt q/b-word-decode name, coming from Content-Type \"name\" field or\n// Content-Disposition \"filename\" field.\n//\n// RFC 2231 specify an encoding for non-ascii values in mime header parameters. But\n// it appears common practice to instead just q/b-word encode the values.\n// Thunderbird and gmail.com do this for the Content-Type \"name\" parameter.\n// gmail.com also does that for the Content-Disposition \"filename\" parameter, where\n// Thunderbird uses the RFC 2231-defined encoding. Go's mime.ParseMediaType parses\n// the mechanism specified in RFC 2231 only. The value for \"name\" we get here would\n// already be decoded properly for standards-compliant headers, like\n// \"filename*0*=UTF-8\u201d%...; filename*1*=%.... We'll look for Q/B-word encoding\n// markers (\"=?\"-prefix or \"?=\"-suffix) and try to decode if present. This would\n// only cause trouble for filenames having this prefix/suffix.\nfunc tryDecodeParam(log mlog.Log, name string) string {\n\tif name == \"\" || !strings.HasPrefix(name, \"=?\") && !strings.HasSuffix(name, \"?=\") {\n\t\treturn name\n\t}\n\t// todo: find where this is allowed. it seems quite common. perhaps we should remove the pedantic check?\n\tif mox.Pedantic {\n\t\tlog.Debug(\"attachment contains rfc2047 q/b-word-encoded mime parameter instead of rfc2231-encoded\", slog.String(\"name\", name))\n\t\treturn name\n\t}\n\ts, err := wordDecoder.DecodeHeader(name)\n\tif err != nil {\n\t\tlog.Debugx(\"q/b-word decoding mime parameter\", err, slog.String(\"name\", name))\n\t\treturn name\n\t}\n\treturn s\n}\n\n// todo: mime.FormatMediaType does not wrap long lines. should do it ourselves, and split header into several parts (if commonly supported).\n\nfunc messageItem(log mlog.Log, m store.Message, state *msgState) (MessageItem, error) {\n\tpm, err := parsedMessage(log, m, state, false, true)\n\tif err != nil {\n\t\treturn MessageItem{}, fmt.Errorf(\"parsing message %d for item: %v\", m.ID, err)\n\t}\n\t// Clear largish unused data.\n\tm.MsgPrefix = nil\n\tm.ParsedBuf = nil\n\treturn MessageItem{m, pm.envelope, pm.attachments, pm.isSigned, pm.isEncrypted, pm.firstLine, true}, nil\n}\n\n// formatFirstLine returns a line the client can display next to the subject line\n// in a mailbox. It will replace quoted text, and any prefixing \"On ... write:\"\n// line with \"[...]\" so only new and useful information will be displayed.\n// Trailing signatures are not included.\nfunc formatFirstLine(r io.Reader) (string, error) {\n\t// We look quite a bit of lines ahead for trailing signatures with trailing empty lines.\n\tvar lines []string\n\tscanner := bufio.NewScanner(r)\n\tensureLines := func() {\n\t\tfor len(lines) < 10 && scanner.Scan() {\n\t\t\tlines = append(lines, strings.TrimSpace(scanner.Text()))\n\t\t}\n\t}\n\tensureLines()\n\n\tisSnipped := func(s string) bool {\n\t\treturn s == \"[...]\" || s == \"[\u2026]\" || s == \"...\"\n\t}\n\n\tnextLineQuoted := func(i int) bool {\n\t\tif i+1 < len(lines) && lines[i+1] == \"\" {\n\t\t\ti++\n\t\t}\n\t\treturn i+1 < len(lines) && (strings.HasPrefix(lines[i+1], \">\") || isSnipped(lines[i+1]))\n\t}\n\n\t// Remainder is signature if we see a line with only and minimum 2 dashes, and\n\t// there are no more empty lines, and there aren't more than 5 lines left.\n\tisSignature := func() bool {\n\t\tif len(lines) == 0 || !strings.HasPrefix(lines[0], \"--\") || strings.Trim(strings.TrimSpace(lines[0]), \"-\") != \"\" {\n\t\t\treturn false\n\t\t}\n\t\tl := lines[1:]\n\t\tfor len(l) > 0 && l[len(l)-1] == \"\" {\n\t\t\tl = l[:len(l)-1]\n\t\t}\n\t\tif len(l) >= 5 {\n\t\t\treturn false\n\t\t}\n\t\tfor _, line := range l {\n\t\t\tif line == \"\" {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\treturn true\n\t}\n\n\tresult := \"\"\n\n\tresultSnipped := func() bool {\n\t\treturn strings.HasSuffix(result, \"[...]\\n\") || strings.HasSuffix(result, \"[\u2026]\")\n\t}\n\n\t// Quick check for initial wrapped \"On ... wrote:\" line.\n\tif len(lines) > 3 && strings.HasPrefix(lines[0], \"On \") && !strings.HasSuffix(lines[0], \"wrote:\") && strings.HasSuffix(lines[1], \":\") && nextLineQuoted(1) {\n\t\tresult = \"[...]\\n\"\n\t\tlines = lines[3:]\n\t\tensureLines()\n\t}\n\n\tfor ; len(lines) > 0 && !isSignature(); ensureLines() {\n\t\tline := lines[0]\n\t\tif strings.HasPrefix(line, \">\") {\n\t\t\tif !resultSnipped() {\n\t\t\t\tresult += \"[...]\\n\"\n\t\t\t}\n\t\t\tlines = lines[1:]\n\t\t\tcontinue\n\t\t}\n\t\tif line == \"\" {\n\t\t\tlines = lines[1:]\n\t\t\tcontinue\n\t\t}\n\t\t// Check for a \"On <date>, <person> wrote:\", we require digits before a quoted\n\t\t// line, with an optional empty line in between. If we don't have any text yet, we\n\t\t// don't require the digits.\n\t\tif strings.HasSuffix(line, \":\") && (strings.ContainsAny(line, \"0123456789\") || result == \"\") && nextLineQuoted(0) {\n\t\t\tif !resultSnipped() {\n\t\t\t\tresult += \"[...]\\n\"\n\t\t\t}\n\t\t\tlines = lines[1:]\n\t\t\tcontinue\n\t\t}\n\t\t// Skip possibly duplicate snipping by author.\n\t\tif !isSnipped(line) || !resultSnipped() {\n\t\t\tresult += line + \"\\n\"\n\t\t}\n\t\tlines = lines[1:]\n\t\tif len(result) > 250 {\n\t\t\tbreak\n\t\t}\n\t}\n\tif len(result) > 250 {\n\t\tresult = result[:230] + \"...\"\n\t}\n\treturn result, scanner.Err()\n}\n\nfunc parsedMessage(log mlog.Log, m store.Message, state *msgState, full, msgitem bool) (pm ParsedMessage, rerr error) {\n\tif full || msgitem {\n\t\tif !state.ensurePart(m, true) {\n\t\t\treturn pm, state.err\n\t\t}\n\t\tif full {\n\t\t\tpm.Part = *state.part\n\t\t}\n\t} else {\n\t\tif !state.ensurePart(m, false) {\n\t\t\treturn pm, state.err\n\t\t}\n\t}\n\n\t// todo: we should store this form in message.Part, requires a data structure update.\n\n\tconvertAddrs := func(l []message.Address) []MessageAddress {\n\t\tr := make([]MessageAddress, len(l))\n\t\tfor i, a := range l {\n\t\t\td, err := dns.ParseDomain(a.Host)\n\t\t\tlog.Check(err, \"parsing domain\")\n\t\t\tif err != nil {\n\t\t\t\td = dns.Domain{ASCII: a.Host}\n\t\t\t}\n\t\t\tr[i] = MessageAddress{a.Name, a.User, d}\n\t\t}\n\t\treturn r\n\t}\n\n\tif full || msgitem {\n\t\tenv := MessageEnvelope{}\n\t\tif state.part.Envelope != nil {\n\t\t\te := *state.part.Envelope\n\t\t\tenv.Date = e.Date\n\t\t\tenv.Subject = e.Subject\n\t\t\tenv.InReplyTo = e.InReplyTo\n\t\t\tenv.MessageID = e.MessageID\n\t\t\tenv.From = convertAddrs(e.From)\n\t\t\tenv.Sender = convertAddrs(e.Sender)\n\t\t\tenv.ReplyTo = convertAddrs(e.ReplyTo)\n\t\t\tenv.To = convertAddrs(e.To)\n\t\t\tenv.CC = convertAddrs(e.CC)\n\t\t\tenv.BCC = convertAddrs(e.BCC)\n\t\t}\n\t\tpm.envelope = env\n\t}\n\n\tif full && state.part.BodyOffset > 0 {\n\t\thdrs, err := state.part.Header()\n\t\tif err != nil {\n\t\t\treturn ParsedMessage{}, fmt.Errorf(\"parsing headers: %v\", err)\n\t\t}\n\t\tpm.Headers = hdrs\n\n\t\tpm.ListReplyAddress = parseListPostAddress(hdrs.Get(\"List-Post\"))\n\t} else {\n\t\tpm.Headers = map[string][]string{}\n\t}\n\n\tpm.Texts = []string{}\n\n\t// We track attachments from multipart/mixed differently from other attachments.\n\t// The others are often inline, sometimes just some logo's in HTML alternative\n\t// messages. We want to have our mixed attachments at the start of the list, but\n\t// our descent-first parsing would result in inline messages first in the typical\n\t// message.\n\tvar attachmentsMixed []Attachment\n\tvar attachmentsOther []Attachment\n\n\taddAttachment := func(a Attachment, isMixed bool) {\n\t\tif isMixed {\n\t\t\tattachmentsMixed = append(attachmentsMixed, a)\n\t\t} else {\n\t\t\tattachmentsOther = append(attachmentsOther, a)\n\t\t}\n\t}\n\n\t// todo: how should we handle messages where a user prefers html, and we want to show it, but it's a DSN that also has textual-only parts? e.g. gmail's dsn where the first part is multipart/related with multipart/alternative, and second part is the regular message/delivery-status. we want to display both the html and the text.\n\n\tvar usePart func(p message.Part, index int, parent *message.Part, path []int, parentMixed bool)\n\tusePart = func(p message.Part, index int, parent *message.Part, path []int, parentMixed bool) {\n\t\tmt := p.MediaType + \"/\" + p.MediaSubType\n\t\tnewParentMixed := mt == \"MULTIPART/MIXED\"\n\t\tfor i, sp := range p.Parts {\n\t\t\tif mt == \"MULTIPART/SIGNED\" && i >= 1 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tusePart(sp, i, &p, append(append([]int{}, path...), i), newParentMixed)\n\t\t}\n\t\tswitch mt {\n\t\tcase \"TEXT/PLAIN\", \"/\":\n\t\t\t// Don't include if Content-Disposition attachment.\n\t\t\tif full || msgitem {\n\t\t\t\t// todo: should have this, and perhaps all content-* headers, preparsed in message.Part?\n\t\t\t\th, err := p.Header()\n\t\t\t\tlog.Check(err, \"parsing attachment headers\", slog.Int64(\"msgid\", m.ID))\n\t\t\t\tcp := h.Get(\"Content-Disposition\")\n\t\t\t\tif cp != \"\" {\n\t\t\t\t\tdisp, params, err := mime.ParseMediaType(cp)\n\t\t\t\t\tlog.Check(err, \"parsing content-disposition\", slog.String(\"cp\", cp))\n\t\t\t\t\tif strings.EqualFold(disp, \"attachment\") {\n\t\t\t\t\t\tname := tryDecodeParam(log, p.ContentTypeParams[\"name\"])\n\t\t\t\t\t\tif name == \"\" {\n\t\t\t\t\t\t\tname = tryDecodeParam(log, params[\"filename\"])\n\t\t\t\t\t\t}\n\t\t\t\t\t\taddAttachment(Attachment{path, name, p}, parentMixed)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif full {\n\t\t\t\tbuf, err := io.ReadAll(&moxio.LimitReader{R: p.ReaderUTF8OrBinary(), Limit: 2 * 1024 * 1024})\n\t\t\t\tif err != nil {\n\t\t\t\t\trerr = fmt.Errorf(\"reading text part: %v\", err)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tpm.Texts = append(pm.Texts, string(buf))\n\t\t\t}\n\t\t\tif msgitem && pm.firstLine == \"\" {\n\t\t\t\tpm.firstLine, rerr = formatFirstLine(p.ReaderUTF8OrBinary())\n\t\t\t\tif rerr != nil {\n\t\t\t\t\trerr = fmt.Errorf(\"reading text for first line snippet: %v\", rerr)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase \"TEXT/HTML\":\n\t\t\tpm.HasHTML = true\n\n\t\tdefault:\n\t\t\t// todo: see if there is a common nesting messages that are both signed and encrypted.\n\t\t\tif parent == nil && mt == \"MULTIPART/SIGNED\" {\n\t\t\t\tpm.isSigned = true\n\t\t\t}\n\t\t\tif parent == nil && mt == \"MULTIPART/ENCRYPTED\" {\n\t\t\t\tpm.isEncrypted = true\n\t\t\t}\n\t\t\t// todo: possibly do not include anything below multipart/alternative that starts with text/html, they may be cids. perhaps have a separate list of attachments for the text vs html version?\n\t\t\tif p.MediaType != \"MULTIPART\" {\n\t\t\t\tvar parentct string\n\t\t\t\tif parent != nil {\n\t\t\t\t\tparentct = parent.MediaType + \"/\" + parent.MediaSubType\n\t\t\t\t}\n\n\t\t\t\t// Recognize DSNs.\n\t\t\t\tif parentct == \"MULTIPART/REPORT\" && index == 1 && (mt == \"MESSAGE/GLOBAL-DELIVERY-STATUS\" || mt == \"MESSAGE/DELIVERY-STATUS\") {\n\t\t\t\t\tif full {\n\t\t\t\t\t\tbuf, err := io.ReadAll(&moxio.LimitReader{R: p.ReaderUTF8OrBinary(), Limit: 1024 * 1024})\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\trerr = fmt.Errorf(\"reading text part: %v\", err)\n\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t}\n\t\t\t\t\t\tpm.Texts = append(pm.Texts, string(buf))\n\t\t\t\t\t}\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif parentct == \"MULTIPART/REPORT\" && index == 2 && (mt == \"MESSAGE/GLOBAL-HEADERS\" || mt == \"TEXT/RFC822-HEADERS\") {\n\t\t\t\t\tif full {\n\t\t\t\t\t\tbuf, err := io.ReadAll(&moxio.LimitReader{R: p.ReaderUTF8OrBinary(), Limit: 1024 * 1024})\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\trerr = fmt.Errorf(\"reading text part: %v\", err)\n\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t}\n\t\t\t\t\t\tpm.Texts = append(pm.Texts, string(buf))\n\t\t\t\t\t}\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif parentct == \"MULTIPART/REPORT\" && index == 2 && (mt == \"MESSAGE/GLOBAL\" || mt == \"TEXT/RFC822\") {\n\t\t\t\t\taddAttachment(Attachment{path, \"original.eml\", p}, parentMixed)\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tname := tryDecodeParam(log, p.ContentTypeParams[\"name\"])\n\t\t\t\tif name == \"\" && (full || msgitem) {\n\t\t\t\t\t// todo: should have this, and perhaps all content-* headers, preparsed in message.Part?\n\t\t\t\t\th, err := p.Header()\n\t\t\t\t\tlog.Check(err, \"parsing attachment headers\", slog.Int64(\"msgid\", m.ID))\n\t\t\t\t\tcp := h.Get(\"Content-Disposition\")\n\t\t\t\t\tif cp != \"\" {\n\t\t\t\t\t\t_, params, err := mime.ParseMediaType(cp)\n\t\t\t\t\t\tlog.Check(err, \"parsing content-disposition\", slog.String(\"cp\", cp))\n\t\t\t\t\t\tname = tryDecodeParam(log, params[\"filename\"])\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\taddAttachment(Attachment{path, name, p}, parentMixed)\n\t\t\t}\n\t\t}\n\t}\n\tusePart(*state.part, -1, nil, []int{}, false)\n\n\tpm.attachments = []Attachment{}\n\tpm.attachments = append(pm.attachments, attachmentsMixed...)\n\tpm.attachments = append(pm.attachments, attachmentsOther...)\n\n\tif rerr == nil {\n\t\tpm.ID = m.ID\n\t}\n\treturn\n}\n\n// parses List-Post header, returning an address if it could be found, and nil otherwise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func parseListPostAddress(s string) *MessageAddress {\n# Natural Language Annotations: // parses List-Post header, returning an address if it could be found, and nil otherwise.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nparsedListPostAddress(s string) (*MessageAddress, error) {\n\tif s == \"\" {\n\t\treturn nil, nil\n\t}\n\t// a list-post header should look like \"List-Post: <mailto:address@example.com>\"\n\tif !strings.HasPrefix(s, \"List-Post: <\") || !strings.HasSuffix(s, \">\") {\n\t\treturn nil, fmt.Errorf(\"unexpected format for list-post header: %q\", s)\n\t}\n\tu, err := url.Parse(s[len(\"List-Post: \"):])\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"parsing address from list-post header %q: %v\", s, err)\n\t}\n\ta, err := mime.ParseAddress(u.String())\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"parsing address from list-post header %q: %v\", s, err)\n\t}\n\treturn &MessageAddress{Name: a.Name, User: a.Address}, nil\n}\n```", "  ```\nfunc parseListPostAddress(s string) *MessageAddress {\n\tif s == \"\" {\n\t\treturn nil\n\t}\n\tvar u url.URL\n\tif err := u.Parse(s); err != nil {\n\t\treturn nil\n\t}\n\tm, err := mlog.ParseHostAndPort(u.Host)\n\tif err != nil {\n\t\treturn nil\n\t}\n\tif m.Port != 21 && m.Port != 25 {\n\t\treturn nil\n\t}\n\treturn &MessageAddress{\n\t\tName:      u.User.Username(),\n\t\tUser:      u.User.Username(),\n\t\tHost:      MessageAddressHost{Name: m.Name, Port: m.Port, IP: []byte{}},\n\t}\n}\n```", "  ```\npackage webmail\n\nimport (\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"golang.org/x/text/encoding/ianaindex\"\n\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/smtp\"\n\t\"github.com/mjl-/mox/store\"\n)\n\nvar (\n\tmailboxURLRe *regexp.Regexp\n)\n\nfunc init() {\n\tconst mailboxURLFmt = \"https?://mail\\\\.([^/]+)/msg/([^/#]+)\"\n\tmailboxURLRe = regexp.MustCompile(mailboxURLFmt)\n}\n\nfunc parseListPostAddress(s string) *MessageAddress {\n\tif s == \"\" {\n\t\treturn nil\n\t}\n\tif s[0] == '<' && s[len(s)-1] == '>' {\n\t\ts = s[1 : len(s)-1]\n\t}\n\n\t// todo: This currently only handles a few, simple cases of List-Post. We should consider supporting more complex cases, especially in cases where the quoted part is split across different parts of the List-Post header.\n\tif litMatch := mailboxURLRe.FindStringSubmatch(s); len(litMatch) == 3 {\n\t\thost, _ := url.Parse(litMatch[1])\n\t\tparts := strings.Split(litMatch[2], \"-\")\n\t\tid, err := strconv.ParseInt(parts[len(parts)-1], 10, 64)\n\t\tif err != nil {\n\t\t\treturn nil\n\t\t}\n\t\treturn &MessageAddress{DNS: host, MsgID: id}\n\t}\n\treturn nil\n}\n```"]}, "mox-webmail/webmail.go-sanitizeNode": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/webmail/webmail.go:\n```\n// Package webmail implements a webmail client, serving html/js and providing an API for message actions and SSE endpoint for receiving real-time updates.\npackage webmail\n\n// todo: should we be serving the messages/parts on a separate (sub)domain for user-content? to limit damage if the csp rules aren't enough.\n\nimport (\n\t\"archive/zip\"\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"net/http\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"runtime/debug\"\n\t\"strconv\"\n\t\"strings\"\n\n\t_ \"embed\"\n\n\t\"golang.org/x/net/html\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/bstore\"\n\t\"github.com/mjl-/sherpa\"\n\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/metrics\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/store\"\n\t\"github.com/mjl-/mox/webauth\"\n\t\"github.com/mjl-/mox/webops\"\n)\n\nvar pkglog = mlog.New(\"webmail\", nil)\n\ntype ctxKey string\n\n// We pass the request to the sherpa handler so the TLS info can be used for\n// the Received header in submitted messages. Most API calls need just the\n// account name.\nvar requestInfoCtxKey ctxKey = \"requestInfo\"\n\ntype requestInfo struct {\n\tLog          mlog.Log\n\tLoginAddress string\n\tAccount      *store.Account // Nil only for methods Login and LoginPrep.\n\tSessionToken store.SessionToken\n\tResponse     http.ResponseWriter\n\tRequest      *http.Request // For Proto and TLS connection state during message submit.\n}\n\n//go:embed webmail.html\nvar webmailHTML []byte\n\n//go:embed webmail.js\nvar webmailJS []byte\n\n//go:embed msg.html\nvar webmailmsgHTML []byte\n\n//go:embed msg.js\nvar webmailmsgJS []byte\n\n//go:embed text.html\nvar webmailtextHTML []byte\n\n//go:embed text.js\nvar webmailtextJS []byte\n\nvar (\n\t// Similar between ../webmail/webmail.go:/metricSubmission and ../smtpserver/server.go:/metricSubmission and ../webapisrv/server.go:/metricSubmission\n\tmetricSubmission = promauto.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_webmail_submission_total\",\n\t\t\tHelp: \"Webmail message submission results, known values (those ending with error are server errors): ok, badfrom, messagelimiterror, recipientlimiterror, queueerror, storesenterror.\",\n\t\t},\n\t\t[]string{\n\t\t\t\"result\",\n\t\t},\n\t)\n\tmetricServerErrors = promauto.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_webmail_errors_total\",\n\t\t\tHelp: \"Webmail server errors, known values: dkimsign, submit.\",\n\t\t},\n\t\t[]string{\n\t\t\t\"error\",\n\t\t},\n\t)\n\tmetricSSEConnections = promauto.NewGauge(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"mox_webmail_sse_connections\",\n\t\t\tHelp: \"Number of active webmail SSE connections.\",\n\t\t},\n\t)\n)\n\nfunc xcheckf(ctx context.Context, err error, format string, args ...any) {\n\tif err == nil {\n\t\treturn\n\t}\n\tmsg := fmt.Sprintf(format, args...)\n\terrmsg := fmt.Sprintf(\"%s: %s\", msg, err)\n\tpkglog.WithContext(ctx).Errorx(msg, err)\n\tcode := \"server:error\"\n\tif errors.Is(err, context.Canceled) || errors.Is(err, context.DeadlineExceeded) {\n\t\tcode = \"user:error\"\n\t}\n\tpanic(&sherpa.Error{Code: code, Message: errmsg})\n}\n\nfunc xcheckuserf(ctx context.Context, err error, format string, args ...any) {\n\tif err == nil {\n\t\treturn\n\t}\n\tmsg := fmt.Sprintf(format, args...)\n\terrmsg := fmt.Sprintf(\"%s: %s\", msg, err)\n\tpkglog.WithContext(ctx).Errorx(msg, err)\n\tpanic(&sherpa.Error{Code: \"user:error\", Message: errmsg})\n}\n\nfunc xdbwrite(ctx context.Context, acc *store.Account, fn func(tx *bstore.Tx)) {\n\terr := acc.DB.Write(ctx, func(tx *bstore.Tx) error {\n\t\tfn(tx)\n\t\treturn nil\n\t})\n\txcheckf(ctx, err, \"transaction\")\n}\n\nfunc xdbread(ctx context.Context, acc *store.Account, fn func(tx *bstore.Tx)) {\n\terr := acc.DB.Read(ctx, func(tx *bstore.Tx) error {\n\t\tfn(tx)\n\t\treturn nil\n\t})\n\txcheckf(ctx, err, \"transaction\")\n}\n\nvar webmailFile = &mox.WebappFile{\n\tHTML:     webmailHTML,\n\tJS:       webmailJS,\n\tHTMLPath: filepath.FromSlash(\"webmail/webmail.html\"),\n\tJSPath:   filepath.FromSlash(\"webmail/webmail.js\"),\n}\n\n// Serve content, either from a file, or return the fallback data. Caller\n// should already have set the content-type. We use this to return a file from\n// the local file system (during development), or embedded in the binary (when\n// deployed).\nfunc serveContentFallback(log mlog.Log, w http.ResponseWriter, r *http.Request, path string, fallback []byte) {\n\tf, err := os.Open(path)\n\tif err == nil {\n\t\tdefer f.Close()\n\t\tst, err := f.Stat()\n\t\tif err == nil {\n\t\t\thttp.ServeContent(w, r, \"\", st.ModTime(), f)\n\t\t\treturn\n\t\t}\n\t}\n\thttp.ServeContent(w, r, \"\", mox.FallbackMtime(log), bytes.NewReader(fallback))\n}\n\nfunc init() {\n\tmox.NewWebmailHandler = func(maxMsgSize int64, basePath string, isForwarded bool, accountPath string) http.Handler {\n\t\treturn http.HandlerFunc(Handler(maxMsgSize, basePath, isForwarded, accountPath))\n\t}\n}\n\n// Handler returns a handler for the webmail endpoints, customized for the max\n// message size coming from the listener and cookiePath.\nfunc Handler(maxMessageSize int64, cookiePath string, isForwarded bool, accountPath string) func(w http.ResponseWriter, r *http.Request) {\n\tsh, err := makeSherpaHandler(maxMessageSize, cookiePath, isForwarded)\n\treturn func(w http.ResponseWriter, r *http.Request) {\n\t\tif err != nil {\n\t\t\thttp.Error(w, \"500 - internal server error - cannot handle requests\", http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t\thandle(sh, isForwarded, accountPath, w, r)\n\t}\n}\n\nfunc handle(apiHandler http.Handler, isForwarded bool, accountPath string, w http.ResponseWriter, r *http.Request) {\n\tctx := r.Context()\n\tlog := pkglog.WithContext(ctx).With(slog.String(\"userauth\", \"\"))\n\n\t// Server-sent event connection, for all initial data (list of mailboxes), list of\n\t// messages, and all events afterwards. Authenticated through a token in the query\n\t// string, which it got from a Token API call.\n\tif r.URL.Path == \"/events\" {\n\t\tserveEvents(ctx, log, accountPath, w, r)\n\t\treturn\n\t}\n\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\terr, ok := x.(*sherpa.Error)\n\t\tif !ok {\n\t\t\tlog.WithContext(ctx).Error(\"handle panic\", slog.Any(\"err\", x))\n\t\t\tdebug.PrintStack()\n\t\t\tmetrics.PanicInc(metrics.Webmailhandle)\n\t\t\tpanic(x)\n\t\t}\n\t\tif strings.HasPrefix(err.Code, \"user:\") {\n\t\t\tlog.Debugx(\"webmail user error\", err)\n\t\t\thttp.Error(w, \"400 - bad request - \"+err.Message, http.StatusBadRequest)\n\t\t} else {\n\t\t\tlog.Errorx(\"webmail server error\", err)\n\t\t\thttp.Error(w, \"500 - internal server error - \"+err.Message, http.StatusInternalServerError)\n\t\t}\n\t}()\n\n\tswitch r.URL.Path {\n\tcase \"/\":\n\t\tswitch r.Method {\n\t\tcase \"GET\", \"HEAD\":\n\t\t\th := w.Header()\n\t\t\th.Set(\"X-Frame-Options\", \"deny\")\n\t\t\th.Set(\"Referrer-Policy\", \"same-origin\")\n\t\t\twebmailFile.Serve(ctx, log, w, r)\n\t\tdefault:\n\t\t\thttp.Error(w, \"405 - method not allowed - use get\", http.StatusMethodNotAllowed)\n\t\t}\n\t\treturn\n\n\tcase \"/msg.js\", \"/text.js\":\n\t\tswitch r.Method {\n\t\tdefault:\n\t\t\thttp.Error(w, \"405 - method not allowed - use get\", http.StatusMethodNotAllowed)\n\t\t\treturn\n\t\tcase \"GET\", \"HEAD\":\n\t\t}\n\n\t\tpath := filepath.Join(\"webmail\", r.URL.Path[1:])\n\t\tvar fallback = webmailmsgJS\n\t\tif r.URL.Path == \"/text.js\" {\n\t\t\tfallback = webmailtextJS\n\t\t}\n\n\t\tw.Header().Set(\"Content-Type\", \"application/javascript; charset=utf-8\")\n\t\tserveContentFallback(log, w, r, path, fallback)\n\t\treturn\n\t}\n\n\tisAPI := strings.HasPrefix(r.URL.Path, \"/api/\")\n\t// Only allow POST for calls, they will not work cross-domain without CORS.\n\tif isAPI && r.URL.Path != \"/api/\" && r.Method != \"POST\" {\n\t\thttp.Error(w, \"405 - method not allowed - use post\", http.StatusMethodNotAllowed)\n\t\treturn\n\t}\n\n\tvar loginAddress, accName string\n\tvar sessionToken store.SessionToken\n\t// All other URLs, except the login endpoint require some authentication.\n\tif r.URL.Path != \"/api/LoginPrep\" && r.URL.Path != \"/api/Login\" {\n\t\tvar ok bool\n\t\tisExport := r.URL.Path == \"/export\"\n\t\trequireCSRF := isAPI || isExport\n\t\taccName, sessionToken, loginAddress, ok = webauth.Check(ctx, log, webauth.Accounts, \"webmail\", isForwarded, w, r, isAPI, requireCSRF, isExport)\n\t\tif !ok {\n\t\t\t// Response has been written already.\n\t\t\treturn\n\t\t}\n\t}\n\n\tif isAPI {\n\t\tvar acc *store.Account\n\t\tif accName != \"\" {\n\t\t\tlog = log.With(slog.String(\"account\", accName))\n\t\t\tvar err error\n\t\t\tacc, err = store.OpenAccount(log, accName)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorx(\"open account\", err)\n\t\t\t\thttp.Error(w, \"500 - internal server error - error opening account\", http.StatusInternalServerError)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tdefer func() {\n\t\t\t\terr := acc.Close()\n\t\t\t\tlog.Check(err, \"closing account\")\n\t\t\t}()\n\t\t}\n\t\treqInfo := requestInfo{log, loginAddress, acc, sessionToken, w, r}\n\t\tctx = context.WithValue(ctx, requestInfoCtxKey, reqInfo)\n\t\tapiHandler.ServeHTTP(w, r.WithContext(ctx))\n\t\treturn\n\t}\n\n\t// We are now expecting the following URLs:\n\t// .../export\n\t// .../msg/<msgid>/{attachments.zip,parsedmessage.js,raw}\n\t// .../msg/<msgid>/{,msg}{text,html,htmlexternal}\n\t// .../msg/<msgid>/{view,viewtext,download}/<partid>\n\n\tif r.URL.Path == \"/export\" {\n\t\twebops.Export(log, accName, w, r)\n\t\treturn\n\t}\n\n\tif !strings.HasPrefix(r.URL.Path, \"/msg/\") {\n\t\thttp.NotFound(w, r)\n\t\treturn\n\t}\n\n\tt := strings.Split(r.URL.Path[len(\"/msg/\"):], \"/\")\n\tif len(t) < 2 {\n\t\thttp.NotFound(w, r)\n\t\treturn\n\t}\n\n\tid, err := strconv.ParseInt(t[0], 10, 64)\n\tif err != nil || id == 0 {\n\t\thttp.NotFound(w, r)\n\t\treturn\n\t}\n\n\t// Many of the requests need either a message or a parsed part. Make it easy to\n\t// fetch/prepare and cleanup. We only do all the work when the request seems legit\n\t// (valid HTTP route and method).\n\txprepare := func() (acc *store.Account, m store.Message, msgr *store.MsgReader, p message.Part, cleanup func(), ok bool) {\n\t\tif r.Method != \"GET\" {\n\t\t\thttp.Error(w, \"405 - method not allowed - post required\", http.StatusMethodNotAllowed)\n\t\t\treturn\n\t\t}\n\n\t\tdefer func() {\n\t\t\tif ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif msgr != nil {\n\t\t\t\terr := msgr.Close()\n\t\t\t\tlog.Check(err, \"closing message reader\")\n\t\t\t\tmsgr = nil\n\t\t\t}\n\t\t\tif acc != nil {\n\t\t\t\terr := acc.Close()\n\t\t\t\tlog.Check(err, \"closing account\")\n\t\t\t\tacc = nil\n\t\t\t}\n\t\t}()\n\n\t\tvar err error\n\n\t\tacc, err = store.OpenAccount(log, accName)\n\t\txcheckf(ctx, err, \"open account\")\n\n\t\tm = store.Message{ID: id}\n\t\terr = acc.DB.Get(ctx, &m)\n\t\tif err == bstore.ErrAbsent || err == nil && m.Expunged {\n\t\t\thttp.NotFound(w, r)\n\t\t\treturn\n\t\t}\n\t\txcheckf(ctx, err, \"get message\")\n\n\t\tmsgr = acc.MessageReader(m)\n\n\t\tp, err = m.LoadPart(msgr)\n\t\txcheckf(ctx, err, \"load parsed message\")\n\n\t\tcleanup = func() {\n\t\t\terr := msgr.Close()\n\t\t\tlog.Check(err, \"closing message reader\")\n\t\t\terr = acc.Close()\n\t\t\tlog.Check(err, \"closing account\")\n\t\t}\n\t\tok = true\n\t\treturn\n\t}\n\n\th := w.Header()\n\n\t// We set a Content-Security-Policy header that is as strict as possible, depending\n\t// on the type of message/part/html/js. We have to be careful because we are\n\t// returning data that is coming in from external places. E.g. HTML could contain\n\t// javascripts that we don't want to execute, especially not on our domain. We load\n\t// resources in an iframe. The CSP policy starts out  with default-src 'none' to\n\t// disallow loading anything, then start allowing what is safe, such as inlined\n\t// datauri images and inline styles. Data can only be loaded when the request is\n\t// coming from the same origin (so other sites cannot include resources\n\t// (messages/parts)).\n\t//\n\t// We want to load resources in sandbox-mode, causing the page to be loaded as from\n\t// a different origin. If sameOrigin is set, we have a looser CSP policy:\n\t// allow-same-origin is set so resources are loaded as coming from this same\n\t// origin. This is needed for the msg* endpoints that render a message, where we\n\t// load the message body in a separate iframe again (with stricter CSP again),\n\t// which we need to access for its inner height. If allowSelfScript is also set\n\t// (for \"msgtext\"), the CSP leaves out the sandbox entirely.\n\t//\n\t// If allowExternal is set, we allow loading image, media (audio/video), styles and\n\t// fronts from external URLs as well as inline URI's. By default we don't allow any\n\t// loading of content, except inlined images (we do that ourselves for images\n\t// embedded in the email), and we allow inline styles (which are safely constrained\n\t// to an iframe).\n\t//\n\t// If allowSelfScript is set, inline scripts and scripts from our origin are\n\t// allowed. Used to display a message including header. The header is rendered with\n\t// javascript, the content is rendered in a separate iframe with a CSP that doesn't\n\t// have allowSelfScript.\n\theaders := func(sameOrigin, allowExternal, allowSelfScript, allowSelfImg bool) {\n\t\t// allow-popups is needed to make opening links in new tabs work.\n\t\tsb := \"sandbox allow-popups allow-popups-to-escape-sandbox; \"\n\t\tif sameOrigin && allowSelfScript {\n\t\t\t// Sandbox with both allow-same-origin and allow-script would not provide security,\n\t\t\t// and would give warning in console about that.\n\t\t\tsb = \"\"\n\t\t} else if sameOrigin {\n\t\t\tsb = \"sandbox allow-popups allow-popups-to-escape-sandbox allow-same-origin; \"\n\t\t}\n\t\tscript := \"\"\n\t\tif allowSelfScript {\n\t\t\tscript = \"; script-src 'unsafe-inline' 'self'; frame-src 'self'; connect-src 'self'\"\n\t\t}\n\t\tvar csp string\n\t\tif allowExternal {\n\t\t\tcsp = sb + \"frame-ancestors 'self'; default-src 'none'; img-src data: http: https: 'unsafe-inline'; style-src 'unsafe-inline' data: http: https:; font-src data: http: https: 'unsafe-inline'; media-src 'unsafe-inline' data: http: https:\" + script\n\t\t} else if allowSelfImg {\n\t\t\tcsp = sb + \"frame-ancestors 'self'; default-src 'none'; img-src data: 'self'; style-src 'unsafe-inline'\" + script\n\t\t} else {\n\t\t\tcsp = sb + \"frame-ancestors 'self'; default-src 'none'; img-src data:; style-src 'unsafe-inline'\" + script\n\t\t}\n\t\th.Set(\"Content-Security-Policy\", csp)\n\t\th.Set(\"X-Frame-Options\", \"sameorigin\") // Duplicate with CSP, but better too much than too little.\n\t\th.Set(\"X-Content-Type-Options\", \"nosniff\")\n\t\th.Set(\"Referrer-Policy\", \"no-referrer\")\n\t}\n\n\tswitch {\n\tcase len(t) == 2 && t[1] == \"attachments.zip\":\n\t\tacc, m, msgr, p, cleanup, ok := xprepare()\n\t\tif !ok {\n\t\t\treturn\n\t\t}\n\t\tdefer cleanup()\n\t\tstate := msgState{acc: acc, m: m, msgr: msgr, part: &p}\n\t\t// note: state is cleared by cleanup\n\n\t\tmi, err := messageItem(log, m, &state)\n\t\txcheckf(ctx, err, \"parsing message\")\n\n\t\theaders(false, false, false, false)\n\t\th.Set(\"Content-Type\", \"application/zip\")\n\t\th.Set(\"Cache-Control\", \"no-store, max-age=0\")\n\t\tvar subjectSlug string\n\t\tif p.Envelope != nil {\n\t\t\ts := p.Envelope.Subject\n\t\t\ts = strings.ToLower(s)\n\t\t\ts = regexp.MustCompile(\"[^a-z0-9_.-]\").ReplaceAllString(s, \"-\")\n\t\t\ts = regexp.MustCompile(\"--*\").ReplaceAllString(s, \"-\")\n\t\t\ts = strings.TrimLeft(s, \"-\")\n\t\t\ts = strings.TrimRight(s, \"-\")\n\t\t\tif s != \"\" {\n\t\t\t\ts = \"-\" + s\n\t\t\t}\n\t\t\tsubjectSlug = s\n\t\t}\n\t\tfilename := fmt.Sprintf(\"email-%d-attachments-%s%s.zip\", m.ID, m.Received.Format(\"20060102-150405\"), subjectSlug)\n\t\tcd := mime.FormatMediaType(\"attachment\", map[string]string{\"filename\": filename})\n\t\th.Set(\"Content-Disposition\", cd)\n\n\t\tzw := zip.NewWriter(w)\n\t\tnames := map[string]bool{}\n\t\tfor _, a := range mi.Attachments {\n\t\t\tap := a.Part\n\t\t\tname := tryDecodeParam(log, ap.ContentTypeParams[\"name\"])\n\t\t\tif name == \"\" {\n\t\t\t\t// We don't check errors, this is all best-effort.\n\t\t\t\th, _ := ap.Header()\n\t\t\t\tdisposition := h.Get(\"Content-Disposition\")\n\t\t\t\t_, params, _ := mime.ParseMediaType(disposition)\n\t\t\t\tname = tryDecodeParam(log, params[\"filename\"])\n\t\t\t}\n\t\t\tif name != \"\" {\n\t\t\t\tname = filepath.Base(name)\n\t\t\t}\n\t\t\tmt := strings.ToLower(ap.MediaType + \"/\" + ap.MediaSubType)\n\t\t\tif name == \"\" || names[name] {\n\t\t\t\text := filepath.Ext(name)\n\t\t\t\tif ext == \"\" {\n\t\t\t\t\t// Handle just a few basic types.\n\t\t\t\t\textensions := map[string]string{\n\t\t\t\t\t\t\"text/plain\":      \".txt\",\n\t\t\t\t\t\t\"text/html\":       \".html\",\n\t\t\t\t\t\t\"image/jpeg\":      \".jpg\",\n\t\t\t\t\t\t\"image/png\":       \".png\",\n\t\t\t\t\t\t\"image/gif\":       \".gif\",\n\t\t\t\t\t\t\"application/zip\": \".zip\",\n\t\t\t\t\t}\n\t\t\t\t\text = extensions[mt]\n\t\t\t\t\tif ext == \"\" {\n\t\t\t\t\t\text = \".bin\"\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tvar stem string\n\t\t\t\tif name != \"\" && strings.HasSuffix(name, ext) {\n\t\t\t\t\tstem = strings.TrimSuffix(name, ext)\n\t\t\t\t} else {\n\t\t\t\t\tstem = \"attachment\"\n\t\t\t\t\tfor _, index := range a.Path {\n\t\t\t\t\t\tstem += fmt.Sprintf(\"-%d\", index)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tname = stem + ext\n\t\t\t\tseq := 0\n\t\t\t\tfor names[name] {\n\t\t\t\t\tseq++\n\t\t\t\t\tname = stem + fmt.Sprintf(\"-%d\", seq) + ext\n\t\t\t\t}\n\t\t\t}\n\t\t\tnames[name] = true\n\n\t\t\tfh := zip.FileHeader{\n\t\t\t\tName:     name,\n\t\t\t\tModified: m.Received,\n\t\t\t}\n\t\t\tnodeflate := map[string]bool{\n\t\t\t\t\"application/x-bzip2\":          true,\n\t\t\t\t\"application/zip\":              true,\n\t\t\t\t\"application/x-zip-compressed\": true,\n\t\t\t\t\"application/gzip\":             true,\n\t\t\t\t\"application/x-gzip\":           true,\n\t\t\t\t\"application/vnd.rar\":          true,\n\t\t\t\t\"application/x-rar-compressed\": true,\n\t\t\t\t\"application/x-7z-compressed\":  true,\n\t\t\t}\n\t\t\t// Sniff content-type as well for compressed data.\n\t\t\tbuf := make([]byte, 512)\n\t\t\tn, _ := io.ReadFull(ap.Reader(), buf)\n\t\t\tvar sniffmt string\n\t\t\tif n > 0 {\n\t\t\t\tsniffmt = strings.ToLower(http.DetectContentType(buf[:n]))\n\t\t\t}\n\t\t\tdeflate := ap.MediaType != \"VIDEO\" && ap.MediaType != \"AUDIO\" && (ap.MediaType != \"IMAGE\" || ap.MediaSubType == \"BMP\") && !nodeflate[mt] && !nodeflate[sniffmt]\n\t\t\tif deflate {\n\t\t\t\tfh.Method = zip.Deflate\n\t\t\t}\n\t\t\t// We cannot return errors anymore: we have already sent an application/zip header.\n\t\t\tif zf, err := zw.CreateHeader(&fh); err != nil {\n\t\t\t\tlog.Check(err, \"adding to zip file\")\n\t\t\t\treturn\n\t\t\t} else if _, err := io.Copy(zf, ap.Reader()); err != nil {\n\t\t\t\tlog.Check(err, \"writing to zip file\")\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\terr = zw.Close()\n\t\tlog.Check(err, \"final write to zip file\")\n\n\t// Raw display of a message, as text/plain.\n\tcase len(t) == 2 && t[1] == \"raw\":\n\t\t_, _, msgr, p, cleanup, ok := xprepare()\n\t\tif !ok {\n\t\t\treturn\n\t\t}\n\t\tdefer cleanup()\n\n\t\t// We intentially use text/plain. We certainly don't want to return a format that\n\t\t// browsers or users would think of executing. We do set the charset if available\n\n```\n# Function Signature: func sanitizeNode(node *html.Node) {\n# Natural Language Annotations: // sanitizeNode removes script elements, on* attributes, javascript: href\n// attributes, adds target=\"_blank\" to all links and to a base tag.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc sanitizeNode(node *html.Node) {\n\tif node.Type == html.ElementNode && node.Data == \"script\" {\n\t\tnode.Parent.RemoveChild(node)\n\t\treturn\n\t}\n\tfor _, a := range node.Attr {\n\t\tif strings.HasPrefix(a.Key, \"on\") || a.Key == \"href\" && strings.HasPrefix(a.Val, \"javascript:\") {\n\t\t\tnode.RemoveAttr(a.Key)\n\t\t}\n\t}\n\tif node.DataAtom == atom.Base {\n\t\tnode.Attr = append(node.Attr, html.Attribute{Key: \"target\", Val: \"_blank\"})\n\t}\n\tif node.Type == html.ElementNode && node.Data == \"a\" {\n\t\tnode.Attr = append(node.Attr, html.Attribute{Key: \"target\", Val: \"_blank\"})\n\t}\n}\n```", "  ```\nfunc sanitizeNode(node *html.Node) {\n\tswitch node.Type {\n\tcase html.ElementNode:\n\t\tname := strings.ToLower(node.Data)\n\t\tif name == \"script\" {\n\t\t\tnode.Parent.RemoveChild(node)\n\t\t\treturn\n\t\t}\n\t\tfor i := len(node.Attr) - 1; i >= 0; i-- {\n\t\t\tif strings.HasPrefix(node.Attr[i].Key, \"on\") || strings.Contains(node.Attr[i].Val, \"javascript:\") {\n\t\t\t\tnode.Attr = append(node.Attr[:i], node.Attr[i+1:]...)\n\t\t\t}\n\t\t}\n\t\tif name == \"a\" {\n\t\t\tfor i := 0; i < len(node.Attr); i++ {\n\t\t\t\tif node.Attr[i].Key == \"href\" {\n\t\t\t\t\tif strings.HasPrefix(node.Attr[i].Val, \"javascript:\") {\n\t\t\t\t\t\tnode.Attr[i].Val = \"#\"\n\t\t\t\t\t}\n\t\t\t\t\tnode.Attr = append(node.Attr[:i+1], node.Attr[i+2:]...)\n\t\t\t\t\tnode.Attr[i].Key = \"target\"\n\t\t\t\t\tnode.Attr[i].Val = \"_blank\"\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif name == \"base\" {\n\t\t\tfor i := 0; i < len(node.Attr); i++ {\n\t\t\t\tif node.Attr[i].Key == \"href\" {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif i < len(node.Attr) {\n\t\t\t\tnode.Attr[i].Val = \"#\"\n\t\t\t}\n\t\t}\n\tcase html.SelfClosingNode:\n\t\tif node.DataAtom == atom.Script {\n\t\t\tnode.Parent.RemoveChild(node)\n\t\t\treturn\n\t\t}\n\tcase html.TextNode:\n\t\tnode.Data = strings.ReplaceAll(node.Data, \"&\", \"&amp;\")\n\t\tnode.Data = strings.ReplaceAll(node.Data, \"<\", \"&lt;\")\n\t\tnode.Data = strings.ReplaceAll(node.Data, \">\", \"&gt;\")\n\t\tnode.Data = strings.ReplaceAll(node.Data, \"\u2018\", \"'\")\n\t\tnode.Data = strings.ReplaceAll(node.Data, \"\u2019\", \"'\")\n\t}\n\tfor c := node.FirstChild; c != nil; c = c.NextSibling {\n\t\tsanitizeNode(c)\n\t}\n}\n```", "  ```\nfunc sanitizeNode(node *html.Node) {\n\tvar nodes []*html.Node\n\tvar removeNodes []*html.Node\n\tswitch node.Type {\n\tcase html.ElementNode:\n\t\t// Remove script elements\n\t\tif node.Data == \"script\" {\n\t\t\tremoveNodes = append(removeNodes, node)\n\t\t}\n\t\t// Remove on* attributes\n\t\tfor _, a := range node.Attr {\n\t\t\tif strings.HasPrefix(a.Key, \"on\") {\n\t\t\t\tnode.Attr = removeAttribute(node.Attr, a.Key)\n\t\t\t}\n\t\t}\n\t\t// Remove javascript: href attributes\n\t\tfor _, a := range node.Attr {\n\t\t\tif a.Key == \"href\" && strings.HasPrefix(a.Val, \"javascript:\") {\n\t\t\t\tnode.Attr = removeAttribute(node.Attr, a.Key)\n\t\t\t}\n\t\t}\n\t\t// Add target=\"_blank\" to all links\n\t\tif node.Data == \"a\" {\n\t\t\tnode.Attr = addAttribute(node.Attr, \"target\", \"_blank\")\n\t\t}\n\t\t// Add base tag\n\t\tif node.Data == \"head\" {\n\t\t\tnode.Attr = addAttribute(node.Attr, \"base\", \"target='_blank'\")\n\t\t}\n\t\t// Add data-* attributes for selected attributes\n\t\tfor _, a := range node.Attr {\n\t\t\tif a.Key == \"href\" || a.Key == \"src\" || a.Key == \"data\" {\n\t\t\t\tnode.Attr = addAttribute(node.Attr, fmt.Sprintf(\"data-%s\", a.Key), a.Val)\n\t\t\t}\n\t\t}\n\t\t// Process children\n\t\tfor c := node.FirstChild; c != nil; c = c.NextSibling {\n\t\t\tnodes = append(nodes, c)\n\t\t}\n\t}\n\t// Remove nodes\n\tfor _, n := range removeNodes {\n\t\tnode.RemoveChild(n)\n\t}\n\t// Recursively sanitize all child nodes\n\tfor _, n := range nodes {\n\t\tsanitizeNode(n)\n\t}\n}\n```"]}, "mox-dnsbl/dnsbl.go-Lookup": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dnsbl/dnsbl.go:\n```\n// Package dnsbl implements DNS block lists (RFC 5782), for checking incoming messages from sources without reputation.\n//\n// A DNS block list contains IP addresses that should be blocked. The DNSBL is\n// queried using DNS \"A\" lookups. The DNSBL starts at a \"zone\", e.g.\n// \"dnsbl.example\". To look up whether an IP address is listed, a DNS name is\n// composed: For 10.11.12.13, that name would be \"13.12.11.10.dnsbl.example\". If\n// the lookup returns \"record does not exist\", the IP is not listed. If an IP\n// address is returned, the IP is listed. If an IP is listed, an additional TXT\n// lookup is done for more information about the block. IPv6 addresses are also\n// looked up with an DNS \"A\" lookup of a name similar to an IPv4 address, but with\n// 4-bit hexadecimal dot-separated characters, in reverse.\n//\n// The health of a DNSBL \"zone\" can be check through a lookup of 127.0.0.1\n// (must not be present) and 127.0.0.2 (must be present).\npackage dnsbl\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/stub\"\n)\n\nvar (\n\tMetricLookup stub.HistogramVec = stub.HistogramVecIgnore{}\n)\n\nvar ErrDNS = errors.New(\"dnsbl: dns error\") // Temporary error.\n\n// Status is the result of a DNSBL lookup.\ntype Status string\n\nvar (\n\tStatusTemperr Status = \"temperror\" // Temporary failure.\n\tStatusPass    Status = \"pass\"      // Not present in block list.\n\tStatusFail    Status = \"fail\"      // Present in block list.\n)\n\n// Lookup checks if \"ip\" occurs in the DNS block list \"zone\" (e.g. dnsbl.example.org).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// CheckHealth checks whether the DNSBL \"zone\" is operating correctly by\n// querying for 127.0.0.2 (must be present) and 127.0.0.1 (must not be present).\n// Users of a DNSBL should periodically check if the DNSBL is still operating\n// properly.\n// For temporary errors, ErrDNS is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func Lookup(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain, ip net.IP) (rstatus Status, rexplanation string, rerr error) {\n# Natural Language Annotations: // Lookup checks if \"ip\" occurs in the DNS block list \"zone\" (e.g. dnsbl.example.org).\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc Lookup(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain, ip net.IP) (rstatus Status, rexplanation string, rerr error) {\n\tstart := time.Now()\n\thost, ipaddr := getDNSHostForIP(zone, ip)\n\treq := &dns.Msg{\n\t\tQuestion: []dns.Question{\n\t\t\t{\n\t\t\t\tName:   dns.Fqdn(host),\n\t\t\t\tQtype:  dns.TypeA,\n\t\t\t\tQclass: dns.ClassINET,\n\t\t\t},\n\t\t},\n\t}\n\tresp, err := resolver.Query(ctx, req)\n\tif err != nil {\n\t\telog.Printf(\"DNS query error for %s: %s\", ipaddr, err)\n\t\trerr = slog.ErrDNS\n\t\treturn\n\t}\n\tif resp.Answer == nil || resp.Answer[0].String() != ipaddr {\n\t\trstatus = StatusPass\n\t} else {\n\t\trstatus = StatusFail\n\t\trexplanation = fmt.Sprintf(\"Found in DNS block list %s: %s\", zone, ipaddr)\n\t}\n\td, err := time.Since(start).Seconds()\n\tif err == nil {\n\t\tMetricLookup.Observe(ctx, d)\n\t}\n\treturn\n}\n```", "  ```\nfunc Lookup(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain, ip net.IP) (rstatus Status, rexplanation string, rerr error) {\n\t// Check if the context has been cancelled or timed out.\n\tif err := ctx.Err(); err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\n\t// Convert the IP address to a DNS name.\n\tname := ipToDNSName(ip)\n\n\t// Make sure the name starts with a \".\".\n\tif !strings.HasPrefix(name, \".\") {\n\t\tname = \".\" + name\n\t}\n\n\t// Combine the name with the zone name.\n\tdnsname := zone.String() + name\n\n\t// Lookup the DNS record.\n\tstart := time.Now()\n\trecords, err := resolver.Lookup(dnsname, dns.TypeANY)\n\tMetricLookup.Observe(time.Since(start).Seconds())\n\n\tif err != nil {\n\t\treturn \"\", \"\", err\n\t}\n\n\t// If the record does not exist, the IP is not listed.\n\tif len(records) == 0 {\n\t\treturn StatusPass, \"\", nil\n\t}\n\n\t// If the record exists, the IP is listed. Additional TXT records\n\t// are looked up to retrieve more information about the block.\n\trexplanation := \"\"\n\tseenTXT := false\n\n\tfor _, record := range records {\n\t\tif record.Type == dns.TypeTXT {\n\t\t\tseenTXT = true\n\t\t\t// This is a TXT record, fetch the description and add it to the explanation.\n\t\t\trexplanation += fmt.Sprintf(\"Blocked as %s\", record.Value[0])\n\t\t}\n\t}\n\n\tif seenTXT {\n\t\treturn StatusFail, rexplanation, nil\n\t}\n\n\treturn StatusTemperr, \"\", ErrDNS\n}\n```\nThe generated code has been formatted for readability. It has also been annoted with comments to explain the reasoning behind the code.\n\nThe code starts by checking if the context has been cancelled or timed out. If it has, it returns an error and the function exits.\n\nNext, it converts the IP address to a DNS name using the `ipToDNSName` function. It then makes sure that the name starts with a \".\".\n\nIt then combines the name with the zone name using `zone.String()` and then looks up the DNS record using the `Lookup` function from the `dns` package. The `Lookup` function takes the DNS name being looked up and the DNS type `dns.TypeANY` as input, and returns a slice of `dns.Record` structures.\n\nThe slice of `dns.Record` structures is then examined to see if the DNS record exists or not. If it does not exist, the function returns with the `StatusPass` status and an empty explanation.\n\nOtherwise, the function looks at the remaining records in the slice and looks for TXT records. If it finds one, it retrieves the description and adds it to the explanation. If it does not find a TXT record, it returns a `StatusTemperr` status and the `ErrDNS` error.\n\nThe `MetricLookup` variable is used to observe the duration of the DNS lookup, and is sent to the `github.com/mjl-/mox/stub.HistogramVecIgnore` variable.", "  ```\nfunc Lookup(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain, ip net.IP) (rstatus Status, rexplanation string, rerr error) {\n\tvar (\n\t\tt0t1 string\n\t\tv    net.IP\n\t\tvint int\n\t)\n\tv = ip.To4()\n\tif v == nil {\n\t\tt0t1 = ip.String()\n\t\tfor vint > 0 && (t0t1[len(t0t1)-1] == '0') {\n\t\t\tt0t1 = t0t1[:len(t0t1)-1]\n\t\t\tvint--\n\t\t}\n\t\tt0t1 = strings.ToLower(t0t1)\n\t\tt0t1 = fmt.Sprintf(\"%x.%s\", vint, t0t1)\n\t\tzone = v + \".\" + zone\n\t} else {\n\t\tt0t1 = ip.To4().String()\n\t}\n\trexplanation, rerr = nil, nil\n\tt0t2 := zone + \".\"\n\tfor t0t1[len(t0t1)-1] == '.' {\n\t\tt0t1 = t0t1[:len(t0t1)-1]\n\t}\n\tt0t1 = t0t1 + \".\" + t0t2\n\tctx, ctx = context.WithTimeout(ctx, slog.DefaultTimeout)\n\tt0t3 := resolver.Question(ctx, t0t1, dns.TypeA)\n\tif t0t3 == nil {\n\t\telog.Info(\"DNSBL lookup failed\", slog.F(\"err\", rerr))\n\t\tt0t4, t0t5 := t0t1[:len(t0t1)-1], 0\n\t\tfor t0t4[len(t0t4)-1] == '.' {\n\t\t\tt0t4 = t0t4[:len(t0t4)-1]\n\t\t\tt0t5++\n\t\t}\n\t\tt0t6 := fmt.Sprintf(\"%v\", t0t5)\n\t\tt0t7, t0t8 := t0t1[:len(t0t1)-len(t0t6)-1], dns.TypeTXT\n\t\telog.Info(\"DNSBL location lookup\", slog.F(\"loc\", t0t7))\n\t\tt0t9, t0t10 := ctx, resolver.Question(ctx, t0t7, t0t8)\n\t\tif t0t10 != nil {\n\t\t\telog.Info(\"DNSBL loc lookup failed fallback to check for the presence of an any addr\", slog.F(\"err\", t0t10))\n\t\t\t_, t0t10 = context.WithTimeout(t0t9, slog.DefaultTimeout)\n\t\t\tt0t10, t0t11 := resolver.Question(t0t10, t0t7+\".*\", t0t8)\n\t\t\tif t0t11 != nil {\n\t\t\t\telog.Info(\"DNSBL loc lookup failed\", slog.F(\"err\", t0t11))\n\t\t\t\trexplanation, rerr = t0t11.Error(), ErrDNS\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif t0t10 != nil {\n\t\t\t\trexplanation, rerr = t0t10.String(), ErrDNS\n\t\t\t\treturn\n\t\t\t}\n\t\t\telog.Info(\"DNSBL failed fallback to check for the presence of an any addr succeeded\", slog.F(\"loc\", t0t7))\n\t\t}\n\t\telog.Info(\"DNSBL location lookup succeded\", slog.F(\"loc\", t0t7))\n\t\trexplanation = fmt.Sprintf(\"blocked by %s\", t0t9)\n\t\trstatus = StatusFail\n\t\treturn\n\t}\n\telog.Info(\"DNSBL lookup succeeded\")\n\trstatus = StatusPass\n\treturn\n}\n```"]}, "mox-dnsbl/dnsbl.go-CheckHealth": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dnsbl/dnsbl.go:\n```\n// Package dnsbl implements DNS block lists (RFC 5782), for checking incoming messages from sources without reputation.\n//\n// A DNS block list contains IP addresses that should be blocked. The DNSBL is\n// queried using DNS \"A\" lookups. The DNSBL starts at a \"zone\", e.g.\n// \"dnsbl.example\". To look up whether an IP address is listed, a DNS name is\n// composed: For 10.11.12.13, that name would be \"13.12.11.10.dnsbl.example\". If\n// the lookup returns \"record does not exist\", the IP is not listed. If an IP\n// address is returned, the IP is listed. If an IP is listed, an additional TXT\n// lookup is done for more information about the block. IPv6 addresses are also\n// looked up with an DNS \"A\" lookup of a name similar to an IPv4 address, but with\n// 4-bit hexadecimal dot-separated characters, in reverse.\n//\n// The health of a DNSBL \"zone\" can be check through a lookup of 127.0.0.1\n// (must not be present) and 127.0.0.2 (must be present).\npackage dnsbl\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/stub\"\n)\n\nvar (\n\tMetricLookup stub.HistogramVec = stub.HistogramVecIgnore{}\n)\n\nvar ErrDNS = errors.New(\"dnsbl: dns error\") // Temporary error.\n\n// Status is the result of a DNSBL lookup.\ntype Status string\n\nvar (\n\tStatusTemperr Status = \"temperror\" // Temporary failure.\n\tStatusPass    Status = \"pass\"      // Not present in block list.\n\tStatusFail    Status = \"fail\"      // Present in block list.\n)\n\n// Lookup checks if \"ip\" occurs in the DNS block list \"zone\" (e.g. dnsbl.example.org).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// CheckHealth checks whether the DNSBL \"zone\" is operating correctly by\n// querying for 127.0.0.2 (must be present) and 127.0.0.1 (must not be present).\n// Users of a DNSBL should periodically check if the DNSBL is still operating\n// properly.\n// For temporary errors, ErrDNS is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func CheckHealth(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain) (rerr error) {\n# Natural Language Annotations: // CheckHealth checks whether the DNSBL \"zone\" is operating correctly by\n// querying for 127.0.0.2 (must be present) and 127.0.0.1 (must not be present).\n// Users of a DNSBL should periodically check if the DNSBL is still operating\n// properly.\n// For temporary errors, ErrDNS is returned.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc CheckHealth(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain) (rerr error) {\n\t// 127.0.0.2 must return a response.\n\tresp1, err := resolver.Lookup(ctx, string(zone), \"127.0.0.2\")\n\tif err != nil {\n\t\treturn fmt.Errorf(\"check DNS health: lookup %s for 127.0.0.2: %w\", zone, err)\n\t}\n\tif resp1.A == \"\" {\n\t\treturn fmt.Errorf(\"check DNS health: no A record for 127.0.0.2 in %s\", zone)\n\t}\n\n\t// 127.0.0.1 must not return a response.\n\tresp2, err := resolver.Lookup(ctx, string(zone), \"127.0.0.1\")\n\tif err != nil {\n\t\treturn fmt.Errorf(\"check DNS health: lookup %s for 127.0.0.1: %w\", zone, err)\n\t}\n\tif resp2.A != \"\" {\n\t\treturn fmt.Errorf(\"check DNS health: got A record for 127.0.0.1 in %s\", zone)\n\t}\n\treturn nil\n}\n```", "  ```\nfunc CheckHealth(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain) (rerr error) {\n\telog.Println(\"Checking DNSBL health of\", zone)\n\t// Quality of Service: \"low\"\n\t// This is the main functionality of the function.\n\tdefer mlog.TraceDuration(\"CheckHealth\", &rerr, elog)()\n\n\t// Step 1: Check for 127.0.0.2 (must be present)\n\t_, err := resolver.Query(ctx, \"127.0.0.2.\"+string(zone), dns.A)\n\tif err == nil {\n\t\treturn nil\n\t}\n\tif !errors.Is(err, dns.ErrorNotFound) {\n\t\trerr = fmt.Errorf(\"dnsbl: error querying 127.0.0.2: %w\", err)\n\t\treturn\n\t}\n\n\t// Step 2: Check for 127.0.0.1 (must not be present)\n\t_, err = resolver.Query(ctx, \"127.0.0.1.\"+string(zone), dns.A)\n\tif err == nil {\n\t\trerr = fmt.Errorf(\"dnsbl: error querying 127.0.0.1: %w\", err)\n\t\treturn\n\t}\n\tif !errors.Is(err, dns.ErrorNotFound) {\n\t\trerr = fmt.Errorf(\"dnsbl: error querying 127.0.0.1: %w\", err)\n\t\treturn\n\t}\n\n\t// If we reach this point, both 127.0.0.2 and 127.0.0.1 are not present.\n\treturn nil\n}\n```", "  ```\nfunc CheckHealth(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain) (rerr error) {\n\tconst mustBePresent = \"127.0.0.2\"\n\tconst mustNotBePresent = \"127.0.0.1\"\n\t// Look up mustBePresent address first.\n\tif err := checkPresence(ctx, elog, resolver, zone, mustBePresent); err != nil {\n\t\treturn fmt.Errorf(\"dnsbl: %w\", err)\n\t}\n\t// Look up mustNotBePresent address second.\n\tif err := checkPresence(ctx, elog, resolver, zone, mustNotBePresent); err != nil {\n\t\treturn fmt.Errorf(\"dnsbl: %w\", err)\n\t}\n\treturn nil\n}\n\n// checkPresence looks up an address in a given DNS zone and returns an error if it\n// is not present or if there is a temporary error.\nfunc checkPresence(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain, addr string) error {\n\tipAddr := net.ParseIP(addr)\n\tif ipAddr == nil {\n\t\treturn errors.New(\"dnsbl: invalid IP address\")\n\t}\n\tipStr := strings.Replace(ipAddr.String(), \".\", \"\", -1)\n\tname := zone.String() + \".\" + ipStr + \".dnsbl.example.org.\"\n\tlabels := []string{\n\t\t\"dnsbl\",\n\t\t\"check\",\n\t\t\"health\",\n\t}\n\tgoMetric := MetricLookup.WithLabelValues(labels...)\n\tdefer func() { goMetric(time.Since(time.Now())) }()\n\tstart := time.Now()\n\tmetrics.AddEvent(&dnssbl.Metrics{\n\t\tReason: \"checkHealth\",\n\t\tStart:  start,\n\t})\n\tdefer metrics.AddEvent(&dnssbl.Metrics{\n\t\tReason: \"checkHealth\",\n\t\tEnd:    time.Now(),\n\t})\n\tres, err := resolver.LookupIP(ctx, dns.Name{Labels: []dns.NameLabel{zone}, Prefix: zone.String()}, dns.TypeA)\n\tif err != nil {\n\t\treturn ErrDNS\n\t}\n\tif _, ok := res.Authoritative[name]; !ok {\n\t\treturn fmt.Errorf(\"dnsbl: %w\", ErrDNS)\n\t}\n\treturn nil\n}\n```"]}, "mox-dmarcrpt/parse.go-ParseMessageReport": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dmarcrpt/parse.go:\n```\n// Package dmarcrpt parses DMARC aggregate feedback reports.\npackage dmarcrpt\n\nimport (\n\t\"archive/zip\"\n\t\"bytes\"\n\t\"compress/gzip\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net/http\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/moxio\"\n)\n\nvar ErrNoReport = errors.New(\"no dmarc aggregate report found in message\")\n\n// ParseReport parses an XML aggregate feedback report.\n// The maximum report size is 20MB.\nfunc ParseReport(r io.Reader) (*Feedback, error) {\n\tr = &moxio.LimitReader{R: r, Limit: 20 * 1024 * 1024}\n\tvar feedback Feedback\n\td := xml.NewDecoder(r)\n\tif err := d.Decode(&feedback); err != nil {\n\t\treturn nil, err\n\t}\n\treturn &feedback, nil\n}\n\n// ParseMessageReport parses an aggregate feedback report from a mail message. The\n// maximum message size is 15MB, the maximum report size after decompression is\n// 20MB.\n\n\n\n\n\n\n\n\n\n\n\nfunc parseMessageReport(log mlog.Log, p message.Part) (*Feedback, error) {\n\t// Pretty much any mime structure is allowed. ../rfc/7489:1861\n\t// In practice, some parties will send the report as the only (non-multipart)\n\t// content of the message.\n\n\tif p.MediaType != \"MULTIPART\" {\n\t\treturn parseReport(p)\n\t}\n\n\tfor {\n\t\tsp, err := p.ParseNextPart(log.Logger)\n\t\tif err == io.EOF {\n\t\t\treturn nil, ErrNoReport\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treport, err := parseMessageReport(log, *sp)\n\t\tif err == ErrNoReport {\n\t\t\tcontinue\n\t\t} else if err != nil || report != nil {\n\t\t\treturn report, err\n\t\t}\n\t}\n}\n\nfunc parseReport(p message.Part) (*Feedback, error) {\n\tct := strings.ToLower(p.MediaType + \"/\" + p.MediaSubType)\n\tr := p.Reader()\n\n\t// If no (useful) content-type is set, try to detect it.\n\tif ct == \"\" || ct == \"application/octet-stream\" {\n\t\tdata := make([]byte, 512)\n\t\tn, err := io.ReadFull(r, data)\n\t\tif err == io.EOF {\n\t\t\treturn nil, ErrNoReport\n\t\t} else if err != nil && err != io.ErrUnexpectedEOF {\n\t\t\treturn nil, fmt.Errorf(\"reading application/octet-stream for content-type detection: %v\", err)\n\t\t}\n\t\tdata = data[:n]\n\t\tct = http.DetectContentType(data)\n\t\tr = io.MultiReader(bytes.NewReader(data), r)\n\t}\n\n\tswitch ct {\n\tcase \"application/zip\":\n\t\t// Google sends messages with direct application/zip content-type.\n\t\treturn parseZip(r)\n\tcase \"application/gzip\", \"application/x-gzip\":\n\t\tgzr, err := gzip.NewReader(r)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"decoding gzip xml report: %s\", err)\n\t\t}\n\t\treturn ParseReport(gzr)\n\tcase \"text/xml\", \"application/xml\":\n\t\treturn ParseReport(r)\n\t}\n\treturn nil, ErrNoReport\n}\n\nfunc parseZip(r io.Reader) (*Feedback, error) {\n\tbuf, err := io.ReadAll(r)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading feedback: %s\", err)\n\t}\n\tzr, err := zip.NewReader(bytes.NewReader(buf), int64(len(buf)))\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"parsing zip file: %s\", err)\n\t}\n\tif len(zr.File) != 1 {\n\t\treturn nil, fmt.Errorf(\"zip contains %d files, expected 1\", len(zr.File))\n\t}\n\tf, err := zr.File[0].Open()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"opening file in zip: %s\", err)\n\t}\n\tdefer f.Close()\n\treturn ParseReport(f)\n}\n\n```\n# Function Signature: func ParseMessageReport(elog *slog.Logger, r io.ReaderAt) (*Feedback, error) {\n# Natural Language Annotations: // ParseMessageReport parses an aggregate feedback report from a mail message. The\n// maximum message size is 15MB, the maximum report size after decompression is\n// 20MB.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc ParseMessageReport(elog *slog.Logger, r io.ReaderAt) (*Feedback, error) {\n\tif r == nil {\n\t\treturn nil, ErrNoReport\n\t}\n\tvar err error\n\tvar p message.Part\n\tif p, err = message.ReadPart(log.Logger, r); err != nil {\n\t\treturn nil, err\n\t}\n\treturn parseMessageReport(elog, p)\n}\n```", "  ```\nfunc ParseMessageReport(elog *slog.Logger, r io.ReaderAt) (*Feedback, error) {\n\t// Pretty much any mime structure is allowed. ../rfc/7489:1861\n\t// In practice, some parties will send the report as the only (non-multipart)\n\t// content of the message.\n\n\tif r.MediaType != \"MULTIPART\" {\n\t\treturn parseReport(r)\n\t}\n\n\tfor {\n\t\tsp, err := r.ParseNextPart(elog.Logger)\n\t\tif err == io.EOF {\n\t\t\treturn nil, ErrNoReport\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treport, err := parseMessageReport(elog, *sp)\n\t\tif err == ErrNoReport {\n\t\t\tcontinue\n\t\t} else if err != nil || report != nil {\n\t\t\treturn report, err\n\t\t}\n\t}\n}\n\nfunc parseReport(r message.Part) (*Feedback, error) {\n\tct := strings.ToLower(r.MediaType + \"/\" + r.MediaSubType)\n\tvar rr io.Reader\n\n\t// If no (useful) content-type is set, try to detect it.\n\tif ct == \"\" || ct == \"application/octet-stream\" {\n\t\tdata := make([]byte, 512)\n\t\tn, err := io.ReadFull(r.Reader(), data)\n\t\tif err == io.EOF {\n\t\t\treturn nil, ErrNoReport\n\t\t} else if err != nil && err != io.ErrUnexpectedEOF {\n\t\t\treturn nil, fmt.Errorf(\"reading application/octet-stream for content-type detection: %v\", err)\n\t\t}\n\t\tdata = data[:n]\n\t\tct = http.DetectContentType(data)\n\t\trr = io.MultiReader(bytes.NewReader(data), r.Reader())\n\t}\n\n\tswitch ct {\n\tcase \"application/zip\", \"application/x-zip\", \"application/x-zip-compressed\":\n\t\t// Google sends messages with direct application/zip content-type.\n\t\treturn parseZip(rr)\n\tcase \"application/gzip\", \"application/x-gzip\":\n\t\tgzr, err := gzip.NewReader(rr)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"decoding gzip xml report: %s\", err)\n\t\t}\n\t\treturn ParseReport(gzr)\n\tcase \"text/xml\", \"application/xml\":\n\t\treturn ParseReport(rr)\n\t}\n\treturn nil, ErrNoReport\n}\n\nfunc parseZip(r io.Reader) (*Feedback, error) {\n\tbuf, err := io.ReadAll(r)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading feedback: %s\", err)\n\t}\n\tzr, err := zip.NewReader(bytes.NewReader(buf), int64(len(buf)))\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"parsing zip file: %s\", err)\n\t}\n\tif len(zr.File) != 1 {\n\t\treturn nil, fmt.Errorf(\"zip contains %d files, expected 1\", len(zr.File))\n\t}\n\tf, err := zr.File[0].Open()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"opening file in zip: %s\", err)\n\t}\n\tdefer f.Close()\n\treturn ParseReport(f)\n}\n```", "  ```\nfunc ParseMessageReport(elog *slog.Logger, r io.ReaderAt) (*Feedback, error) {\n\t// Pretty much any mime structure is allowed. ../rfc/7489:1861\n\t// In practice, some parties will send the report as the only (non-multipart)\n\t// content of the message.\n\n\tif r.MediaType != \"MULTIPART\" {\n\t\treturn parseReport(r)\n\t}\n\n\tfor {\n\t\tsp, err := r.ParseNextPart(slog.Logger)\n\t\tif err == io.EOF {\n\t\t\treturn nil, ErrNoReport\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treport, err := parseMessageReport(elog, *sp)\n\t\tif err == ErrNoReport {\n\t\t\tcontinue\n\t\t} else if err != nil || report != nil {\n\t\t\treturn report, err\n\t\t}\n\t}\n}\n\nfunc parseReport(r io.ReaderAt) (*Feedback, error) {\n\tct := strings.ToLower(r.MediaType + \"/\" + r.MediaSubType)\n\tbuf, err := io.ReadAll(r)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading feedback: %s\", err)\n\t}\n\tif ct == \"\" || ct == \"application/octet-stream\" {\n\t\tct = http.DetectContentType(buf)\n\t\tr = io.MultiReader(bytes.NewReader(buf), r)\n\t}\n\n\tswitch ct {\n\tcase \"application/zip\":\n\t\t// Google sends messages with direct application/zip content-type.\n\t\treturn parseZip(r)\n\tcase \"application/gzip\", \"application/x-gzip\":\n\t\tgzr, err := gzip.NewReader(r)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"decoding gzip xml report: %s\", err)\n\t\t}\n\t\treturn ParseReport(gzr)\n\tcase \"text/xml\", \"application/xml\":\n\t\treturn ParseReport(r)\n\t}\n\treturn nil, ErrNoReport\n}\n\nfunc parseZip(r io.ReaderAt) (*Feedback, error) {\n\tbuf, err := io.ReadAll(r)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading feedback: %s\", err)\n\t}\n\tzr, err := zip.NewReader(bytes.NewReader(buf), int64(len(buf)))\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"parsing zip file: %s\", err)\n\t}\n\tif len(zr.File) != 1 {\n\t\treturn nil, fmt.Errorf(\"zip contains %d files, expected 1\", len(zr.File))\n\t}\n\tf, err := zr.File[0].Open()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"opening file in zip: %s\", err)\n\t}\n\tdefer f.Close()\n\treturn ParseReport(f)\n}\n```"]}, "mox-mox-/lifecycle.go-Register": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/mox-/lifecycle.go:\n```\npackage mox\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"os\"\n\t\"runtime\"\n\t\"runtime/debug\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n)\n\n// We start up as root, bind to sockets, open private key/cert files and fork and\n// exec as unprivileged user. During startup as root, we gather the fd's for the\n// listen addresses in passedListeners and files in passedFiles, and pass their\n// addresses and paths in environment variables to the new process.\nvar passedListeners = map[string]*os.File{} // Listen address to file descriptor.\nvar passedFiles = map[string][]*os.File{}   // Path to file descriptors.\n\n// RestorePassedFiles reads addresses from $MOX_SOCKETS and paths from $MOX_FILES\n// and prepares an os.File for each file descriptor, which are used by later calls\n// of Listen or opening files.\nfunc RestorePassedFiles() {\n\ts := os.Getenv(\"MOX_SOCKETS\")\n\tif s == \"\" {\n\t\tvar linuxhint string\n\t\tif runtime.GOOS == \"linux\" {\n\t\t\tlinuxhint = \" If you updated from v0.0.1, update the mox.service file to start as root (privileges are dropped): ./mox config printservice >mox.service && sudo systemctl daemon-reload && sudo systemctl restart mox.\"\n\t\t}\n\t\tpkglog.Fatal(\"mox must be started as root, and will drop privileges after binding required sockets (missing environment variable MOX_SOCKETS).\" + linuxhint)\n\t}\n\n\t// 0,1,2 are stdin,stdout,stderr, 3 is the first passed fd (first listeners, then files).\n\tvar o uintptr = 3\n\tfor _, addr := range strings.Split(s, \",\") {\n\t\tpassedListeners[addr] = os.NewFile(o, addr)\n\t\to++\n\t}\n\n\tfiles := os.Getenv(\"MOX_FILES\")\n\tif files == \"\" {\n\t\treturn\n\t}\n\tfor _, path := range strings.Split(files, \",\") {\n\t\tpassedFiles[path] = append(passedFiles[path], os.NewFile(o, path))\n\t\to++\n\t}\n}\n\n// CleanupPassedFiles closes the listening socket file descriptors and files passed\n// in by the parent process. To be called by the unprivileged child after listeners\n// have been recreated (they dup the file descriptor), and by the privileged\n// process after starting its child.\nfunc CleanupPassedFiles() {\n\tfor _, f := range passedListeners {\n\t\terr := f.Close()\n\t\tpkglog.Check(err, \"closing listener socket file descriptor\")\n\t}\n\tfor _, fl := range passedFiles {\n\t\tfor _, f := range fl {\n\t\t\terr := f.Close()\n\t\t\tpkglog.Check(err, \"closing path file descriptor\")\n\t\t}\n\t}\n}\n\n// For privileged file descriptor operations (listen and opening privileged files),\n// perform them immediately, regardless of running as root or other user, in case\n// ForkExecUnprivileged is not used.\nvar FilesImmediate bool\n\n// Listen returns a newly created network listener when starting as root, and\n// otherwise (not root) returns a network listener from a file descriptor that was\n// passed by the parent root process.\nfunc Listen(network, addr string) (net.Listener, error) {\n\tif os.Getuid() != 0 && !FilesImmediate {\n\t\tf, ok := passedListeners[addr]\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"no file descriptor for listener %s\", addr)\n\t\t}\n\t\tln, err := net.FileListener(f)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"making network listener from file descriptor for address %s: %v\", addr, err)\n\t\t}\n\t\treturn ln, nil\n\t}\n\n\tif _, ok := passedListeners[addr]; ok {\n\t\treturn nil, fmt.Errorf(\"duplicate listener: %s\", addr)\n\t}\n\n\tln, err := net.Listen(network, addr)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// On windows, we cannot duplicate a socket. We don't need to for mox localserve\n\t// with FilesImmediate.\n\tif !FilesImmediate {\n\t\ttcpln, ok := ln.(*net.TCPListener)\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"listener not a tcp listener, but %T, for network %s, address %s\", ln, network, addr)\n\t\t}\n\t\tf, err := tcpln.File()\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"dup listener: %v\", err)\n\t\t}\n\t\tpassedListeners[addr] = f\n\t}\n\treturn ln, err\n}\n\n// Open a privileged file, such as a TLS private key. When running as root\n// (during startup), the file is opened and the file descriptor is stored.\n// These file descriptors are passed to the unprivileged process. When in the\n// unprivileged processed, we lookup a passed file descriptor.\n// The same calls should be made in the privileged and unprivileged process.\nfunc OpenPrivileged(path string) (*os.File, error) {\n\tif os.Getuid() != 0 && !FilesImmediate {\n\t\tfl := passedFiles[path]\n\t\tif len(fl) == 0 {\n\t\t\treturn nil, fmt.Errorf(\"no file descriptor for file %s\", path)\n\t\t}\n\t\tf := fl[0]\n\t\tpassedFiles[path] = fl[1:]\n\t\treturn f, nil\n\t}\n\n\tf, err := os.Open(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpassedFiles[path] = append(passedFiles[path], f)\n\n\t// Open again, the caller will be closing this file.\n\treturn os.Open(path)\n}\n\n// Shutdown is canceled when a graceful shutdown is initiated. SMTP, IMAP, periodic\n// processes should check this before starting a new operation. If this context is\n// canaceled, the operation should not be started, and new connections/commands should\n// receive a message that the service is currently not available.\nvar Shutdown context.Context\nvar ShutdownCancel func()\n\n// This context should be used as parent by most operations. It is canceled 1\n// second after graceful shutdown was initiated with the cancelation of the\n// Shutdown context. This should abort active operations.\n//\n// Operations typically have context timeouts, 30s for single i/o like DNS queries,\n// and 1 minute for operations with more back and forth. These are set through a\n// context.WithTimeout based on this context, so those contexts are still canceled\n// when shutting down.\n//\n// HTTP servers don't get graceful shutdown, their connections are just aborted.\n// todo: should shut down http connections as well, and shut down the listener and/or return 503 for new requests.\nvar Context context.Context\nvar ContextCancel func()\n\n// Connections holds all active protocol sockets (smtp, imap). They will be given\n// an immediate read/write deadline shortly after initiating mox shutdown, after\n// which the connections get 1 more second for error handling before actual\n// shutdown.\nvar Connections = &connections{\n\tconns:  map[net.Conn]connKind{},\n\tgauges: map[connKind]prometheus.GaugeFunc{},\n\tactive: map[connKind]int64{},\n}\n\ntype connKind struct {\n\tprotocol string\n\tlistener string\n}\n\ntype connections struct {\n\tsync.Mutex\n\tconns  map[net.Conn]connKind\n\tdones  []chan struct{}\n\tgauges map[connKind]prometheus.GaugeFunc\n\n\tactiveMutex sync.Mutex\n\tactive      map[connKind]int64\n}\n\n// Register adds a connection for receiving an immediate i/o deadline on shutdown.\n// When the connection is closed, Remove must be called to cancel the registration.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Unregister removes a connection for shutdown.\nfunc (c *connections) Unregister(nc net.Conn) {\n\tc.Lock()\n\tdefer c.Unlock()\n\tck := c.conns[nc]\n\n\tdefer func() {\n\t\tc.activeMutex.Lock()\n\t\tc.active[ck]--\n\t\tc.activeMutex.Unlock()\n\t}()\n\n\tdelete(c.conns, nc)\n\tif len(c.conns) > 0 {\n\t\treturn\n\t}\n\tfor _, done := range c.dones {\n\t\tdone <- struct{}{}\n\t}\n\tc.dones = nil\n}\n\n// Shutdown sets an immediate i/o deadline on all open registered sockets. Called\n// some time after mox shutdown is initiated.\n// The deadline will cause i/o's to be aborted, which should result in the\n// connection being unregistered.\nfunc (c *connections) Shutdown() {\n\tnow := time.Now()\n\tc.Lock()\n\tdefer c.Unlock()\n\tfor nc := range c.conns {\n\t\tif err := nc.SetDeadline(now); err != nil {\n\t\t\tpkglog.Errorx(\"setting immediate read/write deadline for shutdown\", err)\n\t\t}\n\t}\n}\n\n// Done returns a new channel on which a value is sent when no more sockets are\n// open, which could be immediate.\nfunc (c *connections) Done() chan struct{} {\n\tc.Lock()\n\tdefer c.Unlock()\n\tdone := make(chan struct{}, 1)\n\tif len(c.conns) == 0 {\n\t\tdone <- struct{}{}\n\t\treturn done\n\t}\n\tc.dones = append(c.dones, done)\n\treturn done\n}\n\n```\n# Function Signature: func (c *connections) Register(nc net.Conn, protocol, listener string) {\n# Natural Language Annotations: // Register adds a connection for receiving an immediate i/o deadline on shutdown.\n// When the connection is closed, Remove must be called to cancel the registration.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (c *connections) Register(nc net.Conn, protocol, listener string) {\n\tck := connKind{protocol, listener}\n\tc.Lock()\n\tdefer c.Unlock()\n\tc.conns[nc] = ck\n\tc.activeMutex.Lock()\n\tc.active[ck]++\n\tc.activeMutex.Unlock()\n\tif c.gauges[ck] == nil {\n\t\tc.gauges[ck] = promauto.With(prometheus.GaugeOpts{\n\t\t\tName:        fmt.Sprintf(\"mox_active_%s\", strings.ToLower(ck.protocol)),\n\t\t\tHelp:        fmt.Sprintf(\"Active %s connections.\", ck.protocol),\n\t\t\tConstLabels: prometheus.Labels{\"listener\": ck.listener},\n\t\t}).GaugeFunc()\n\t}\n\tc.gauges[ck]()\n}\n```", "  ```\nfunc (c *connections) Register(nc net.Conn, protocol, listener string) {\n\tc.Lock()\n\tdefer c.Unlock()\n\n\tck := connKind{\n\t\tprotocol: protocol,\n\t\tlistener: listener,\n\t}\n\n\tif _, ok := c.conns[nc]; ok {\n\t\treturn\n\t}\n\n\t// If this is the first connection, create a channel for Done() to communicate\n\t// with Shutdown().\n\tif len(c.conns) == 0 {\n\t\tc.dones = append(c.dones, make(chan struct{}, 1))\n\t}\n\n\tc.conns[nc] = ck\n\tc.activeMutex.Lock()\n\tc.active[ck]++\n\tc.activeMutex.Unlock()\n}\n```", "  ```\nfunc (c *connections) Register(nc net.Conn, protocol, listener string) {\n    c.Lock()\n    defer c.Unlock()\n\n    ck := connKind{protocol, listener}\n    c.conns[nc] = ck\n\n    // Register gauge for connection protocol if it doesn't exist yet\n    if _, exists := c.gauges[ck]; !exists {\n        gaugeFnc := promauto.With(c.activeMutex).NewGaugeFunc(\n            prometheus.GaugeOpts{\n                Name:        fmt.Sprintf(\"%s_active\", ck.protocol),\n                Help:        fmt.Sprintf(\"Number of %s connections currently active.\", ck.protocol),\n                ConstLabels: prometheus.Labels{\"listener\": ck.listener},\n            },\n            func() float64 {\n                c.activeMutex.Lock()\n                defer c.activeMutex.Unlock()\n                return float64(c.active[ck])\n            },\n        )\n        c.gauges[ck] = gaugeFnc\n    }\n\n    c.activeMutex.Lock()\n    active := c.active[ck]\n    c.active[ck] = active + 1\n    c.activeMutex.Unlock()\n\n    if c.gauges[ck] != nil {\n        c.gauges[ck].Inc()\n    }\n}\n```"]}, "mox-smtpclient/dial.go-Dial": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/smtpclient/dial.go:\n```\npackage smtpclient\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// DialHook can be used during tests to override the regular dialer from being used.\nvar DialHook func(ctx context.Context, dialer Dialer, timeout time.Duration, addr string, laddr net.Addr) (net.Conn, error)\n\nfunc dial(ctx context.Context, dialer Dialer, timeout time.Duration, addr string, laddr net.Addr) (net.Conn, error) {\n\t// todo: see if we can remove this function and DialHook in favor of the Dialer interface.\n\n\tif DialHook != nil {\n\t\treturn DialHook(ctx, dialer, timeout, addr, laddr)\n\t}\n\n\t// If this is a net.Dialer, use its settings and add the timeout and localaddr.\n\t// This is the typical case, but SOCKS5 support can use a different dialer.\n\tif d, ok := dialer.(*net.Dialer); ok {\n\t\tnd := *d\n\t\tnd.Timeout = timeout\n\t\tnd.LocalAddr = laddr\n\t\treturn nd.DialContext(ctx, \"tcp\", addr)\n\t}\n\treturn dialer.DialContext(ctx, \"tcp\", addr)\n}\n\n// Dialer is used to dial mail servers, an interface to facilitate testing.\ntype Dialer interface {\n\tDialContext(ctx context.Context, network, addr string) (c net.Conn, err error)\n}\n\n// Dial connects to host by dialing ips, taking previous attempts in dialedIPs into\n// accounts (for greylisting, blocklisting and ipv4/ipv6).\n//\n// If the previous attempt used IPv4, this attempt will use IPv6 (useful in case\n// one of the IPs is in a DNSBL).\n//\n// The second attempt for an address family we prefer the same IP as earlier, to\n// increase our chances if remote is doing greylisting.\n//\n// Dial updates dialedIPs, callers may want to save it so it can be taken into\n// account for future delivery attempts.\n//\n// The first matching protocol family from localIPs is set for the local side\n// of the TCP connection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func Dial(ctx context.Context, elog *slog.Logger, dialer Dialer, host dns.IPDomain, ips []net.IP, port int, dialedIPs map[string][]net.IP, localIPs []net.IP) (conn net.Conn, ip net.IP, rerr error) {\n# Natural Language Annotations: // Dial connects to host by dialing ips, taking previous attempts in dialedIPs into\n// accounts (for greylisting, blocklisting and ipv4/ipv6).\n//\n// If the previous attempt used IPv4, this attempt will use IPv6 (useful in case\n// one of the IPs is in a DNSBL).\n//\n// The second attempt for an address family we prefer the same IP as earlier, to\n// increase our chances if remote is doing greylisting.\n//\n// Dial updates dialedIPs, callers may want to save it so it can be taken into\n// account for future delivery attempts.\n//\n// The first matching protocol family from localIPs is set for the local side\n// of the TCP connection.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc Dial(ctx context.Context, elog *slog.Logger, dialer Dialer, host dns.IPDomain, ips []net.IP, port int, dialedIPs map[string][]net.IP, localIPs []net.IP) (conn net.Conn, ip net.IP, rerr error) {\n\tfor _, ip := range ips {\n\t\t// If one of the previous attempts was using IPv4, try IPv6 first\n\t\tif _, ok := dialedIPs[host.String()]; ok {\n\t\t\tif net.ParseIP(dialedIPs[host.String()].(net.IP).String()).To4() != nil {\n\t\t\t\tip.To16()\n\t\t\t}\n\t\t}\n\n\t\t// Prefer the same IP as the previous attempt if possible\n\t\tif _, ok := dialedIPs[host.String()]; ok {\n\t\t\tfor _, localIP := range localIPs {\n\t\t\t\tif localIP.IP.String() == ip.IP.String() {\n\t\t\t\t\tconn, rerr = dialer.DialContext(ctx, \"tcp\", net.JoinHostPort(ip.String(), strconv.Itoa(port)))\n\t\t\t\t\tif rerr != nil {\n\t\t\t\t\t\telog.Printf(\"Dial(%s) failed: %v\", ip.String(), err)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tdialedIPs[host.String()] = ip\n\t\t\t\t\treturn conn, ip, nil\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// If no previous attempts, just use the first IP we found that is not in dialedIPs\n\t\tif len(dialedIPs[host.String()]) <= 0 {\n\t\t\tconn, rerr = dialer.DialContext(ctx, \"tcp\", net.JoinHostPort(ip.String(), strconv.Itoa(port)))\n\t\t\tif rerr != nil {\n\t\t\t\telog.Printf(\"Dial(%s) failed: %v\", ip.String(), err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tdialedIPs[host.String()] = ip\n\t\t\treturn conn, ip, nil\n\t\t}\n\t}\n\treturn nil, nil, fmt.Errorf(\"could not find a working IP address for %s\", host.String())\n}\n```\nPlease note that this function signature and annotations are not intended for production use, but rather as a placeholder to generate a basic function based on the provided parameters. Also, there are several improvements that could be made to this function such as error handling, and more robust type checking, but this is outside the scope of this exercise.", "  ```\nfunc Dial(ctx context.Context, elog *slog.Logger, dialer Dialer, host dns.IPDomain, ips []net.IP, port int, dialedIPs map[string][]net.IP, localIPs []net.IP) (conn net.Conn, ip net.IP, rerr error) {\n\tvar err error\n\n\t// If previous attempts were made for the same IP protocol, prefer the same IP.\n\tpreferedIP := \"\"\n\tif protocol, ok := dialedIPs[host]; ok {\n\t\tpreferedIP = protocol[0]\n\t}\n\n\t// Attempt to dial each IP, and each IP by precedence: IPv4, IPv6\n\tfor _, ip := range ips {\n\t\tif preferedIP == ip.String() {\n\t\t\t// Skip this IP if it's the same as the precedence-prefered IP\n\t\t\tcontinue\n\t\t}\n\n\t\taddr := net.JoinHostPort(ip.String(), port)\n\t\tctx, cancel := context.WithTimeout(ctx, dialer.Timeout)\n\t\tdefer cancel()\n\n\t\tvar family int\n\t\tif familyFromIP(ip) == syscall.AF_INET6 {\n\t\t\tfamily = syscall.AF_INET6\n\t\t} else {\n\t\t\tfamily = syscall.AF_INET\n\t\t}\n\t\tconn, err = dialer.DialContext(ctx, \"tcp\", addr)\n\t\tif err != nil {\n\t\t\telog.Printf(\"Dial failed for %s %s (%s)\", ip, addr, err)\n\t\t\tcontinue\n\t\t}\n\t\t// Use the local IP that matches the protocol family of the dialed IP\n\t\tvar localAddr net.Addr\n\t\tfor _, lip := range localIPs {\n\t\t\tif familyFromIP(lip) == family {\n\t\t\t\tlocalAddr = lip\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\n\t\tif localAddr != nil {\n\t\t\terr = conn.(*net.TCPConn).SetLaddr(localAddr)\n\t\t\tif err != nil {\n\t\t\t\telog.Printf(\"SetLaddr failed for %s (%s)\", localAddr, err)\n\t\t\t}\n\t\t} else {\n\t\t\telog.Printf(\"No local %s bind address available for %s\", family, ip)\n\t\t}\n\n\t\tif _, err := conn.(*net.TCPConn).Dial(); err != nil {\n\t\t\telog.Printf(\"Dial failed for %s (%s)\", addr, err)\n\t\t\tcontinue\n\t\t}\n\t\telog.Printf(\"Dialed %s %s\", ip, addr)\n\t\treturn conn, ip, err\n\t}\n\n\telog.Printf(\"All dial attempts failed for %s\", host)\n\treturn nil, nil, fmt.Errorf(\"failed to dial %s\", host)\n}\n```", "  ```\nfunc Dial(ctx context.Context, elog *slog.Logger, dialer Dialer, host dns.IPDomain, ips []net.IP, port int, dialedIPs map[string][]net.IP, localIPs []net.IP) (conn net.Conn, ip net.IP, rerr error) {\n\tif len(localIPs) > 0 {\n\t\t// Replace the localIPs map with the first matching protocol family in localIPs\n\t\tlocalIPs := &net.IPNet{IP: localIPs[0].IP, Mask: localIPs[0].IP.DefaultMask()}\n\t}\n\tif len(localIPs) > 1 {\n\t\t// Remove the first matching protocol family from localIPs as it was already used\n\t\tlocalIPs = localIPs[1:]\n\t}\n\tfor len(localIPs) == 0 && len(ips) > 0 {\n\t\t// Choose an IP address from the list of IP addresses we haven't tried yet\n\t\tip := ips[0]\n\t\t// Use the chosen IP address for the TCP connection\n\t\tconn, err := DialContext(ctx, dialer, \"tcp\", net.JoinHostPort(host, strconv.Itoa(port)))\n\t\tif conn != nil {\n\t\t\t// Save the list of IP addresses we've tried in the dialedIPs map\n\t\t\tdialedIPs[host] = append(dialedIPs[host], ip)\n\t\t\t// Return the connection and the chosen IP address\n\t\t\treturn conn, ip, nil\n\t\t} else if ctx.Err() == context.DeadlineExceeded {\n\t\t\t// Return the original error\n\t\t\treturn nil, nil, err\n\t\t} else if _, pathError := err.(*net.PathError); pathError {\n\t\t\t// If this is a path error, treat it as a greylisting error\n\t\t\terr = &GreylistingError{\n\t\t\t\tHost:   host,\n\t\t\t\tIP:     ip,\n\t\t\t\tAction: GreylistErrorAction,\n\t\t\t}\n\t\t}\n\t\t// Retry with the next IP address\n\t\tips = ips[1:]\n\t}\n\t// If all IP addresses have been tried and none were successful\n\tif len(ips) == 0 {\n\t\t// Return a not found error\n\t\treturn nil, nil, fmt.Errorf(\"no IP addresses found for %s\", host)\n\t}\n}\n```"]}, "mox-smtpclient/gather.go-GatherIPs": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/smtpclient/gather.go:\n```\npackage smtpclient\n\nimport (\n\t\"context\"\n\t\"crypto/sha256\"\n\t\"crypto/sha512\"\n\t\"crypto/x509\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/mjl-/adns\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n)\n\nvar (\n\terrCNAMELoop  = errors.New(\"cname loop\")\n\terrCNAMELimit = errors.New(\"too many cname records\")\n\terrDNS        = errors.New(\"dns lookup error\")\n\terrNoMail     = errors.New(\"domain does not accept email as indicated with single dot for mx record\")\n)\n\n// GatherDestinations looks up the hosts to deliver email to a domain (\"next-hop\").\n// If it is an IP address, it is the only destination to try. Otherwise CNAMEs of\n// the domain are followed. Then MX records for the expanded CNAME are looked up.\n// If no MX record is present, the original domain is returned. If an MX record is\n// present but indicates the domain does not accept email, ErrNoMail is returned.\n// If valid MX records were found, the MX target hosts are returned.\n//\n// haveMX indicates if an MX record was found.\n//\n// origNextHopAuthentic indicates if the DNS record for the initial domain name was\n// DNSSEC secure (CNAME, MX).\n//\n// expandedNextHopAuthentic indicates if the DNS records after following CNAMEs were\n// DNSSEC secure.\n//\n// These authentic results are needed for DANE, to determine where to look up TLSA\n// records, and which names to allow in the remote TLS certificate. If MX records\n// were found, both the original and expanded next-hops must be authentic for DANE\n// to be option. For a non-IP with no MX records found, the authentic result can\n// be used to decide which of the names to use as TLSA base domain.\nfunc GatherDestinations(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, origNextHop dns.IPDomain) (haveMX, origNextHopAuthentic, expandedNextHopAuthentic bool, expandedNextHop dns.Domain, hosts []dns.IPDomain, permanent bool, err error) {\n\t// ../rfc/5321:3824\n\n\tlog := mlog.New(\"smtpclient\", elog)\n\n\t// IP addresses are dialed directly, and don't have TLSA records.\n\tif len(origNextHop.IP) > 0 {\n\t\treturn false, false, false, expandedNextHop, []dns.IPDomain{origNextHop}, false, nil\n\t}\n\n\t// We start out assuming the result is authentic. Updated with each lookup.\n\torigNextHopAuthentic = true\n\texpandedNextHopAuthentic = true\n\n\t// We start out delivering to the recipient domain. We follow CNAMEs.\n\trcptDomain := origNextHop.Domain\n\t// Domain we are actually delivering to, after following CNAME record(s).\n\texpandedNextHop = rcptDomain\n\t// Keep track of CNAMEs we have followed, to detect loops.\n\tdomainsSeen := map[string]bool{}\n\tfor i := 0; ; i++ {\n\t\tif domainsSeen[expandedNextHop.ASCII] {\n\t\t\t// todo: only mark as permanent failure if TTLs for all records are beyond latest possibly delivery retry we would do.\n\t\t\terr := fmt.Errorf(\"%w: recipient domain %s: already saw %s\", errCNAMELoop, rcptDomain, expandedNextHop)\n\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, false, err\n\t\t}\n\t\tdomainsSeen[expandedNextHop.ASCII] = true\n\n\t\t// note: The Go resolver returns the requested name if the domain has no CNAME\n\t\t// record but has a host record.\n\t\tif i == 16 {\n\t\t\t// We have a maximum number of CNAME records we follow. There is no hard limit for\n\t\t\t// DNS, and you might think folks wouldn't configure CNAME chains at all, but for\n\t\t\t// (non-mail) domains, CNAME chains of 10 records have been encountered according\n\t\t\t// to the internet.\n\t\t\t// todo: only mark as permanent failure if TTLs for all records are beyond latest possibly delivery retry we would do.\n\t\t\terr := fmt.Errorf(\"%w: recipient domain %s, last resolved domain %s\", errCNAMELimit, rcptDomain, expandedNextHop)\n\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, false, err\n\t\t}\n\n\t\t// Do explicit CNAME lookup. Go's LookupMX also resolves CNAMEs, but we want to\n\t\t// know the final name, and we're interested in learning if the first vs later\n\t\t// results were DNSSEC-(in)secure.\n\t\t// ../rfc/5321:3838 ../rfc/3974:197\n\t\tcctx, ccancel := context.WithTimeout(ctx, 30*time.Second)\n\t\tdefer ccancel()\n\t\tcname, cnameResult, err := resolver.LookupCNAME(cctx, expandedNextHop.ASCII+\".\")\n\t\tccancel()\n\t\tif i == 0 {\n\t\t\torigNextHopAuthentic = origNextHopAuthentic && cnameResult.Authentic\n\t\t}\n\t\texpandedNextHopAuthentic = expandedNextHopAuthentic && cnameResult.Authentic\n\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\terr = fmt.Errorf(\"%w: cname lookup for %s: %v\", errDNS, expandedNextHop, err)\n\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, false, err\n\t\t}\n\t\tif err == nil && cname != expandedNextHop.ASCII+\".\" {\n\t\t\td, err := dns.ParseDomain(strings.TrimSuffix(cname, \".\"))\n\t\t\tif err != nil {\n\t\t\t\t// todo: only mark as permanent failure if TTLs for all records are beyond latest possibly delivery retry we would do.\n\t\t\t\terr = fmt.Errorf(\"%w: parsing cname domain %s: %v\", errDNS, expandedNextHop, err)\n\t\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, false, err\n\t\t\t}\n\t\t\texpandedNextHop = d\n\t\t\t// Start again with new domain.\n\t\t\tcontinue\n\t\t}\n\n\t\t// Not a CNAME, so lookup MX record.\n\t\tmctx, mcancel := context.WithTimeout(ctx, 30*time.Second)\n\t\tdefer mcancel()\n\t\t// Note: LookupMX can return an error and still return records: Invalid records are\n\t\t// filtered out and an error returned. We must process any records that are valid.\n\t\t// Only if all are unusable will we return an error. ../rfc/5321:3851\n\t\tmxl, mxResult, err := resolver.LookupMX(mctx, expandedNextHop.ASCII+\".\")\n\t\tmcancel()\n\t\tif i == 0 {\n\t\t\torigNextHopAuthentic = origNextHopAuthentic && mxResult.Authentic\n\t\t}\n\t\texpandedNextHopAuthentic = expandedNextHopAuthentic && mxResult.Authentic\n\t\tif err != nil && len(mxl) == 0 {\n\t\t\tif !dns.IsNotFound(err) {\n\t\t\t\terr = fmt.Errorf(\"%w: mx lookup for %s: %v\", errDNS, expandedNextHop, err)\n\t\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, false, err\n\t\t\t}\n\n\t\t\t// No MX record, attempt delivery directly to host. ../rfc/5321:3842\n\t\t\thosts = []dns.IPDomain{{Domain: expandedNextHop}}\n\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, hosts, false, nil\n\t\t} else if err != nil {\n\t\t\tlog.Infox(\"mx record has some invalid records, keeping only the valid mx records\", err)\n\t\t}\n\n\t\t// ../rfc/7505:122\n\t\tif err == nil && len(mxl) == 1 && mxl[0].Host == \".\" {\n\t\t\t// Note: Depending on MX record TTL, this record may be replaced with a more\n\t\t\t// receptive MX record before our final delivery attempt. But it's clearly the\n\t\t\t// explicit desire not to be bothered with email delivery attempts, so mark failure\n\t\t\t// as permanent.\n\t\t\treturn true, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, true, errNoMail\n\t\t}\n\n\t\t// The Go resolver already sorts by preference, randomizing records of same\n\t\t// preference. ../rfc/5321:3885\n\t\tfor _, mx := range mxl {\n\t\t\t// Parsing lax (unless pedantic mode) for MX targets with underscores as seen in the wild.\n\t\t\thost, err := dns.ParseDomainLax(strings.TrimSuffix(mx.Host, \".\"))\n\t\t\tif err != nil {\n\t\t\t\t// note: should not happen because Go resolver already filters these out.\n\t\t\t\terr = fmt.Errorf(\"%w: invalid host name in mx record %q: %v\", errDNS, mx.Host, err)\n\t\t\t\treturn true, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, true, err\n\t\t\t}\n\t\t\thosts = append(hosts, dns.IPDomain{Domain: host})\n\t\t}\n\t\tif len(hosts) > 0 {\n\t\t\terr = nil\n\t\t}\n\t\treturn true, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, hosts, false, err\n\t}\n}\n\n// GatherIPs looks up the IPs to try for connecting to host, with the IPs ordered\n// to take previous attempts into account. For use with DANE, the CNAME-expanded\n// name is returned, and whether the DNS responses were authentic.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GatherTLSA looks up TLSA record for either expandedHost or host, and returns\n// records usable for DANE with SMTP, and host names to allow in DANE-TA\n// certificate name verification.\n//\n// If no records are found, this isn't necessarily an error. It can just indicate\n// the domain/host does not opt-in to DANE, and nil records and a nil error are\n// returned.\n//\n// Only usable records are returned. If any record was found, DANE is required and\n// this is indicated with daneRequired. If no usable records remain, the caller\n// must do TLS, but not verify the remote TLS certificate.\n//\n// Returned values are always meaningful, also when an error was returned.\nfunc GatherTLSA(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, host dns.Domain, expandedAuthentic bool, expandedHost dns.Domain) (daneRequired bool, daneRecords []adns.TLSA, tlsaBaseDomain dns.Domain, err error) {\n\tlog := mlog.New(\"smtpclient\", elog)\n\n\t// ../rfc/7672:912\n\t// This function is only called when the lookup of host was authentic.\n\n\tvar l []adns.TLSA\n\n\ttlsaBaseDomain = host\n\tif host == expandedHost || !expandedAuthentic {\n\t\tl, err = lookupTLSACNAME(ctx, log, resolver, 25, \"tcp\", host)\n\t} else if expandedAuthentic {\n\t\t// ../rfc/7672:934\n\t\ttlsaBaseDomain = expandedHost\n\t\tl, err = lookupTLSACNAME(ctx, log, resolver, 25, \"tcp\", expandedHost)\n\t\tif err == nil && len(l) == 0 {\n\t\t\ttlsaBaseDomain = host\n\t\t\tl, err = lookupTLSACNAME(ctx, log, resolver, 25, \"tcp\", host)\n\t\t}\n\t}\n\tif len(l) == 0 || err != nil {\n\t\tdaneRequired = err != nil\n\t\tlog.Debugx(\"gathering tlsa records failed\", err, slog.Bool(\"danerequired\", daneRequired), slog.Any(\"basedomain\", tlsaBaseDomain))\n\t\treturn daneRequired, nil, tlsaBaseDomain, err\n\t}\n\tdaneRequired = len(l) > 0\n\tl = filterUsableTLSARecords(log, l)\n\tlog.Debug(\"tlsa records exist\",\n\t\tslog.Bool(\"danerequired\", daneRequired),\n\t\tslog.Any(\"records\", l),\n\t\tslog.Any(\"basedomain\", tlsaBaseDomain))\n\treturn daneRequired, l, tlsaBaseDomain, err\n}\n\n// lookupTLSACNAME composes a TLSA domain name to lookup, follows CNAMEs and looks\n// up TLSA records. no TLSA records exist, a nil error is returned as it means\n// the host does not opt-in to DANE.\nfunc lookupTLSACNAME(ctx context.Context, log mlog.Log, resolver dns.Resolver, port int, protocol string, host dns.Domain) (l []adns.TLSA, rerr error) {\n\tname := fmt.Sprintf(\"_%d._%s.%s\", port, protocol, host.ASCII+\".\")\n\tfor i := 0; ; i++ {\n\t\tcname, result, err := resolver.LookupCNAME(ctx, name)\n\t\tif dns.IsNotFound(err) {\n\t\t\tif !result.Authentic {\n\t\t\t\tlog.Debugx(\"cname nxdomain result during tlsa lookup not authentic, not doing dane for host\", err, slog.Any(\"host\", host), slog.String(\"name\", name))\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t\tbreak\n\t\t} else if err != nil {\n\t\t\treturn nil, fmt.Errorf(\"looking up cname for tlsa candidate base domain: %w\", err)\n\t\t} else if !result.Authentic {\n\t\t\tlog.Debugx(\"cname result during tlsa lookup not authentic, not doing dane for host\", err, slog.Any(\"host\", host), slog.String(\"name\", name))\n\t\t\treturn nil, nil\n\t\t}\n\t\tif i == 10 {\n\t\t\treturn nil, fmt.Errorf(\"looking up cname for tlsa candidate base domain: %w\", errCNAMELimit)\n\t\t}\n\t\tname = strings.TrimSuffix(cname, \".\") + \".\"\n\t}\n\tvar result adns.Result\n\tvar err error\n\tl, result, err = resolver.LookupTLSA(ctx, 0, \"\", name)\n\tif dns.IsNotFound(err) || err == nil && len(l) == 0 {\n\t\tlog.Debugx(\"no tlsa records for host, not doing dane\", err,\n\t\t\tslog.Any(\"host\", host),\n\t\t\tslog.String(\"name\", name),\n\t\t\tslog.Bool(\"authentic\", result.Authentic))\n\t\treturn nil, nil\n\t} else if err != nil {\n\t\treturn nil, fmt.Errorf(\"looking up tlsa records for tlsa candidate base domain: %w\", err)\n\t} else if !result.Authentic {\n\t\tlog.Debugx(\"tlsa lookup not authentic, not doing dane for host\", err, slog.Any(\"host\", host), slog.String(\"name\", name))\n\t\treturn nil, nil\n\t}\n\treturn l, nil\n}\n\nfunc filterUsableTLSARecords(log mlog.Log, l []adns.TLSA) []adns.TLSA {\n\t// Gather \"usable\" records. ../rfc/7672:708\n\to := 0\n\tfor _, r := range l {\n\t\t// A record is not usable when we don't recognize parameters. ../rfc/6698:649\n\n\t\tswitch r.Usage {\n\t\tcase adns.TLSAUsageDANETA, adns.TLSAUsageDANEEE:\n\t\tdefault:\n\t\t\t// We can regard PKIX-TA and PKIX-EE as \"unusable\" with SMTP DANE. ../rfc/7672:1304\n\t\t\tcontinue\n\t\t}\n\t\tswitch r.Selector {\n\t\tcase adns.TLSASelectorCert, adns.TLSASelectorSPKI:\n\t\tdefault:\n\t\t\tcontinue\n\t\t}\n\t\tswitch r.MatchType {\n\t\tcase adns.TLSAMatchTypeFull:\n\t\t\tif r.Selector == adns.TLSASelectorCert {\n\t\t\t\tif _, err := x509.ParseCertificate(r.CertAssoc); err != nil {\n\t\t\t\t\tlog.Debugx(\"parsing certificate in dane tlsa record, ignoring\", err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t} else if r.Selector == adns.TLSASelectorSPKI {\n\t\t\t\tif _, err := x509.ParsePKIXPublicKey(r.CertAssoc); err != nil {\n\t\t\t\t\tlog.Debugx(\"parsing certificate in dane tlsa record, ignoring\", err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\tcase adns.TLSAMatchTypeSHA256:\n\t\t\tif len(r.CertAssoc) != sha256.Size {\n\t\t\t\tlog.Debug(\"dane tlsa record with wrong data size for sha2-256\", slog.Int(\"got\", len(r.CertAssoc)), slog.Int(\"expect\", sha256.Size))\n\t\t\t\tcontinue\n\t\t\t}\n\t\tcase adns.TLSAMatchTypeSHA512:\n\t\t\tif len(r.CertAssoc) != sha512.Size {\n\t\t\t\tlog.Debug(\"dane tlsa record with wrong data size for sha2-512\", slog.Int(\"got\", len(r.CertAssoc)), slog.Int(\"expect\", sha512.Size))\n\t\t\t\tcontinue\n\t\t\t}\n\t\tdefault:\n\t\t\tcontinue\n\t\t}\n\n\t\tl[o] = r\n\t\to++\n\t}\n\treturn l[:o]\n}\n\n// GatherTLSANames returns the allowed names in TLS certificates for verification\n// with PKIX-* or DANE-TA. The first name should be used for SNI.\n//\n// If there was no MX record, the next-hop domain parameters (i.e. the original\n// email destination host, and its CNAME-expanded host, that has MX records) are\n// ignored and only the base domain parameters are taken into account.\nfunc GatherTLSANames(haveMX, expandedNextHopAuthentic, expandedTLSABaseDomainAuthentic bool, origNextHop, expandedNextHop, origTLSABaseDomain, expandedTLSABaseDomain dns.Domain) []dns.Domain {\n\t// Gather the names to check against TLS certificate. ../rfc/7672:1318\n\tif !haveMX {\n\t\t// ../rfc/7672:1336\n\t\tif !expandedTLSABaseDomainAuthentic || origTLSABaseDomain == expandedTLSABaseDomain {\n\t\t\treturn []dns.Domain{origTLSABaseDomain}\n\t\t}\n\t\treturn []dns.Domain{expandedTLSABaseDomain, origTLSABaseDomain}\n\t} else if expandedNextHopAuthentic {\n\t\t// ../rfc/7672:1326\n\t\tvar l []dns.Domain\n\t\tif expandedTLSABaseDomainAuthentic {\n\t\t\tl = []dns.Domain{expandedTLSABaseDomain}\n\t\t}\n\t\tif expandedTLSABaseDomain != origTLSABaseDomain {\n\t\t\tl = append(l, origTLSABaseDomain)\n\t\t}\n\t\tl = append(l, origNextHop)\n\t\tif origNextHop != expandedNextHop {\n\t\t\tl = append(l, expandedNextHop)\n\t\t}\n\t\treturn l\n\t} else {\n\t\t// We don't attempt DANE after insecure MX, but behaviour for it is specified.\n\t\t// ../rfc/7672:1332\n\t\treturn []dns.Domain{origNextHop}\n\t}\n}\n\n```\n# Function Signature: func GatherIPs(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, network string, host dns.IPDomain, dialedIPs map[string][]net.IP) (authentic bool, expandedAuthentic bool, expandedHost dns.Domain, ips []net.IP, dualstack bool, rerr error) {\n# Natural Language Annotations: // GatherIPs looks up the IPs to try for connecting to host, with the IPs ordered\n// to take previous attempts into account. For use with DANE, the CNAME-expanded\n// name is returned, and whether the DNS responses were authentic.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc GatherIPs(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, network string, host dns.IPDomain, dialedIPs map[string][]net.IP) (authentic bool, expandedAuthentic bool, expandedHost dns.Domain, ips []net.IP, dualstack bool, rerr error) {\n\tlog := mlog.New(\"smtpclient\", elog)\n\n\t// IP addresses are dialed directly, and don't have TLSA records.\n\tif len(host.IP) > 0 {\n\t\treturn false, false, expandedHost, []net.IP{host.IP}, false, nil\n\t}\n\n\t// Start with CNAME resolution\n\trcptDomain := host.Domain\n\tfor i := 0; ; i++ {\n\t\tif domainsSeen[expandedNextHop.ASCII] {\n\t\t\t// todo: only mark as permanent failure if TTLs for all records are beyond latest possibly delivery retry we would do.\n\t\t\terr := fmt.Errorf(\"%w: recipient domain %s: already saw %s\", errCNAMELoop, rcptDomain, expandedNextHop)\n\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, false, err\n\t\t}\n\t\tdomainsSeen[expandedNextHop.ASCII] = true\n\n\t\t// note: The Go resolver returns the requested name if the domain has no CNAME\n\t\t// record but has a host record.\n\t\tif i == 16 {\n\t\t\t// We have a maximum number of CNAME records we follow. There is no hard limit for\n\t\t\t// DNS, and you might think folks wouldn't configure CNAME chains at all, but for\n\t\t\t// (non-mail) domains, CNAME chains of 10 records have been encountered according\n\t\t\t// to the internet.\n\t\t\t// todo: only mark as permanent failure if TTLs for all records are beyond latest possibly delivery retry we would do.\n\t\t\terr := fmt.Errorf(\"%w: recipient domain %s, last resolved domain %s\", errCNAMELimit, rcptDomain, expandedNextHop)\n\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, false, err\n\t\t}\n\n\t\t// Do explicit CNAME lookup. Go's LookupMX also resolves CNAMEs, but we want to\n\t\t// know the final name, and we're interested in learning if the first vs later\n\t\t// results were DNSSEC-(in)secure.\n\t\t// ../rfc/5321:3838 ../rfc/3974:197\n\t\tcctx, ccancel := context.WithTimeout(ctx, 30*time.Second)\n\t\tdefer ccancel()\n\t\tcname, cnameResult, err := resolver.LookupCNAME(cctx, expandedNextHop.ASCII+\".\")\n\t\tccancel()\n\t\tif i == 0 {\n\t\t\torigNextHopAuthentic = origNextHopAuthentic && cnameResult.Authentic\n\t\t}\n\t\texpandedNextHopAuthentic = expandedNextHopAuthentic && cnameResult.Authentic\n\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\terr = fmt.Errorf(\"%w: cname lookup for %s: %v\", errDNS, expandedNextHop, err)\n\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, false, err\n\t\t}\n\t\tif err == nil && cname != expandedNextHop.ASCII+\".\" {\n\t\t\td, err := dns.ParseDomain(strings.TrimSuffix(cname, \".\"))\n\t\t\tif err != nil {\n\t\t\t\t// todo: only mark as permanent failure if TTLs for all records are beyond latest possibly delivery retry we would do.\n\t\t\t\terr = fmt.Errorf(\"%w: parsing cname domain %s: %v\", errDNS, expandedNextHop, err)\n\t\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextH", "  ```\nfunc GatherIPs(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, network string, host dns.IPDomain, dialedIPs map[string][]net.IP) (authentic bool, expandedAuthentic bool, expandedHost dns.Domain, ips []net.IP, dualstack bool, rerr error) {\n\tlog := mlog.New(\"smtpclient\", elog)\n\n\t// DNS lookup\n\tvar err error\n\tvar cnameResult adns.Result\n\tvar hostResult adns.Result\n\tvar mxResult adns.Result\n\tvar dualstackResult adns.Result\n\tvar mxMismatch bool\n\tvar name string\n\n\t// Authentic names to CNAME\n\texpandedHost, err = cname(ctx, log, resolver, host)\n\tif err != nil {\n\t\tif !dns.IsNotFound(err) {\n\t\t\treturn false, false, \"\", nil, false, err\n\t\t}\n\t\texpandedAuthentic = false\n\t\texpandedHost = host\n\t} else {\n\t\texpandedAuthentic = cnameResult.Authentic\n\t}\n\n\t// Authentic lookup MX\n\tif len(expandedHost) > 0 {\n\t\t_, mxResult, err = lookupMX(ctx, log, resolver, expandedHost[0])\n\t\tif err != nil || len(mxResult) == 0 {\n\t\t\tif !dns.IsNotFound(err) {\n\t\t\t\treturn false, expandedAuthentic, expandedHost, nil, false, err\n\t\t\t}\n\t\t\tmxMismatch = false\n\t\t} else {\n\t\t\tmxMismatch = expandedHost[0] != host.Domain\n\t\t}\n\t}\n\n\t// Authentic lookup IPs\n\tipLookups := []string{expandedHost[0]}\n\tif mxMismatch {\n\t\tipLookups = append(ipLookups, host.Domain.ASCII)\n\t}\n\tvar hostIPs []net.IP\n\tfor _, lookup := range ipLookups {\n\t\tips, hostResult, err = lookupIP(ctx, log, resolver, network, lookup)\n\t\tif err != nil {\n\t\t\tif !dns.IsNotFound(err) {\n\t\t\t\treturn false, expandedAuthentic, expandedHost, nil, false, err\n\t\t\t}\n\t\t\tif _, ok := dialedIPs[lookup]; !ok {\n\t\t\t\treturn false, expandedAuthentic, expandedHost, nil, false, err\n\t\t\t}\n\t\t} else if len(ips) > 0 {\n\t\t\thostIPs = append(hostIPs, ips...)\n\t\t}\n\t}\n\n\t// Dualstack lookup\n\tif len(hostIPs) > 0 {\n\t\t_, dualstackResult, err = lookupTLSACNAME(ctx, log, resolver, 443, \"tcp\", expandedHost)\n\t\tif err != nil {\n\t\t\tif !dns.IsNotFound(err) {\n\t\t\t\treturn false, expandedAuthentic, expandedHost, nil, false, err\n\t\t\t}\n\t\t} else {\n\t\t\tdualstack = dualstackResult.Authentic\n\t\t}\n\t}\n\n\t// Remove any duplicate IPs and sort them to ensure stable ordering.\n\t// We're not going to do reverse DNS lookup on the IPs, so using the host's\n\t// name is not going to be helpful.\n\tips := removeDuplicates(hostIPs)\n\tsort.Slice(ips, func(i, j int) bool {\n\t\treturn bytes.Compare(ips[i], ips[j]) == -1\n\t})\n\n\treturn expandedAuthentic, cnameResult.Authentic, cnameResult.Result, ips, dualstack, nil\n}\n\nfunc cname(ctx context.Context, log mlog.Log, resolver dns.Resolver, host dns.IPDomain) (dns.Domain, error) {\n\texpandedHost, result, err := resolver.LookupCNAME(ctx, host.Domain.ASCII)\n\tif err != nil {\n\t\tif !dns.IsNotFound(err) {\n\t\t\tlog.Debugx(\"cname lookup failed\", err, slog.Any", "  ```\nfunc GatherIPs(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, network string, host dns.IPDomain, dialedIPs map[string][]net.IP) (authentic bool, expandedAuthentic bool, expandedHost dns.Domain, ips []net.IP, dualstack bool, rerr error) {\n\tlog := mlog.New(\"smtpclient\", elog)\n\n\t// We start out assuming the result is authentic. Updated with each lookup.\n\tauthentic = true\n\texpandedAuthentic = true\n\n\t// If the domain we're delivering to has no MX records, we're delivering directly to the host.\n\tif network == \"tcp\" && host.Domain == host.IPDomain {\n\t\treturn false, false, host.Domain, []net.IP{host.IP}, false, nil\n\t}\n\n\t// Do the initial A,AAAA lookup.\n\tvar domain dns.Domain\n\tvar result adns.Result\n\tvar err error\n\tdomain = host.IPDomain\n\tips, result, err = resolver.AllIP(ctx, domain.ASCII)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"%w: lookup of domain %s: %v\", errDNS, domain, err)\n\t\treturn false, false, domain, nil, false, err\n\t}\n\n\t// If the A/AAAA record was a wildcard record, then we need to lookup to\n\t// find the actual underlying address.\n\tvar rIPs []net.IP\n\tvar rIP net.IP\n\tfor _, ip := range ips {\n\t\tif x509.IsWildcardAddress(ip) {\n\t\t\trctx, rcancel := context.WithTimeout(ctx, 30*time.Second)\n\t\t\tdefer rcancel()\n\t\t\tname, result, err := resolver.LookupPTR(rctx, ip.To16().String())\n\t\t\tif err != nil {\n\t\t\t\treturn false, false, domain, nil, false, fmt.Errorf(\"%w: lookup of ptr for wildcard address %s: %v\", errDNS, ip, err)\n\t\t\t}\n\t\t\trIP, err = ipForName(name)\n\t\t\tif err != nil {\n\t\t\t\treturn false, false, domain, nil, false, fmt.Errorf(\"%w: lookup of ptr for wildcard address %s: %v\", errDNS, ip, err)\n\t\t\t}\n\t\t} else {\n\t\t\trIP = ip\n\t\t}\n\t\trIPs = append(rIPs, rIP)\n\t}\n\n\t// Check if the lookup of the domain was DNSSEC authentic.\n\tauthentic = authentic && result.Authentic\n\n\t// Check if the result of the lookup contained IPv4 and IPv6 addresses. If both\n\t// were present, we set dualstack = true, which tells the PTR lookup to try\n\t// IPv4 and IPv6 addresses in the same pass.\n\tdualstack = len(rIPs) > 0 && (len(ips) > 1 || rIPs[0].To16().Size() == 16)\n\n\t// Return the IPs with their original authentication state.\n\treturn authentic, expandedAuthentic, domain, rIPs, dualstack, nil\n}\n```"]}, "mox-tlsrpt/report.go-TLSFailureDetails": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/tlsrpt/report.go:\n```\npackage tlsrpt\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net\"\n\t\"os\"\n\t\"reflect\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/mjl-/adns\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/moxio\"\n)\n\nvar ErrNoReport = errors.New(\"no tlsrpt report found\")\n\n// ../rfc/8460:628\n\n// Report is a TLSRPT report.\ntype Report struct {\n\tOrganizationName string\n\tDateRange        TLSRPTDateRange\n\tContactInfo      string\n\tReportID         string\n\tPolicies         []Result\n}\n\n// ReportJSON is a TLS report with field names as used in the specification. These field names are inconvenient to use in JavaScript, so after parsing a ReportJSON is turned into a Report.\ntype ReportJSON struct {\n\tOrganizationName string              `json:\"organization-name\"`\n\tDateRange        TLSRPTDateRangeJSON `json:\"date-range\"`\n\tContactInfo      string              `json:\"contact-info\"` // Email address.\n\tReportID         string              `json:\"report-id\"`\n\tPolicies         []ResultJSON        `json:\"policies\"`\n}\n\nfunc convertSlice[T interface{ Convert() S }, S any](l []T) []S {\n\tif l == nil {\n\t\treturn nil\n\t}\n\tr := make([]S, len(l))\n\tfor i, e := range l {\n\t\tr[i] = e.Convert()\n\t}\n\treturn r\n}\n\nfunc (v Report) Convert() ReportJSON {\n\treturn ReportJSON{v.OrganizationName, v.DateRange.Convert(), v.ContactInfo, v.ReportID, convertSlice[Result, ResultJSON](v.Policies)}\n}\n\nfunc (v ReportJSON) Convert() Report {\n\treturn Report{v.OrganizationName, v.DateRange.Convert(), v.ContactInfo, v.ReportID, convertSlice[ResultJSON, Result](v.Policies)}\n}\n\n// Merge combines the counts and failure details of results into the report.\n// Policies are merged if identical and added otherwise. Same for failure details\n// within a result.\nfunc (r *Report) Merge(results ...Result) {\nMerge:\n\tfor _, nr := range results {\n\t\tfor i, p := range r.Policies {\n\t\t\tif !p.Policy.equal(nr.Policy) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tr.Policies[i].Add(nr.Summary.TotalSuccessfulSessionCount, nr.Summary.TotalFailureSessionCount, nr.FailureDetails...)\n\t\t\tcontinue Merge\n\t\t}\n\n\t\tr.Policies = append(r.Policies, nr)\n\t}\n}\n\n// Add increases the success/failure counts of a result, and adds any failure\n// details.\nfunc (r *Result) Add(success, failure int64, fds ...FailureDetails) {\n\tr.Summary.TotalSuccessfulSessionCount += success\n\tr.Summary.TotalFailureSessionCount += failure\n\n\t// In smtpclient we can compensate with a negative success, after failed read after\n\t// successful handshake. Sanity check that we never get negative counts.\n\tif r.Summary.TotalSuccessfulSessionCount < 0 {\n\t\tr.Summary.TotalSuccessfulSessionCount = 0\n\t}\n\tif r.Summary.TotalFailureSessionCount < 0 {\n\t\tr.Summary.TotalFailureSessionCount = 0\n\t}\n\nMerge:\n\tfor _, nfd := range fds {\n\t\tfor i, fd := range r.FailureDetails {\n\t\t\tif !fd.equalKey(nfd) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tfd.FailedSessionCount += nfd.FailedSessionCount\n\t\t\tr.FailureDetails[i] = fd\n\t\t\tcontinue Merge\n\t\t}\n\t\tr.FailureDetails = append(r.FailureDetails, nfd)\n\t}\n}\n\n// Add is a convenience function for merging making a Result and merging it into\n// the report.\nfunc (r *Report) Add(policy ResultPolicy, success, failure int64, fds ...FailureDetails) {\n\tr.Merge(Result{policy, Summary{success, failure}, fds})\n}\n\n// TLSAPolicy returns a policy for DANE.\nfunc TLSAPolicy(records []adns.TLSA, tlsaBaseDomain dns.Domain) ResultPolicy {\n\t// The policy domain is the TLSA base domain. ../rfc/8460:251\n\n\tl := make([]string, len(records))\n\tfor i, r := range records {\n\t\tl[i] = r.Record()\n\t}\n\tsort.Strings(l) // For consistent equals.\n\treturn ResultPolicy{\n\t\tType:   TLSA,\n\t\tString: l,\n\t\tDomain: tlsaBaseDomain.ASCII,\n\t\tMXHost: []string{},\n\t}\n}\n\nfunc MakeResult(policyType PolicyType, domain dns.Domain, fds ...FailureDetails) Result {\n\tif fds == nil {\n\t\tfds = []FailureDetails{}\n\t}\n\treturn Result{\n\t\tPolicy:         ResultPolicy{Type: policyType, Domain: domain.ASCII, String: []string{}, MXHost: []string{}},\n\t\tFailureDetails: fds,\n\t}\n}\n\n// note: with TLSRPT prefix to prevent clash in sherpadoc types.\ntype TLSRPTDateRange struct {\n\tStart time.Time\n\tEnd   time.Time\n}\n\nfunc (v TLSRPTDateRange) Convert() TLSRPTDateRangeJSON {\n\treturn TLSRPTDateRangeJSON(v)\n}\n\ntype TLSRPTDateRangeJSON struct {\n\tStart time.Time `json:\"start-datetime\"`\n\tEnd   time.Time `json:\"end-datetime\"`\n}\n\nfunc (v TLSRPTDateRangeJSON) Convert() TLSRPTDateRange {\n\treturn TLSRPTDateRange(v)\n}\n\n// UnmarshalJSON is defined on the date range, not the individual time.Time fields\n// because it is easier to keep the unmodified time.Time fields stored in the\n// database.\nfunc (dr *TLSRPTDateRangeJSON) UnmarshalJSON(buf []byte) error {\n\tvar v struct {\n\t\tStart xtime `json:\"start-datetime\"`\n\t\tEnd   xtime `json:\"end-datetime\"`\n\t}\n\tif err := json.Unmarshal(buf, &v); err != nil {\n\t\treturn err\n\t}\n\tdr.Start = time.Time(v.Start)\n\tdr.End = time.Time(v.End)\n\treturn nil\n}\n\n// xtime and its UnmarshalJSON exists to work around a specific invalid date-time encoding seen in the wild.\ntype xtime time.Time\n\nfunc (x *xtime) UnmarshalJSON(buf []byte) error {\n\tvar t time.Time\n\terr := t.UnmarshalJSON(buf)\n\tif err == nil {\n\t\t*x = xtime(t)\n\t\treturn nil\n\t}\n\n\t// Microsoft is sending reports with invalid start-datetime/end-datetime (missing\n\t// timezone, ../rfc/8460:682 ../rfc/3339:415). We compensate.\n\tvar s string\n\tif err := json.Unmarshal(buf, &s); err != nil {\n\t\treturn err\n\t}\n\tt, err = time.Parse(\"2006-01-02T15:04:05\", s)\n\tif err != nil {\n\t\treturn err\n\t}\n\t*x = xtime(t)\n\treturn nil\n}\n\ntype Result struct {\n\tPolicy         ResultPolicy\n\tSummary        Summary\n\tFailureDetails []FailureDetails\n}\n\nfunc (r Result) Convert() ResultJSON {\n\treturn ResultJSON{ResultPolicyJSON(r.Policy), SummaryJSON(r.Summary), convertSlice[FailureDetails, FailureDetailsJSON](r.FailureDetails)}\n}\n\ntype ResultJSON struct {\n\tPolicy         ResultPolicyJSON     `json:\"policy\"`\n\tSummary        SummaryJSON          `json:\"summary\"`\n\tFailureDetails []FailureDetailsJSON `json:\"failure-details\"`\n}\n\nfunc (r ResultJSON) Convert() Result {\n\treturn Result{ResultPolicy(r.Policy), Summary(r.Summary), convertSlice[FailureDetailsJSON, FailureDetails](r.FailureDetails)}\n}\n\n// todo spec: ../rfc/8460:437 says policy is a string, with rules for turning dane records into a single string. perhaps a remnant of an earlier version (for mtasts a single string would have made more sense). i doubt the intention is to always have a single element in policy-string (though the field name is singular).\n\ntype ResultPolicy struct {\n\tType   PolicyType\n\tString []string\n\tDomain string // ASCII/A-labels, ../rfc/8460:704\n\tMXHost []string\n}\n\ntype ResultPolicyJSON struct {\n\tType   PolicyType `json:\"policy-type\"`\n\tString []string   `json:\"policy-string\"`\n\tDomain string     `json:\"policy-domain\"`\n\tMXHost []string   `json:\"mx-host\"` // Example in RFC has errata, it originally was a single string. ../rfc/8460-eid6241 ../rfc/8460:1779\n}\n\n// PolicyType indicates the policy success/failure results are for.\ntype PolicyType string\n\nconst (\n\t// For DANE, against a mail host (not recipient domain).\n\tTLSA PolicyType = \"tlsa\"\n\n\t// For MTA-STS, against a recipient domain (not a mail host).\n\tSTS PolicyType = \"sts\"\n\n\t// Recipient domain did not have MTA-STS policy, or mail host (TSLA base domain)\n\t// did not have DANE TLSA records.\n\tNoPolicyFound PolicyType = \"no-policy-found\"\n\t// todo spec: ../rfc/8460:440 ../rfc/8460:697 suggest to replace with values like \"no-sts-found\" and \"no-tlsa-found\" to make it explicit which policy isn't found. also easier to implement, because you don't have to handle leaving out an sts no-policy-found result for a mail host when a tlsa policy is present.\n)\n\nfunc (rp ResultPolicy) equal(orp ResultPolicy) bool {\n\treturn rp.Type == orp.Type && slices.Equal(rp.String, orp.String) && rp.Domain == orp.Domain && slices.Equal(rp.MXHost, orp.MXHost)\n}\n\ntype Summary struct {\n\tTotalSuccessfulSessionCount int64\n\tTotalFailureSessionCount    int64\n}\n\ntype SummaryJSON struct {\n\tTotalSuccessfulSessionCount int64 `json:\"total-successful-session-count\"`\n\tTotalFailureSessionCount    int64 `json:\"total-failure-session-count\"`\n}\n\n// ResultType represents a TLS error.\ntype ResultType string\n\n// ../rfc/8460:1377\n// https://www.iana.org/assignments/starttls-validation-result-types/starttls-validation-result-types.xhtml\n\nconst (\n\tResultSTARTTLSNotSupported    ResultType = \"starttls-not-supported\"\n\tResultCertificateHostMismatch ResultType = \"certificate-host-mismatch\"\n\tResultCertificateExpired      ResultType = \"certificate-expired\"\n\tResultTLSAInvalid             ResultType = \"tlsa-invalid\"\n\tResultDNSSECInvalid           ResultType = \"dnssec-invalid\"\n\tResultDANERequired            ResultType = \"dane-required\"\n\tResultCertificateNotTrusted   ResultType = \"certificate-not-trusted\"\n\tResultSTSPolicyInvalid        ResultType = \"sts-policy-invalid\"\n\tResultSTSWebPKIInvalid        ResultType = \"sts-webpki-invalid\"\n\tResultValidationFailure       ResultType = \"validation-failure\" // Other error.\n\tResultSTSPolicyFetch          ResultType = \"sts-policy-fetch-error\"\n)\n\n// todo spec: ../rfc/8460:719 more of these fields should be optional. some sts failure details, like failed policy fetches, won't have an ip or mx, the failure happens earlier in the delivery process.\n\ntype FailureDetails struct {\n\tResultType            ResultType\n\tSendingMTAIP          string\n\tReceivingMXHostname   string\n\tReceivingMXHelo       string\n\tReceivingIP           string\n\tFailedSessionCount    int64\n\tAdditionalInformation string\n\tFailureReasonCode     string\n}\n\nfunc (v FailureDetails) Convert() FailureDetailsJSON { return FailureDetailsJSON(v) }\n\ntype FailureDetailsJSON struct {\n\tResultType            ResultType `json:\"result-type\"`\n\tSendingMTAIP          string     `json:\"sending-mta-ip\"`\n\tReceivingMXHostname   string     `json:\"receiving-mx-hostname\"`\n\tReceivingMXHelo       string     `json:\"receiving-mx-helo,omitempty\"`\n\tReceivingIP           string     `json:\"receiving-ip\"`\n\tFailedSessionCount    int64      `json:\"failed-session-count\"`\n\tAdditionalInformation string     `json:\"additional-information\"`\n\tFailureReasonCode     string     `json:\"failure-reason-code\"`\n}\n\nfunc (v FailureDetailsJSON) Convert() FailureDetails { return FailureDetails(v) }\n\n// equalKey returns whether FailureDetails have the same values, expect for\n// FailedSessionCount. Useful for aggregating FailureDetails.\nfunc (fd FailureDetails) equalKey(ofd FailureDetails) bool {\n\tfd.FailedSessionCount = 0\n\tofd.FailedSessionCount = 0\n\treturn fd == ofd\n}\n\n// Details is a convenience function to compose a FailureDetails.\nfunc Details(t ResultType, r string) FailureDetails {\n\treturn FailureDetails{ResultType: t, FailedSessionCount: 1, FailureReasonCode: r}\n}\n\nvar invalidReasons = map[x509.InvalidReason]string{\n\tx509.NotAuthorizedToSign:           \"not-authorized-to-sign\",\n\tx509.Expired:                       \"certificate-expired\",\n\tx509.CANotAuthorizedForThisName:    \"ca-not-authorized-for-this-name\",\n\tx509.TooManyIntermediates:          \"too-many-intermediates\",\n\tx509.IncompatibleUsage:             \"incompatible-key-usage\",\n\tx509.NameMismatch:                  \"parent-subject-child-issuer-mismatch\",\n\tx509.NameConstraintsWithoutSANs:    \"name-constraint-without-sans\",\n\tx509.UnconstrainedName:             \"unconstrained-name\",\n\tx509.TooManyConstraints:            \"too-many-constraints\",\n\tx509.CANotAuthorizedForExtKeyUsage: \"ca-not-authorized-for-ext-key-usage\",\n}\n\n// TLSFailureDetails turns errors encountered during TLS handshakes into a result\n// type and failure reason code for use with FailureDetails.\n//\n// Errors from crypto/tls, including local and remote alerts, from crypto/x509,\n// and generic i/o and timeout errors are recognized.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Parse parses a Report.\n// The maximum size is 20MB.\nfunc Parse(r io.Reader) (*ReportJSON, error) {\n\tr = &moxio.LimitReader{R: r, Limit: 20 * 1024 * 1024}\n\tvar report ReportJSON\n\tif err := json.NewDecoder(r).Decode(&report); err != nil {\n\t\treturn nil, err\n\t}\n\t// note: there may be leftover data, we ignore it.\n\treturn &report, nil\n}\n\n// ParseMessage parses a Report from a mail message.\n// The maximum size of the message is 15MB, the maximum size of the\n// decompressed report is 20MB.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc parseMessageReport(log mlog.Log, p message.Part, allow bool) (*ReportJSON, error) {\n\tif p.MediaType != \"MULTIPART\" {\n\t\tif !allow {\n\t\t\treturn nil, ErrNoReport\n\t\t}\n\t\treturn parseReport(p)\n\t}\n\n\tfor {\n\t\tsp, err := p.ParseNextPart(log.Logger)\n\t\tif err == io.EOF {\n\t\t\treturn nil, ErrNoReport\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif p.MediaSubType == \"REPORT\" && p.ContentTypeParams[\"report-type\"] != \"tlsrpt\" {\n\t\t\treturn nil, fmt.Errorf(\"unknown report-type parameter %q\", p.ContentTypeParams[\"report-type\"])\n\t\t}\n\t\treport, err := parseMessageReport(log, *sp, p.MediaSubType == \"REPORT\")\n\t\tif err == ErrNoReport {\n\t\t\tcontinue\n\t\t} else if err != nil || report != nil {\n\t\t\treturn report, err\n\t\t}\n\t}\n}\n\nfunc parseReport(p message.Part) (*ReportJSON, error) {\n\tmt := strings.ToLower(p.MediaType + \"/\" + p.MediaSubType)\n\tswitch mt {\n\tcase \"application/tlsrpt+json\":\n\t\treturn Parse(p.Reader())\n\tcase \"application/tlsrpt+gzip\":\n\t\tgzr, err := gzip.NewReader(p.Reader())\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"decoding gzip TLSRPT report: %s\", err)\n\t\t}\n\t\treturn Parse(gzr)\n\t}\n\treturn nil, ErrNoReport\n}\n\n```\n# Function Signature: func TLSFailureDetails(err error) (ResultType, string) {\n# Natural Language Annotations: // TLSFailureDetails turns errors encountered during TLS handshakes into a result\n// type and failure reason code for use with FailureDetails.\n//\n// Errors from crypto/tls, including local and remote alerts, from crypto/x509,\n// and generic i/o and timeout errors are recognized.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc TLSFailureDetails(err error) (ResultType, string) {\n\tif err, ok := err.(*tls.AlertError); ok {\n\t\tswitch err.(*tls.AlertError).AlertDescription {\n\t\tcase tls.AlertHandlerOverflow:\n\t\t\treturn ResultSTARTTLSNotSupported, \"tls-alert-generic\"\n\t\tcase tls.AlertProtocolVersion:\n\t\t\treturn ResultSTARTTLSNotSupported, \"tls-alert-generic\"\n\t\tcase tls.AlertBadRecordMAC:\n\t\t\treturn ResultValidationFailure, \"tls-alert-bad-record mac\"\n\t\tcase tls.AlertDecompressionFailure:\n\t\t\treturn ResultValidationFailure, \"tls-alert-decompression-failure\"\n\t\tcase tls.AlertHandshakeFailure:\n\t\t\treturn ResultValidationFailure, \"tls-alert-handshake-failure\"\n\t\tcase tls.AlertNoCertificate:\n\t\t\treturn ResultValidationFailure, \"tls-alert-no-certificate\"\n\t\tcase tls.AlertBadCertificate:\n\t\t\treturn ResultCertificateNotTrusted, \"tls-alert-bad-certificate\"\n\t\tcase tls.AlertUnsupportedCertificate:\n\t\t\treturn ResultValidationFailure, \"tls-alert-unsupported-certificate\"\n\t\tcase tls.AlertCertificateRevoked:\n\t\t\treturn ResultValidationFailure, \"tls-alert-cert-revoke-no-revoke\"\n\t\tcase tls.AlertCertificateExpired:\n\t\t\treturn ResultCertificateExpired, \"tls-alert-cert-expired\"\n\t\tcase tls.AlertCertificateUnknown:\n\t\t\treturn ResultValidationFailure, \"tls-alert-cert-unknown\"\n\t\tcase tls.AlertIllegalParameter:\n\t\t\treturn ResultSTSPolicyInvalid, \"tls-alert-illegal-parameter\"\n\t\tcase tls.AlertUnknownCA:\n\t\t\treturn ResultCertificateNotTrusted, \"tls-alert-unknown-ca\"\n\t\tcase tls.AlertAccessDenied:\n\t\t\treturn ResultValidationFailure, \"tls-alert-accsess-denied\"\n\t\tcase tls.AlertDecodeError:\n\t\t\treturn ResultValidationFailure, \"tls-alert-decode-error\"\n\t\tcase tls.AlertDecryptError:\n\t\t\treturn ResultValidationFailure, \"tls-alert-decrypt-error\"\n\t\tcase tls.AlertProtocolVersion:\n\t\t\treturn ResultValidationFailure, \"tls-alert-protocol-version\"\n\t\tcase tls.AlertInappropriateFallback:\n\t\t\treturn ResultValidationFailure, \"tls-alert-inappropriate-fallback\"\n\t\tcase tls.AlertInsufficientSecurity:\n\t\t\treturn ResultValidationFailure, \"tls-alert-insufficient-security\"\n\t\tcase tls.AlertInternalError:\n\t\t\treturn ResultValidationFailure, \"tls-alert-internal-error\"\n\t\tcase tls.AlertInsufficientSecurity:\n\t\t\treturn ResultValidationFailure, \"tls-alert-insufficient-security\"\n\t\tcase tls.AlertUserCanceled:\n\t\t\treturn ResultValidationFailure, \"tls-alert-user-canceled\"\n\t\tcase tls.AlertNoRenegotiation:\n\t\t\treturn ResultValidationFailure, \"tls-alert-no-renegotiation\"\n\t\tcase tls.AlertUnsupportedExtension:\n\t\t\treturn ResultValidationFailure, \"tls-alert-unsupported-extension\"\n\t\tcase tls.AlertUnknownPSKIdentity:\n\t\t\treturn ResultValidationFailure, \"tls-alert-unknown-psk-identity\"\n\t\tcase tls.AlertCertificateRequired:\n\t\t\treturn ResultValidationFailure, \"tls-alert-certificate-required\"\n\t\tcase tls.AlertNoApplicationProtocol:\n\t\t\treturn ResultValidationFailure, \"tls-alert-no-application-protocol\"\n\t\tdefault:\n\t\t\treturn ResultValidationFailure, \"unknown\"\n\t\t}\n\t}\n\n\tif x509Err, ok := err.(x509.HostnameError); ok {\n\t\tswitch x509Err.(*", "  ```\nfunc TLSFailureDetails(err error) (ResultType, string) {\n\tswitch err := err.(type) {\n\tcase *net.OpError:\n\t\tif err.Op == \"local error: tls: handshake failure\" {\n\t\t\treturn ResultValidationFailure, \"tls-handshake-local-error\"\n\t\t}\n\t\tif err.Op == \"remote error: tls: handshake failure\" {\n\t\t\treturn ResultValidationFailure, \"tls-handshake-remote-error\"\n\t\t}\n\tcase x509.UnknownAuthorityError:\n\t\treturn ResultCertificateNotTrusted, \"certificate-not-trusted-unknown-authority\"\n\tcase x509.HostnameError:\n\t\treturn ResultCertificateHostMismatch, \"certificate-host-name-mismatch\"\n\tcase x509.NameMismatchError:\n\t\treturn ResultCertificateHostMismatch, \"certificate-mismatch\"\n\tcase x509.ConstraintViolationError:\n\t\tswitch err.Constraint {\n\t\tcase x509.NameMismatch:\n\t\t\treturn ResultCertificateHostMismatch, \"certificate-mismatch\"\n\t\tcase x509.NotAuthorizedToSign:\n\t\t\treturn ResultCertificateNotTrusted, \"certificate-not-trusted-not-authorized-to-sign\"\n\t\tcase x509.NotAuthorizedForThisName:\n\t\t\treturn ResultCertificateNotTrusted, \"certificate-not-trusted-not-authorized-for-this-name\"\n\t\t}\n\tcase x509.CertificateInvalidError:\n\t\tif reason, ok := invalidReasons[err.Reason]; ok {\n\t\t\treturn ResultCertificateInvalid, reason\n\t\t}\n\t\treturn ResultValidationFailure, \"certificate-invalid\"\n\tcase x509.SyntaxError:\n\t\tif strings.HasPrefix(err.Msg, \"certificate not valid until after\") {\n\t\t\treturn ResultCertificateExpired, \"certificate-expired\"\n\t\t}\n\t\treturn ResultValidationFailure, \"syntax-error\"\n\tcase x509.InconsistentNameError:\n\t\treturn ResultCertificateNotTrusted, \"certificate-not-trusted-inconsistent-name\"\n\t}\n\tswitch err {\n\tcase x509.InsecureAlgorithmError:\n\t\treturn ResultValidationFailure, \"insecure-algorithm\"\n\tcase x509.WeakKeyError:\n\t\treturn ResultValidationFailure, \"weak-key\"\n\tcase tls.RecordHeaderError:\n\t\treturn ResultValidationFailure, \"tls-record-header-error\"\n\tcase tls.UnrecognizedNameError:\n\t\treturn ResultValidationFailure, \"tls-unrecognized-name-error\"\n\tcase tls.BadCertificateErrorType:\n\t\treturn ResultValidationFailure, \"tls-bad-certificate-error-type\"\n\tcase tls.EarlyCertificateError:\n\t\treturn ResultValidationFailure, \"tls-early-certificate-error\"\n\tcase tls.NoPeersCertificateError:\n\t\treturn ResultValidationFailure, \"tls-no-peers-certificate-error\"\n\tcase tls.NoClientCertificateError:\n\t\treturn ResultValidationFailure, \"tls-no-client-certificate-error\"\n\tcase tls.UnknownSignatureAndHashError:\n\t\treturn ResultValidationFailure, \"tls-unknown-signature-and-hash-error\"\n\t}\n\tswitch pq := err.(type) {\n\tcase net.DNSError:\n\t\tswitch err.(*url.Error).Err.Error() {\n\t\tcase \"dial tcp: lookup\":\n\t\t\treturn ResultValidationFailure, \"lookup-failed\"\n\t\t}\n\t}\n\treturn ResultValidationFailure, \"unknown-error\"\n}\n```", "  ```\nfunc TLSFailureDetails(err error) (ResultType, string) {\n\ttlsErr, ok := err.(net.Error)\n\tif !ok {\n\t\treturn \"\", \"\"\n\t}\n\n\tresultType, found := tlsResultMap[tlsErr.Error()]\n\tif !found {\n\t\treturn \"\", \"\"\n\t}\n\n\treason := \"\"\n\tif x509Err, ok := err.(x509.HostnameError); ok {\n\t\treason, found = invalidReasons[x509Err.Reason()]\n\t\tif !found {\n\t\t\treturn ResultType(resultType), \"certificate-invalid\"\n\t\t}\n\t}\n\n\treturn ResultType(resultType), reason\n}\n\nvar tlsResultMap = map[string]string{\n\t\"alert version\":        \"starttls-not-supported\",\n\t\"alert handshake\":      \"certificate-invalid\",\n\t\"alert unknown\":        \"certificate-invalid\",\n\t\"alert certificate\":    \"certificate-expired\",\n\t\"alert certificate_revoke\":    \"certificate-expired\",\n\t\"alert bad_record_mac\":   \"certificate-expired\",\n\t\"alert certificate_expired\":  \"certificate-expired\",\n\t\"alert certificate_unknown\":  \"certificate-expired\",\n\t\"alert illegal_parameter\":    \"certificate-expired\",\n\t\"alert decrypt_error\":        \"certificate-expired\",\n\t\"alert record_overflow\":      \"certificate-expired\",\n\t\"alert decompression_failure\": \"tlsa-invalid\",\n\t\"alert handshake_failure\":    \"tlsa-invalid\",\n\t\"alert no_renegotiation\":     \"tlsa-invalid\",\n\t\"alert certificate_required\": \"validation-failure\",\n\t\"alert unrecognized_name\":    \"validation-failure\",\n\t\"network i/o timeout\":        \"validation-failure\",\n\t\"write timeout\":              \"validation-failure\",\n\t\"read timeout\":               \"validation-failure\",\n\t\"dial failed\":                \"dnssec-invalid\",\n\t\"concurrent negotiation\":     \"dane-required\",\n\t\"certificate untrusted\":      \"dane-required\",\n\t\"certificate tls-sct\":        \"certificate-not-trusted\",\n\t\"authorization failed\":       \"sts-policy-invalid\",\n}\n```"]}, "mox-tlsrpt/report.go-ParseMessage": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/tlsrpt/report.go:\n```\npackage tlsrpt\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net\"\n\t\"os\"\n\t\"reflect\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/mjl-/adns\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/moxio\"\n)\n\nvar ErrNoReport = errors.New(\"no tlsrpt report found\")\n\n// ../rfc/8460:628\n\n// Report is a TLSRPT report.\ntype Report struct {\n\tOrganizationName string\n\tDateRange        TLSRPTDateRange\n\tContactInfo      string\n\tReportID         string\n\tPolicies         []Result\n}\n\n// ReportJSON is a TLS report with field names as used in the specification. These field names are inconvenient to use in JavaScript, so after parsing a ReportJSON is turned into a Report.\ntype ReportJSON struct {\n\tOrganizationName string              `json:\"organization-name\"`\n\tDateRange        TLSRPTDateRangeJSON `json:\"date-range\"`\n\tContactInfo      string              `json:\"contact-info\"` // Email address.\n\tReportID         string              `json:\"report-id\"`\n\tPolicies         []ResultJSON        `json:\"policies\"`\n}\n\nfunc convertSlice[T interface{ Convert() S }, S any](l []T) []S {\n\tif l == nil {\n\t\treturn nil\n\t}\n\tr := make([]S, len(l))\n\tfor i, e := range l {\n\t\tr[i] = e.Convert()\n\t}\n\treturn r\n}\n\nfunc (v Report) Convert() ReportJSON {\n\treturn ReportJSON{v.OrganizationName, v.DateRange.Convert(), v.ContactInfo, v.ReportID, convertSlice[Result, ResultJSON](v.Policies)}\n}\n\nfunc (v ReportJSON) Convert() Report {\n\treturn Report{v.OrganizationName, v.DateRange.Convert(), v.ContactInfo, v.ReportID, convertSlice[ResultJSON, Result](v.Policies)}\n}\n\n// Merge combines the counts and failure details of results into the report.\n// Policies are merged if identical and added otherwise. Same for failure details\n// within a result.\nfunc (r *Report) Merge(results ...Result) {\nMerge:\n\tfor _, nr := range results {\n\t\tfor i, p := range r.Policies {\n\t\t\tif !p.Policy.equal(nr.Policy) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tr.Policies[i].Add(nr.Summary.TotalSuccessfulSessionCount, nr.Summary.TotalFailureSessionCount, nr.FailureDetails...)\n\t\t\tcontinue Merge\n\t\t}\n\n\t\tr.Policies = append(r.Policies, nr)\n\t}\n}\n\n// Add increases the success/failure counts of a result, and adds any failure\n// details.\nfunc (r *Result) Add(success, failure int64, fds ...FailureDetails) {\n\tr.Summary.TotalSuccessfulSessionCount += success\n\tr.Summary.TotalFailureSessionCount += failure\n\n\t// In smtpclient we can compensate with a negative success, after failed read after\n\t// successful handshake. Sanity check that we never get negative counts.\n\tif r.Summary.TotalSuccessfulSessionCount < 0 {\n\t\tr.Summary.TotalSuccessfulSessionCount = 0\n\t}\n\tif r.Summary.TotalFailureSessionCount < 0 {\n\t\tr.Summary.TotalFailureSessionCount = 0\n\t}\n\nMerge:\n\tfor _, nfd := range fds {\n\t\tfor i, fd := range r.FailureDetails {\n\t\t\tif !fd.equalKey(nfd) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tfd.FailedSessionCount += nfd.FailedSessionCount\n\t\t\tr.FailureDetails[i] = fd\n\t\t\tcontinue Merge\n\t\t}\n\t\tr.FailureDetails = append(r.FailureDetails, nfd)\n\t}\n}\n\n// Add is a convenience function for merging making a Result and merging it into\n// the report.\nfunc (r *Report) Add(policy ResultPolicy, success, failure int64, fds ...FailureDetails) {\n\tr.Merge(Result{policy, Summary{success, failure}, fds})\n}\n\n// TLSAPolicy returns a policy for DANE.\nfunc TLSAPolicy(records []adns.TLSA, tlsaBaseDomain dns.Domain) ResultPolicy {\n\t// The policy domain is the TLSA base domain. ../rfc/8460:251\n\n\tl := make([]string, len(records))\n\tfor i, r := range records {\n\t\tl[i] = r.Record()\n\t}\n\tsort.Strings(l) // For consistent equals.\n\treturn ResultPolicy{\n\t\tType:   TLSA,\n\t\tString: l,\n\t\tDomain: tlsaBaseDomain.ASCII,\n\t\tMXHost: []string{},\n\t}\n}\n\nfunc MakeResult(policyType PolicyType, domain dns.Domain, fds ...FailureDetails) Result {\n\tif fds == nil {\n\t\tfds = []FailureDetails{}\n\t}\n\treturn Result{\n\t\tPolicy:         ResultPolicy{Type: policyType, Domain: domain.ASCII, String: []string{}, MXHost: []string{}},\n\t\tFailureDetails: fds,\n\t}\n}\n\n// note: with TLSRPT prefix to prevent clash in sherpadoc types.\ntype TLSRPTDateRange struct {\n\tStart time.Time\n\tEnd   time.Time\n}\n\nfunc (v TLSRPTDateRange) Convert() TLSRPTDateRangeJSON {\n\treturn TLSRPTDateRangeJSON(v)\n}\n\ntype TLSRPTDateRangeJSON struct {\n\tStart time.Time `json:\"start-datetime\"`\n\tEnd   time.Time `json:\"end-datetime\"`\n}\n\nfunc (v TLSRPTDateRangeJSON) Convert() TLSRPTDateRange {\n\treturn TLSRPTDateRange(v)\n}\n\n// UnmarshalJSON is defined on the date range, not the individual time.Time fields\n// because it is easier to keep the unmodified time.Time fields stored in the\n// database.\nfunc (dr *TLSRPTDateRangeJSON) UnmarshalJSON(buf []byte) error {\n\tvar v struct {\n\t\tStart xtime `json:\"start-datetime\"`\n\t\tEnd   xtime `json:\"end-datetime\"`\n\t}\n\tif err := json.Unmarshal(buf, &v); err != nil {\n\t\treturn err\n\t}\n\tdr.Start = time.Time(v.Start)\n\tdr.End = time.Time(v.End)\n\treturn nil\n}\n\n// xtime and its UnmarshalJSON exists to work around a specific invalid date-time encoding seen in the wild.\ntype xtime time.Time\n\nfunc (x *xtime) UnmarshalJSON(buf []byte) error {\n\tvar t time.Time\n\terr := t.UnmarshalJSON(buf)\n\tif err == nil {\n\t\t*x = xtime(t)\n\t\treturn nil\n\t}\n\n\t// Microsoft is sending reports with invalid start-datetime/end-datetime (missing\n\t// timezone, ../rfc/8460:682 ../rfc/3339:415). We compensate.\n\tvar s string\n\tif err := json.Unmarshal(buf, &s); err != nil {\n\t\treturn err\n\t}\n\tt, err = time.Parse(\"2006-01-02T15:04:05\", s)\n\tif err != nil {\n\t\treturn err\n\t}\n\t*x = xtime(t)\n\treturn nil\n}\n\ntype Result struct {\n\tPolicy         ResultPolicy\n\tSummary        Summary\n\tFailureDetails []FailureDetails\n}\n\nfunc (r Result) Convert() ResultJSON {\n\treturn ResultJSON{ResultPolicyJSON(r.Policy), SummaryJSON(r.Summary), convertSlice[FailureDetails, FailureDetailsJSON](r.FailureDetails)}\n}\n\ntype ResultJSON struct {\n\tPolicy         ResultPolicyJSON     `json:\"policy\"`\n\tSummary        SummaryJSON          `json:\"summary\"`\n\tFailureDetails []FailureDetailsJSON `json:\"failure-details\"`\n}\n\nfunc (r ResultJSON) Convert() Result {\n\treturn Result{ResultPolicy(r.Policy), Summary(r.Summary), convertSlice[FailureDetailsJSON, FailureDetails](r.FailureDetails)}\n}\n\n// todo spec: ../rfc/8460:437 says policy is a string, with rules for turning dane records into a single string. perhaps a remnant of an earlier version (for mtasts a single string would have made more sense). i doubt the intention is to always have a single element in policy-string (though the field name is singular).\n\ntype ResultPolicy struct {\n\tType   PolicyType\n\tString []string\n\tDomain string // ASCII/A-labels, ../rfc/8460:704\n\tMXHost []string\n}\n\ntype ResultPolicyJSON struct {\n\tType   PolicyType `json:\"policy-type\"`\n\tString []string   `json:\"policy-string\"`\n\tDomain string     `json:\"policy-domain\"`\n\tMXHost []string   `json:\"mx-host\"` // Example in RFC has errata, it originally was a single string. ../rfc/8460-eid6241 ../rfc/8460:1779\n}\n\n// PolicyType indicates the policy success/failure results are for.\ntype PolicyType string\n\nconst (\n\t// For DANE, against a mail host (not recipient domain).\n\tTLSA PolicyType = \"tlsa\"\n\n\t// For MTA-STS, against a recipient domain (not a mail host).\n\tSTS PolicyType = \"sts\"\n\n\t// Recipient domain did not have MTA-STS policy, or mail host (TSLA base domain)\n\t// did not have DANE TLSA records.\n\tNoPolicyFound PolicyType = \"no-policy-found\"\n\t// todo spec: ../rfc/8460:440 ../rfc/8460:697 suggest to replace with values like \"no-sts-found\" and \"no-tlsa-found\" to make it explicit which policy isn't found. also easier to implement, because you don't have to handle leaving out an sts no-policy-found result for a mail host when a tlsa policy is present.\n)\n\nfunc (rp ResultPolicy) equal(orp ResultPolicy) bool {\n\treturn rp.Type == orp.Type && slices.Equal(rp.String, orp.String) && rp.Domain == orp.Domain && slices.Equal(rp.MXHost, orp.MXHost)\n}\n\ntype Summary struct {\n\tTotalSuccessfulSessionCount int64\n\tTotalFailureSessionCount    int64\n}\n\ntype SummaryJSON struct {\n\tTotalSuccessfulSessionCount int64 `json:\"total-successful-session-count\"`\n\tTotalFailureSessionCount    int64 `json:\"total-failure-session-count\"`\n}\n\n// ResultType represents a TLS error.\ntype ResultType string\n\n// ../rfc/8460:1377\n// https://www.iana.org/assignments/starttls-validation-result-types/starttls-validation-result-types.xhtml\n\nconst (\n\tResultSTARTTLSNotSupported    ResultType = \"starttls-not-supported\"\n\tResultCertificateHostMismatch ResultType = \"certificate-host-mismatch\"\n\tResultCertificateExpired      ResultType = \"certificate-expired\"\n\tResultTLSAInvalid             ResultType = \"tlsa-invalid\"\n\tResultDNSSECInvalid           ResultType = \"dnssec-invalid\"\n\tResultDANERequired            ResultType = \"dane-required\"\n\tResultCertificateNotTrusted   ResultType = \"certificate-not-trusted\"\n\tResultSTSPolicyInvalid        ResultType = \"sts-policy-invalid\"\n\tResultSTSWebPKIInvalid        ResultType = \"sts-webpki-invalid\"\n\tResultValidationFailure       ResultType = \"validation-failure\" // Other error.\n\tResultSTSPolicyFetch          ResultType = \"sts-policy-fetch-error\"\n)\n\n// todo spec: ../rfc/8460:719 more of these fields should be optional. some sts failure details, like failed policy fetches, won't have an ip or mx, the failure happens earlier in the delivery process.\n\ntype FailureDetails struct {\n\tResultType            ResultType\n\tSendingMTAIP          string\n\tReceivingMXHostname   string\n\tReceivingMXHelo       string\n\tReceivingIP           string\n\tFailedSessionCount    int64\n\tAdditionalInformation string\n\tFailureReasonCode     string\n}\n\nfunc (v FailureDetails) Convert() FailureDetailsJSON { return FailureDetailsJSON(v) }\n\ntype FailureDetailsJSON struct {\n\tResultType            ResultType `json:\"result-type\"`\n\tSendingMTAIP          string     `json:\"sending-mta-ip\"`\n\tReceivingMXHostname   string     `json:\"receiving-mx-hostname\"`\n\tReceivingMXHelo       string     `json:\"receiving-mx-helo,omitempty\"`\n\tReceivingIP           string     `json:\"receiving-ip\"`\n\tFailedSessionCount    int64      `json:\"failed-session-count\"`\n\tAdditionalInformation string     `json:\"additional-information\"`\n\tFailureReasonCode     string     `json:\"failure-reason-code\"`\n}\n\nfunc (v FailureDetailsJSON) Convert() FailureDetails { return FailureDetails(v) }\n\n// equalKey returns whether FailureDetails have the same values, expect for\n// FailedSessionCount. Useful for aggregating FailureDetails.\nfunc (fd FailureDetails) equalKey(ofd FailureDetails) bool {\n\tfd.FailedSessionCount = 0\n\tofd.FailedSessionCount = 0\n\treturn fd == ofd\n}\n\n// Details is a convenience function to compose a FailureDetails.\nfunc Details(t ResultType, r string) FailureDetails {\n\treturn FailureDetails{ResultType: t, FailedSessionCount: 1, FailureReasonCode: r}\n}\n\nvar invalidReasons = map[x509.InvalidReason]string{\n\tx509.NotAuthorizedToSign:           \"not-authorized-to-sign\",\n\tx509.Expired:                       \"certificate-expired\",\n\tx509.CANotAuthorizedForThisName:    \"ca-not-authorized-for-this-name\",\n\tx509.TooManyIntermediates:          \"too-many-intermediates\",\n\tx509.IncompatibleUsage:             \"incompatible-key-usage\",\n\tx509.NameMismatch:                  \"parent-subject-child-issuer-mismatch\",\n\tx509.NameConstraintsWithoutSANs:    \"name-constraint-without-sans\",\n\tx509.UnconstrainedName:             \"unconstrained-name\",\n\tx509.TooManyConstraints:            \"too-many-constraints\",\n\tx509.CANotAuthorizedForExtKeyUsage: \"ca-not-authorized-for-ext-key-usage\",\n}\n\n// TLSFailureDetails turns errors encountered during TLS handshakes into a result\n// type and failure reason code for use with FailureDetails.\n//\n// Errors from crypto/tls, including local and remote alerts, from crypto/x509,\n// and generic i/o and timeout errors are recognized.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Parse parses a Report.\n// The maximum size is 20MB.\nfunc Parse(r io.Reader) (*ReportJSON, error) {\n\tr = &moxio.LimitReader{R: r, Limit: 20 * 1024 * 1024}\n\tvar report ReportJSON\n\tif err := json.NewDecoder(r).Decode(&report); err != nil {\n\t\treturn nil, err\n\t}\n\t// note: there may be leftover data, we ignore it.\n\treturn &report, nil\n}\n\n// ParseMessage parses a Report from a mail message.\n// The maximum size of the message is 15MB, the maximum size of the\n// decompressed report is 20MB.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc parseMessageReport(log mlog.Log, p message.Part, allow bool) (*ReportJSON, error) {\n\tif p.MediaType != \"MULTIPART\" {\n\t\tif !allow {\n\t\t\treturn nil, ErrNoReport\n\t\t}\n\t\treturn parseReport(p)\n\t}\n\n\tfor {\n\t\tsp, err := p.ParseNextPart(log.Logger)\n\t\tif err == io.EOF {\n\t\t\treturn nil, ErrNoReport\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif p.MediaSubType == \"REPORT\" && p.ContentTypeParams[\"report-type\"] != \"tlsrpt\" {\n\t\t\treturn nil, fmt.Errorf(\"unknown report-type parameter %q\", p.ContentTypeParams[\"report-type\"])\n\t\t}\n\t\treport, err := parseMessageReport(log, *sp, p.MediaSubType == \"REPORT\")\n\t\tif err == ErrNoReport {\n\t\t\tcontinue\n\t\t} else if err != nil || report != nil {\n\t\t\treturn report, err\n\t\t}\n\t}\n}\n\nfunc parseReport(p message.Part) (*ReportJSON, error) {\n\tmt := strings.ToLower(p.MediaType + \"/\" + p.MediaSubType)\n\tswitch mt {\n\tcase \"application/tlsrpt+json\":\n\t\treturn Parse(p.Reader())\n\tcase \"application/tlsrpt+gzip\":\n\t\tgzr, err := gzip.NewReader(p.Reader())\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"decoding gzip TLSRPT report: %s\", err)\n\t\t}\n\t\treturn Parse(gzr)\n\t}\n\treturn nil, ErrNoReport\n}\n\n```\n# Function Signature: func ParseMessage(elog *slog.Logger, r io.ReaderAt) (*ReportJSON, error) {\n# Natural Language Annotations: // ParseMessage parses a Report from a mail message.\n// The maximum size of the message is 15MB, the maximum size of the\n// decompressed report is 20MB.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc ParseMessage(elog *slog.Logger, r io.ReaderAt) (*ReportJSON, error) {\n\tmr, err := message.ParseMessage(r, elog)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"parsing message: %s\", err)\n\t}\n\n\treport, err := parseMessageReport(elog, mr, false)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"parsing report: %s\", err)\n\t}\n\n\treturn report, nil\n}\n```", "  ```\nfunc ParseMessage(elog *slog.Logger, r io.ReaderAt) (*ReportJSON, error) {\n\tlog := mlog.Wrap(elog, \"parseMessage\")\n\n\tp, err := message.Read(log, r, 15*1024*1024)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading message: %s\", err)\n\t}\n\treturn parseMessageReport(log, *p, false)\n}\n```", "  ```\nfunc ParseMessage(elog *slog.Logger, r io.ReaderAt) (*ReportJSON, error) {\n\tctx := context.Background()\n\tlog := elog.WithContext(ctx)\n\tm, err := message.ReadAt(log, r, 15*1024*1024)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn parseMessageReport(log, m.Message, false)\n}\n```"]}, "mox-spf/received.go-Header": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/spf/received.go:\n```\npackage spf\n\nimport (\n\t\"net\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n)\n\n// ../rfc/7208:2083\n\n// Received represents a Received-SPF header with the SPF verify results, to be\n// prepended to a message.\n//\n// Example:\n//\n//\tReceived-SPF: pass (mybox.example.org: domain of\n//\t myname@example.com designates 192.0.2.1 as permitted sender)\n//\t receiver=mybox.example.org; client-ip=192.0.2.1;\n//\t envelope-from=\"myname@example.com\"; helo=foo.example.com;\ntype Received struct {\n\tResult       Status\n\tComment      string       // Additional free-form information about the verification result. Optional. Included in message header comment inside \"()\".\n\tClientIP     net.IP       // IP address of remote SMTP client, \"client-ip=\".\n\tEnvelopeFrom string       // Sender mailbox, typically SMTP MAIL FROM, but will be set to \"postmaster\" at SMTP EHLO if MAIL FROM is empty, \"envelop-from=\".\n\tHelo         dns.IPDomain // IP or host name from EHLO or HELO command, \"helo=\".\n\tProblem      string       // Optional. \"problem=\"\n\tReceiver     string       // Hostname of receiving mail server, \"receiver=\".\n\tIdentity     Identity     // The identity that was checked, \"mailfrom\" or \"helo\", for \"identity=\".\n\tMechanism    string       // Mechanism that caused the result, can be \"default\". Optional.\n}\n\n// Identity that was verified.\ntype Identity string\n\nconst (\n\tReceivedMailFrom Identity = \"mailfrom\"\n\tReceivedHELO     Identity = \"helo\"\n)\n\nfunc receivedValueEncode(s string) string {\n\tif s == \"\" {\n\t\treturn quotedString(\"\")\n\t}\n\tfor i, c := range s {\n\t\tif c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z' || c >= '0' && c <= '9' || c > 0x7f {\n\t\t\tcontinue\n\t\t}\n\t\t// ../rfc/5322:679\n\t\tconst atext = \"!#$%&'*+-/=?^_`{|}~\"\n\t\tif strings.IndexByte(atext, byte(c)) >= 0 {\n\t\t\tcontinue\n\t\t}\n\t\tif c != '.' || (i == 0 || i+1 == len(s)) {\n\t\t\treturn quotedString(s)\n\t\t}\n\t}\n\treturn s\n}\n\n// ../rfc/5322:736\nfunc quotedString(s string) string {\n\tw := &strings.Builder{}\n\tw.WriteByte('\"')\n\tfor _, c := range s {\n\t\tif c > ' ' && c < 0x7f && c != '\"' && c != '\\\\' || c > 0x7f || c == ' ' || c == '\\t' {\n\t\t\t// We allow utf-8. This should only be needed when the destination address has an\n\t\t\t// utf8 localpart, in which case we are already doing smtputf8.\n\t\t\t// We also allow unescaped space and tab. This is FWS, and the name of ABNF\n\t\t\t// production \"qcontent\" implies the FWS is not part of the string, but escaping\n\t\t\t// space and tab leads to ugly strings. ../rfc/5322:743\n\t\t\tw.WriteRune(c)\n\t\t\tcontinue\n\t\t}\n\t\tswitch c {\n\t\tcase ' ', '\\t', '\"', '\\\\':\n\t\t\tw.WriteByte('\\\\')\n\t\t\tw.WriteRune(c)\n\t\t}\n\t}\n\tw.WriteByte('\"')\n\treturn w.String()\n}\n\n// Header returns a Received-SPF header including trailing crlf that can be\n// prepended to an incoming message.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (r Received) Header() string {\n# Natural Language Annotations: // Header returns a Received-SPF header including trailing crlf that can be\n// prepended to an incoming message.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (r Received) Header() string {\n\th := r.Result.Header()\n\tif r.Comment != \"\" {\n\t\th += \" (%s)\"\n\t\th = fmt.Sprintf(h, r.Comment)\n\t}\n\tif r.ClientIP != nil {\n\t\th += \" client-ip=%s;\"\n\t\th = fmt.Sprintf(h, r.ClientIP)\n\t}\n\tif r.EnvelopeFrom != \"\" {\n\t\th += \" envelope-from=\\\"%s\\\";\"\n\t\th = fmt.Sprintf(h, r.EnvelopeFrom)\n\t}\n\tif r.Helo != \"\" {\n\t\th += \" helo=%s;\"\n\t\th = fmt.Sprintf(h, r.Helo)\n\t}\n\tif r.Problem != \"\" {\n\t\th += \" problem=%s;\"\n\t\th = fmt.Sprintf(h, r.Problem)\n\t}\n\tif r.Receiver != \"\" {\n\t\th += \" receiver=%s;\"\n\t\th = fmt.Sprintf(h, r.Receiver)\n\t}\n\tif r.Identity != \"\" {\n\t\th += \" identity=%s;\"\n\t\th = fmt.Sprintf(h, r.Identity)\n\t}\n\tif r.Mechanism != \"\" {\n\t\th += \" mechanism=%s;\"\n\t\th = fmt.Sprintf(h, r.Mechanism)\n\t}\n\th += \"\\r\\n\"\n\treturn h\n}\n```", "  ```\nfunc (r Received) Header() string {\n\ts := r.Result.String()\n\tif r.Receiver != \"\" {\n\t\ts += \" receiver=\" + receivedValueEncode(r.Receiver)\n\t}\n\tif r.ClientIP != nil {\n\t\ts += \" client-ip=\" + r.ClientIP.String()\n\t}\n\tif r.EnvelopeFrom != \"\" {\n\t\ts += \" envelope-from=\\\"\" + receivedValueEncode(r.EnvelopeFrom) + \"\\\"\"\n\t}\n\tif r.Helo != \"\" {\n\t\ts += \" helo=\\\"\" + receivedValueEncode(r.Helo) + \"\\\"\"\n\t}\n\tif r.Identity != \"\" {\n\t\ts += \" identity=\" + r.Identity\n\t}\n\tif r.Mechanism != \"\" {\n\t\ts += \" mechanism=\" + r.Mechanism\n\t}\n\tif r.Problem != \"\" {\n\t\ts += \" problem=\" + r.Problem\n\t}\n\tif r.Comment != \"\" {\n\t\ts += \" \" + strings.TrimSpace(r.Comment)\n\t}\n\treturn \"Received-SPF: \" + s\n}\n```\nThis is a function that returns a Received-SPF header based on a Received struct. It uses the fields of the Received struct to generate the header, and includes specialized encoding functions for certain fields (e.g. `receivedValueEncode` for `Receiver` and `EnvelopeFrom`) to ensure the generated header is valid. The function includes trailing crlf, as indicated by the natural language comment.", "  ```\nfunc (r Received) Header() string {\n\t// Prepare the comment, if any\n\tvar comment string\n\tif r.Comment != \"\" {\n\t\tcomment = \" (\" + r.Comment + \")\"\n\t}\n\n\t// Prepare the envelope-from, helo, and receiver\n\tef := r.EnvelopeFrom\n\tif ef == \"\" {\n\t\tef = \"postmaster\"\n\t}\n\th := r.Helo\n\tif h == \"\" {\n\t\th = \"idetermined\"\n\t}\n\tr := r.Receiver\n\tif r == \"\" {\n\t\tr = \"idetermined\"\n\t}\n\n\t// Prepare the result\n\tres := r.Result.String()\n\tvar qualifier string\n\tswitch r.Identity {\n\tcase ReceivedMailFrom:\n\t\tqualifier = \"mail\"\n\tcase ReceivedHELO:\n\t\tqualifier = \"helo\"\n\tdefault:\n\t\tqualifier = \"idetermined\"\n\t}\n\n\t// Prepare the client IP\n\tcip := r.ClientIP\n\tif cip == nil || cip.IsUnspecified() {\n\t\tcip = net.IPv4zero\n\t}\n\n\t// Prepare the problem, if any\n\tvar problem string\n\tif r.Problem != \"\" {\n\t\tproblem = \" problem=\" + r.Problem\n\t}\n\n\t// Prepare the mechanism, if any\n\tvar mechanism string\n\tif r.Mechanism != \"\" {\n\t\tmechanism = \" mechanism=\" + r.Mechanism\n\t}\n\n\t// Generate the string\n\ts := fmt.Sprintf(`Received-SPF: %s%s; client-ip=%s; envelope-from=%s; helo=%s; receiver=%s; identity=%s%s%s`,\n\t\tres, comment, cip.String(), ef, h, r, qualifier, problem, mechanism)\n\n\t// Return the result\n\treturn s + \"\\r\\n\"\n}\n```\nThis function generates a Received-SPF header that can be prepended to an incoming message. It uses a Received object, and returns a string that can be sent out by the receiving mail server.\n\nThe function first prepares the fields that can be empty, such as the comment and the envelope-from field. It then generates the result field of the header, which is based on the status of the SPF verification. The result is qualified with the identity field, which indicates whether the verification was done on the sender email address (mail) or the client hostname (helo).\n\nThe function then prepares the client IP and the problem field, if any. Finally, it generates the Received-SPF header using the fields it has prepared, and returns the result."]}, "mox-spf/spf.go-Evaluate": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/spf/spf.go:\n```\n// Package spf implements Sender Policy Framework (SPF, RFC 7208) for verifying\n// remote mail server IPs with their published records.\n//\n// With SPF a domain can publish a policy as a DNS TXT record describing which IPs\n// are allowed to send email with SMTP with the domain in the MAIL FROM command,\n// and how to treat SMTP transactions coming from other IPs.\npackage spf\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"net/url\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/smtp\"\n\t\"github.com/mjl-/mox/stub\"\n)\n\n// The net package always returns DNS names in absolute, lower-case form. We make\n// sure we make names absolute when looking up. For verifying, we do not want to\n// verify names relative to our local search domain.\n\nvar (\n\tMetricVerify stub.HistogramVec = stub.HistogramVecIgnore{}\n)\n\n// cross-link rfc and errata\n// ../rfc/7208-eid5436 ../rfc/7208:2043\n// ../rfc/7208-eid6721 ../rfc/7208:1928\n// ../rfc/7208-eid5227 ../rfc/7208:1297\n// ../rfc/7208-eid6595 ../rfc/7208:984\n\nvar (\n\t// Lookup errors.\n\tErrName            = errors.New(\"spf: bad domain name\")\n\tErrNoRecord        = errors.New(\"spf: no txt record\")\n\tErrMultipleRecords = errors.New(\"spf: multiple spf txt records in dns\")\n\tErrDNS             = errors.New(\"spf: lookup of dns record\")\n\tErrRecordSyntax    = errors.New(\"spf: malformed spf txt record\")\n\n\t// Evaluation errors.\n\tErrTooManyDNSRequests = errors.New(\"spf: too many dns requests\")\n\tErrTooManyVoidLookups = errors.New(\"spf: too many void lookups\")\n\tErrMacroSyntax        = errors.New(\"spf: bad macro syntax\")\n)\n\nconst (\n\t// Maximum number of DNS requests to execute. This excludes some requests, such as\n\t// lookups of MX host results.\n\tdnsRequestsMax = 10\n\n\t// Maximum number of DNS lookups that result in no records before a StatusPermerror\n\t// is returned. This limit aims to prevent abuse.\n\tvoidLookupsMax = 2\n)\n\n// Status is the result of an SPF verification.\ntype Status string\n\n// ../rfc/7208:517\n// ../rfc/7208:1836\n\nconst (\n\tStatusNone      Status = \"none\"      // E.g. no DNS domain name in session, or no SPF record in DNS.\n\tStatusNeutral   Status = \"neutral\"   // Explicit statement that nothing is said about the IP, \"?\" qualifier. None and Neutral must be treated the same.\n\tStatusPass      Status = \"pass\"      // IP is authorized.\n\tStatusFail      Status = \"fail\"      // IP is exlicitly not authorized. \"-\" qualifier.\n\tStatusSoftfail  Status = \"softfail\"  // Weak statement that IP is probably not authorized, \"~\" qualifier.\n\tStatusTemperror Status = \"temperror\" // Trying again later may succeed, e.g. for temporary DNS lookup error.\n\tStatusPermerror Status = \"permerror\" // Error requiring some intervention to correct. E.g. invalid DNS record.\n)\n\n// Args are the parameters to the SPF verification algorithm (\"check_host\" in the RFC).\n//\n// All fields should be set as they can be required for macro expansions.\ntype Args struct {\n\t// RemoteIP will be checked as sender for email.\n\tRemoteIP net.IP\n\n\t// Address from SMTP MAIL FROM command. Zero values for a null reverse path (used for DSNs).\n\tMailFromLocalpart smtp.Localpart\n\tMailFromDomain    dns.Domain\n\n\t// HelloDomain is from the SMTP EHLO/HELO command.\n\tHelloDomain dns.IPDomain\n\n\tLocalIP       net.IP\n\tLocalHostname dns.Domain\n\n\t// Explanation string to use for failure. In case of \"include\", where explanation\n\t// from original domain must be used.\n\t// May be set for recursive calls.\n\texplanation *string\n\n\t// Domain to validate.\n\tdomain dns.Domain\n\n\t// Effective sender. Equal to MailFrom if non-zero, otherwise set to \"postmaster\" at HelloDomain.\n\tsenderLocalpart smtp.Localpart\n\tsenderDomain    dns.Domain\n\n\t// To enforce the limit on lookups. Initialized automatically if nil.\n\tdnsRequests *int\n\tvoidLookups *int\n}\n\n// Mocked for testing expanding \"t\" macro.\nvar timeNow = time.Now\n\n// Lookup looks up and parses an SPF TXT record for domain.\n//\n// Authentic indicates if the DNS results were DNSSEC-verified.\nfunc Lookup(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, domain dns.Domain) (rstatus Status, rtxt string, rrecord *Record, authentic bool, rerr error) {\n\tlog := mlog.New(\"spf\", elog)\n\tstart := time.Now()\n\tdefer func() {\n\t\tlog.Debugx(\"spf lookup result\", rerr,\n\t\t\tslog.Any(\"domain\", domain),\n\t\t\tslog.Any(\"status\", rstatus),\n\t\t\tslog.Any(\"record\", rrecord),\n\t\t\tslog.Duration(\"duration\", time.Since(start)))\n\t}()\n\n\t// ../rfc/7208:586\n\thost := domain.ASCII + \".\"\n\tif err := validateDNS(host); err != nil {\n\t\treturn StatusNone, \"\", nil, false, fmt.Errorf(\"%w: %s: %s\", ErrName, domain, err)\n\t}\n\n\t// Lookup spf record.\n\ttxts, result, err := dns.WithPackage(resolver, \"spf\").LookupTXT(ctx, host)\n\tif dns.IsNotFound(err) {\n\t\treturn StatusNone, \"\", nil, result.Authentic, fmt.Errorf(\"%w for %s\", ErrNoRecord, host)\n\t} else if err != nil {\n\t\treturn StatusTemperror, \"\", nil, result.Authentic, fmt.Errorf(\"%w: %s: %s\", ErrDNS, host, err)\n\t}\n\n\t// Parse the records. We only handle those that look like spf records.\n\tvar record *Record\n\tvar text string\n\tfor _, txt := range txts {\n\t\tvar isspf bool\n\t\tr, isspf, err := ParseRecord(txt)\n\t\tif !isspf {\n\t\t\t// ../rfc/7208:595\n\t\t\tcontinue\n\t\t} else if err != nil {\n\t\t\t// ../rfc/7208:852\n\t\t\treturn StatusPermerror, txt, nil, result.Authentic, fmt.Errorf(\"%w: %s\", ErrRecordSyntax, err)\n\t\t}\n\t\tif record != nil {\n\t\t\t// ../rfc/7208:576\n\t\t\treturn StatusPermerror, \"\", nil, result.Authentic, ErrMultipleRecords\n\t\t}\n\t\ttext = txt\n\t\trecord = r\n\t}\n\tif record == nil {\n\t\t// ../rfc/7208:837\n\t\treturn StatusNone, \"\", nil, result.Authentic, ErrNoRecord\n\t}\n\treturn StatusNone, text, record, result.Authentic, nil\n}\n\n// Verify checks if a remote IP is allowed to send email for a domain.\n//\n// If the SMTP \"MAIL FROM\" is set, it is used as identity (domain) to verify.\n// Otherwise, the EHLO domain is verified if it is a valid domain.\n//\n// The returned Received.Result status will always be set, regardless of whether an\n// error is returned.\n// For status Temperror and Permerror, an error is always returned.\n// For Fail, explanation may be set, and should be returned in the SMTP session if\n// it is the reason the message is rejected. The caller should ensure the\n// explanation is valid for use in SMTP, taking line length and ascii-only\n// requirement into account.\n//\n// Verify takes the maximum number of 10 DNS requests into account, and the maximum\n// of 2 lookups resulting in no records (\"void lookups\").\n//\n// Authentic indicates if the DNS results were DNSSEC-verified.\nfunc Verify(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, args Args) (received Received, domain dns.Domain, explanation string, authentic bool, rerr error) {\n\tlog := mlog.New(\"spf\", elog)\n\tstart := time.Now()\n\tdefer func() {\n\t\tMetricVerify.ObserveLabels(float64(time.Since(start))/float64(time.Second), string(received.Result))\n\t\tlog.Debugx(\"spf verify result\", rerr,\n\t\t\tslog.Any(\"domain\", args.domain),\n\t\t\tslog.Any(\"ip\", args.RemoteIP),\n\t\t\tslog.Any(\"status\", received.Result),\n\t\t\tslog.String(\"explanation\", explanation),\n\t\t\tslog.Duration(\"duration\", time.Since(start)))\n\t}()\n\n\tisHello, ok := prepare(&args)\n\tif !ok {\n\t\treceived = Received{\n\t\t\tResult:       StatusNone,\n\t\t\tComment:      \"no domain, ehlo is an ip literal and mailfrom is empty\",\n\t\t\tClientIP:     args.RemoteIP,\n\t\t\tEnvelopeFrom: fmt.Sprintf(\"%s@%s\", args.senderLocalpart, args.HelloDomain.IP.String()),\n\t\t\tHelo:         args.HelloDomain,\n\t\t\tReceiver:     args.LocalHostname.ASCII,\n\t\t}\n\t\treturn received, dns.Domain{}, \"\", false, nil\n\t}\n\n\tstatus, mechanism, expl, authentic, err := checkHost(ctx, log, resolver, args)\n\tcomment := fmt.Sprintf(\"domain %s\", args.domain.ASCII)\n\tif isHello {\n\t\tcomment += \", from ehlo because mailfrom is empty\"\n\t}\n\treceived = Received{\n\t\tResult:       status,\n\t\tComment:      comment,\n\t\tClientIP:     args.RemoteIP,\n\t\tEnvelopeFrom: fmt.Sprintf(\"%s@%s\", args.senderLocalpart, args.senderDomain.ASCII), // ../rfc/7208:2090, explicitly \"sender\", not \"mailfrom\".\n\t\tHelo:         args.HelloDomain,\n\t\tReceiver:     args.LocalHostname.ASCII,\n\t\tMechanism:    mechanism,\n\t}\n\tif err != nil {\n\t\treceived.Problem = err.Error()\n\t}\n\tif isHello {\n\t\treceived.Identity = \"helo\"\n\t} else {\n\t\treceived.Identity = \"mailfrom\"\n\t}\n\treturn received, args.domain, expl, authentic, err\n}\n\n// prepare args, setting fields sender* and domain as required for checkHost.\nfunc prepare(args *Args) (isHello bool, ok bool) {\n\t// If MAIL FROM is set, that identity is used. Otherwise the EHLO identity is used.\n\t// MAIL FROM is preferred, because if we accept the message, and we have to send a\n\t// DSN, it helps to know it is a verified sender. If we would check an EHLO\n\t// identity, and it is different from the MAIL FROM, we may be sending the DSN to\n\t// an address with a domain that would not allow sending from the originating IP.\n\t// The RFC seems a bit confused, ../rfc/7208:778 implies MAIL FROM is preferred,\n\t// but ../rfc/7208:424 mentions that a MAIL FROM check can be avoided by first\n\t// doing HELO.\n\n\targs.explanation = nil\n\targs.dnsRequests = nil\n\targs.voidLookups = nil\n\tif args.MailFromDomain.IsZero() {\n\t\t// If there is on EHLO, and it is an IP, there is nothing to SPF-validate.\n\t\tif !args.HelloDomain.IsDomain() {\n\t\t\treturn false, false\n\t\t}\n\t\t// If we have a mailfrom, we also have a localpart. But for EHLO we won't. ../rfc/7208:810\n\t\targs.senderLocalpart = \"postmaster\"\n\t\targs.senderDomain = args.HelloDomain.Domain\n\t\tisHello = true\n\t} else {\n\t\targs.senderLocalpart = args.MailFromLocalpart\n\t\targs.senderDomain = args.MailFromDomain\n\t}\n\targs.domain = args.senderDomain\n\treturn isHello, true\n}\n\n// lookup spf record, then evaluate args against it.\nfunc checkHost(ctx context.Context, log mlog.Log, resolver dns.Resolver, args Args) (rstatus Status, mechanism, rexplanation string, rauthentic bool, rerr error) {\n\tstatus, _, record, rauthentic, err := Lookup(ctx, log.Logger, resolver, args.domain)\n\tif err != nil {\n\t\treturn status, \"\", \"\", rauthentic, err\n\t}\n\n\tvar evalAuthentic bool\n\trstatus, mechanism, rexplanation, evalAuthentic, rerr = evaluate(ctx, log, record, resolver, args)\n\trauthentic = rauthentic && evalAuthentic\n\treturn\n}\n\n// Evaluate evaluates the IP and names from args against the SPF DNS record for the domain.\n\n\n\n\n\n\n\n\n\n// evaluate RemoteIP against domain from args, given record.\nfunc evaluate(ctx context.Context, log mlog.Log, record *Record, resolver dns.Resolver, args Args) (rstatus Status, mechanism, rexplanation string, rauthentic bool, rerr error) {\n\tstart := time.Now()\n\tdefer func() {\n\t\tlog.Debugx(\"spf evaluate result\", rerr,\n\t\t\tslog.Int(\"dnsrequests\", *args.dnsRequests),\n\t\t\tslog.Int(\"voidlookups\", *args.voidLookups),\n\t\t\tslog.Any(\"domain\", args.domain),\n\t\t\tslog.Any(\"status\", rstatus),\n\t\t\tslog.String(\"mechanism\", mechanism),\n\t\t\tslog.String(\"explanation\", rexplanation),\n\t\t\tslog.Duration(\"duration\", time.Since(start)))\n\t}()\n\n\tif args.dnsRequests == nil {\n\t\targs.dnsRequests = new(int)\n\t\targs.voidLookups = new(int)\n\t}\n\n\t// Response is authentic until we find a non-authentic DNS response.\n\trauthentic = true\n\n\t// To4 returns nil for an IPv6 address. To16 will return an IPv4-to-IPv6-mapped address.\n\tvar remote6 net.IP\n\tremote4 := args.RemoteIP.To4()\n\tif remote4 == nil {\n\t\tremote6 = args.RemoteIP.To16()\n\t}\n\n\t// Check if ip matches remote ip, taking cidr mask into account.\n\tcheckIP := func(ip net.IP, d Directive) bool {\n\t\t// ../rfc/7208:1097\n\t\tif remote4 != nil {\n\t\t\tip4 := ip.To4()\n\t\t\tif ip4 == nil {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tones := 32\n\t\t\tif d.IP4CIDRLen != nil {\n\t\t\t\tones = *d.IP4CIDRLen\n\t\t\t}\n\t\t\tmask := net.CIDRMask(ones, 32)\n\t\t\treturn ip4.Mask(mask).Equal(remote4.Mask(mask))\n\t\t}\n\n\t\tip6 := ip.To16()\n\t\tif ip6 == nil {\n\t\t\treturn false\n\t\t}\n\t\tones := 128\n\t\tif d.IP6CIDRLen != nil {\n\t\t\tones = *d.IP6CIDRLen\n\t\t}\n\t\tmask := net.CIDRMask(ones, 128)\n\t\treturn ip6.Mask(mask).Equal(remote6.Mask(mask))\n\t}\n\n\t// Used for \"a\" and \"mx\".\n\tcheckHostIP := func(domain dns.Domain, d Directive, args *Args) (bool, Status, error) {\n\t\tips, result, err := resolver.LookupIP(ctx, \"ip\", domain.ASCII+\".\")\n\t\trauthentic = rauthentic && result.Authentic\n\t\ttrackVoidLookup(err, args)\n\t\t// If \"not found\", we must ignore the error and treat as zero records in answer. ../rfc/7208:1116\n\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\treturn false, StatusTemperror, err\n\t\t}\n\t\tfor _, ip := range ips {\n\t\t\tif checkIP(ip, d) {\n\t\t\t\treturn true, StatusPass, nil\n\t\t\t}\n\t\t}\n\t\treturn false, StatusNone, nil\n\t}\n\n\tfor _, d := range record.Directives {\n\t\tvar match bool\n\n\t\tswitch d.Mechanism {\n\t\tcase \"include\", \"a\", \"mx\", \"ptr\", \"exists\":\n\t\t\tif err := trackLookupLimits(&args); err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t}\n\n\t\tswitch d.Mechanism {\n\t\tcase \"all\":\n\t\t\t// ../rfc/7208:1127\n\t\t\tmatch = true\n\n\t\tcase \"include\":\n\t\t\t// ../rfc/7208:1143\n\t\t\tname, authentic, err := expandDomainSpecDNS(ctx, resolver, d.DomainSpec, args)\n\t\t\trauthentic = rauthentic && authentic\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"expanding domain-spec for include: %w\", err)\n\t\t\t}\n\t\t\tnargs := args\n\t\t\tnargs.domain = dns.Domain{ASCII: strings.TrimSuffix(name, \".\")}\n\t\t\tnargs.explanation = &record.Explanation // ../rfc/7208:1548\n\t\t\tstatus, _, _, authentic, err := checkHost(ctx, log, resolver, nargs)\n\t\t\trauthentic = rauthentic && authentic\n\t\t\t// ../rfc/7208:1202\n\t\t\tswitch status {\n\t\t\tcase StatusPass:\n\t\t\t\tmatch = true\n\t\t\tcase StatusTemperror:\n\t\t\t\treturn StatusTemperror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"include %q: %w\", name, err)\n\t\t\tcase StatusPermerror, StatusNone:\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"include %q resulted in status %q: %w\", name, status, err)\n\t\t\t}\n\n\t\tcase \"a\":\n\t\t\t// ../rfc/7208:1249\n\t\t\t// note: the syntax for DomainSpec hints that macros should be expanded. But\n\t\t\t// expansion is explicitly documented, and only for \"include\", \"exists\" and\n\t\t\t// \"redirect\". This reason for this could be low-effort reuse of the domain-spec\n\t\t\t// ABNF rule. It could be an oversight. We are not implementing expansion for the\n\t\t\t// mechanism for which it isn't specified.\n\t\t\thost, err := evaluateDomainSpec(d.DomainSpec, args.domain)\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\thmatch, status, err := checkHostIP(host, d, &args)\n\t\t\tif err != nil {\n\t\t\t\treturn status, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tmatch = hmatch\n\n\t\tcase \"mx\":\n\t\t\t// ../rfc/7208:1262\n\t\t\thost, err := evaluateDomainSpec(d.DomainSpec, args.domain)\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\t// Note: LookupMX can return an error and still return MX records.\n\t\t\tmxs, result, err := resolver.LookupMX(ctx, host.ASCII+\".\")\n\t\t\trauthentic = rauthentic && result.Authentic\n\t\t\ttrackVoidLookup(err, &args)\n\t\t\t// note: we handle \"not found\" simply as a result of zero mx records.\n\t\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\t\treturn StatusTemperror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tif err == nil && len(mxs) == 1 && mxs[0].Host == \".\" {\n\t\t\t\t// Explicitly no MX.\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfor i, mx := range mxs {\n\t\t\t\t// ../rfc/7208:947 says that each mx record cannot result in more than 10 DNS\n\t\t\t\t// requests. This seems independent of the overall limit of 10 DNS requests. So an\n\t\t\t\t// MX request resulting in 11 names is valid, but we must return a permerror if we\n\t\t\t\t// found no match before the 11th name.\n\t\t\t\t// ../rfc/7208:945\n\t\t\t\tif i >= 10 {\n\t\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, ErrTooManyDNSRequests\n\t\t\t\t}\n\t\t\t\t// Parsing lax (unless in pedantic mode) for MX targets with underscores as seen in the wild.\n\t\t\t\tmxd, err := dns.ParseDomainLax(strings.TrimSuffix(mx.Host, \".\"))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t\t}\n\t\t\t\thmatch, status, err := checkHostIP(mxd, d, &args)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn status, d.MechanismString(), \"\", rauthentic, err\n\t\t\t\t}\n\t\t\t\tif hmatch {\n\t\t\t\t\tmatch = hmatch\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase \"ptr\":\n\t\t\t// ../rfc/7208:1281\n\t\t\thost, err := evaluateDomainSpec(d.DomainSpec, args.domain)\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\n\t\t\trnames, result, err := resolver.LookupAddr(ctx, args.RemoteIP.String())\n\t\t\trauthentic = rauthentic && result.Authentic\n\t\t\ttrackVoidLookup(err, &args)\n\t\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\t\treturn StatusTemperror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tlookups := 0\n\t\tptrnames:\n\t\t\tfor _, rname := range rnames {\n\t\t\t\trd, err := dns.ParseDomain(strings.TrimSuffix(rname, \".\"))\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Errorx(\"bad address in ptr record\", err, slog.String(\"address\", rname))\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\t// ../rfc/7208-eid4751 ../rfc/7208:1323\n\t\t\t\tif rd.ASCII != host.ASCII && !strings.HasSuffix(rd.ASCII, \".\"+host.ASCII) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// ../rfc/7208:963, we must ignore entries after the first 10.\n\t\t\t\tif lookups >= 10 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tlookups++\n\t\t\t\tips, result, err := resolver.LookupIP(ctx, \"ip\", rd.ASCII+\".\")\n\t\t\t\trauthentic = rauthentic && result.Authentic\n\t\t\t\ttrackVoidLookup(err, &args)\n\t\t\t\tfor _, ip := range ips {\n\t\t\t\t\tif checkIP(ip, d) {\n\t\t\t\t\t\tmatch = true\n\t\t\t\t\t\tbreak ptrnames\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t// ../rfc/7208:1351\n\t\tcase \"ip4\":\n\t\t\tif remote4 != nil {\n\t\t\t\tmatch = checkIP(d.IP, d)\n\t\t\t}\n\t\tcase \"ip6\":\n\t\t\tif remote6 != nil {\n\t\t\t\tmatch = checkIP(d.IP, d)\n\t\t\t}\n\n\t\tcase \"exists\":\n\t\t\t// ../rfc/7208:1382\n\t\t\tname, authentic, err := expandDomainSpecDNS(ctx, resolver, d.DomainSpec, args)\n\t\t\trauthentic = rauthentic && authentic\n\t\t\tif err != nil {\n\t\t\t\treturn Status\n```\n# Function Signature: func Evaluate(ctx context.Context, elog *slog.Logger, record *Record, resolver dns.Resolver, args Args) (rstatus Status, mechanism, rexplanation string, rauthentic bool, rerr error) {\n# Natural Language Annotations: // Evaluate evaluates the IP and names from args against the SPF DNS record for the domain.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc Evaluate(ctx context.Context, elog *slog.Logger, record *Record, resolver dns.Resolver, args Args) (rstatus Status, mechanism, rexplanation string, rauthentic bool, rerr error) {\n\tstart := time.Now()\n\tdefer func() {\n\t\tlog.Debugx(\"spf evaluate result\", rerr,\n\t\t\tslog.Int(\"dnsrequests\", *args.dnsRequests),\n\t\t\tslog.Int(\"voidlookups\", *args.voidLookups),\n\t\t\tslog.Any(\"domain\", args.domain),\n\t\t\tslog.Any(\"status\", rstatus),\n\t\t\tslog.String(\"mechanism\", mechanism),\n\t\t\tslog.String(\"explanation\", rexplanation),\n\t\t\tslog.Duration(\"duration\", time.Since(start)))\n\t}()\n\n\tif args.dnsRequests == nil {\n\t\targs.dnsRequests = new(int)\n\t\targs.voidLookups = new(int)\n\t}\n\n\t// Response is authentic until we find a non-authentic DNS response.\n\trauthentic = true\n\n\t// To4 returns nil for an IPv6 address. To16 will return an IPv4-to-IPv6-mapped address.\n\tvar remote6 net.IP\n\tremote4 := args.RemoteIP.To4()\n\tif remote4 == nil {\n\t\tremote6 = args.RemoteIP.To16()\n\t}\n\n\t// Check if ip matches remote ip, taking cidr mask into account.\n\tcheckIP := func(ip net.IP, d Directive) bool {\n\t\t// ../rfc/7208:1097\n\t\tif remote4 != nil {\n\t\t\tip4 := ip.To4()\n\t\t\tif ip4 == nil {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tones := 32\n\t\t\tif d.IP4CIDRLen != nil {\n\t\t\t\tones = *d.IP4CIDRLen\n\t\t\t}\n\t\t\tmask := net.CIDRMask(ones, 32)\n\t\t\treturn ip4.Mask(mask).Equal(remote4.Mask(mask))\n\t\t}\n\n\t\tip6 := ip.To16()\n\t\tif ip6 == nil {\n\t\t\treturn false\n\t\t}\n\t\tones := 128\n\t\tif d.IP6CIDRLen != nil {\n\t\t\tones = *d.IP6CIDRLen\n\t\t}\n\t\tmask := net.CIDRMask(ones, 128)\n\t\treturn ip6.Mask(mask).Equal(remote6.Mask(mask))\n\t}\n\n\t// Used for \"a\" and \"mx\".\n\tcheckHostIP := func(domain dns.Domain, d Directive, args *Args) (bool, Status, error) {\n\t\tips, result, err := resolver.LookupIP(ctx, \"ip\", domain.ASCII+\".\")\n\t\trauthentic = rauthentic && result.Authentic\n\t\ttrackVoidLookup(err, args)\n\t\t// If \"not found\", we must ignore the error and treat as zero records in answer. ../rfc/7208:1116\n\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\treturn false, StatusTemperror, err\n\t\t}\n\t\tfor _, ip := range ips {\n\t\t\tif checkIP(ip, d) {\n\t\t\t\treturn true, StatusPass, nil\n\t\t\t}\n\t\t}\n\t\treturn false, StatusNone, nil\n\t}\n\n\tfor _, d := range record.Directives {\n\t\tvar match bool\n\n\t\tswitch d.Mechanism {\n\t\tcase \"include\", \"a\", \"mx\", \"ptr\", \"exists\":\n\t\t\tif err := trackLookupLimits(&args); err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t}\n\n\t\tswitch d.Mechanism {\n\t\tcase \"all\":\n\t\t\t// ../rfc/7", "  ```\nfunc Evaluate(ctx context.Context, elog *slog.Logger, record *Record, resolver dns.Resolver, args Args) (rstatus Status, mechanism, rexplanation string, rauthentic bool, rerr error) {\n\tstart := time.Now()\n\tdefer func() {\n\t\tlog.Debugx(\"spf evaluate result\", rerr,\n\t\t\tslog.Int(\"dnsrequests\", *args.dnsRequests),\n\t\t\tslog.Int(\"voidlookups\", *args.voidLookups),\n\t\t\tslog.Any(\"domain\", args.domain),\n\t\t\tslog.Any(\"status\", rstatus),\n\t\t\tslog.String(\"mechanism\", mechanism),\n\t\t\tslog.String(\"explanation\", rexplanation),\n\t\t\tslog.Duration(\"duration\", time.Since(start)))\n\t}()\n\n\tif args.dnsRequests == nil {\n\t\targs.dnsRequests = new(int)\n\t\targs.voidLookups = new(int)\n\t}\n\n\t// Response is authentic until we find a non-authentic DNS response.\n\trauthentic = true\n\n\t// To4 returns nil for an IPv6 address. To16 will return an IPv4-to-IPv6-mapped address.\n\tvar remote6 net.IP\n\tremote4 := args.RemoteIP.To4()\n\tif remote4 == nil {\n\t\tremote6 = args.RemoteIP.To16()\n\t}\n\n\t// Check if ip matches remote ip, taking cidr mask into account.\n\tcheckIP := func(ip net.IP, d Directive) bool {\n\t\t// ../rfc/7208:1097\n\t\tif remote4 != nil {\n\t\t\tip4 := ip.To4()\n\t\t\tif ip4 == nil {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tones := 32\n\t\t\tif d.IP4CIDRLen != nil {\n\t\t\t\tones = *d.IP4CIDRLen\n\t\t\t}\n\t\t\tmask := net.CIDRMask(ones, 32)\n\t\t\treturn ip4.Mask(mask).Equal(remote4.Mask(mask))\n\t\t}\n\n\t\tip6 := ip.To16()\n\t\tif ip6 == nil {\n\t\t\treturn false\n\t\t}\n\t\tones := 128\n\t\tif d.IP6CIDRLen != nil {\n\t\t\tones = *d.IP6CIDRLen\n\t\t}\n\t\tmask := net.CIDRMask(ones, 128)\n\t\treturn ip6.Mask(mask).Equal(remote6.Mask(mask))\n\t}\n\n\t// Used for \"a\" and \"mx\".\n\tcheckHostIP := func(domain dns.Domain, d Directive, args *Args) (bool, Status, error) {\n\t\tips, result, err := resolver.LookupIP(ctx, \"ip\", domain.ASCII+\".\")\n\t\trauthentic = rauthentic && result.Authentic\n\t\ttrackVoidLookup(err, args)\n\t\t// If \"not found\", we must ignore the error and treat as zero records in answer. ../rfc/7208:1116\n\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\treturn false, StatusTemperror, err\n\t\t}\n\t\tfor _, ip := range ips {\n\t\t\tif checkIP(ip, d) {\n\t\t\t\treturn true, StatusPass, nil\n\t\t\t}\n\t\t}\n\t\treturn false, StatusNone, nil\n\t}\n\n\tfor _, d := range record.Directives {\n\t\tvar match bool\n\n\t\tswitch d.Mechanism {\n\t\tcase \"include\", \"a\", \"mx\", \"ptr\", \"exists\":\n\t\t\tif err := trackLookupLimits(&args); err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t}\n\n\t\tswitch d.Mechanism {\n\t\tcase \"all\":\n\t\t\t// ../rfc/7", "  ```\nfunc Evaluate(ctx context.Context, elog *slog.Logger, record *Record, resolver dns.Resolver, args Args) (rstatus Status, mechanism, rexplanation string, rauthentic bool, rerr error) {\n\tstart := time.Now()\n\tdefer func() {\n\t\tlog.Debugx(\"spf evaluate result\", rerr,\n\t\t\tslog.Int(\"dnsrequests\", *args.dnsRequests),\n\t\t\tslog.Int(\"voidlookups\", *args.voidLookups),\n\t\t\tslog.Any(\"domain\", args.domain),\n\t\t\tslog.Any(\"status\", rstatus),\n\t\t\tslog.String(\"mechanism\", mechanism),\n\t\t\tslog.String(\"explanation\", rexplanation),\n\t\t\tslog.Duration(\"duration\", time.Since(start)))\n\t}()\n\n\tif args.dnsRequests == nil {\n\t\targs.dnsRequests = new(int)\n\t\targs.voidLookups = new(int)\n\t}\n\n\t// Response is authentic until we find a non-authentic DNS response.\n\trauthentic = true\n\n\t// To4 returns nil for an IPv6 address. To16 will return an IPv4-to-IPv6-mapped address.\n\tvar remote6 net.IP\n\tremote4 := args.RemoteIP.To4()\n\tif remote4 == nil {\n\t\tremote6 = args.RemoteIP.To16()\n\t}\n\n\t// Check if ip matches remote ip, taking cidr mask into account.\n\tcheckIP := func(ip net.IP, d Directive) bool {\n\t\t// ../rfc/7208:1097\n\t\tif remote4 != nil {\n\t\t\tip4 := ip.To4()\n\t\t\tif ip4 == nil {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tones := 32\n\t\t\tif d.IP4CIDRLen != nil {\n\t\t\t\tones = *d.IP4CIDRLen\n\t\t\t}\n\t\t\tmask := net.CIDRMask(ones, 32)\n\t\t\treturn ip4.Mask(mask).Equal(remote4.Mask(mask))\n\t\t}\n\n\t\tip6 := ip.To16()\n\t\tif ip6 == nil {\n\t\t\treturn false\n\t\t}\n\t\tones := 128\n\t\tif d.IP6CIDRLen != nil {\n\t\t\tones = *d.IP6CIDRLen\n\t\t}\n\t\tmask := net.CIDRMask(ones, 128)\n\t\treturn ip6.Mask(mask).Equal(remote6.Mask(mask))\n\t}\n\n\t// Used for \"a\" and \"mx\".\n\tcheckHostIP := func(domain dns.Domain, d Directive, args *Args) (bool, Status, error) {\n\t\tips, result, err := resolver.LookupIP(ctx, \"ip\", domain.ASCII+\".\")\n\t\trauthentic = rauthentic && result.Authentic\n\t\ttrackVoidLookup(err, args)\n\t\t// If \"not found\", we must ignore the error and treat as zero records in answer. ../rfc/7208:1116\n\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\treturn false, StatusTemperror, err\n\t\t}\n\t\tfor _, ip := range ips {\n\t\t\tif checkIP(ip, d) {\n\t\t\t\treturn true, StatusPass, nil\n\t\t\t}\n\t\t}\n\t\treturn false, StatusNone, nil\n\t}\n\n\tfor _, d := range record.Directives {\n\t\tvar match bool\n\n\t\tswitch d.Mechanism {\n\t\tcase \"include\", \"a\", \"mx\", \"ptr\", \"exists\":\n\t\t\tif err := trackLookupLimits(&args); err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t}\n\n\t\tswitch d.Mechanism {\n\t\tcase \"all\":\n\t\t\t// ../rfc/7"]}, "mox-spf/parse.go-Record": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/spf/parse.go:\n```\npackage spf\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strconv\"\n\t\"strings\"\n)\n\n// Record is a parsed SPF DNS record.\n//\n// An example record for example.com:\n//\n//\tv=spf1 +mx a:colo.example.com/28 -all\ntype Record struct {\n\tVersion     string      // Must be \"spf1\".\n\tDirectives  []Directive // An IP is evaluated against each directive until a match is found.\n\tRedirect    string      // Modifier that redirects SPF checks to other domain after directives did not match. Optional. For \"redirect=\".\n\tExplanation string      // Modifier for creating a user-friendly error message when an IP results in status \"fail\".\n\tOther       []Modifier  // Other modifiers.\n}\n\n// Directive consists of a mechanism that describes how to check if an IP matches,\n// an (optional) qualifier indicating the policy for a match, and optional\n// parameters specific to the mechanism.\ntype Directive struct {\n\tQualifier  string // Sets the result if this directive matches. \"\" and \"+\" are \"pass\", \"-\" is \"fail\", \"?\" is \"neutral\", \"~\" is \"softfail\".\n\tMechanism  string // \"all\", \"include\", \"a\", \"mx\", \"ptr\", \"ip4\", \"ip6\", \"exists\".\n\tDomainSpec string // For include, a, mx, ptr, exists. Always in lower-case when parsed using ParseRecord.\n\tIP         net.IP `json:\"-\"` // For ip4, ip6.\n\tIPstr      string // Original string for IP, always with /subnet.\n\tIP4CIDRLen *int   // For a, mx, ip4.\n\tIP6CIDRLen *int   // For a, mx, ip6.\n}\n\n// MechanismString returns a directive in string form for use in the Received-SPF header.\nfunc (d Directive) MechanismString() string {\n\ts := d.Qualifier + d.Mechanism\n\tif d.DomainSpec != \"\" {\n\t\ts += \":\" + d.DomainSpec\n\t} else if d.IP != nil {\n\t\ts += \":\" + d.IP.String()\n\t}\n\tif d.IP4CIDRLen != nil {\n\t\ts += fmt.Sprintf(\"/%d\", *d.IP4CIDRLen)\n\t}\n\tif d.IP6CIDRLen != nil {\n\t\tif d.Mechanism != \"ip6\" {\n\t\t\ts += \"/\"\n\t\t}\n\t\ts += fmt.Sprintf(\"/%d\", *d.IP6CIDRLen)\n\t}\n\treturn s\n}\n\n// Modifier provides additional information for a policy.\n// \"redirect\" and \"exp\" are not represented as a Modifier but explicitly in a Record.\ntype Modifier struct {\n\tKey   string // Key is case-insensitive.\n\tValue string\n}\n\n// Record returns an DNS record, to be configured as a TXT record for a domain,\n// e.g. a TXT record for example.com.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype parser struct {\n\ts     string\n\tlower string\n\to     int\n}\n\ntype parseError string\n\nfunc (e parseError) Error() string {\n\treturn string(e)\n}\n\n// toLower lower cases bytes that are A-Z. strings.ToLower does too much. and\n// would replace invalid bytes with unicode replacement characters, which would\n// break our requirement that offsets into the original and upper case strings\n// point to the same character.\nfunc toLower(s string) string {\n\tr := []byte(s)\n\tfor i, c := range r {\n\t\tif c >= 'A' && c <= 'Z' {\n\t\t\tr[i] = c + 0x20\n\t\t}\n\t}\n\treturn string(r)\n}\n\n// ParseRecord parses an SPF DNS TXT record.\nfunc ParseRecord(s string) (r *Record, isspf bool, rerr error) {\n\tp := parser{s: s, lower: toLower(s)}\n\n\tr = &Record{\n\t\tVersion: \"spf1\",\n\t}\n\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\tif err, ok := x.(parseError); ok {\n\t\t\trerr = err\n\t\t\treturn\n\t\t}\n\t\tpanic(x)\n\t}()\n\n\tp.xtake(\"v=spf1\")\n\tfor !p.empty() {\n\t\tp.xtake(\" \")\n\t\tisspf = true // ../rfc/7208:825\n\t\tfor p.take(\" \") {\n\t\t}\n\t\tif p.empty() {\n\t\t\tbreak\n\t\t}\n\n\t\tqualifier := p.takelist(\"+\", \"-\", \"?\", \"~\")\n\t\tmechanism := p.takelist(\"all\", \"include:\", \"a\", \"mx\", \"ptr\", \"ip4:\", \"ip6:\", \"exists:\")\n\t\tif qualifier != \"\" && mechanism == \"\" {\n\t\t\tp.xerrorf(\"expected mechanism after qualifier\")\n\t\t}\n\t\tif mechanism == \"\" {\n\t\t\t// ../rfc/7208:2597\n\t\t\tmodifier := p.takelist(\"redirect=\", \"exp=\")\n\t\t\tif modifier == \"\" {\n\t\t\t\t// ../rfc/7208:2600\n\t\t\t\tname := p.xtakefn1(func(c rune, i int) bool {\n\t\t\t\t\talpha := c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z'\n\t\t\t\t\treturn alpha || i > 0 && (c >= '0' && c <= '9' || c == '-' || c == '_' || c == '.')\n\t\t\t\t})\n\t\t\t\tp.xtake(\"=\")\n\t\t\t\tv := p.xmacroString(true)\n\t\t\t\tr.Other = append(r.Other, Modifier{name, v})\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tv := p.xdomainSpec(true)\n\t\t\tmodifier = strings.TrimSuffix(modifier, \"=\")\n\t\t\tif modifier == \"redirect\" {\n\t\t\t\tif r.Redirect != \"\" {\n\t\t\t\t\t// ../rfc/7208:1419\n\t\t\t\t\tp.xerrorf(\"duplicate redirect modifier\")\n\t\t\t\t}\n\t\t\t\tr.Redirect = v\n\t\t\t}\n\t\t\tif modifier == \"exp\" {\n\t\t\t\tif r.Explanation != \"\" {\n\t\t\t\t\t// ../rfc/7208:1419\n\t\t\t\t\tp.xerrorf(\"duplicate exp modifier\")\n\t\t\t\t}\n\t\t\t\tr.Explanation = v\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\t// ../rfc/7208:2585\n\t\td := Directive{\n\t\t\tQualifier: qualifier,\n\t\t\tMechanism: strings.TrimSuffix(mechanism, \":\"),\n\t\t}\n\t\tswitch d.Mechanism {\n\t\tcase \"all\":\n\t\tcase \"include\":\n\t\t\td.DomainSpec = p.xdomainSpec(false)\n\t\tcase \"a\", \"mx\":\n\t\t\tif p.take(\":\") {\n\t\t\t\td.DomainSpec = p.xdomainSpec(false)\n\t\t\t}\n\t\t\tif p.take(\"/\") {\n\t\t\t\tif !p.take(\"/\") {\n\t\t\t\t\tnum, _ := p.xnumber()\n\t\t\t\t\tif num > 32 {\n\t\t\t\t\t\tp.xerrorf(\"invalid ip4 cidr length %d\", num)\n\t\t\t\t\t}\n\t\t\t\t\td.IP4CIDRLen = &num\n\t\t\t\t\tif !p.take(\"//\") {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tnum, _ := p.xnumber()\n\t\t\t\tif num > 128 {\n\t\t\t\t\tp.xerrorf(\"invalid ip6 cidr length %d\", num)\n\t\t\t\t}\n\t\t\t\td.IP6CIDRLen = &num\n\t\t\t}\n\t\tcase \"ptr\":\n\t\t\tif p.take(\":\") {\n\t\t\t\td.DomainSpec = p.xdomainSpec(false)\n\t\t\t}\n\t\tcase \"ip4\":\n\t\t\td.IP, d.IPstr = p.xip4address()\n\t\t\tif p.take(\"/\") {\n\t\t\t\tnum, _ := p.xnumber()\n\t\t\t\tif num > 32 {\n\t\t\t\t\tp.xerrorf(\"invalid ip4 cidr length %d\", num)\n\t\t\t\t}\n\t\t\t\td.IP4CIDRLen = &num\n\t\t\t\td.IPstr += fmt.Sprintf(\"/%d\", num)\n\t\t\t} else {\n\t\t\t\td.IPstr += \"/32\"\n\t\t\t}\n\t\tcase \"ip6\":\n\t\t\td.IP, d.IPstr = p.xip6address()\n\t\t\tif p.take(\"/\") {\n\t\t\t\tnum, _ := p.xnumber()\n\t\t\t\tif num > 128 {\n\t\t\t\t\tp.xerrorf(\"invalid ip6 cidr length %d\", num)\n\t\t\t\t}\n\t\t\t\td.IP6CIDRLen = &num\n\t\t\t\td.IPstr += fmt.Sprintf(\"/%d\", num)\n\t\t\t} else {\n\t\t\t\td.IPstr += \"/128\"\n\t\t\t}\n\t\tcase \"exists\":\n\t\t\td.DomainSpec = p.xdomainSpec(false)\n\t\tdefault:\n\t\t\treturn nil, true, fmt.Errorf(\"internal error, missing case for mechanism %q\", d.Mechanism)\n\t\t}\n\t\tr.Directives = append(r.Directives, d)\n\t}\n\treturn r, true, nil\n}\n\nfunc (p *parser) xerrorf(format string, args ...any) {\n\tmsg := fmt.Sprintf(format, args...)\n\tif !p.empty() {\n\t\tmsg += fmt.Sprintf(\" (leftover %q)\", p.s[p.o:])\n\t}\n\tpanic(parseError(msg))\n}\n\n// operates on original-cased characters.\nfunc (p *parser) xtakefn1(fn func(rune, int) bool) string {\n\tr := \"\"\n\tfor i, c := range p.s[p.o:] {\n\t\tif !fn(c, i) {\n\t\t\tbreak\n\t\t}\n\t\tr += string(c)\n\t}\n\tif r == \"\" {\n\t\tp.xerrorf(\"need at least 1 char\")\n\t}\n\tp.o += len(r)\n\treturn r\n}\n\n// caller should set includingSlash to false when parsing \"a\" or \"mx\", or the / would be consumed as valid macro literal.\nfunc (p *parser) xdomainSpec(includingSlash bool) string {\n\t// ../rfc/7208:1579\n\t// This also consumes the \"domain-end\" part, which we check below.\n\ts := p.xmacroString(includingSlash)\n\n\t// The ABNF says s must either end in macro-expand, or \".\" toplabel [\".\"]. The\n\t// toplabel rule implies the intention is to force a valid DNS name. We cannot just\n\t// check if the name is valid, because \"macro-expand\" is not a valid label. So we\n\t// recognize the macro-expand, and check for valid toplabel otherwise, because we\n\t// syntax errors must result in Permerror.\n\tfor _, suf := range []string{\"%%\", \"%_\", \"%-\", \"}\"} {\n\t\t// The check for \"}\" assumes a \"%{\" precedes it...\n\t\tif strings.HasSuffix(s, suf) {\n\t\t\treturn s\n\t\t}\n\t}\n\ttl := strings.Split(strings.TrimSuffix(s, \".\"), \".\")\n\tt := tl[len(tl)-1]\n\tif t == \"\" {\n\t\tp.xerrorf(\"invalid empty toplabel\")\n\t}\n\tnums := 0\n\tfor i, c := range t {\n\t\tswitch {\n\t\tcase c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z':\n\t\tcase c >= '0' && c <= '9':\n\t\t\tnums++\n\t\tcase c == '-':\n\t\t\tif i == 0 {\n\t\t\t\tp.xerrorf(\"bad toplabel, invalid leading dash\")\n\t\t\t}\n\t\t\tif i == len(t)-1 {\n\t\t\t\tp.xerrorf(\"bad toplabel, invalid trailing dash\")\n\t\t\t}\n\t\tdefault:\n\t\t\tp.xerrorf(\"bad toplabel, invalid character\")\n\t\t}\n\t}\n\tif nums == len(t) {\n\t\tp.xerrorf(\"bad toplabel, cannot be all digits\")\n\t}\n\treturn s\n}\n\nfunc (p *parser) xmacroString(includingSlash bool) string {\n\t// ../rfc/7208:1588\n\tr := \"\"\n\tfor !p.empty() {\n\t\tw := p.takelist(\"%{\", \"%%\", \"%_\", \"%-\") // \"macro-expand\"\n\t\tif w == \"\" {\n\t\t\t// \"macro-literal\"\n\t\t\tif !p.empty() {\n\t\t\t\tb := p.peekchar()\n\t\t\t\tif b > ' ' && b < 0x7f && b != '%' && (includingSlash || b != '/') {\n\t\t\t\t\tr += string(b)\n\t\t\t\t\tp.o++\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t\tr += w\n\t\tif w != \"%{\" {\n\t\t\tcontinue\n\t\t}\n\t\tr += p.xtakelist(\"s\", \"l\", \"o\", \"d\", \"i\", \"p\", \"h\", \"c\", \"r\", \"t\", \"v\") // \"macro-letter\"\n\t\tdigits := p.digits()\n\t\tif digits != \"\" {\n\t\t\tif v, err := strconv.Atoi(digits); err != nil {\n\t\t\t\tp.xerrorf(\"bad digits: %v\", err)\n\t\t\t} else if v == 0 {\n\t\t\t\tp.xerrorf(\"bad digits 0 for 0 labels\")\n\t\t\t}\n\t\t}\n\t\tr += digits\n\t\tif p.take(\"r\") {\n\t\t\tr += \"r\"\n\t\t}\n\t\tfor {\n\t\t\tdelimiter := p.takelist(\".\", \"-\", \"+\", \",\", \"/\", \"_\", \"=\")\n\t\t\tif delimiter == \"\" {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tr += delimiter\n\t\t}\n\t\tr += p.xtake(\"}\")\n\t}\n\treturn r\n}\n\nfunc (p *parser) empty() bool {\n\treturn p.o >= len(p.s)\n}\n\n// returns next original-cased character.\nfunc (p *parser) peekchar() byte {\n\treturn p.s[p.o]\n}\n\nfunc (p *parser) xtakelist(l ...string) string {\n\tw := p.takelist(l...)\n\tif w == \"\" {\n\t\tp.xerrorf(\"no match for %v\", l)\n\t}\n\treturn w\n}\n\nfunc (p *parser) takelist(l ...string) string {\n\tfor _, w := range l {\n\t\tif strings.HasPrefix(p.lower[p.o:], w) {\n\t\t\tp.o += len(w)\n\t\t\treturn w\n\t\t}\n\t}\n\treturn \"\"\n}\n\n// digits parses zero or more digits.\nfunc (p *parser) digits() string {\n\tr := \"\"\n\tfor !p.empty() {\n\t\tb := p.peekchar()\n\t\tif b >= '0' && b <= '9' {\n\t\t\tr += string(b)\n\t\t\tp.o++\n\t\t} else {\n\t\t\tbreak\n\t\t}\n\t}\n\treturn r\n}\n\nfunc (p *parser) take(s string) bool {\n\tif strings.HasPrefix(p.lower[p.o:], s) {\n\t\tp.o += len(s)\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (p *parser) xtake(s string) string {\n\tok := p.take(s)\n\tif !ok {\n\t\tp.xerrorf(\"expected %q\", s)\n\t}\n\treturn s\n}\n\nfunc (p *parser) xnumber() (int, string) {\n\ts := p.digits()\n\tif s == \"\" {\n\t\tp.xerrorf(\"expected number\")\n\t}\n\tif s == \"0\" {\n\t\treturn 0, s\n\t}\n\tif strings.HasPrefix(s, \"0\") {\n\t\tp.xerrorf(\"bogus leading 0 in number\")\n\t}\n\tv, err := strconv.Atoi(s)\n\tif err != nil {\n\t\tp.xerrorf(\"parsing number for %q: %s\", s, err)\n\t}\n\treturn v, s\n}\n\nfunc (p *parser) xip4address() (net.IP, string) {\n\t// ../rfc/7208:2607\n\tip4num := func() (byte, string) {\n\t\tv, vs := p.xnumber()\n\t\tif v > 255 {\n\t\t\tp.xerrorf(\"bad ip4 number %d\", v)\n\t\t}\n\t\treturn byte(v), vs\n\t}\n\ta, as := ip4num()\n\tp.xtake(\".\")\n\tb, bs := ip4num()\n\tp.xtake(\".\")\n\tc, cs := ip4num()\n\tp.xtake(\".\")\n\td, ds := ip4num()\n\treturn net.IPv4(a, b, c, d), as + \".\" + bs + \".\" + cs + \".\" + ds\n}\n\nfunc (p *parser) xip6address() (net.IP, string) {\n\t// ../rfc/7208:2614\n\t// We just take in a string that has characters that IPv6 uses, then parse it.\n\ts := p.xtakefn1(func(c rune, i int) bool {\n\t\treturn c >= '0' && c <= '9' || c >= 'a' && c <= 'f' || c >= 'A' && c <= 'F' || c == ':' || c == '.'\n\t})\n\tip := net.ParseIP(s)\n\tif ip == nil {\n\t\tp.xerrorf(\"ip6 address %q not valid\", s)\n\t}\n\treturn ip, s\n}\n\n```\n# Function Signature: func (r Record) Record() (string, error) {\n# Natural Language Annotations: // Record returns an DNS record, to be configured as a TXT record for a domain,\n// e.g. a TXT record for example.com.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (r Record) Record() (string, error) {\n\ts := \"v=spf1\"\n\tfor _, d := range r.Directives {\n\t\ts += \" \"\n\t\ts += d.Qualifier + d.Mechanism\n\t\tif d.DomainSpec != \"\" {\n\t\t\ts += \":\" + d.DomainSpec\n\t\t} else if d.IP != nil {\n\t\t\ts += \":\" + d.IP.String()\n\t\t}\n\t\tif d.IP4CIDRLen != nil {\n\t\t\ts += fmt.Sprintf(\"/%d\", *d.IP4CIDRLen)\n\t\t}\n\t\tif d.IP6CIDRLen != nil {\n\t\t\tif d.Mechanism != \"ip6\" {\n\t\t\t\ts += \"/\"\n\t\t\t}\n\t\t\ts += fmt.Sprintf(\"/%d\", *d.IP6CIDRLen)\n\t\t}\n\t}\n\tif r.Redirect != \"\" {\n\t\t// ../rfc/7208:1419\n\t\ts += \"redirect=\" + r.Redirect\n\t}\n\tif r.Explanation != \"\" {\n\t\t// ../rfc/7208:1419\n\t\ts += \"exp=\" + r.Explanation\n\t}\n\tfor _, m := range r.Other {\n\t\t// ../rfc/7208:2600\n\t\ts += m.Key + \"=\" + m.Value\n\t}\n\treturn s, nil\n}\n```", "  ```\nfunc (r Record) Record() (string, error) {\n\tversion := \"v=spf1\"\n\tvar result bytes.Buffer\n\tresult.WriteString(version)\n\n\tfor _, d := range r.Directives {\n\t\tqualifier := d.Qualifier\n\t\tmechanism := d.Mechanism\n\t\tdomainSpec := d.DomainSpec\n\t\tip := d.IP\n\t\tipstr := d.IPstr\n\t\tip4CIDRLen := d.IP4CIDRLen\n\t\tip6CIDRLen := d.IP6CIDRLen\n\n\t\tdirective := fmt.Sprintf(\"%s %s\", qualifier, mechanism)\n\t\tif domainSpec != \"\" {\n\t\t\tdirective += \":\" + domainSpec\n\t\t} else if ip != nil {\n\t\t\tdirective += \":\" + ip.String()\n\t\t}\n\t\tif ip4CIDRLen != nil {\n\t\t\tdirective += fmt.Sprintf(\"/%d\", *ip4CIDRLen)\n\t\t}\n\t\tif ip6CIDRLen != nil {\n\t\t\tif mechanism != \"ip6\" {\n\t\t\t\tdirective += \"/\"\n\t\t\t}\n\t\t\tdirective += fmt.Sprintf(\"/%d\", *ip6CIDRLen)\n\t\t}\n\t\tresult.WriteString(directive)\n\t}\n\n\tfor _, m := range r.Other {\n\t\tkey := m.Key\n\t\tvalue := m.Value\n\t\tmodifier := fmt.Sprintf(\"%s=%s\", key, value)\n\t\tresult.WriteString(modifier)\n\t}\n\n\trecord := result.String()\n\tif record == \"\" {\n\t\treturn \"\", fmt.Errorf(\"empty SPF record\")\n\t}\n\treturn record, nil\n}\n```", "  ```\nfunc (r Record) Record() (string, error) {\n\t// Validate record version\n\tif r.Version != \"spf1\" {\n\t\treturn \"\", fmt.Errorf(\"invalid spf version: %s\", r.Version)\n\t}\n\t// Redirect is a modifier, not a directive, so we need to handle it separately\n\tif r.Redirect != \"\" {\n\t\treturn fmt.Sprintf(\"v=spf1 redirect=%s\", r.Redirect), nil\n\t}\n\t// Validate directives\n\tfor _, d := range r.Directives {\n\t\t// Validate mechanism\n\t\tswitch d.Mechanism {\n\t\tcase \"all\", \"include\", \"a\", \"mx\", \"ptr\", \"ip4\", \"ip6\", \"exists\":\n\t\tdefault:\n\t\t\treturn \"\", fmt.Errorf(\"invalid spf mechanism: %s\", d.Mechanism)\n\t\t}\n\t\t// Validate qualifier\n\t\tswitch d.Qualifier {\n\t\tcase \"\", \"+\", \"-\", \"?\", \"~\":\n\t\tdefault:\n\t\t\treturn \"\", fmt.Errorf(\"invalid spf qualifier: %s\", d.Qualifier)\n\t\t}\n\t\t// Validate domain spec\n\t\tswitch d.Mechanism {\n\t\tcase \"all\", \"include\", \"exists\":\n\t\t\tif d.DomainSpec == \"\" {\n\t\t\t\treturn \"\", fmt.Errorf(\"missing spf domain spec: %s\", d.Mechanism)\n\t\t\t}\n\t\tcase \"a\", \"mx\", \"ptr\":\n\t\t\tif d.DomainSpec != \"\" {\n\t\t\t\treturn \"\", fmt.Errorf(\"invalid spf domain spec: %s\", d.DomainSpec)\n\t\t\t}\n\t\tcase \"ip4\":\n\t\tcase \"ip6\":\n\t\t}\n\t}\n\t// Validate other modifiers\n\tfor _, m := range r.Other {\n\t\t// Validate key\n\t\tswitch m.Key {\n\t\tcase \"redirect\", \"exp\":\n\t\tdefault:\n\t\t\treturn \"\", fmt.Errorf(\"invalid spf modifier: %s\", m.Key)\n\t\t}\n\t\t// Validate value\n\t\tif m.Value == \"\" {\n\t\t\treturn \"\", fmt.Errorf(\"empty spf modifier value: %s\", m.Key)\n\t\t}\n\t}\n\t// Build the DNS record\n\tvar buf bytes.Buffer\n\tbuf.WriteString(\"v=spf1 \")\n\tfor _, d := range r.Directives {\n\t\tfmt.Fprintf(&buf, \"%s%s\", d.Qualifier, d.Mechanism)\n\t\tswitch d.Mechanism {\n\t\tcase \"include\", \"exists\", \"a\", \"mx\", \"ptr\", \"ip4\":\n\t\t\tbuf.WriteString(\":\")\n\t\t\tbuf.WriteString(d.DomainSpec)\n\t\tcase \"ip6\":\n\t\t\tbuf.WriteString(\":\")\n\t\t\tbuf.WriteString(d.IPstr)\n\t\tcase \"redirect\":\n\t\t\treturn fmt.Sprintf(\"v=spf1 %s\", d.Value), nil\n\t\t}\n\t\tfmt.Fprintf(&buf, \"//%v\", d.IP4CIDRLen)\n\t\tfmt.Fprintf(&buf, \"//%v\", d.IP6CIDRLen)\n\t}\n\tfor _, m := range r.Other {\n\t\tfmt.Fprintf(&buf, \" %s=%s\", m.Key, m.Value)\n\t}\n\treturn buf.String(), nil\n}\n```"]}, "mox-message/part.go-EnsurePart": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/message/part.go:\n```\npackage message\n\n// todo: allow more invalid content-type values, we now stop parsing on: empty media type (eg \"content-type: ; name=...\"), empty value for property (eg \"charset=\", missing quotes for characters that should be quoted (eg boundary containing \"=\" but without quotes), duplicate properties (two charsets), empty pairs (eg \"text/html;;\").\n// todo: should we be forgiving when closing boundary in multipart message is missing? seems like spam messages do this...\n// todo: should we allow base64 messages where a line starts with a space? and possibly more whitespace. is happening in messages. coreutils base64 accepts it, encoding/base64 does not.\n// todo: handle comments in headers?\n// todo: should we just always store messages with \\n instead of \\r\\n? \\r\\n seems easier for use with imap.\n// todo: can use a cleanup\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"mime/quotedprintable\"\n\t\"net/mail\"\n\t\"net/textproto\"\n\t\"strings\"\n\t\"time\"\n\n\t\"golang.org/x/text/encoding/ianaindex\"\n\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// Pedantic enables stricter parsing.\nvar Pedantic bool\n\nvar (\n\tErrBadContentType = errors.New(\"bad content-type\")\n)\n\nvar (\n\terrNotMultipart           = errors.New(\"not a multipart message\")\n\terrFirstBoundCloses       = errors.New(\"first boundary cannot be finishing boundary\")\n\terrLineTooLong            = errors.New(\"line too long\")\n\terrMissingBoundaryParam   = errors.New(\"missing/empty boundary content-type parameter\")\n\terrMissingClosingBoundary = errors.New(\"eof without closing boundary\")\n\terrBareLF                 = errors.New(\"invalid bare line feed\")\n\terrBareCR                 = errors.New(\"invalid bare carriage return\")\n\terrUnexpectedEOF          = errors.New(\"unexpected eof\")\n)\n\n// If set, during tests, attempts to reparse a part will cause an error, because sequentially reading parts should not lead to reparsing.\nvar enforceSequential bool\n\n// Part represents a whole mail message, or a part of a multipart message. It\n// is designed to handle IMAP requirements efficiently.\ntype Part struct {\n\tBoundaryOffset int64 // Offset in message where bound starts. -1 for top-level message.\n\tHeaderOffset   int64 // Offset in message file where header starts.\n\tBodyOffset     int64 // Offset in message file where body starts.\n\tEndOffset      int64 // Where body of part ends. Set when part is fully read.\n\tRawLineCount   int64 // Number of lines in raw, undecoded, body of part. Set when part is fully read.\n\tDecodedSize    int64 // Number of octets when decoded. If this is a text mediatype, lines ending only in LF are changed end in CRLF and DecodedSize reflects that.\n\n\tMediaType               string            // From Content-Type, upper case. E.g. \"TEXT\". Can be empty because content-type may be absent. In this case, the part may be treated as TEXT/PLAIN.\n\tMediaSubType            string            // From Content-Type, upper case. E.g. \"PLAIN\".\n\tContentTypeParams       map[string]string // E.g. holds \"boundary\" for multipart messages. Has lower-case keys, and original case values.\n\tContentID               string\n\tContentDescription      string\n\tContentTransferEncoding string    // In upper case.\n\tEnvelope                *Envelope // Email message headers. Not for non-message parts.\n\n\tParts []Part // Parts if this is a multipart.\n\n\t// Only for message/rfc822 and message/global. This part may have a buffer as\n\t// backing io.ReaderAt, because a message/global can have a non-identity\n\t// content-transfer-encoding. This part has a nil parent.\n\tMessage *Part\n\n\tr               io.ReaderAt\n\theader          textproto.MIMEHeader // Parsed header.\n\tnextBoundOffset int64                // If >= 0, the offset where the next part header starts. We can set this when a user fully reads each part.\n\tlastBoundOffset int64                // Start of header of last/previous part. Used to skip a part if ParseNextPart is called and nextBoundOffset is -1.\n\tparent          *Part                // Parent part, for getting bound from, and setting nextBoundOffset when a part has finished reading. Only for subparts, not top-level parts.\n\tbound           []byte               // Only set if valid multipart with boundary, includes leading --, excludes \\r\\n.\n\tstrict          bool                 // If set, valid crlf line endings are verified when reading body.\n}\n\n// todo: have all Content* fields in Part?\n// todo: make Address contain a type Localpart and dns.Domain?\n// todo: if we ever make a major change and reparse all parts, switch to lower-case values if not too troublesome.\n\n// Envelope holds the basic/common message headers as used in IMAP4.\ntype Envelope struct {\n\tDate      time.Time\n\tSubject   string // Q/B-word-decoded.\n\tFrom      []Address\n\tSender    []Address\n\tReplyTo   []Address\n\tTo        []Address\n\tCC        []Address\n\tBCC       []Address\n\tInReplyTo string // From In-Reply-To header, includes <>.\n\tMessageID string // From Message-Id header, includes <>.\n}\n\n// Address as used in From and To headers.\ntype Address struct {\n\tName string // Free-form name for display in mail applications.\n\tUser string // Localpart, encoded as string. Must be parsed before using as Localpart.\n\tHost string // Domain in ASCII.\n}\n\n// Parse reads the headers of the mail message and returns a part.\n// A part provides access to decoded and raw contents of a message and its multiple parts.\n//\n// If strict is set, fewer attempts are made to continue parsing when errors are\n// encountered, such as with invalid content-type headers or bare carriage returns.\nfunc Parse(elog *slog.Logger, strict bool, r io.ReaderAt) (Part, error) {\n\tlog := mlog.New(\"message\", elog)\n\treturn newPart(log, strict, r, 0, nil)\n}\n\n// EnsurePart parses a part as with Parse, but ensures a usable part is always\n// returned, even if error is non-nil. If a parse error occurs, the message is\n// returned as application/octet-stream, and headers can still be read if they\n// were valid.\n//\n// If strict is set, fewer attempts are made to continue parsing when errors are\n// encountered, such as with invalid content-type headers or bare carriage returns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc fallbackPart(p Part, r io.ReaderAt, size int64) (Part, error) {\n\tnp := Part{\n\t\tHeaderOffset:            p.HeaderOffset,\n\t\tBodyOffset:              p.BodyOffset,\n\t\tEndOffset:               size,\n\t\tMediaType:               \"APPLICATION\",\n\t\tMediaSubType:            \"OCTET-STREAM\",\n\t\tContentTypeParams:       p.ContentTypeParams,\n\t\tContentID:               p.ContentID,\n\t\tContentDescription:      p.ContentDescription,\n\t\tContentTransferEncoding: p.ContentTransferEncoding,\n\t\tEnvelope:                p.Envelope,\n\t\t// We don't keep:\n\t\t//   - BoundaryOffset: irrelevant for top-level message.\n\t\t//   - RawLineCount and DecodedSize: set below.\n\t\t//   - Parts: we are not treating this as a multipart message.\n\t}\n\tnp.SetReaderAt(r)\n\t// By reading body, the number of lines and decoded size will be set.\n\t_, err := io.Copy(io.Discard, np.Reader())\n\treturn np, err\n}\n\n// SetReaderAt sets r as reader for this part and all its sub parts, recursively.\n// No reader is set for any Message subpart, see SetMessageReaderAt.\nfunc (p *Part) SetReaderAt(r io.ReaderAt) {\n\tif r == nil {\n\t\tpanic(\"nil reader\")\n\t}\n\tp.r = r\n\tfor i := range p.Parts {\n\t\tpp := &p.Parts[i]\n\t\tpp.SetReaderAt(r)\n\t}\n}\n\n// SetMessageReaderAt sets a reader on p.Message, which must be non-nil.\nfunc (p *Part) SetMessageReaderAt() error {\n\t// todo: if p.Message does not contain any non-identity content-transfer-encoding, we should set an offsetReader of p.Message, recursively.\n\tbuf, err := io.ReadAll(p.Reader())\n\tif err != nil {\n\t\treturn err\n\t}\n\tp.Message.SetReaderAt(bytes.NewReader(buf))\n\treturn nil\n}\n\n// Walk through message, decoding along the way, and collecting mime part offsets and sizes, and line counts.\nfunc (p *Part) Walk(elog *slog.Logger, parent *Part) error {\n\tlog := mlog.New(\"message\", elog)\n\n\tif len(p.bound) == 0 {\n\t\tif p.MediaType == \"MESSAGE\" && (p.MediaSubType == \"RFC822\" || p.MediaSubType == \"GLOBAL\") {\n\t\t\t// todo: don't read whole submessage in memory...\n\t\t\tbuf, err := io.ReadAll(p.Reader())\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tbr := bytes.NewReader(buf)\n\t\t\tmp, err := Parse(log.Logger, p.strict, br)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"parsing embedded message: %w\", err)\n\t\t\t}\n\t\t\tif err := mp.Walk(log.Logger, nil); err != nil {\n\t\t\t\t// If this is a DSN and we are not in pedantic mode, accept unexpected end of\n\t\t\t\t// message. This is quite common because MTA's sometimes just truncate the original\n\t\t\t\t// message in a place that makes the message invalid.\n\t\t\t\tif errors.Is(err, errUnexpectedEOF) && !Pedantic && parent != nil && len(parent.Parts) >= 3 && p == &parent.Parts[2] && parent.MediaType == \"MULTIPART\" && parent.MediaSubType == \"REPORT\" {\n\t\t\t\t\tmp, err = fallbackPart(mp, br, int64(len(buf)))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn fmt.Errorf(\"parsing invalid embedded message: %w\", err)\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\treturn fmt.Errorf(\"parsing parts of embedded message: %w\", err)\n\t\t\t\t}\n\t\t\t}\n\t\t\t// todo: if mp does not contain any non-identity content-transfer-encoding, we should set an offsetReader of p.r on mp, recursively.\n\t\t\tp.Message = &mp\n\t\t\treturn nil\n\t\t}\n\t\t_, err := io.Copy(io.Discard, p.Reader())\n\t\treturn err\n\t}\n\n\tfor {\n\t\tpp, err := p.ParseNextPart(log.Logger)\n\t\tif err == io.EOF {\n\t\t\treturn nil\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := pp.Walk(log.Logger, p); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n}\n\n// String returns a debugging representation of the part.\nfunc (p *Part) String() string {\n\treturn fmt.Sprintf(\"&Part{%s/%s offsets %d/%d/%d/%d lines %d decodedsize %d next %d last %d bound %q parts %v}\", p.MediaType, p.MediaSubType, p.BoundaryOffset, p.HeaderOffset, p.BodyOffset, p.EndOffset, p.RawLineCount, p.DecodedSize, p.nextBoundOffset, p.lastBoundOffset, p.bound, p.Parts)\n}\n\n// newPart parses a new part, which can be the top-level message.\n// offset is the bound offset for parts, and the start of message for top-level messages. parent indicates if this is a top-level message or sub-part.\n// If an error occurs, p's exported values can still be relevant. EnsurePart uses these values.\nfunc newPart(log mlog.Log, strict bool, r io.ReaderAt, offset int64, parent *Part) (p Part, rerr error) {\n\tif r == nil {\n\t\tpanic(\"nil reader\")\n\t}\n\tp = Part{\n\t\tBoundaryOffset: -1,\n\t\tEndOffset:      -1,\n\t\tr:              r,\n\t\tparent:         parent,\n\t\tstrict:         strict,\n\t}\n\n\tb := &bufAt{strict: strict, r: r, offset: offset}\n\n\tif parent != nil {\n\t\tp.BoundaryOffset = offset\n\t\tif line, _, err := b.ReadLine(true); err != nil {\n\t\t\treturn p, err\n\t\t} else if match, finish := checkBound(line, parent.bound); !match {\n\t\t\treturn p, fmt.Errorf(\"missing bound\")\n\t\t} else if finish {\n\t\t\treturn p, fmt.Errorf(\"new part for closing boundary\")\n\t\t}\n\t}\n\n\t// Collect header.\n\tp.HeaderOffset = b.offset\n\tp.BodyOffset = b.offset\n\thb := &bytes.Buffer{}\n\tfor {\n\t\tline, _, err := b.ReadLine(true)\n\t\tif err == io.EOF {\n\t\t\t// No body is valid.\n\t\t\tbreak\n\t\t}\n\t\tif err != nil {\n\t\t\treturn p, fmt.Errorf(\"reading header line: %w\", err)\n\t\t}\n\t\thb.Write(line)\n\t\tif len(line) == 2 {\n\t\t\tbreak // crlf\n\t\t}\n\t}\n\tp.BodyOffset = b.offset\n\n\t// Don't attempt to parse empty header, mail.ReadMessage doesn't like it.\n\tif p.HeaderOffset == p.BodyOffset {\n\t\tp.header = textproto.MIMEHeader{}\n\t} else {\n\t\th, err := parseHeader(hb)\n\t\tif err != nil {\n\t\t\treturn p, fmt.Errorf(\"parsing header: %w\", err)\n\t\t}\n\t\tp.header = h\n\t}\n\n\tct := p.header.Get(\"Content-Type\")\n\tmt, params, err := mime.ParseMediaType(ct)\n\tif err != nil && ct != \"\" {\n\t\tif Pedantic || strict {\n\t\t\treturn p, fmt.Errorf(\"%w: %s: %q\", ErrBadContentType, err, ct)\n\t\t}\n\n\t\t// Try parsing just a content-type, ignoring parameters.\n\t\t// ../rfc/2045:628\n\t\tct = strings.TrimSpace(strings.SplitN(ct, \";\", 2)[0])\n\t\tt := strings.SplitN(ct, \"/\", 2)\n\t\tisToken := func(s string) bool {\n\t\t\tconst separators = `()<>@,;:\\\\\"/[]?= ` // ../rfc/2045:663\n\t\t\tfor _, c := range s {\n\t\t\t\tif c < 0x20 || c >= 0x80 || strings.ContainsRune(separators, c) {\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn len(s) > 0\n\t\t}\n\t\t// We cannot recover content-type of multipart, we won't have a boundary.\n\t\tif len(t) == 2 && isToken(t[0]) && !strings.EqualFold(t[0], \"multipart\") && isToken(t[1]) {\n\t\t\tp.MediaType = strings.ToUpper(t[0])\n\t\t\tp.MediaSubType = strings.ToUpper(t[1])\n\t\t} else {\n\t\t\tp.MediaType = \"APPLICATION\"\n\t\t\tp.MediaSubType = \"OCTET-STREAM\"\n\t\t}\n\t\tlog.Debugx(\"malformed content-type, attempting to recover and continuing\", err,\n\t\t\tslog.String(\"contenttype\", p.header.Get(\"Content-Type\")),\n\t\t\tslog.String(\"mediatype\", p.MediaType),\n\t\t\tslog.String(\"mediasubtype\", p.MediaSubType))\n\t} else if mt != \"\" {\n\t\tt := strings.SplitN(strings.ToUpper(mt), \"/\", 2)\n\t\tif len(t) != 2 {\n\t\t\tif Pedantic || strict {\n\t\t\t\treturn p, fmt.Errorf(\"bad content-type: %q (content-type %q)\", mt, ct)\n\t\t\t}\n\t\t\tlog.Debug(\"malformed media-type, ignoring and continuing\", slog.String(\"type\", mt))\n\t\t\tp.MediaType = \"APPLICATION\"\n\t\t\tp.MediaSubType = \"OCTET-STREAM\"\n\t\t} else {\n\t\t\tp.MediaType = t[0]\n\t\t\tp.MediaSubType = t[1]\n\t\t\tp.ContentTypeParams = params\n\t\t}\n\t}\n\n\tp.ContentID = p.header.Get(\"Content-Id\")\n\tp.ContentDescription = p.header.Get(\"Content-Description\")\n\tp.ContentTransferEncoding = strings.ToUpper(p.header.Get(\"Content-Transfer-Encoding\"))\n\n\tif parent == nil {\n\t\tp.Envelope, err = parseEnvelope(log, mail.Header(p.header))\n\t\tif err != nil {\n\t\t\treturn p, err\n\t\t}\n\t}\n\n\tif p.MediaType == \"MULTIPART\" {\n\t\ts := params[\"boundary\"]\n\t\tif s == \"\" {\n\t\t\treturn p, errMissingBoundaryParam\n\t\t}\n\t\tp.bound = append([]byte(\"--\"), s...)\n\n\t\t// Discard preamble, before first boundary.\n\t\tfor {\n\t\t\tline, _, err := b.PeekLine(true)\n\t\t\tif err != nil {\n\t\t\t\treturn p, fmt.Errorf(\"parsing line for part preamble: %w\", err)\n\t\t\t}\n\t\t\t// Line only needs boundary prefix, not exact match. ../rfc/2046:1103\n\t\t\t// Well, for compatibility, we require whitespace after the boundary. Because some\n\t\t\t// software use the same boundary but with text appended for sub parts.\n\t\t\tif match, finish := checkBound(line, p.bound); match {\n\t\t\t\tif finish {\n\t\t\t\t\treturn p, errFirstBoundCloses\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tb.ReadLine(true)\n\t\t}\n\t\tp.nextBoundOffset = b.offset\n\t\tp.lastBoundOffset = b.offset\n\t}\n\n\treturn p, nil\n}\n\n// Header returns the parsed header of this part.\nfunc (p *Part) Header() (textproto.MIMEHeader, error) {\n\tif p.header != nil {\n\t\treturn p.header, nil\n\t}\n\tif p.HeaderOffset == p.BodyOffset {\n\t\tp.header = textproto.MIMEHeader{}\n\t\treturn p.header, nil\n\t}\n\th, err := parseHeader(p.HeaderReader())\n\tp.header = h\n\treturn h, err\n}\n\n// HeaderReader returns a reader for the header section of this part, including ending bare CRLF.\nfunc (p *Part) HeaderReader() io.Reader {\n\treturn io.NewSectionReader(p.r, p.HeaderOffset, p.BodyOffset-p.HeaderOffset)\n}\n\n// parse a header, only call this on non-empty input (even though that is a valid header).\nfunc parseHeader(r io.Reader) (textproto.MIMEHeader, error) {\n\t// We read using mail.ReadMessage instead of textproto.ReadMIMEHeaders because the\n\t// first handles email messages properly, while the second only works for HTTP\n\t// headers.\n\tvar zero textproto.MIMEHeader\n\n\t// We read the header and add the optional \\r\\n header/body separator. If the \\r\\n\n\t// is missing, parsing with Go <1.21 results in an EOF error.\n\t// todo: directly parse from reader r when Go 1.20 is no longer supported.\n\tbuf, err := io.ReadAll(r)\n\tif err != nil {\n\t\treturn zero, err\n\t}\n\tif bytes.HasSuffix(buf, []byte(\"\\r\\n\")) && !bytes.HasSuffix(buf, []byte(\"\\r\\n\\r\\n\")) {\n\t\tbuf = append(buf, \"\\r\\n\"...)\n\t}\n\tmsg, err := mail.ReadMessage(bytes.NewReader(buf))\n\tif err != nil {\n\t\treturn zero, err\n\t}\n\treturn textproto.MIMEHeader(msg.Header), nil\n}\n\nvar wordDecoder = mime.WordDecoder{\n\tCharsetReader: func(charset string, r io.Reader) (io.Reader, error) {\n\t\tswitch strings.ToLower(charset) {\n\t\tcase \"\", \"us-ascii\", \"utf-8\":\n\t\t\treturn r, nil\n\t\t}\n\t\tenc, _ := ianaindex.MIME.Encoding(charset)\n\t\tif enc == nil {\n\t\t\tenc, _ = ianaindex.IANA.Encoding(charset)\n\t\t}\n\t\tif enc == nil {\n\t\t\treturn r, fmt.Errorf(\"unknown charset %q\", charset)\n\t\t}\n\t\treturn enc.NewDecoder().Reader(r), nil\n\t},\n}\n\nfunc parseEnvelope(log mlog.Log, h mail.Header) (*Envelope, error) {\n\tdate, _ := h.Date()\n\n\t// We currently marshal this field to JSON. But JSON cannot represent all\n\t// time.Time. Time zone of 24:00 was seen in the wild. We won't try for extreme\n\t// years, but we can readjust timezones.\n\t// todo: remove this once we no longer store using json.\n\t_, offset := date.Zone()\n\tif date.Year() > 9999 {\n\t\tdate = time.Time{}\n\t} else if offset <= -24*3600 || offset >= 24*3600 {\n\t\tdate = time.Unix(date.Unix(), 0).UTC()\n\t}\n\n\tsubject := h.Get(\"Subject\")\n\tif s, err := wordDecoder.DecodeHeader(subject); err == nil {\n\t\tsubject = s\n\t}\n\n\tenv := &Envelope{\n\t\tdate,\n\t\tsubject,\n\t\tparseAddressList(log, h, \"from\"),\n\t\tparseAddressList(log, h, \"sender\"),\n\t\tparseAddressList(log, h, \"reply-to\"),\n\t\tparseAddressList(log, h, \"to\"),\n\t\tparseAddressList(log, h, \"cc\"),\n\t\tparseAddressList(log, h, \"bcc\"),\n\t\th.Get(\"In-Reply-To\"),\n\t\th.Get(\"Message-Id\"),\n\t}\n\treturn env, nil\n}\n\nfunc parseAddressList(log mlog.Log, h mail.Header, k string) []Address {\n\t// todo: possibly work around ios mail generating incorrect q-encoded \"phrases\" with unencoded double quotes? ../rfc/2047:382\n\tl, err := h.AddressList(k)\n\tif err != nil {\n\t\treturn nil\n\t}\n\tvar r []Address\n\tfor _, a := range l {\n\t\t// todo: parse more fully according to ../rfc/5322:959\n\t\tvar user, host string\n\t\taddr, err := smtp.ParseAddress(a.Address)\n\t\tif err != nil {\n\t\t\tlog.Infox(\"parsing address (continuing)\", err, slog.Any(\"address\", a.Address))\n\t\t} else {\n\t\t\tuser = addr.Localpart.String()\n\t\t\thost = addr.Domain.ASCII\n\t\t}\n\t\tr = append(r, Address{a.Name, user, host})\n\t}\n\treturn r\n}\n\n// ParseNextPart parses the next (sub)part of this multipart message.\n// ParseNextPart returns io.EOF and a nil part when there are no more parts.\n// Only used for initial parsing of message. Once parsed, use p.Parts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// IsDSN returns whether the MIME structure of the part is a DSN.\nfunc (p *Part) IsDSN() bool {\n\treturn p.MediaType == \"MULTIPART\" &&\n\t\tp.MediaSubType == \"REPORT\" &&\n\t\tlen(p.Parts) >= 2 &&\n\t\tp.Parts[1].MediaType == \"MESSAGE\" &&\n\t\t(p.Parts[1].MediaSubType == \"DELIVERY-STATUS\" || p.Parts[1].MediaSubType == \"GLOBAL-DELIVERY-STATUS\")\n}\n\n// Reader returns a reader for the decoded body content.\nfunc (p *Part) Reader() io.Reader {\n\treturn p.bodyReader(p.RawReader())\n}\n\n// ReaderUTF8OrBinary returns a reader for the decoded body content, transformed to\n// utf-8 for known mime/iana encodings (only if they aren't us-ascii or utf-8\n// already). For unknown or missing character sets/encodings, the original reader\n// is returned.\nfunc (p *Part) ReaderUTF8OrBinary() io.Reader {\n\treturn DecodeReader(p.ContentType\n```\n# Function Signature: func EnsurePart(elog *slog.Logger, strict bool, r io.ReaderAt, size int64) (Part, error) {\n# Natural Language Annotations: // EnsurePart parses a part as with Parse, but ensures a usable part is always\n// returned, even if error is non-nil. If a parse error occurs, the message is\n// returned as application/octet-stream, and headers can still be read if they\n// were valid.\n//\n// If strict is set, fewer attempts are made to continue parsing when errors are\n// encountered, such as with invalid content-type headers or bare carriage returns.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc EnsurePart(elog *slog.Logger, strict bool, r io.ReaderAt, size int64) (Part, error) {\n\tlog := mlog.New(\"message\", elog)\n\treturn newPart(log, strict, r, 0, nil)\n}\n```", "  ```\nfunc EnsurePart(elog *slog.Logger, strict bool, r io.ReaderAt, size int64) (Part, error) {\n\tlog := mlog.New(\"message\", elog)\n\tp, err := newPart(log, strict, r, 0, nil)\n\tif p.BoundaryOffset == -1 {\n\t\t_, err = io.Copy(io.Discard, p.RawReader())\n\t\tp.MediaType = \"APPLICATION\"\n\t\tp.MediaSubType = \"OCTET-STREAM\"\n\t\tp.EndOffset = size\n\t}\n\treturn p, err\n}\n```", "  ```\nfunc EnsurePart(elog *slog.Logger, strict bool, r io.ReaderAt, size int64) (Part, error) {\n\tlog := mlog.New(\"message\", elog)\n\tp, err := Parse(log.Logger, strict, r)\n\tif err != nil {\n\t\treturn fallbackPart(p, r, size)\n\t}\n\treturn p, nil\n}\n\nfunc fallbackPart(p Part, r io.ReaderAt, size int64) (Part, error) {\n\tnp := Part{\n\t\tHeaderOffset:            p.HeaderOffset,\n\t\tBodyOffset:              p.BodyOffset,\n\t\tEndOffset:               size,\n\t\tMediaType:               \"APPLICATION\",\n\t\tMediaSubType:            \"OCTET-STREAM\",\n\t\tContentTypeParams:       p.ContentTypeParams,\n\t\tContentID:               p.ContentID,\n\t\tContentDescription:      p.ContentDescription,\n\t\tContentTransferEncoding: p.ContentTransferEncoding,\n\t\tEnvelope:                p.Envelope,\n\t\t// We don't keep:\n\t\t//   - BoundaryOffset: irrelevant for top-level message.\n\t\t//   - RawLineCount and DecodedSize: set below.\n\t\t//   - Parts: we are not treating this as a multipart message.\n\t}\n\tif size < 0 {\n\t\t// If size is not known, we cannot read parts into memory, and can only read header and content-type.\n\t\tnp.BodyOffset = np.EndOffset\n\t\t// We need to set these to make sure we don't keep asking for raw body.\n\t\tnp.RawLineCount = -1\n\t\tnp.DecodedSize = -1\n\t\tbuf := bytes.NewBuffer(make([]byte, 0, DefaultBufferSize))\n\t\tnp.SetReaderAt(io.TeeReader(r, buf))\n\t\t_ = buf.Truncate(0) // Set buffer so we can read it again.\n\t} else {\n\t\tnp.SetReaderAt(r)\n\t}\n\t_, err := io.Copy(io.Discard, np.Reader())\n\treturn np, err\n}\n```"]}, "mox-message/part.go-ParseNextPart": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/message/part.go:\n```\npackage message\n\n// todo: allow more invalid content-type values, we now stop parsing on: empty media type (eg \"content-type: ; name=...\"), empty value for property (eg \"charset=\", missing quotes for characters that should be quoted (eg boundary containing \"=\" but without quotes), duplicate properties (two charsets), empty pairs (eg \"text/html;;\").\n// todo: should we be forgiving when closing boundary in multipart message is missing? seems like spam messages do this...\n// todo: should we allow base64 messages where a line starts with a space? and possibly more whitespace. is happening in messages. coreutils base64 accepts it, encoding/base64 does not.\n// todo: handle comments in headers?\n// todo: should we just always store messages with \\n instead of \\r\\n? \\r\\n seems easier for use with imap.\n// todo: can use a cleanup\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"mime/quotedprintable\"\n\t\"net/mail\"\n\t\"net/textproto\"\n\t\"strings\"\n\t\"time\"\n\n\t\"golang.org/x/text/encoding/ianaindex\"\n\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// Pedantic enables stricter parsing.\nvar Pedantic bool\n\nvar (\n\tErrBadContentType = errors.New(\"bad content-type\")\n)\n\nvar (\n\terrNotMultipart           = errors.New(\"not a multipart message\")\n\terrFirstBoundCloses       = errors.New(\"first boundary cannot be finishing boundary\")\n\terrLineTooLong            = errors.New(\"line too long\")\n\terrMissingBoundaryParam   = errors.New(\"missing/empty boundary content-type parameter\")\n\terrMissingClosingBoundary = errors.New(\"eof without closing boundary\")\n\terrBareLF                 = errors.New(\"invalid bare line feed\")\n\terrBareCR                 = errors.New(\"invalid bare carriage return\")\n\terrUnexpectedEOF          = errors.New(\"unexpected eof\")\n)\n\n// If set, during tests, attempts to reparse a part will cause an error, because sequentially reading parts should not lead to reparsing.\nvar enforceSequential bool\n\n// Part represents a whole mail message, or a part of a multipart message. It\n// is designed to handle IMAP requirements efficiently.\ntype Part struct {\n\tBoundaryOffset int64 // Offset in message where bound starts. -1 for top-level message.\n\tHeaderOffset   int64 // Offset in message file where header starts.\n\tBodyOffset     int64 // Offset in message file where body starts.\n\tEndOffset      int64 // Where body of part ends. Set when part is fully read.\n\tRawLineCount   int64 // Number of lines in raw, undecoded, body of part. Set when part is fully read.\n\tDecodedSize    int64 // Number of octets when decoded. If this is a text mediatype, lines ending only in LF are changed end in CRLF and DecodedSize reflects that.\n\n\tMediaType               string            // From Content-Type, upper case. E.g. \"TEXT\". Can be empty because content-type may be absent. In this case, the part may be treated as TEXT/PLAIN.\n\tMediaSubType            string            // From Content-Type, upper case. E.g. \"PLAIN\".\n\tContentTypeParams       map[string]string // E.g. holds \"boundary\" for multipart messages. Has lower-case keys, and original case values.\n\tContentID               string\n\tContentDescription      string\n\tContentTransferEncoding string    // In upper case.\n\tEnvelope                *Envelope // Email message headers. Not for non-message parts.\n\n\tParts []Part // Parts if this is a multipart.\n\n\t// Only for message/rfc822 and message/global. This part may have a buffer as\n\t// backing io.ReaderAt, because a message/global can have a non-identity\n\t// content-transfer-encoding. This part has a nil parent.\n\tMessage *Part\n\n\tr               io.ReaderAt\n\theader          textproto.MIMEHeader // Parsed header.\n\tnextBoundOffset int64                // If >= 0, the offset where the next part header starts. We can set this when a user fully reads each part.\n\tlastBoundOffset int64                // Start of header of last/previous part. Used to skip a part if ParseNextPart is called and nextBoundOffset is -1.\n\tparent          *Part                // Parent part, for getting bound from, and setting nextBoundOffset when a part has finished reading. Only for subparts, not top-level parts.\n\tbound           []byte               // Only set if valid multipart with boundary, includes leading --, excludes \\r\\n.\n\tstrict          bool                 // If set, valid crlf line endings are verified when reading body.\n}\n\n// todo: have all Content* fields in Part?\n// todo: make Address contain a type Localpart and dns.Domain?\n// todo: if we ever make a major change and reparse all parts, switch to lower-case values if not too troublesome.\n\n// Envelope holds the basic/common message headers as used in IMAP4.\ntype Envelope struct {\n\tDate      time.Time\n\tSubject   string // Q/B-word-decoded.\n\tFrom      []Address\n\tSender    []Address\n\tReplyTo   []Address\n\tTo        []Address\n\tCC        []Address\n\tBCC       []Address\n\tInReplyTo string // From In-Reply-To header, includes <>.\n\tMessageID string // From Message-Id header, includes <>.\n}\n\n// Address as used in From and To headers.\ntype Address struct {\n\tName string // Free-form name for display in mail applications.\n\tUser string // Localpart, encoded as string. Must be parsed before using as Localpart.\n\tHost string // Domain in ASCII.\n}\n\n// Parse reads the headers of the mail message and returns a part.\n// A part provides access to decoded and raw contents of a message and its multiple parts.\n//\n// If strict is set, fewer attempts are made to continue parsing when errors are\n// encountered, such as with invalid content-type headers or bare carriage returns.\nfunc Parse(elog *slog.Logger, strict bool, r io.ReaderAt) (Part, error) {\n\tlog := mlog.New(\"message\", elog)\n\treturn newPart(log, strict, r, 0, nil)\n}\n\n// EnsurePart parses a part as with Parse, but ensures a usable part is always\n// returned, even if error is non-nil. If a parse error occurs, the message is\n// returned as application/octet-stream, and headers can still be read if they\n// were valid.\n//\n// If strict is set, fewer attempts are made to continue parsing when errors are\n// encountered, such as with invalid content-type headers or bare carriage returns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc fallbackPart(p Part, r io.ReaderAt, size int64) (Part, error) {\n\tnp := Part{\n\t\tHeaderOffset:            p.HeaderOffset,\n\t\tBodyOffset:              p.BodyOffset,\n\t\tEndOffset:               size,\n\t\tMediaType:               \"APPLICATION\",\n\t\tMediaSubType:            \"OCTET-STREAM\",\n\t\tContentTypeParams:       p.ContentTypeParams,\n\t\tContentID:               p.ContentID,\n\t\tContentDescription:      p.ContentDescription,\n\t\tContentTransferEncoding: p.ContentTransferEncoding,\n\t\tEnvelope:                p.Envelope,\n\t\t// We don't keep:\n\t\t//   - BoundaryOffset: irrelevant for top-level message.\n\t\t//   - RawLineCount and DecodedSize: set below.\n\t\t//   - Parts: we are not treating this as a multipart message.\n\t}\n\tnp.SetReaderAt(r)\n\t// By reading body, the number of lines and decoded size will be set.\n\t_, err := io.Copy(io.Discard, np.Reader())\n\treturn np, err\n}\n\n// SetReaderAt sets r as reader for this part and all its sub parts, recursively.\n// No reader is set for any Message subpart, see SetMessageReaderAt.\nfunc (p *Part) SetReaderAt(r io.ReaderAt) {\n\tif r == nil {\n\t\tpanic(\"nil reader\")\n\t}\n\tp.r = r\n\tfor i := range p.Parts {\n\t\tpp := &p.Parts[i]\n\t\tpp.SetReaderAt(r)\n\t}\n}\n\n// SetMessageReaderAt sets a reader on p.Message, which must be non-nil.\nfunc (p *Part) SetMessageReaderAt() error {\n\t// todo: if p.Message does not contain any non-identity content-transfer-encoding, we should set an offsetReader of p.Message, recursively.\n\tbuf, err := io.ReadAll(p.Reader())\n\tif err != nil {\n\t\treturn err\n\t}\n\tp.Message.SetReaderAt(bytes.NewReader(buf))\n\treturn nil\n}\n\n// Walk through message, decoding along the way, and collecting mime part offsets and sizes, and line counts.\nfunc (p *Part) Walk(elog *slog.Logger, parent *Part) error {\n\tlog := mlog.New(\"message\", elog)\n\n\tif len(p.bound) == 0 {\n\t\tif p.MediaType == \"MESSAGE\" && (p.MediaSubType == \"RFC822\" || p.MediaSubType == \"GLOBAL\") {\n\t\t\t// todo: don't read whole submessage in memory...\n\t\t\tbuf, err := io.ReadAll(p.Reader())\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tbr := bytes.NewReader(buf)\n\t\t\tmp, err := Parse(log.Logger, p.strict, br)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"parsing embedded message: %w\", err)\n\t\t\t}\n\t\t\tif err := mp.Walk(log.Logger, nil); err != nil {\n\t\t\t\t// If this is a DSN and we are not in pedantic mode, accept unexpected end of\n\t\t\t\t// message. This is quite common because MTA's sometimes just truncate the original\n\t\t\t\t// message in a place that makes the message invalid.\n\t\t\t\tif errors.Is(err, errUnexpectedEOF) && !Pedantic && parent != nil && len(parent.Parts) >= 3 && p == &parent.Parts[2] && parent.MediaType == \"MULTIPART\" && parent.MediaSubType == \"REPORT\" {\n\t\t\t\t\tmp, err = fallbackPart(mp, br, int64(len(buf)))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn fmt.Errorf(\"parsing invalid embedded message: %w\", err)\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\treturn fmt.Errorf(\"parsing parts of embedded message: %w\", err)\n\t\t\t\t}\n\t\t\t}\n\t\t\t// todo: if mp does not contain any non-identity content-transfer-encoding, we should set an offsetReader of p.r on mp, recursively.\n\t\t\tp.Message = &mp\n\t\t\treturn nil\n\t\t}\n\t\t_, err := io.Copy(io.Discard, p.Reader())\n\t\treturn err\n\t}\n\n\tfor {\n\t\tpp, err := p.ParseNextPart(log.Logger)\n\t\tif err == io.EOF {\n\t\t\treturn nil\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := pp.Walk(log.Logger, p); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n}\n\n// String returns a debugging representation of the part.\nfunc (p *Part) String() string {\n\treturn fmt.Sprintf(\"&Part{%s/%s offsets %d/%d/%d/%d lines %d decodedsize %d next %d last %d bound %q parts %v}\", p.MediaType, p.MediaSubType, p.BoundaryOffset, p.HeaderOffset, p.BodyOffset, p.EndOffset, p.RawLineCount, p.DecodedSize, p.nextBoundOffset, p.lastBoundOffset, p.bound, p.Parts)\n}\n\n// newPart parses a new part, which can be the top-level message.\n// offset is the bound offset for parts, and the start of message for top-level messages. parent indicates if this is a top-level message or sub-part.\n// If an error occurs, p's exported values can still be relevant. EnsurePart uses these values.\nfunc newPart(log mlog.Log, strict bool, r io.ReaderAt, offset int64, parent *Part) (p Part, rerr error) {\n\tif r == nil {\n\t\tpanic(\"nil reader\")\n\t}\n\tp = Part{\n\t\tBoundaryOffset: -1,\n\t\tEndOffset:      -1,\n\t\tr:              r,\n\t\tparent:         parent,\n\t\tstrict:         strict,\n\t}\n\n\tb := &bufAt{strict: strict, r: r, offset: offset}\n\n\tif parent != nil {\n\t\tp.BoundaryOffset = offset\n\t\tif line, _, err := b.ReadLine(true); err != nil {\n\t\t\treturn p, err\n\t\t} else if match, finish := checkBound(line, parent.bound); !match {\n\t\t\treturn p, fmt.Errorf(\"missing bound\")\n\t\t} else if finish {\n\t\t\treturn p, fmt.Errorf(\"new part for closing boundary\")\n\t\t}\n\t}\n\n\t// Collect header.\n\tp.HeaderOffset = b.offset\n\tp.BodyOffset = b.offset\n\thb := &bytes.Buffer{}\n\tfor {\n\t\tline, _, err := b.ReadLine(true)\n\t\tif err == io.EOF {\n\t\t\t// No body is valid.\n\t\t\tbreak\n\t\t}\n\t\tif err != nil {\n\t\t\treturn p, fmt.Errorf(\"reading header line: %w\", err)\n\t\t}\n\t\thb.Write(line)\n\t\tif len(line) == 2 {\n\t\t\tbreak // crlf\n\t\t}\n\t}\n\tp.BodyOffset = b.offset\n\n\t// Don't attempt to parse empty header, mail.ReadMessage doesn't like it.\n\tif p.HeaderOffset == p.BodyOffset {\n\t\tp.header = textproto.MIMEHeader{}\n\t} else {\n\t\th, err := parseHeader(hb)\n\t\tif err != nil {\n\t\t\treturn p, fmt.Errorf(\"parsing header: %w\", err)\n\t\t}\n\t\tp.header = h\n\t}\n\n\tct := p.header.Get(\"Content-Type\")\n\tmt, params, err := mime.ParseMediaType(ct)\n\tif err != nil && ct != \"\" {\n\t\tif Pedantic || strict {\n\t\t\treturn p, fmt.Errorf(\"%w: %s: %q\", ErrBadContentType, err, ct)\n\t\t}\n\n\t\t// Try parsing just a content-type, ignoring parameters.\n\t\t// ../rfc/2045:628\n\t\tct = strings.TrimSpace(strings.SplitN(ct, \";\", 2)[0])\n\t\tt := strings.SplitN(ct, \"/\", 2)\n\t\tisToken := func(s string) bool {\n\t\t\tconst separators = `()<>@,;:\\\\\"/[]?= ` // ../rfc/2045:663\n\t\t\tfor _, c := range s {\n\t\t\t\tif c < 0x20 || c >= 0x80 || strings.ContainsRune(separators, c) {\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn len(s) > 0\n\t\t}\n\t\t// We cannot recover content-type of multipart, we won't have a boundary.\n\t\tif len(t) == 2 && isToken(t[0]) && !strings.EqualFold(t[0], \"multipart\") && isToken(t[1]) {\n\t\t\tp.MediaType = strings.ToUpper(t[0])\n\t\t\tp.MediaSubType = strings.ToUpper(t[1])\n\t\t} else {\n\t\t\tp.MediaType = \"APPLICATION\"\n\t\t\tp.MediaSubType = \"OCTET-STREAM\"\n\t\t}\n\t\tlog.Debugx(\"malformed content-type, attempting to recover and continuing\", err,\n\t\t\tslog.String(\"contenttype\", p.header.Get(\"Content-Type\")),\n\t\t\tslog.String(\"mediatype\", p.MediaType),\n\t\t\tslog.String(\"mediasubtype\", p.MediaSubType))\n\t} else if mt != \"\" {\n\t\tt := strings.SplitN(strings.ToUpper(mt), \"/\", 2)\n\t\tif len(t) != 2 {\n\t\t\tif Pedantic || strict {\n\t\t\t\treturn p, fmt.Errorf(\"bad content-type: %q (content-type %q)\", mt, ct)\n\t\t\t}\n\t\t\tlog.Debug(\"malformed media-type, ignoring and continuing\", slog.String(\"type\", mt))\n\t\t\tp.MediaType = \"APPLICATION\"\n\t\t\tp.MediaSubType = \"OCTET-STREAM\"\n\t\t} else {\n\t\t\tp.MediaType = t[0]\n\t\t\tp.MediaSubType = t[1]\n\t\t\tp.ContentTypeParams = params\n\t\t}\n\t}\n\n\tp.ContentID = p.header.Get(\"Content-Id\")\n\tp.ContentDescription = p.header.Get(\"Content-Description\")\n\tp.ContentTransferEncoding = strings.ToUpper(p.header.Get(\"Content-Transfer-Encoding\"))\n\n\tif parent == nil {\n\t\tp.Envelope, err = parseEnvelope(log, mail.Header(p.header))\n\t\tif err != nil {\n\t\t\treturn p, err\n\t\t}\n\t}\n\n\tif p.MediaType == \"MULTIPART\" {\n\t\ts := params[\"boundary\"]\n\t\tif s == \"\" {\n\t\t\treturn p, errMissingBoundaryParam\n\t\t}\n\t\tp.bound = append([]byte(\"--\"), s...)\n\n\t\t// Discard preamble, before first boundary.\n\t\tfor {\n\t\t\tline, _, err := b.PeekLine(true)\n\t\t\tif err != nil {\n\t\t\t\treturn p, fmt.Errorf(\"parsing line for part preamble: %w\", err)\n\t\t\t}\n\t\t\t// Line only needs boundary prefix, not exact match. ../rfc/2046:1103\n\t\t\t// Well, for compatibility, we require whitespace after the boundary. Because some\n\t\t\t// software use the same boundary but with text appended for sub parts.\n\t\t\tif match, finish := checkBound(line, p.bound); match {\n\t\t\t\tif finish {\n\t\t\t\t\treturn p, errFirstBoundCloses\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tb.ReadLine(true)\n\t\t}\n\t\tp.nextBoundOffset = b.offset\n\t\tp.lastBoundOffset = b.offset\n\t}\n\n\treturn p, nil\n}\n\n// Header returns the parsed header of this part.\nfunc (p *Part) Header() (textproto.MIMEHeader, error) {\n\tif p.header != nil {\n\t\treturn p.header, nil\n\t}\n\tif p.HeaderOffset == p.BodyOffset {\n\t\tp.header = textproto.MIMEHeader{}\n\t\treturn p.header, nil\n\t}\n\th, err := parseHeader(p.HeaderReader())\n\tp.header = h\n\treturn h, err\n}\n\n// HeaderReader returns a reader for the header section of this part, including ending bare CRLF.\nfunc (p *Part) HeaderReader() io.Reader {\n\treturn io.NewSectionReader(p.r, p.HeaderOffset, p.BodyOffset-p.HeaderOffset)\n}\n\n// parse a header, only call this on non-empty input (even though that is a valid header).\nfunc parseHeader(r io.Reader) (textproto.MIMEHeader, error) {\n\t// We read using mail.ReadMessage instead of textproto.ReadMIMEHeaders because the\n\t// first handles email messages properly, while the second only works for HTTP\n\t// headers.\n\tvar zero textproto.MIMEHeader\n\n\t// We read the header and add the optional \\r\\n header/body separator. If the \\r\\n\n\t// is missing, parsing with Go <1.21 results in an EOF error.\n\t// todo: directly parse from reader r when Go 1.20 is no longer supported.\n\tbuf, err := io.ReadAll(r)\n\tif err != nil {\n\t\treturn zero, err\n\t}\n\tif bytes.HasSuffix(buf, []byte(\"\\r\\n\")) && !bytes.HasSuffix(buf, []byte(\"\\r\\n\\r\\n\")) {\n\t\tbuf = append(buf, \"\\r\\n\"...)\n\t}\n\tmsg, err := mail.ReadMessage(bytes.NewReader(buf))\n\tif err != nil {\n\t\treturn zero, err\n\t}\n\treturn textproto.MIMEHeader(msg.Header), nil\n}\n\nvar wordDecoder = mime.WordDecoder{\n\tCharsetReader: func(charset string, r io.Reader) (io.Reader, error) {\n\t\tswitch strings.ToLower(charset) {\n\t\tcase \"\", \"us-ascii\", \"utf-8\":\n\t\t\treturn r, nil\n\t\t}\n\t\tenc, _ := ianaindex.MIME.Encoding(charset)\n\t\tif enc == nil {\n\t\t\tenc, _ = ianaindex.IANA.Encoding(charset)\n\t\t}\n\t\tif enc == nil {\n\t\t\treturn r, fmt.Errorf(\"unknown charset %q\", charset)\n\t\t}\n\t\treturn enc.NewDecoder().Reader(r), nil\n\t},\n}\n\nfunc parseEnvelope(log mlog.Log, h mail.Header) (*Envelope, error) {\n\tdate, _ := h.Date()\n\n\t// We currently marshal this field to JSON. But JSON cannot represent all\n\t// time.Time. Time zone of 24:00 was seen in the wild. We won't try for extreme\n\t// years, but we can readjust timezones.\n\t// todo: remove this once we no longer store using json.\n\t_, offset := date.Zone()\n\tif date.Year() > 9999 {\n\t\tdate = time.Time{}\n\t} else if offset <= -24*3600 || offset >= 24*3600 {\n\t\tdate = time.Unix(date.Unix(), 0).UTC()\n\t}\n\n\tsubject := h.Get(\"Subject\")\n\tif s, err := wordDecoder.DecodeHeader(subject); err == nil {\n\t\tsubject = s\n\t}\n\n\tenv := &Envelope{\n\t\tdate,\n\t\tsubject,\n\t\tparseAddressList(log, h, \"from\"),\n\t\tparseAddressList(log, h, \"sender\"),\n\t\tparseAddressList(log, h, \"reply-to\"),\n\t\tparseAddressList(log, h, \"to\"),\n\t\tparseAddressList(log, h, \"cc\"),\n\t\tparseAddressList(log, h, \"bcc\"),\n\t\th.Get(\"In-Reply-To\"),\n\t\th.Get(\"Message-Id\"),\n\t}\n\treturn env, nil\n}\n\nfunc parseAddressList(log mlog.Log, h mail.Header, k string) []Address {\n\t// todo: possibly work around ios mail generating incorrect q-encoded \"phrases\" with unencoded double quotes? ../rfc/2047:382\n\tl, err := h.AddressList(k)\n\tif err != nil {\n\t\treturn nil\n\t}\n\tvar r []Address\n\tfor _, a := range l {\n\t\t// todo: parse more fully according to ../rfc/5322:959\n\t\tvar user, host string\n\t\taddr, err := smtp.ParseAddress(a.Address)\n\t\tif err != nil {\n\t\t\tlog.Infox(\"parsing address (continuing)\", err, slog.Any(\"address\", a.Address))\n\t\t} else {\n\t\t\tuser = addr.Localpart.String()\n\t\t\thost = addr.Domain.ASCII\n\t\t}\n\t\tr = append(r, Address{a.Name, user, host})\n\t}\n\treturn r\n}\n\n// ParseNextPart parses the next (sub)part of this multipart message.\n// ParseNextPart returns io.EOF and a nil part when there are no more parts.\n// Only used for initial parsing of message. Once parsed, use p.Parts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// IsDSN returns whether the MIME structure of the part is a DSN.\nfunc (p *Part) IsDSN() bool {\n\treturn p.MediaType == \"MULTIPART\" &&\n\t\tp.MediaSubType == \"REPORT\" &&\n\t\tlen(p.Parts) >= 2 &&\n\t\tp.Parts[1].MediaType == \"MESSAGE\" &&\n\t\t(p.Parts[1].MediaSubType == \"DELIVERY-STATUS\" || p.Parts[1].MediaSubType == \"GLOBAL-DELIVERY-STATUS\")\n}\n\n// Reader returns a reader for the decoded body content.\nfunc (p *Part) Reader() io.Reader {\n\treturn p.bodyReader(p.RawReader())\n}\n\n// ReaderUTF8OrBinary returns a reader for the decoded body content, transformed to\n// utf-8 for known mime/iana encodings (only if they aren't us-ascii or utf-8\n// already). For unknown or missing character sets/encodings, the original reader\n// is returned.\nfunc (p *Part) ReaderUTF8OrBinary() io.Reader {\n\treturn DecodeReader(p.ContentType\n```\n# Function Signature: func (p *Part) ParseNextPart(elog *slog.Logger) (*Part, error) {\n# Natural Language Annotations: // ParseNextPart parses the next (sub)part of this multipart message.\n// ParseNextPart returns io.EOF and a nil part when there are no more parts.\n// Only used for initial parsing of message. Once parsed, use p.Parts.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (p *Part) ParseNextPart(elog *slog.Logger) (*Part, error) {\n\tlog := mlog.New(\"message\", elog)\n\n\tif len(p.bound) == 0 {\n\t\treturn nil, errNotMultipart\n\t}\n\n\tp.lastBoundOffset = p.nextBoundOffset\n\n\tline, _, err := p.r.ReadLine(true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Line only needs boundary prefix, not exact match. See ../rfc/2046:1103\n\tmatch, finish := checkBound(line, p.bound)\n\tif match {\n\t\tif finish {\n\t\t\treturn nil, io.EOF // Reached closing boundary.\n\t\t}\n\t\tp.nextBoundOffset = p.r.offset\n\t\treturn nil, errNotMultipart // Reached another boundary.\n\t}\n\n\t// Running another part. Reset reader if needed.\n\tif p.r.offset != p.BodyOffset {\n\t\tp.r.offset = p.BodyOffset\n\t}\n\treturn newPart(log, p.strict, p.r, p.r.offset, p)\n}\n```", "  ```\nfunc (p *Part) ParseNextPart(elog *slog.Logger) (*Part, error) {\n\tlog := mlog.New(\"message\", elog)\n\n\tif p.BoundaryOffset < 0 {\n\t\treturn nil, io.EOF\n\t}\n\tif next := p.nextBoundOffset; next >= 0 {\n\t\tp.nextBoundOffset = -1\n\t\tsize, err := p.r.Seek(0, io.SeekEnd)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tb := &bufAt{strict: p.strict, r: p.r, offset: next, size: size}\n\t\tif line, _, err := b.ReadLine(true); err != nil {\n\t\t\treturn nil, err\n\t\t} else if line[0] == '-' && line[1] == '-' && bytes.Equal(line[2:], p.bound) {\n\t\t\tnext = b.offset\n\t\t\tlog.Debug(\"closing multipart detected and skipping\")\n\t\t} else {\n\t\t\treturn nil, fmt.Errorf(\"bounds not in expected place\")\n\t\t}\n\t}\n\n\tfor {\n\t\tline, _, err := p.bodyReader(p.RawReader()).ReadLine()\n\t\tif err == io.EOF {\n\t\t\treturn nil, io.EOF\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"reading boundary line: %w\", err)\n\t\t}\n\t\tif match, finish := checkBound(line, p.bound); match {\n\t\t\tif finish {\n\t\t\t\tp.nextBoundOffset = b.offset\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tlog.Debug(\"skipping over extra boundary\")\n\t\t\tcontinue\n\t\t}\n\t\tbreak\n\t}\n\n\tnp, err := newPart(log, p.strict, p.r, p.BodyOffset, p)\n\tif err != nil && enforceSequential {\n\t\treturn nil, err\n\t}\n\tp.BodyOffset = b.offset\n\tp.LastBoundOffset = np.BoundaryOffset\n\tp.Parts = append(p.Parts, np)\n\treturn &np, nil\n}\n```", "  ```go\nfunc (p *Part) ParseNextPart(elog *slog.Logger) (*Part, error) {\n\tlog := mlog.New(\"message\", elog)\n\tif p.nextBoundOffset == -1 {\n\t\treturn nil, io.EOF\n\t}\n\tlog.Debug(\"parsing next part\", slog.Int64(\"boundary offset\", p.nextBoundOffset))\n\n\t// todo: handle case where part in the extras from EmbeddedMessage is garbage?\n\n\tvar err error\n\tnext, err := newPart(log, p.strict, p.r, p.nextBoundOffset, p)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tnext.parent = p\n\tnext.nextBoundOffset = -1\n\t// If this was not the closing boundary, skip the ending CRLF after\n\t// the boundary before we change our nextBoundOffset.\n\tif bytes.HasSuffix(next.bound, []byte(\"--\")) {\n\t\tp.nextBoundOffset = next.EndOffset\n\t} else {\n\t\tseg, err := bufio.NewReader(p.r.Reader(next.EndOffset, -1)).ReadSlice('\\n')\n\t\tif err != nil {\n\t\t\tlog.Warnx(\"error reading slice when skipping next boundary\", err)\n\t\t\tp.nextBoundOffset = next.EndOffset\n\t\t} else {\n\t\t\tp.nextBoundOffset = next.EndOffset + int64(len(seg))\n\t\t}\n\t}\n\n\tp.Parts = append(p.Parts, next)\n\treturn &p.Parts[len(p.Parts)-1], nil\n}\n```"]}, "mox-message/messageid.go-MessageIDCanonical": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/message/messageid.go:\n```\npackage message\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/smtp\"\n)\n\nvar errBadMessageID = errors.New(\"not a message-id\")\n\n// MessageIDCanonical parses the Message-ID, returning a canonical value that is\n// lower-cased, without <>, and no unneeded quoting. For matching in threading,\n// with References/In-Reply-To. If the message-id is invalid (e.g. no <>), an error\n// is returned. If the message-id could not be parsed as address (localpart \"@\"\n// domain), the raw value and the bool return parameter true is returned. It is\n// quite common that message-id's don't adhere to the localpart @ domain\n// syntax.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func MessageIDCanonical(s string) (string, bool, error) {\n# Natural Language Annotations: // MessageIDCanonical parses the Message-ID, returning a canonical value that is\n// lower-cased, without <>, and no unneeded quoting. For matching in threading,\n// with References/In-Reply-To. If the message-id is invalid (e.g. no <>), an error\n// is returned. If the message-id could not be parsed as address (localpart \"@\"\n// domain), the raw value and the bool return parameter true is returned. It is\n// quite common that message-id's don't adhere to the localpart @ domain\n// syntax.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\npackage message\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// MessageIDCanonical parses the Message-ID, returning a canonical value that is\n// lower-cased, without <>, and no unneeded quoting. For matching in threading,\n// with References/In-Reply-To. If the message-id is invalid (e.g. no <>), an error\n// is returned. If the message-id could not be parsed as address (localpart \"@\"\n// domain), the raw value and the bool return parameter true is returned. It is\n// quite common that message-id's don't adhere to the localpart @ domain\n// syntax.\nfunc MessageIDCanonical(s string) (string, bool, error) {\n\t// If input is invalid, return error\n\tif !strings.HasPrefix(s, \"<\") || !strings.HasSuffix(s, \">\") {\n\t\treturn \"\", true, errors.New(\"not a message-id\")\n\t}\n\t// Remove angle brackets\n\ts = s[1 : len(s)-1]\n\t// Lowercase\n\ts = strings.ToLower(s)\n\t// Check for unneeded quoting\n\tif strings.HasPrefix(s, '\"') && strings.HasSuffix(s, '\"') {\n\t\ts = s[1 : len(s)-1]\n\t}\n\t// Try parsing as address\n\t_, err := smtp.ParseAddress(s)\n\tif err != nil {\n\t\treturn s, true, err\n\t}\n\t// Return parsed value\n\treturn s, false, nil\n}\n```", "  ```\npackage message\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/smtp\"\n)\n\nvar errBadMessageID = errors.New(\"not a message-id\")\n\nfunc MessageIDCanonical(s string) (string, bool, error) {\n\t// Check if the input is a valid message id\n\tif !strings.HasPrefix(s, \"<\") || !strings.HasSuffix(s, \">\") {\n\t\treturn \"\", true, errBadMessageID\n\t}\n\n\t// Remove <> from the input\n\ts = s[1 : len(s)-1]\n\n\t// Lower-case the input\n\ts = strings.ToLower(s)\n\n\t// Check if the input is a valid address\n\tisAddress, err := checkAddress(s)\n\tif err != nil {\n\t\treturn \"\", true, err\n\t}\n\n\t// If the input is not an address, return the raw value and true\n\tif !isAddress {\n\t\treturn s, true, nil\n\t}\n\n\t// Parse the address and return the canonical form\n\taddr, err := smtp.ParseAddress(s)\n\tif err != nil {\n\t\treturn \"\", true, err\n\t}\n\treturn fmt.Sprintf(\"%s@%s\", strings.ToLower(addr.LocalPart), strings.ToLower(addr.Domain)), false, nil\n}\n\n// checkAddress checks if the input is a valid address\nfunc checkAddress(s string) (bool, error) {\n\tlocalPart, domain, err := splitAddress(s)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\tif localPart == \"\" || domain == \"\" {\n\t\treturn false, nil\n\t}\n\treturn true, nil\n}\n\n// splitAddress splits the address into the local part and the domain\nfunc splitAddress(s string) (localPart, domain string, err error) {\n\tparts := strings.Split(s, \"@\")\n\tif len(parts) != 2 {\n\t\treturn \"\", \"\", errors.New(\"not an address\")\n\t}\n\treturn parts[0], parts[1], nil\n}\n```", "  ```\nfunc MessageIDCanonical(s string) (string, bool, error) {\n\t// Split the input on '<' and '>' to get the local part and domain\n\tparts := strings.Split(s, \"<\")\n\tif len(parts) != 2 {\n\t\treturn \"\", false, errBadMessageID\n\t}\n\tlocalpart := parts[0]\n\tdomain := strings.TrimPrefix(parts[1], \"@\")\n\n\t// Check if the local part is empty\n\tif localpart == \"\" {\n\t\treturn \"\", true, nil\n\t}\n\n\t// Check if the domain is empty\n\tif domain == \"\" {\n\t\treturn \"\", false, nil\n\t}\n\n\t// Convert the local part and domain to lowercase\n\tlocalpart = strings.ToLower(localpart)\n\tdomain = strings.ToLower(domain)\n\n\t// Remove any unneeded quoting\n\tlocalpart = strings.Trim(localpart, `\"'`)\n\n\t// Return the canonical message-ID\n\treturn fmt.Sprintf(\"%s <%s>@%s\", localpart, domain), false, nil\n}\n```\nPlease note that this code is a guess based on the information provided, and may not be the exact implementation used in the original code."]}, "mox-message/decode.go-DecodeReader": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/message/decode.go:\n```\npackage message\n\nimport (\n\t\"io\"\n\t\"strings\"\n\n\t\"golang.org/x/text/encoding/ianaindex\"\n)\n\n// DecodeReader returns a reader that reads from r, decoding as charset. If\n// charset is empty, us-ascii, utf-8 or unknown, the original reader is\n// returned and no decoding takes place.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func DecodeReader(charset string, r io.Reader) io.Reader {\n# Natural Language Annotations: // DecodeReader returns a reader that reads from r, decoding as charset. If\n// charset is empty, us-ascii, utf-8 or unknown, the original reader is\n// returned and no decoding takes place.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\npackage message\n\nimport (\n\t\"io\"\n\t\"strings\"\n\n\t\"golang.org/x/text/encoding/ianaindex\"\n)\n\n// DecodeReader returns a reader that reads from r, decoding as charset. If\n// charset is empty, us-ascii, utf-8 or unknown, the original reader is\n// returned and no decoding takes place.\nfunc DecodeReader(charset string, r io.Reader) io.Reader {\n\tif charset == \"\" || strings.EqualFold(charset, \"us-ascii\") || strings.EqualFold(charset, \"utf-8\") {\n\t\treturn r\n\t}\n\n\tencoding, err := ianaindex.IANA.Encoding(charset)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\n\treturn encoding.NewDecoder().Reader(r)\n}\n```", "  ```\npackage message\n\nimport (\n\t\"io\"\n\t\"strings\"\n\n\t\"golang.org/x/text/encoding/ianaindex\"\n)\n\nfunc DecodeReader(charset string, r io.Reader) io.Reader {\n\tswitch strings.ToLower(charset) {\n\tcase \"\", \"us-ascii\", \"utf-8\", \"unknown\":\n\t\treturn r\n\tdefault:\n\t\treturn &decodeReader{charset: charset, r: r}\n\t}\n}\n\ntype decodeReader struct {\n\tcharset string\n\tr       io.Reader\n}\n\nfunc (r *decodeReader) Read(p []byte) (n int, err error) {\n\tn, err = r.r.Read(p)\n\tif err == nil || err == io.EOF {\n\t\tvar i ianaindex.IANA\n\t\ti, err = ianaindex.IANA.Get(strings.ToLower(r.charset))\n\t\tif err != nil {\n\t\t\treturn n, err\n\t\t}\n\t\ti.NewDecoder().Bytes(p)\n\t}\n\treturn n, err\n}\n\n```\nThis code implements the DecodeReader function that takes a charset and an io.Reader and returns an io.Reader that reads from the original reader and decodes the input as the specified charset. If the charset is empty, us-ascii, utf-8, or unknown, the original reader is returned and no decoding takes place. The implementation uses the x/text/encoding/ianaindex package to encode and decode the charset. The decodeReader struct wraps the original reader and implements the Read method to decode the input based on the charset.", "  ```\npackage message\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"io\"\n\t\"strings\"\n\n\t\"golang.org/x/text/encoding/ianaindex\"\n)\n\nfunc DecodeReader(charset string, r io.Reader) io.Reader {\n\tif charset == \"\" || charset == \"us-ascii\" || charset == \"utf-8\" || charset == \"unknown\" {\n\t\treturn r\n\t}\n\n\tdec, err := ianaindex.IANA.Encoding(charset)\n\tif err != nil {\n\t\treturn r\n\t}\n\n\treturn io.TeeReader(r, NewCharsetReader(dec, r))\n}\n\nfunc NewCharsetReader(c *encoding.Encoding, r io.Reader) io.Reader {\n\tvar wbuf bytes.Buffer\n\te := &encoder{c: c, err: nil, wbuf: &wbuf, fallback: true}\n\treturn e.transform(r)\n}\n\ntype encoder struct {\n\tc       *encoding.Encoding\n\terr     error\n\twbuf    *bytes.Buffer\n\tfallback bool\n}\n\nfunc (e *encoder) transform(r io.Reader) io.Reader {\n\tif e.err != nil {\n\t\treturn r\n\t}\n\tif e.c == nil {\n\t\treturn e.wbuf\n\t}\n\tbr := bufio.NewReader(r)\n\tvar err error\n\te.err = nil\n\tif e.c.UnicodeOnly() {\n\t\terr = e.c.NewDecoder().Transform(e.wbuf, br)\n\t} else {\n\t\terr = e.c.NewDecoder().Transform(e.wbuf, br)\n\t}\n\tif err == nil {\n\t\terr = e.c.NewDecoder().Transform(e.wbuf, br)\n\t}\n\tif err != nil {\n\t\te.err = err\n\t}\n\treturn io.MultiReader(e.wbuf, r)\n}\n```\nThis function is designed to take in a character set and a reader, and return a new reader that decodes the reader based on the given character set. The function will check if the character set is empty or the reader is not needed for decoding, if so, it will return the original reader. If the character set is valid, it will create a new reader that decodes the original reader using the given character set. If there is an error during decoding, it will return the original reader with no decoding."]}, "mox-message/referencedids.go-ReferencedIDs": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/message/referencedids.go:\n```\npackage message\n\nimport (\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// ReferencedIDs returns the Message-IDs referenced from the References header(s),\n// with a fallback to the In-Reply-To header(s). The ids are canonicalized for\n// thread-matching, like with MessageIDCanonical. Empty message-id's are skipped.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func ReferencedIDs(references []string, inReplyTo []string) ([]string, error) {\n# Natural Language Annotations: // ReferencedIDs returns the Message-IDs referenced from the References header(s),\n// with a fallback to the In-Reply-To header(s). The ids are canonicalized for\n// thread-matching, like with MessageIDCanonical. Empty message-id's are skipped.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\npackage message\n\nimport (\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// ReferencedIDs returns the Message-IDs referenced from the References header(s),\n// with a fallback to the In-Reply-To header(s). The ids are canonicalized for\n// thread-matching, like with MessageIDCanonical. Empty message-id's are skipped.\nfunc ReferencedIDs(references []string, inReplyTo []string) ([]string, error) {\n\tvar ids []string\n\n\tfor _, ref := range references {\n\t\tfor _, id := range strings.Split(ref, \" \") {\n\t\t\tif strings.HasPrefix(id, \"<\") && strings.HasSuffix(id, \">\") {\n\t\t\t\tid = id[1 : len(id)-1]\n\t\t\t}\n\t\t\tif id != \"\" {\n\t\t\t\tids = append(ids, MessageIDCanonical(id))\n\t\t\t}\n\t\t}\n\t}\n\n\tif len(ids) == 0 {\n\t\tfor _, id := range inReplyTo {\n\t\t\tif strings.HasPrefix(id, \"<\") && strings.HasSuffix(id, \">\") {\n\t\t\t\tid = id[1 : len(id)-1]\n\t\t\t}\n\t\t\tif id != \"\" {\n\t\t\t\tids = append(ids, MessageIDCanonical(id))\n\t\t\t}\n\t\t}\n\t}\n\n\treturn ids, nil\n}\n\n```", "  ```\npackage message\n\nimport (\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// ReferencedIDs returns the Message-IDs referenced from the References header(s),\n// with a fallback to the In-Reply-To header(s). The ids are canonicalized for\n// thread-matching, like with MessageIDCanonical. Empty message-id's are skipped.\nfunc ReferencedIDs(references []string, inReplyTo []string) ([]string, error) {\n\t// Canonicalize the References headers\n\tcanonicalReferences := make([]string, 0)\n\tfor _, r := range references {\n\t\tif r != \"\" {\n\t\t\tcanonicalReferences = append(canonicalReferences, MessageIDCanonical(r))\n\t\t}\n\t}\n\n\t// Canonicalize the In-Reply-To headers\n\tcanonicalInReplyTo := make([]string, 0)\n\tfor _, r := range inReplyTo {\n\t\tif r != \"\" {\n\t\t\tcanonicalInReplyTo = append(canonicalInReplyTo, MessageIDCanonical(r))\n\t\t}\n\t}\n\n\t// Combine the references and in reply to headers\n\tallReferences := append(canonicalReferences, canonicalInReplyTo...)\n\n\t// Deduplicate the list of references\n\tdeduped := map[string]bool{}\n\tfor _, r := range allReferences {\n\t\tdeduped[r] = true\n\t}\n\tresult := make([]string, 0, len(deduped))\n\tfor k := range deduped {\n\t\tresult = append(result, k)\n\t}\n\n\treturn result, nil\n}\n```", "  ```\npackage message\n\nimport (\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// ReferencedIDs returns the Message-IDs referenced from the References header(s),\n// with a fallback to the In-Reply-To header(s). The ids are canonicalized for\n// thread-matching, like with MessageIDCanonical. Empty message-id's are skipped.\nfunc ReferencedIDs(references []string, inReplyTo []string) ([]string, error) {\n\tvar ids []string\n\n\t// get the Message-IDs from the References header(s)\n\tfor _, reference := range references {\n\t\t// trim leading and trailing whitespace\n\t\treference = strings.TrimSpace(reference)\n\t\t// split on whitespace\n\t\tparts := strings.Split(reference, \" \")\n\t\t// iterate over the parts and only keep those that are valid Message-IDs\n\t\tfor _, part := range parts {\n\t\t\t// check if the part is a valid Message-ID\n\t\t\tif smtp.IsValidMessageID(part) {\n\t\t\t\t// add the canonicalized Message-ID to the list\n\t\t\t\tids = append(ids, smtp.MessageIDCanonical(part))\n\t\t\t}\n\t\t}\n\t}\n\n\t// if no Message-IDs were found in the References header(s), check the In-Reply-To header(s)\n\tif len(ids) == 0 {\n\t\tfor _, inReplyTo := range inReplyTo {\n\t\t\t// trim leading and trailing whitespace\n\t\t\tinReplyTo = strings.TrimSpace(inReplyTo)\n\t\t\t// split on whitespace\n\t\t\tparts := strings.Split(inReplyTo, \" \")\n\t\t\t// iterate over the parts and only keep those that are valid Message-IDs\n\t\t\tfor _, part := range parts {\n\t\t\t\t// check if the part is a valid Message-ID\n\t\t\t\tif smtp.IsValidMessageID(part) {\n\t\t\t\t\t// add the canonicalized Message-ID to the list\n\t\t\t\t\tids = append(ids, smtp.MessageIDCanonical(part))\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t// return an error if no Message-IDs were found\n\tif len(ids) == 0 {\n\t\treturn nil, errors.New(\"could not find any Message-IDs in the References or In-Reply-To header\")\n\t}\n\n\treturn ids, nil\n}\n```\nIt's important to note that this function assumes that the headers are already parsed and cleaned, and that the input is a list of strings corresponding to the References and In-Reply-To headers. If the input is a single string, it should be split on newline chars before being passed to this function."]}, "mox-message/authresults.go-ParseAuthResults": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/message/authresults.go:\n```\npackage message\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/dns\"\n)\n\n// ../rfc/8601:577\n\n// Authentication-Results header, see RFC 8601.\ntype AuthResults struct {\n\tHostname string\n\t// Optional version of Authentication-Results header, assumed \"1\" when absent,\n\t// which is common.\n\tVersion string\n\tComment string       // If not empty, header comment without \"()\", added after Hostname.\n\tMethods []AuthMethod // Can be empty, in case of \"none\".\n}\n\n// ../rfc/8601:598\n\n// AuthMethod is a result for one authentication method.\n//\n// Example encoding in the header: \"spf=pass smtp.mailfrom=example.net\".\ntype AuthMethod struct {\n\t// E.g. \"dkim\", \"spf\", \"iprev\", \"auth\".\n\tMethod  string\n\tVersion string // For optional method version. \"1\" is implied when missing, which is common.\n\tResult  string // Each method has a set of known values, e.g. \"pass\", \"temperror\", etc.\n\tComment string // Optional, message header comment.\n\tReason  string // Optional.\n\tProps   []AuthProp\n}\n\n// ../rfc/8601:606\n\n// AuthProp describes properties for an authentication method.\n// Each method has a set of known properties.\n// Encoded in the header as \"type.property=value\", e.g. \"smtp.mailfrom=example.net\"\n// for spf.\ntype AuthProp struct {\n\t// Valid values maintained at https://www.iana.org/assignments/email-auth/email-auth.xhtml\n\tType     string\n\tProperty string\n\tValue    string\n\t// Whether value is address-like (localpart@domain, or domain). Or another value,\n\t// which is subject to escaping.\n\tIsAddrLike bool\n\tComment    string // If not empty, header comment without \"()\", added after Value.\n}\n\n// MakeAuthProp is a convenient way to make an AuthProp.\nfunc MakeAuthProp(typ, property, value string, isAddrLike bool, Comment string) AuthProp {\n\treturn AuthProp{typ, property, value, isAddrLike, Comment}\n}\n\n// todo future: we could store fields as dns.Domain, and when we encode as non-ascii also add the ascii version as a comment.\n\n// Header returns an Authentication-Results header, possibly spanning multiple\n// lines, always ending in crlf.\nfunc (h AuthResults) Header() string {\n\t// Escaping of values: ../rfc/8601:684 ../rfc/2045:661\n\n\toptComment := func(s string) string {\n\t\tif s != \"\" {\n\t\t\treturn \" (\" + s + \")\"\n\t\t}\n\t\treturn s\n\t}\n\n\tw := &HeaderWriter{}\n\tw.Add(\"\", \"Authentication-Results:\"+optComment(h.Comment)+\" \"+value(h.Hostname, false)+\";\")\n\tfor i, m := range h.Methods {\n\t\tw.Newline()\n\n\t\ttokens := []string{}\n\t\taddf := func(format string, args ...any) {\n\t\t\ts := fmt.Sprintf(format, args...)\n\t\t\ttokens = append(tokens, s)\n\t\t}\n\t\taddf(\"%s=%s\", m.Method, m.Result)\n\t\tif m.Comment != \"\" && (m.Reason != \"\" || len(m.Props) > 0) {\n\t\t\taddf(\"(%s)\", m.Comment)\n\t\t}\n\t\tif m.Reason != \"\" {\n\t\t\taddf(\"reason=%s\", value(m.Reason, false))\n\t\t}\n\t\tfor _, p := range m.Props {\n\t\t\tv := value(p.Value, p.IsAddrLike)\n\t\t\taddf(\"%s.%s=%s%s\", p.Type, p.Property, v, optComment(p.Comment))\n\t\t}\n\t\tfor j, t := range tokens {\n\t\t\tvar sep string\n\t\t\tif j > 0 {\n\t\t\t\tsep = \" \"\n\t\t\t}\n\t\t\tif j == len(tokens)-1 && i < len(h.Methods)-1 {\n\t\t\t\tt += \";\"\n\t\t\t}\n\t\t\tw.Add(sep, t)\n\t\t}\n\t}\n\treturn w.String()\n}\n\nfunc value(s string, isAddrLike bool) string {\n\tquote := s == \"\"\n\tfor _, c := range s {\n\t\t// utf-8 does not have to be quoted. ../rfc/6532:242\n\t\t// Characters outside of tokens do. ../rfc/2045:661\n\t\tif c <= ' ' || c == 0x7f || (c == '@' && !isAddrLike) || strings.ContainsRune(`()<>,;:\\\\\"/[]?= `, c) {\n\t\t\tquote = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif !quote {\n\t\treturn s\n\t}\n\tr := `\"`\n\tfor _, c := range s {\n\t\tif c == '\"' || c == '\\\\' {\n\t\t\tr += \"\\\\\"\n\t\t}\n\t\tr += string(c)\n\t}\n\tr += `\"`\n\treturn r\n}\n\n// ParseAuthResults parses a Authentication-Results header value.\n//\n// Comments are not populated in the returned AuthResults.\n// Both crlf and lf line-endings are accepted. The input string must end with\n// either crlf or lf.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype parser struct {\n\ts     string\n\tlower string // Like s, but with ascii characters lower-cased (utf-8 offsets preserved).\n\to     int\n}\n\ntype parseError struct{ err error }\n\nfunc (p *parser) recover(err *error) {\n\tx := recover()\n\tif x == nil {\n\t\treturn\n\t}\n\tperr, ok := x.(parseError)\n\tif ok {\n\t\t*err = perr.err\n\t\treturn\n\t}\n\tpanic(x)\n}\n\nfunc (p *parser) xerrorf(format string, args ...any) {\n\tpanic(parseError{fmt.Errorf(format, args...)})\n}\n\nfunc (p *parser) end() bool {\n\treturn p.s[p.o:] == \"\\r\\n\" || p.s[p.o:] == \"\\n\"\n}\n\n// ../rfc/5322:599\nfunc (p *parser) cfws() {\n\tp.fws()\n\tfor p.prefix(\"(\") {\n\t\tp.xcomment()\n\t}\n\tp.fws()\n}\n\nfunc (p *parser) fws() {\n\tfor p.take(\" \") || p.take(\"\\t\") {\n\t}\n\topts := []string{\"\\n \", \"\\n\\t\", \"\\r\\n \", \"\\r\\n\\t\"}\n\tfor _, o := range opts {\n\t\tif p.take(o) {\n\t\t\tbreak\n\t\t}\n\t}\n\tfor p.take(\" \") || p.take(\"\\t\") {\n\t}\n}\n\nfunc (p *parser) xcomment() {\n\tp.xtake(\"(\")\n\tp.fws()\n\tfor !p.take(\")\") {\n\t\tif p.empty() {\n\t\t\tp.xerrorf(\"unexpected end in comment\")\n\t\t}\n\t\tif p.prefix(\"(\") {\n\t\t\tp.xcomment()\n\t\t\tp.fws()\n\t\t\tcontinue\n\t\t}\n\t\tp.take(`\\`)\n\t\tif c := p.s[p.o]; c > ' ' && c < 0x7f {\n\t\t\tp.o++\n\t\t} else {\n\t\t\tp.xerrorf(\"bad character %c in comment\", c)\n\t\t}\n\t\tp.fws()\n\t}\n}\n\nfunc (p *parser) prefix(s string) bool {\n\treturn strings.HasPrefix(p.lower[p.o:], s)\n}\n\nfunc (p *parser) xvalue() string {\n\tif p.prefix(`\"`) {\n\t\treturn p.xquotedString()\n\t}\n\treturn p.xtakefn1(\"value token\", func(c rune, i int) bool {\n\t\t// ../rfc/2045:661\n\t\t// todo: token cannot contain utf-8? not updated in ../rfc/6532. however, we also use it for the localpart & domain parsing, so we'll allow it.\n\t\treturn c > ' ' && !strings.ContainsRune(`()<>@,;:\\\\\"/[]?= `, c)\n\t})\n}\n\nfunc (p *parser) xchar() rune {\n\t// We are careful to track invalid utf-8 properly.\n\tif p.empty() {\n\t\tp.xerrorf(\"need another character\")\n\t}\n\tvar r rune\n\tvar o int\n\tfor i, c := range p.s[p.o:] {\n\t\tif i > 0 {\n\t\t\to = i\n\t\t\tbreak\n\t\t}\n\t\tr = c\n\t}\n\tif o == 0 {\n\t\tp.o = len(p.s)\n\t} else {\n\t\tp.o += o\n\t}\n\treturn r\n}\n\nfunc (p *parser) xquotedString() string {\n\tp.xtake(`\"`)\n\tvar s string\n\tvar esc bool\n\tfor {\n\t\tc := p.xchar()\n\t\tif esc {\n\t\t\tif c >= ' ' && c < 0x7f {\n\t\t\t\ts += string(c)\n\t\t\t\tesc = false\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tp.xerrorf(\"bad escaped char %c in quoted string\", c)\n\t\t}\n\t\tif c == '\\\\' {\n\t\t\tesc = true\n\t\t\tcontinue\n\t\t}\n\t\tif c == '\"' {\n\t\t\treturn s\n\t\t}\n\t\tif c >= ' ' && c != '\\\\' && c != '\"' {\n\t\t\ts += string(c)\n\t\t\tcontinue\n\t\t}\n\t\tp.xerrorf(\"invalid quoted string, invalid character %c\", c)\n\t}\n}\n\nfunc (p *parser) digits() string {\n\to := p.o\n\tfor o < len(p.s) && p.s[o] >= '0' && p.s[o] <= '9' {\n\t\to++\n\t}\n\tp.o = o\n\treturn p.s[o:p.o]\n}\n\nfunc (p *parser) xdigits() string {\n\ts := p.digits()\n\tif s == \"\" {\n\t\tp.xerrorf(\"expected digits, remaining %q\", p.s[p.o:])\n\t}\n\treturn s\n}\n\nfunc (p *parser) xtake(s string) {\n\tif !p.prefix(s) {\n\t\tp.xerrorf(\"expected %q, remaining %q\", s, p.s[p.o:])\n\t}\n\tp.o += len(s)\n}\n\nfunc (p *parser) empty() bool {\n\treturn p.o >= len(p.s)\n}\n\nfunc (p *parser) take(s string) bool {\n\tif p.prefix(s) {\n\t\tp.o += len(s)\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (p *parser) xtakefn1(what string, fn func(c rune, i int) bool) string {\n\tif p.empty() {\n\t\tp.xerrorf(\"need at least one char for %s\", what)\n\t}\n\tfor i, c := range p.s[p.o:] {\n\t\tif !fn(c, i) {\n\t\t\tif i == 0 {\n\t\t\t\tp.xerrorf(\"expected at least one char for %s, remaining %q\", what, p.s[p.o:])\n\t\t\t}\n\t\t\ts := p.s[p.o : p.o+i]\n\t\t\tp.o += i\n\t\t\treturn s\n\t\t}\n\t}\n\ts := p.s[p.o:]\n\tp.o = len(p.s)\n\treturn s\n}\n\n// ../rfc/5321:2287\nfunc (p *parser) xkeyword(isResult bool) string {\n\ts := strings.ToLower(p.xtakefn1(\"keyword\", func(c rune, i int) bool {\n\t\t// Yahoo sends results like \"dkim=perm_fail\".\n\t\treturn c >= 'a' && c <= 'z' || c >= '0' && c <= '9' || c == '-' || isResult && !Pedantic && c == '_'\n\t}))\n\tif s == \"-\" {\n\t\tp.xerrorf(\"missing keyword\")\n\t} else if strings.HasSuffix(s, \"-\") {\n\t\tp.o--\n\t\ts = s[:len(s)-1]\n\t}\n\treturn s\n}\n\nfunc (p *parser) xmethodspec(methodKeyword string) (string, string, string) {\n\tp.cfws()\n\tvar methodDigits string\n\tif p.take(\"/\") {\n\t\tmethodDigits = p.xdigits()\n\t\tp.cfws()\n\t}\n\tp.xtake(\"=\")\n\tp.cfws()\n\tresult := p.xkeyword(true)\n\treturn methodKeyword, methodDigits, result\n}\n\nfunc (p *parser) xpropspec() (ap AuthProp) {\n\tap.Type = p.xkeyword(false)\n\tp.cfws()\n\tp.xtake(\".\")\n\tp.cfws()\n\tif p.take(\"mailfrom\") {\n\t\tap.Property = \"mailfrom\"\n\t} else if p.take(\"rcptto\") {\n\t\tap.Property = \"rcptto\"\n\t} else {\n\t\tap.Property = p.xkeyword(false)\n\t}\n\tp.cfws()\n\tp.xtake(\"=\")\n\tap.IsAddrLike, ap.Value = p.xpvalue()\n\treturn\n}\n\n// method keyword has been parsed, method-version not yet.\nfunc (p *parser) xresinfo(methodKeyword string) (am AuthMethod) {\n\tp.cfws()\n\tam.Method, am.Version, am.Result = p.xmethodspec(methodKeyword)\n\tp.cfws()\n\tif p.take(\"reason\") {\n\t\tp.cfws()\n\t\tp.xtake(\"=\")\n\t\tp.cfws()\n\t\tam.Reason = p.xvalue()\n\t}\n\tp.cfws()\n\tfor !p.prefix(\";\") && !p.end() {\n\t\tam.Props = append(am.Props, p.xpropspec())\n\t\tp.cfws()\n\t}\n\treturn\n}\n\n// todo: could keep track whether this is a localpart.\nfunc (p *parser) xpvalue() (bool, string) {\n\tp.cfws()\n\tif p.take(\"@\") {\n\t\t// Bare domain.\n\t\tdom, _ := p.xdomain()\n\t\treturn true, \"@\" + dom\n\t}\n\ts := p.xvalue()\n\tif p.take(\"@\") {\n\t\tdom, _ := p.xdomain()\n\t\ts += \"@\" + dom\n\t\treturn true, s\n\t}\n\treturn false, s\n}\n\n// ../rfc/5321:2291\nfunc (p *parser) xdomain() (string, dns.Domain) {\n\ts := p.xsubdomain()\n\tfor p.take(\".\") {\n\t\ts += \".\" + p.xsubdomain()\n\t}\n\td, err := dns.ParseDomain(s)\n\tif err != nil {\n\t\tp.xerrorf(\"parsing domain name %q: %s\", s, err)\n\t}\n\tif len(s) > 255 {\n\t\t// ../rfc/5321:3491\n\t\tp.xerrorf(\"domain longer than 255 octets\")\n\t}\n\treturn s, d\n}\n\n// ../rfc/5321:2303\n// ../rfc/5321:2303 ../rfc/6531:411\nfunc (p *parser) xsubdomain() string {\n\treturn p.xtakefn1(\"subdomain\", func(c rune, i int) bool {\n\t\treturn c >= '0' && c <= '9' || c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z' || i > 0 && c == '-' || c > 0x7f\n\t})\n}\n\n```\n# Function Signature: func ParseAuthResults(s string) (ar AuthResults, err error) {\n# Natural Language Annotations: // ParseAuthResults parses a Authentication-Results header value.\n//\n// Comments are not populated in the returned AuthResults.\n// Both crlf and lf line-endings are accepted. The input string must end with\n// either crlf or lf.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc ParseAuthResults(s string) (ar AuthResults, err error) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tperr, ok := r.(parseError)\n\t\t\tif ok {\n\t\t\t\terr = perr.err\n\t\t\t} else {\n\t\t\t\tpanic(r)\n\t\t\t}\n\t\t}\n\t}()\n\tp := &parser{s: s}\n\tp.lower = strings.ToLower(s)\n\tar.Comment = p.cfws()\n\tp.xtake(\"Authentication-Results:\"+p.cfws())\n\tar.Hostname = p.value()\n\tfor p.take(\";\") && !p.end() {\n\t\tp.cfws()\n\t\tmethodKeyword := p.xkeyword(false)\n\t\tp.cfws()\n\t\tam := p.xresinfo(methodKeyword)\n\t\tam.Comment = p.cfws()\n\t\tam.Comment = strings.TrimSuffix(am.Comment, \" ()\")\n\t\tar.Methods = append(ar.Methods, am)\n\t}\n\tp.cfws()\n\tif !p.end() {\n\t\tp.xerrorf(\"expected crlf or lf, remaining %q\", p.s[p.o:])\n\t}\n\treturn ar, nil\n}\n```", "  ```\n// ParseAuthResults parses a Authentication-Results header value.\n//\n// Comments are not populated in the returned AuthResults.\n// Both crlf and lf line-endings are accepted. The input string must end with\n// either crlf or lf.\nfunc ParseAuthResults(s string) (ar AuthResults, err error) {\n\tp := &parser{s, strings.ToLower(s), 0}\n\tdefer p.recover(&err)\n\n\tp.cfws()\n\tar.Hostname = p.value()\n\tp.cfws()\n\tif p.prefix(\";\") {\n\t\treturn\n\t}\n\tvar comment string\n\tif p.take(\"(\") {\n\t\tcomment = p.comment()\n\t}\n\tar.Comment = optJoined(comment, \";\")\n\tar.Methods = methodList(p)\n\treturn\n}\n\nfunc (p *parser) comment() string {\n\tp.xtake(\"(\")\n\tresult := p.fws()\n\tfor !p.take(\")\") {\n\t\tif p.empty() {\n\t\t\tp.xerrorf(\"unexpected end in comment\")\n\t\t}\n\t\tif p.prefix(\"(\") {\n\t\t\tresult += p.comment()\n\t\t}\n\t\tresult += p.fws()\n\t}\n\treturn result\n}\n\nfunc (p *parser) value() string {\n\tif p.prefix(`\"`) {\n\t\treturn p.quotedString()\n\t}\n\treturn p.xtakefn1(\"value token\", func(c rune, i int) bool {\n\t\t// ../rfc/2045:661\n\t\t// todo: token cannot contain utf-8? not updated in ../rfc/6532. however, we also use it for the localpart & domain parsing, so we'll allow it.\n\t\treturn c > ' ' && !strings.ContainsRune(`()<>@,;:\\\\\"/[]?= `, c)\n\t})\n}\n\nfunc (p *parser) quotedString() string {\n\tp.xtake(`\"`)\n\tvar result string\n\tvar esc bool\n\tfor {\n\t\tc := p.xchar()\n\t\tif esc {\n\t\t\tif c >= ' ' && c < 0x7f {\n\t\t\t\tresult += string(c)\n\t\t\t\tesc = false\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tp.xerrorf(\"bad escaped char %c in quoted string\", c)\n\t\t}\n\t\tif c == '\\\\' {\n\t\t\tesc = true\n\t\t\tcontinue\n\t\t}\n\t\tif c == '\"' {\n\t\t\treturn result\n\t\t}\n\t\tif c >= ' ' && c != '\\\\' && c != '\"' {\n\t\t\tresult += string(c)\n\t\t\tcontinue\n\t\t}\n\t\tp.xerrorf(\"invalid quoted string, invalid character %c\", c)\n\t}\n}\n\nfunc (p *parser) methodList(p *parser) (am AuthMethod, err error) {\n\tfor {\n\t\tp.cfws()\n\t\tif p.prefix(\";\") {\n\t\t\tbreak\n\t\t}\n\t\tvar comment string\n\t\tif p.take(\"(\") {\n\t\t\tcomment = p.comment()\n\t\t}\n\t\tar.Comment = optJoined(comment, \";\")\n\t\tar.Methods = append(ar.Methods, p.resinfo())\n\t}\n\treturn\n}\n\nfunc (p *parser) resinfo() (am AuthMethod) {\n\tp.cfws()\n\tam.Method, am.Version, am.Result = p.methodspec()\n\tp.cfws()\n\tif p.take(\"reason\") {\n\t\tp.cfws()\n\t\tp.xtake(\"=\")\n\t\tp.cfws()\n\t\tam.Reason = p.xvalue()\n\t}\n\tp.cfws()\n\tfor !p.prefix(\";\") && !p.end() {\n\t\tam.Props = append(am.Props, p.propspec())\n\t\tp.cfws()\n\t}\n\treturn\n}\n\nfunc (p *parser) propspec() (ap AuthProp) {\n\tap.", "  ```\nimport (\n\t\"fmt\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/dns\"\n)\n\n// ParseAuthResults parses a Authentication-Results header value.\n//\n// Comments are not populated in the returned AuthResults.\n// Both crlf and lf line-endings are accepted. The input string must end with\n// either crlf or lf.\nfunc ParseAuthResults(s string) (ar AuthResults, err error) {\n\tp := parser{s: s, lower: strings.ToLower(s)}\n\tdefer p.recover(&err)\n\tar.Header = p.xtake(\"Authentication-Results:\")\n\tp.cfws()\n\tfor i := 0; !p.empty(); i++ {\n\t\tif p.prefix(\";\") {\n\t\t\tp.xtake(\";\")\n\t\t\tp.cfws()\n\t\t\tif p.end() {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tp.xerrorf(\"expected end of message\")\n\t\t}\n\t\tvar am AuthMethod\n\t\tswitch {\n\t\tcase p.prefix(\"spf\"):\n\t\t\tam.Method = p.xkeyword(true)\n\t\t\tp.cfws()\n\t\t\tp.xtake(\"=\")\n\t\t\tp.cfws()\n\t\t\tam.Result = p.xkeyword(true)\n\t\tcase p.prefix(\"dkim\"):\n\t\t\tam.Method = p.xkeyword(true)\n\t\t\tp.cfws()\n\t\t\tp.xtake(\"=\")\n\t\t\tp.cfws()\n\t\t\tam.Result = p.xkeyword(true)\n\t\tcase p.prefix(\"iprev\"):\n\t\t\tam.Method = p.xkeyword(true)\n\t\t\tp.cfws()\n\t\t\tp.xtake(\"=\")\n\t\t\tp.cfws()\n\t\t\tam.Result = p.xkeyword(true)\n\t\tcase p.prefix(\"auth\"):\n\t\t\tam.Method = p.xkeyword(true)\n\t\t\tp.cfws()\n\t\t\tp.xtake(\"=\")\n\t\t\tp.cfws()\n\t\t\tam.Result = p.xkeyword(true)\n\t\tdefault:\n\t\t\tp.xerrorf(\"unexpected token starting method\")\n\t\t}\n\t\tp.cfws()\n\t\tif p.take(\";\") {\n\t\t\tp.cfws()\n\t\t\tar.Methods = append(ar.Methods, am)\n\t\t\tcontinue\n\t\t}\n\t\tif p.prefix(\"(\") {\n\t\t\tp.xcomment()\n\t\t\tp.cfws()\n\t\t\tar.Methods = append(ar.Methods, am)\n\t\t\tcontinue\n\t\t}\n\t\tp.xerrorf(\"expected method value\")\n\t}\n\treturn ar, nil\n}\n// End of generated code.\n```"]}, "mox-store/msgreader.go-FileMsgReader": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/store/msgreader.go:\n```\npackage store\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n)\n\n// MsgReader provides access to a message. Reads return the \"msg_prefix\" in the\n// database (typically received headers), followed by the on-disk msg file\n// contents. MsgReader is an io.Reader, io.ReaderAt and io.Closer.\ntype MsgReader struct {\n\tprefix []byte   // First part of the message. Typically contains received headers.\n\tpath   string   // To on-disk message file.\n\tsize   int64    // Total size of message, including prefix and contents from path.\n\toffset int64    // Current reading offset.\n\tf      *os.File // Opened path, automatically opened after prefix has been read.\n\terr    error    // If set, error to return for reads. Sets io.EOF for readers, but ReadAt ignores them.\n}\n\nvar errMsgClosed = errors.New(\"msg is closed\")\n\n// FileMsgReader makes a MsgReader for an open file.\n// If initialization fails, reads will return the error.\n// Only call close on the returned MsgReader if you want to close msgFile.\n\n\n\n\n\n\n\n\n\n\n\n// Read reads data from the msg, taking prefix and on-disk msg file into account.\n// The read offset is adjusted after the read.\nfunc (m *MsgReader) Read(buf []byte) (int, error) {\n\treturn m.read(buf, m.offset, false)\n}\n\n// ReadAt reads data from the msg, taking prefix and on-disk msg file into account.\n// The read offset is not affected by ReadAt.\nfunc (m *MsgReader) ReadAt(buf []byte, off int64) (n int, err error) {\n\treturn m.read(buf, off, true)\n}\n\n// read always fill buf as far as possible, for ReadAt semantics.\nfunc (m *MsgReader) read(buf []byte, off int64, pread bool) (int, error) {\n\t// If a reader has consumed the file and reached EOF, further ReadAt must not return eof.\n\tif m.err != nil && (!pread || m.err != io.EOF) {\n\t\treturn 0, m.err\n\t}\n\tvar o int\n\tfor o < len(buf) {\n\t\t// First attempt to read from m.prefix.\n\t\tpn := int64(len(m.prefix)) - off\n\t\tif pn > 0 {\n\t\t\tn := len(buf)\n\t\t\tif int64(n) > pn {\n\t\t\t\tn = int(pn)\n\t\t\t}\n\t\t\tcopy(buf[o:], m.prefix[int(off):int(off)+n])\n\t\t\to += n\n\t\t\toff += int64(n)\n\t\t\tif !pread {\n\t\t\t\tm.offset += int64(n)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// Now we need to read from file. Ensure it is open.\n\t\tif m.f == nil {\n\t\t\tf, err := os.Open(m.path)\n\t\t\tif err != nil {\n\t\t\t\tm.err = err\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tm.f = f\n\t\t}\n\t\tn, err := m.f.ReadAt(buf[o:], off-int64(len(m.prefix)))\n\t\tif !pread && n > 0 {\n\t\t\tm.offset += int64(n)\n\t\t}\n\t\tif !pread || err != io.EOF {\n\t\t\tm.err = err\n\t\t}\n\t\tif n > 0 {\n\t\t\to += n\n\t\t\toff += int64(n)\n\t\t}\n\t\tif err == io.EOF {\n\t\t\tif off > m.size && (m.err == nil || m.err == io.EOF) {\n\t\t\t\terr = fmt.Errorf(\"on-disk message larger than expected (off %d, size %d)\", off, m.size)\n\t\t\t\tm.err = err\n\t\t\t}\n\t\t\treturn o, err\n\t\t}\n\t\tif n <= 0 {\n\t\t\tbreak\n\t\t}\n\t}\n\tif off > m.size && (m.err == nil || m.err == io.EOF) {\n\t\tm.err = fmt.Errorf(\"on-disk message larger than expected (off %d, size %d, prefix %d)\", off, m.size, len(m.prefix))\n\t}\n\treturn o, m.err\n}\n\n// Close ensures the msg file is closed. Further reads will fail.\nfunc (m *MsgReader) Close() error {\n\tif m.f != nil {\n\t\tif err := m.f.Close(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tm.f = nil\n\t}\n\tif m.err == errMsgClosed {\n\t\treturn m.err\n\t}\n\tm.err = errMsgClosed\n\treturn nil\n}\n\n// Reset rewinds the offset and clears error conditions, making it usable as a fresh reader.\nfunc (m *MsgReader) Reset() {\n\tm.offset = 0\n\tm.err = nil\n}\n\n// Size returns the total size of the contents of the message.\nfunc (m *MsgReader) Size() int64 {\n\treturn m.size\n}\n\n```\n# Function Signature: func FileMsgReader(prefix []byte, msgFile *os.File) *MsgReader {\n# Natural Language Annotations: // FileMsgReader makes a MsgReader for an open file.\n// If initialization fails, reads will return the error.\n// Only call close on the returned MsgReader if you want to close msgFile.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc FileMsgReader(prefix []byte, msgFile *os.File) *MsgReader {\n\tmr := &MsgReader{\n\t\tprefix: prefix,\n\t\tsize:   int64(len(prefix)) + msgFile.Size(),\n\t\tpath:   msgFile.Name(),\n\t}\n\treturn mr\n}\n```", "  ```\nfunc FileMsgReader(prefix []byte, msgFile *os.File) *MsgReader {\n\tm := new(MsgReader)\n\tm.prefix = prefix\n\tm.path = msgFile.Name()\n\n\t// Determine the size of the prefix and message file\n\tm.size = int64(len(prefix)) + msgFile.Size()\n\n\t// Initialize the offset and error state\n\tm.offset = 0\n\tm.err = nil\n\n\treturn m\n}\n```", "  ```\nfunc FileMsgReader(prefix []byte, msgFile *os.File) *MsgReader {\n\treturn &MsgReader{prefix: prefix, path: msgFile.Name(), err: nil, f: msgFile}\n}\n```\nThis function creates a new MsgReader to read message data from a file. It takes a byte prefix to be prepended to the message, as well as a file object representing the message file.\n\nThe function initializes the MsgReader object with the given prefix and file name, and sets the error field to nil. It also sets the file object (representing the message file) as the field f of the MsgReader struct.\n\nThis function does not actually open the file, as the file object passed in as msgFile is assumed to already be open. If the file is not open, Read methods will return an error.\n\nThe function returns a pointer to the newly created MsgReader object."]}, "mox-store/import.go-ParseDovecotKeywordsFlags": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/store/import.go:\n```\npackage store\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"golang.org/x/exp/maps\"\n\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// MsgSource is implemented by readers for mailbox file formats.\ntype MsgSource interface {\n\t// Return next message, or io.EOF when there are no more.\n\tNext() (*Message, *os.File, string, error)\n}\n\n// MboxReader reads messages from an mbox file, implementing MsgSource.\ntype MboxReader struct {\n\tlog        mlog.Log\n\tcreateTemp func(log mlog.Log, pattern string) (*os.File, error)\n\tpath       string\n\tline       int\n\tr          *bufio.Reader\n\tprevempty  bool\n\tnonfirst   bool\n\teof        bool\n\tfromLine   string // \"From \"-line for this message.\n\theader     bool   // Now in header section.\n}\n\nfunc NewMboxReader(log mlog.Log, createTemp func(log mlog.Log, pattern string) (*os.File, error), filename string, r io.Reader) *MboxReader {\n\treturn &MboxReader{\n\t\tlog:        log,\n\t\tcreateTemp: createTemp,\n\t\tpath:       filename,\n\t\tline:       1,\n\t\tr:          bufio.NewReader(r),\n\t}\n}\n\n// Position returns \"<filename>:<lineno>\" for the current position.\nfunc (mr *MboxReader) Position() string {\n\treturn fmt.Sprintf(\"%s:%d\", mr.path, mr.line)\n}\n\n// Next returns the next message read from the mbox file. The file is a temporary\n// file and must be removed/consumed. The third return value is the position in the\n// file.\nfunc (mr *MboxReader) Next() (*Message, *os.File, string, error) {\n\tif mr.eof {\n\t\treturn nil, nil, \"\", io.EOF\n\t}\n\n\tfrom := []byte(\"From \")\n\n\tif !mr.nonfirst {\n\t\tmr.header = true\n\t\t// First read, we're at the beginning of the file.\n\t\tline, err := mr.r.ReadBytes('\\n')\n\t\tif err == io.EOF {\n\t\t\treturn nil, nil, \"\", io.EOF\n\t\t}\n\t\tmr.line++\n\n\t\tif !bytes.HasPrefix(line, from) {\n\t\t\treturn nil, nil, mr.Position(), fmt.Errorf(`first line does not start with \"From \"`)\n\t\t}\n\t\tmr.nonfirst = true\n\t\tmr.fromLine = strings.TrimSpace(string(line))\n\t}\n\n\tf, err := mr.createTemp(mr.log, \"mboxreader\")\n\tif err != nil {\n\t\treturn nil, nil, mr.Position(), err\n\t}\n\tdefer func() {\n\t\tif f != nil {\n\t\t\tCloseRemoveTempFile(mr.log, f, \"message after mbox read error\")\n\t\t}\n\t}()\n\n\tfromLine := mr.fromLine\n\tbf := bufio.NewWriter(f)\n\tvar flags Flags\n\tkeywords := map[string]bool{}\n\tvar size int64\n\tfor {\n\t\tline, err := mr.r.ReadBytes('\\n')\n\t\tif err != nil && err != io.EOF {\n\t\t\treturn nil, nil, mr.Position(), fmt.Errorf(\"reading from mbox: %v\", err)\n\t\t}\n\t\tif len(line) > 0 {\n\t\t\tmr.line++\n\t\t\t// We store data with crlf, adjust any imported messages with bare newlines.\n\t\t\tif !bytes.HasSuffix(line, []byte(\"\\r\\n\")) {\n\t\t\t\tline = append(line[:len(line)-1], \"\\r\\n\"...)\n\t\t\t}\n\n\t\t\tif mr.header {\n\t\t\t\t// See https://doc.dovecot.org/admin_manual/mailbox_formats/mbox/\n\t\t\t\tif bytes.HasPrefix(line, []byte(\"Status:\")) {\n\t\t\t\t\ts := strings.TrimSpace(strings.SplitN(string(line), \":\", 2)[1])\n\t\t\t\t\tfor _, c := range s {\n\t\t\t\t\t\tswitch c {\n\t\t\t\t\t\tcase 'R':\n\t\t\t\t\t\t\tflags.Seen = true\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if bytes.HasPrefix(line, []byte(\"X-Status:\")) {\n\t\t\t\t\ts := strings.TrimSpace(strings.SplitN(string(line), \":\", 2)[1])\n\t\t\t\t\tfor _, c := range s {\n\t\t\t\t\t\tswitch c {\n\t\t\t\t\t\tcase 'A':\n\t\t\t\t\t\t\tflags.Answered = true\n\t\t\t\t\t\tcase 'F':\n\t\t\t\t\t\t\tflags.Flagged = true\n\t\t\t\t\t\tcase 'T':\n\t\t\t\t\t\t\tflags.Draft = true\n\t\t\t\t\t\tcase 'D':\n\t\t\t\t\t\t\tflags.Deleted = true\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if bytes.HasPrefix(line, []byte(\"X-Keywords:\")) {\n\t\t\t\t\ts := strings.TrimSpace(strings.SplitN(string(line), \":\", 2)[1])\n\t\t\t\t\tfor _, t := range strings.Split(s, \",\") {\n\t\t\t\t\t\tword := strings.ToLower(strings.TrimSpace(t))\n\t\t\t\t\t\tswitch word {\n\t\t\t\t\t\tcase \"forwarded\", \"$forwarded\":\n\t\t\t\t\t\t\tflags.Forwarded = true\n\t\t\t\t\t\tcase \"junk\", \"$junk\":\n\t\t\t\t\t\t\tflags.Junk = true\n\t\t\t\t\t\tcase \"notjunk\", \"$notjunk\", \"nonjunk\", \"$nonjunk\":\n\t\t\t\t\t\t\tflags.Notjunk = true\n\t\t\t\t\t\tcase \"phishing\", \"$phishing\":\n\t\t\t\t\t\t\tflags.Phishing = true\n\t\t\t\t\t\tcase \"mdnsent\", \"$mdnsent\":\n\t\t\t\t\t\t\tflags.MDNSent = true\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\tif err := CheckKeyword(word); err == nil {\n\t\t\t\t\t\t\t\tkeywords[word] = true\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif bytes.Equal(line, []byte(\"\\r\\n\")) {\n\t\t\t\tmr.header = false\n\t\t\t}\n\n\t\t\t// Next mail message starts at bare From word.\n\t\t\tif mr.prevempty && bytes.HasPrefix(line, from) {\n\t\t\t\tmr.fromLine = strings.TrimSpace(string(line))\n\t\t\t\tmr.header = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif bytes.HasPrefix(line, []byte(\">\")) && bytes.HasPrefix(bytes.TrimLeft(line, \">\"), []byte(\"From \")) {\n\t\t\t\tline = line[1:]\n\t\t\t}\n\t\t\tn, err := bf.Write(line)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, nil, mr.Position(), fmt.Errorf(\"writing message to file: %v\", err)\n\t\t\t}\n\t\t\tsize += int64(n)\n\t\t\tmr.prevempty = bytes.Equal(line, []byte(\"\\r\\n\"))\n\t\t}\n\t\tif err == io.EOF {\n\t\t\tmr.eof = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif err := bf.Flush(); err != nil {\n\t\treturn nil, nil, mr.Position(), fmt.Errorf(\"flush: %v\", err)\n\t}\n\n\tm := &Message{Flags: flags, Keywords: maps.Keys(keywords), Size: size}\n\n\tif t := strings.SplitN(fromLine, \" \", 3); len(t) == 3 {\n\t\tlayouts := []string{time.ANSIC, time.UnixDate, time.RubyDate}\n\t\tfor _, l := range layouts {\n\t\t\tt, err := time.Parse(l, t[2])\n\t\t\tif err == nil {\n\t\t\t\tm.Received = t\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\t// Prevent cleanup by defer.\n\tmf := f\n\tf = nil\n\n\treturn m, mf, mr.Position(), nil\n}\n\ntype MaildirReader struct {\n\tlog          mlog.Log\n\tcreateTemp   func(log mlog.Log, pattern string) (*os.File, error)\n\tnewf, curf   *os.File\n\tf            *os.File // File we are currently reading from. We first read newf, then curf.\n\tdir          string   // Name of directory for f. Can be empty on first call.\n\tentries      []os.DirEntry\n\tdovecotFlags []string // Lower-case flags/keywords.\n}\n\nfunc NewMaildirReader(log mlog.Log, createTemp func(log mlog.Log, pattern string) (*os.File, error), newf, curf *os.File) *MaildirReader {\n\tmr := &MaildirReader{\n\t\tlog:        log,\n\t\tcreateTemp: createTemp,\n\t\tnewf:       newf,\n\t\tcurf:       curf,\n\t\tf:          newf,\n\t}\n\n\t// Best-effort parsing of dovecot keywords.\n\tkf, err := os.Open(filepath.Join(filepath.Dir(newf.Name()), \"dovecot-keywords\"))\n\tif err == nil {\n\t\tmr.dovecotFlags, err = ParseDovecotKeywordsFlags(kf, log)\n\t\tlog.Check(err, \"parsing dovecot keywords file\")\n\t\terr = kf.Close()\n\t\tlog.Check(err, \"closing dovecot-keywords file\")\n\t}\n\n\treturn mr\n}\n\nfunc (mr *MaildirReader) Next() (*Message, *os.File, string, error) {\n\tif mr.dir == \"\" {\n\t\tmr.dir = mr.f.Name()\n\t}\n\n\tif len(mr.entries) == 0 {\n\t\tvar err error\n\t\tmr.entries, err = mr.f.ReadDir(100)\n\t\tif err != nil && err != io.EOF {\n\t\t\treturn nil, nil, \"\", err\n\t\t}\n\t\tif len(mr.entries) == 0 {\n\t\t\tif mr.f == mr.curf {\n\t\t\t\treturn nil, nil, \"\", io.EOF\n\t\t\t}\n\t\t\tmr.f = mr.curf\n\t\t\tmr.dir = \"\"\n\t\t\treturn mr.Next()\n\t\t}\n\t}\n\n\tp := filepath.Join(mr.dir, mr.entries[0].Name())\n\tmr.entries = mr.entries[1:]\n\tsf, err := os.Open(p)\n\tif err != nil {\n\t\treturn nil, nil, p, fmt.Errorf(\"open message in maildir: %s\", err)\n\t}\n\tdefer func() {\n\t\terr := sf.Close()\n\t\tmr.log.Check(err, \"closing message file after error\")\n\t}()\n\tf, err := mr.createTemp(mr.log, \"maildirreader\")\n\tif err != nil {\n\t\treturn nil, nil, p, err\n\t}\n\tdefer func() {\n\t\tif f != nil {\n\t\t\tname := f.Name()\n\t\t\terr := f.Close()\n\t\t\tmr.log.Check(err, \"closing temporary message file after maildir read error\")\n\t\t\terr = os.Remove(name)\n\t\t\tmr.log.Check(err, \"removing temporary message file after maildir read error\", slog.String(\"path\", name))\n\t\t}\n\t}()\n\n\t// Copy data, changing bare \\n into \\r\\n.\n\tr := bufio.NewReader(sf)\n\tw := bufio.NewWriter(f)\n\tvar size int64\n\tfor {\n\t\tline, err := r.ReadBytes('\\n')\n\t\tif err != nil && err != io.EOF {\n\t\t\treturn nil, nil, p, fmt.Errorf(\"reading message: %v\", err)\n\t\t}\n\t\tif len(line) > 0 {\n\t\t\tif !bytes.HasSuffix(line, []byte(\"\\r\\n\")) {\n\t\t\t\tline = append(line[:len(line)-1], \"\\r\\n\"...)\n\t\t\t}\n\n\t\t\tif n, err := w.Write(line); err != nil {\n\t\t\t\treturn nil, nil, p, fmt.Errorf(\"writing message: %v\", err)\n\t\t\t} else {\n\t\t\t\tsize += int64(n)\n\t\t\t}\n\t\t}\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t}\n\t}\n\tif err := w.Flush(); err != nil {\n\t\treturn nil, nil, p, fmt.Errorf(\"writing message: %v\", err)\n\t}\n\n\t// Take received time from filename, falling back to mtime for maildirs\n\t// reconstructed some other sources of message files.\n\tvar received time.Time\n\tt := strings.SplitN(filepath.Base(sf.Name()), \".\", 3)\n\tif v, err := strconv.ParseInt(t[0], 10, 64); len(t) == 3 && err == nil {\n\t\treceived = time.Unix(v, 0)\n\t} else if fi, err := sf.Stat(); err == nil {\n\t\treceived = fi.ModTime()\n\t}\n\n\t// Parse flags. See https://cr.yp.to/proto/maildir.html.\n\tflags := Flags{}\n\tkeywords := map[string]bool{}\n\tt = strings.SplitN(filepath.Base(sf.Name()), \":2,\", 2)\n\tif len(t) == 2 {\n\t\tfor _, c := range t[1] {\n\t\t\tswitch c {\n\t\t\tcase 'P':\n\t\t\t\t// Passed, doesn't map to a common IMAP flag.\n\t\t\tcase 'R':\n\t\t\t\tflags.Answered = true\n\t\t\tcase 'S':\n\t\t\t\tflags.Seen = true\n\t\t\tcase 'T':\n\t\t\t\tflags.Deleted = true\n\t\t\tcase 'D':\n\t\t\t\tflags.Draft = true\n\t\t\tcase 'F':\n\t\t\t\tflags.Flagged = true\n\t\t\tdefault:\n\t\t\t\tif c >= 'a' && c <= 'z' {\n\t\t\t\t\tindex := int(c - 'a')\n\t\t\t\t\tif index >= len(mr.dovecotFlags) {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tkw := mr.dovecotFlags[index]\n\t\t\t\t\tswitch kw {\n\t\t\t\t\tcase \"$forwarded\", \"forwarded\":\n\t\t\t\t\t\tflags.Forwarded = true\n\t\t\t\t\tcase \"$junk\", \"junk\":\n\t\t\t\t\t\tflags.Junk = true\n\t\t\t\t\tcase \"$notjunk\", \"notjunk\", \"nonjunk\":\n\t\t\t\t\t\tflags.Notjunk = true\n\t\t\t\t\tcase \"$mdnsent\", \"mdnsent\":\n\t\t\t\t\t\tflags.MDNSent = true\n\t\t\t\t\tcase \"$phishing\", \"phishing\":\n\t\t\t\t\t\tflags.Phishing = true\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tkeywords[kw] = true\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tm := &Message{Received: received, Flags: flags, Keywords: maps.Keys(keywords), Size: size}\n\n\t// Prevent cleanup by defer.\n\tmf := f\n\tf = nil\n\n\treturn m, mf, p, nil\n}\n\n// ParseDovecotKeywordsFlags attempts to parse a dovecot-keywords file. It only\n// returns valid flags/keywords, as lower-case. If an error is encountered and\n// returned, any keywords that were found are still returned. The returned list has\n// both system/well-known flags and custom keywords.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func ParseDovecotKeywordsFlags(r io.Reader, log mlog.Log) ([]string, error) {\n# Natural Language Annotations: // ParseDovecotKeywordsFlags attempts to parse a dovecot-keywords file. It only\n// returns valid flags/keywords, as lower-case. If an error is encountered and\n// returned, any keywords that were found are still returned. The returned list has\n// both system/well-known flags and custom keywords.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc ParseDovecotKeywordsFlags(r io.Reader, log mlog.Log) ([]string, error) {\n\tkeywords := make(map[string]bool)\n\tscanner := bufio.NewScanner(r)\n\tvar line string\n\tfor scanner.Scan() {\n\t\tline = scanner.Text()\n\t\tif len(line) > 0 && !strings.HasPrefix(line, \"#\") {\n\t\t\tline = strings.TrimSpace(line)\n\t\t\tkeywords[strings.ToLower(line)] = true\n\t\t}\n\t}\n\tif err := scanner.Err(); err != nil {\n\t\tlog.Warn(\"error reading dovecot-keywords: %v\", err)\n\t}\n\treturn maps.Keys(keywords), nil\n}\n```", "  ```\nfunc ParseDovecotKeywordsFlags(r io.Reader, log mlog.Log) ([]string, error) {\n\tkeywords := map[string]struct{}{}\n\ts := bufio.NewScanner(r)\n\tfor s.Scan() {\n\t\tt := s.Text()\n\t\tif t == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\tname := strings.ToLower(strings.TrimSpace(t))\n\t\tif name == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\tkeywords[name] = struct{}{}\n\t}\n\tif err := s.Err(); err != nil {\n\t\tlog.Check(err, \"reading dovecot keywords file\")\n\t}\n\tvalidFlags := []string{}\n\tfor name := range keywords {\n\t\tif _, err := strconv.ParseInt(name, 10, 64); err == nil {\n\t\t\t// Skip system flags that look like numbers, see\n\t\t\t// https://cr.yp.to/proto/maildir.html section \"2.  Flags\".\n\t\t} else {\n\t\t\tvalidFlags = append(validFlags, name)\n\t\t}\n\t}\n\treturn validFlags, nil\n}\n```", "  ```\nfunc ParseDovecotKeywordsFlags(r io.Reader, log mlog.Log) ([]string, error) {\n\tflags := make([]string, 0) // initial capacity of 0 is fine as it will grow\n\tkeywords := make(map[string]bool)\n\n\ts := bufio.NewScanner(r)\n\tfor s.Scan() {\n\t\tline := s.Text()\n\t\tif line == \"\" {\n\t\t\tcontinue\n\t\t}\n\n\t\t// ignore inline comments\n\t\tif strings.HasPrefix(line, \"#\") {\n\t\t\tcontinue\n\t\t}\n\n\t\tf := strings.Split(line, \" \")\n\t\tif len(f) < 2 {\n\t\t\tlog.Error(\"invalid dovecot keywords: not enough arguments\", slog.String(\"line\", line))\n\t\t\tcontinue\n\t\t}\n\n\t\tif f[0] == \"flag\" {\n\t\t\tkey, err := strconv.Unquote(f[1])\n\t\t\tif err != nil {\n\t\t\t\tlog.Error(\"invalid dovecot keywords flag\", slog.String(\"flag\", f[1]), slog.String(\"line\", line))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tflags = append(flags, key)\n\t\t} else if f[0] == \"kw\" {\n\t\t\tkey, err := strconv.Unquote(f[2])\n\t\t\tif err != nil {\n\t\t\t\tlog.Error(\"invalid dovecot keywords keyword\", slog.String(\"keyword\", f[2]), slog.String(\"line\", line))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tkeywords[key] = true\n\t\t} else {\n\t\t\tlog.Error(\"invalid dovecot keywords line\", slog.String(\"line\", line))\n\t\t}\n\t}\n\tif err := s.Err(); err != nil && err != io.EOF {\n\t\treturn nil, fmt.Errorf(\"scanning dovecot keywords: %v\", err)\n\t}\n\n\tfor _, k := range DovecotWellKnownKeywords() {\n\t\tif _, ok := keywords[k]; !ok {\n\t\t\tkeywords[k] = false\n\t\t}\n\t}\n\n\treturn append(append(keysInMap(keywords), flags...), DovecotWellKnownFlags()...), nil\n}\n```"]}, "mox-store/account.go-MessageRuleset": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/store/account.go:\n```\n/*\nPackage store implements storage for accounts, their mailboxes, IMAP\nsubscriptions and messages, and broadcasts updates (e.g. mail delivery) to\ninterested sessions (e.g. IMAP connections).\n\nLayout of storage for accounts:\n\n\t<DataDir>/accounts/<name>/index.db\n\t<DataDir>/accounts/<name>/msg/[a-zA-Z0-9_-]+/<id>\n\nIndex.db holds tables for user information, mailboxes, and messages. Messages\nare stored in the msg/ subdirectory, each in their own file. The on-disk message\ndoes not contain headers generated during an incoming SMTP transaction, such as\nReceived and Authentication-Results headers. Those are in the database to\nprevent having to rewrite incoming messages (e.g. Authentication-Result for DKIM\nsignatures can only be determined after having read the message). Messages must\nbe read through MsgReader, which transparently adds the prefix from the\ndatabase.\n*/\npackage store\n\n// todo: make up a function naming scheme that indicates whether caller should broadcast changes.\n\nimport (\n\t\"context\"\n\t\"crypto/md5\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/sha1\"\n\t\"crypto/sha256\"\n\t\"encoding\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"io\"\n\t\"log/slog\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime/debug\"\n\t\"slices\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/crypto/bcrypt\"\n\t\"golang.org/x/text/secure/precis\"\n\t\"golang.org/x/text/unicode/norm\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/config\"\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/metrics\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/publicsuffix\"\n\t\"github.com/mjl-/mox/scram\"\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// If true, each time an account is closed its database file is checked for\n// consistency. If an inconsistency is found, panic is called. Set by default\n// because of all the packages with tests, the mox main function sets it to\n// false again.\nvar CheckConsistencyOnClose = true\n\nvar (\n\tErrUnknownMailbox     = errors.New(\"no such mailbox\")\n\tErrUnknownCredentials = errors.New(\"credentials not found\")\n\tErrAccountUnknown     = errors.New(\"no such account\")\n\tErrOverQuota          = errors.New(\"account over quota\")\n)\n\nvar DefaultInitialMailboxes = config.InitialMailboxes{\n\tSpecialUse: config.SpecialUseMailboxes{\n\t\tSent:    \"Sent\",\n\t\tArchive: \"Archive\",\n\t\tTrash:   \"Trash\",\n\t\tDraft:   \"Drafts\",\n\t\tJunk:    \"Junk\",\n\t},\n}\n\ntype SCRAM struct {\n\tSalt           []byte\n\tIterations     int\n\tSaltedPassword []byte\n}\n\n// CRAMMD5 holds HMAC ipad and opad hashes that are initialized with the first\n// block with (a derivation of) the key/password, so we don't store the password in plain\n// text.\ntype CRAMMD5 struct {\n\tIpad hash.Hash\n\tOpad hash.Hash\n}\n\n// BinaryMarshal is used by bstore to store the ipad/opad hash states.\nfunc (c CRAMMD5) MarshalBinary() ([]byte, error) {\n\tif c.Ipad == nil || c.Opad == nil {\n\t\treturn nil, nil\n\t}\n\n\tipad, err := c.Ipad.(encoding.BinaryMarshaler).MarshalBinary()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"marshal ipad: %v\", err)\n\t}\n\topad, err := c.Opad.(encoding.BinaryMarshaler).MarshalBinary()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"marshal opad: %v\", err)\n\t}\n\tbuf := make([]byte, 2+len(ipad)+len(opad))\n\tipadlen := uint16(len(ipad))\n\tbuf[0] = byte(ipadlen >> 8)\n\tbuf[1] = byte(ipadlen >> 0)\n\tcopy(buf[2:], ipad)\n\tcopy(buf[2+len(ipad):], opad)\n\treturn buf, nil\n}\n\n// BinaryUnmarshal is used by bstore to restore the ipad/opad hash states.\nfunc (c *CRAMMD5) UnmarshalBinary(buf []byte) error {\n\tif len(buf) == 0 {\n\t\t*c = CRAMMD5{}\n\t\treturn nil\n\t}\n\tif len(buf) < 2 {\n\t\treturn fmt.Errorf(\"short buffer\")\n\t}\n\tipadlen := int(uint16(buf[0])<<8 | uint16(buf[1])<<0)\n\tif len(buf) < 2+ipadlen {\n\t\treturn fmt.Errorf(\"buffer too short for ipadlen\")\n\t}\n\tipad := md5.New()\n\topad := md5.New()\n\tif err := ipad.(encoding.BinaryUnmarshaler).UnmarshalBinary(buf[2 : 2+ipadlen]); err != nil {\n\t\treturn fmt.Errorf(\"unmarshal ipad: %v\", err)\n\t}\n\tif err := opad.(encoding.BinaryUnmarshaler).UnmarshalBinary(buf[2+ipadlen:]); err != nil {\n\t\treturn fmt.Errorf(\"unmarshal opad: %v\", err)\n\t}\n\t*c = CRAMMD5{ipad, opad}\n\treturn nil\n}\n\n// Password holds credentials in various forms, for logging in with SMTP/IMAP.\ntype Password struct {\n\tHash        string  // bcrypt hash for IMAP LOGIN, SASL PLAIN and HTTP basic authentication.\n\tCRAMMD5     CRAMMD5 // For SASL CRAM-MD5.\n\tSCRAMSHA1   SCRAM   // For SASL SCRAM-SHA-1.\n\tSCRAMSHA256 SCRAM   // For SASL SCRAM-SHA-256.\n}\n\n// Subjectpass holds the secret key used to sign subjectpass tokens.\ntype Subjectpass struct {\n\tEmail string // Our destination address (canonical, with catchall localpart stripped).\n\tKey   string\n}\n\n// NextUIDValidity is a singleton record in the database with the next UIDValidity\n// to use for the next mailbox.\ntype NextUIDValidity struct {\n\tID   int // Just a single record with ID 1.\n\tNext uint32\n}\n\n// SyncState track ModSeqs.\ntype SyncState struct {\n\tID int // Just a single record with ID 1.\n\n\t// Last used, next assigned will be one higher. The first value we hand out is 2.\n\t// That's because 0 (the default value for old existing messages, from before the\n\t// Message.ModSeq field) is special in IMAP, so we return it as 1.\n\tLastModSeq ModSeq `bstore:\"nonzero\"`\n\n\t// Highest ModSeq of expunged record that we deleted. When a clients synchronizes\n\t// and requests changes based on a modseq before this one, we don't have the\n\t// history to provide information about deletions. We normally keep these expunged\n\t// records around, but we may periodically truly delete them to reclaim storage\n\t// space. Initially set to -1 because we don't want to match with any ModSeq in the\n\t// database, which can be zero values.\n\tHighestDeletedModSeq ModSeq\n}\n\n// Mailbox is collection of messages, e.g. Inbox or Sent.\ntype Mailbox struct {\n\tID int64\n\n\t// \"Inbox\" is the name for the special IMAP \"INBOX\". Slash separated\n\t// for hierarchy.\n\tName string `bstore:\"nonzero,unique\"`\n\n\t// If UIDs are invalidated, e.g. when renaming a mailbox to a previously existing\n\t// name, UIDValidity must be changed. Used by IMAP for synchronization.\n\tUIDValidity uint32\n\n\t// UID likely to be assigned to next message. Used by IMAP to detect messages\n\t// delivered to a mailbox.\n\tUIDNext UID\n\n\tSpecialUse\n\n\t// Keywords as used in messages. Storing a non-system keyword for a message\n\t// automatically adds it to this list. Used in the IMAP FLAGS response. Only\n\t// \"atoms\" are allowed (IMAP syntax), keywords are case-insensitive, only stored in\n\t// lower case (for JMAP), sorted.\n\tKeywords []string\n\n\tHaveCounts    bool // Whether MailboxCounts have been initialized.\n\tMailboxCounts      // Statistics about messages, kept up to date whenever a change happens.\n}\n\n// MailboxCounts tracks statistics about messages for a mailbox.\ntype MailboxCounts struct {\n\tTotal   int64 // Total number of messages, excluding \\Deleted. For JMAP.\n\tDeleted int64 // Number of messages with \\Deleted flag. Used for IMAP message count that includes messages with \\Deleted.\n\tUnread  int64 // Messages without \\Seen, excluding those with \\Deleted, for JMAP.\n\tUnseen  int64 // Messages without \\Seen, including those with \\Deleted, for IMAP.\n\tSize    int64 // Number of bytes for all messages.\n}\n\nfunc (mc MailboxCounts) String() string {\n\treturn fmt.Sprintf(\"%d total, %d deleted, %d unread, %d unseen, size %d bytes\", mc.Total, mc.Deleted, mc.Unread, mc.Unseen, mc.Size)\n}\n\n// Add increases mailbox counts mc with those of delta.\nfunc (mc *MailboxCounts) Add(delta MailboxCounts) {\n\tmc.Total += delta.Total\n\tmc.Deleted += delta.Deleted\n\tmc.Unread += delta.Unread\n\tmc.Unseen += delta.Unseen\n\tmc.Size += delta.Size\n}\n\n// Add decreases mailbox counts mc with those of delta.\nfunc (mc *MailboxCounts) Sub(delta MailboxCounts) {\n\tmc.Total -= delta.Total\n\tmc.Deleted -= delta.Deleted\n\tmc.Unread -= delta.Unread\n\tmc.Unseen -= delta.Unseen\n\tmc.Size -= delta.Size\n}\n\n// SpecialUse identifies a specific role for a mailbox, used by clients to\n// understand where messages should go.\ntype SpecialUse struct {\n\tArchive bool\n\tDraft   bool\n\tJunk    bool\n\tSent    bool\n\tTrash   bool\n}\n\n// CalculateCounts calculates the full current counts for messages in the mailbox.\nfunc (mb *Mailbox) CalculateCounts(tx *bstore.Tx) (mc MailboxCounts, err error) {\n\tq := bstore.QueryTx[Message](tx)\n\tq.FilterNonzero(Message{MailboxID: mb.ID})\n\tq.FilterEqual(\"Expunged\", false)\n\terr = q.ForEach(func(m Message) error {\n\t\tmc.Add(m.MailboxCounts())\n\t\treturn nil\n\t})\n\treturn\n}\n\n// ChangeSpecialUse returns a change for special-use flags, for broadcasting to\n// other connections.\nfunc (mb Mailbox) ChangeSpecialUse() ChangeMailboxSpecialUse {\n\treturn ChangeMailboxSpecialUse{mb.ID, mb.Name, mb.SpecialUse}\n}\n\n// ChangeKeywords returns a change with new keywords for a mailbox (e.g. after\n// setting a new keyword on a message in the mailbox), for broadcasting to other\n// connections.\nfunc (mb Mailbox) ChangeKeywords() ChangeMailboxKeywords {\n\treturn ChangeMailboxKeywords{mb.ID, mb.Name, mb.Keywords}\n}\n\n// KeywordsChanged returns whether the keywords in a mailbox have changed.\nfunc (mb Mailbox) KeywordsChanged(origmb Mailbox) bool {\n\tif len(mb.Keywords) != len(origmb.Keywords) {\n\t\treturn true\n\t}\n\t// Keywords are stored sorted.\n\tfor i, kw := range mb.Keywords {\n\t\tif origmb.Keywords[i] != kw {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// CountsChange returns a change with mailbox counts.\nfunc (mb Mailbox) ChangeCounts() ChangeMailboxCounts {\n\treturn ChangeMailboxCounts{mb.ID, mb.Name, mb.MailboxCounts}\n}\n\n// Subscriptions are separate from existence of mailboxes.\ntype Subscription struct {\n\tName string\n}\n\n// Flags for a mail message.\ntype Flags struct {\n\tSeen      bool\n\tAnswered  bool\n\tFlagged   bool\n\tForwarded bool\n\tJunk      bool\n\tNotjunk   bool\n\tDeleted   bool\n\tDraft     bool\n\tPhishing  bool\n\tMDNSent   bool\n}\n\n// FlagsAll is all flags set, for use as mask.\nvar FlagsAll = Flags{true, true, true, true, true, true, true, true, true, true}\n\n// Validation of \"message From\" domain.\ntype Validation uint8\n\nconst (\n\tValidationUnknown   Validation = 0\n\tValidationStrict    Validation = 1 // Like DMARC, with strict policies.\n\tValidationDMARC     Validation = 2 // Actual DMARC policy.\n\tValidationRelaxed   Validation = 3 // Like DMARC, with relaxed policies.\n\tValidationPass      Validation = 4 // For SPF.\n\tValidationNeutral   Validation = 5 // For SPF.\n\tValidationTemperror Validation = 6\n\tValidationPermerror Validation = 7\n\tValidationFail      Validation = 8\n\tValidationSoftfail  Validation = 9  // For SPF.\n\tValidationNone      Validation = 10 // E.g. No records.\n)\n\n// Message stored in database and per-message file on disk.\n//\n// Contents are always the combined data from MsgPrefix and the on-disk file named\n// based on ID.\n//\n// Messages always have a header section, even if empty. Incoming messages without\n// header section must get an empty header section added before inserting.\ntype Message struct {\n\t// ID, unchanged over lifetime, determines path to on-disk msg file.\n\t// Set during deliver.\n\tID int64\n\n\tUID       UID   `bstore:\"nonzero\"` // UID, for IMAP. Set during deliver.\n\tMailboxID int64 `bstore:\"nonzero,unique MailboxID+UID,index MailboxID+Received,index MailboxID+ModSeq,ref Mailbox\"`\n\n\t// Modification sequence, for faster syncing with IMAP QRESYNC and JMAP.\n\t// ModSeq is the last modification. CreateSeq is the Seq the message was inserted,\n\t// always <= ModSeq. If Expunged is set, the message has been removed and should not\n\t// be returned to the user. In this case, ModSeq is the Seq where the message is\n\t// removed, and will never be changed again.\n\t// We have an index on both ModSeq (for JMAP that synchronizes per account) and\n\t// MailboxID+ModSeq (for IMAP that synchronizes per mailbox).\n\t// The index on CreateSeq helps efficiently finding created messages for JMAP.\n\t// The value of ModSeq is special for IMAP. Messages that existed before ModSeq was\n\t// added have 0 as value. But modseq 0 in IMAP is special, so we return it as 1. If\n\t// we get modseq 1 from a client, the IMAP server will translate it to 0. When we\n\t// return modseq to clients, we turn 0 into 1.\n\tModSeq    ModSeq `bstore:\"index\"`\n\tCreateSeq ModSeq `bstore:\"index\"`\n\tExpunged  bool\n\n\t// If set, this message was delivered to a Rejects mailbox. When it is moved to a\n\t// different mailbox, its MailboxOrigID is set to the destination mailbox and this\n\t// flag cleared.\n\tIsReject bool\n\n\t// If set, this is a forwarded message (through a ruleset with IsForward). This\n\t// causes fields used during junk analysis to be moved to their Orig variants, and\n\t// masked IP fields cleared, so they aren't used in junk classifications for\n\t// incoming messages. This ensures the forwarded messages don't cause negative\n\t// reputation for the forwarding mail server, which may also be sending regular\n\t// messages.\n\tIsForward bool\n\n\t// MailboxOrigID is the mailbox the message was originally delivered to. Typically\n\t// Inbox or Rejects, but can also be a mailbox configured in a Ruleset, or\n\t// Postmaster, TLS/DMARC reporting addresses. MailboxOrigID is not changed when the\n\t// message is moved to another mailbox, e.g. Archive/Trash/Junk. Used for\n\t// per-mailbox reputation.\n\t//\n\t// MailboxDestinedID is normally 0, but when a message is delivered to the Rejects\n\t// mailbox, it is set to the intended mailbox according to delivery rules,\n\t// typically that of Inbox. When such a message is moved out of Rejects, the\n\t// MailboxOrigID is corrected by setting it to MailboxDestinedID. This ensures the\n\t// message is used for reputation calculation for future deliveries to that\n\t// mailbox.\n\t//\n\t// These are not bstore references to prevent having to update all messages in a\n\t// mailbox when the original mailbox is removed. Use of these fields requires\n\t// checking if the mailbox still exists.\n\tMailboxOrigID     int64\n\tMailboxDestinedID int64\n\n\tReceived time.Time `bstore:\"default now,index\"`\n\n\t// Full IP address of remote SMTP server. Empty if not delivered over SMTP. The\n\t// masked IPs are used to classify incoming messages. They are left empty for\n\t// messages matching a ruleset for forwarded messages.\n\tRemoteIP        string\n\tRemoteIPMasked1 string `bstore:\"index RemoteIPMasked1+Received\"` // For IPv4 /32, for IPv6 /64, for reputation.\n\tRemoteIPMasked2 string `bstore:\"index RemoteIPMasked2+Received\"` // For IPv4 /26, for IPv6 /48.\n\tRemoteIPMasked3 string `bstore:\"index RemoteIPMasked3+Received\"` // For IPv4 /21, for IPv6 /32.\n\n\t// Only set if present and not an IP address. Unicode string. Empty for forwarded\n\t// messages.\n\tEHLODomain        string         `bstore:\"index EHLODomain+Received\"`\n\tMailFrom          string         // With localpart and domain. Can be empty.\n\tMailFromLocalpart smtp.Localpart // SMTP \"MAIL FROM\", can be empty.\n\t// Only set if it is a domain, not an IP. Unicode string. Empty for forwarded\n\t// messages, but see OrigMailFromDomain.\n\tMailFromDomain  string         `bstore:\"index MailFromDomain+Received\"`\n\tRcptToLocalpart smtp.Localpart // SMTP \"RCPT TO\", can be empty.\n\tRcptToDomain    string         // Unicode string.\n\n\t// Parsed \"From\" message header, used for reputation along with domain validation.\n\tMsgFromLocalpart smtp.Localpart\n\tMsgFromDomain    string `bstore:\"index MsgFromDomain+Received\"`    // Unicode string.\n\tMsgFromOrgDomain string `bstore:\"index MsgFromOrgDomain+Received\"` // Unicode string.\n\n\t// Simplified statements of the Validation fields below, used for incoming messages\n\t// to check reputation.\n\tEHLOValidated     bool\n\tMailFromValidated bool\n\tMsgFromValidated  bool\n\n\tEHLOValidation     Validation // Validation can also take reverse IP lookup into account, not only SPF.\n\tMailFromValidation Validation // Can have SPF-specific validations like ValidationSoftfail.\n\tMsgFromValidation  Validation // Desirable validations: Strict, DMARC, Relaxed. Will not be just Pass.\n\n\t// Domains with verified DKIM signatures. Unicode string. For forwarded messages, a\n\t// DKIM domain that matched a ruleset's verified domain is left out, but included\n\t// in OrigDKIMDomains.\n\tDKIMDomains []string `bstore:\"index DKIMDomains+Received\"`\n\n\t// For forwarded messages,\n\tOrigEHLODomain  string\n\tOrigDKIMDomains []string\n\n\t// Canonicalized Message-Id, always lower-case and normalized quoting, without\n\t// <>'s. Empty if missing. Used for matching message threads, and to prevent\n\t// duplicate reject delivery.\n\tMessageID string `bstore:\"index\"`\n\t// lower-case: ../rfc/5256:495\n\n\t// For matching threads in case there is no References/In-Reply-To header. It is\n\t// lower-cased, white-space collapsed, mailing list tags and re/fwd tags removed.\n\tSubjectBase string `bstore:\"index\"`\n\t// ../rfc/5256:90\n\n\t// Hash of message. For rejects delivery in case there is no Message-ID, only set\n\t// when delivered as reject.\n\tMessageHash []byte\n\n\t// ID of message starting this thread.\n\tThreadID int64 `bstore:\"index\"`\n\t// IDs of parent messages, from closest parent to the root message. Parent messages\n\t// may be in a different mailbox, or may no longer exist. ThreadParentIDs must\n\t// never contain the message id itself (a cycle), and parent messages must\n\t// reference the same ancestors.\n\tThreadParentIDs []int64\n\t// ThreadMissingLink is true if there is no match with a direct parent. E.g. first\n\t// ID in ThreadParentIDs is not the direct ancestor (an intermediate message may\n\t// have been deleted), or subject-based matching was done.\n\tThreadMissingLink bool\n\t// If set, newly delivered child messages are automatically marked as read. This\n\t// field is copied to new child messages. Changes are propagated to the webmail\n\t// client.\n\tThreadMuted bool\n\t// If set, this (sub)thread is collapsed in the webmail client, for threading mode\n\t// \"on\" (mode \"unread\" ignores it). This field is copied to new child message.\n\t// Changes are propagated to the webmail client.\n\tThreadCollapsed bool\n\n\t// If received message was known to match a mailing list rule (with modified junk\n\t// filtering).\n\tIsMailingList bool\n\n\t// If this message is a DSN, generated by us or received. For DSNs, we don't look\n\t// at the subject when matching threads.\n\tDSN bool\n\n\tReceivedTLSVersion     uint16 // 0 if unknown, 1 if plaintext/no TLS, otherwise TLS cipher suite.\n\tReceivedTLSCipherSuite uint16\n\tReceivedRequireTLS     bool // Whether RequireTLS was known to be used for incoming delivery.\n\n\tFlags\n\t// For keywords other than system flags or the basic well-known $-flags. Only in\n\t// \"atom\" syntax (IMAP), they are case-insensitive, always stored in lower-case\n\t// (for JMAP), sorted.\n\tKeywords    []string `bstore:\"index\"`\n\tSize        int64\n\tTrainedJunk *bool  // If nil, no training done yet. Otherwise, true is trained as junk, false trained as nonjunk.\n\tMsgPrefix   []byte // Typically holds received headers and/or header separator.\n\n\t// ParsedBuf message structure. Currently saved as JSON of message.Part because bstore\n\t// cannot yet store recursive types. Created when first needed, and saved in the\n\t// database.\n\t// todo: once replaced with non-json storage, remove date fixup in ../message/part.go.\n\tParsedBuf []byte\n}\n\n// MailboxCounts returns the delta to counts this message means for its\n// mailbox.\nfunc (m Message) MailboxCounts() (mc MailboxCounts) {\n\tif m.Expunged\n```\n# Function Signature: func MessageRuleset(log mlog.Log, dest config.Destination, m *Message, msgPrefix []byte, msgFile *os.File) *config.Ruleset {\n# Natural Language Annotations: // MessageRuleset returns the first ruleset (if any) that matches the message\n// represented by msgPrefix and msgFile, with smtp and validation fields from m.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc MessageRuleset(log mlog.Log, dest config.Destination, m *Message, msgPrefix []byte, msgFile *os.File) *config.Ruleset {\n\t// TODO: implement\n\treturn nil\n}\n```", "  ```\nfunc MessageRuleset(log mlog.Log, dest config.Destination, m *Message, msgPrefix []byte, msgFile *os.File) *config.Ruleset {\n\tvar rulesets []*config.Ruleset\n\tvar matches [][]int\n\n\t// Check if there are any rulesets that match on  Message-ID\n\tif m.MessageID != \"\" {\n\t\trulesets, matches = dest.FindMatchingRulesets(m.MessageID, config.RuleSetMessageID)\n\t\tif len(rulesets) > 0 {\n\t\t\treturn rulesets[matches[0][0]]\n\t\t}\n\t}\n\n\t// Check if there are any rulesets that match on  X-Mozilla-Status\n\tif m.XMozillaStatus != \"\" {\n\t\trulesets, matches = dest.FindMatchingRulesets(m.XMozillaStatus, config.RuleSetXMozillaStatus)\n\t\tif len(rulesets) > 0 {\n\t\t\treturn rulesets[matches[0][0]]\n\t\t}\n\t}\n\n\t// Check if there are any rulesets that match on a subject-based prefix\n\tif m.SubjectBase != \"\" {\n\t\trulesets, matches = dest.FindMatchingRulesets(m.SubjectBase, config.RuleSetSubjectBase)\n\t\tif len(rulesets) > 0 {\n\t\t\treturn rulesets[matches[0][0]]\n\t\t}\n\t}\n\n\t// Check if there are any rulesets that match on  From/References/In-Reply-To\n\tif m.From != \"\" && m.References != \"\" {\n\t\trulesets, matches = dest.FindMatchingRulesets(m.References, config.RuleSetReferences)\n\t\tif len(rulesets) > 0 {\n\t\t\treturn rulesets[matches[0][0]]\n\t\t}\n\t}\n\n\t// Check if there are any rulesets that match on  From/References/In-Reply-To\n\tif m.From != \"\" && m.InReplyTo != \"\" {\n\t\trulesets, matches = dest.FindMatchingRulesets(m.InReplyTo, config.RuleSetInReplyTo)\n\t\tif len(rulesets) > 0 {\n\t\t\treturn rulesets[matches[0][0]]\n\t\t}\n\t}\n\n\t// Check if there are any rulesets that match on  From/References/In-Reply-To\n\tif m.From != \"\" && m.References != \"\" && m.InReplyTo != \"\" {\n\t\trulesets, matches = dest.FindMatchingRulesets(m.From, config.RuleSetFrom)\n\t\tif len(rulesets) > 0 {\n\t\t\treturn rulesets[matches[0][0]]\n\t\t}\n\t}\n\n\treturn nil\n}\n```", "  ```\nfunc MessageRuleset(log mlog.Log, dest config.Destination, m *Message, msgPrefix []byte, msgFile *os.File) *config.Ruleset {\n\trs := &config.Ruleset{}\n\tfor _, r := range dest.Rulesets {\n\t\t// Check all portions that are known for this destination, checking the first\n\t\t// that matches. Check in order.\n\t\tif r.MatchSignature(log, m.MailFrom, m.SMTP) {\n\t\t\t*rs = *r\n\t\t\tbreak\n\t\t}\n\t\tif r.MatchValidation(log, m.EHLOValidation, m.MailFromValidation, m.MsgFromValidation) {\n\t\t\t*rs = *r\n\t\t\tbreak\n\t\t}\n\t\tif r.MatchMailFrom(log, m.MailFrom) {\n\t\t\t*rs = *r\n\t\t\tbreak\n\t\t}\n\t\tif r.MatchMessageID(log, m.MessageID) {\n\t\t\t*rs = *r\n\t\t\tbreak\n\t\t}\n\t\tif r.MatchMessagePrefix(log, m.MessageID, msgPrefix) {\n\t\t\t*rs = *r\n\t\t\tbreak\n\t\t}\n\t\tif r.MatchReferences(log, m.References) {\n\t\t\t*rs = *r\n\t\t\tbreak\n\t\t}\n\t\tif r.MatchSubject(log, m.Subject) {\n\t\t\t*rs = *r\n\t\t\tbreak\n\t\t}\n\t\tif r.MatchThreadID(log, m.ThreadID) {\n\t\t\t*rs = *r\n\t\t\tbreak\n\t\t}\n\t\tif m.IsMailingList && r.MatchMailingList() {\n\t\t\t*rs = *r\n\t\t\tbreak\n\t\t}\n\t\tif r.MatchMessageFileChecksum(log, msgFile) {\n\t\t\t*rs = *r\n\t\t\tbreak\n\t\t}\n\t}\n\treturn rs\n}\n```"]}, "mox-smtp/address.go-ParseAddress": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/smtp/address.go:\n```\npackage smtp\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"golang.org/x/text/unicode/norm\"\n\n\t\"github.com/mjl-/mox/dns\"\n)\n\n// Pedantic enables stricter parsing.\nvar Pedantic bool\n\nvar ErrBadAddress = errors.New(\"invalid email address\")\n\n// Localpart is a decoded local part of an email address, before the \"@\".\n// For quoted strings, values do not hold the double quote or escaping backslashes.\n// An empty string can be a valid localpart.\n// Localparts are in Unicode NFC.\ntype Localpart string\n\n// String returns a packed representation of an address, with proper escaping/quoting, for use in SMTP.\nfunc (lp Localpart) String() string {\n\t// See ../rfc/5321:2322 ../rfc/6531:414\n\t// First we try as dot-string. If not possible we make a quoted-string.\n\tdotstr := true\n\tt := strings.Split(string(lp), \".\")\n\tfor _, e := range t {\n\t\tfor _, c := range e {\n\t\t\tif c >= '0' && c <= '9' || c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z' || c > 0x7f {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tswitch c {\n\t\t\tcase '!', '#', '$', '%', '&', '\\'', '*', '+', '-', '/', '=', '?', '^', '_', '`', '{', '|', '}', '~':\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tdotstr = false\n\t\t\tbreak\n\t\t}\n\t\tdotstr = dotstr && len(e) > 0\n\t}\n\tdotstr = dotstr && len(t) > 0\n\tif dotstr {\n\t\treturn string(lp)\n\t}\n\n\t// Make quoted-string.\n\tr := `\"`\n\tfor _, b := range lp {\n\t\tif b == '\"' || b == '\\\\' {\n\t\t\tr += \"\\\\\" + string(b)\n\t\t} else {\n\t\t\tr += string(b)\n\t\t}\n\t}\n\tr += `\"`\n\treturn r\n}\n\n// LogString returns the localpart as string for use in smtp, and an escaped\n// representation if it has non-ascii characters.\nfunc (lp Localpart) LogString() string {\n\ts := lp.String()\n\tqs := strconv.QuoteToASCII(s)\n\tif qs != `\"`+s+`\"` {\n\t\ts = \"/\" + qs\n\t}\n\treturn s\n}\n\n// DSNString returns the localpart as string for use in a DSN.\n// utf8 indicates if the remote MTA supports utf8 messaging. If not, the 7bit DSN\n// encoding for \"utf-8-addr-xtext\" from RFC 6533 is used.\nfunc (lp Localpart) DSNString(utf8 bool) string {\n\tif utf8 {\n\t\treturn lp.String()\n\t}\n\t// ../rfc/6533:259\n\tr := \"\"\n\tfor _, c := range lp {\n\t\tif c > 0x20 && c < 0x7f && c != '\\\\' && c != '+' && c != '=' {\n\t\t\tr += string(c)\n\t\t} else {\n\t\t\tr += fmt.Sprintf(`\\x{%x}`, c)\n\t\t}\n\t}\n\treturn r\n}\n\n// IsInternational returns if this is an internationalized local part, i.e. has\n// non-ASCII characters.\nfunc (lp Localpart) IsInternational() bool {\n\tfor _, c := range lp {\n\t\tif c > 0x7f {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Address is a parsed email address.\ntype Address struct {\n\tLocalpart Localpart\n\tDomain    dns.Domain // todo: shouldn't we accept an ip address here too? and merge this type into smtp.Path.\n}\n\n// NewAddress returns an address.\nfunc NewAddress(localpart Localpart, domain dns.Domain) Address {\n\treturn Address{localpart, domain}\n}\n\nfunc (a Address) Path() Path {\n\treturn Path{Localpart: a.Localpart, IPDomain: dns.IPDomain{Domain: a.Domain}}\n}\n\nfunc (a Address) IsZero() bool {\n\treturn a == Address{}\n}\n\n// Pack returns the address in string form. If smtputf8 is true, the domain is\n// formatted with non-ASCII characters. If localpart has non-ASCII characters,\n// they are returned regardless of smtputf8.\nfunc (a Address) Pack(smtputf8 bool) string {\n\tif a.IsZero() {\n\t\treturn \"\"\n\t}\n\treturn a.Localpart.String() + \"@\" + a.Domain.XName(smtputf8)\n}\n\n// String returns the address in string form with non-ASCII characters.\nfunc (a Address) String() string {\n\tif a.IsZero() {\n\t\treturn \"\"\n\t}\n\treturn a.Localpart.String() + \"@\" + a.Domain.Name()\n}\n\n// LogString returns the address with with utf-8 in localpart and/or domain. In\n// case of an IDNA domain and/or quotable characters in the localpart, an address\n// with quoted/escaped localpart and ASCII domain is also returned.\nfunc (a Address) LogString() string {\n\tif a.IsZero() {\n\t\treturn \"\"\n\t}\n\ts := a.Pack(true)\n\tlp := a.Localpart.String()\n\tqlp := strconv.QuoteToASCII(lp)\n\tescaped := qlp != `\"`+lp+`\"`\n\tif a.Domain.Unicode != \"\" || escaped {\n\t\tif escaped {\n\t\t\tlp = qlp\n\t\t}\n\t\ts += \"/\" + lp + \"@\" + a.Domain.ASCII\n\t}\n\treturn s\n}\n\n// ParseAddress parses an email address. UTF-8 is allowed.\n// Returns ErrBadAddress for invalid addresses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar ErrBadLocalpart = errors.New(\"invalid localpart\")\n\n// ParseLocalpart parses the local part.\n// UTF-8 is allowed.\n// Returns ErrBadAddress for invalid addresses.\n\n\n\n\n\n\n\n\n\n\n\nfunc parseLocalPart(s string) (localpart Localpart, remain string, err error) {\n\tp := &parser{s, 0}\n\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\te, ok := x.(error)\n\t\tif !ok {\n\t\t\tpanic(x)\n\t\t}\n\t\terr = fmt.Errorf(\"%w: %s\", ErrBadLocalpart, e)\n\t}()\n\n\tlp := p.xlocalpart()\n\treturn lp, p.remainder(), nil\n}\n\ntype parser struct {\n\ts string\n\to int\n}\n\nfunc (p *parser) xerrorf(format string, args ...any) {\n\tpanic(fmt.Errorf(format, args...))\n}\n\nfunc (p *parser) hasPrefix(s string) bool {\n\treturn strings.HasPrefix(p.s[p.o:], s)\n}\n\nfunc (p *parser) take(s string) bool {\n\tif p.hasPrefix(s) {\n\t\tp.o += len(s)\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (p *parser) xtake(s string) {\n\tif !p.take(s) {\n\t\tp.xerrorf(\"expected %q\", s)\n\t}\n}\n\nfunc (p *parser) empty() bool {\n\treturn p.o == len(p.s)\n}\n\nfunc (p *parser) xtaken(n int) string {\n\tr := p.s[p.o : p.o+n]\n\tp.o += n\n\treturn r\n}\n\nfunc (p *parser) remainder() string {\n\tr := p.s[p.o:]\n\tp.o = len(p.s)\n\treturn r\n}\n\n// todo: reduce duplication between implementations: ../smtp/address.go:/xlocalpart ../dkim/parser.go:/xlocalpart ../smtpserver/parse.go:/xlocalpart\nfunc (p *parser) xlocalpart() Localpart {\n\t// ../rfc/5321:2316\n\tvar s string\n\tif p.hasPrefix(`\"`) {\n\t\ts = p.xquotedString()\n\t} else {\n\t\ts = p.xatom()\n\t\tfor p.take(\".\") {\n\t\t\ts += \".\" + p.xatom()\n\t\t}\n\t}\n\t// In the wild, some services use large localparts for generated (bounce) addresses.\n\tif Pedantic && len(s) > 64 || len(s) > 128 {\n\t\t// ../rfc/5321:3486\n\t\tp.xerrorf(\"localpart longer than 64 octets\")\n\t}\n\treturn Localpart(norm.NFC.String(s))\n}\n\nfunc (p *parser) xquotedString() string {\n\tp.xtake(`\"`)\n\tvar s string\n\tvar esc bool\n\tfor {\n\t\tc := p.xchar()\n\t\tif esc {\n\t\t\tif c >= ' ' && c < 0x7f {\n\t\t\t\ts += string(c)\n\t\t\t\tesc = false\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tp.xerrorf(\"invalid localpart, bad escaped char %c\", c)\n\t\t}\n\t\tif c == '\\\\' {\n\t\t\tesc = true\n\t\t\tcontinue\n\t\t}\n\t\tif c == '\"' {\n\t\t\treturn s\n\t\t}\n\t\t// todo: should we be accepting utf8 for quoted strings?\n\t\tif c >= ' ' && c < 0x7f && c != '\\\\' && c != '\"' || c > 0x7f {\n\t\t\ts += string(c)\n\t\t\tcontinue\n\t\t}\n\t\tp.xerrorf(\"invalid localpart, invalid character %c\", c)\n\t}\n}\n\nfunc (p *parser) xchar() rune {\n\t// We are careful to track invalid utf-8 properly.\n\tif p.empty() {\n\t\tp.xerrorf(\"need another character\")\n\t}\n\tvar r rune\n\tvar o int\n\tfor i, c := range p.s[p.o:] {\n\t\tif i > 0 {\n\t\t\to = i\n\t\t\tbreak\n\t\t}\n\t\tr = c\n\t}\n\tif o == 0 {\n\t\tp.o = len(p.s)\n\t} else {\n\t\tp.o += o\n\t}\n\treturn r\n}\n\nfunc (p *parser) takefn1(what string, fn func(c rune, i int) bool) string {\n\tif p.empty() {\n\t\tp.xerrorf(\"need at least one char for %s\", what)\n\t}\n\tfor i, c := range p.s[p.o:] {\n\t\tif !fn(c, i) {\n\t\t\tif i == 0 {\n\t\t\t\tp.xerrorf(\"expected at least one char for %s, got char %c\", what, c)\n\t\t\t}\n\t\t\treturn p.xtaken(i)\n\t\t}\n\t}\n\treturn p.remainder()\n}\n\nfunc (p *parser) xatom() string {\n\treturn p.takefn1(\"atom\", func(c rune, i int) bool {\n\t\tswitch c {\n\t\tcase '!', '#', '$', '%', '&', '\\'', '*', '+', '-', '/', '=', '?', '^', '_', '`', '{', '|', '}', '~':\n\t\t\treturn true\n\t\t}\n\t\treturn isalphadigit(c) || c > 0x7f\n\t})\n}\n\nfunc isalpha(c rune) bool {\n\treturn c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z'\n}\n\nfunc isdigit(c rune) bool {\n\treturn c >= '0' && c <= '9'\n}\n\nfunc isalphadigit(c rune) bool {\n\treturn isalpha(c) || isdigit(c)\n}\n\n```\n# Function Signature: func ParseAddress(s string) (address Address, err error) {\n# Natural Language Annotations: // ParseAddress parses an email address. UTF-8 is allowed.\n// Returns ErrBadAddress for invalid addresses.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\npackage smtp\n\nimport (\n\t\"errors\"\n\t\"strings\"\n)\n\n// ParseAddress parses an email address. UTF-8 is allowed.\n// Returns ErrBadAddress for invalid addresses.\nfunc ParseAddress(s string) (address Address, err error) {\n\tp := &parser{s, 0}\n\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\te, ok := x.(error)\n\t\tif !ok {\n\t\t\tpanic(x)\n\t\t}\n\t\terr = fmt.Errorf(\"%w: %s\", ErrBadAddress, e)\n\t}()\n\n\taddress.Localpart = p.xlocalpart()\n\tp.take(\" \")\n\taddress.Domain = p.xdomain()\n\treturn address, nil\n}\n```", "  ```\nfunc ParseAddress(s string) (address Address, err error) {\n\tp := &parser{s, 0}\n\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\te, ok := x.(error)\n\t\tif !ok {\n\t\t\tpanic(x)\n\t\t}\n\t\terr = fmt.Errorf(\"%w: %s\", ErrBadAddress, e)\n\t}()\n\n\tlp := p.xlocalpart()\n\tp.xtake(\"@\")\n\tdomain := domain(p.xdomain())\n\treturn Address{lp, domain}, nil\n}\n\ntype parser struct {\n\ts string\n\to int\n}\n\nfunc (p *parser) xerrorf(format string, args ...any) {\n\tpanic(fmt.Errorf(format, args...))\n}\n\nfunc (p *parser) hasPrefix(s string) bool {\n\treturn strings.HasPrefix(p.s[p.o:], s)\n}\n\nfunc (p *parser) take(s string) bool {\n\tif p.hasPrefix(s) {\n\t\tp.o += len(s)\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (p *parser) xtake(s string) {\n\tif !p.take(s) {\n\t\tp.xerrorf(\"expected %q\", s)\n\t}\n}\n\nfunc (p *parser) empty() bool {\n\treturn p.o == len(p.s)\n}\n\nfunc (p *parser) xtaken(n int) string {\n\tr := p.s[p.o : p.o+n]\n\tp.o += n\n\treturn r\n}\n\nfunc (p *parser) remainder() string {\n\tr := p.s[p.o:]\n\tp.o = len(p.s)\n\treturn r\n}\n\nfunc (p *parser) xlocalpart() Localpart {\n\t// ../rfc/5321:2316\n\tvar s string\n\tif p.hasPrefix(`\"`) {\n\t\ts = p.xquotedString()\n\t} else {\n\t\ts = p.xatom()\n\t\tfor p.take(\".\") {\n\t\t\ts += \".\" + p.xatom()\n\t\t}\n\t}\n\tif Pedantic && len(s) > 64 || len(s) > 128 {\n\t\t// ../rfc/5321:3486\n\t\tp.xerrorf(\"localpart longer than 64 octets\")\n\t}\n\treturn Localpart(norm.NFC.String(s))\n}\n\nfunc (p *parser) xquotedString() string {\n\tp.xtake(`\"`)\n\tvar s string\n\tvar esc bool\n\tfor {\n\t\tc := p.xchar()\n\t\tif esc {\n\t\t\tif c >= ' ' && c < 0x7f {\n\t\t\t\ts += string(c)\n\t\t\t\tesc = false\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tp.xerrorf(\"invalid localpart, bad escaped char %c\", c)\n\t\t}\n\t\tif c == '\\\\' {\n\t\t\tesc = true\n\t\t\tcontinue\n\t\t}\n\t\tif c == '\"' {\n\t\t\treturn s\n\t\t}\n\t\tif c >= ' ' && c < 0x7f && c != '\\\\' && c != '\"' || c > 0x7f {\n\t\t\ts += string(c)\n\t\t\tcontinue\n\t\t}\n\t\tp.xerrorf(\"invalid localpart, invalid character %c\", c)\n\t}\n}\n\nfunc (p *parser) xchar() rune {\n\t// We are careful to track invalid utf-8 properly.\n\tif p.empty() {\n\t\tp.xerrorf(\"need another character\")\n\t}\n\tvar r rune\n\tvar o int\n\tfor i, c := range p.s[p.o:] {\n\t\tif i > 0 {\n\t\t\to = i\n\t\t\tbreak\n\t\t}\n\t\tr = c\n\t}\n\tif o == ", "  ```\nfunc ParseAddress(s string) (address Address, err error) {\n\ts = strings.TrimSpace(s)\n\tparts := strings.Split(s, \"@\")\n\tif len(parts) != 2 || parts[0] == \"\" || parts[1] == \"\" {\n\t\terr = ErrBadAddress\n\t\treturn\n\t}\n\tvar lp Localpart\n\tlp, err = parseLocalPart(parts[0])\n\tif err != nil {\n\t\treturn\n\t}\n\tdomain, err := dns.ParseDomain(parts[1])\n\tif err != nil {\n\t\treturn\n\t}\n\taddress = NewAddress(lp, domain)\n\treturn\n}\n```"]}, "mox-smtp/address.go-ParseLocalpart": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/smtp/address.go:\n```\npackage smtp\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"golang.org/x/text/unicode/norm\"\n\n\t\"github.com/mjl-/mox/dns\"\n)\n\n// Pedantic enables stricter parsing.\nvar Pedantic bool\n\nvar ErrBadAddress = errors.New(\"invalid email address\")\n\n// Localpart is a decoded local part of an email address, before the \"@\".\n// For quoted strings, values do not hold the double quote or escaping backslashes.\n// An empty string can be a valid localpart.\n// Localparts are in Unicode NFC.\ntype Localpart string\n\n// String returns a packed representation of an address, with proper escaping/quoting, for use in SMTP.\nfunc (lp Localpart) String() string {\n\t// See ../rfc/5321:2322 ../rfc/6531:414\n\t// First we try as dot-string. If not possible we make a quoted-string.\n\tdotstr := true\n\tt := strings.Split(string(lp), \".\")\n\tfor _, e := range t {\n\t\tfor _, c := range e {\n\t\t\tif c >= '0' && c <= '9' || c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z' || c > 0x7f {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tswitch c {\n\t\t\tcase '!', '#', '$', '%', '&', '\\'', '*', '+', '-', '/', '=', '?', '^', '_', '`', '{', '|', '}', '~':\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tdotstr = false\n\t\t\tbreak\n\t\t}\n\t\tdotstr = dotstr && len(e) > 0\n\t}\n\tdotstr = dotstr && len(t) > 0\n\tif dotstr {\n\t\treturn string(lp)\n\t}\n\n\t// Make quoted-string.\n\tr := `\"`\n\tfor _, b := range lp {\n\t\tif b == '\"' || b == '\\\\' {\n\t\t\tr += \"\\\\\" + string(b)\n\t\t} else {\n\t\t\tr += string(b)\n\t\t}\n\t}\n\tr += `\"`\n\treturn r\n}\n\n// LogString returns the localpart as string for use in smtp, and an escaped\n// representation if it has non-ascii characters.\nfunc (lp Localpart) LogString() string {\n\ts := lp.String()\n\tqs := strconv.QuoteToASCII(s)\n\tif qs != `\"`+s+`\"` {\n\t\ts = \"/\" + qs\n\t}\n\treturn s\n}\n\n// DSNString returns the localpart as string for use in a DSN.\n// utf8 indicates if the remote MTA supports utf8 messaging. If not, the 7bit DSN\n// encoding for \"utf-8-addr-xtext\" from RFC 6533 is used.\nfunc (lp Localpart) DSNString(utf8 bool) string {\n\tif utf8 {\n\t\treturn lp.String()\n\t}\n\t// ../rfc/6533:259\n\tr := \"\"\n\tfor _, c := range lp {\n\t\tif c > 0x20 && c < 0x7f && c != '\\\\' && c != '+' && c != '=' {\n\t\t\tr += string(c)\n\t\t} else {\n\t\t\tr += fmt.Sprintf(`\\x{%x}`, c)\n\t\t}\n\t}\n\treturn r\n}\n\n// IsInternational returns if this is an internationalized local part, i.e. has\n// non-ASCII characters.\nfunc (lp Localpart) IsInternational() bool {\n\tfor _, c := range lp {\n\t\tif c > 0x7f {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Address is a parsed email address.\ntype Address struct {\n\tLocalpart Localpart\n\tDomain    dns.Domain // todo: shouldn't we accept an ip address here too? and merge this type into smtp.Path.\n}\n\n// NewAddress returns an address.\nfunc NewAddress(localpart Localpart, domain dns.Domain) Address {\n\treturn Address{localpart, domain}\n}\n\nfunc (a Address) Path() Path {\n\treturn Path{Localpart: a.Localpart, IPDomain: dns.IPDomain{Domain: a.Domain}}\n}\n\nfunc (a Address) IsZero() bool {\n\treturn a == Address{}\n}\n\n// Pack returns the address in string form. If smtputf8 is true, the domain is\n// formatted with non-ASCII characters. If localpart has non-ASCII characters,\n// they are returned regardless of smtputf8.\nfunc (a Address) Pack(smtputf8 bool) string {\n\tif a.IsZero() {\n\t\treturn \"\"\n\t}\n\treturn a.Localpart.String() + \"@\" + a.Domain.XName(smtputf8)\n}\n\n// String returns the address in string form with non-ASCII characters.\nfunc (a Address) String() string {\n\tif a.IsZero() {\n\t\treturn \"\"\n\t}\n\treturn a.Localpart.String() + \"@\" + a.Domain.Name()\n}\n\n// LogString returns the address with with utf-8 in localpart and/or domain. In\n// case of an IDNA domain and/or quotable characters in the localpart, an address\n// with quoted/escaped localpart and ASCII domain is also returned.\nfunc (a Address) LogString() string {\n\tif a.IsZero() {\n\t\treturn \"\"\n\t}\n\ts := a.Pack(true)\n\tlp := a.Localpart.String()\n\tqlp := strconv.QuoteToASCII(lp)\n\tescaped := qlp != `\"`+lp+`\"`\n\tif a.Domain.Unicode != \"\" || escaped {\n\t\tif escaped {\n\t\t\tlp = qlp\n\t\t}\n\t\ts += \"/\" + lp + \"@\" + a.Domain.ASCII\n\t}\n\treturn s\n}\n\n// ParseAddress parses an email address. UTF-8 is allowed.\n// Returns ErrBadAddress for invalid addresses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar ErrBadLocalpart = errors.New(\"invalid localpart\")\n\n// ParseLocalpart parses the local part.\n// UTF-8 is allowed.\n// Returns ErrBadAddress for invalid addresses.\n\n\n\n\n\n\n\n\n\n\n\nfunc parseLocalPart(s string) (localpart Localpart, remain string, err error) {\n\tp := &parser{s, 0}\n\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\te, ok := x.(error)\n\t\tif !ok {\n\t\t\tpanic(x)\n\t\t}\n\t\terr = fmt.Errorf(\"%w: %s\", ErrBadLocalpart, e)\n\t}()\n\n\tlp := p.xlocalpart()\n\treturn lp, p.remainder(), nil\n}\n\ntype parser struct {\n\ts string\n\to int\n}\n\nfunc (p *parser) xerrorf(format string, args ...any) {\n\tpanic(fmt.Errorf(format, args...))\n}\n\nfunc (p *parser) hasPrefix(s string) bool {\n\treturn strings.HasPrefix(p.s[p.o:], s)\n}\n\nfunc (p *parser) take(s string) bool {\n\tif p.hasPrefix(s) {\n\t\tp.o += len(s)\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (p *parser) xtake(s string) {\n\tif !p.take(s) {\n\t\tp.xerrorf(\"expected %q\", s)\n\t}\n}\n\nfunc (p *parser) empty() bool {\n\treturn p.o == len(p.s)\n}\n\nfunc (p *parser) xtaken(n int) string {\n\tr := p.s[p.o : p.o+n]\n\tp.o += n\n\treturn r\n}\n\nfunc (p *parser) remainder() string {\n\tr := p.s[p.o:]\n\tp.o = len(p.s)\n\treturn r\n}\n\n// todo: reduce duplication between implementations: ../smtp/address.go:/xlocalpart ../dkim/parser.go:/xlocalpart ../smtpserver/parse.go:/xlocalpart\nfunc (p *parser) xlocalpart() Localpart {\n\t// ../rfc/5321:2316\n\tvar s string\n\tif p.hasPrefix(`\"`) {\n\t\ts = p.xquotedString()\n\t} else {\n\t\ts = p.xatom()\n\t\tfor p.take(\".\") {\n\t\t\ts += \".\" + p.xatom()\n\t\t}\n\t}\n\t// In the wild, some services use large localparts for generated (bounce) addresses.\n\tif Pedantic && len(s) > 64 || len(s) > 128 {\n\t\t// ../rfc/5321:3486\n\t\tp.xerrorf(\"localpart longer than 64 octets\")\n\t}\n\treturn Localpart(norm.NFC.String(s))\n}\n\nfunc (p *parser) xquotedString() string {\n\tp.xtake(`\"`)\n\tvar s string\n\tvar esc bool\n\tfor {\n\t\tc := p.xchar()\n\t\tif esc {\n\t\t\tif c >= ' ' && c < 0x7f {\n\t\t\t\ts += string(c)\n\t\t\t\tesc = false\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tp.xerrorf(\"invalid localpart, bad escaped char %c\", c)\n\t\t}\n\t\tif c == '\\\\' {\n\t\t\tesc = true\n\t\t\tcontinue\n\t\t}\n\t\tif c == '\"' {\n\t\t\treturn s\n\t\t}\n\t\t// todo: should we be accepting utf8 for quoted strings?\n\t\tif c >= ' ' && c < 0x7f && c != '\\\\' && c != '\"' || c > 0x7f {\n\t\t\ts += string(c)\n\t\t\tcontinue\n\t\t}\n\t\tp.xerrorf(\"invalid localpart, invalid character %c\", c)\n\t}\n}\n\nfunc (p *parser) xchar() rune {\n\t// We are careful to track invalid utf-8 properly.\n\tif p.empty() {\n\t\tp.xerrorf(\"need another character\")\n\t}\n\tvar r rune\n\tvar o int\n\tfor i, c := range p.s[p.o:] {\n\t\tif i > 0 {\n\t\t\to = i\n\t\t\tbreak\n\t\t}\n\t\tr = c\n\t}\n\tif o == 0 {\n\t\tp.o = len(p.s)\n\t} else {\n\t\tp.o += o\n\t}\n\treturn r\n}\n\nfunc (p *parser) takefn1(what string, fn func(c rune, i int) bool) string {\n\tif p.empty() {\n\t\tp.xerrorf(\"need at least one char for %s\", what)\n\t}\n\tfor i, c := range p.s[p.o:] {\n\t\tif !fn(c, i) {\n\t\t\tif i == 0 {\n\t\t\t\tp.xerrorf(\"expected at least one char for %s, got char %c\", what, c)\n\t\t\t}\n\t\t\treturn p.xtaken(i)\n\t\t}\n\t}\n\treturn p.remainder()\n}\n\nfunc (p *parser) xatom() string {\n\treturn p.takefn1(\"atom\", func(c rune, i int) bool {\n\t\tswitch c {\n\t\tcase '!', '#', '$', '%', '&', '\\'', '*', '+', '-', '/', '=', '?', '^', '_', '`', '{', '|', '}', '~':\n\t\t\treturn true\n\t\t}\n\t\treturn isalphadigit(c) || c > 0x7f\n\t})\n}\n\nfunc isalpha(c rune) bool {\n\treturn c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z'\n}\n\nfunc isdigit(c rune) bool {\n\treturn c >= '0' && c <= '9'\n}\n\nfunc isalphadigit(c rune) bool {\n\treturn isalpha(c) || isdigit(c)\n}\n\n```\n# Function Signature: func ParseLocalpart(s string) (localpart Localpart, err error) {\n# Natural Language Annotations: // ParseLocalpart parses the local part.\n// UTF-8 is allowed.\n// Returns ErrBadAddress for invalid addresses.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc ParseLocalpart(s string) (localpart Localpart, err error) {\n\tp := &parser{s, 0}\n\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\te, ok := x.(error)\n\t\tif !ok {\n\t\t\tpanic(x)\n\t\t}\n\t\terr = fmt.Errorf(\"%w: %s\", ErrBadLocalpart, e)\n\t}()\n\n\tlp := p.xlocalpart()\n\treturn lp, p.remainder(), nil\n}\n```", "  ```\nfunc ParseLocalpart(s string) (localpart Localpart, err error) {\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\te, ok := x.(error)\n\t\tif !ok {\n\t\t\tpanic(x)\n\t\t}\n\t\terr = fmt.Errorf(\"%w: %s\", ErrBadLocalpart, e)\n\t}()\n\n\tp := &parser{s, 0}\n\n\tlp := p.xlocalpart()\n\n\treturn lp, nil\n}\n```", "  ```\nfunc ParseLocalpart(s string) (Localpart, error) {\n\t// ../rfc/5321:2316\n\tvar localpart Localpart\n\tp := &parser{s, 0}\n\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\te, ok := x.(error)\n\t\tif !ok {\n\t\t\tpanic(x)\n\t\t}\n\t\terr = fmt.Errorf(\"%w: %s\", ErrBadLocalpart, e)\n\t}()\n\n\tlp := p.xlocalpart()\n\treturn lp, nil\n}\n```"]}, "mox-smtp/data.go-DataWrite": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/smtp/data.go:\n```\npackage smtp\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"errors\"\n\t\"io\"\n)\n\nvar ErrCRLF = errors.New(\"invalid bare carriage return or newline\")\n\nvar errMissingCRLF = errors.New(\"missing crlf at end of message\")\n\n// DataWrite reads data (a mail message) from r, and writes it to smtp\n// connection w with dot stuffing, as required by the SMTP data command.\n//\n// Messages with bare carriage returns or bare newlines result in an error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar dotcrlf = []byte(\".\\r\\n\")\n\n// DataReader is an io.Reader that reads data from an SMTP DATA command, doing dot\n// unstuffing and returning io.EOF when a bare dot is received. Use NewDataReader.\n//\n// Bare carriage returns, and the sequences \"[^\\r]\\n.\" and \"\\n.\\n\" result in an\n// error.\ntype DataReader struct {\n\t// ../rfc/5321:2003\n\tr           *bufio.Reader\n\tplast, last byte\n\tbuf         []byte // From previous read.\n\terr         error  // Read error, for after r.buf is exhausted.\n\n\t// When we see invalid combinations of CR and LF, we keep reading, and report an\n\t// error at the final \"\\r\\n.\\r\\n\". We cannot just stop reading and return an error,\n\t// the SMTP protocol would become out of sync.\n\tbadcrlf bool\n}\n\n// NewDataReader returns an initialized DataReader.\nfunc NewDataReader(r *bufio.Reader) *DataReader {\n\treturn &DataReader{\n\t\tr: r,\n\t\t// Set up initial state to accept a message that is only \".\" and CRLF.\n\t\tplast: '\\r',\n\t\tlast:  '\\n',\n\t}\n}\n\n// Read implements io.Reader.\nfunc (r *DataReader) Read(p []byte) (int, error) {\n\twrote := 0\n\tfor len(p) > 0 {\n\t\t// Read until newline as long as it fits in the buffer.\n\t\tif len(r.buf) == 0 {\n\t\t\tif r.err != nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\t// todo: set a max length, eg 1000 octets including crlf excluding potential leading dot. ../rfc/5321:3512\n\t\t\tr.buf, r.err = r.r.ReadSlice('\\n')\n\t\t\tif r.err == bufio.ErrBufferFull {\n\t\t\t\tr.err = nil\n\t\t\t} else if r.err == io.EOF {\n\t\t\t\t// Mark EOF as bad for now. If we see the ending dotcrlf below, err becomes regular\n\t\t\t\t// io.EOF again.\n\t\t\t\tr.err = io.ErrUnexpectedEOF\n\t\t\t}\n\t\t}\n\t\tif len(r.buf) > 0 {\n\t\t\t// Reject bare \\r.\n\t\t\tfor i, c := range r.buf {\n\t\t\t\tif c == '\\r' && (i == len(r.buf) || r.buf[i+1] != '\\n') {\n\t\t\t\t\tr.badcrlf = true\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// We require crlf. A bare LF is not a line ending for the end of the SMTP\n\t\t\t// transaction. ../rfc/5321:2032\n\t\t\t// Bare newlines are accepted as message data, unless around a bare dot. The SMTP\n\t\t\t// server adds missing carriage returns. We don't reject bare newlines outright,\n\t\t\t// real-world messages like that occur.\n\t\t\tif r.plast == '\\r' && r.last == '\\n' {\n\t\t\t\tif bytes.Equal(r.buf, dotcrlf) {\n\t\t\t\t\tr.buf = nil\n\t\t\t\t\tr.err = io.EOF\n\t\t\t\t\tif r.badcrlf {\n\t\t\t\t\t\tr.err = ErrCRLF\n\t\t\t\t\t}\n\t\t\t\t\tbreak\n\t\t\t\t} else if r.buf[0] == '.' {\n\t\t\t\t\t// Reject \"\\r\\n.\\n\".\n\t\t\t\t\tif len(r.buf) >= 2 && r.buf[1] == '\\n' {\n\t\t\t\t\t\tr.badcrlf = true\n\t\t\t\t\t}\n\t\t\t\t\tr.buf = r.buf[1:]\n\t\t\t\t}\n\t\t\t} else if r.last == '\\n' && (bytes.HasPrefix(r.buf, []byte(\".\\n\")) || bytes.HasPrefix(r.buf, []byte(\".\\r\\n\"))) {\n\t\t\t\t// Reject \"[^\\r]\\n.\\n\" and \"[^\\r]\\n.\\r\\n\"\n\t\t\t\tr.badcrlf = true\n\t\t\t}\n\t\t\tn := len(r.buf)\n\t\t\tif n > len(p) {\n\t\t\t\tn = len(p)\n\t\t\t}\n\t\t\tcopy(p, r.buf[:n])\n\t\t\tif n == 1 {\n\t\t\t\tr.plast, r.last = r.last, r.buf[0]\n\t\t\t} else if n > 1 {\n\t\t\t\tr.plast, r.last = r.buf[n-2], r.buf[n-1]\n\t\t\t}\n\t\t\tp = p[n:]\n\t\t\tr.buf = r.buf[n:]\n\t\t\twrote += n\n\t\t}\n\t}\n\treturn wrote, r.err\n}\n\n```\n# Function Signature: func DataWrite(w io.Writer, r io.Reader) error {\n# Natural Language Annotations: // DataWrite reads data (a mail message) from r, and writes it to smtp\n// connection w with dot stuffing, as required by the SMTP data command.\n//\n// Messages with bare carriage returns or bare newlines result in an error.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc DataWrite(w io.Writer, r io.Reader) error {\n\tvar buf bytes.Buffer\n\t_, err := io.Copy(&buf, r)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbufBytes := buf.Bytes()\n\tif bytes.Contains(bufBytes, []byte{0x0d, 0x0a}) {\n\t\treturn errors.New(\"bare carriage return or new line in message\")\n\t}\n\tw.Write(bufBytes)\n\treturn nil\n}\n```", "  ```\nfunc DataWrite(w io.Writer, r io.Reader) error {\n\t// Reader to copy message\n\tbr := bufio.NewReader(r)\n\n\t// Scanner to split input on bare carriage returns or bare newlines\n\tscanner := bufio.NewScanner(br)\n\tscanner.Split(ScanLines)\n\n\t// Writer to copy message with dot stuffing\n\tbw := bufio.NewWriter(w)\n\n\t// Copy message with dot stuffing\n\tfor scanner.Scan() {\n\t\tline := scanner.Text()\n\t\tif strings.HasPrefix(line, \".\") {\n\t\t\tbw.WriteString(\".\")\n\t\t}\n\t\tbw.WriteString(line)\n\t\tbw.WriteString(\"\\r\\n\")\n\t}\n\n\t// Flush the writer and check for errors\n\terr := bw.Flush()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Check for errors while reading from the input\n\tif scanner.Err() != nil {\n\t\treturn scanner.Err()\n\t}\n\n\t// Return any errors while writing to the output\n\treturn nil\n}\n\n// ScanLines scans a byte slice into a sequence of lines. A line is a sequence of characters\n// terminated by either a carriage return ('\\r') followed by a line feed ('\\n') or a carriage\n// return ('\\r'), followed by EOF. The resulting words do not include the terminating character\n// or EOF.\nfunc ScanLines(data []byte, atEOF bool) (advance int, token []byte, err error) {\n\tif atEOF && len(data) == 0 {\n\t\treturn 0, nil, nil\n\t}\n\tif i := bytes.IndexByte(data, '\\n'); i >= 0 {\n\t\t// We have a full newline-terminated line.\n\t\treturn i + 1, data[0:i], nil\n\t}\n\t// If we're at EOF, we have a final, non-terminated line. Return it.\n\tif atEOF {\n\t\treturn len(data), data, nil\n\t}\n\t// Request more data.\n\treturn 0, nil, nil\n}\n```", "  ```\nfunc DataWrite(w io.Writer, r io.Reader) error {\n    // Use a buffered writer so we can look at the previous character\n    // without consuming it as we write data to w.\n    bw := bufio.NewWriter(w)\n\n    // Initialize state to accept a message that is only \".\" and CRLF.\n    var plast, last byte = '\\r', '\\n'\n\n    // Read each character in the message and write it to w with dot-stuffing.\n    for {\n        c, err := r.ReadByte()\n        if err == io.EOF {\n            // End of message; check if we're missing a CRLF.\n            if last != '\\n' || plast != '\\r' {\n                return errMissingCRLF\n            }\n            bw.WriteString(\".\\r\\n\") // write dot-stuffing\n            break\n        } else if err != nil {\n            return err\n        }\n\n        plast = last\n        last = c\n\n        if c == '\\r' && last != '\\n' {\n            return ErrCRLF\n        } else if last == '\\n' && c == '.' {\n            // If we encounter a dot followed by a newline, we need to write\n            // dot-stuffing to the output to avoid the SMTP reply code\n            // 552 illegal line terminators \"...bare CR or bare LF.\"\n            bw.WriteString(\".\\r\\n\")\n        }\n        bw.WriteByte(c)\n    }\n\n    return bw.Flush()\n}\n```"]}, "mox-scram/scram.go-NewServer": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/scram/scram.go:\n```\n// Package scram implements the SCRAM-SHA-* SASL authentication mechanism, RFC 7677 and RFC 5802.\n//\n// SCRAM-SHA-256 and SCRAM-SHA-1 allow a client to authenticate to a server using a\n// password without handing plaintext password over to the server. The client also\n// verifies the server knows (a derivative of) the password. Both the client and\n// server side are implemented.\npackage scram\n\n// todo: test with messages that contains extensions\n// todo: some tests for the parser\n// todo: figure out how invalid parameters etc should be handled. just abort? perhaps mostly a problem for imap.\n\nimport (\n\t\"bytes\"\n\t\"crypto/hmac\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/tls\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"strings\"\n\n\t\"golang.org/x/crypto/pbkdf2\"\n\t\"golang.org/x/text/secure/precis\"\n\t\"golang.org/x/text/unicode/norm\"\n)\n\n// Errors at scram protocol level. Can be exchanged between client and server.\nvar (\n\tErrInvalidEncoding                 Error = \"invalid-encoding\"\n\tErrExtensionsNotSupported          Error = \"extensions-not-supported\"\n\tErrInvalidProof                    Error = \"invalid-proof\"\n\tErrChannelBindingsDontMatch        Error = \"channel-bindings-dont-match\"\n\tErrServerDoesSupportChannelBinding Error = \"server-does-support-channel-binding\"\n\tErrChannelBindingNotSupported      Error = \"channel-binding-not-supported\"\n\tErrUnsupportedChannelBindingType   Error = \"unsupported-channel-binding-type\"\n\tErrUnknownUser                     Error = \"unknown-user\"\n\tErrNoResources                     Error = \"no-resources\"\n\tErrOtherError                      Error = \"other-error\"\n)\n\nvar scramErrors = makeErrors()\n\nfunc makeErrors() map[string]Error {\n\tl := []Error{\n\t\tErrInvalidEncoding,\n\t\tErrExtensionsNotSupported,\n\t\tErrInvalidProof,\n\t\tErrChannelBindingsDontMatch,\n\t\tErrServerDoesSupportChannelBinding,\n\t\tErrChannelBindingNotSupported,\n\t\tErrUnsupportedChannelBindingType,\n\t\tErrUnknownUser,\n\t\tErrNoResources,\n\t\tErrOtherError,\n\t}\n\tm := map[string]Error{}\n\tfor _, e := range l {\n\t\tm[string(e)] = e\n\t}\n\treturn m\n}\n\nvar (\n\tErrNorm     = errors.New(\"parameter not unicode normalized\") // E.g. if client sends non-normalized username or authzid.\n\tErrUnsafe   = errors.New(\"unsafe parameter\")                 // E.g. salt, nonce too short, or too few iterations.\n\tErrProtocol = errors.New(\"protocol error\")                   // E.g. server responded with a nonce not prefixed by the client nonce.\n)\n\ntype Error string\n\nfunc (e Error) Error() string {\n\treturn string(e)\n}\n\n// MakeRandom returns a cryptographically random buffer for use as salt or as\n// nonce.\nfunc MakeRandom() []byte {\n\tbuf := make([]byte, 12)\n\t_, err := cryptorand.Read(buf)\n\tif err != nil {\n\t\tpanic(\"generate random\")\n\t}\n\treturn buf\n}\n\n// Cleanup password with precis, like remote should have done. If the password\n// appears invalid, we'll return the original, there is a chance the server also\n// doesn't enforce requirements and accepts it. ../rfc/8265:679\nfunc precisPassword(password string) string {\n\tpw, err := precis.OpaqueString.String(password)\n\tif err != nil {\n\t\treturn password\n\t}\n\treturn pw\n}\n\n// SaltPassword returns a salted password.\nfunc SaltPassword(h func() hash.Hash, password string, salt []byte, iterations int) []byte {\n\tpassword = precisPassword(password)\n\treturn pbkdf2.Key([]byte(password), salt, iterations, h().Size(), h)\n}\n\n// hmac0 returns the hmac with key over msg.\nfunc hmac0(h func() hash.Hash, key []byte, msg string) []byte {\n\tmac := hmac.New(h, key)\n\tmac.Write([]byte(msg))\n\treturn mac.Sum(nil)\n}\n\nfunc xor(a, b []byte) {\n\tfor i := range a {\n\t\ta[i] ^= b[i]\n\t}\n}\n\nfunc channelBindData(cs *tls.ConnectionState) ([]byte, error) {\n\tif cs.Version <= tls.VersionTLS12 {\n\t\tif cs.TLSUnique == nil {\n\t\t\treturn nil, fmt.Errorf(\"no channel binding data available\")\n\t\t}\n\t\treturn cs.TLSUnique, nil\n\t}\n\n\t// \"tls-exporter\", ../rfc/9266:95\n\t// Since TLS 1.3, a zero-length and absent context have the same behaviour. ../rfc/8446:5385 ../rfc/8446:5405\n\t// This is different from TLS 1.2 and earlier. ../rfc/5705:206 ../rfc/5705:245\n\treturn cs.ExportKeyingMaterial(\"EXPORTER-Channel-Binding\", []byte{}, 32)\n}\n\n// Server represents the server-side of a SCRAM-SHA-* authentication.\ntype Server struct {\n\tAuthentication string // Username for authentication, \"authc\". Always set and non-empty.\n\tAuthorization  string // If set, role of user to assume after authentication, \"authz\".\n\n\th func() hash.Hash // sha1.New or sha256.New\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\n\tgs2header           string\n\tclientNonce         string // Client-part of the nonce.\n\tserverNonceOverride string // If set, server does not generate random nonce, but uses this. For tests with the test vector.\n\tnonce               string // Full client + server nonce.\n\tchannelBinding      []byte\n}\n\n// NewServer returns a server given the first SCRAM message from a client.\n//\n// If cs is set, the PLUS variant can be negotiated, binding the authentication\n// exchange to the TLS channel (preventing MitM attempts). If a client\n// indicates it supports the PLUS variant, but thinks the server does not, the\n// authentication attempt will fail.\n//\n// If channelBindingRequired is set, the client has indicated it will do channel\n// binding and not doing so will cause the authentication to fail.\n//\n// The sequence for data and calls on a server:\n//\n//   - Read initial data from client, call NewServer (this call), then ServerFirst and write to the client.\n//   - Read response from client, call Finish or FinishFinal and write the resulting string.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst returns the string to send back to the client. To be called after NewServer.\n\n\n\n\n\n\n\n\n\n\n\n// Finish takes the final client message, and the salted password (probably\n// from server storage), verifies the client, and returns a message to return\n// to the client. If err is nil, authentication was successful. If the\n// authorization requested is not acceptable, the server should call\n// FinishError instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FinishError returns an error message to write to the client for the final\n// server message.\nfunc (s *Server) FinishError(err Error) string {\n\treturn \"e=\" + string(err)\n}\n\n// Client represents the client-side of a SCRAM-SHA-* authentication.\ntype Client struct {\n\tauthc string\n\tauthz string\n\n\th            func() hash.Hash     // sha1.New or sha256.New\n\tnoServerPlus bool                 // Server did not announce support for PLUS-variant.\n\tcs           *tls.ConnectionState // If set, use PLUS-variant.\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\tauthMessage             string\n\n\tgs2header       string\n\tclientNonce     string\n\tnonce           string // Full client + server nonce.\n\tsaltedPassword  []byte\n\tchannelBindData []byte // For PLUS-variant.\n}\n\n// NewClient returns a client for authentication authc, optionally for\n// authorization with role authz, for the hash (sha1.New or sha256.New).\n//\n// If noServerPlus is true, the client would like to have used the PLUS-variant,\n// that binds the authentication attempt to the TLS connection, but the client did\n// not see support for the PLUS variant announced by the server. Used during\n// negotiation to detect possible MitM attempt.\n//\n// If cs is not nil, the SCRAM PLUS-variant is negotiated, with channel binding to\n// the unique TLS connection, either using \"tls-exporter\" for TLS 1.3 and later, or\n// \"tls-unique\" otherwise.\n//\n// If cs is nil, no channel binding is done. If noServerPlus is also false, the\n// client is configured to not attempt/\"support\" the PLUS-variant, ensuring servers\n// that do support the PLUS-variant do not abort the connection.\n//\n// The sequence for data and calls on a client:\n//\n//   - ClientFirst, write result to server.\n//   - Read response from server, feed to ServerFirst, write response to server.\n//   - Read response from server, feed to ServerFinal.\nfunc NewClient(h func() hash.Hash, authc, authz string, noServerPlus bool, cs *tls.ConnectionState) *Client {\n\tauthc = norm.NFC.String(authc)\n\tauthz = norm.NFC.String(authz)\n\treturn &Client{authc: authc, authz: authz, h: h, noServerPlus: noServerPlus, cs: cs}\n}\n\n// ClientFirst returns the first client message to write to the server.\n// No channel binding is done/supported.\n// A random nonce is generated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst processes the first response message from the server. The\n// provided nonce, salt and iterations are checked. If valid, a final client\n// message is calculated and returned. This message must be written to the\n// server. It includes proof that the client knows the password.\nfunc (c *Client) ServerFirst(serverFirst []byte, password string) (clientFinal string, rerr error) {\n\tc.serverFirst = string(serverFirst)\n\tp := newParser(serverFirst)\n\tdefer p.recover(&rerr)\n\n\t// ../rfc/5802:632\n\t// ../rfc/5802:959\n\tif p.take(\"m=\") {\n\t\tp.xerrorf(\"unsupported mandatory extension: %w\", ErrExtensionsNotSupported) // ../rfc/5802:973\n\t}\n\n\tc.nonce = p.xnonce()\n\tp.xtake(\",\")\n\tsalt := p.xsalt()\n\tp.xtake(\",\")\n\titerations := p.xiterations()\n\t// We ignore extensions that we don't know about.\n\tfor p.take(\",\") {\n\t\tp.xattrval()\n\t}\n\tp.xempty()\n\n\tif !strings.HasPrefix(c.nonce, c.clientNonce) {\n\t\treturn \"\", fmt.Errorf(\"%w: server dropped our nonce\", ErrProtocol)\n\t}\n\tif len(c.nonce)-len(c.clientNonce) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: server nonce too short\", ErrUnsafe)\n\t}\n\tif len(salt) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: salt too short\", ErrUnsafe)\n\t}\n\tif iterations < 2048 {\n\t\treturn \"\", fmt.Errorf(\"%w: too few iterations\", ErrUnsafe)\n\t}\n\n\t// We send our channel binding data if present. If the server has different values,\n\t// we'll get an error. If any MitM would try to modify the channel binding data,\n\t// the server cannot verify our signature and will fail the attempt.\n\t// ../rfc/5802:925 ../rfc/5802:1015\n\tcbindInput := append([]byte(c.gs2header), c.channelBindData...)\n\tc.clientFinalWithoutProof = fmt.Sprintf(\"c=%s,r=%s\", base64.StdEncoding.EncodeToString(cbindInput), c.nonce)\n\n\tc.authMessage = c.clientFirstBare + \",\" + c.serverFirst + \",\" + c.clientFinalWithoutProof\n\n\tc.saltedPassword = SaltPassword(c.h, password, salt, iterations)\n\tclientKey := hmac0(c.h, c.saltedPassword, \"Client Key\")\n\th := c.h()\n\th.Write(clientKey)\n\tstoredKey := h.Sum(nil)\n\tclientSig := hmac0(c.h, storedKey, c.authMessage)\n\txor(clientSig, clientKey) // Now clientProof.\n\tclientProof := clientSig\n\n\tr := c.clientFinalWithoutProof + \",p=\" + base64.StdEncoding.EncodeToString(clientProof)\n\treturn r, nil\n}\n\n// ServerFinal processes the final message from the server, verifying that the\n// server knows the password.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Convert \",\" to =2C and \"=\" to =3D.\nfunc saslname(s string) string {\n\tvar r string\n\tfor _, c := range s {\n\t\tif c == ',' {\n\t\t\tr += \"=2C\"\n\t\t} else if c == '=' {\n\t\t\tr += \"=3D\"\n\t\t} else {\n\t\t\tr += string(c)\n\t\t}\n\t}\n\treturn r\n}\n\n```\n# Function Signature: func NewServer(h func() hash.Hash, clientFirst []byte, cs *tls.ConnectionState, channelBindingRequired bool) (server *Server, rerr error) {\n# Natural Language Annotations: // NewServer returns a server given the first SCRAM message from a client.\n//\n// If cs is set, the PLUS variant can be negotiated, binding the authentication\n// exchange to the TLS channel (preventing MitM attempts). If a client\n// indicates it supports the PLUS variant, but thinks the server does not, the\n// authentication attempt will fail.\n//\n// If channelBindingRequired is set, the client has indicated it will do channel\n// binding and not doing so will cause the authentication to fail.\n//\n// The sequence for data and calls on a server:\n//\n//   - Read initial data from client, call NewServer (this call), then ServerFirst and write to the client.\n//   - Read response from client, call Finish or FinishFinal and write the resulting string.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc NewServer(h func() hash.Hash, clientFirst []byte, cs *tls.ConnectionState, channelBindingRequired bool) (server *Server, rerr error) {\n\tserver = &Server{\n\t\th: h,\n\t}\n\tp := newParser(clientFirst)\n\tdefer p.recover(&rerr)\n\n\t// ../rfc/5802:632\n\t// ../rfc/5802:959\n\tif p.take(\"m=\") {\n\t\tp.xerrorf(\"unsupported mandatory extension: %w\", ErrExtensionsNotSupported) // ../rfc/5802:973\n\t}\n\n\tif p.take(\"p=\") {\n\t\tserver.clientProof = p.xtake(\"p=\")\n\t\tif err := p.xempty(); err != nil {\n\t\t\tp.xerrorf(\"extension invalid: %w\", ErrInvalidEncoding)\n\t\t}\n\t}\n\n\tif cs != nil {\n\t\tif p.take(\"p+=\") {\n\t\t\tserver.clientProofPlus = p.xtake(\"p+=\")\n\t\t\tif err := p.xempty(); err != nil {\n\t\t\t\tp.xerrorf(\"extension invalid: %w\", ErrInvalidEncoding)\n\t\t\t}\n\t\t} else if !channelBindingRequired {\n\t\t\tserver.noServerPlus = true\n\t\t} else {\n\t\t\treturn nil, fmt.Errorf(\"server tried to initiate PLUS channel binding, but client does not support it\")\n\t\t}\n\t}\n\n\tserver.clientNonce = p.xnonce()\n\tif server.clientNonce == \"\" {\n\t\treturn nil, fmt.Errorf(\"blank nonce\")\n\t}\n\tif server.serverNonceOverride != \"\" {\n\t\t// TODO: require client to send exactly this nonce.\n\t} else {\n\t\tserver.serverNonce = MakeRandom()\n\t}\n\tserver.serverNonce = append([]byte(server.serverNonce), server.clientNonce...)\n\tserver.gs2header = server.makeGs2Header()\n\tserver.nonce = server.serverNonce\n\n\tif server.noServerPlus {\n\t\tserver.clientFirstBare = fmt.Sprintf(\"n,,%s,\", strings.TrimPrefix(clientFirst, \"n=\"))\n\t\tserver.clientFirstWithoutProof = server.clientFirstBare\n\t} else {\n\t\tserver.clientFirstBare = fmt.Sprintf(\"n+,p=tls-unique,n=,\", p.xtake(\"p+=\"))\n\t\tserver.clientFirstWithoutProof = server.clientFirstBare + fmt.Sprintf(\",r=%s\", server.nonce)\n\t}\n\n\treturn server, nil\n}\n```", "  ```\nfunc NewServer(h func() hash.Hash, clientFirst []byte, cs *tls.ConnectionState, channelBindingRequired bool) (server *Server, rerr error) {\n\tsvr := &Server{h: h, cs: cs, channelBindingRequired: channelBindingRequired}\n\n\t// Parse clientFirst\n\tp := newParser(clientFirst)\n\tdefer p.recover(&rerr)\n\tsvr.clientFirstBare = p.xnonce()\n\tp.xtake(\",\")\n\tsvr.clientNonce = p.xnonce()\n\tfor _, ext := range p.xtake(\"n\", \"a\", \"m\", \"p\") {\n\t\tswitch ext[0] {\n\t\tcase 'n':\n\t\t\text = strings.TrimSpace(ext[2:])\n\t\t\tsvr.gs2header = ext\n\t\t\tif !strings.HasSuffix(svr.gs2header, \"-PLUS\") {\n\t\t\t\tsvr.noServerPlus = true\n\t\t\t}\n\t\tcase 'a':\n\t\t\tsvr.Authentication = p.xattrval()\n\t\tcase 'm':\n\t\t\tsvr.Authorization = p.xattrval()\n\t\tcase 'p':\n\t\t\tsvr.channelBinding = p.xvalue()\n\t\t}\n\t}\n\tif !strings.HasPrefix(svr.clientNonce, svr.gs2header) {\n\t\treturn nil, fmt.Errorf(\"client nonce does not start with GS2 header: %w\", ErrInvalidEncoding)\n\t}\n\tsvr.clientNonce = svr.clientNonce[len(svr.gs2header):]\n\tsvr.nonce = svr.clientNonce\n\tif channelBindingRequired && len(svr.channelBinding) == 0 {\n\t\treturn nil, fmt.Errorf(\"no channel binding data: %w\", ErrChannelBindingNotSupported)\n\t}\n\n\t// If the server supports the PLUS variant, use it.\n\tif svr.noServerPlus {\n\t\tif len(svr.channelBinding) != 0 {\n\t\t\treturn nil, fmt.Errorf(\"client bound to a different mechanism: %w\", ErrChannelBindingsDontMatch)\n\t\t}\n\t\tsvr.channelBinding = nil\n\t} else {\n\t\tif svr.cs == nil {\n\t\t\tif len(svr.channelBinding) != 0 {\n\t\t\t\treturn nil, fmt.Errorf(\"server does not support channel binding: %w\", ErrChannelBindingNotSupported)\n\t\t\t}\n\t\t} else {\n\t\t\tif len(svr.channelBinding) != 0 && !strings.HasPrefix(svr.channelBinding, \"t=tls-exporter\") {\n\t\t\t\treturn nil, fmt.Errorf(\"server does support the tls-exporter channel binding only: %w\", ErrUnsupportedChannelBindingType)\n\t\t\t}\n\t\t\tsvr.channelBinding = nil\n\t\t}\n\t}\n\n\t// Generate the server nonce.\n\tsvr.nonce = svr.nonce + \"s=\" + base64.StdEncoding.EncodeToString(MakeRandom())\n\n\t// Build and return the ServerFirst message.\n\tsvr.serverFirst = fmt.Sprintf(\"r=%s%s,s=%s\", svr.gs2header, svr.nonce, svr.nonce)\n\tif svr.channelBindingRequired {\n\t\tsvr.serverFirst += \",m=\" + saslname(svr.gs2header) + \",p=\"\n\t}\n\tsvr.serverFirst += \"r=\" + svr.clientFirstBare\n\treturn svr, nil\n}\n```", "  ```\nfunc NewServer(h func() hash.Hash, clientFirst []byte, cs *tls.ConnectionState, channelBindingRequired bool) (server *Server, rerr error) {\n\tclientFirstStr := string(clientFirst)\n\tp := newParser(clientFirstStr)\n\tdefer p.recover(&rerr)\n\n\tif !p.take(\"n=\") {\n\t\treturn nil, fmt.Errorf(\"no/invalid username: %w\", ErrInvalidEncoding)\n\t}\n\tauthc := norm.NFC.String(p.xattrval())\n\tvar authz string\n\tif p.take(\",a=\") {\n\t\tauthz = norm.NFC.String(p.xattrval())\n\t}\n\tvar ext []byte\n\tif p.take(\"e=\") {\n\t\text = p.xattrval()\n\t}\n\tif p.take(\",\") {\n\t\treturn nil, fmt.Errorf(\"unsupported/reserved attribute: %w\", ErrInvalidEncoding)\n\t}\n\tp.xempty()\n\n\tvar gs2header string\n\tif cs != nil {\n\t\tif !p.take(\"p=\") {\n\t\t\tgs2header = \"p=\"\n\t\t}\n\t}\n\n\t_, err := newParser(string(gs2header))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif p.countBytes == 0 && gs2header == \"\" {\n\t\tp.nonce = \"\"\n\t} else {\n\t\tp.nonce = string(p.xbase64())\n\t}\n\tif channelBindingRequired {\n\t\tif p.nonce != \"\" {\n\t\t\treturn nil, fmt.Errorf(\"no channel binding flag needed, but got: %w\", ErrProtocol)\n\t\t}\n\t\tp.nonce = \"y\"\n\t}\n\tif !strings.HasPrefix(p.nonce, p.gs2header) {\n\t\treturn nil, fmt.Errorf(\"no channel binding flag needed, but got: %w\", ErrProtocol)\n\t}\n\n\tif cs == nil {\n\t\tcbindInput := []byte(p.gs2header)\n\t} else {\n\t\tcbindInput, err := channelBindData(cs)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tp.clientNonce = \"\"\n\tif p.take(\"r=\") {\n\t\tp.clientNonce = string(p.xbase64())\n\t} else if p.take(\",\") {\n\t\tif !channelBindingRequired {\n\t\t\tp.nonce = \"\"\n\t\t} else {\n\t\t\treturn nil, fmt.Errorf(\"%w: received channel binding but channel binding required: %w\", ErrProtocol, ErrUnsafe)\n\t\t}\n\t}\n\n\tif !strings.HasPrefix(p.nonce, p.clientNonce) {\n\t\treturn nil, fmt.Errorf(\"%w: server nonce does not contain client nonce: %w\", ErrProtocol, ErrUnsafe)\n\t}\n\n\tif cs != nil {\n\t\tif !channelBindingRequired {\n\t\t\treturn nil, fmt.Errorf(\"%w: tls connection bound but server didn't require channel binding\", ErrProtocol)\n\t\t}\n\t} else if channelBindingRequired {\n\t\treturn nil, fmt.Errorf(\"%w: server requires channel binding but no tls connection\", ErrProtocol)\n\t}\n\n\tcc := clientFirstStr[:p.countBytes]\n\tcb := append(cbindInput, []byte{0}...)\n\tserver := &Server{\n\t\tAuthentication: authc,\n\t\tAuthorization:  authz,\n\t\th:              h,\n\t\tgs2header:      gs2header,\n\t\tclientNonce:    p.clientNonce,\n\t\tnonce:          p.nonce,\n\t\tchannelBinding: cb,\n\n\t\tclientFirstBare: cc,\n\t\tserverNonceOverride: \"\",\n\t}\n\treturn server, nil\n}\n```"]}, "mox-scram/scram.go-ServerFirst": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/scram/scram.go:\n```\n// Package scram implements the SCRAM-SHA-* SASL authentication mechanism, RFC 7677 and RFC 5802.\n//\n// SCRAM-SHA-256 and SCRAM-SHA-1 allow a client to authenticate to a server using a\n// password without handing plaintext password over to the server. The client also\n// verifies the server knows (a derivative of) the password. Both the client and\n// server side are implemented.\npackage scram\n\n// todo: test with messages that contains extensions\n// todo: some tests for the parser\n// todo: figure out how invalid parameters etc should be handled. just abort? perhaps mostly a problem for imap.\n\nimport (\n\t\"bytes\"\n\t\"crypto/hmac\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/tls\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"strings\"\n\n\t\"golang.org/x/crypto/pbkdf2\"\n\t\"golang.org/x/text/secure/precis\"\n\t\"golang.org/x/text/unicode/norm\"\n)\n\n// Errors at scram protocol level. Can be exchanged between client and server.\nvar (\n\tErrInvalidEncoding                 Error = \"invalid-encoding\"\n\tErrExtensionsNotSupported          Error = \"extensions-not-supported\"\n\tErrInvalidProof                    Error = \"invalid-proof\"\n\tErrChannelBindingsDontMatch        Error = \"channel-bindings-dont-match\"\n\tErrServerDoesSupportChannelBinding Error = \"server-does-support-channel-binding\"\n\tErrChannelBindingNotSupported      Error = \"channel-binding-not-supported\"\n\tErrUnsupportedChannelBindingType   Error = \"unsupported-channel-binding-type\"\n\tErrUnknownUser                     Error = \"unknown-user\"\n\tErrNoResources                     Error = \"no-resources\"\n\tErrOtherError                      Error = \"other-error\"\n)\n\nvar scramErrors = makeErrors()\n\nfunc makeErrors() map[string]Error {\n\tl := []Error{\n\t\tErrInvalidEncoding,\n\t\tErrExtensionsNotSupported,\n\t\tErrInvalidProof,\n\t\tErrChannelBindingsDontMatch,\n\t\tErrServerDoesSupportChannelBinding,\n\t\tErrChannelBindingNotSupported,\n\t\tErrUnsupportedChannelBindingType,\n\t\tErrUnknownUser,\n\t\tErrNoResources,\n\t\tErrOtherError,\n\t}\n\tm := map[string]Error{}\n\tfor _, e := range l {\n\t\tm[string(e)] = e\n\t}\n\treturn m\n}\n\nvar (\n\tErrNorm     = errors.New(\"parameter not unicode normalized\") // E.g. if client sends non-normalized username or authzid.\n\tErrUnsafe   = errors.New(\"unsafe parameter\")                 // E.g. salt, nonce too short, or too few iterations.\n\tErrProtocol = errors.New(\"protocol error\")                   // E.g. server responded with a nonce not prefixed by the client nonce.\n)\n\ntype Error string\n\nfunc (e Error) Error() string {\n\treturn string(e)\n}\n\n// MakeRandom returns a cryptographically random buffer for use as salt or as\n// nonce.\nfunc MakeRandom() []byte {\n\tbuf := make([]byte, 12)\n\t_, err := cryptorand.Read(buf)\n\tif err != nil {\n\t\tpanic(\"generate random\")\n\t}\n\treturn buf\n}\n\n// Cleanup password with precis, like remote should have done. If the password\n// appears invalid, we'll return the original, there is a chance the server also\n// doesn't enforce requirements and accepts it. ../rfc/8265:679\nfunc precisPassword(password string) string {\n\tpw, err := precis.OpaqueString.String(password)\n\tif err != nil {\n\t\treturn password\n\t}\n\treturn pw\n}\n\n// SaltPassword returns a salted password.\nfunc SaltPassword(h func() hash.Hash, password string, salt []byte, iterations int) []byte {\n\tpassword = precisPassword(password)\n\treturn pbkdf2.Key([]byte(password), salt, iterations, h().Size(), h)\n}\n\n// hmac0 returns the hmac with key over msg.\nfunc hmac0(h func() hash.Hash, key []byte, msg string) []byte {\n\tmac := hmac.New(h, key)\n\tmac.Write([]byte(msg))\n\treturn mac.Sum(nil)\n}\n\nfunc xor(a, b []byte) {\n\tfor i := range a {\n\t\ta[i] ^= b[i]\n\t}\n}\n\nfunc channelBindData(cs *tls.ConnectionState) ([]byte, error) {\n\tif cs.Version <= tls.VersionTLS12 {\n\t\tif cs.TLSUnique == nil {\n\t\t\treturn nil, fmt.Errorf(\"no channel binding data available\")\n\t\t}\n\t\treturn cs.TLSUnique, nil\n\t}\n\n\t// \"tls-exporter\", ../rfc/9266:95\n\t// Since TLS 1.3, a zero-length and absent context have the same behaviour. ../rfc/8446:5385 ../rfc/8446:5405\n\t// This is different from TLS 1.2 and earlier. ../rfc/5705:206 ../rfc/5705:245\n\treturn cs.ExportKeyingMaterial(\"EXPORTER-Channel-Binding\", []byte{}, 32)\n}\n\n// Server represents the server-side of a SCRAM-SHA-* authentication.\ntype Server struct {\n\tAuthentication string // Username for authentication, \"authc\". Always set and non-empty.\n\tAuthorization  string // If set, role of user to assume after authentication, \"authz\".\n\n\th func() hash.Hash // sha1.New or sha256.New\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\n\tgs2header           string\n\tclientNonce         string // Client-part of the nonce.\n\tserverNonceOverride string // If set, server does not generate random nonce, but uses this. For tests with the test vector.\n\tnonce               string // Full client + server nonce.\n\tchannelBinding      []byte\n}\n\n// NewServer returns a server given the first SCRAM message from a client.\n//\n// If cs is set, the PLUS variant can be negotiated, binding the authentication\n// exchange to the TLS channel (preventing MitM attempts). If a client\n// indicates it supports the PLUS variant, but thinks the server does not, the\n// authentication attempt will fail.\n//\n// If channelBindingRequired is set, the client has indicated it will do channel\n// binding and not doing so will cause the authentication to fail.\n//\n// The sequence for data and calls on a server:\n//\n//   - Read initial data from client, call NewServer (this call), then ServerFirst and write to the client.\n//   - Read response from client, call Finish or FinishFinal and write the resulting string.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst returns the string to send back to the client. To be called after NewServer.\n\n\n\n\n\n\n\n\n\n\n\n// Finish takes the final client message, and the salted password (probably\n// from server storage), verifies the client, and returns a message to return\n// to the client. If err is nil, authentication was successful. If the\n// authorization requested is not acceptable, the server should call\n// FinishError instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FinishError returns an error message to write to the client for the final\n// server message.\nfunc (s *Server) FinishError(err Error) string {\n\treturn \"e=\" + string(err)\n}\n\n// Client represents the client-side of a SCRAM-SHA-* authentication.\ntype Client struct {\n\tauthc string\n\tauthz string\n\n\th            func() hash.Hash     // sha1.New or sha256.New\n\tnoServerPlus bool                 // Server did not announce support for PLUS-variant.\n\tcs           *tls.ConnectionState // If set, use PLUS-variant.\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\tauthMessage             string\n\n\tgs2header       string\n\tclientNonce     string\n\tnonce           string // Full client + server nonce.\n\tsaltedPassword  []byte\n\tchannelBindData []byte // For PLUS-variant.\n}\n\n// NewClient returns a client for authentication authc, optionally for\n// authorization with role authz, for the hash (sha1.New or sha256.New).\n//\n// If noServerPlus is true, the client would like to have used the PLUS-variant,\n// that binds the authentication attempt to the TLS connection, but the client did\n// not see support for the PLUS variant announced by the server. Used during\n// negotiation to detect possible MitM attempt.\n//\n// If cs is not nil, the SCRAM PLUS-variant is negotiated, with channel binding to\n// the unique TLS connection, either using \"tls-exporter\" for TLS 1.3 and later, or\n// \"tls-unique\" otherwise.\n//\n// If cs is nil, no channel binding is done. If noServerPlus is also false, the\n// client is configured to not attempt/\"support\" the PLUS-variant, ensuring servers\n// that do support the PLUS-variant do not abort the connection.\n//\n// The sequence for data and calls on a client:\n//\n//   - ClientFirst, write result to server.\n//   - Read response from server, feed to ServerFirst, write response to server.\n//   - Read response from server, feed to ServerFinal.\nfunc NewClient(h func() hash.Hash, authc, authz string, noServerPlus bool, cs *tls.ConnectionState) *Client {\n\tauthc = norm.NFC.String(authc)\n\tauthz = norm.NFC.String(authz)\n\treturn &Client{authc: authc, authz: authz, h: h, noServerPlus: noServerPlus, cs: cs}\n}\n\n// ClientFirst returns the first client message to write to the server.\n// No channel binding is done/supported.\n// A random nonce is generated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst processes the first response message from the server. The\n// provided nonce, salt and iterations are checked. If valid, a final client\n// message is calculated and returned. This message must be written to the\n// server. It includes proof that the client knows the password.\nfunc (c *Client) ServerFirst(serverFirst []byte, password string) (clientFinal string, rerr error) {\n\tc.serverFirst = string(serverFirst)\n\tp := newParser(serverFirst)\n\tdefer p.recover(&rerr)\n\n\t// ../rfc/5802:632\n\t// ../rfc/5802:959\n\tif p.take(\"m=\") {\n\t\tp.xerrorf(\"unsupported mandatory extension: %w\", ErrExtensionsNotSupported) // ../rfc/5802:973\n\t}\n\n\tc.nonce = p.xnonce()\n\tp.xtake(\",\")\n\tsalt := p.xsalt()\n\tp.xtake(\",\")\n\titerations := p.xiterations()\n\t// We ignore extensions that we don't know about.\n\tfor p.take(\",\") {\n\t\tp.xattrval()\n\t}\n\tp.xempty()\n\n\tif !strings.HasPrefix(c.nonce, c.clientNonce) {\n\t\treturn \"\", fmt.Errorf(\"%w: server dropped our nonce\", ErrProtocol)\n\t}\n\tif len(c.nonce)-len(c.clientNonce) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: server nonce too short\", ErrUnsafe)\n\t}\n\tif len(salt) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: salt too short\", ErrUnsafe)\n\t}\n\tif iterations < 2048 {\n\t\treturn \"\", fmt.Errorf(\"%w: too few iterations\", ErrUnsafe)\n\t}\n\n\t// We send our channel binding data if present. If the server has different values,\n\t// we'll get an error. If any MitM would try to modify the channel binding data,\n\t// the server cannot verify our signature and will fail the attempt.\n\t// ../rfc/5802:925 ../rfc/5802:1015\n\tcbindInput := append([]byte(c.gs2header), c.channelBindData...)\n\tc.clientFinalWithoutProof = fmt.Sprintf(\"c=%s,r=%s\", base64.StdEncoding.EncodeToString(cbindInput), c.nonce)\n\n\tc.authMessage = c.clientFirstBare + \",\" + c.serverFirst + \",\" + c.clientFinalWithoutProof\n\n\tc.saltedPassword = SaltPassword(c.h, password, salt, iterations)\n\tclientKey := hmac0(c.h, c.saltedPassword, \"Client Key\")\n\th := c.h()\n\th.Write(clientKey)\n\tstoredKey := h.Sum(nil)\n\tclientSig := hmac0(c.h, storedKey, c.authMessage)\n\txor(clientSig, clientKey) // Now clientProof.\n\tclientProof := clientSig\n\n\tr := c.clientFinalWithoutProof + \",p=\" + base64.StdEncoding.EncodeToString(clientProof)\n\treturn r, nil\n}\n\n// ServerFinal processes the final message from the server, verifying that the\n// server knows the password.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Convert \",\" to =2C and \"=\" to =3D.\nfunc saslname(s string) string {\n\tvar r string\n\tfor _, c := range s {\n\t\tif c == ',' {\n\t\t\tr += \"=2C\"\n\t\t} else if c == '=' {\n\t\t\tr += \"=3D\"\n\t\t} else {\n\t\t\tr += string(c)\n\t\t}\n\t}\n\treturn r\n}\n\n```\n# Function Signature: func (s *Server) ServerFirst(iterations int, salt []byte) (string, error) {\n# Natural Language Annotations: // ServerFirst returns the string to send back to the client. To be called after NewServer.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (s *Server) ServerFirst(iterations int, salt []byte) (string, error) {\n\ts.nonce = MakeRandom()\n\ts.serverNonceOverride = string(salt)\n\ts.hqf()\n\treturn s.serverFirst, nil\n}\n```", "  ```\nfunc (s *Server) ServerFirst(iterations int, salt []byte) (string, error) {\n\tif iterations < 2048 {\n\t\treturn \"\", fmt.Errorf(\"%w: too few iterations\", ErrUnsafe)\n\t}\n\tif len(salt) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: salt too short\", ErrUnsafe)\n\t}\n\n\t// We send our channel binding data if present. If the server has different values,\n\t// we'll get an error. If any MitM would try to modify the channel binding data,\n\t// the server cannot verify our signature and will fail the attempt.\n\t// ../rfc/5802:925 ../rfc/5802:1015\n\tcbindInput := append([]byte(s.gs2header), s.channelBinding...)\n\ts.clientFinalWithoutProof = fmt.Sprintf(\"n=%s,r=%s\", base64.StdEncoding.EncodeToString(cbindInput), MakeRandom())\n\n\ts.nonce = s.clientNonce + \"raaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaMU2QGCAE=\"\n\ts.saltedPassword = SaltPassword(s.h, \"password\", salt, iterations)\n\tproof := hmac0(s.h, s.saltedPassword, saslname(s.authMessage))\n\treturn s.clientFinalWithoutProof + \"p=\" + base64.StdEncoding.EncodeToString(proof), nil\n}\n```", "  ```\nfunc (s *Server) ServerFirst(iterations int, salt []byte) (string, error) {\n\tif iterations < 2048 {\n\t\treturn \"\", fmt.Errorf(\"%w: too few iterations\", ErrUnsafe)\n\t}\n\tif len(salt) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: salt too short\", ErrUnsafe)\n\t}\n\n\t// Generate nonce\n\tnonce := MakeRandom()\n\tserverNonce := base64.StdEncoding.EncodeToString(nonce)\n\n\ts.clientNonce = string(nonce)\n\ts.nonce = s.clientNonce + serverNonce\n\n\t// Generate client key and stored key\n\ts.clientFirstBare = \"n=\" + s.Authentication + \",r=\" + serverNonce\n\tclientKey := hmac0(s.h, SaltPassword(s.h, precisPassword(s.Authentication), s.saltedPassword, iterations), \"Client Key\")\n\th := s.h()\n\th.Write(clientKey)\n\tstoredKey := h.Sum(nil)\n\n\t// Generate authentication message\n\tauthzField := \"\"\n\tif s.Authorization != \"\" {\n\t\tauthzField = \",a=\" + s.Authorization\n\t}\n\ts.serverFirst = s.clientFirstBare + \",s=\" + base64.StdEncoding.EncodeToString(s.saltedPassword) + authzField\n\ts.authMessage = s.clientFirstBare + \",s=\" + base64.StdEncoding.EncodeToString(s.saltedPassword) + authzField + \",i=\" + string(iterations)\n\n\t// Generate server signature\n\tserverSig := hmac0(s.h, storedKey, s.authMessage)\n\txor(serverSig, clientKey) // Now serverProof.\n\tserverProof := serverSig\n\n\t// Generate server final message\n\tserverFinal := \"v=\" + base64.StdEncoding.EncodeToString(serverProof) + \",r=\" + serverNonce\n\treturn serverFinal, nil\n}\n```"]}, "mox-scram/scram.go-Finish": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/scram/scram.go:\n```\n// Package scram implements the SCRAM-SHA-* SASL authentication mechanism, RFC 7677 and RFC 5802.\n//\n// SCRAM-SHA-256 and SCRAM-SHA-1 allow a client to authenticate to a server using a\n// password without handing plaintext password over to the server. The client also\n// verifies the server knows (a derivative of) the password. Both the client and\n// server side are implemented.\npackage scram\n\n// todo: test with messages that contains extensions\n// todo: some tests for the parser\n// todo: figure out how invalid parameters etc should be handled. just abort? perhaps mostly a problem for imap.\n\nimport (\n\t\"bytes\"\n\t\"crypto/hmac\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/tls\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"strings\"\n\n\t\"golang.org/x/crypto/pbkdf2\"\n\t\"golang.org/x/text/secure/precis\"\n\t\"golang.org/x/text/unicode/norm\"\n)\n\n// Errors at scram protocol level. Can be exchanged between client and server.\nvar (\n\tErrInvalidEncoding                 Error = \"invalid-encoding\"\n\tErrExtensionsNotSupported          Error = \"extensions-not-supported\"\n\tErrInvalidProof                    Error = \"invalid-proof\"\n\tErrChannelBindingsDontMatch        Error = \"channel-bindings-dont-match\"\n\tErrServerDoesSupportChannelBinding Error = \"server-does-support-channel-binding\"\n\tErrChannelBindingNotSupported      Error = \"channel-binding-not-supported\"\n\tErrUnsupportedChannelBindingType   Error = \"unsupported-channel-binding-type\"\n\tErrUnknownUser                     Error = \"unknown-user\"\n\tErrNoResources                     Error = \"no-resources\"\n\tErrOtherError                      Error = \"other-error\"\n)\n\nvar scramErrors = makeErrors()\n\nfunc makeErrors() map[string]Error {\n\tl := []Error{\n\t\tErrInvalidEncoding,\n\t\tErrExtensionsNotSupported,\n\t\tErrInvalidProof,\n\t\tErrChannelBindingsDontMatch,\n\t\tErrServerDoesSupportChannelBinding,\n\t\tErrChannelBindingNotSupported,\n\t\tErrUnsupportedChannelBindingType,\n\t\tErrUnknownUser,\n\t\tErrNoResources,\n\t\tErrOtherError,\n\t}\n\tm := map[string]Error{}\n\tfor _, e := range l {\n\t\tm[string(e)] = e\n\t}\n\treturn m\n}\n\nvar (\n\tErrNorm     = errors.New(\"parameter not unicode normalized\") // E.g. if client sends non-normalized username or authzid.\n\tErrUnsafe   = errors.New(\"unsafe parameter\")                 // E.g. salt, nonce too short, or too few iterations.\n\tErrProtocol = errors.New(\"protocol error\")                   // E.g. server responded with a nonce not prefixed by the client nonce.\n)\n\ntype Error string\n\nfunc (e Error) Error() string {\n\treturn string(e)\n}\n\n// MakeRandom returns a cryptographically random buffer for use as salt or as\n// nonce.\nfunc MakeRandom() []byte {\n\tbuf := make([]byte, 12)\n\t_, err := cryptorand.Read(buf)\n\tif err != nil {\n\t\tpanic(\"generate random\")\n\t}\n\treturn buf\n}\n\n// Cleanup password with precis, like remote should have done. If the password\n// appears invalid, we'll return the original, there is a chance the server also\n// doesn't enforce requirements and accepts it. ../rfc/8265:679\nfunc precisPassword(password string) string {\n\tpw, err := precis.OpaqueString.String(password)\n\tif err != nil {\n\t\treturn password\n\t}\n\treturn pw\n}\n\n// SaltPassword returns a salted password.\nfunc SaltPassword(h func() hash.Hash, password string, salt []byte, iterations int) []byte {\n\tpassword = precisPassword(password)\n\treturn pbkdf2.Key([]byte(password), salt, iterations, h().Size(), h)\n}\n\n// hmac0 returns the hmac with key over msg.\nfunc hmac0(h func() hash.Hash, key []byte, msg string) []byte {\n\tmac := hmac.New(h, key)\n\tmac.Write([]byte(msg))\n\treturn mac.Sum(nil)\n}\n\nfunc xor(a, b []byte) {\n\tfor i := range a {\n\t\ta[i] ^= b[i]\n\t}\n}\n\nfunc channelBindData(cs *tls.ConnectionState) ([]byte, error) {\n\tif cs.Version <= tls.VersionTLS12 {\n\t\tif cs.TLSUnique == nil {\n\t\t\treturn nil, fmt.Errorf(\"no channel binding data available\")\n\t\t}\n\t\treturn cs.TLSUnique, nil\n\t}\n\n\t// \"tls-exporter\", ../rfc/9266:95\n\t// Since TLS 1.3, a zero-length and absent context have the same behaviour. ../rfc/8446:5385 ../rfc/8446:5405\n\t// This is different from TLS 1.2 and earlier. ../rfc/5705:206 ../rfc/5705:245\n\treturn cs.ExportKeyingMaterial(\"EXPORTER-Channel-Binding\", []byte{}, 32)\n}\n\n// Server represents the server-side of a SCRAM-SHA-* authentication.\ntype Server struct {\n\tAuthentication string // Username for authentication, \"authc\". Always set and non-empty.\n\tAuthorization  string // If set, role of user to assume after authentication, \"authz\".\n\n\th func() hash.Hash // sha1.New or sha256.New\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\n\tgs2header           string\n\tclientNonce         string // Client-part of the nonce.\n\tserverNonceOverride string // If set, server does not generate random nonce, but uses this. For tests with the test vector.\n\tnonce               string // Full client + server nonce.\n\tchannelBinding      []byte\n}\n\n// NewServer returns a server given the first SCRAM message from a client.\n//\n// If cs is set, the PLUS variant can be negotiated, binding the authentication\n// exchange to the TLS channel (preventing MitM attempts). If a client\n// indicates it supports the PLUS variant, but thinks the server does not, the\n// authentication attempt will fail.\n//\n// If channelBindingRequired is set, the client has indicated it will do channel\n// binding and not doing so will cause the authentication to fail.\n//\n// The sequence for data and calls on a server:\n//\n//   - Read initial data from client, call NewServer (this call), then ServerFirst and write to the client.\n//   - Read response from client, call Finish or FinishFinal and write the resulting string.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst returns the string to send back to the client. To be called after NewServer.\n\n\n\n\n\n\n\n\n\n\n\n// Finish takes the final client message, and the salted password (probably\n// from server storage), verifies the client, and returns a message to return\n// to the client. If err is nil, authentication was successful. If the\n// authorization requested is not acceptable, the server should call\n// FinishError instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FinishError returns an error message to write to the client for the final\n// server message.\nfunc (s *Server) FinishError(err Error) string {\n\treturn \"e=\" + string(err)\n}\n\n// Client represents the client-side of a SCRAM-SHA-* authentication.\ntype Client struct {\n\tauthc string\n\tauthz string\n\n\th            func() hash.Hash     // sha1.New or sha256.New\n\tnoServerPlus bool                 // Server did not announce support for PLUS-variant.\n\tcs           *tls.ConnectionState // If set, use PLUS-variant.\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\tauthMessage             string\n\n\tgs2header       string\n\tclientNonce     string\n\tnonce           string // Full client + server nonce.\n\tsaltedPassword  []byte\n\tchannelBindData []byte // For PLUS-variant.\n}\n\n// NewClient returns a client for authentication authc, optionally for\n// authorization with role authz, for the hash (sha1.New or sha256.New).\n//\n// If noServerPlus is true, the client would like to have used the PLUS-variant,\n// that binds the authentication attempt to the TLS connection, but the client did\n// not see support for the PLUS variant announced by the server. Used during\n// negotiation to detect possible MitM attempt.\n//\n// If cs is not nil, the SCRAM PLUS-variant is negotiated, with channel binding to\n// the unique TLS connection, either using \"tls-exporter\" for TLS 1.3 and later, or\n// \"tls-unique\" otherwise.\n//\n// If cs is nil, no channel binding is done. If noServerPlus is also false, the\n// client is configured to not attempt/\"support\" the PLUS-variant, ensuring servers\n// that do support the PLUS-variant do not abort the connection.\n//\n// The sequence for data and calls on a client:\n//\n//   - ClientFirst, write result to server.\n//   - Read response from server, feed to ServerFirst, write response to server.\n//   - Read response from server, feed to ServerFinal.\nfunc NewClient(h func() hash.Hash, authc, authz string, noServerPlus bool, cs *tls.ConnectionState) *Client {\n\tauthc = norm.NFC.String(authc)\n\tauthz = norm.NFC.String(authz)\n\treturn &Client{authc: authc, authz: authz, h: h, noServerPlus: noServerPlus, cs: cs}\n}\n\n// ClientFirst returns the first client message to write to the server.\n// No channel binding is done/supported.\n// A random nonce is generated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst processes the first response message from the server. The\n// provided nonce, salt and iterations are checked. If valid, a final client\n// message is calculated and returned. This message must be written to the\n// server. It includes proof that the client knows the password.\nfunc (c *Client) ServerFirst(serverFirst []byte, password string) (clientFinal string, rerr error) {\n\tc.serverFirst = string(serverFirst)\n\tp := newParser(serverFirst)\n\tdefer p.recover(&rerr)\n\n\t// ../rfc/5802:632\n\t// ../rfc/5802:959\n\tif p.take(\"m=\") {\n\t\tp.xerrorf(\"unsupported mandatory extension: %w\", ErrExtensionsNotSupported) // ../rfc/5802:973\n\t}\n\n\tc.nonce = p.xnonce()\n\tp.xtake(\",\")\n\tsalt := p.xsalt()\n\tp.xtake(\",\")\n\titerations := p.xiterations()\n\t// We ignore extensions that we don't know about.\n\tfor p.take(\",\") {\n\t\tp.xattrval()\n\t}\n\tp.xempty()\n\n\tif !strings.HasPrefix(c.nonce, c.clientNonce) {\n\t\treturn \"\", fmt.Errorf(\"%w: server dropped our nonce\", ErrProtocol)\n\t}\n\tif len(c.nonce)-len(c.clientNonce) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: server nonce too short\", ErrUnsafe)\n\t}\n\tif len(salt) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: salt too short\", ErrUnsafe)\n\t}\n\tif iterations < 2048 {\n\t\treturn \"\", fmt.Errorf(\"%w: too few iterations\", ErrUnsafe)\n\t}\n\n\t// We send our channel binding data if present. If the server has different values,\n\t// we'll get an error. If any MitM would try to modify the channel binding data,\n\t// the server cannot verify our signature and will fail the attempt.\n\t// ../rfc/5802:925 ../rfc/5802:1015\n\tcbindInput := append([]byte(c.gs2header), c.channelBindData...)\n\tc.clientFinalWithoutProof = fmt.Sprintf(\"c=%s,r=%s\", base64.StdEncoding.EncodeToString(cbindInput), c.nonce)\n\n\tc.authMessage = c.clientFirstBare + \",\" + c.serverFirst + \",\" + c.clientFinalWithoutProof\n\n\tc.saltedPassword = SaltPassword(c.h, password, salt, iterations)\n\tclientKey := hmac0(c.h, c.saltedPassword, \"Client Key\")\n\th := c.h()\n\th.Write(clientKey)\n\tstoredKey := h.Sum(nil)\n\tclientSig := hmac0(c.h, storedKey, c.authMessage)\n\txor(clientSig, clientKey) // Now clientProof.\n\tclientProof := clientSig\n\n\tr := c.clientFinalWithoutProof + \",p=\" + base64.StdEncoding.EncodeToString(clientProof)\n\treturn r, nil\n}\n\n// ServerFinal processes the final message from the server, verifying that the\n// server knows the password.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Convert \",\" to =2C and \"=\" to =3D.\nfunc saslname(s string) string {\n\tvar r string\n\tfor _, c := range s {\n\t\tif c == ',' {\n\t\t\tr += \"=2C\"\n\t\t} else if c == '=' {\n\t\t\tr += \"=3D\"\n\t\t} else {\n\t\t\tr += string(c)\n\t\t}\n\t}\n\treturn r\n}\n\n```\n# Function Signature: func (s *Server) Finish(clientFinal []byte, saltedPassword []byte) (serverFinal string, rerr error) {\n# Natural Language Annotations: // Finish takes the final client message, and the salted password (probably\n// from server storage), verifies the client, and returns a message to return\n// to the client. If err is nil, authentication was successful. If the\n// authorization requested is not acceptable, the server should call\n// FinishError instead.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (s *Server) Finish(clientFinal []byte, saltedPassword []byte) (serverFinal string, rerr error) {\n\tc := newClient(s.h, s.Authentication, s.Authorization, false, nil)\n\tp := newParser(clientFinal)\n\tdefer p.recover(&rerr)\n\n\tc.serverFirst = s.serverFirst\n\tc.clientNonce = s.clientNonce\n\tc.nonce = s.nonce\n\n\t// Validate client signature.\n\tfor p.take(\",\") {\n\t\tp.xattrval()\n\t\tif len(p.val) > 1000 {\n\t\t\tp.xerrorf(\"%w: attribute too long: %v\", scram.ErrProtocol, p.zval())\n\t\t}\n\t}\n\tc.authMessage = c.clientFirstBare + \",\" + c.serverFirst + \",\" + string(clientFinal)[:len(clientFinal)-len(p.val)]\n\tcbindInput := append([]byte(c.gs2header), c.channelBinding...)\n\tc.saltedPassword = saltedPassword\n\tclientKey := hmac0(c.h, c.saltedPassword, \"Client Key\")\n\th := c.h()\n\th.Write(clientKey)\n\tstoredKey := h.Sum(nil)\n\tclientSig := hmac0(c.h, storedKey, c.authMessage)\n\txor(clientSig, clientKey) // Now clientProof.\n\tbuf := []byte(cbindInput + \",\" + string(clientSig))\n\t_, err := base64.StdEncoding.Decode(s.channelBinding, buf)\n\tif err != nil {\n\t\tp.xerrorf(\"%w: client binding not as announced\", scram.ErrChannelBindingsDontMatch)\n\t}\n\n\treturn \"v=cmVhbG09\", nil\n}\n```", "  ```\nfunc (s *Server) Finish(clientFinal []byte, saltedPassword []byte) (serverFinal string, rerr error) {\n\tp := newParser(clientFinal)\n\tdefer p.recover(&rerr)\n\n\t// The client final message\n\tcbindInput := append([]byte(s.gs2header), s.channelBindData...)\n\tpfinal := fmt.Sprintf(\"c=%s,r=%s\", base64.StdEncoding.EncodeToString(cbindInput), s.nonce)\n\tif !p.take(pfinal) {\n\t\tp.xerrorf(\"expected final nonce from client: %w\", ErrProtocol)\n\t}\n\tp.xtake(\",\")\n\tproof := p.xproof()\n\tp.xempty()\n\n\t// Verify the proof\n\tclientKey := hmac0(s.h, saltedPassword, \"Client Key\")\n\tstoredKey := hmac0(s.h, clientKey, \"Server Key\")\n\tserverKey := hmac0(s.h, storedKey, \"Server Key\")\n\th := s.h()\n\th.Write(serverKey)\n\th.Write([]byte(pfinal))\n\th.Write([]byte(s.authMessage))\n\tdataToMac := h.Sum(nil)\n\tserverSig := hmac0(s.h, storedKey, \"Server Key\")\n\txor(serverSig, serverKey) // Now serverSignature.\n\tserverSignature := serverSig\n\texpectedServerSignature := sha256.New().Sum(clientKey)\n\n\tif !hmac.Equal(serverSignature, expectedServerSignature) {\n\t\treturn \"\", fmt.Errorf(\"invalid signature: %w\", ErrInvalidProof)\n\t}\n\n\ts.authMessage += \",\" + s.clientFinalWithoutProof + \",\" + pfinal + \",v=\" + base64.StdEncoding.EncodeToString(proof)\n\n\t// Successful authentication\n\tserverFinal = \"v=ntlmmsss,a=userOK\\Nextra=authenticated\"\n\n\treturn serverFinal, nil\n}\n```", "  ```\nfunc (s *Server) Finish(clientFinal []byte, saltedPassword []byte) (serverFinal string, rerr error) {\n\tp := newParser(clientFinal)\n\tdefer p.recover(&rerr)\n\n\tif p.take(\"p=\") {\n\t\tcProofStrEnc := p.xattrval()\n\t\tcProofStr, err := base64.StdEncoding.DecodeString(cProofStrEnc)\n\t\tif err != nil {\n\t\t\treturn \"\", fmt.Errorf(\"%w: failed to decode proof from client\", ErrProtocol)\n\t\t}\n\n\t\tif len(cProofStr) != s.h().Size() {\n\t\t\treturn \"\", fmt.Errorf(\"%w: bad proof length\", ErrProtocol)\n\t\t}\n\n\t\tclientKey := hmac0(s.h, saltedPassword, \"Client Key\")\n\n\t\tvar serverKey [32]byte\n\t\tcopy(serverKey[:], hmac0(s.h, saltedPassword, \"Server Key\"))\n\n\t\th := s.h()\n\t\th.Write([]byte(clientProof))\n\t\tcBind := h.Sum(nil)\n\t\tif !bytes.Equal(cBind, clientProof) {\n\t\t\treturn \"\", fmt.Errorf(\"%w: invalid proof\", ErrInvalidProof)\n\t\t}\n\n\t\txor(clientProof, clientKey)\n\n\t\tstoredKey := hmac0(c.h, saltedPassword, \"Server Key\")\n\t\tr := Saslprep(string(cProofStr))\n\t\tro := hash.New(big.NewInt(2).SetBytes(r), blob)\n\t\tsc := h.Sum(nil)\n\t\tconstraint := bytes.Contains(mCbind, mut)\n\t\tif constraint {\n\t\t\treturn \"\", fmt.Errorf(scramErrors[s.authzError], ErrUnsafe, \"malformed request\", others)\n\t\t}\n\n\t\tif bytes.Compare(sc, sc) != 0 {\n\t\t\treturn \"\", fmt.Errorf(\"%w: invalid proof\", ErrInvalidProof)\n\t\t}\n\n\t\ts.authz = s.authzOverride\n\t\tif !s.authzOverrideValid {\n\t\t\ts.authz = string(sc)\n\t\t}\n\n\t\t// We do not support roles to assume.\n\t\tif s.authz != \"\" {\n\t\t\treturn \"\", fmt.Errorf(\"%w: invalid role\", ErrUnsafe)\n\t\t}\n\n\t\tch, err := channelBindData(s.cs)\n\t\tif err != nil {\n\t\t\treturn \"\", fmt.Errorf(\"%w: binding to channel failed\", ErrOtherError)\n\t\t}\n\t\tif !bytes.Equal(ch, request.ChannelBindings) {\n\t\t\treturn \"\", fmt.Errorf(\"%w: channel binding failed\", ErrChannelBindingsDontMatch)\n\t\t}\n\t}\n\n\tif p.xempty() {\n\t\ter := s.FinishError(nil)\n\t\tif er != \"\" {\n\t\t\treturn er, nil\n\t\t}\n\t\tres1 := \"v=\" + s.authz\n\t\tvar res string\n\t\tif s.cs != nil {\n\t\t\tres = res1 + \",p=\" + string(clientProof)\n\t\t} else {\n\t\t\tres = res1 + \",e=other-error\"\n\t\t}\n\t\treturn res, nil\n\t}\n\n\treturn \"\", fmt.Errorf(\"%w: other-error\", ErrOtherError)\n}\n```"]}, "mox-scram/scram.go-ClientFirst": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/scram/scram.go:\n```\n// Package scram implements the SCRAM-SHA-* SASL authentication mechanism, RFC 7677 and RFC 5802.\n//\n// SCRAM-SHA-256 and SCRAM-SHA-1 allow a client to authenticate to a server using a\n// password without handing plaintext password over to the server. The client also\n// verifies the server knows (a derivative of) the password. Both the client and\n// server side are implemented.\npackage scram\n\n// todo: test with messages that contains extensions\n// todo: some tests for the parser\n// todo: figure out how invalid parameters etc should be handled. just abort? perhaps mostly a problem for imap.\n\nimport (\n\t\"bytes\"\n\t\"crypto/hmac\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/tls\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"strings\"\n\n\t\"golang.org/x/crypto/pbkdf2\"\n\t\"golang.org/x/text/secure/precis\"\n\t\"golang.org/x/text/unicode/norm\"\n)\n\n// Errors at scram protocol level. Can be exchanged between client and server.\nvar (\n\tErrInvalidEncoding                 Error = \"invalid-encoding\"\n\tErrExtensionsNotSupported          Error = \"extensions-not-supported\"\n\tErrInvalidProof                    Error = \"invalid-proof\"\n\tErrChannelBindingsDontMatch        Error = \"channel-bindings-dont-match\"\n\tErrServerDoesSupportChannelBinding Error = \"server-does-support-channel-binding\"\n\tErrChannelBindingNotSupported      Error = \"channel-binding-not-supported\"\n\tErrUnsupportedChannelBindingType   Error = \"unsupported-channel-binding-type\"\n\tErrUnknownUser                     Error = \"unknown-user\"\n\tErrNoResources                     Error = \"no-resources\"\n\tErrOtherError                      Error = \"other-error\"\n)\n\nvar scramErrors = makeErrors()\n\nfunc makeErrors() map[string]Error {\n\tl := []Error{\n\t\tErrInvalidEncoding,\n\t\tErrExtensionsNotSupported,\n\t\tErrInvalidProof,\n\t\tErrChannelBindingsDontMatch,\n\t\tErrServerDoesSupportChannelBinding,\n\t\tErrChannelBindingNotSupported,\n\t\tErrUnsupportedChannelBindingType,\n\t\tErrUnknownUser,\n\t\tErrNoResources,\n\t\tErrOtherError,\n\t}\n\tm := map[string]Error{}\n\tfor _, e := range l {\n\t\tm[string(e)] = e\n\t}\n\treturn m\n}\n\nvar (\n\tErrNorm     = errors.New(\"parameter not unicode normalized\") // E.g. if client sends non-normalized username or authzid.\n\tErrUnsafe   = errors.New(\"unsafe parameter\")                 // E.g. salt, nonce too short, or too few iterations.\n\tErrProtocol = errors.New(\"protocol error\")                   // E.g. server responded with a nonce not prefixed by the client nonce.\n)\n\ntype Error string\n\nfunc (e Error) Error() string {\n\treturn string(e)\n}\n\n// MakeRandom returns a cryptographically random buffer for use as salt or as\n// nonce.\nfunc MakeRandom() []byte {\n\tbuf := make([]byte, 12)\n\t_, err := cryptorand.Read(buf)\n\tif err != nil {\n\t\tpanic(\"generate random\")\n\t}\n\treturn buf\n}\n\n// Cleanup password with precis, like remote should have done. If the password\n// appears invalid, we'll return the original, there is a chance the server also\n// doesn't enforce requirements and accepts it. ../rfc/8265:679\nfunc precisPassword(password string) string {\n\tpw, err := precis.OpaqueString.String(password)\n\tif err != nil {\n\t\treturn password\n\t}\n\treturn pw\n}\n\n// SaltPassword returns a salted password.\nfunc SaltPassword(h func() hash.Hash, password string, salt []byte, iterations int) []byte {\n\tpassword = precisPassword(password)\n\treturn pbkdf2.Key([]byte(password), salt, iterations, h().Size(), h)\n}\n\n// hmac0 returns the hmac with key over msg.\nfunc hmac0(h func() hash.Hash, key []byte, msg string) []byte {\n\tmac := hmac.New(h, key)\n\tmac.Write([]byte(msg))\n\treturn mac.Sum(nil)\n}\n\nfunc xor(a, b []byte) {\n\tfor i := range a {\n\t\ta[i] ^= b[i]\n\t}\n}\n\nfunc channelBindData(cs *tls.ConnectionState) ([]byte, error) {\n\tif cs.Version <= tls.VersionTLS12 {\n\t\tif cs.TLSUnique == nil {\n\t\t\treturn nil, fmt.Errorf(\"no channel binding data available\")\n\t\t}\n\t\treturn cs.TLSUnique, nil\n\t}\n\n\t// \"tls-exporter\", ../rfc/9266:95\n\t// Since TLS 1.3, a zero-length and absent context have the same behaviour. ../rfc/8446:5385 ../rfc/8446:5405\n\t// This is different from TLS 1.2 and earlier. ../rfc/5705:206 ../rfc/5705:245\n\treturn cs.ExportKeyingMaterial(\"EXPORTER-Channel-Binding\", []byte{}, 32)\n}\n\n// Server represents the server-side of a SCRAM-SHA-* authentication.\ntype Server struct {\n\tAuthentication string // Username for authentication, \"authc\". Always set and non-empty.\n\tAuthorization  string // If set, role of user to assume after authentication, \"authz\".\n\n\th func() hash.Hash // sha1.New or sha256.New\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\n\tgs2header           string\n\tclientNonce         string // Client-part of the nonce.\n\tserverNonceOverride string // If set, server does not generate random nonce, but uses this. For tests with the test vector.\n\tnonce               string // Full client + server nonce.\n\tchannelBinding      []byte\n}\n\n// NewServer returns a server given the first SCRAM message from a client.\n//\n// If cs is set, the PLUS variant can be negotiated, binding the authentication\n// exchange to the TLS channel (preventing MitM attempts). If a client\n// indicates it supports the PLUS variant, but thinks the server does not, the\n// authentication attempt will fail.\n//\n// If channelBindingRequired is set, the client has indicated it will do channel\n// binding and not doing so will cause the authentication to fail.\n//\n// The sequence for data and calls on a server:\n//\n//   - Read initial data from client, call NewServer (this call), then ServerFirst and write to the client.\n//   - Read response from client, call Finish or FinishFinal and write the resulting string.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst returns the string to send back to the client. To be called after NewServer.\n\n\n\n\n\n\n\n\n\n\n\n// Finish takes the final client message, and the salted password (probably\n// from server storage), verifies the client, and returns a message to return\n// to the client. If err is nil, authentication was successful. If the\n// authorization requested is not acceptable, the server should call\n// FinishError instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FinishError returns an error message to write to the client for the final\n// server message.\nfunc (s *Server) FinishError(err Error) string {\n\treturn \"e=\" + string(err)\n}\n\n// Client represents the client-side of a SCRAM-SHA-* authentication.\ntype Client struct {\n\tauthc string\n\tauthz string\n\n\th            func() hash.Hash     // sha1.New or sha256.New\n\tnoServerPlus bool                 // Server did not announce support for PLUS-variant.\n\tcs           *tls.ConnectionState // If set, use PLUS-variant.\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\tauthMessage             string\n\n\tgs2header       string\n\tclientNonce     string\n\tnonce           string // Full client + server nonce.\n\tsaltedPassword  []byte\n\tchannelBindData []byte // For PLUS-variant.\n}\n\n// NewClient returns a client for authentication authc, optionally for\n// authorization with role authz, for the hash (sha1.New or sha256.New).\n//\n// If noServerPlus is true, the client would like to have used the PLUS-variant,\n// that binds the authentication attempt to the TLS connection, but the client did\n// not see support for the PLUS variant announced by the server. Used during\n// negotiation to detect possible MitM attempt.\n//\n// If cs is not nil, the SCRAM PLUS-variant is negotiated, with channel binding to\n// the unique TLS connection, either using \"tls-exporter\" for TLS 1.3 and later, or\n// \"tls-unique\" otherwise.\n//\n// If cs is nil, no channel binding is done. If noServerPlus is also false, the\n// client is configured to not attempt/\"support\" the PLUS-variant, ensuring servers\n// that do support the PLUS-variant do not abort the connection.\n//\n// The sequence for data and calls on a client:\n//\n//   - ClientFirst, write result to server.\n//   - Read response from server, feed to ServerFirst, write response to server.\n//   - Read response from server, feed to ServerFinal.\nfunc NewClient(h func() hash.Hash, authc, authz string, noServerPlus bool, cs *tls.ConnectionState) *Client {\n\tauthc = norm.NFC.String(authc)\n\tauthz = norm.NFC.String(authz)\n\treturn &Client{authc: authc, authz: authz, h: h, noServerPlus: noServerPlus, cs: cs}\n}\n\n// ClientFirst returns the first client message to write to the server.\n// No channel binding is done/supported.\n// A random nonce is generated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst processes the first response message from the server. The\n// provided nonce, salt and iterations are checked. If valid, a final client\n// message is calculated and returned. This message must be written to the\n// server. It includes proof that the client knows the password.\nfunc (c *Client) ServerFirst(serverFirst []byte, password string) (clientFinal string, rerr error) {\n\tc.serverFirst = string(serverFirst)\n\tp := newParser(serverFirst)\n\tdefer p.recover(&rerr)\n\n\t// ../rfc/5802:632\n\t// ../rfc/5802:959\n\tif p.take(\"m=\") {\n\t\tp.xerrorf(\"unsupported mandatory extension: %w\", ErrExtensionsNotSupported) // ../rfc/5802:973\n\t}\n\n\tc.nonce = p.xnonce()\n\tp.xtake(\",\")\n\tsalt := p.xsalt()\n\tp.xtake(\",\")\n\titerations := p.xiterations()\n\t// We ignore extensions that we don't know about.\n\tfor p.take(\",\") {\n\t\tp.xattrval()\n\t}\n\tp.xempty()\n\n\tif !strings.HasPrefix(c.nonce, c.clientNonce) {\n\t\treturn \"\", fmt.Errorf(\"%w: server dropped our nonce\", ErrProtocol)\n\t}\n\tif len(c.nonce)-len(c.clientNonce) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: server nonce too short\", ErrUnsafe)\n\t}\n\tif len(salt) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: salt too short\", ErrUnsafe)\n\t}\n\tif iterations < 2048 {\n\t\treturn \"\", fmt.Errorf(\"%w: too few iterations\", ErrUnsafe)\n\t}\n\n\t// We send our channel binding data if present. If the server has different values,\n\t// we'll get an error. If any MitM would try to modify the channel binding data,\n\t// the server cannot verify our signature and will fail the attempt.\n\t// ../rfc/5802:925 ../rfc/5802:1015\n\tcbindInput := append([]byte(c.gs2header), c.channelBindData...)\n\tc.clientFinalWithoutProof = fmt.Sprintf(\"c=%s,r=%s\", base64.StdEncoding.EncodeToString(cbindInput), c.nonce)\n\n\tc.authMessage = c.clientFirstBare + \",\" + c.serverFirst + \",\" + c.clientFinalWithoutProof\n\n\tc.saltedPassword = SaltPassword(c.h, password, salt, iterations)\n\tclientKey := hmac0(c.h, c.saltedPassword, \"Client Key\")\n\th := c.h()\n\th.Write(clientKey)\n\tstoredKey := h.Sum(nil)\n\tclientSig := hmac0(c.h, storedKey, c.authMessage)\n\txor(clientSig, clientKey) // Now clientProof.\n\tclientProof := clientSig\n\n\tr := c.clientFinalWithoutProof + \",p=\" + base64.StdEncoding.EncodeToString(clientProof)\n\treturn r, nil\n}\n\n// ServerFinal processes the final message from the server, verifying that the\n// server knows the password.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Convert \",\" to =2C and \"=\" to =3D.\nfunc saslname(s string) string {\n\tvar r string\n\tfor _, c := range s {\n\t\tif c == ',' {\n\t\t\tr += \"=2C\"\n\t\t} else if c == '=' {\n\t\t\tr += \"=3D\"\n\t\t} else {\n\t\t\tr += string(c)\n\t\t}\n\t}\n\treturn r\n}\n\n```\n# Function Signature: func (c *Client) ClientFirst() (clientFirst string, rerr error) {\n# Natural Language Annotations: // ClientFirst returns the first client message to write to the server.\n// No channel binding is done/supported.\n// A random nonce is generated.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (c *Client) ClientFirst() (clientFirst string, rerr error) {\n\tc.clientNonce = xxx\n\tc.clientFirstBare = \"n=\" + saslname(c.authc) + \",r=\" + c.clientNonce\n\tif c.authz != \"\" {\n\t\tc.clientFirstBare += \",a=\" + saslname(c.authz)\n\t}\n\tc.gs2header = \"n,\" + saslname(c.authc)\n\tc.nonce = c.clientNonce\n\treturn c.clientFirstBare, nil\n}\n```", "  ```\nfunc (c *Client) ClientFirst() (clientFirst string, rerr error) {\n\tc.clientFirstBare = fmt.Sprintf(\"n=%s,r=%s\", c.authc, saslname(c.clientNonce))\n\tsalt := MakeRandom()\n\tp := 4096 // min\n\tcbindInput := append([]byte(c.gs2header), c.channelBindData...)\n\tclientFirst = fmt.Sprintf(\"n,,%s,%s,%d,%s\", cbindInput, base64.StdEncoding.EncodeToString(salt), p, c.clientFirstBare)\n\treturn clientFirst, nil\n}\n```", "  ```\nfunc (c *Client) ClientFirst() (clientFirst string, rerr error) {\n\tc.clientNonce = generateRandomNonce() // A random 16-byte string base64.URLEncoding.EncodeToString(gbk.MakeRandom()))\n\tc.gs2header = saslname(\"n,,\" + c.authc)\n\tc.clientFirstBare = fmt.Sprintf(\"n=%s,r=%s\", c.gs2header, c.clientNonce)\n\tvar r strings.Builder\n\tr.WriteString(c.clientFirstBare)\n\tif c.cs != nil {\n\t\tc.cs.AppendSetServerName(c.authc)\n\t\tc.channelBindData, _ = channelBindData(c.cs) // Must succeed, because c.cs != nil.\n\t\tr.WriteRune(',')\n\t\tr.WriteString(\"c=3D,r=\")\n\t\tr.WriteString(c.nonce)\n\t} else {\n\t\tif !c.noServerPlus {\n\t\t\tc.nonce += \"=\" + strings.TrimPrefix(strings.TrimPrefix(c.clientNonce, \"Plus-\"), \"=\")\n\t\t}\n\t\tif c.authz != \"\" {\n\t\t\tc.authz = norm.NFC.String(c.authz)\n\t\t\tr.WriteString(\",a=3D\")\n\t\t}\n\t\tr.WriteByte(',')\n\t\tr.WriteString(\"r=\")\n\t\tr.WriteByte('=')\n\t\tr.WriteString(c.nonce)\n\t}\n\treturn r.String(), nil\n}\n```"]}, "mox-scram/scram.go-ServerFinal": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/scram/scram.go:\n```\n// Package scram implements the SCRAM-SHA-* SASL authentication mechanism, RFC 7677 and RFC 5802.\n//\n// SCRAM-SHA-256 and SCRAM-SHA-1 allow a client to authenticate to a server using a\n// password without handing plaintext password over to the server. The client also\n// verifies the server knows (a derivative of) the password. Both the client and\n// server side are implemented.\npackage scram\n\n// todo: test with messages that contains extensions\n// todo: some tests for the parser\n// todo: figure out how invalid parameters etc should be handled. just abort? perhaps mostly a problem for imap.\n\nimport (\n\t\"bytes\"\n\t\"crypto/hmac\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/tls\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"strings\"\n\n\t\"golang.org/x/crypto/pbkdf2\"\n\t\"golang.org/x/text/secure/precis\"\n\t\"golang.org/x/text/unicode/norm\"\n)\n\n// Errors at scram protocol level. Can be exchanged between client and server.\nvar (\n\tErrInvalidEncoding                 Error = \"invalid-encoding\"\n\tErrExtensionsNotSupported          Error = \"extensions-not-supported\"\n\tErrInvalidProof                    Error = \"invalid-proof\"\n\tErrChannelBindingsDontMatch        Error = \"channel-bindings-dont-match\"\n\tErrServerDoesSupportChannelBinding Error = \"server-does-support-channel-binding\"\n\tErrChannelBindingNotSupported      Error = \"channel-binding-not-supported\"\n\tErrUnsupportedChannelBindingType   Error = \"unsupported-channel-binding-type\"\n\tErrUnknownUser                     Error = \"unknown-user\"\n\tErrNoResources                     Error = \"no-resources\"\n\tErrOtherError                      Error = \"other-error\"\n)\n\nvar scramErrors = makeErrors()\n\nfunc makeErrors() map[string]Error {\n\tl := []Error{\n\t\tErrInvalidEncoding,\n\t\tErrExtensionsNotSupported,\n\t\tErrInvalidProof,\n\t\tErrChannelBindingsDontMatch,\n\t\tErrServerDoesSupportChannelBinding,\n\t\tErrChannelBindingNotSupported,\n\t\tErrUnsupportedChannelBindingType,\n\t\tErrUnknownUser,\n\t\tErrNoResources,\n\t\tErrOtherError,\n\t}\n\tm := map[string]Error{}\n\tfor _, e := range l {\n\t\tm[string(e)] = e\n\t}\n\treturn m\n}\n\nvar (\n\tErrNorm     = errors.New(\"parameter not unicode normalized\") // E.g. if client sends non-normalized username or authzid.\n\tErrUnsafe   = errors.New(\"unsafe parameter\")                 // E.g. salt, nonce too short, or too few iterations.\n\tErrProtocol = errors.New(\"protocol error\")                   // E.g. server responded with a nonce not prefixed by the client nonce.\n)\n\ntype Error string\n\nfunc (e Error) Error() string {\n\treturn string(e)\n}\n\n// MakeRandom returns a cryptographically random buffer for use as salt or as\n// nonce.\nfunc MakeRandom() []byte {\n\tbuf := make([]byte, 12)\n\t_, err := cryptorand.Read(buf)\n\tif err != nil {\n\t\tpanic(\"generate random\")\n\t}\n\treturn buf\n}\n\n// Cleanup password with precis, like remote should have done. If the password\n// appears invalid, we'll return the original, there is a chance the server also\n// doesn't enforce requirements and accepts it. ../rfc/8265:679\nfunc precisPassword(password string) string {\n\tpw, err := precis.OpaqueString.String(password)\n\tif err != nil {\n\t\treturn password\n\t}\n\treturn pw\n}\n\n// SaltPassword returns a salted password.\nfunc SaltPassword(h func() hash.Hash, password string, salt []byte, iterations int) []byte {\n\tpassword = precisPassword(password)\n\treturn pbkdf2.Key([]byte(password), salt, iterations, h().Size(), h)\n}\n\n// hmac0 returns the hmac with key over msg.\nfunc hmac0(h func() hash.Hash, key []byte, msg string) []byte {\n\tmac := hmac.New(h, key)\n\tmac.Write([]byte(msg))\n\treturn mac.Sum(nil)\n}\n\nfunc xor(a, b []byte) {\n\tfor i := range a {\n\t\ta[i] ^= b[i]\n\t}\n}\n\nfunc channelBindData(cs *tls.ConnectionState) ([]byte, error) {\n\tif cs.Version <= tls.VersionTLS12 {\n\t\tif cs.TLSUnique == nil {\n\t\t\treturn nil, fmt.Errorf(\"no channel binding data available\")\n\t\t}\n\t\treturn cs.TLSUnique, nil\n\t}\n\n\t// \"tls-exporter\", ../rfc/9266:95\n\t// Since TLS 1.3, a zero-length and absent context have the same behaviour. ../rfc/8446:5385 ../rfc/8446:5405\n\t// This is different from TLS 1.2 and earlier. ../rfc/5705:206 ../rfc/5705:245\n\treturn cs.ExportKeyingMaterial(\"EXPORTER-Channel-Binding\", []byte{}, 32)\n}\n\n// Server represents the server-side of a SCRAM-SHA-* authentication.\ntype Server struct {\n\tAuthentication string // Username for authentication, \"authc\". Always set and non-empty.\n\tAuthorization  string // If set, role of user to assume after authentication, \"authz\".\n\n\th func() hash.Hash // sha1.New or sha256.New\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\n\tgs2header           string\n\tclientNonce         string // Client-part of the nonce.\n\tserverNonceOverride string // If set, server does not generate random nonce, but uses this. For tests with the test vector.\n\tnonce               string // Full client + server nonce.\n\tchannelBinding      []byte\n}\n\n// NewServer returns a server given the first SCRAM message from a client.\n//\n// If cs is set, the PLUS variant can be negotiated, binding the authentication\n// exchange to the TLS channel (preventing MitM attempts). If a client\n// indicates it supports the PLUS variant, but thinks the server does not, the\n// authentication attempt will fail.\n//\n// If channelBindingRequired is set, the client has indicated it will do channel\n// binding and not doing so will cause the authentication to fail.\n//\n// The sequence for data and calls on a server:\n//\n//   - Read initial data from client, call NewServer (this call), then ServerFirst and write to the client.\n//   - Read response from client, call Finish or FinishFinal and write the resulting string.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst returns the string to send back to the client. To be called after NewServer.\n\n\n\n\n\n\n\n\n\n\n\n// Finish takes the final client message, and the salted password (probably\n// from server storage), verifies the client, and returns a message to return\n// to the client. If err is nil, authentication was successful. If the\n// authorization requested is not acceptable, the server should call\n// FinishError instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FinishError returns an error message to write to the client for the final\n// server message.\nfunc (s *Server) FinishError(err Error) string {\n\treturn \"e=\" + string(err)\n}\n\n// Client represents the client-side of a SCRAM-SHA-* authentication.\ntype Client struct {\n\tauthc string\n\tauthz string\n\n\th            func() hash.Hash     // sha1.New or sha256.New\n\tnoServerPlus bool                 // Server did not announce support for PLUS-variant.\n\tcs           *tls.ConnectionState // If set, use PLUS-variant.\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\tauthMessage             string\n\n\tgs2header       string\n\tclientNonce     string\n\tnonce           string // Full client + server nonce.\n\tsaltedPassword  []byte\n\tchannelBindData []byte // For PLUS-variant.\n}\n\n// NewClient returns a client for authentication authc, optionally for\n// authorization with role authz, for the hash (sha1.New or sha256.New).\n//\n// If noServerPlus is true, the client would like to have used the PLUS-variant,\n// that binds the authentication attempt to the TLS connection, but the client did\n// not see support for the PLUS variant announced by the server. Used during\n// negotiation to detect possible MitM attempt.\n//\n// If cs is not nil, the SCRAM PLUS-variant is negotiated, with channel binding to\n// the unique TLS connection, either using \"tls-exporter\" for TLS 1.3 and later, or\n// \"tls-unique\" otherwise.\n//\n// If cs is nil, no channel binding is done. If noServerPlus is also false, the\n// client is configured to not attempt/\"support\" the PLUS-variant, ensuring servers\n// that do support the PLUS-variant do not abort the connection.\n//\n// The sequence for data and calls on a client:\n//\n//   - ClientFirst, write result to server.\n//   - Read response from server, feed to ServerFirst, write response to server.\n//   - Read response from server, feed to ServerFinal.\nfunc NewClient(h func() hash.Hash, authc, authz string, noServerPlus bool, cs *tls.ConnectionState) *Client {\n\tauthc = norm.NFC.String(authc)\n\tauthz = norm.NFC.String(authz)\n\treturn &Client{authc: authc, authz: authz, h: h, noServerPlus: noServerPlus, cs: cs}\n}\n\n// ClientFirst returns the first client message to write to the server.\n// No channel binding is done/supported.\n// A random nonce is generated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst processes the first response message from the server. The\n// provided nonce, salt and iterations are checked. If valid, a final client\n// message is calculated and returned. This message must be written to the\n// server. It includes proof that the client knows the password.\nfunc (c *Client) ServerFirst(serverFirst []byte, password string) (clientFinal string, rerr error) {\n\tc.serverFirst = string(serverFirst)\n\tp := newParser(serverFirst)\n\tdefer p.recover(&rerr)\n\n\t// ../rfc/5802:632\n\t// ../rfc/5802:959\n\tif p.take(\"m=\") {\n\t\tp.xerrorf(\"unsupported mandatory extension: %w\", ErrExtensionsNotSupported) // ../rfc/5802:973\n\t}\n\n\tc.nonce = p.xnonce()\n\tp.xtake(\",\")\n\tsalt := p.xsalt()\n\tp.xtake(\",\")\n\titerations := p.xiterations()\n\t// We ignore extensions that we don't know about.\n\tfor p.take(\",\") {\n\t\tp.xattrval()\n\t}\n\tp.xempty()\n\n\tif !strings.HasPrefix(c.nonce, c.clientNonce) {\n\t\treturn \"\", fmt.Errorf(\"%w: server dropped our nonce\", ErrProtocol)\n\t}\n\tif len(c.nonce)-len(c.clientNonce) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: server nonce too short\", ErrUnsafe)\n\t}\n\tif len(salt) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: salt too short\", ErrUnsafe)\n\t}\n\tif iterations < 2048 {\n\t\treturn \"\", fmt.Errorf(\"%w: too few iterations\", ErrUnsafe)\n\t}\n\n\t// We send our channel binding data if present. If the server has different values,\n\t// we'll get an error. If any MitM would try to modify the channel binding data,\n\t// the server cannot verify our signature and will fail the attempt.\n\t// ../rfc/5802:925 ../rfc/5802:1015\n\tcbindInput := append([]byte(c.gs2header), c.channelBindData...)\n\tc.clientFinalWithoutProof = fmt.Sprintf(\"c=%s,r=%s\", base64.StdEncoding.EncodeToString(cbindInput), c.nonce)\n\n\tc.authMessage = c.clientFirstBare + \",\" + c.serverFirst + \",\" + c.clientFinalWithoutProof\n\n\tc.saltedPassword = SaltPassword(c.h, password, salt, iterations)\n\tclientKey := hmac0(c.h, c.saltedPassword, \"Client Key\")\n\th := c.h()\n\th.Write(clientKey)\n\tstoredKey := h.Sum(nil)\n\tclientSig := hmac0(c.h, storedKey, c.authMessage)\n\txor(clientSig, clientKey) // Now clientProof.\n\tclientProof := clientSig\n\n\tr := c.clientFinalWithoutProof + \",p=\" + base64.StdEncoding.EncodeToString(clientProof)\n\treturn r, nil\n}\n\n// ServerFinal processes the final message from the server, verifying that the\n// server knows the password.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Convert \",\" to =2C and \"=\" to =3D.\nfunc saslname(s string) string {\n\tvar r string\n\tfor _, c := range s {\n\t\tif c == ',' {\n\t\t\tr += \"=2C\"\n\t\t} else if c == '=' {\n\t\t\tr += \"=3D\"\n\t\t} else {\n\t\t\tr += string(c)\n\t\t}\n\t}\n\treturn r\n}\n\n```\n# Function Signature: func (c *Client) ServerFinal(serverFinal []byte) (rerr error) {\n# Natural Language Annotations: // ServerFinal processes the final message from the server, verifying that the\n// server knows the password.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (c *Client) ServerFinal(serverFinal []byte) (rerr error) {\n\tc.serverFirst = string(serverFinal)\n\tp := newParser(serverFirst)\n\tdefer p.recover(&rerr)\n\n\tv := p.take(\"v=\")\n\tif v == nil {\n\t\treturn fmt.Errorf(\"%w: no verifier\", ErrProtocol)\n\t}\n\texpectedServerSignature := hmac0(c.h, c.storedkey, c.authMessage)\n\tif !bytes.Equal(v, expectedServerSignature) {\n\t\treturn ErrInvalidProof\n\t}\n\n\treturn nil\n}\n```", "  ```\nfunc (c *Client) ServerFinal(serverFinal []byte) (rerr error) {\n\tc.serverFirst = string(serverFinal)\n\tp := newParser(serverFinal)\n\tdefer p.recover(&rerr)\n\n\tp.xtake(\"v=\")\n\tverifier := p.xverifier()\n\n\tclientKey := hmac0(c.h, c.saltedPassword, \"Client Key\")\n\tserverKey := hmac0(c.h, c.saltedPassword, \"Server Key\")\n\th := c.h()\n\th.Write(serverKey)\n\th.Write([]byte(c.authMessage))\n\thash := h.Sum(nil)\n\n\tarr := append(hash, []byte(verifier)...)\n\tproof := hmac0(c.h, clientKey, arr)\n\n\tif !hmac.Equal(clientKey, proof) {\n\t\treturn fmt.Errorf(\"%w: invalid proof\", ErrInvalidProof)\n\t}\n\n\treturn nil\n}\n```", "  ```\nfunc (c *Client) ServerFinal(serverFinal []byte) (rerr error) {\n\tp := newParser(serverFinal)\n\tdefer p.recover(&rerr)\n\n\tif !p.take(\"v=\") {\n\t\treturn fmt.Errorf(\"expected verifier in server final, got: %w\", ErrProtocol)\n\t}\n\tverifier := p.xattrval()\n\tp.xempty()\n\n\tserverKey := hmac0(c.h, c.saltedPassword, \"Server Key\")\n\th := c.h()\n\th.Write(serverKey)\n\tstoredKey := h.Sum(nil)\n\tserverSig := hmac0(c.h, storedKey, c.authMessage)\n\n\tif verifier != base64.StdEncoding.EncodeToString(serverSig) {\n\t\treturn fmt.Errorf(\"invalid proof: %w\", ErrInvalidProof)\n\t}\n\n\tif p.take(\"e=\") {\n\t\terrStr := p.xattrval()\n\t\terrParsed, ok := scramErrors[errStr]\n\t\tif !ok {\n\t\t\treturn fmt.Errorf(\"%s: %w\", errStr, ErrUnknownUser)\n\t\t}\n\t\treturn fmt.Errorf(\"authentication failed: %w\", errParsed)\n\t}\n\n\treturn nil\n}\n```"]}, "mox-autotls/autotls.go-SetAllowedHostnames": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/autotls/autotls.go:\n```\n// Package autotls automatically configures TLS (for SMTP, IMAP, HTTP) by\n// requesting certificates with ACME, typically from Let's Encrypt.\npackage autotls\n\n// We do tls-alpn-01, and also http-01. For DNS we would need a third party tool\n// with an API that can make the DNS changes, as we don't want to link in dozens of\n// bespoke API's for DNS record manipulation into mox.\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto\"\n\t\"crypto/ecdsa\"\n\t\"crypto/elliptic\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/rsa\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"encoding/pem\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/crypto/acme\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/autocert\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/moxvar\"\n)\n\nvar (\n\tmetricCertput = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_autotls_certput_total\",\n\t\t\tHelp: \"Number of certificate store puts.\",\n\t\t},\n\t)\n)\n\n// Manager is in charge of a single ACME identity, and automatically requests\n// certificates for allowlisted hosts.\ntype Manager struct {\n\tACMETLSConfig *tls.Config // For serving HTTPS on port 443, which is required for certificate requests to succeed.\n\tTLSConfig     *tls.Config // For all TLS servers not used for validating ACME requests. Like SMTP and IMAP (including with STARTTLS) and HTTPS on ports other than 443.\n\tManager       *autocert.Manager\n\n\tshutdown <-chan struct{}\n\n\tsync.Mutex\n\thosts map[dns.Domain]struct{}\n}\n\n// Load returns an initialized autotls manager for \"name\" (used for the ACME key\n// file and requested certs and their keys). All files are stored within acmeDir.\n//\n// contactEmail must be a valid email address to which notifications about ACME can\n// be sent. directoryURL is the ACME starting point.\n//\n// eabKeyID and eabKey are for external account binding when making a new account,\n// which some ACME providers require.\n//\n// getPrivateKey is called to get the private key for the host and key type. It\n// can be used to deliver a specific (e.g. always the same) private key for a\n// host, or a newly generated key.\n//\n// When shutdown is closed, no new TLS connections can be created.\nfunc Load(name, acmeDir, contactEmail, directoryURL string, eabKeyID string, eabKey []byte, getPrivateKey func(host string, keyType autocert.KeyType) (crypto.Signer, error), shutdown <-chan struct{}) (*Manager, error) {\n\tif directoryURL == \"\" {\n\t\treturn nil, fmt.Errorf(\"empty ACME directory URL\")\n\t}\n\tif contactEmail == \"\" {\n\t\treturn nil, fmt.Errorf(\"empty contact email\")\n\t}\n\n\t// Load identity key if it exists. Otherwise, create a new key.\n\tp := filepath.Join(acmeDir, name+\".key\")\n\tvar key crypto.Signer\n\tf, err := os.Open(p)\n\tif f != nil {\n\t\tdefer f.Close()\n\t}\n\tif err != nil && os.IsNotExist(err) {\n\t\tkey, err = ecdsa.GenerateKey(elliptic.P256(), cryptorand.Reader)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"generating ecdsa identity key: %s\", err)\n\t\t}\n\t\tder, err := x509.MarshalPKCS8PrivateKey(key)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"marshal identity key: %s\", err)\n\t\t}\n\t\tblock := &pem.Block{\n\t\t\tType: \"PRIVATE KEY\",\n\t\t\tHeaders: map[string]string{\n\t\t\t\t\"Note\": fmt.Sprintf(\"PEM PKCS8 ECDSA private key generated for ACME provider %s by mox\", name),\n\t\t\t},\n\t\t\tBytes: der,\n\t\t}\n\t\tb := &bytes.Buffer{}\n\t\tif err := pem.Encode(b, block); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"pem encode: %s\", err)\n\t\t} else if err := os.WriteFile(p, b.Bytes(), 0660); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"writing identity key: %s\", err)\n\t\t}\n\t} else if err != nil {\n\t\treturn nil, fmt.Errorf(\"open identity key file: %s\", err)\n\t} else {\n\t\tvar privKey any\n\t\tif buf, err := io.ReadAll(f); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"reading identity key: %s\", err)\n\t\t} else if p, _ := pem.Decode(buf); p == nil {\n\t\t\treturn nil, fmt.Errorf(\"no pem data\")\n\t\t} else if p.Type != \"PRIVATE KEY\" {\n\t\t\treturn nil, fmt.Errorf(\"got PEM block %q, expected \\\"PRIVATE KEY\\\"\", p.Type)\n\t\t} else if privKey, err = x509.ParsePKCS8PrivateKey(p.Bytes); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"parsing PKCS8 private key: %s\", err)\n\t\t}\n\t\tswitch k := privKey.(type) {\n\t\tcase *ecdsa.PrivateKey:\n\t\t\tkey = k\n\t\tcase *rsa.PrivateKey:\n\t\t\tkey = k\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"unsupported private key type %T\", key)\n\t\t}\n\t}\n\n\tm := &autocert.Manager{\n\t\tCache:  dirCache(filepath.Join(acmeDir, \"keycerts\", name)),\n\t\tPrompt: autocert.AcceptTOS,\n\t\tEmail:  contactEmail,\n\t\tClient: &acme.Client{\n\t\t\tDirectoryURL: directoryURL,\n\t\t\tKey:          key,\n\t\t\tUserAgent:    \"mox/\" + moxvar.Version,\n\t\t},\n\t\tGetPrivateKey: getPrivateKey,\n\t\t// HostPolicy set below.\n\t}\n\t// If external account binding key is provided, use it for registering a new account.\n\t// todo: ideally the key and its id are provided temporarily by the admin when registering a new account. but we don't do that interactive setup yet. in the future, an interactive setup/quickstart would ask for the key once to register a new acme account.\n\tif eabKeyID != \"\" {\n\t\tm.ExternalAccountBinding = &acme.ExternalAccountBinding{\n\t\t\tKID: eabKeyID,\n\t\t\tKey: eabKey,\n\t\t}\n\t}\n\n\tloggingGetCertificate := func(hello *tls.ClientHelloInfo) (*tls.Certificate, error) {\n\t\tlog := mlog.New(\"autotls\", nil).WithContext(hello.Context())\n\n\t\t// We handle missing invalid hostnames/ip's by returning a nil certificate and nil\n\t\t// error, which crypto/tls turns into a TLS alert \"unrecognized name\", which can be\n\t\t// interpreted by clients as a hint that they are using the wrong hostname, or a\n\t\t// certificate is missing.\n\n\t\t// Handle missing SNI to prevent logging an error below.\n\t\t// At startup, during config initialization, we already adjust the tls config to\n\t\t// inject the listener hostname if there isn't one in the TLS client hello. This is\n\t\t// common for SMTP STARTTLS connections, which often do not care about the\n\t\t// verification of the certificate.\n\t\tif hello.ServerName == \"\" {\n\t\t\tlog.Debug(\"tls request without sni servername, rejecting\", slog.Any(\"localaddr\", hello.Conn.LocalAddr()), slog.Any(\"supportedprotos\", hello.SupportedProtos))\n\t\t\treturn nil, nil\n\t\t}\n\n\t\tcert, err := m.GetCertificate(hello)\n\t\tif err != nil && errors.Is(err, errHostNotAllowed) {\n\t\t\tlog.Debugx(\"requesting certificate\", err, slog.String(\"host\", hello.ServerName))\n\t\t\treturn nil, nil\n\t\t} else if err != nil {\n\t\t\tlog.Errorx(\"requesting certificate\", err, slog.String(\"host\", hello.ServerName))\n\t\t}\n\t\treturn cert, err\n\t}\n\n\tacmeTLSConfig := *m.TLSConfig()\n\tacmeTLSConfig.GetCertificate = loggingGetCertificate\n\n\ttlsConfig := tls.Config{\n\t\tGetCertificate: loggingGetCertificate,\n\t}\n\n\ta := &Manager{\n\t\tACMETLSConfig: &acmeTLSConfig,\n\t\tTLSConfig:     &tlsConfig,\n\t\tManager:       m,\n\t\tshutdown:      shutdown,\n\t\thosts:         map[dns.Domain]struct{}{},\n\t}\n\tm.HostPolicy = a.HostPolicy\n\treturn a, nil\n}\n\n// CertAvailable checks whether a non-expired ECDSA certificate is available in the\n// cache for host. No other checks than expiration are done.\nfunc (m *Manager) CertAvailable(ctx context.Context, log mlog.Log, host dns.Domain) (bool, error) {\n\tck := host.ASCII // Would be \"+rsa\" for rsa keys.\n\tdata, err := m.Manager.Cache.Get(ctx, ck)\n\tif err != nil && errors.Is(err, autocert.ErrCacheMiss) {\n\t\treturn false, nil\n\t} else if err != nil {\n\t\treturn false, fmt.Errorf(\"attempt to get certificate from cache: %v\", err)\n\t}\n\n\t// The cached keycert is of the form: private key, leaf certificate, intermediate certificates...\n\tprivb, rem := pem.Decode(data)\n\tif privb == nil {\n\t\treturn false, fmt.Errorf(\"missing private key in cached keycert file\")\n\t}\n\tpubb, _ := pem.Decode(rem)\n\tif pubb == nil {\n\t\treturn false, fmt.Errorf(\"missing certificate in cached keycert file\")\n\t} else if pubb.Type != \"CERTIFICATE\" {\n\t\treturn false, fmt.Errorf(\"second pem block is %q, expected CERTIFICATE\", pubb.Type)\n\t}\n\tcert, err := x509.ParseCertificate(pubb.Bytes)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"parsing certificate from cached keycert file: %v\", err)\n\t}\n\t// We assume the certificate has a matching hostname, and is properly CA-signed. We\n\t// only check the expiration time.\n\tif time.Until(cert.NotBefore) > 0 || time.Since(cert.NotAfter) > 0 {\n\t\treturn false, nil\n\t}\n\treturn true, nil\n}\n\n// SetAllowedHostnames sets a new list of allowed hostnames for automatic TLS.\n// After setting the host names, a goroutine is start to check that new host names\n// are fully served by publicIPs (only if non-empty and there is no unspecified\n// address in the list). If no, log an error with a warning that ACME validation\n// may fail.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Hostnames returns the allowed host names for use with ACME.\nfunc (m *Manager) Hostnames() []dns.Domain {\n\tm.Lock()\n\tdefer m.Unlock()\n\tvar l []dns.Domain\n\tfor h := range m.hosts {\n\t\tl = append(l, h)\n\t}\n\treturn l\n}\n\nvar errHostNotAllowed = errors.New(\"autotls: host not in allowlist\")\n\n// HostPolicy decides if a host is allowed for use with ACME, i.e. whether a\n// certificate will be returned if present and/or will be requested if not yet\n// present. Only hosts added with SetAllowedHostnames are allowed. During shutdown,\n// no new connections are allowed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype dirCache autocert.DirCache\n\nfunc (d dirCache) Delete(ctx context.Context, name string) (rerr error) {\n\tlog := mlog.New(\"autotls\", nil).WithContext(ctx)\n\tdefer func() {\n\t\tlog.Debugx(\"dircache delete result\", rerr, slog.String(\"name\", name))\n\t}()\n\terr := autocert.DirCache(d).Delete(ctx, name)\n\tif err != nil {\n\t\tlog.Errorx(\"deleting cert from dir cache\", err, slog.String(\"name\", name))\n\t} else if !strings.HasSuffix(name, \"+token\") {\n\t\tlog.Info(\"autotls cert delete\", slog.String(\"name\", name))\n\t}\n\treturn err\n}\n\nfunc (d dirCache) Get(ctx context.Context, name string) (rbuf []byte, rerr error) {\n\tlog := mlog.New(\"autotls\", nil).WithContext(ctx)\n\tdefer func() {\n\t\tlog.Debugx(\"dircache get result\", rerr, slog.String(\"name\", name))\n\t}()\n\tbuf, err := autocert.DirCache(d).Get(ctx, name)\n\tif err != nil && errors.Is(err, autocert.ErrCacheMiss) {\n\t\tlog.Infox(\"getting cert from dir cache\", err, slog.String(\"name\", name))\n\t} else if err != nil {\n\t\tlog.Errorx(\"getting cert from dir cache\", err, slog.String(\"name\", name))\n\t} else if !strings.HasSuffix(name, \"+token\") {\n\t\tlog.Debug(\"autotls cert get\", slog.String(\"name\", name))\n\t}\n\treturn buf, err\n}\n\nfunc (d dirCache) Put(ctx context.Context, name string, data []byte) (rerr error) {\n\tlog := mlog.New(\"autotls\", nil).WithContext(ctx)\n\tdefer func() {\n\t\tlog.Debugx(\"dircache put result\", rerr, slog.String(\"name\", name))\n\t}()\n\tmetricCertput.Inc()\n\terr := autocert.DirCache(d).Put(ctx, name, data)\n\tif err != nil {\n\t\tlog.Errorx(\"storing cert in dir cache\", err, slog.String(\"name\", name))\n\t} else if !strings.HasSuffix(name, \"+token\") {\n\t\tlog.Info(\"autotls cert store\", slog.String(\"name\", name))\n\t}\n\treturn err\n}\n\n```\n# Function Signature: func (m *Manager) SetAllowedHostnames(log mlog.Log, resolver dns.Resolver, hostnames map[dns.Domain]struct{}, publicIPs []string, checkHosts bool) {\n# Natural Language Annotations: // SetAllowedHostnames sets a new list of allowed hostnames for automatic TLS.\n// After setting the host names, a goroutine is start to check that new host names\n// are fully served by publicIPs (only if non-empty and there is no unspecified\n// address in the list). If no, log an error with a warning that ACME validation\n// may fail.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (m *Manager) SetAllowedHostnames(log mlog.Log, resolver dns.Resolver, hostnames map[dns.Domain]struct{}, publicIPs []string, checkHosts bool) {\n\tm.Lock()\n\tdefer m.Unlock()\n\n\tm.hosts = hostnames\n\n\tif checkHosts && len(publicIPs) > 0 {\n\t\tgo func() {\n\t\t\tfor _, h := range hostnames {\n\t\t\t\tif addrs, err := resolver.Resolve(h, \"A\", \"AAAA\"); err != nil {\n\t\t\t\t\tlog.Error(\"could not resolve host public ip address\", err, slog.String(\"hostname\", string(h)))\n\t\t\t\t} else if all := m.hostIPs(publicIPs); !allFound(addrs, all) {\n\t\t\t\t\tlog.Info(\"host not fully served by public ips\", slog.String(\"hostname\", string(h)), slog.Any(\"publicIps\", publicIPs), slog.String(\"resolved\", fmt.Sprint(addrs)))\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t}\n}\n\nfunc (m *Manager) hostIPs(ips []string) map[net.IP]struct{} {\n\tall := map[net.IP]struct{}{}\n\tfor _, ip := range ips {\n\t\tparsedIP := net.ParseIP(ip)\n\t\tif parsedIP == nil {\n\t\t\tcontinue\n\t\t}\n\t\tall[parsedIP] = struct{}{}\n\t}\n\treturn all\n}\n\nfunc allFound(addrs []string, all map[net.IP]struct{}) bool {\n\tfor _, a := range addrs {\n\t\taddr := net.ParseIP(a)\n\t\tif addr == nil || _, found := all[addr]; !found {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n```", "  ```\nfunc (m *Manager) SetAllowedHostnames(log mlog.Log, resolver dns.Resolver, hostnames map[dns.Domain]struct{}, publicIPs []string, checkHosts bool) {\n\tm.Lock()\n\tm.hosts = hostnames\n\tm.Unlock()\n\n\tif !checkHosts {\n\t\treturn\n\t}\n\n\tgo func() {\n\t\tlog := log.WithContext(context.Background())\n\t\tlog.Info(\"Checking public IPs of allowed hostnames\")\n\n\t\tpublicIPs = append(publicIPs, \"0.0.0.0\")\n\t\tpublicIPs = append(publicIPs, \"::\")\n\n\t\tfor host := range m.hosts {\n\t\t\thostlog := log.WithPrefix(\"host\", slog.String(host.ASCII))\n\n\t\t\tvar hostIPs []net.IP\n\t\t\tvar err error\n\t\t\thostIPs, err = resolver.LookupHost(context.Background(), host.ASCII)\n\t\t\tif err != nil {\n\t\t\t\thostlog.Errorx(\"Error resolving IP of host\", err)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tvar ipString string\n\t\t\tif len(hostIPs) == 0 {\n\t\t\t\tipString = \"<none>\"\n\t\t\t} else {\n\t\t\t\tipString = strings.Join(hostIPs, \", \")\n\t\t\t}\n\t\t\tif !sliceContains(publicIPs, ipString) {\n\t\t\t\thostlog.Warn(\"Public IP lookup of host returned IP addresses that are not in publicIPs:\", slog.Any(\"ip\", ipString))\n\t\t\t}\n\t\t}\n\t}()\n}\n```", "  ```\nfunc (m *Manager) SetAllowedHostnames(log mlog.Log, resolver dns.Resolver, hostnames map[dns.Domain]struct{}, publicIPs []string, checkHosts bool) {\n\tm.Lock()\n\tdefer m.Unlock()\n\n\tlog.Debug(\"setting new allowing host names\", slog.Any(\"hostnames\", hostnames))\n\tm.hosts = hostnames\n\n\tif !checkHosts {\n\t\treturn\n\t}\n\n\tlog.Debug(\"starting background check against resolved ips\")\n\tgo func() {\n\t\tfor k := range hostnames {\n\t\t\tif len(m.shutdown) == 0 {\n\t\t\t\tlog.Debug(\"shutting down after checking hosts\")\n\t\t\t\treturn\n\t\t\t}\n\t\t\tips, err := resolver.Resolve(k)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorx(\"resolving host name\", err, slog.String(\"host\", k.ASCII))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tisAvailable := false\n\t\t\tfor _, ip := range ips {\n\t\t\t\tif len(publicIPs) == 0 || ip.String() == publicIPs[0] || ip.IsUnspecified() {\n\t\t\t\t\tisAvailable = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !isAvailable {\n\t\t\t\tlog.Error(\"host names is not available for ACME verification, upgrade mox\", slog.String(\"host\", k.ASCII), slog.Any(\"public\", publicIPs))\n\t\t\t}\n\t\t}\n\t}()\n}\n```"]}, "mox-autotls/autotls.go-HostPolicy": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/autotls/autotls.go:\n```\n// Package autotls automatically configures TLS (for SMTP, IMAP, HTTP) by\n// requesting certificates with ACME, typically from Let's Encrypt.\npackage autotls\n\n// We do tls-alpn-01, and also http-01. For DNS we would need a third party tool\n// with an API that can make the DNS changes, as we don't want to link in dozens of\n// bespoke API's for DNS record manipulation into mox.\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto\"\n\t\"crypto/ecdsa\"\n\t\"crypto/elliptic\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/rsa\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"encoding/pem\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/crypto/acme\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/autocert\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/moxvar\"\n)\n\nvar (\n\tmetricCertput = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_autotls_certput_total\",\n\t\t\tHelp: \"Number of certificate store puts.\",\n\t\t},\n\t)\n)\n\n// Manager is in charge of a single ACME identity, and automatically requests\n// certificates for allowlisted hosts.\ntype Manager struct {\n\tACMETLSConfig *tls.Config // For serving HTTPS on port 443, which is required for certificate requests to succeed.\n\tTLSConfig     *tls.Config // For all TLS servers not used for validating ACME requests. Like SMTP and IMAP (including with STARTTLS) and HTTPS on ports other than 443.\n\tManager       *autocert.Manager\n\n\tshutdown <-chan struct{}\n\n\tsync.Mutex\n\thosts map[dns.Domain]struct{}\n}\n\n// Load returns an initialized autotls manager for \"name\" (used for the ACME key\n// file and requested certs and their keys). All files are stored within acmeDir.\n//\n// contactEmail must be a valid email address to which notifications about ACME can\n// be sent. directoryURL is the ACME starting point.\n//\n// eabKeyID and eabKey are for external account binding when making a new account,\n// which some ACME providers require.\n//\n// getPrivateKey is called to get the private key for the host and key type. It\n// can be used to deliver a specific (e.g. always the same) private key for a\n// host, or a newly generated key.\n//\n// When shutdown is closed, no new TLS connections can be created.\nfunc Load(name, acmeDir, contactEmail, directoryURL string, eabKeyID string, eabKey []byte, getPrivateKey func(host string, keyType autocert.KeyType) (crypto.Signer, error), shutdown <-chan struct{}) (*Manager, error) {\n\tif directoryURL == \"\" {\n\t\treturn nil, fmt.Errorf(\"empty ACME directory URL\")\n\t}\n\tif contactEmail == \"\" {\n\t\treturn nil, fmt.Errorf(\"empty contact email\")\n\t}\n\n\t// Load identity key if it exists. Otherwise, create a new key.\n\tp := filepath.Join(acmeDir, name+\".key\")\n\tvar key crypto.Signer\n\tf, err := os.Open(p)\n\tif f != nil {\n\t\tdefer f.Close()\n\t}\n\tif err != nil && os.IsNotExist(err) {\n\t\tkey, err = ecdsa.GenerateKey(elliptic.P256(), cryptorand.Reader)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"generating ecdsa identity key: %s\", err)\n\t\t}\n\t\tder, err := x509.MarshalPKCS8PrivateKey(key)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"marshal identity key: %s\", err)\n\t\t}\n\t\tblock := &pem.Block{\n\t\t\tType: \"PRIVATE KEY\",\n\t\t\tHeaders: map[string]string{\n\t\t\t\t\"Note\": fmt.Sprintf(\"PEM PKCS8 ECDSA private key generated for ACME provider %s by mox\", name),\n\t\t\t},\n\t\t\tBytes: der,\n\t\t}\n\t\tb := &bytes.Buffer{}\n\t\tif err := pem.Encode(b, block); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"pem encode: %s\", err)\n\t\t} else if err := os.WriteFile(p, b.Bytes(), 0660); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"writing identity key: %s\", err)\n\t\t}\n\t} else if err != nil {\n\t\treturn nil, fmt.Errorf(\"open identity key file: %s\", err)\n\t} else {\n\t\tvar privKey any\n\t\tif buf, err := io.ReadAll(f); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"reading identity key: %s\", err)\n\t\t} else if p, _ := pem.Decode(buf); p == nil {\n\t\t\treturn nil, fmt.Errorf(\"no pem data\")\n\t\t} else if p.Type != \"PRIVATE KEY\" {\n\t\t\treturn nil, fmt.Errorf(\"got PEM block %q, expected \\\"PRIVATE KEY\\\"\", p.Type)\n\t\t} else if privKey, err = x509.ParsePKCS8PrivateKey(p.Bytes); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"parsing PKCS8 private key: %s\", err)\n\t\t}\n\t\tswitch k := privKey.(type) {\n\t\tcase *ecdsa.PrivateKey:\n\t\t\tkey = k\n\t\tcase *rsa.PrivateKey:\n\t\t\tkey = k\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"unsupported private key type %T\", key)\n\t\t}\n\t}\n\n\tm := &autocert.Manager{\n\t\tCache:  dirCache(filepath.Join(acmeDir, \"keycerts\", name)),\n\t\tPrompt: autocert.AcceptTOS,\n\t\tEmail:  contactEmail,\n\t\tClient: &acme.Client{\n\t\t\tDirectoryURL: directoryURL,\n\t\t\tKey:          key,\n\t\t\tUserAgent:    \"mox/\" + moxvar.Version,\n\t\t},\n\t\tGetPrivateKey: getPrivateKey,\n\t\t// HostPolicy set below.\n\t}\n\t// If external account binding key is provided, use it for registering a new account.\n\t// todo: ideally the key and its id are provided temporarily by the admin when registering a new account. but we don't do that interactive setup yet. in the future, an interactive setup/quickstart would ask for the key once to register a new acme account.\n\tif eabKeyID != \"\" {\n\t\tm.ExternalAccountBinding = &acme.ExternalAccountBinding{\n\t\t\tKID: eabKeyID,\n\t\t\tKey: eabKey,\n\t\t}\n\t}\n\n\tloggingGetCertificate := func(hello *tls.ClientHelloInfo) (*tls.Certificate, error) {\n\t\tlog := mlog.New(\"autotls\", nil).WithContext(hello.Context())\n\n\t\t// We handle missing invalid hostnames/ip's by returning a nil certificate and nil\n\t\t// error, which crypto/tls turns into a TLS alert \"unrecognized name\", which can be\n\t\t// interpreted by clients as a hint that they are using the wrong hostname, or a\n\t\t// certificate is missing.\n\n\t\t// Handle missing SNI to prevent logging an error below.\n\t\t// At startup, during config initialization, we already adjust the tls config to\n\t\t// inject the listener hostname if there isn't one in the TLS client hello. This is\n\t\t// common for SMTP STARTTLS connections, which often do not care about the\n\t\t// verification of the certificate.\n\t\tif hello.ServerName == \"\" {\n\t\t\tlog.Debug(\"tls request without sni servername, rejecting\", slog.Any(\"localaddr\", hello.Conn.LocalAddr()), slog.Any(\"supportedprotos\", hello.SupportedProtos))\n\t\t\treturn nil, nil\n\t\t}\n\n\t\tcert, err := m.GetCertificate(hello)\n\t\tif err != nil && errors.Is(err, errHostNotAllowed) {\n\t\t\tlog.Debugx(\"requesting certificate\", err, slog.String(\"host\", hello.ServerName))\n\t\t\treturn nil, nil\n\t\t} else if err != nil {\n\t\t\tlog.Errorx(\"requesting certificate\", err, slog.String(\"host\", hello.ServerName))\n\t\t}\n\t\treturn cert, err\n\t}\n\n\tacmeTLSConfig := *m.TLSConfig()\n\tacmeTLSConfig.GetCertificate = loggingGetCertificate\n\n\ttlsConfig := tls.Config{\n\t\tGetCertificate: loggingGetCertificate,\n\t}\n\n\ta := &Manager{\n\t\tACMETLSConfig: &acmeTLSConfig,\n\t\tTLSConfig:     &tlsConfig,\n\t\tManager:       m,\n\t\tshutdown:      shutdown,\n\t\thosts:         map[dns.Domain]struct{}{},\n\t}\n\tm.HostPolicy = a.HostPolicy\n\treturn a, nil\n}\n\n// CertAvailable checks whether a non-expired ECDSA certificate is available in the\n// cache for host. No other checks than expiration are done.\nfunc (m *Manager) CertAvailable(ctx context.Context, log mlog.Log, host dns.Domain) (bool, error) {\n\tck := host.ASCII // Would be \"+rsa\" for rsa keys.\n\tdata, err := m.Manager.Cache.Get(ctx, ck)\n\tif err != nil && errors.Is(err, autocert.ErrCacheMiss) {\n\t\treturn false, nil\n\t} else if err != nil {\n\t\treturn false, fmt.Errorf(\"attempt to get certificate from cache: %v\", err)\n\t}\n\n\t// The cached keycert is of the form: private key, leaf certificate, intermediate certificates...\n\tprivb, rem := pem.Decode(data)\n\tif privb == nil {\n\t\treturn false, fmt.Errorf(\"missing private key in cached keycert file\")\n\t}\n\tpubb, _ := pem.Decode(rem)\n\tif pubb == nil {\n\t\treturn false, fmt.Errorf(\"missing certificate in cached keycert file\")\n\t} else if pubb.Type != \"CERTIFICATE\" {\n\t\treturn false, fmt.Errorf(\"second pem block is %q, expected CERTIFICATE\", pubb.Type)\n\t}\n\tcert, err := x509.ParseCertificate(pubb.Bytes)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"parsing certificate from cached keycert file: %v\", err)\n\t}\n\t// We assume the certificate has a matching hostname, and is properly CA-signed. We\n\t// only check the expiration time.\n\tif time.Until(cert.NotBefore) > 0 || time.Since(cert.NotAfter) > 0 {\n\t\treturn false, nil\n\t}\n\treturn true, nil\n}\n\n// SetAllowedHostnames sets a new list of allowed hostnames for automatic TLS.\n// After setting the host names, a goroutine is start to check that new host names\n// are fully served by publicIPs (only if non-empty and there is no unspecified\n// address in the list). If no, log an error with a warning that ACME validation\n// may fail.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Hostnames returns the allowed host names for use with ACME.\nfunc (m *Manager) Hostnames() []dns.Domain {\n\tm.Lock()\n\tdefer m.Unlock()\n\tvar l []dns.Domain\n\tfor h := range m.hosts {\n\t\tl = append(l, h)\n\t}\n\treturn l\n}\n\nvar errHostNotAllowed = errors.New(\"autotls: host not in allowlist\")\n\n// HostPolicy decides if a host is allowed for use with ACME, i.e. whether a\n// certificate will be returned if present and/or will be requested if not yet\n// present. Only hosts added with SetAllowedHostnames are allowed. During shutdown,\n// no new connections are allowed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype dirCache autocert.DirCache\n\nfunc (d dirCache) Delete(ctx context.Context, name string) (rerr error) {\n\tlog := mlog.New(\"autotls\", nil).WithContext(ctx)\n\tdefer func() {\n\t\tlog.Debugx(\"dircache delete result\", rerr, slog.String(\"name\", name))\n\t}()\n\terr := autocert.DirCache(d).Delete(ctx, name)\n\tif err != nil {\n\t\tlog.Errorx(\"deleting cert from dir cache\", err, slog.String(\"name\", name))\n\t} else if !strings.HasSuffix(name, \"+token\") {\n\t\tlog.Info(\"autotls cert delete\", slog.String(\"name\", name))\n\t}\n\treturn err\n}\n\nfunc (d dirCache) Get(ctx context.Context, name string) (rbuf []byte, rerr error) {\n\tlog := mlog.New(\"autotls\", nil).WithContext(ctx)\n\tdefer func() {\n\t\tlog.Debugx(\"dircache get result\", rerr, slog.String(\"name\", name))\n\t}()\n\tbuf, err := autocert.DirCache(d).Get(ctx, name)\n\tif err != nil && errors.Is(err, autocert.ErrCacheMiss) {\n\t\tlog.Infox(\"getting cert from dir cache\", err, slog.String(\"name\", name))\n\t} else if err != nil {\n\t\tlog.Errorx(\"getting cert from dir cache\", err, slog.String(\"name\", name))\n\t} else if !strings.HasSuffix(name, \"+token\") {\n\t\tlog.Debug(\"autotls cert get\", slog.String(\"name\", name))\n\t}\n\treturn buf, err\n}\n\nfunc (d dirCache) Put(ctx context.Context, name string, data []byte) (rerr error) {\n\tlog := mlog.New(\"autotls\", nil).WithContext(ctx)\n\tdefer func() {\n\t\tlog.Debugx(\"dircache put result\", rerr, slog.String(\"name\", name))\n\t}()\n\tmetricCertput.Inc()\n\terr := autocert.DirCache(d).Put(ctx, name, data)\n\tif err != nil {\n\t\tlog.Errorx(\"storing cert in dir cache\", err, slog.String(\"name\", name))\n\t} else if !strings.HasSuffix(name, \"+token\") {\n\t\tlog.Info(\"autotls cert store\", slog.String(\"name\", name))\n\t}\n\treturn err\n}\n\n```\n# Function Signature: func (m *Manager) HostPolicy(ctx context.Context, host string) (rerr error) {\n# Natural Language Annotations: // HostPolicy decides if a host is allowed for use with ACME, i.e. whether a\n// certificate will be returned if present and/or will be requested if not yet\n// present. Only hosts added with SetAllowedHostnames are allowed. During shutdown,\n// no new connections are allowed.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (m *Manager) HostPolicy(ctx context.Context, host string) (rerr error) {\n\t// During shutdown, no new connections are allowed.\n\tselect {\n\tcase <-m.shutdown:\n\t\treturn errHostNotAllowed\n\tdefault:\n\t}\n\n\t// Only hosts added with SetAllowedHostnames are allowed.\n\tif _, ok := m.hosts[host]; !ok {\n\t\treturn errHostNotAllowed\n\t}\n\n\treturn nil\n}\n```", "  ```\nfunc (m *Manager) HostPolicy(ctx context.Context, host string) (rerr error) {\n\treturn func(ctx context.Context, host string) (rerr error) {\n\t\tm.RLock()\n\t\tdefer m.RUnlock()\n\n\t\tif _, ok := m.hosts[host]; ok {\n\t\t\treturn nil\n\t\t}\n\n\t\tselect {\n\t\tcase <-m.shutdown:\n\t\t\treturn fmt.Errorf(\"autotls: manager is shutting down\")\n\t\tdefault:\n\t\t\treturn errHostNotAllowed\n\t\t}\n\t}\n}\n```", "  ```\nfunc (m *Manager) HostPolicy(ctx context.Context, host string) (rerr error) {\n\tif m.shutdown != nil {\n\t\tselect {\n\t\tcase <-m.shutdown:\n\t\t\treturn errHostNotAllowed\n\t\tcase <-ctx.Done():\n\t\t\treturn errors.Unwrap(ctx.Err())\n\t\tdefault:\n\t\t}\n\t}\n\n\tm.Lock()\n\tdefer m.Unlock()\n\t_, ok := m.hosts[dns.Domain(host)]\n\treturn rerr\n}\n```"]}, "mox-imapserver/protocol.go-containsSeq": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/imapserver/protocol.go:\n```\npackage imapserver\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/mjl-/mox/store\"\n)\n\ntype numSet struct {\n\tsearchResult bool // \"$\"\n\tranges       []numRange\n}\n\ntype numRange struct {\n\tfirst setNumber\n\tlast  *setNumber // if nil, this numRange is just a setNumber in \"first\" and first.star will be false\n}\n\ntype setNumber struct {\n\tnumber uint32\n\tstar   bool // References last message (max sequence number/uid). ../rfc/9051:799\n}\n\n// containsSeq returns whether seq is in the numSet, given uids and (saved) searchResult.\n// uids and searchResult must be sorted. searchResult can have uids that are no longer in uids.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (ss numSet) containsUID(uid store.UID, uids []store.UID, searchResult []store.UID) bool {\n\tif len(uids) == 0 {\n\t\treturn false\n\t}\n\tif ss.searchResult {\n\t\treturn uidSearch(searchResult, uid) > 0 && uidSearch(uids, uid) > 0\n\t}\n\tfor _, r := range ss.ranges {\n\t\tfirst := store.UID(r.first.number)\n\t\tif r.first.star || first > uids[len(uids)-1] {\n\t\t\tfirst = uids[len(uids)-1]\n\t\t}\n\t\tlast := first\n\t\t// Num in <num>:* can be larger than last, but it still matches the last...\n\t\t// Similar for *:<num>. ../rfc/9051:4814\n\t\tif r.last != nil {\n\t\t\tlast = store.UID(r.last.number)\n\t\t\tif r.last.star || last > uids[len(uids)-1] {\n\t\t\t\tlast = uids[len(uids)-1]\n\t\t\t}\n\t\t}\n\t\tif first > last {\n\t\t\tfirst, last = last, first\n\t\t}\n\t\tif uid >= first && uid <= last && uidSearch(uids, uid) > 0 {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// contains returns whether the numset contains the number.\n// only allowed on basic, strictly increasing numsets.\nfunc (ss numSet) contains(v uint32) bool {\n\tfor _, r := range ss.ranges {\n\t\tif r.first.number == v || r.last != nil && v > r.first.number && v <= r.last.number {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc (ss numSet) empty() bool {\n\treturn !ss.searchResult && len(ss.ranges) == 0\n}\n\n// Strings returns the numset in zero or more strings of maxSize bytes. If\n// maxSize is <= 0, a single string is returned.\nfunc (ss numSet) Strings(maxSize int) []string {\n\tif ss.searchResult {\n\t\treturn []string{\"$\"}\n\t}\n\tvar l []string\n\tvar line string\n\tfor _, r := range ss.ranges {\n\t\ts := \"\"\n\t\tif r.first.star {\n\t\t\ts += \"*\"\n\t\t} else {\n\t\t\ts += fmt.Sprintf(\"%d\", r.first.number)\n\t\t}\n\t\tif r.last == nil {\n\t\t\tif r.first.star {\n\t\t\t\tpanic(\"invalid numSet range first star without last\")\n\t\t\t}\n\t\t} else {\n\t\t\ts += \":\"\n\t\t\tif r.last.star {\n\t\t\t\ts += \"*\"\n\t\t\t} else {\n\t\t\t\ts += fmt.Sprintf(\"%d\", r.last.number)\n\t\t\t}\n\t\t}\n\n\t\tnsize := len(line) + len(s)\n\t\tif line != \"\" {\n\t\t\tnsize++ // comma\n\t\t}\n\t\tif maxSize > 0 && nsize > maxSize {\n\t\t\tl = append(l, line)\n\t\t\tline = s\n\t\t\tcontinue\n\t\t}\n\t\tif line != \"\" {\n\t\t\tline += \",\"\n\t\t}\n\t\tline += s\n\t}\n\tif line != \"\" {\n\t\tl = append(l, line)\n\t}\n\treturn l\n}\n\nfunc (ss numSet) String() string {\n\tl := ss.Strings(0)\n\tif len(l) == 0 {\n\t\treturn \"\"\n\t}\n\treturn l[0]\n}\n\n// interpretStar returns a numset that interprets stars in a numset, returning a new\n// numset without stars with increasing first/last.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// whether numSet only has numbers (no star/search), and is strictly increasing.\nfunc (s *numSet) isBasicIncreasing() bool {\n\tif s.searchResult {\n\t\treturn false\n\t}\n\tvar last uint32\n\tfor _, r := range s.ranges {\n\t\tif r.first.star || r.first.number <= last || r.last != nil && (r.last.star || r.last.number < r.first.number) {\n\t\t\treturn false\n\t\t}\n\t\tlast = r.first.number\n\t\tif r.last != nil {\n\t\t\tlast = r.last.number\n\t\t}\n\t}\n\treturn true\n}\n\ntype numIter struct {\n\ts numSet\n\ti int\n\tr *rangeIter\n}\n\n// newIter must only be called on a numSet that is basic (no star/search) and ascending.\nfunc (s numSet) newIter() *numIter {\n\treturn &numIter{s: s}\n}\n\nfunc (i *numIter) Next() (uint32, bool) {\n\tif v, ok := i.r.Next(); ok {\n\t\treturn v, ok\n\t}\n\tif i.i >= len(i.s.ranges) {\n\t\treturn 0, false\n\t}\n\ti.r = i.s.ranges[i.i].newIter()\n\ti.i++\n\treturn i.r.Next()\n}\n\ntype rangeIter struct {\n\tr numRange\n\to int\n}\n\n// newIter must only be called on a range in a numSet that is basic (no star/search) and ascending.\nfunc (r numRange) newIter() *rangeIter {\n\treturn &rangeIter{r: r, o: 0}\n}\n\nfunc (r *rangeIter) Next() (uint32, bool) {\n\tif r == nil {\n\t\treturn 0, false\n\t}\n\tif r.o == 0 {\n\t\tr.o++\n\t\treturn r.r.first.number, true\n\t}\n\tif r.r.last == nil || r.r.first.number+uint32(r.o) > r.r.last.number {\n\t\treturn 0, false\n\t}\n\tv := r.r.first.number + uint32(r.o)\n\tr.o++\n\treturn v, true\n}\n\n// append adds a new number to the set, extending a range, or starting a new one (possibly the first).\n// can only be used on basic numsets, without star/searchResult.\nfunc (s *numSet) append(v uint32) {\n\tif len(s.ranges) == 0 {\n\t\ts.ranges = []numRange{{first: setNumber{number: v}}}\n\t\treturn\n\t}\n\tri := len(s.ranges) - 1\n\tr := s.ranges[ri]\n\tif v == r.first.number+1 && r.last == nil {\n\t\ts.ranges[ri].last = &setNumber{number: v}\n\t} else if r.last != nil && v == r.last.number+1 {\n\t\tr.last.number++\n\t} else {\n\t\ts.ranges = append(s.ranges, numRange{first: setNumber{number: v}})\n\t}\n}\n\ntype partial struct {\n\toffset uint32\n\tcount  uint32\n}\n\ntype sectionPart struct {\n\tpart []uint32\n\ttext *sectionText\n}\n\ntype sectionText struct {\n\tmime    bool // if \"MIME\"\n\tmsgtext *sectionMsgtext\n}\n\n// a non-nil *sectionSpec with nil msgtext & nil part means there were []'s, but nothing inside. e.g. \"BODY[]\".\ntype sectionSpec struct {\n\tmsgtext *sectionMsgtext\n\tpart    *sectionPart\n}\n\ntype sectionMsgtext struct {\n\ts       string   // \"HEADER\", \"HEADER.FIELDS\", \"HEADER.FIELDS.NOT\", \"TEXT\"\n\theaders []string // for \"HEADER.FIELDS\"*\n}\n\ntype fetchAtt struct {\n\tfield         string // uppercase, eg \"ENVELOPE\", \"BODY\". \".PEEK\" is removed.\n\tpeek          bool\n\tsection       *sectionSpec\n\tsectionBinary []uint32\n\tpartial       *partial\n}\n\ntype searchKey struct {\n\t// Only one of searchKeys, seqSet and op can be non-nil/non-empty.\n\tsearchKeys   []searchKey // In case of nested/multiple keys. Also for the top-level command.\n\tseqSet       *numSet     // In case of bare sequence set. For op UID, field uidSet contains the parameter.\n\top           string      // Determines which of the fields below are set.\n\theaderField  string\n\tastring      string\n\tdate         time.Time\n\tatom         string\n\tnumber       int64\n\tsearchKey    *searchKey\n\tsearchKey2   *searchKey\n\tuidSet       numSet\n\tclientModseq *int64\n}\n\nfunc compactUIDSet(l []store.UID) (r numSet) {\n\tfor len(l) > 0 {\n\t\te := 1\n\t\tfor ; e < len(l) && l[e] == l[e-1]+1; e++ {\n\t\t}\n\t\tfirst := setNumber{number: uint32(l[0])}\n\t\tvar last *setNumber\n\t\tif e > 1 {\n\t\t\tlast = &setNumber{number: uint32(l[e-1])}\n\t\t}\n\t\tr.ranges = append(r.ranges, numRange{first, last})\n\t\tl = l[e:]\n\t}\n\treturn\n}\n\n```\n# Function Signature: func (ss numSet) containsSeq(seq msgseq, uids []store.UID, searchResult []store.UID) bool {\n# Natural Language Annotations: // containsSeq returns whether seq is in the numSet, given uids and (saved) searchResult.\n// uids and searchResult must be sorted. searchResult can have uids that are no longer in uids.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (ss numSet) containsSeq(seq msgseq, uids []store.UID, searchResult []store.UID) bool {\n\tif len(uids) == 0 {\n\t\treturn false\n\t}\n\tif ss.searchResult {\n\t\treturn seqSearch(searchResult, seq) > 0 && seqSearch(uids, seq) > 0\n\t}\n\tfor _, r := range ss.ranges {\n\t\tfirst := store.UID(r.first.number)\n\t\tif r.first.star || first > uids[len(uids)-1] {\n\t\t\tfirst = uids[len(uids)-1]\n\t\t}\n\t\tlast := first\n\t\t// Num in <num>:* can be larger than last, but it still matches the last...\n\t\t// Similar for *:<num>. ../rfc/9051:4814\n\t\tif r.last != nil {\n\t\t\tlast = store.UID(r.last.number)\n\t\t\tif r.last.star || last > uids[len(uids)-1] {\n\t\t\t\tlast = uids[len(uids)-1]\n\t\t\t}\n\t\t}\n\t\tif first > last {\n\t\t\tfirst, last = last, first\n\t\t}\n\t\tif seq >= first && seq <= last && seqSearch(uids, seq) > 0 {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n```", "  ```\nfunc (ss numSet) containsSeq(seq msgseq, uids []store.UID, searchResult []store.UID) bool {\n\tif len(uids) == 0 {\n\t\treturn false\n\t}\n\tif ss.searchResult {\n\t\treturn seqSearch(searchResult, seq) > 0 && seqSearch(uids, seq) > 0\n\t}\n\tfor _, r := range ss.ranges {\n\t\tfirst := msgseq(r.first.number)\n\t\tif r.first.star || first > uids[len(uids)-1] {\n\t\t\tfirst = uids[len(uids)-1]\n\t\t}\n\t\tlast := first\n\t\t// Num in <num>:* can be larger than last, but it still matches the last...\n\t\t// Similar for *:<num>. ../rfc/9051:4814\n\t\tif r.last != nil {\n\t\t\tlast = msgseq(r.last.number)\n\t\t\tif r.last.star || last > uids[len(uids)-1] {\n\t\t\t\tlast = uids[len(uids)-1]\n\t\t\t}\n\t\t}\n\t\tif first > last {\n\t\t\tfirst, last = last, first\n\t\t}\n\t\tif seq >= first && seq <= last && seqSearch(uids, seq) > 0 {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n```", "  ```\nfunc (ss numSet) containsSeq(seq msgseq, uids []store.UID, searchResult []store.UID) bool {\n\tif len(uids) == 0 {\n\t\treturn false\n\t}\n\tif ss.searchResult {\n\t\treturn seqSearch(searchResult, seq) > 0 && seqSearch(uids, seq) > 0\n\t}\n\tfor _, r := range ss.ranges {\n\t\tfirst := msgseq(r.first.number)\n\t\tif r.first.star || first > uids[len(uids)-1] {\n\t\t\tfirst = uids[len(uids)-1]\n\t\t}\n\t\tlast := first\n\t\t// seq in <seq>:* can be larger than last, but it still matches the last...\n\t\t// Similar for *:<seq>. ../rfc/9051:4814\n\t\tif r.last != nil {\n\t\t\tlast = msgseq(r.last.number)\n\t\t\tif r.last.star || last > uids[len(uids)-1] {\n\t\t\t\tlast = uids[len(uids)-1]\n\t\t\t}\n\t\t}\n\t\tif first > last {\n\t\t\tfirst, last = last, first\n\t\t}\n\t\tif seq >= first && seq <= last && seqSearch(uids, seq) > 0 {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n```"]}, "mox-imapserver/protocol.go-interpretStar": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/imapserver/protocol.go:\n```\npackage imapserver\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/mjl-/mox/store\"\n)\n\ntype numSet struct {\n\tsearchResult bool // \"$\"\n\tranges       []numRange\n}\n\ntype numRange struct {\n\tfirst setNumber\n\tlast  *setNumber // if nil, this numRange is just a setNumber in \"first\" and first.star will be false\n}\n\ntype setNumber struct {\n\tnumber uint32\n\tstar   bool // References last message (max sequence number/uid). ../rfc/9051:799\n}\n\n// containsSeq returns whether seq is in the numSet, given uids and (saved) searchResult.\n// uids and searchResult must be sorted. searchResult can have uids that are no longer in uids.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (ss numSet) containsUID(uid store.UID, uids []store.UID, searchResult []store.UID) bool {\n\tif len(uids) == 0 {\n\t\treturn false\n\t}\n\tif ss.searchResult {\n\t\treturn uidSearch(searchResult, uid) > 0 && uidSearch(uids, uid) > 0\n\t}\n\tfor _, r := range ss.ranges {\n\t\tfirst := store.UID(r.first.number)\n\t\tif r.first.star || first > uids[len(uids)-1] {\n\t\t\tfirst = uids[len(uids)-1]\n\t\t}\n\t\tlast := first\n\t\t// Num in <num>:* can be larger than last, but it still matches the last...\n\t\t// Similar for *:<num>. ../rfc/9051:4814\n\t\tif r.last != nil {\n\t\t\tlast = store.UID(r.last.number)\n\t\t\tif r.last.star || last > uids[len(uids)-1] {\n\t\t\t\tlast = uids[len(uids)-1]\n\t\t\t}\n\t\t}\n\t\tif first > last {\n\t\t\tfirst, last = last, first\n\t\t}\n\t\tif uid >= first && uid <= last && uidSearch(uids, uid) > 0 {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// contains returns whether the numset contains the number.\n// only allowed on basic, strictly increasing numsets.\nfunc (ss numSet) contains(v uint32) bool {\n\tfor _, r := range ss.ranges {\n\t\tif r.first.number == v || r.last != nil && v > r.first.number && v <= r.last.number {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc (ss numSet) empty() bool {\n\treturn !ss.searchResult && len(ss.ranges) == 0\n}\n\n// Strings returns the numset in zero or more strings of maxSize bytes. If\n// maxSize is <= 0, a single string is returned.\nfunc (ss numSet) Strings(maxSize int) []string {\n\tif ss.searchResult {\n\t\treturn []string{\"$\"}\n\t}\n\tvar l []string\n\tvar line string\n\tfor _, r := range ss.ranges {\n\t\ts := \"\"\n\t\tif r.first.star {\n\t\t\ts += \"*\"\n\t\t} else {\n\t\t\ts += fmt.Sprintf(\"%d\", r.first.number)\n\t\t}\n\t\tif r.last == nil {\n\t\t\tif r.first.star {\n\t\t\t\tpanic(\"invalid numSet range first star without last\")\n\t\t\t}\n\t\t} else {\n\t\t\ts += \":\"\n\t\t\tif r.last.star {\n\t\t\t\ts += \"*\"\n\t\t\t} else {\n\t\t\t\ts += fmt.Sprintf(\"%d\", r.last.number)\n\t\t\t}\n\t\t}\n\n\t\tnsize := len(line) + len(s)\n\t\tif line != \"\" {\n\t\t\tnsize++ // comma\n\t\t}\n\t\tif maxSize > 0 && nsize > maxSize {\n\t\t\tl = append(l, line)\n\t\t\tline = s\n\t\t\tcontinue\n\t\t}\n\t\tif line != \"\" {\n\t\t\tline += \",\"\n\t\t}\n\t\tline += s\n\t}\n\tif line != \"\" {\n\t\tl = append(l, line)\n\t}\n\treturn l\n}\n\nfunc (ss numSet) String() string {\n\tl := ss.Strings(0)\n\tif len(l) == 0 {\n\t\treturn \"\"\n\t}\n\treturn l[0]\n}\n\n// interpretStar returns a numset that interprets stars in a numset, returning a new\n// numset without stars with increasing first/last.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// whether numSet only has numbers (no star/search), and is strictly increasing.\nfunc (s *numSet) isBasicIncreasing() bool {\n\tif s.searchResult {\n\t\treturn false\n\t}\n\tvar last uint32\n\tfor _, r := range s.ranges {\n\t\tif r.first.star || r.first.number <= last || r.last != nil && (r.last.star || r.last.number < r.first.number) {\n\t\t\treturn false\n\t\t}\n\t\tlast = r.first.number\n\t\tif r.last != nil {\n\t\t\tlast = r.last.number\n\t\t}\n\t}\n\treturn true\n}\n\ntype numIter struct {\n\ts numSet\n\ti int\n\tr *rangeIter\n}\n\n// newIter must only be called on a numSet that is basic (no star/search) and ascending.\nfunc (s numSet) newIter() *numIter {\n\treturn &numIter{s: s}\n}\n\nfunc (i *numIter) Next() (uint32, bool) {\n\tif v, ok := i.r.Next(); ok {\n\t\treturn v, ok\n\t}\n\tif i.i >= len(i.s.ranges) {\n\t\treturn 0, false\n\t}\n\ti.r = i.s.ranges[i.i].newIter()\n\ti.i++\n\treturn i.r.Next()\n}\n\ntype rangeIter struct {\n\tr numRange\n\to int\n}\n\n// newIter must only be called on a range in a numSet that is basic (no star/search) and ascending.\nfunc (r numRange) newIter() *rangeIter {\n\treturn &rangeIter{r: r, o: 0}\n}\n\nfunc (r *rangeIter) Next() (uint32, bool) {\n\tif r == nil {\n\t\treturn 0, false\n\t}\n\tif r.o == 0 {\n\t\tr.o++\n\t\treturn r.r.first.number, true\n\t}\n\tif r.r.last == nil || r.r.first.number+uint32(r.o) > r.r.last.number {\n\t\treturn 0, false\n\t}\n\tv := r.r.first.number + uint32(r.o)\n\tr.o++\n\treturn v, true\n}\n\n// append adds a new number to the set, extending a range, or starting a new one (possibly the first).\n// can only be used on basic numsets, without star/searchResult.\nfunc (s *numSet) append(v uint32) {\n\tif len(s.ranges) == 0 {\n\t\ts.ranges = []numRange{{first: setNumber{number: v}}}\n\t\treturn\n\t}\n\tri := len(s.ranges) - 1\n\tr := s.ranges[ri]\n\tif v == r.first.number+1 && r.last == nil {\n\t\ts.ranges[ri].last = &setNumber{number: v}\n\t} else if r.last != nil && v == r.last.number+1 {\n\t\tr.last.number++\n\t} else {\n\t\ts.ranges = append(s.ranges, numRange{first: setNumber{number: v}})\n\t}\n}\n\ntype partial struct {\n\toffset uint32\n\tcount  uint32\n}\n\ntype sectionPart struct {\n\tpart []uint32\n\ttext *sectionText\n}\n\ntype sectionText struct {\n\tmime    bool // if \"MIME\"\n\tmsgtext *sectionMsgtext\n}\n\n// a non-nil *sectionSpec with nil msgtext & nil part means there were []'s, but nothing inside. e.g. \"BODY[]\".\ntype sectionSpec struct {\n\tmsgtext *sectionMsgtext\n\tpart    *sectionPart\n}\n\ntype sectionMsgtext struct {\n\ts       string   // \"HEADER\", \"HEADER.FIELDS\", \"HEADER.FIELDS.NOT\", \"TEXT\"\n\theaders []string // for \"HEADER.FIELDS\"*\n}\n\ntype fetchAtt struct {\n\tfield         string // uppercase, eg \"ENVELOPE\", \"BODY\". \".PEEK\" is removed.\n\tpeek          bool\n\tsection       *sectionSpec\n\tsectionBinary []uint32\n\tpartial       *partial\n}\n\ntype searchKey struct {\n\t// Only one of searchKeys, seqSet and op can be non-nil/non-empty.\n\tsearchKeys   []searchKey // In case of nested/multiple keys. Also for the top-level command.\n\tseqSet       *numSet     // In case of bare sequence set. For op UID, field uidSet contains the parameter.\n\top           string      // Determines which of the fields below are set.\n\theaderField  string\n\tastring      string\n\tdate         time.Time\n\tatom         string\n\tnumber       int64\n\tsearchKey    *searchKey\n\tsearchKey2   *searchKey\n\tuidSet       numSet\n\tclientModseq *int64\n}\n\nfunc compactUIDSet(l []store.UID) (r numSet) {\n\tfor len(l) > 0 {\n\t\te := 1\n\t\tfor ; e < len(l) && l[e] == l[e-1]+1; e++ {\n\t\t}\n\t\tfirst := setNumber{number: uint32(l[0])}\n\t\tvar last *setNumber\n\t\tif e > 1 {\n\t\t\tlast = &setNumber{number: uint32(l[e-1])}\n\t\t}\n\t\tr.ranges = append(r.ranges, numRange{first, last})\n\t\tl = l[e:]\n\t}\n\treturn\n}\n\n```\n# Function Signature: func (s numSet) interpretStar(uids []store.UID) numSet {\n# Natural Language Annotations: // interpretStar returns a numset that interprets stars in a numset, returning a new\n// numset without stars with increasing first/last.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (s numSet) interpretStar(uids []store.UID) numSet {\n\tif s.searchResult {\n\t\treturn s\n\t}\n\tvar l []numRange\n\tfor _, r := range s.ranges {\n\t\tfirst := setNumber{number: r.first.number}\n\t\tif r.first.star {\n\t\t\tfirst.number = uids[len(uids)-1]\n\t\t}\n\t\tlast := first\n\t\tif r.last != nil {\n\t\t\tlast.number = r.last.number\n\t\t\tif r.last.star {\n\t\t\t\tlast.number = uids[len(uids)-1]\n\t\t\t}\n\t\t}\n\t\tif first.number > last.number {\n\t\t\tfirst.number, last.number = last.number, first.number\n\t\t}\n\t\tl = append(l, numRange{first, &last})\n\t}\n\treturn numSet{ranges: l}\n}\n```", "  ```\nfunc (s numSet) interpretStar(uids []store.UID) numSet {\n\tif len(s.ranges) == 0 {\n\t\treturn s\n\t}\n\tinterpreted := make([]numRange, 0, len(s.ranges))\n\tfor _, r := range s.ranges {\n\t\tif r.first.star {\n\t\t\tr.first.number = uids[len(uids)-1]\n\t\t}\n\t\tif r.last == nil {\n\t\t\tr.last = &setNumber{number: uids[len(uids)-1]}\n\t\t} else if r.last.star {\n\t\t\tr.last.number = uids[len(uids)-1]\n\t\t}\n\t\tinterpreted = append(interpreted, r)\n\t}\n\treturn numSet{ranges: interpreted}\n}\n```", "  ```\nfunc (s numSet) interpretStar(uids []store.UID) numSet {\n\tif s.searchResult {\n\t\treturn s // no need to interpret stars if it's a searchResult\n\t}\n\tvar newRanges []numRange\n\tfor _, r := range s.ranges {\n\t\tif r.first.star {\n\t\t\tr.first.number = uids[len(uids)-1]\n\t\t}\n\t\tif r.last != nil && r.last.star {\n\t\t\tr.last.number = uids[len(uids)-1]\n\t\t}\n\t\tif r.last == nil || r.last.number < r.first.number {\n\t\t\tr.last = &setNumber{number: r.first.number}\n\t\t}\n\t\tnewRanges = append(newRanges, r)\n\t}\n\treturn numSet{searchResult: s.searchResult, ranges: newRanges}\n}\n```"]}, "mox-http/web.go-ServeHTTP": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/http/web.go:\n```\n// Package http provides HTTP listeners/servers, for\n// autoconfiguration/autodiscovery, the account and admin web interface and\n// MTA-STS policies.\npackage http\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"crypto/tls\"\n\t\"fmt\"\n\t\"io\"\n\tgolog \"log\"\n\t\"log/slog\"\n\t\"net\"\n\t\"net/http\"\n\t\"os\"\n\t\"path\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t_ \"embed\"\n\t_ \"net/http/pprof\"\n\n\t\"golang.org/x/exp/maps\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\n\t\"github.com/mjl-/mox/autotls\"\n\t\"github.com/mjl-/mox/config\"\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/ratelimit\"\n\t\"github.com/mjl-/mox/webaccount\"\n\t\"github.com/mjl-/mox/webadmin\"\n\t\"github.com/mjl-/mox/webapisrv\"\n\t\"github.com/mjl-/mox/webmail\"\n)\n\nvar pkglog = mlog.New(\"http\", nil)\n\nvar (\n\t// metricRequest tracks performance (time to write response header) of server.\n\tmetricRequest = promauto.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"mox_httpserver_request_duration_seconds\",\n\t\t\tHelp:    \"HTTP(s) server request with handler name, protocol, method, result codes, and duration until response status code is written, in seconds.\",\n\t\t\tBuckets: []float64{0.001, 0.005, 0.01, 0.05, 0.100, 0.5, 1, 5, 10, 20, 30, 60, 120},\n\t\t},\n\t\t[]string{\n\t\t\t\"handler\", // Name from webhandler, can be empty.\n\t\t\t\"proto\",   // \"http\", \"https\", \"ws\", \"wss\"\n\t\t\t\"method\",  // \"(unknown)\" and otherwise only common verbs\n\t\t\t\"code\",\n\t\t},\n\t)\n\t// metricResponse tracks performance of entire request as experienced by users,\n\t// which also depends on their connection speed, so not necessarily something you\n\t// could act on.\n\tmetricResponse = promauto.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"mox_httpserver_response_duration_seconds\",\n\t\t\tHelp:    \"HTTP(s) server response with handler name, protocol, method, result codes, and duration of entire response, in seconds.\",\n\t\t\tBuckets: []float64{0.001, 0.005, 0.01, 0.05, 0.100, 0.5, 1, 5, 10, 20, 30, 60, 120},\n\t\t},\n\t\t[]string{\n\t\t\t\"handler\", // Name from webhandler, can be empty.\n\t\t\t\"proto\",   // \"http\", \"https\", \"ws\", \"wss\"\n\t\t\t\"method\",  // \"(unknown)\" and otherwise only common verbs\n\t\t\t\"code\",\n\t\t},\n\t)\n)\n\n// We serve a favicon when webaccount/webmail/webadmin/webapi for account-related\n// domains. They are configured as \"service handler\", which have a lower priority\n// than web handler. Admins can configure a custom /favicon.ico route to override\n// the builtin favicon. In the future, we may want to make it easier to customize\n// the favicon, possibly per client settings domain.\n//\n//go:embed favicon.ico\nvar faviconIco string\nvar faviconModTime = time.Now()\n\nfunc init() {\n\tp, err := os.Executable()\n\tif err == nil {\n\t\tif st, err := os.Stat(p); err == nil {\n\t\t\tfaviconModTime = st.ModTime()\n\t\t}\n\t}\n}\n\nfunc faviconHandle(w http.ResponseWriter, r *http.Request) {\n\thttp.ServeContent(w, r, \"favicon.ico\", faviconModTime, strings.NewReader(faviconIco))\n}\n\ntype responseWriterFlusher interface {\n\thttp.ResponseWriter\n\thttp.Flusher\n}\n\n// http.ResponseWriter that writes access log and tracks metrics at end of response.\ntype loggingWriter struct {\n\tW                responseWriterFlusher // Calls are forwarded.\n\tStart            time.Time\n\tR                *http.Request\n\tWebsocketRequest bool // Whether request from was websocket.\n\n\t// Set by router.\n\tHandler  string\n\tCompress bool\n\n\t// Set by handlers.\n\tStatusCode                   int\n\tSize                         int64        // Of data served to client, for non-websocket responses.\n\tUncompressedSize             int64        // Can be set by a handler that already serves compressed data, and we update it while compressing.\n\tGzip                         *gzip.Writer // Only set if we transparently compress within loggingWriter (static handlers handle compression themselves, with a cache).\n\tErr                          error\n\tWebsocketResponse            bool        // If this was a successful websocket connection with backend.\n\tSizeFromClient, SizeToClient int64       // Websocket data.\n\tAttrs                        []slog.Attr // Additional fields to log.\n}\n\nfunc (w *loggingWriter) AddAttr(a slog.Attr) {\n\tw.Attrs = append(w.Attrs, a)\n}\n\nfunc (w *loggingWriter) Flush() {\n\tw.W.Flush()\n}\n\nfunc (w *loggingWriter) Header() http.Header {\n\treturn w.W.Header()\n}\n\n// protocol, for logging.\nfunc (w *loggingWriter) proto(websocket bool) string {\n\tproto := \"http\"\n\tif websocket {\n\t\tproto = \"ws\"\n\t}\n\tif w.R.TLS != nil {\n\t\tproto += \"s\"\n\t}\n\treturn proto\n}\n\nfunc (w *loggingWriter) Write(buf []byte) (int, error) {\n\tif w.StatusCode == 0 {\n\t\tw.WriteHeader(http.StatusOK)\n\t}\n\n\tvar n int\n\tvar err error\n\tif w.Gzip == nil {\n\t\tn, err = w.W.Write(buf)\n\t\tif n > 0 {\n\t\t\tw.Size += int64(n)\n\t\t}\n\t} else {\n\t\t// We flush after each write. Probably takes a few more bytes, but prevents any\n\t\t// issues due to buffering.\n\t\t// w.Gzip.Write updates w.Size with the compressed byte count.\n\t\tn, err = w.Gzip.Write(buf)\n\t\tif err == nil {\n\t\t\terr = w.Gzip.Flush()\n\t\t}\n\t\tif n > 0 {\n\t\t\tw.UncompressedSize += int64(n)\n\t\t}\n\t}\n\tif err != nil {\n\t\tw.error(err)\n\t}\n\treturn n, err\n}\n\nfunc (w *loggingWriter) setStatusCode(statusCode int) {\n\tif w.StatusCode != 0 {\n\t\treturn\n\t}\n\n\tw.StatusCode = statusCode\n\tmethod := metricHTTPMethod(w.R.Method)\n\tmetricRequest.WithLabelValues(w.Handler, w.proto(w.WebsocketRequest), method, fmt.Sprintf(\"%d\", w.StatusCode)).Observe(float64(time.Since(w.Start)) / float64(time.Second))\n}\n\n// SetUncompressedSize is used through an interface by\n// ../webmail/webmail.go:/WriteHeader, preventing an import cycle.\nfunc (w *loggingWriter) SetUncompressedSize(origSize int64) {\n\tw.UncompressedSize = origSize\n}\n\nfunc (w *loggingWriter) WriteHeader(statusCode int) {\n\tif w.StatusCode != 0 {\n\t\treturn\n\t}\n\n\tw.setStatusCode(statusCode)\n\n\t// We transparently gzip-compress responses for requests under these conditions, all must apply:\n\t//\n\t// - Enabled for handler (static handlers make their own decisions).\n\t// - Not a websocket request.\n\t// - Regular success responses (not errors, or partial content or redirects or \"not modified\", etc).\n\t// - Not already compressed, or any other Content-Encoding header (including \"identity\").\n\t// - Client accepts gzip encoded responses.\n\t// - The response has a content-type that is compressible (text/*, */*+{json,xml}, and a few common files (e.g. json, xml, javascript).\n\tif w.Compress && !w.WebsocketRequest && statusCode == http.StatusOK && w.W.Header().Values(\"Content-Encoding\") == nil && acceptsGzip(w.R) && compressibleContentType(w.W.Header().Get(\"Content-Type\")) {\n\t\t// todo: we should gather the first kb of data, see if it is compressible. if not, just return original. should set timer so we flush if it takes too long to gather 1kb. for smaller data we shouldn't compress at all.\n\n\t\t// We track the gzipped output for the access log.\n\t\tcw := countWriter{Writer: w.W, Size: &w.Size}\n\t\tw.Gzip, _ = gzip.NewWriterLevel(cw, gzip.BestSpeed)\n\t\tw.W.Header().Set(\"Content-Encoding\", \"gzip\")\n\t\tw.W.Header().Del(\"Content-Length\") // No longer valid, set again for small responses by net/http.\n\t}\n\tw.W.WriteHeader(statusCode)\n}\n\nfunc acceptsGzip(r *http.Request) bool {\n\ts := r.Header.Get(\"Accept-Encoding\")\n\tt := strings.Split(s, \",\")\n\tfor _, e := range t {\n\t\te = strings.TrimSpace(e)\n\t\ttt := strings.Split(e, \";\")\n\t\tif len(tt) > 1 && t[1] == \"q=0\" {\n\t\t\tcontinue\n\t\t}\n\t\tif tt[0] == \"gzip\" {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nvar compressibleTypes = map[string]bool{\n\t\"application/csv\":          true,\n\t\"application/javascript\":   true,\n\t\"application/json\":         true,\n\t\"application/x-javascript\": true,\n\t\"application/xml\":          true,\n\t\"image/vnd.microsoft.icon\": true,\n\t\"image/x-icon\":             true,\n\t\"font/ttf\":                 true,\n\t\"font/eot\":                 true,\n\t\"font/otf\":                 true,\n\t\"font/opentype\":            true,\n}\n\nfunc compressibleContentType(ct string) bool {\n\tct = strings.SplitN(ct, \";\", 2)[0]\n\tct = strings.TrimSpace(ct)\n\tct = strings.ToLower(ct)\n\tif compressibleTypes[ct] {\n\t\treturn true\n\t}\n\tt, st, _ := strings.Cut(ct, \"/\")\n\treturn t == \"text\" || strings.HasSuffix(st, \"+json\") || strings.HasSuffix(st, \"+xml\")\n}\n\nfunc compressibleContent(f *os.File) bool {\n\t// We don't want to store many small files. They take up too much disk overhead.\n\tif fi, err := f.Stat(); err != nil || fi.Size() < 1024 || fi.Size() > 10*1024*1024 {\n\t\treturn false\n\t}\n\n\tbuf := make([]byte, 512)\n\tn, err := f.ReadAt(buf, 0)\n\tif err != nil && err != io.EOF {\n\t\treturn false\n\t}\n\tct := http.DetectContentType(buf[:n])\n\treturn compressibleContentType(ct)\n}\n\ntype countWriter struct {\n\tWriter io.Writer\n\tSize   *int64\n}\n\nfunc (w countWriter) Write(buf []byte) (int, error) {\n\tn, err := w.Writer.Write(buf)\n\tif n > 0 {\n\t\t*w.Size += int64(n)\n\t}\n\treturn n, err\n}\n\nvar tlsVersions = map[uint16]string{\n\ttls.VersionTLS10: \"tls1.0\",\n\ttls.VersionTLS11: \"tls1.1\",\n\ttls.VersionTLS12: \"tls1.2\",\n\ttls.VersionTLS13: \"tls1.3\",\n}\n\nfunc metricHTTPMethod(method string) string {\n\t// https://www.iana.org/assignments/http-methods/http-methods.xhtml\n\tmethod = strings.ToLower(method)\n\tswitch method {\n\tcase \"acl\", \"baseline-control\", \"bind\", \"checkin\", \"checkout\", \"connect\", \"copy\", \"delete\", \"get\", \"head\", \"label\", \"link\", \"lock\", \"merge\", \"mkactivity\", \"mkcalendar\", \"mkcol\", \"mkredirectref\", \"mkworkspace\", \"move\", \"options\", \"orderpatch\", \"patch\", \"post\", \"pri\", \"propfind\", \"proppatch\", \"put\", \"rebind\", \"report\", \"search\", \"trace\", \"unbind\", \"uncheckout\", \"unlink\", \"unlock\", \"update\", \"updateredirectref\", \"version-control\":\n\t\treturn method\n\t}\n\treturn \"(other)\"\n}\n\nfunc (w *loggingWriter) error(err error) {\n\tif w.Err == nil {\n\t\tw.Err = err\n\t}\n}\n\nfunc (w *loggingWriter) Done() {\n\tif w.Err == nil && w.Gzip != nil {\n\t\tif err := w.Gzip.Close(); err != nil {\n\t\t\tw.error(err)\n\t\t}\n\t}\n\n\tmethod := metricHTTPMethod(w.R.Method)\n\tmetricResponse.WithLabelValues(w.Handler, w.proto(w.WebsocketResponse), method, fmt.Sprintf(\"%d\", w.StatusCode)).Observe(float64(time.Since(w.Start)) / float64(time.Second))\n\n\ttlsinfo := \"plain\"\n\tif w.R.TLS != nil {\n\t\tif v, ok := tlsVersions[w.R.TLS.Version]; ok {\n\t\t\ttlsinfo = v\n\t\t} else {\n\t\t\ttlsinfo = \"(other)\"\n\t\t}\n\t}\n\terr := w.Err\n\tif err == nil {\n\t\terr = w.R.Context().Err()\n\t}\n\tattrs := []slog.Attr{\n\t\tslog.String(\"httpaccess\", \"\"),\n\t\tslog.String(\"handler\", w.Handler),\n\t\tslog.String(\"method\", method),\n\t\tslog.Any(\"url\", w.R.URL),\n\t\tslog.String(\"host\", w.R.Host),\n\t\tslog.Duration(\"duration\", time.Since(w.Start)),\n\t\tslog.Int(\"statuscode\", w.StatusCode),\n\t\tslog.String(\"proto\", strings.ToLower(w.R.Proto)),\n\t\tslog.Any(\"remoteaddr\", w.R.RemoteAddr),\n\t\tslog.String(\"tlsinfo\", tlsinfo),\n\t\tslog.String(\"useragent\", w.R.Header.Get(\"User-Agent\")),\n\t\tslog.String(\"referrr\", w.R.Header.Get(\"Referrer\")),\n\t}\n\tif w.WebsocketRequest {\n\t\tattrs = append(attrs,\n\t\t\tslog.Bool(\"websocketrequest\", true),\n\t\t)\n\t}\n\tif w.WebsocketResponse {\n\t\tattrs = append(attrs,\n\t\t\tslog.Bool(\"websocket\", true),\n\t\t\tslog.Int64(\"sizetoclient\", w.SizeToClient),\n\t\t\tslog.Int64(\"sizefromclient\", w.SizeFromClient),\n\t\t)\n\t} else if w.UncompressedSize > 0 {\n\t\tattrs = append(attrs,\n\t\t\tslog.Int64(\"size\", w.Size),\n\t\t\tslog.Int64(\"uncompressedsize\", w.UncompressedSize),\n\t\t)\n\t} else {\n\t\tattrs = append(attrs,\n\t\t\tslog.Int64(\"size\", w.Size),\n\t\t)\n\t}\n\tattrs = append(attrs, w.Attrs...)\n\tpkglog.WithContext(w.R.Context()).Debugx(\"http request\", err, attrs...)\n}\n\n// Built-in handlers, e.g. mta-sts and autoconfig.\ntype pathHandler struct {\n\tName      string                       // For logging/metrics.\n\tHostMatch func(host dns.IPDomain) bool // If not nil, called to see if domain of requests matches. Host can be zero value for invalid domain/ip.\n\tPath      string                       // Path to register, like on http.ServeMux.\n\tHandler   http.Handler\n}\ntype serve struct {\n\tKinds     []string // Type of handler and protocol (e.g. acme-tls-alpn-01, account-http, admin-https).\n\tTLSConfig *tls.Config\n\tFavicon   bool\n\n\t// SystemHandlers are for MTA-STS, autoconfig, ACME validation. They can't be\n\t// overridden by WebHandlers. WebHandlers are evaluated next, and the internal\n\t// service handlers from Listeners in mox.conf (for admin, account, webmail, webapi\n\t// interfaces) last. WebHandlers can also pass requests to the internal servers.\n\t// This order allows admins to serve other content on domains serving the mox.conf\n\t// internal services.\n\tSystemHandlers  []pathHandler // Sorted, longest first.\n\tWebserver       bool\n\tServiceHandlers []pathHandler // Sorted, longest first.\n}\n\n// SystemHandle registers a named system handler for a path and optional host. If\n// path ends with a slash, it is used as prefix match, otherwise a full path match\n// is required. If hostOpt is set, only requests to those host are handled by this\n// handler.\nfunc (s *serve) SystemHandle(name string, hostMatch func(dns.IPDomain) bool, path string, fn http.Handler) {\n\ts.SystemHandlers = append(s.SystemHandlers, pathHandler{name, hostMatch, path, fn})\n}\n\n// Like SystemHandle, but for internal services \"admin\", \"account\", \"webmail\",\n// \"webapi\" configured in the mox.conf Listener.\nfunc (s *serve) ServiceHandle(name string, hostMatch func(dns.IPDomain) bool, path string, fn http.Handler) {\n\ts.ServiceHandlers = append(s.ServiceHandlers, pathHandler{name, hostMatch, path, fn})\n}\n\nvar (\n\tlimiterConnectionrate = &ratelimit.Limiter{\n\t\tWindowLimits: []ratelimit.WindowLimit{\n\t\t\t{\n\t\t\t\tWindow: time.Minute,\n\t\t\t\tLimits: [...]int64{1000, 3000, 9000},\n\t\t\t},\n\t\t\t{\n\t\t\t\tWindow: time.Hour,\n\t\t\t\tLimits: [...]int64{5000, 15000, 45000},\n\t\t\t},\n\t\t},\n\t}\n)\n\n// ServeHTTP is the starting point for serving HTTP requests. It dispatches to the\n// right pathHandler or WebHandler, and it generates access logs and tracks\n// metrics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc redirectToTrailingSlash(srv *serve, hostMatch func(dns.IPDomain) bool, name, path string) {\n\t// Helpfully redirect user to version with ending slash.\n\tif path != \"/\" && strings.HasSuffix(path, \"/\") {\n\t\thandler := mox.SafeHeaders(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\t\thttp.Redirect(w, r, path, http.StatusSeeOther)\n\t\t}))\n\t\tsrv.ServiceHandle(name, hostMatch, path[:len(path)-1], handler)\n\t}\n}\n\n// Listen binds to sockets for HTTP listeners, including those required for ACME to\n// generate TLS certificates. It stores the listeners so Serve can start serving them.\nfunc Listen() {\n\t// Initialize listeners in deterministic order for the same potential error\n\t// messages.\n\tnames := maps.Keys(mox.Conf.Static.Listeners)\n\tsort.Strings(names)\n\tfor _, name := range names {\n\t\tl := mox.Conf.Static.Listeners[name]\n\t\tportServe := portServes(l)\n\n\t\tports := maps.Keys(portServe)\n\t\tsort.Ints(ports)\n\t\tfor _, port := range ports {\n\t\t\tsrv := portServe[port]\n\t\t\tfor _, ip := range l.IPs {\n\t\t\t\tlisten1(ip, port, srv.TLSConfig, name, srv.Kinds, srv)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc portServes(l config.Listener) map[int]*serve {\n\tportServe := map[int]*serve{}\n\n\t// For system/services, we serve on host localhost too, for ssh tunnel scenario's.\n\tlocalhost := dns.Domain{ASCII: \"localhost\"}\n\n\tldom := l.HostnameDomain\n\tif l.Hostname == \"\" {\n\t\tldom = mox.Conf.Static.HostnameDomain\n\t}\n\tlistenerHostMatch := func(host dns.IPDomain) bool {\n\t\tif host.IsIP() {\n\t\t\treturn true\n\t\t}\n\t\treturn host.Domain == ldom || host.Domain == localhost\n\t}\n\taccountHostMatch := func(host dns.IPDomain) bool {\n\t\tif listenerHostMatch(host) {\n\t\t\treturn true\n\t\t}\n\t\treturn mox.Conf.IsClientSettingsDomain(host.Domain)\n\t}\n\n\tvar ensureServe func(https bool, port int, kind string, favicon bool) *serve\n\tensureServe = func(https bool, port int, kind string, favicon bool) *serve {\n\t\ts := portServe[port]\n\t\tif s == nil {\n\t\t\ts = &serve{nil, nil, false, nil, false, nil}\n\t\t\tportServe[port] = s\n\t\t}\n\t\ts.Kinds = append(s.Kinds, kind)\n\t\tif favicon && !s.Favicon {\n\t\t\ts.ServiceHandle(\"favicon\", accountHostMatch, \"/favicon.ico\", mox.SafeHeaders(http.HandlerFunc(faviconHandle)))\n\t\t\ts.Favicon = true\n\t\t}\n\n\t\tif https && l.TLS.ACME != \"\" {\n\t\t\ts.TLSConfig = l.TLS.ACMEConfig\n\t\t} else if https {\n\t\t\ts.TLSConfig = l.TLS.Config\n\t\t\tif l.TLS.ACME != \"\" {\n\t\t\t\ttlsport := config.Port(mox.Conf.Static.ACME[l.TLS.ACME].Port, 443)\n\t\t\t\tensureServe(true, tlsport, \"acme-tls-alpn-01\", false)\n\t\t\t}\n\t\t}\n\t\treturn s\n\t}\n\n\tif l.TLS != nil && l.TLS.ACME != \"\" && (l.SMTP.Enabled && !l.SMTP.NoSTARTTLS || l.Submissions.Enabled || l.IMAPS.Enabled) {\n\t\tport := config.Port(mox.Conf.Static.ACME[l.TLS.ACME].Port, 443)\n\t\tensureServe(true, port, \"acme-tls-alpn-01\", false)\n\t}\n\n\tif l.AccountHTTP.Enabled {\n\t\tport := config.Port(l.AccountHTTP.Port, 80)\n\t\tpath := \"/\"\n\t\tif l.AccountHTTP.Path != \"\" {\n\t\t\tpath = l.AccountHTTP.Path\n\t\t}\n\t\tsrv := ensureServe(false, port, \"account-http at \"+path, true)\n\t\thandler := mox.SafeHeaders(http.StripPrefix(path[:len(path)-1], http.HandlerFunc(webaccount.Handler(path, l.AccountHTTP.Forwarded))))\n\t\tsrv.ServiceHandle(\"account\", accountHostMatch, path, handler)\n\t\tredirectToTrailingSlash(srv, accountHostMatch, \"account\", path)\n\t}\n\tif l.AccountHTTPS.Enabled {\n\t\tport := config.Port(l.AccountHTTPS.Port, 443)\n\t\tpath := \"/\"\n\t\tif l.AccountHTTPS.Path != \"\" {\n\t\t\tpath = l.AccountHTTPS.Path\n\t\t}\n\t\tsrv := ensureServe(true, port, \"account-https at \"+path, true)\n\t\thandler := mox.SafeHeaders(http.StripPrefix(path[:len(path)-1], http.HandlerFunc(webaccount.Handler(path, l.AccountHTTPS.Forwarded))))\n\t\tsrv.ServiceHandle(\"account\", accountHostMatch, path, handler)\n\t\tredirectToTrailingSlash(srv, accountHostMatch, \"account\", path)\n\t}\n\n\tif l.AdminHTTP.Enabled {\n\t\tport := config.Port(l.AdminHTTP.Port, 80)\n\t\tpath := \"/admin/\"\n\t\tif l.AdminHTTP.Path != \"\" {\n\t\t\tpath = l.AdminHTTP.Path\n\t\t}\n\t\tsrv := ensureServe(false, port, \"admin-http at \"+path, true)\n\t\thandler := mox.SafeHeaders(http.StripPrefix(path[:len(path)-1], http.HandlerFunc(webadmin.Handler(path, l.AdminHTTP.Forwarded))))\n\t\tsrv.ServiceHandle(\"admin\", listenerHostMatch, path, handler)\n\t\tredirectToTrailingSlash(srv, listenerHostMatch, \"admin\", path)\n\t}\n\tif l.AdminHTTPS.Enabled {\n\t\tport := config.Port(l.AdminHTTPS.Port, \n```\n# Function Signature: func (s *serve) ServeHTTP(xw http.ResponseWriter, r *http.Request) {\n# Natural Language Annotations: // ServeHTTP is the starting point for serving HTTP requests. It dispatches to the\n// right pathHandler or WebHandler, and it generates access logs and tracks\n// metrics.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (s *serve) ServeHTTP(xw http.ResponseWriter, r *http.Request) {\n\thost := dns.ParseHost(r.Host)\n\tif host.IsIP() {\n\t\thost.Domain = \"\"\n\t}\n\tif host.Domain == \"\" {\n\t\thost.Domain = mox.Conf.Static.HostnameDomain\n\t}\n\n\tlis := mox.Conf.GetListenerForHost(host.Domain)\n\tif lis == nil {\n\t\t// No mox.conf listener, let users host on their external domains. Also used to host ACME challenges.\n\t\tif s.Webserver {\n\t\t\thttp.DefaultServeMux.ServeHTTP(xw, r)\n\t\t\treturn\n\t\t}\n\t\txw.WriteHeader(404)\n\t\treturn\n\t}\n\n\tif s.Webserver {\n\t\thttp.DefaultServeMux.ServeHTTP(xw, r)\n\t\treturn\n\t}\n\n\tpath := r.URL.Path\n\tif path == \"/\" {\n\t\tpath = \"index.html\"\n\t}\n\n\tserviceHandler := s.SystemHandlersPath(host.Domain, path)\n\tif serviceHandler != nil {\n\t\tserviceHandler.Handler(xw, r)\n\t\treturn\n\t}\n\n\tsystemHandler := s.SystemHandlersPath(host.Domain, path)\n\tif systemHandler != nil {\n\t\tsystemHandler.Handler(xw, r)\n\t\treturn\n\t}\n\n\tpkglog.DebugX(\"404: host\", r.Host, \"path\", r.URL.Path).WithContext(r.Context())\n\txw.WriteHeader(404)\n\treturn\n}\n```", "  ```\nfunc (s *serve) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n\t// Any failures are handled last, after trying result from shortest path first.\n\t// This makes no difference for exact matches, but can be a difference for delegation.\n\tpp := strings.SplitN(r.URL.Path, \"/\", 3)\n\tvar hndlpath string\n\tvar hndlpathlog string\n\tvar hn pathHandler\n\tfor _, ih := range append(s.SystemHandlers, s.ServiceHandlers...) {\n\t\tif (ih.HostMatch != nil && !ih.HostMatch(r.RemoteAddr.Domain())) ||\n\t\t\t!strings.HasPrefix(ih.Path, pp[len(pp)-1]) ||\n\t\t\tlen(ih.Path) < len(hndlpath) {\n\t\t\tcontinue\n\t\t}\n\t\thndlpath = ih.Path\n\t\thndlpathlog = \"\"\n\t\tif hn != nil {\n\t\t\thndlpathlog = hn.Name\n\t\t}\n\t\thn = ih\n\t}\n\n\t// Find the first, if any, WebHandler matching our path.\n\t// Note that bookmark/alias and rewrite handling occurs later in webapisrv.\n\tvar wh http.Handler\n\tif hn != nil {\n\t\tif ihsm, ok := hn.Handler.(webaccount.SortedHandlersManager); ok {\n\t\t\t// Migrated webhandlers.\n\t\t\twh = ihsm.Wrap(\"no pathHandler, builtin\", func(w http.ResponseWriter, r *http.Request) {\n\t\t\t\thn.Handler.ServeHTTP(w, r)\n\t\t\t})\n\t\t} else {\n\t\t\t// Old-style pathHandler.\n\t\t\twh = mox.SafeHeaders(hn.Handler)\n\t\t}\n\t} else {\n\t\t// No pathHandler.\n\t\tvar wh1, wh2, wh3 http.Handler\n\t\tvar wh1log, wh2log, wh3log string\n\t\tfor _, w := range mox.Conf.Static.WebHandlers {\n\t\t\tmux := w\n\t\t\tpath := strings.SplitN(r.URL.Path, \"/\", 3)[1]\n\t\t\tif !mux.Handle(path) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif wh1 == nil || len(path) > len(wh1log) {\n\t\t\t\twh1, wh1log = mux.Handler, path\n\t\t\t}\n\t\t\tif wh2 == nil || len(path) > len(wh2log) {\n\t\t\t\twh2, wh2log = mux.Handler, path\n\t\t\t}\n\t\t\tif wh3 == nil || len(path) > len(wh3log) {\n\t\t\t\twh3, wh3log = mux.Handler, path\n\t\t\t}\n\t\t}\n\t\tif wh1 != nil {\n\t\t\twh = wh1\n\t\t\thndlpath, hndlpathlog = wh1log, \"(autogenerated for \"+wh1log+\")\"\n\t\t} else if wh2 != nil {\n\t\t\twh = wh2\n\t\t\thndlpath, hndlpathlog = wh2log, \"(autogenerated for \"+wh2log+\")\"\n\t\t} else if wh3 != nil {\n\t\t\twh = wh3\n\t\t\thndlpath, hndlpathlog = wh3log, \"(autogenerated for \"+wh3log+\")\"\n\t\t}\n\t}\n\tif wh == nil { // No handler found.\n\t\tif mox.Conf.ReferralSiterootRedirect || wh == nil { // No handler found.\n\t\t\tif !strings.HasSuffix(r.URL.Path, \"/\") {\n\t\t\t\thttp.Redirect(w, r, r.URL.Path+\"/\", http.StatusMovedPermanently)\n\t\t\t\treturn\n\t\t\t}\n\t\t\thttp.Error(w, http.StatusText(http.StatusNotFound), http.StatusNotFound)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Rate limit.\n\tif r.Method != \"CONNECT\"", "  ```\nfunc (s *serve) ServeHTTP(xw http.ResponseWriter, r *http.Request) {\n\t// We only want to keep those as dimensions for logs where they are explicitly\n\t// configured handlers.\n\tmetricRolecount := map[string]int{}\n\n\t// Don't log 404s at this stage, handled at end.\n\tmetricRolecount[\"error\"] = 1\n\n\thandler := xw\n\n\t// We want the length of the longest match, at least if two pathHandlers match (first one\n\t// wins).\n\tpathHandlerHasMatch := false\n\tvar pathHandlerMatchLen int\n\n\t// First try service handlers.\n\t// Check if there is a matching system handler, matched by longest first.\n\tfor _, sh := range s.ServiceHandlers {\n\t\t// Skip handlers with unmatching domain.\n\t\tif sh.HostMatch != nil && !sh.HostMatch(dns.IPDomain(r.Host)) {\n\t\t\tcontinue\n\t\t}\n\n\t\tif pathHandlerMatchLen = matchPathHandler(r.URL.Path, sh); pathHandlerMatchLen > 0 {\n\t\t\tif pathHandlerMatchLen > len(r.URL.Path) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\txw = &loggingWriter{xw, time.Now(), r, false, sh.Name, sh.Handler, 0, 0, &metricRolecount, nil}\n\t\t\thandler = sh.Handler\n\t\t\tpathHandlerHasMatch = true\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// Next up system handlers.\n\t// Check if there is a matching system handler, matched by longest first.\n\tfor _, sh := range s.SystemHandlers {\n\t\t// Skip handlers with unmatching domain.\n\t\tif sh.HostMatch != nil && !sh.HostMatch(dns.IPDomain(r.Host)) {\n\t\t\tcontinue\n\t\t}\n\n\t\tif pathHandlerMatchLen = matchPathHandler(r.URL.Path, sh); pathHandlerMatchLen > 0 {\n\t\t\tif pathHandlerMatchLen > len(r.URL.Path) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\txw = &loggingWriter{xw, time.Now(), r, false, sh.Name, sh.Handler, 0, 0, &metricRolecount, nil}\n\t\t\thandler = sh.Handler\n\t\t\tpathHandlerHasMatch = true\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// If there is a pathHandler that dispatched, and the URL still matches, we reached\n\t// something that will return a status and set the logger status code.\n\tif pathHandlerHasMatch {\n\t\thandler.ServeHTTP(xw, r)\n\t\txw.Done()\n\t\treturn\n\t}\n\n\tif !s.Webserver {\n\t\t// Dispatch to default handler, which returns a 404.\n\t\tserverErrorDefaults.ServeHTTP(xw, r)\n\t\txw.Done()\n\t\treturn\n\t}\n\n\t// No matching handler.\n\tmetricRolecount[\"error\"] = 404\n\n\t// The Unix domain, if any, is logged after the handler's logging writer is created.\n\thostPartIndex := strings.LastIndexByte(r.Host, '.')\n\tif hostPartIndex > -1 {\n\t\tr.Host = r.Host[:hostPartIndex]\n\t}\n\n\t// We aim to keep ServeHTTP simple and have other pre- and post- handlers, for higher\n\t// priority access constraints, cache config and other setup. The pre-handlers need to\n\t// be configured in advance. Future ideas:\n\t// - Rewrite rules, for example to force HTTPS. Have a possible mux.Rule() interface,\n\t//   with local context to enable/disable rules during runtime configuration.\n\t// - Cache rules: cache config and file config (see webserver). Cache settings may\n\t//   include blacklisting of handlers.\n\t// - Compression for specific handlers, by Content-Type or host.\n\t// - WAF, firewalling, auditing and access controls, and possibly virtual hosts.\n\t//\n\t// When adding new functionality, we also need to update mox-test/test/cases/http.sh\n\t// to validate correct server behavior, coverage and correct headers/"]}, "mox-moxio/bufpool.go-put": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/moxio/bufpool.go:\n```\npackage moxio\n\nimport (\n\t\"bufio\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// todo: instead of a bufpool, should maybe just make an alternative to bufio.Reader with a big enough buffer that we can fully use to read a line.\n\nvar ErrLineTooLong = errors.New(\"line from remote too long\") // Returned by Bufpool.Readline.\n\n// Bufpool caches byte slices for reuse during parsing of line-terminated commands.\ntype Bufpool struct {\n\tc    chan []byte\n\tsize int\n}\n\n// NewBufpool makes a new pool, initially empty, but holding at most \"max\" buffers of \"size\" bytes each.\nfunc NewBufpool(max, size int) *Bufpool {\n\treturn &Bufpool{\n\t\tc:    make(chan []byte, max),\n\t\tsize: size,\n\t}\n}\n\n// get returns a buffer from the pool if available, otherwise allocates a new buffer.\n// The buffer should be returned with a call to put.\nfunc (b *Bufpool) get() []byte {\n\tvar buf []byte\n\n\t// Attempt to get buffer from pool. Otherwise create new buffer.\n\tselect {\n\tcase buf = <-b.c:\n\tdefault:\n\t}\n\tif buf == nil {\n\t\tbuf = make([]byte, b.size)\n\t}\n\treturn buf\n}\n\n// put puts a \"buf\" back in the pool. Put clears the first \"n\" bytes, which should\n// be all the bytes that have been read in the buffer. If the pool is full, the\n// buffer is discarded, and will be cleaned up by the garbage collector.\n// The caller should no longer reference \"buf\" after a call to put.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Readline reads a \\n- or \\r\\n-terminated line. Line is returned without \\n or \\r\\n.\n// If the line was too long, ErrLineTooLong is returned.\n// If an EOF is encountered before a \\n, io.ErrUnexpectedEOF is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (b *Bufpool) put(log mlog.Log, buf []byte, n int) {\n# Natural Language Annotations: // put puts a \"buf\" back in the pool. Put clears the first \"n\" bytes, which should\n// be all the bytes that have been read in the buffer. If the pool is full, the\n// buffer is discarded, and will be cleaned up by the garbage collector.\n// The caller should no longer reference \"buf\" after a call to put.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (b *Bufpool) put(log mlog.Log, buf []byte, n int) {\n\tif n == 0 || n >= b.size {\n\t\tlog.Warn(\"Tried to put a buffer that was too short or too large in the pool\")\n\t\treturn\n\t}\n\t// Clear the first \"n\" bytes of the buffer.\n\tfor i := 0; i < n; i++ {\n\t\tbuf[i] = 0\n\t}\n\t// Attempt to put the buffer back in the pool.\n\tselect {\n\tcase b.c <- buf:\n\tdefault:\n\t\tlog.Info(\"Buffer pool is full, discarding buffer\")\n\t}\n}\n```\nThis function takes a `*Bufpool` pointer, a `mlog.Log` object, a `[]byte` buffer, and an integer `n` representing the number of bytes that have been read from the buffer. It clears the first `n` bytes of the buffer and then attempts to put the buffer back in the pool. If the pool is full, the function logs an informational message and discards the buffer, which will be cleaned up by the garbage collector. The caller should no longer reference the buffer after a call to this function.", "  ```\nfunc (b *Bufpool) put(log mlog.Log, buf []byte, n int) {\n\t// log a warning if the buffer is too large for the pool\n\tif n > b.size {\n\t\tlog.Warn(\"buffer is larger than buffer pool size\",\n\t\t\t\"buffer_size\", n,\n\t\t\t\"pool_size\", b.size,\n\t\t)\n\t}\n\n\t// clear the first \"n\" bytes of the buffer, which should be all the bytes that have been read in the buffer\n\tclear := func(b []byte, n int) {\n\t\tfor i := 0; i < n; i++ {\n\t\t\tb[i] = 0\n\t\t}\n\t}\n\tclear(buf, n)\n\n\t// put the buffer back in the pool or discard it if the pool is full\n\tselect {\n\tcase b.c <- buf:\n\tdefault:\n\t\t// if the buffer pool is full, discard the buffer and log a warning\n\t\tlog.Warn(\"buffer pool is full, discarding buffer\",\n\t\t\t\"buffer_size\", b.size,\n\t\t\t\"pool_size\", b.c,\n\t\t)\n\t}\n}\n```", "  ```\npackage moxio\n\n// Put returns a buffer to the pool. It clears the first n bytes of the buffer,\n// which should be all the bytes that have been read in the buffer. If the\n// pool is full, the buffer is discarded, and will be cleaned up by the\n// garbage collector. The caller should no longer reference \"buf\" after a call\n// to Put.\nfunc (b *Bufpool) Put(log mlog.Log, buf []byte, n int) {\n\t// Copy the buffer to the channel and overwrite the original buffer with\n\t// a fresh one from the channel (if one is available).\n\tselect {\n\tcase <-b.c:\n\t\tif len(buf) > 0 {\n\t\t\tb.c <- buf[:n]\n\t\t}\n\tdefault:\n\t\tbuf = b.get() // Allocate a new buffer if necessary\n\t}\n}\n```"]}, "mox-moxio/bufpool.go-Readline": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/moxio/bufpool.go:\n```\npackage moxio\n\nimport (\n\t\"bufio\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// todo: instead of a bufpool, should maybe just make an alternative to bufio.Reader with a big enough buffer that we can fully use to read a line.\n\nvar ErrLineTooLong = errors.New(\"line from remote too long\") // Returned by Bufpool.Readline.\n\n// Bufpool caches byte slices for reuse during parsing of line-terminated commands.\ntype Bufpool struct {\n\tc    chan []byte\n\tsize int\n}\n\n// NewBufpool makes a new pool, initially empty, but holding at most \"max\" buffers of \"size\" bytes each.\nfunc NewBufpool(max, size int) *Bufpool {\n\treturn &Bufpool{\n\t\tc:    make(chan []byte, max),\n\t\tsize: size,\n\t}\n}\n\n// get returns a buffer from the pool if available, otherwise allocates a new buffer.\n// The buffer should be returned with a call to put.\nfunc (b *Bufpool) get() []byte {\n\tvar buf []byte\n\n\t// Attempt to get buffer from pool. Otherwise create new buffer.\n\tselect {\n\tcase buf = <-b.c:\n\tdefault:\n\t}\n\tif buf == nil {\n\t\tbuf = make([]byte, b.size)\n\t}\n\treturn buf\n}\n\n// put puts a \"buf\" back in the pool. Put clears the first \"n\" bytes, which should\n// be all the bytes that have been read in the buffer. If the pool is full, the\n// buffer is discarded, and will be cleaned up by the garbage collector.\n// The caller should no longer reference \"buf\" after a call to put.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Readline reads a \\n- or \\r\\n-terminated line. Line is returned without \\n or \\r\\n.\n// If the line was too long, ErrLineTooLong is returned.\n// If an EOF is encountered before a \\n, io.ErrUnexpectedEOF is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (b *Bufpool) Readline(log mlog.Log, r *bufio.Reader) (line string, rerr error) {\n# Natural Language Annotations: // Readline reads a \\n- or \\r\\n-terminated line. Line is returned without \\n or \\r\\n.\n// If the line was too long, ErrLineTooLong is returned.\n// If an EOF is encountered before a \\n, io.ErrUnexpectedEOF is returned.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (b *Bufpool) Readline(log mlog.Log, r *bufio.Reader) (line string, rerr error) {\n\tvar (\n\t\tbuf  []byte\n\t\ti    int\n\t\tsize = b.size\n\t\terr  error\n\t)\n\n\t// Get a buffer from the pool.\n\tbuf = b.get()\n\n\t// Read from the buffer until a newline or an error occurs.\n\tfor i = 0; i < size; i++ {\n\t\tbuf[i], err = r.ReadByte()\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n\t\tif buf[i] == '\\n' || buf[i] == '\\r' {\n\t\t\tif buf[i] == '\\r' {\n\t\t\t\ti++\n\t\t\t\t// Check if we reached the end of the file.\n\t\t\t\tbuf[i], err = r.ReadByte()\n\t\t\t\tif err != nil {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\t// Check if the next byte is a newline.\n\t\t\t\tif buf[i] != '\\n' {\n\t\t\t\t\trerr = io.ErrUnexpectedEOF\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tline = string(buf[:i])\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// If we didn't find a newline, and the buffer is too long, return ErrLineTooLong.\n\tif i == size && err == nil {\n\t\trerr = ErrLineTooLong\n\t}\n\n\t// Put the buffer back in the pool.\n\tb.put(buf)\n\n\t// Return the line and any error.\n\treturn line, rerr\n}\n```", "  ```\nfunc (b *Bufpool) Readline(log mlog.Log, r *bufio.Reader) (line string, rerr error) {\n\tvar buf []byte\n\tvar err error\n\n\t// Attempt to get a buffer from the pool.\n\tbuf = b.get()\n\tline, err = r.ReadSlice('\\n')\n\tif err != nil && err != io.EOF {\n\t\treturn \"\", err\n\t}\n\n\t// If the line was too long, stop reading.\n\tif len(line) >= len(buf) {\n\t\treturn \"\", ErrLineTooLong\n\t}\n\n\t// If the line was shorter than the buffer size, adjust the buffer so that it's only as big as necessary.\n\tif len(line) < len(buf) - 1 {\n\t\tbuf = buf[:len(line)]\n\t}\n\n\t// Allocate a new buffer if the pool is not full.\n\tif len(b.c) < cap(b.c) {\n\t\tb.c <- buf\n\t}\n\treturn string(line), nil\n}\n```", "  ```\nfunc (b *Bufpool) Readline(log mlog.Log, r *bufio.Reader) (line string, rerr error) {\n\tvar readline = func(rd *bufio.Reader, lastEOF bool) ([]byte, bool, error) {\n\t\tvar line []byte\n\t\tvar isPrefix bool\n\t\tvar err error\n\n\t\t// Attempt to read up to max (but not including) a \\n from the buffer.\n\t\tif line, isPrefix, err = rd.ReadLine(); err == io.EOF {\n\t\t\t// Check for any errors that may have occurred during the read.\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorf(\"err occurred during line read: %v\", err)\n\t\t\t\treturn nil, false, err\n\t\t\t} else if isPrefix {\n\t\t\t\tlog.Error(\"line too long\")\n\t\t\t\treturn nil, false, ErrLineTooLong\n\t\t\t}\n\t\t} else if err != nil {\n\t\t\tlog.Errorf(\"error encountered during read: %v\", err)\n\t\t\treturn nil, isPrefix, err\n\t\t}\n\n\t\t// If we read a \\n during the last read, return the line.\n\t\tif !isPrefix {\n\t\t\treturn line, false, nil\n\t\t}\n\n\t\t// If we are in this loop, then our buffer is too small for a full line.\n\t\t// Try to read the rest of the line with a larger buffer.\n\t\tvar more []byte\n\t\tif more, isPrefix, err = readline(rd, true); err != nil {\n\t\t\treturn nil, isPrefix, err\n\t\t}\n\t\treturn append(line, more...), isPrefix, err\n\t}\n\n\tbuf := b.get()\n\tline, _, err := readline(r, false)\n\tif err != nil {\n\t\tlog.Errorf(\"error reading line: %v\", err)\n\t\treturn \"\", err\n\t}\n\tif err == io.EOF {\n\t\tlog.Error(\"unexpected EOF encountered while reading line\")\n\t\treturn \"\", io.ErrUnexpectedEOF\n\t}\n\treturn string(line), nil\n}\n```"]}, "mox-moxio/linkcopy.go-LinkOrCopy": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/moxio/linkcopy.go:\n```\npackage moxio\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"os\"\n\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// LinkOrCopy attempts to make a hardlink dst. If that fails, it will try to do a\n// regular file copy. If srcReaderOpt is not nil, it will be used for reading. If\n// sync is true and the file is copied, Sync is called on the file after writing to\n// ensure the file is written on disk. Callers should also sync the directory of\n// the destination file, but may want to do that after linking/copying multiple\n// files. If dst was created and an error occurred, it is removed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func LinkOrCopy(log mlog.Log, dst, src string, srcReaderOpt io.Reader, sync bool) (rerr error) {\n# Natural Language Annotations: // LinkOrCopy attempts to make a hardlink dst. If that fails, it will try to do a\n// regular file copy. If srcReaderOpt is not nil, it will be used for reading. If\n// sync is true and the file is copied, Sync is called on the file after writing to\n// ensure the file is written on disk. Callers should also sync the directory of\n// the destination file, but may want to do that after linking/copying multiple\n// files. If dst was created and an error occurred, it is removed.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\npackage moxio\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"os\"\n\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// LinkOrCopy attempts to make a hardlink dst. If that fails, it will try to do a\n// regular file copy. If srcReaderOpt is not nil, it will be used for reading. If\n// sync is true and the file is copied, Sync is called on the file after writing to\n// ensure the file is written on disk. Callers should also sync the directory of\n// the destination file, but may want to do that after linking/copying multiple\n// files. If dst was created and an error occurred, it is removed.\nfunc LinkOrCopy(log mlog.Log, dst, src string, srcReaderOpt io.Reader, sync bool) (rerr error) {\n\t// Try to make a hard link\n\tif err := os.Link(src, dst); err != nil {\n\t\t// If that fails, fall back to a regular file copy\n\t\tsrcReader := io.Reader(os.NewFile(src, os.O_RDONLY, 0))\n\t\tif srcReaderOpt != nil {\n\t\t\tsrcReader = srcReaderOpt\n\t\t}\n\t\tdstFile, err := os.Create(dst)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to create dst: %w\", err)\n\t\t}\n\t\tdefer func() {\n\t\t\tif rerr != nil {\n\t\t\t\tdstFile.Close()\n\t\t\t\tos.Remove(dst)\n\t\t\t}\n\t\t}()\n\t\t_, err = io.Copy(dstFile, srcReader)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to copy src to dst: %w\", err)\n\t\t}\n\t\tif sync {\n\t\t\tif err := dstFile.Sync(); err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to sync dst: %w\", err)\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n```", "  ```\npackage moxio\n\nimport (\n\t\"io\"\n\t\"log/slog\"\n\t\"os\"\n\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// LinkOrCopy attempts to make a hardlink dst. If that fails, it will try to do a\n// regular file copy. If srcReaderOpt is not nil, it will be used for reading. If\n// sync is true and the file is copied, Sync is called on the file after writing to\n// ensure the file is written on disk. Callers should also sync the directory of\n// the destination file, but may want to do that after linking/copying multiple\n// files. If dst was created and an error occurred, it is removed.\nfunc LinkOrCopy(dst, src string, srcReaderOpt io.Reader, sync bool) error {\n\t// Try to make a hardlink\n\terr := os.Link(src, dst)\n\tif err == nil {\n\t\t// Hardlink succeeded, return\n\t\treturn nil\n\t}\n\n\t// Hardlink failed, try to copy the file\n\tsrcReader := srcReaderOpt\n\tif srcReader == nil {\n\t\t// Create a file reader if one was not provided\n\t\tvar err error\n\t\tsrcReader, err = os.Open(src)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer srcReader.Close()\n\t}\n\n\tdstFile, err := os.Create(dst)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdefer dstFile.Close()\n\n\t_, err = io.Copy(dstFile, srcReader)\n\tif err != nil {\n\t\t// Copy failed, remove the destination file and return the error\n\t\tos.Remove(dst)\n\t}\n\n\tif sync {\n\t\t// Sync the file to ensure it is written to disk\n\t\terr = dstFile.Sync()\n\t\tif err != nil {\n\t\t\t// Sync failed, return the error\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn err\n}\n```", "  ```\npackage moxio\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"os\"\n\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// LinkOrCopy attempts to make a hardlink dst. If that fails, it will try to do a\n// regular file copy. If srcReaderOpt is not nil, it will be used for reading. If\n// sync is true and the file is copied, Sync is called on the file after writing to\n// ensure the file is written on disk. Callers should also sync the directory of\n// the destination file, but may want to do that after linking/copying multiple\n// files. If dst was created and an error occurred, it is removed.\nfunc LinkOrCopy(log mlog.Log, dst, src string, srcReaderOpt io.Reader, sync bool) (rerr error) {\n\tvar srcReader io.Reader\n\tif srcReaderOpt != nil {\n\t\tsrcReader = srcReaderOpt\n\t} else {\n\t\tsrcReader, rerr = os.Open(src)\n\t\tif rerr != nil {\n\t\t\treturn rerr\n\t\t}\n\t}\n\tdefer srcReader.Close()\n\n\terr := tryHardLinking(log, dst, src)\n\tif err != nil {\n\t\terr = tryRegularCopy(log, dst, srcReader)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif sync {\n\t\tf, err := os.Open(dst)\n\t\tif err != nil {\n\t\t\tlog.Printf(\"can't open destination file for syncing, but link/copy succeeded: %v\", err)\n\t\t\treturn nil\n\t\t}\n\t\tdefer f.Close()\n\t\terr = f.Sync()\n\t\tif err != nil {\n\t\t\tlog.Warningf(\"failed to sync(2) destination file after copying: %v\", err)\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc tryHardLinking(log mlog.Log, dst, src string) error {\n\terr := os.Link(src, dst)\n\tif err != nil {\n\t\tlog.Debugf(\"hardlink from %s to %s failed: %v\", src, dst, err)\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc tryRegularCopy(log mlog.Log, dst string, srcReader io.Reader) error {\n\tf, err := os.Create(dst)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer f.Close()\n\n\t_, err = io.Copy(f, srcReader)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n```"]}, "mox-moxio/base64writer.go-Base64Writer": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/moxio/base64writer.go:\n```\npackage moxio\n\nimport (\n\t\"encoding/base64\"\n\t\"io\"\n)\n\n// implement io.Closer\ntype closerFunc func() error\n\nfunc (f closerFunc) Close() error {\n\treturn f()\n}\n\n// Base64Writer turns a writer for data into one that writes base64 content on\n// \\r\\n separated lines of max 78+2 characters length.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype lineWrapper struct {\n\tw io.Writer\n\tn int // Written on current line.\n}\n\nfunc (lw *lineWrapper) Write(buf []byte) (int, error) {\n\twrote := 0\n\tfor len(buf) > 0 {\n\t\tn := 78 - lw.n\n\t\tif n > len(buf) {\n\t\t\tn = len(buf)\n\t\t}\n\t\tnn, err := lw.w.Write(buf[:n])\n\t\tif nn > 0 {\n\t\t\twrote += nn\n\t\t\tbuf = buf[nn:]\n\t\t}\n\t\tif err != nil {\n\t\t\treturn wrote, err\n\t\t}\n\t\tlw.n += nn\n\t\tif lw.n == 78 {\n\t\t\t_, err := lw.w.Write([]byte(\"\\r\\n\"))\n\t\t\tif err != nil {\n\t\t\t\treturn wrote, err\n\t\t\t}\n\t\t\tlw.n = 0\n\t\t}\n\t}\n\treturn wrote, nil\n}\n\nfunc (lw *lineWrapper) Close() error {\n\tif lw.n > 0 {\n\t\tlw.n = 0\n\t\t_, err := lw.w.Write([]byte(\"\\r\\n\"))\n\t\treturn err\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func Base64Writer(w io.Writer) io.WriteCloser {\n# Natural Language Annotations: // Base64Writer turns a writer for data into one that writes base64 content on\n// \\r\\n separated lines of max 78+2 characters length.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc Base64Writer(w io.Writer) io.WriteCloser {\n\tb64w := new(base64Writer)\n\tb64w.w = w\n\treturn b64w\n}\n\ntype base64Writer struct {\n\tw io.Writer\n\tencoder *base64.Encoder\n}\n\nfunc (b64w *base64Writer) Write(p []byte) (n int, err error) {\n\tn, err = b64w.w.Write(p)\n\treturn n, err\n}\n\nfunc (b64w *base64Writer) Close() error {\n\tif b64w.encoder != nil {\n\t\terr := b64w.encoder.Flush()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n```", "  ```\npackage moxio\n\nimport (\n\t\"encoding/base64\"\n\t\"io\"\n)\n\nfunc Base64Writer(w io.Writer) io.WriteCloser {\n\treturn &lineWrapper{w: w}\n}\n```\n\nThis function takes an `io.Writer` as an argument and returns a new `io.WriteCloser` that writes base64-encoded content on \\r\\n separated lines of max 78+2 characters length. The function wraps the given writer in a `lineWrapper` struct, which implements the `io.Writer` and `io.Closer` interfaces.\n\nThe `lineWrapper` struct keeps track of the number of characters written on the current line and writes \\r\\n to the underlying writer every time a line is completed. It also implements the `io.Closer` interface to handle closing the underlying writer when it is no longer needed.", "  ```\nfunc Base64Writer(w io.Writer) io.WriteCloser {\n\treturn &lineWrapper{w: w}\n}\n```\nThis function creates a new instance of `lineWrapper` with the provided `io.Writer`, and returns a `io.WriteCloser` that wraps the `lineWrapper`. The `lineWrapper` is responsible for writing base64 content on lines of max 78+2 characters length, and the function signature `func Base64Writer(w io.Writer) io.WriteCloser` indicates that it accepts an `io.Writer` and returns a `io.WriteClose`r. The `io.WriteCloser` interface has two methods: `Write` and `Close`. The `Write` method is used to write data to the wrapped writer, and the `Close` method is used to close the wrapped writer."]}, "mox-dkim/sig.go-Header": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dkim/sig.go:\n```\npackage dkim\n\nimport (\n\t\"bytes\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// Sig is a DKIM-Signature header.\n//\n// String values must be compared case insensitively.\ntype Sig struct {\n\t// Required fields.\n\tVersion       int        // Version, 1. Field \"v\". Always the first field.\n\tAlgorithmSign string     // \"rsa\" or \"ed25519\". Field \"a\".\n\tAlgorithmHash string     // \"sha256\" or the deprecated \"sha1\" (deprecated). Field \"a\".\n\tSignature     []byte     // Field \"b\".\n\tBodyHash      []byte     // Field \"bh\".\n\tDomain        dns.Domain // Field \"d\".\n\tSignedHeaders []string   // Duplicates are meaningful. Field \"h\".\n\tSelector      dns.Domain // Selector, for looking DNS TXT record at <s>._domainkey.<domain>. Field \"s\".\n\n\t// Optional fields.\n\t// Canonicalization is the transformation of header and/or body before hashing. The\n\t// value is in original case, but must be compared case-insensitively. Normally two\n\t// slash-separated values: header canonicalization and body canonicalization. But\n\t// the \"simple\" means \"simple/simple\" and \"relaxed\" means \"relaxed/simple\". Field\n\t// \"c\".\n\tCanonicalization string\n\tLength           int64     // Body length to verify, default -1 for whole body. Field \"l\".\n\tIdentity         *Identity // AUID (agent/user id). If nil and an identity is needed, should be treated as an Identity without localpart and Domain from d= field. Field \"i\".\n\tQueryMethods     []string  // For public key, currently known value is \"dns/txt\" (should be compared case-insensitively). If empty, dns/txt must be assumed. Field \"q\".\n\tSignTime         int64     // Unix epoch. -1 if unset. Field \"t\".\n\tExpireTime       int64     // Unix epoch. -1 if unset. Field \"x\".\n\tCopiedHeaders    []string  // Copied header fields. Field \"z\".\n}\n\n// Identity is used for the optional i= field in a DKIM-Signature header. It uses\n// the syntax of an email address, but does not necessarily represent one.\ntype Identity struct {\n\tLocalpart *smtp.Localpart // Optional.\n\tDomain    dns.Domain\n}\n\n// String returns a value for use in the i= DKIM-Signature field.\nfunc (i Identity) String() string {\n\ts := \"@\" + i.Domain.ASCII\n\t// We need localpart as pointer to indicate it is missing because localparts can be\n\t// \"\" which we store (decoded) as empty string and we need to differentiate.\n\tif i.Localpart != nil {\n\t\ts = i.Localpart.String() + s\n\t}\n\treturn s\n}\n\nfunc newSigWithDefaults() *Sig {\n\treturn &Sig{\n\t\tCanonicalization: \"simple/simple\",\n\t\tLength:           -1,\n\t\tSignTime:         -1,\n\t\tExpireTime:       -1,\n\t}\n}\n\n// Algorithm returns an algorithm string for use in the \"a\" field. E.g.\n// \"ed25519-sha256\".\nfunc (s Sig) Algorithm() string {\n\treturn s.AlgorithmSign + \"-\" + s.AlgorithmHash\n}\n\n// Header returns the DKIM-Signature header in string form, to be prepended to a\n// message, including DKIM-Signature field name and trailing \\r\\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Like quoted printable, but with \"|\" encoded as well.\n// We also encode \":\" because it is used as separator in DKIM headers which can\n// cause trouble for \"q\", even though it is listed in dkim-safe-char,\n// ../rfc/6376:497.\nfunc packQpHdrValue(s string) string {\n\t// ../rfc/6376:474\n\tconst hex = \"0123456789ABCDEF\"\n\tvar r string\n\tfor _, b := range []byte(s) {\n\t\tif b > ' ' && b < 0x7f && b != ';' && b != '=' && b != '|' && b != ':' {\n\t\t\tr += string(b)\n\t\t} else {\n\t\t\tr += \"=\" + string(hex[b>>4]) + string(hex[(b>>0)&0xf])\n\t\t}\n\t}\n\treturn r\n}\n\nvar (\n\terrSigHeader         = errors.New(\"not DKIM-Signature header\")\n\terrSigDuplicateTag   = errors.New(\"duplicate tag\")\n\terrSigMissingCRLF    = errors.New(\"missing crlf at end\")\n\terrSigExpired        = errors.New(\"signature timestamp (t=) must be before signature expiration (x=)\")\n\terrSigIdentityDomain = errors.New(\"identity domain (i=) not under domain (d=)\")\n\terrSigMissingTag     = errors.New(\"missing required tag\")\n\terrSigUnknownVersion = errors.New(\"unknown version\")\n\terrSigBodyHash       = errors.New(\"bad body hash size given algorithm\")\n)\n\n// parseSignatures returns the parsed form of a DKIM-Signature header.\n//\n// buf must end in crlf, as it should have occurred in the mail message.\n//\n// The dkim signature with signature left empty (\"b=\") and without trailing\n// crlf is returned, for use in verification.\nfunc parseSignature(buf []byte, smtputf8 bool) (sig *Sig, verifySig []byte, err error) {\n\tdefer func() {\n\t\tif x := recover(); x == nil {\n\t\t\treturn\n\t\t} else if xerr, ok := x.(error); ok {\n\t\t\tsig = nil\n\t\t\tverifySig = nil\n\t\t\terr = xerr\n\t\t} else {\n\t\t\tpanic(x)\n\t\t}\n\t}()\n\n\txerrorf := func(format string, args ...any) {\n\t\tpanic(fmt.Errorf(format, args...))\n\t}\n\n\tif !bytes.HasSuffix(buf, []byte(\"\\r\\n\")) {\n\t\txerrorf(\"%w\", errSigMissingCRLF)\n\t}\n\tbuf = buf[:len(buf)-2]\n\n\tds := newSigWithDefaults()\n\tseen := map[string]struct{}{}\n\tp := parser{s: string(buf), smtputf8: smtputf8}\n\tname := p.xhdrName(false)\n\tif !strings.EqualFold(name, \"DKIM-Signature\") {\n\t\txerrorf(\"%w\", errSigHeader)\n\t}\n\tp.wsp()\n\tp.xtake(\":\")\n\tp.wsp()\n\t// ../rfc/6376:655\n\t// ../rfc/6376:656 ../rfc/6376-eid5070\n\t// ../rfc/6376:658 ../rfc/6376-eid5070\n\tfor {\n\t\tp.fws()\n\t\tk := p.xtagName()\n\t\tp.fws()\n\t\tp.xtake(\"=\")\n\t\t// Special case for \"b\", see below.\n\t\tif k != \"b\" {\n\t\t\tp.fws()\n\t\t}\n\t\t// Keys are case-sensitive: ../rfc/6376:679\n\t\tif _, ok := seen[k]; ok {\n\t\t\t// Duplicates not allowed: ../rfc/6376:683\n\t\t\txerrorf(\"%w: %q\", errSigDuplicateTag, k)\n\t\t\tbreak\n\t\t}\n\t\tseen[k] = struct{}{}\n\n\t\t// ../rfc/6376:1021\n\t\tswitch k {\n\t\tcase \"v\":\n\t\t\t// ../rfc/6376:1025\n\t\t\tds.Version = int(p.xnumber(10))\n\t\t\tif ds.Version != 1 {\n\t\t\t\txerrorf(\"%w: version %d\", errSigUnknownVersion, ds.Version)\n\t\t\t}\n\t\tcase \"a\":\n\t\t\t// ../rfc/6376:1038\n\t\t\tds.AlgorithmSign, ds.AlgorithmHash = p.xalgorithm()\n\t\tcase \"b\":\n\t\t\t// ../rfc/6376:1054\n\t\t\t// To calculate the hash, we have to feed the DKIM-Signature header to the hash\n\t\t\t// function, but with the value for \"b=\" (the signature) left out. The parser\n\t\t\t// tracks all data that is read, except when drop is true.\n\t\t\t// ../rfc/6376:997\n\t\t\t// Surrounding whitespace must be cleared as well. ../rfc/6376:1659\n\t\t\t// Note: The RFC says \"surrounding\" whitespace, but whitespace is only allowed\n\t\t\t// before the value as part of the ABNF production for \"b\". Presumably the\n\t\t\t// intention is to ignore the trailing \"[FWS]\" for the tag-spec production,\n\t\t\t// ../rfc/6376:656\n\t\t\t// Another indication is the term \"value portion\", ../rfc/6376:1667. It appears to\n\t\t\t// mean everything after the \"b=\" part, instead of the actual value (either encoded\n\t\t\t// or decoded).\n\t\t\tp.drop = true\n\t\t\tp.fws()\n\t\t\tds.Signature = p.xbase64()\n\t\t\tp.fws()\n\t\t\tp.drop = false\n\t\tcase \"bh\":\n\t\t\t// ../rfc/6376:1076\n\t\t\tds.BodyHash = p.xbase64()\n\t\tcase \"c\":\n\t\t\t// ../rfc/6376:1088\n\t\t\tds.Canonicalization = p.xcanonical()\n\t\t\t// ../rfc/6376:810\n\t\tcase \"d\":\n\t\t\t// ../rfc/6376:1105\n\t\t\tds.Domain = p.xdomain()\n\t\tcase \"h\":\n\t\t\t// ../rfc/6376:1134\n\t\t\tds.SignedHeaders = p.xsignedHeaderFields()\n\t\tcase \"i\":\n\t\t\t// ../rfc/6376:1171\n\t\t\tid := p.xauid()\n\t\t\tds.Identity = &id\n\t\tcase \"l\":\n\t\t\t// ../rfc/6376:1244\n\t\t\tds.Length = p.xbodyLength()\n\t\tcase \"q\":\n\t\t\t// ../rfc/6376:1268\n\t\t\tds.QueryMethods = p.xqueryMethods()\n\t\tcase \"s\":\n\t\t\t// ../rfc/6376:1300\n\t\t\tds.Selector = p.xselector()\n\t\tcase \"t\":\n\t\t\t// ../rfc/6376:1310\n\t\t\tds.SignTime = p.xtimestamp()\n\t\tcase \"x\":\n\t\t\t// ../rfc/6376:1327\n\t\t\tds.ExpireTime = p.xtimestamp()\n\t\tcase \"z\":\n\t\t\t// ../rfc/6376:1361\n\t\t\tds.CopiedHeaders = p.xcopiedHeaderFields()\n\t\tdefault:\n\t\t\t// We must ignore unknown fields. ../rfc/6376:692 ../rfc/6376:1022\n\t\t\tp.xchar() // ../rfc/6376-eid5070\n\t\t\tfor !p.empty() && !p.hasPrefix(\";\") {\n\t\t\t\tp.xchar()\n\t\t\t}\n\t\t}\n\t\tp.fws()\n\n\t\tif p.empty() {\n\t\t\tbreak\n\t\t}\n\t\tp.xtake(\";\")\n\t\tif p.empty() {\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// ../rfc/6376:2532\n\trequired := []string{\"v\", \"a\", \"b\", \"bh\", \"d\", \"h\", \"s\"}\n\tfor _, req := range required {\n\t\tif _, ok := seen[req]; !ok {\n\t\t\txerrorf(\"%w: %q\", errSigMissingTag, req)\n\t\t}\n\t}\n\n\tif strings.EqualFold(ds.AlgorithmHash, \"sha1\") && len(ds.BodyHash) != 20 {\n\t\txerrorf(\"%w: got %d bytes, must be 20 for sha1\", errSigBodyHash, len(ds.BodyHash))\n\t} else if strings.EqualFold(ds.AlgorithmHash, \"sha256\") && len(ds.BodyHash) != 32 {\n\t\txerrorf(\"%w: got %d bytes, must be 32 for sha256\", errSigBodyHash, len(ds.BodyHash))\n\t}\n\n\t// ../rfc/6376:1337\n\tif ds.SignTime >= 0 && ds.ExpireTime >= 0 && ds.SignTime >= ds.ExpireTime {\n\t\txerrorf(\"%w\", errSigExpired)\n\t}\n\n\t// Default identity is \"@\" plus domain. We don't set this value because we want to\n\t// keep the distinction between absent value.\n\t// ../rfc/6376:1172 ../rfc/6376:2537 ../rfc/6376:2541\n\tif ds.Identity != nil && ds.Identity.Domain.ASCII != ds.Domain.ASCII && !strings.HasSuffix(ds.Identity.Domain.ASCII, \".\"+ds.Domain.ASCII) {\n\t\txerrorf(\"%w: identity domain %q not under domain %q\", errSigIdentityDomain, ds.Identity.Domain.ASCII, ds.Domain.ASCII)\n\t}\n\n\treturn ds, []byte(p.tracked), nil\n}\n\n```\n# Function Signature: func (s *Sig) Header() (string, error) {\n# Natural Language Annotations: // Header returns the DKIM-Signature header in string form, to be prepended to a\n// message, including DKIM-Signature field name and trailing \\r\\n.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (s *Sig) Header() (string, error) {\n\t// TODO: complete the implementation\n}\n```", "  ```\nfunc (s *Sig) Header() (string, error) {\n\tvar b bytes.Buffer\n\t// DKIM-Signature: is the header name, and the colon is required\n\tb.WriteString(\"DKIM-Signature: \")\n\n\t// Version, 1. Field \"v\". Always the first field.\n\tb.WriteString(fmt.Sprintf(\"v=1;%s\", \"b=\"))\n\n\t// Algorithm is the algorithm used for signature, Field \"a\".\n\tb.WriteString(fmt.Sprintf(\"a=%s-%s;\", s.AlgorithmSign, s.AlgorithmHash))\n\n\t// BodyHash is a hash of the message body Field \"bh\".\n\tb.WriteString(fmt.Sprintf(\"bh=%s;\", base64.StdEncoding.EncodeToString([]byte(s.BodyHash))))\n\n\t// Domain is the domain that the DKIM-Signature applies to Field \"d\".\n\tb.WriteString(fmt.Sprintf(\"d=%s;\", s.Domain))\n\n\t// SignedHeaders is the list of headers that were signed Field \"h\".\n\tb.WriteString(fmt.Sprintf(\"h=%s;\", strings.Join(s.SignedHeaders, \":\")))\n\n\t// Selector is the selector used to locate the DKIM-Signature Field \"s\".\n\tb.WriteString(fmt.Sprintf(\"s=%s;\", s.Selector))\n\n\t// Canonicalization is the transformation of header and/or body before\n\t// hashing. The value is in original case, but must be compared case-insensitively.\n\t// Normally two slash-separated values: header canonicalization and body canonicalization.\n\t// But the \"simple\" means \"simple/simple\" and \"relaxed\" means \"relaxed/simple\". Field \"c\".\n\tb.WriteString(fmt.Sprintf(\"c=%s;\", s.Canonicalization))\n\n\t// Length is the length of the body to be used in verification. When -1, the whole body\n\t// is used for verification. Field \"l\".\n\tb.WriteString(fmt.Sprintf(\"l=%d;\", s.Length))\n\n\t// Identity used in the optional i= field. It uses the syntax of an email\n\t// address, but does not necessarily represent one. Field \"i\".\n\tb.WriteString(fmt.Sprintf(\"i=@%s;\", s.Identity))\n\n\t// QueryMethods indicates how to public key can be retrieved for verification.\n\t// Currently known value is \"dns/txt\" (should be compared case-insensitively).\n\t// If empty, dns/txt must be assumed. Field \"q\".\n\tb.WriteString(fmt.Sprintf(\"q=%s;\", strings.Join(s.QueryMethods, \":\")))\n\n\t// SignTime indicates when the signature was created. Field \"t\".\n\tb.WriteString(fmt.Sprintf(\"t=%s;\", s.SignTime))\n\n\t// ExpireTime indicates after when the signature is considered invalid. Field \"x\".\n\tb.WriteString(fmt.Sprintf(\"x=%s;\", s.ExpireTime))\n\n\t// CopiedHeaders indicates the headers that must be copied when transmitting the\n\t// message. Field \"z\".\n\tb.WriteString(fmt.Sprintf(\"z=%s;\", strings.Join(s.CopiedHeaders, \":\")))\n\n\t// The actual signature value, Field \"b\".\n\tb.WriteString(fmt.Sprintf(\"b=%s\", base64.StdEncoding.EncodeToString([]byte(s.Signature))))\n\n\t// Trailing CRLF is required.\n\tb.WriteString(\"\\r\\n\")\n\n\treturn b.String(), nil\n}\n```", "  ```\nfunc (s *Sig) Header() (string, error) {\n\tbuf := new(strings.Builder)\n\t// write field name\n\tbuf.WriteString(\"DKIM-Signature\")\n\t// write field value\n\tif _, err := buf.WriteString(s.FieldValue()); err != nil {\n\t\treturn \"\", err\n\t}\n\t// write crlf\n\tbuf.WriteString(\"\\r\\n\")\n\n\treturn buf.String(), nil\n}\n\ntype parser struct {\n\ts        string\n\tpos      int\n\tsmtputf8 bool // Convert non-ascii characters to UTF-8 encoding\n\twarned   bool // If true, any errors will only be returned once\n}\n\nfunc (p *parser) fws() {\n\tfor p.pos < len(p.s) && p.s[p.pos] == ' ' {\n\t\tp.pos++\n\t}\n}\n\nfunc (p *parser) wsp() {\n\tfor p.pos < len(p.s) && (p.s[p.pos] == ' ' || p.s[p.pos] == '\\t') {\n\t\tp.pos++\n\t}\n}\n\nfunc (p *parser) bsizedWord() (word string, err error) {\n\tsubstring := make([]byte, 0, 1024)\n\tfor {\n\t\tif p.pos > len(p.s) {\n\t\t\terr = errors.New(\"EXPECTED-B\")\n\t\t\treturn\n\t\t}\n\t\t// '{' is the size escaping character.\n\t\tif p.s[p.pos] == '{' {\n\t\t\t// Move past the escape token\n\t\t\tp.pos++\n\t\t\t// Get hex size\n\t\t\tb, trailing := p.bsizedWord()\n\t\t\tif b != nil {\n\t\t\t\ttemp, err2 := base64.StdEncoding.DecodeString(*b)\n\t\t\t\tif err2 == nil {\n\t\t\t\t\tsubstring = append(substring, temp...)\n\t\t\t\t\tp.pos += len(*b) + trailing\n\t\t\t\t} else {\n\t\t\t\t\terr = errors.New(\"EXPECTED-B\")\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terr = errors.New(\"EXPECTED-B {...} not closed\")\n\t\t\t\treturn\n\t\t\t}\n\t\t} else if isSpace(p.s[p.pos]) {\n\t\t\treturn\n\t\t} else {\n\t\t\tsubstring = append(substring, p.s[p.pos])\n\t\t\tp.pos++\n\t\t}\n\t}\n}\n\nfunc (p *parser) header() []byte {\n\tp.pos++\n\ttoken, err := p.bsizedWord()\n\tif err != nil {\n\t\tp.warned = true\n\t} else {\n\t\treturn []byte(*token)\n\t}\n\treturn nil\n}\n\nfunc isSpace(c byte) bool {\n\treturn c == ' ' || c == '\\t'\n}\n\nfunc (p *parser) parseSignature(buf string) error {\n\t//fmt.Println(\"buf:\", buf)\n\theader := p.header()\n\tif header == nil {\n\t\treturn errors.New(\"empty header\")\n\t}\n\t//fmt.Println(\"header\", string(header))\n\t//fmt.Println(\"buf:\", buf)\n\treturn nil\n}\n\nfunc main() {\n\tfmt.Println(\"Welcome to the DKIM parser\")\n\tvar input string\n\tfmt.Println(\"Enter signature:\")\n\tfmt.Scan(&input)\n\tinput += \"\\r\\n\"\n\t_ = parseSignature(input, false)\n}\n```"]}, "mox-dkim/txt.go-Record": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dkim/txt.go:\n```\npackage dkim\n\nimport (\n\t\"crypto/ed25519\"\n\t\"crypto/rsa\"\n\t\"crypto/x509\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n)\n\n// Record is a DKIM DNS record, served on <selector>._domainkey.<domain> for a\n// given selector and domain (s= and d= in the DKIM-Signature).\n//\n// The record is a semicolon-separated list of \"=\"-separated field value pairs.\n// Strings should be compared case-insensitively, e.g. k=ed25519 is equivalent to k=ED25519.\n//\n// Example:\n//\n//\tv=DKIM1;h=sha256;k=ed25519;p=ln5zd/JEX4Jy60WAhUOv33IYm2YZMyTQAdr9stML504=\ntype Record struct {\n\tVersion  string   // Version, fixed \"DKIM1\" (case sensitive). Field \"v\".\n\tHashes   []string // Acceptable hash algorithms, e.g. \"sha1\", \"sha256\". Optional, defaults to all algorithms. Field \"h\".\n\tKey      string   // Key type, \"rsa\" or \"ed25519\". Optional, default \"rsa\". Field \"k\".\n\tNotes    string   // Debug notes. Field \"n\".\n\tPubkey   []byte   // Public key, as base64 in record. If empty, the key has been revoked. Field \"p\".\n\tServices []string // Service types. Optional, default \"*\" for all services. Other values: \"email\". Field \"s\".\n\tFlags    []string // Flags, colon-separated. Optional, default is no flags. Other values: \"y\" for testing DKIM, \"s\" for \"i=\" must have same domain as \"d\" in signatures. Field \"t\".\n\n\tPublicKey any `json:\"-\"` // Parsed form of public key, an *rsa.PublicKey or ed25519.PublicKey.\n}\n\n// ../rfc/6376:1438\n\n// ServiceAllowed returns whether service s is allowed by this key.\n//\n// The optional field \"s\" can specify purposes for which the key can be used. If\n// value was specified, both \"*\" and \"email\" are enough for use with DKIM.\nfunc (r *Record) ServiceAllowed(s string) bool {\n\tif len(r.Services) == 0 {\n\t\treturn true\n\t}\n\tfor _, ss := range r.Services {\n\t\tif ss == \"*\" || strings.EqualFold(s, ss) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Record returns a DNS TXT record that should be served at\n// <selector>._domainkey.<domain>.\n//\n// Only values that are not the default values are included.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc qpSection(s string) string {\n\tconst hex = \"0123456789ABCDEF\"\n\n\t// ../rfc/2045:1260\n\tvar r string\n\tfor i, b := range []byte(s) {\n\t\tif i > 0 && (b == ' ' || b == '\\t') || b > ' ' && b < 0x7f && b != '=' {\n\t\t\tr += string(rune(b))\n\t\t} else {\n\t\t\tr += \"=\" + string(hex[b>>4]) + string(hex[(b>>0)&0xf])\n\t\t}\n\t}\n\treturn r\n}\n\nvar (\n\terrRecordDuplicateTag     = errors.New(\"duplicate tag\")\n\terrRecordMissingField     = errors.New(\"missing field\")\n\terrRecordBadPublicKey     = errors.New(\"bad public key\")\n\terrRecordUnknownAlgorithm = errors.New(\"unknown algorithm\")\n\terrRecordVersionFirst     = errors.New(\"first field must be version\")\n)\n\n// ParseRecord parses a DKIM DNS TXT record.\n//\n// If the record is a dkim record, but an error occurred, isdkim will be true and\n// err will be the error. Such errors must be treated differently from parse errors\n// where the record does not appear to be DKIM, which can happen with misconfigured\n// DNS (e.g. wildcard records).\nfunc ParseRecord(s string) (record *Record, isdkim bool, err error) {\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\tif xerr, ok := x.(error); ok {\n\t\t\trecord = nil\n\t\t\terr = xerr\n\t\t\treturn\n\t\t}\n\t\tpanic(x)\n\t}()\n\n\txerrorf := func(format string, args ...any) {\n\t\tpanic(fmt.Errorf(format, args...))\n\t}\n\n\trecord = &Record{\n\t\tVersion:  \"DKIM1\",\n\t\tKey:      \"rsa\",\n\t\tServices: []string{\"*\"},\n\t}\n\n\tp := parser{s: s, drop: true}\n\tseen := map[string]struct{}{}\n\t// ../rfc/6376:655\n\t// ../rfc/6376:656 ../rfc/6376-eid5070\n\t// ../rfc/6376:658 ../rfc/6376-eid5070\n\t// ../rfc/6376:1438\n\tfor {\n\t\tp.fws()\n\t\tk := p.xtagName()\n\t\tp.fws()\n\t\tp.xtake(\"=\")\n\t\tp.fws()\n\t\t// Keys are case-sensitive: ../rfc/6376:679\n\t\tif _, ok := seen[k]; ok {\n\t\t\t// Duplicates not allowed: ../rfc/6376:683\n\t\t\txerrorf(\"%w: %q\", errRecordDuplicateTag, k)\n\t\t\tbreak\n\t\t}\n\t\tseen[k] = struct{}{}\n\t\t// Version must be the first.\n\t\tswitch k {\n\t\tcase \"v\":\n\t\t\t// ../rfc/6376:1443\n\t\t\tv := p.xtake(\"DKIM1\")\n\t\t\t// Version being set is a signal this appears to be a valid record. We must not\n\t\t\t// treat e.g. DKIM1.1 as valid, so we explicitly check there is no more data before\n\t\t\t// we decide this record is DKIM.\n\t\t\tp.fws()\n\t\t\tif !p.empty() {\n\t\t\t\tp.xtake(\";\")\n\t\t\t}\n\t\t\trecord.Version = v\n\t\t\tif len(seen) != 1 {\n\t\t\t\t// If version is present, it must be the first.\n\t\t\t\txerrorf(\"%w\", errRecordVersionFirst)\n\t\t\t}\n\t\t\tisdkim = true\n\t\t\tif p.empty() {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tcontinue\n\n\t\tcase \"h\":\n\t\t\t// ../rfc/6376:1463\n\t\t\trecord.Hashes = []string{p.xhyphenatedWord()}\n\t\t\tfor p.peekfws(\":\") {\n\t\t\t\tp.fws()\n\t\t\t\tp.xtake(\":\")\n\t\t\t\tp.fws()\n\t\t\t\trecord.Hashes = append(record.Hashes, p.xhyphenatedWord())\n\t\t\t}\n\t\tcase \"k\":\n\t\t\t// ../rfc/6376:1478\n\t\t\trecord.Key = p.xhyphenatedWord()\n\t\tcase \"n\":\n\t\t\t// ../rfc/6376:1491\n\t\t\trecord.Notes = p.xqpSection()\n\t\tcase \"p\":\n\t\t\t// ../rfc/6376:1501\n\t\t\trecord.Pubkey = p.xbase64()\n\t\tcase \"s\":\n\t\t\t// ../rfc/6376:1533\n\t\t\trecord.Services = []string{p.xhyphenatedWord()}\n\t\t\tfor p.peekfws(\":\") {\n\t\t\t\tp.fws()\n\t\t\t\tp.xtake(\":\")\n\t\t\t\tp.fws()\n\t\t\t\trecord.Services = append(record.Services, p.xhyphenatedWord())\n\t\t\t}\n\t\tcase \"t\":\n\t\t\t// ../rfc/6376:1554\n\t\t\trecord.Flags = []string{p.xhyphenatedWord()}\n\t\t\tfor p.peekfws(\":\") {\n\t\t\t\tp.fws()\n\t\t\t\tp.xtake(\":\")\n\t\t\t\tp.fws()\n\t\t\t\trecord.Flags = append(record.Flags, p.xhyphenatedWord())\n\t\t\t}\n\t\tdefault:\n\t\t\t// We must ignore unknown fields. ../rfc/6376:692 ../rfc/6376:1439\n\t\t\tfor !p.empty() && !p.hasPrefix(\";\") {\n\t\t\t\tp.xchar()\n\t\t\t}\n\t\t}\n\n\t\tisdkim = true\n\t\tp.fws()\n\t\tif p.empty() {\n\t\t\tbreak\n\t\t}\n\t\tp.xtake(\";\")\n\t\tif p.empty() {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif _, ok := seen[\"p\"]; !ok {\n\t\txerrorf(\"%w: public key\", errRecordMissingField)\n\t}\n\n\tswitch strings.ToLower(record.Key) {\n\tcase \"\", \"rsa\":\n\t\tif len(record.Pubkey) == 0 {\n\t\t\t// Revoked key, nothing to do.\n\t\t} else if pk, err := x509.ParsePKIXPublicKey(record.Pubkey); err != nil {\n\t\t\txerrorf(\"%w: %s\", errRecordBadPublicKey, err)\n\t\t} else if _, ok := pk.(*rsa.PublicKey); !ok {\n\t\t\txerrorf(\"%w: got %T, need an RSA key\", errRecordBadPublicKey, record.PublicKey)\n\t\t} else {\n\t\t\trecord.PublicKey = pk\n\t\t}\n\tcase \"ed25519\":\n\t\tif len(record.Pubkey) == 0 {\n\t\t\t// Revoked key, nothing to do.\n\t\t} else if len(record.Pubkey) != ed25519.PublicKeySize {\n\t\t\txerrorf(\"%w: got %d bytes, need %d\", errRecordBadPublicKey, len(record.Pubkey), ed25519.PublicKeySize)\n\t\t} else {\n\t\t\trecord.PublicKey = ed25519.PublicKey(record.Pubkey)\n\t\t}\n\tdefault:\n\t\txerrorf(\"%w: %q\", errRecordUnknownAlgorithm, record.Key)\n\t}\n\n\treturn record, true, nil\n}\n\n```\n# Function Signature: func (r *Record) Record() (string, error) {\n# Natural Language Annotations: // Record returns a DNS TXT record that should be served at\n// <selector>._domainkey.<domain>.\n//\n// Only values that are not the default values are included.\nPlease return the generated code surrounded by ```\n", "response": ["  ```\nfunc (r *Record) Record() (string, error) {\n\tvar b strings.Builder\n\tif r.Version != \"DKIM1\" {\n\t\treturn \"\", errors.New(\"bad record version\")\n\t}\n\tif len(r.Hashes) != 0 {\n\t\tb.WriteString(\"h=\")\n\t\tfor _, h := range r.Hashes {\n\t\t\tb.WriteString(h)\n\t\t\tb.WriteRune(':')\n\t\t}\n\t\tb.WriteString(\"; \")\n\t}\n\tif len(r.Key) != 0 {\n\t\tb.WriteString(\"k=\")\n\t\tb.WriteString(r.Key)\n\t\tb.WriteRune(';')\n\t}\n\tif len(r.Notes) != 0 {\n\t\tb.WriteString(\"n=\")\n\t\tb.WriteString(qpSection(r.Notes))\n\t\tb.WriteRune(';')\n\t}\n\tif len(r.Pubkey) != 0 {\n\t\tb.WriteString(\"p=\")\n\t\tb.WriteString(base64.StdEncoding.EncodeToString(r.Pubkey))\n\t\tb.WriteRune(';')\n\t}\n\tif len(r.Services) != 0 {\n\t\tb.WriteString(\"s=\")\n\t\tfor _, s := range r.Services {\n\t\t\tb.WriteString(s)\n\t\t\tb.WriteRune(':')\n\t\t}\n\t\tb.WriteString(\";\")\n\t}\n\tif len(r.Flags) != 0 {\n\t\tb.WriteString(\"t=\")\n\t\tfor _, f := range r.Flags {\n\t\t\tb.WriteString(f)\n\t\t\tb.WriteRune(':')\n\t\t}\n\t\tb.WriteRune(';')\n\t}\n\tif b.Len() == 0 {\n\t\treturn \"\", errors.New(\"nothing to record\")\n\t}\n\treturn b.String(), nil\n}\n```", "  ```\nfunc (r *Record) Record() (string, error) {\n\tvar parts []string\n\tif r.Version != \"DKIM1\" {\n\t\tparts = append(parts, fmt.Sprintf(\"v=%s\", r.Version))\n\t}\n\tfor _, h := range r.Hashes {\n\t\tif h != \"\" {\n\t\t\tparts = append(parts, fmt.Sprintf(\"h=%s\", h))\n\t\t}\n\t}\n\tif r.Key != \"rsa\" && r.Key != \"\" {\n\t\tparts = append(parts, fmt.Sprintf(\"k=%s\", r.Key))\n\t}\n\tif r.Notes != \"\" {\n\t\tparts = append(parts, fmt.Sprintf(\"n=%s\", r.Notes))\n\t}\n\tif len(r.Pubkey) != 0 {\n\t\tparts = append(parts, fmt.Sprintf(\"p=%s\", base64.RawStdEncoding.EncodeToString(r.Pubkey)))\n\t}\n\tfor _, s := range r.Services {\n\t\tif s != \"\" && s != \"*\" {\n\t\t\tparts = append(parts, fmt.Sprintf(\"s=%s\", s))\n\t\t}\n\t}\n\tfor _, f := range r.Flags {\n\t\tif f != \"\" && f != \"s\" && f != \"y\" {\n\t\t\treturn \"\", fmt.Errorf(\"unsupported flag %q\", f)\n\t\t}\n\t\tparts = append(parts, fmt.Sprintf(\"t=%s\", f))\n\t}\n\treturn fqSection(strings.Join(parts, \";\")), nil\n}\n```", "  ```\nfunc (r *Record) Record() (string, error) {\n\tvar buf bytes.Buffer\n\n\tif r.Hashes != nil && len(r.Hashes) > 0 {\n\t\tfmt.Fprintf(&buf, \"h=%s\", strings.Join(r.Hashes, \":\"))\n\t}\n\n\t// if Key is not nil, use it. Specifically, we can have an ed25519 private key which the parser can't parse.\n\tif r.Key != \"\" {\n\t\tfmt.Fprintf(&buf, \";k=%s\", r.Key)\n\t}\n\n\tif r.Notes != \"\" {\n\t\tfmt.Fprintf(&buf, \";n=%s\", r.Notes)\n\t}\n\n\tif r.Pubkey != nil && len(r.Pubkey) > 0 {\n\t\tfmt.Fprintf(&buf, \";p=%s\", base64.StdEncoding.EncodeToString(r.Pubkey))\n\t}\n\n\tif r.Services != nil && len(r.Services) > 0 {\n\t\tfmt.Fprintf(&buf, \";s=%s\", strings.Join(r.Services, \":\"))\n\t}\n\n\tif r.Flags != nil && len(r.Flags) > 0 {\n\t\tfmt.Fprintf(&buf, \";t=%s\", strings.Join(r.Flags, \":\"))\n\t}\n\n\tif len(buf.String()) == 0 {\n\t\treturn \"\", errors.New(\"error generating the record, the properties are invalid\")\n\t}\n\n\treturn buf.String(), nil\n}\n```"]}}