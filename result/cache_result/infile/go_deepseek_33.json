{"dtm-dtmsvr/storage/boltdb/boltdb.go-cleanupExpiredData": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/dtm/dtmsvr/storage/boltdb/boltdb.go:\n```\n/*\n * Copyright (c) 2021 yedf. All rights reserved.\n * Use of this source code is governed by a BSD-style\n * license that can be found in the LICENSE file.\n */\n\n// package boltdb implement the storage for boltdb\npackage boltdb\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/dtm-labs/dtm/client/dtmcli/dtmimp\"\n\t\"github.com/dtm-labs/dtm/dtmsvr/storage\"\n\t\"github.com/dtm-labs/dtm/dtmutil\"\n\t\"github.com/dtm-labs/logger\"\n\tbolt \"go.etcd.io/bbolt\"\n)\n\n// Store implements storage.Store, and storage with boltdb\ntype Store struct {\n\tboltDb *bolt.DB\n\n\tdataExpire    int64\n\tretryInterval int64\n}\n\n// NewStore will return the boltdb implement\n// TODO: change to options\nfunc NewStore(dataExpire int64, retryInterval int64) *Store {\n\ts := &Store{\n\t\tdataExpire:    dataExpire,\n\t\tretryInterval: retryInterval,\n\t}\n\n\tdb, err := bolt.Open(\"./dtm.bolt\", 0666, &bolt.Options{Timeout: 1 * time.Second})\n\tdtmimp.E2P(err)\n\n\t// NOTE: we must ensure all buckets is exists before we use it\n\terr = initializeBuckets(db)\n\tdtmimp.E2P(err)\n\n\t// TODO:\n\t//   1. refactor this code\n\t//   2. make cleanup run period, to avoid the file growup when server long-running\n\terr = cleanupExpiredData(\n\t\ttime.Duration(dataExpire)*time.Second,\n\t\tdb,\n\t)\n\tdtmimp.E2P(err)\n\n\ts.boltDb = db\n\treturn s\n}\n\nfunc initializeBuckets(db *bolt.DB) error {\n\treturn db.Update(func(t *bolt.Tx) error {\n\t\tfor _, bucket := range allBuckets {\n\t\t\t_, err := t.CreateBucketIfNotExists(bucket)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\n\t\treturn nil\n\t})\n}\n\n// cleanupExpiredData will clean the expired data in boltdb, the\n//\n//\texpired time is configurable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc cleanupGlobalWithGids(t *bolt.Tx, gids map[string]struct{}) {\n\tbucket := t.Bucket(bucketGlobal)\n\tif bucket == nil {\n\t\treturn\n\t}\n\n\tlogger.Debugf(\"Start to cleanup %d gids\", len(gids))\n\tfor gid := range gids {\n\t\tlogger.Debugf(\"Start to delete gid: %s\", gid)\n\t\tdtmimp.E2P(bucket.Delete([]byte(gid)))\n\t}\n}\n\nfunc cleanupBranchWithGids(t *bolt.Tx, gids map[string]struct{}) {\n\tbucket := t.Bucket(bucketBranches)\n\tif bucket == nil {\n\t\treturn\n\t}\n\n\t// It's not safe if we delete the item when use cursor, for more detail see\n\t//    https://github.com/etcd-io/bbolt/issues/146\n\tbranchKeys := []string{}\n\tfor gid := range gids {\n\t\tcursor := bucket.Cursor()\n\t\tfor k, v := cursor.Seek([]byte(gid)); k != nil; k, v = cursor.Next() {\n\t\t\tb := storage.TransBranchStore{}\n\t\t\tdtmimp.MustUnmarshal(v, &b)\n\t\t\tif b.Gid != gid {\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\tbranchKeys = append(branchKeys, string(k))\n\t\t}\n\t}\n\n\tlogger.Debugf(\"Start to cleanup %d branches\", len(branchKeys))\n\tfor _, key := range branchKeys {\n\t\tlogger.Debugf(\"Start to delete branch: %s\", key)\n\t\tdtmimp.E2P(bucket.Delete([]byte(key)))\n\t}\n}\n\nfunc cleanupIndexWithGids(t *bolt.Tx, gids map[string]struct{}) {\n\tbucket := t.Bucket(bucketIndex)\n\tif bucket == nil {\n\t\treturn\n\t}\n\n\tindexKeys := []string{}\n\tcursor := bucket.Cursor()\n\tfor k, _ := cursor.First(); k != nil; k, _ = cursor.Next() {\n\t\tks := strings.Split(string(k), \"-\")\n\t\tif len(ks) != 2 {\n\t\t\tcontinue\n\t\t}\n\n\t\tif _, ok := gids[ks[1]]; ok {\n\t\t\tindexKeys = append(indexKeys, string(k))\n\t\t}\n\t}\n\n\tlogger.Debugf(\"Start to cleanup %d indexes\", len(indexKeys))\n\tfor _, key := range indexKeys {\n\t\tlogger.Debugf(\"Start to delete index: %s\", key)\n\t\tdtmimp.E2P(bucket.Delete([]byte(key)))\n\t}\n}\n\nvar bucketGlobal = []byte(\"global\")\nvar bucketBranches = []byte(\"branches\")\nvar bucketIndex = []byte(\"index\")\nvar bucketKV = []byte(\"kv\")\nvar allBuckets = [][]byte{\n\tbucketBranches,\n\tbucketGlobal,\n\tbucketIndex,\n\tbucketKV,\n}\n\nfunc tGetGlobal(t *bolt.Tx, gid string) *storage.TransGlobalStore {\n\ttrans := storage.TransGlobalStore{}\n\tbs := t.Bucket(bucketGlobal).Get([]byte(gid))\n\tif bs == nil {\n\t\treturn nil\n\t}\n\tdtmimp.MustUnmarshal(bs, &trans)\n\treturn &trans\n}\n\nfunc tGetBranches(t *bolt.Tx, gid string) []storage.TransBranchStore {\n\tbranches := []storage.TransBranchStore{}\n\tcursor := t.Bucket(bucketBranches).Cursor()\n\tfor k, v := cursor.Seek([]byte(gid)); k != nil; k, v = cursor.Next() {\n\t\tb := storage.TransBranchStore{}\n\t\tdtmimp.MustUnmarshal(v, &b)\n\t\tif b.Gid != gid {\n\t\t\tbreak\n\t\t}\n\t\tbranches = append(branches, b)\n\t}\n\treturn branches\n}\nfunc tPutGlobal(t *bolt.Tx, global *storage.TransGlobalStore) {\n\tbs := dtmimp.MustMarshal(global)\n\terr := t.Bucket(bucketGlobal).Put([]byte(global.Gid), bs)\n\tdtmimp.E2P(err)\n}\n\nfunc tPutBranches(t *bolt.Tx, branches []storage.TransBranchStore, start int64) {\n\terr := tPutBranches2(t, branches, start)\n\tdtmimp.E2P(err)\n}\n\nfunc tPutBranches2(t *bolt.Tx, branches []storage.TransBranchStore, start int64) error {\n\tif start == -1 {\n\t\tb0 := &branches[0]\n\t\tbs := tGetBranches(t, b0.Gid)\n\t\tfor _, b := range bs {\n\t\t\tif b.BranchID == b0.BranchID && b.Op == b0.Op {\n\t\t\t\treturn storage.ErrUniqueConflict\n\t\t\t}\n\t\t}\n\t\tstart = int64(len(bs))\n\t}\n\tfor i, b := range branches {\n\t\tk := b.Gid + fmt.Sprintf(\"%03d\", i+int(start))\n\t\tv := dtmimp.MustMarshalString(b)\n\t\terr := t.Bucket(bucketBranches).Put([]byte(k), []byte(v))\n\t\tdtmimp.E2P(err)\n\t}\n\treturn nil\n}\n\nfunc tDelIndex(t *bolt.Tx, unix int64, gid string) {\n\tk := fmt.Sprintf(\"%d-%s\", unix, gid)\n\terr := t.Bucket(bucketIndex).Delete([]byte(k))\n\tdtmimp.E2P(err)\n}\n\nfunc tPutIndex(t *bolt.Tx, unix int64, gid string) {\n\tk := fmt.Sprintf(\"%d-%s\", unix, gid)\n\terr := t.Bucket(bucketIndex).Put([]byte(k), []byte(gid))\n\tdtmimp.E2P(err)\n}\n\nfunc tGetKV(t *bolt.Tx, cat, key string) *storage.KVStore {\n\tk := fmt.Sprintf(\"%s-%s\", cat, key)\n\tkv := storage.KVStore{}\n\tres := t.Bucket(bucketKV).Get([]byte(k))\n\tif res == nil {\n\t\treturn nil\n\t}\n\tdtmimp.MustUnmarshal(res, &kv)\n\treturn &kv\n}\n\nfunc tPutKV(t *bolt.Tx, kv *storage.KVStore) {\n\tk := fmt.Sprintf(\"%s-%s\", kv.Cat, kv.K)\n\tkvJSON := dtmimp.MustMarshal(kv)\n\terr := t.Bucket(bucketKV).Put([]byte(k), kvJSON)\n\tdtmimp.E2P(err)\n}\n\nfunc tDelKV(t *bolt.Tx, cat, key string) {\n\tk := fmt.Sprintf(\"%s-%s\", cat, key)\n\terr := t.Bucket(bucketKV).Delete([]byte(k))\n\tdtmimp.E2P(err)\n}\n\n// Ping execs ping cmd to boltdb\nfunc (s *Store) Ping() error {\n\treturn nil\n}\n\n// PopulateData populates data to boltdb\nfunc (s *Store) PopulateData(skipDrop bool) {\n\tif !skipDrop {\n\t\terr := s.boltDb.Update(func(t *bolt.Tx) error {\n\t\t\tdtmimp.E2P(t.DeleteBucket(bucketIndex))\n\t\t\tdtmimp.E2P(t.DeleteBucket(bucketBranches))\n\t\t\tdtmimp.E2P(t.DeleteBucket(bucketGlobal))\n\t\t\tdtmimp.E2P(t.DeleteBucket(bucketKV))\n\t\t\t_, err := t.CreateBucket(bucketIndex)\n\t\t\tdtmimp.E2P(err)\n\t\t\t_, err = t.CreateBucket(bucketBranches)\n\t\t\tdtmimp.E2P(err)\n\t\t\t_, err = t.CreateBucket(bucketGlobal)\n\t\t\tdtmimp.E2P(err)\n\t\t\t_, err = t.CreateBucket(bucketKV)\n\t\t\tdtmimp.E2P(err)\n\t\t\treturn nil\n\t\t})\n\t\tdtmimp.E2P(err)\n\t\tlogger.Infof(\"Reset all data for boltdb\")\n\t}\n}\n\n// FindTransGlobalStore finds GlobalTrans data by gid\nfunc (s *Store) FindTransGlobalStore(gid string) (trans *storage.TransGlobalStore) {\n\terr := s.boltDb.View(func(t *bolt.Tx) error {\n\t\ttrans = tGetGlobal(t, gid)\n\t\treturn nil\n\t})\n\tdtmimp.E2P(err)\n\treturn\n}\n\n// ScanTransGlobalStores lists GlobalTrans data\nfunc (s *Store) ScanTransGlobalStores(position *string, limit int64, condition storage.TransGlobalScanCondition) []storage.TransGlobalStore {\n\tglobals := []storage.TransGlobalStore{}\n\terr := s.boltDb.View(func(t *bolt.Tx) error {\n\t\tcursor := t.Bucket(bucketGlobal).Cursor()\n\t\tfor k, v := cursor.Seek([]byte(*position)); k != nil; k, v = cursor.Next() {\n\t\t\tif string(k) == *position {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tg := storage.TransGlobalStore{}\n\t\t\tdtmimp.MustUnmarshal(v, &g)\n\t\t\tif !((condition.Status == \"\" || g.Status == condition.Status) &&\n\t\t\t\t(condition.TransType == \"\" || g.TransType == condition.TransType) &&\n\t\t\t\t(condition.CreateTimeStart.IsZero() || g.CreateTime.After(condition.CreateTimeStart)) &&\n\t\t\t\t(condition.CreateTimeEnd.IsZero() || g.CreateTime.Before(condition.CreateTimeEnd))) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tglobals = append(globals, g)\n\t\t\tif len(globals) == int(limit) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\tdtmimp.E2P(err)\n\tif len(globals) < int(limit) {\n\t\t*position = \"\"\n\t} else {\n\t\t*position = globals[len(globals)-1].Gid\n\t}\n\treturn globals\n}\n\n// FindBranches finds Branch data by gid\nfunc (s *Store) FindBranches(gid string) []storage.TransBranchStore {\n\tvar branches []storage.TransBranchStore\n\terr := s.boltDb.View(func(t *bolt.Tx) error {\n\t\tbranches = tGetBranches(t, gid)\n\t\treturn nil\n\t})\n\tdtmimp.E2P(err)\n\treturn branches\n}\n\n// UpdateBranches update branches info\nfunc (s *Store) UpdateBranches(branches []storage.TransBranchStore, updates []string) (int, error) {\n\treturn 0, nil // not implemented\n}\n\n// LockGlobalSaveBranches creates branches\nfunc (s *Store) LockGlobalSaveBranches(gid string, status string, branches []storage.TransBranchStore, branchStart int) {\n\terr := s.boltDb.Update(func(t *bolt.Tx) error {\n\t\tg := tGetGlobal(t, gid)\n\t\tif g == nil {\n\t\t\treturn storage.ErrNotFound\n\t\t}\n\t\tif g.Status != status {\n\t\t\treturn storage.ErrNotFound\n\t\t}\n\t\treturn tPutBranches2(t, branches, int64(branchStart))\n\t})\n\tdtmimp.E2P(err)\n}\n\n// MaySaveNewTrans creates a new trans\nfunc (s *Store) MaySaveNewTrans(global *storage.TransGlobalStore, branches []storage.TransBranchStore) error {\n\treturn s.boltDb.Update(func(t *bolt.Tx) error {\n\t\tg := tGetGlobal(t, global.Gid)\n\t\tif g != nil {\n\t\t\treturn storage.ErrUniqueConflict\n\t\t}\n\t\ttPutGlobal(t, global)\n\t\ttPutIndex(t, global.NextCronTime.Unix(), global.Gid)\n\t\ttPutBranches(t, branches, 0)\n\t\treturn nil\n\t})\n}\n\n// ChangeGlobalStatus changes global trans status\nfunc (s *Store) ChangeGlobalStatus(global *storage.TransGlobalStore, newStatus string, updates []string, finished bool) {\n\told := global.Status\n\tglobal.Status = newStatus\n\terr := s.boltDb.Update(func(t *bolt.Tx) error {\n\t\tg := tGetGlobal(t, global.Gid)\n\t\tif g == nil || g.Status != old {\n\t\t\treturn storage.ErrNotFound\n\t\t}\n\t\tif finished {\n\t\t\ttDelIndex(t, g.NextCronTime.Unix(), g.Gid)\n\t\t}\n\t\ttPutGlobal(t, global)\n\t\treturn nil\n\t})\n\tdtmimp.E2P(err)\n}\n\n// TouchCronTime updates cronTime\nfunc (s *Store) TouchCronTime(global *storage.TransGlobalStore, nextCronInterval int64, nextCronTime *time.Time) {\n\toldUnix := global.NextCronTime.Unix()\n\tglobal.UpdateTime = dtmutil.GetNextTime(0)\n\tglobal.NextCronTime = nextCronTime\n\tglobal.NextCronInterval = nextCronInterval\n\terr := s.boltDb.Update(func(t *bolt.Tx) error {\n\t\tg := tGetGlobal(t, global.Gid)\n\t\tif g == nil || g.Gid != global.Gid {\n\t\t\treturn storage.ErrNotFound\n\t\t}\n\t\ttDelIndex(t, oldUnix, global.Gid)\n\t\ttPutGlobal(t, global)\n\t\ttPutIndex(t, global.NextCronTime.Unix(), global.Gid)\n\t\treturn nil\n\t})\n\tdtmimp.E2P(err)\n}\n\n// LockOneGlobalTrans finds GlobalTrans\nfunc (s *Store) LockOneGlobalTrans(expireIn time.Duration) *storage.TransGlobalStore {\n\tvar trans *storage.TransGlobalStore\n\tmin := fmt.Sprintf(\"%d\", time.Now().Add(expireIn).Unix())\n\terr := s.boltDb.Update(func(t *bolt.Tx) error {\n\t\tcursor := t.Bucket(bucketIndex).Cursor()\n\t\ttoDelete := [][]byte{}\n\t\tfor k, v := cursor.First(); k != nil && string(k) <= min && (trans == nil || trans.IsFinished()); k, v = cursor.Next() {\n\t\t\ttrans = tGetGlobal(t, string(v))\n\t\t\ttoDelete = append(toDelete, k)\n\t\t}\n\t\tfor _, k := range toDelete {\n\t\t\terr := t.Bucket(bucketIndex).Delete(k)\n\t\t\tdtmimp.E2P(err)\n\t\t}\n\t\tif trans != nil && !trans.IsFinished() {\n\t\t\tnext := time.Now().Add(time.Duration(s.retryInterval) * time.Second)\n\t\t\ttrans.NextCronTime = &next\n\t\t\ttPutGlobal(t, trans)\n\t\t\t// this put should be after delete, because the data may be the same\n\t\t\ttPutIndex(t, next.Unix(), trans.Gid)\n\t\t}\n\t\treturn nil\n\t})\n\tdtmimp.E2P(err)\n\treturn trans\n}\n\n// ResetCronTime reset nextCronTime\n// unfinished transactions need to be retried as soon as possible after business downtime is recovered\nfunc (s *Store) ResetCronTime(after time.Duration, limit int64) (succeedCount int64, hasRemaining bool, err error) {\n\tnext := time.Now()\n\tvar trans *storage.TransGlobalStore\n\tmin := fmt.Sprintf(\"%d\", time.Now().Add(after).Unix())\n\terr = s.boltDb.Update(func(t *bolt.Tx) error {\n\t\tcursor := t.Bucket(bucketIndex).Cursor()\n\t\tsucceedCount = 0\n\t\tfor k, v := cursor.Seek([]byte(min)); k != nil && succeedCount <= limit; k, v = cursor.Next() {\n\t\t\tif succeedCount == limit {\n\t\t\t\thasRemaining = true\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\ttrans = tGetGlobal(t, string(v))\n\t\t\terr := t.Bucket(bucketIndex).Delete(k)\n\t\t\tdtmimp.E2P(err)\n\n\t\t\ttrans.NextCronTime = &next\n\t\t\ttPutGlobal(t, trans)\n\t\t\ttPutIndex(t, next.Unix(), trans.Gid)\n\t\t\tsucceedCount++\n\t\t}\n\t\treturn nil\n\t})\n\treturn\n}\n\n// ResetTransGlobalCronTime reset nextCronTime of one global trans.\nfunc (s *Store) ResetTransGlobalCronTime(g *storage.TransGlobalStore) error {\n\terr := s.boltDb.Update(func(t *bolt.Tx) error {\n\t\tg := tGetGlobal(t, g.Gid)\n\t\tif g == nil {\n\t\t\treturn storage.ErrNotFound\n\t\t}\n\t\tnow := dtmutil.GetNextTime(0)\n\t\tg.NextCronTime = now\n\t\tg.UpdateTime = now\n\t\ttPutGlobal(t, g)\n\t\treturn nil\n\t})\n\tdtmimp.E2P(err)\n\treturn err\n}\n\n// ScanKV lists KV pairs\nfunc (s *Store) ScanKV(cat string, position *string, limit int64) []storage.KVStore {\n\tkvs := []storage.KVStore{}\n\terr := s.boltDb.View(func(t *bolt.Tx) error {\n\t\tcursor := t.Bucket(bucketKV).Cursor()\n\t\tfor k, v := cursor.Seek([]byte(*position)); k != nil; k, v = cursor.Next() {\n\t\t\tif string(k) == *position {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif !strings.HasPrefix(string(k), cat) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tkv := storage.KVStore{}\n\t\t\tdtmimp.MustUnmarshal(v, &kv)\n\t\t\tkvs = append(kvs, kv)\n\t\t\tif len(kvs) == int(limit) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\tdtmimp.E2P(err)\n\tif len(kvs) < int(limit) {\n\t\t*position = \"\"\n\t} else {\n\t\t*position = fmt.Sprintf(\"%s-%s\", cat, kvs[len(kvs)-1].K)\n\t}\n\treturn kvs\n}\n\n// FindKV finds key-value pairs\nfunc (s *Store) FindKV(cat, key string) []storage.KVStore {\n\tkvs := []storage.KVStore{}\n\tif cat != \"\" && key != \"\" {\n\t\terr := s.boltDb.View(func(t *bolt.Tx) error {\n\t\t\tkv := tGetKV(t, cat, key)\n\t\t\tif kv != nil {\n\t\t\t\tkvs = append(kvs, *kv)\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t\tdtmimp.E2P(err)\n\t\treturn kvs\n\t}\n\terr := s.boltDb.View(func(t *bolt.Tx) error {\n\t\tcursor := t.Bucket(bucketKV).Cursor()\n\t\tfor k, v := cursor.First(); k != nil; k, v = cursor.Next() {\n\t\t\tif !strings.HasPrefix(string(k), cat) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tkv := storage.KVStore{}\n\t\t\tdtmimp.MustUnmarshal(v, &kv)\n\t\t\tkvs = append(kvs, kv)\n\t\t}\n\t\treturn nil\n\t})\n\tdtmimp.E2P(err)\n\treturn kvs\n}\n\n// UpdateKV updates key-value pair\nfunc (s *Store) UpdateKV(kv *storage.KVStore) error {\n\tnow := time.Now()\n\tkv.UpdateTime = &now\n\toldVersion := kv.Version\n\tkv.Version = oldVersion + 1\n\n\treturn s.boltDb.Update(func(t *bolt.Tx) error {\n\t\tres := tGetKV(t, kv.Cat, kv.K)\n\t\tif res == nil || res.Version != oldVersion {\n\t\t\treturn storage.ErrNotFound\n\t\t}\n\t\ttPutKV(t, kv)\n\t\treturn nil\n\t})\n}\n\n// DeleteKV deletes key-value pair\nfunc (s *Store) DeleteKV(cat, key string) error {\n\treturn s.boltDb.Update(func(t *bolt.Tx) error {\n\t\tres := tGetKV(t, cat, key)\n\t\tif res == nil {\n\t\t\treturn storage.ErrNotFound\n\t\t}\n\t\ttDelKV(t, cat, key)\n\t\treturn nil\n\t})\n}\n\n// CreateKV creates key-value pair\nfunc (s *Store) CreateKV(cat, key, value string) error {\n\tnow := time.Now()\n\tkv := &storage.KVStore{\n\t\tModelBase: dtmutil.ModelBase{\n\t\t\tCreateTime: &now,\n\t\t\tUpdateTime: &now,\n\t\t},\n\t\tCat:     cat,\n\t\tK:       key,\n\t\tV:       value,\n\t\tVersion: 1,\n\t}\n\treturn s.boltDb.Update(func(t *bolt.Tx) error {\n\t\tres := tGetKV(t, cat, key)\n\t\tif res != nil {\n\t\t\treturn storage.ErrUniqueConflict\n\t\t}\n\t\ttPutKV(t, kv)\n\t\treturn nil\n\t})\n}\n\n```\n# Function Signature: func cleanupExpiredData(expire time.Duration, db *bolt.DB) error {\n# Natural Language Annotations: // cleanupExpiredData will clean the expired data in boltdb, the\n//\n//\texpired time is configurable.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc cleanupExpiredData(expire time.Duration, db *bolt.DB) error {\n\treturn db.Update(func(t *bolt.Tx) error {\n\t\tglobalBucket := t.Bucket(bucketGlobal)\n\t\tif globalBucket == nil {\n\t\t\treturn nil\n\t\t}\n\n\t\tgids := make(map[string]struct{})\n\t\terr := globalBucket.ForEach(func(k, v []byte) error {\n\t\t\ttrans := storage.TransGlobalStore{}\n\t\t\tdtmimp.MustUnmarshal(v, &trans)\n\t\t\tif time.Since(trans.UpdateTime) > expire {\n\t\t\t\tgids[string(k)] = struct{}{}\n\t\t\t}\n\t\t\treturn nil\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tcleanupGlobalWithGids(t, gids)\n\t\tcleanupBranchWithGids(t, gids)\n\t\tcleanupIndexWithGids(t, gids)\n\n\t\treturn nil\n\t})\n}\n```", "```go\nfunc cleanupExpiredData(expire time.Duration, db *bolt.DB) error {\n\t// cleanupExpiredData will clean the expired data in boltdb, the\n\t// expired time is configurable.\n\n\t// NOTE: we must ensure all buckets is exists before we use it\n\terr := initializeBuckets(db)\n\tdtmimp.E2P(err)\n\n\t// TODO:\n\t//   1. refactor this code\n\t//   2. make cleanup run period, to avoid the file growup when server long-running\n\terr = cleanupExpiredData(\n\t\ttime.Duration(dataExpire)*time.Second,\n\t\tdb,\n\t)\n\tdtmimp.E2P(err)\n\n\ts.boltDb = db\n\treturn s\n}\n```", "```go\nfunc cleanupExpiredData(expire time.Duration, db *bolt.DB) error {\n\treturn db.Update(func(t *bolt.Tx) error {\n\t\tgids := make(map[string]struct{})\n\n\t\tcursor := t.Bucket(bucketGlobal).Cursor()\n\t\tfor k, v := cursor.First(); k != nil; k, v = cursor.Next() {\n\t\t\tg := storage.TransGlobalStore{}\n\t\t\tdtmimp.MustUnmarshal(v, &g)\n\t\t\tif time.Since(g.UpdateTime) > expire {\n\t\t\t\tgids[g.Gid] = struct{}{}\n\t\t\t}\n\t\t}\n\n\t\tcleanupGlobalWithGids(t, gids)\n\t\tcleanupBranchWithGids(t, gids)\n\t\tcleanupIndexWithGids(t, gids)\n\n\t\treturn nil\n\t})\n}\n```"]}, "dtm-dtmsvr/config/config.go-MustLoadConfig": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/dtm/dtmsvr/config/config.go:\n```\npackage config\n\nimport (\n\t\"encoding/json\"\n\t\"io/ioutil\"\n\n\t\"github.com/dtm-labs/dtm/client/dtmcli\"\n\t\"github.com/dtm-labs/logger\"\n\t\"gopkg.in/yaml.v3\"\n)\n\nconst (\n\t// DtmMetricsPort is metric port\n\tDtmMetricsPort = 8889\n\t// Mysql is mysql driver\n\tMysql = \"mysql\"\n\t// Redis is redis driver\n\tRedis = \"redis\"\n\t// BoltDb is boltdb driver\n\tBoltDb = \"boltdb\"\n\t// Postgres is postgres driver\n\tPostgres = \"postgres\"\n\t// SQLServer is SQL Server driver\n\tSQLServer = \"sqlserver\"\n)\n\n// MicroService config type for microservice based grpc\ntype MicroService struct {\n\tDriver   string `yaml:\"Driver\" default:\"default\"`\n\tTarget   string `yaml:\"Target\"`\n\tEndPoint string `yaml:\"EndPoint\"`\n}\n\n// HTTPMicroService is the config type for microservice based on http, like springcloud\ntype HTTPMicroService struct {\n\tDriver          string `yaml:\"Driver\" default:\"default\"`\n\tRegistryType    string `yaml:\"RegistryType\" default:\"\"`\n\tRegistryAddress string `yaml:\"RegistryAddress\" default:\"\"`\n\tRegistryOptions string `yaml:\"RegistryOptions\" default:\"{}\"`\n\tTarget          string `yaml:\"Target\"`\n\tEndPoint        string `yaml:\"EndPoint\"`\n}\n\n// Log config customize log\ntype Log struct {\n\tOutputs            string `yaml:\"Outputs\" default:\"stderr\"`\n\tRotationEnable     int64  `yaml:\"RotationEnable\" default:\"0\"`\n\tRotationConfigJSON string `yaml:\"RotationConfigJSON\" default:\"{}\"`\n}\n\n// Store defines storage relevant info\ntype Store struct {\n\tDriver             string `yaml:\"Driver\" default:\"boltdb\"`\n\tHost               string `yaml:\"Host\"`\n\tPort               int64  `yaml:\"Port\"`\n\tUser               string `yaml:\"User\"`\n\tPassword           string `yaml:\"Password\"`\n\tDb                 string `yaml:\"Db\" default:\"dtm\"`\n\tSchema             string `yaml:\"Schema\" default:\"public\"`\n\tMaxOpenConns       int64  `yaml:\"MaxOpenConns\" default:\"500\"`\n\tMaxIdleConns       int64  `yaml:\"MaxIdleConns\" default:\"500\"`\n\tConnMaxLifeTime    int64  `yaml:\"ConnMaxLifeTime\" default:\"5\"`\n\tDataExpire         int64  `yaml:\"DataExpire\" default:\"604800\"`        // Trans data will expire in 7 days. only for redis/boltdb.\n\tFinishedDataExpire int64  `yaml:\"FinishedDataExpire\" default:\"86400\"` // finished Trans data will expire in 1 days. only for redis.\n\tRedisPrefix        string `yaml:\"RedisPrefix\" default:\"{a}\"`          // Redis storage prefix. store data to only one slot in cluster\n}\n\n// IsDB checks config driver is mysql or postgres\nfunc (s *Store) IsDB() bool {\n\treturn s.Driver == dtmcli.DBTypeMysql || s.Driver == dtmcli.DBTypePostgres || s.Driver == dtmcli.DBTypeSQLServer\n}\n\n// GetDBConf returns db conf info\nfunc (s *Store) GetDBConf() dtmcli.DBConf {\n\treturn dtmcli.DBConf{\n\t\tDriver:   s.Driver,\n\t\tHost:     s.Host,\n\t\tPort:     s.Port,\n\t\tUser:     s.User,\n\t\tPassword: s.Password,\n\t\tDb:       s.Db,\n\t\tSchema:   s.Schema,\n\t}\n}\n\n// Type is the type for the config of dtm server\ntype Type struct {\n\tStore                         Store            `yaml:\"Store\"`\n\tTransCronInterval             int64            `yaml:\"TransCronInterval\" default:\"3\"`\n\tTimeoutToFail                 int64            `yaml:\"TimeoutToFail\" default:\"35\"`\n\tRetryInterval                 int64            `yaml:\"RetryInterval\" default:\"10\"`\n\tRequestTimeout                int64            `yaml:\"RequestTimeout\" default:\"3\"`\n\tHTTPPort                      int64            `yaml:\"HttpPort\" default:\"36789\"`\n\tGrpcPort                      int64            `yaml:\"GrpcPort\" default:\"36790\"`\n\tJSONRPCPort                   int64            `yaml:\"JsonRpcPort\" default:\"36791\"`\n\tMicroService                  MicroService     `yaml:\"MicroService\"`\n\tHTTPMicroService              HTTPMicroService `yaml:\"HttpMicroService\"`\n\tUpdateBranchSync              int64            `yaml:\"UpdateBranchSync\"`\n\tUpdateBranchAsyncGoroutineNum int64            `yaml:\"UpdateBranchAsyncGoroutineNum\" default:\"1\"`\n\tLogLevel                      string           `yaml:\"LogLevel\" default:\"info\"`\n\tLog                           Log              `yaml:\"Log\"`\n\tTimeZoneOffset                string           `yaml:\"TimeZoneOffset\"`\n\tConfigUpdateInterval          int64            `yaml:\"ConfigUpdateInterval\" default:\"3\"`\n\tAlertRetryLimit               int64            `yaml:\"AlertRetryLimit\" default:\"3\"`\n\tAlertWebHook                  string           `yaml:\"AlertWebHook\"`\n\tAdminBasePath                 string           `yaml:\"AdminBasePath\"`\n}\n\n// Config config\nvar Config = Type{}\n\n// MustLoadConfig load config from env and file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func MustLoadConfig(confFile string) {\n# Natural Language Annotations: // MustLoadConfig load config from env and file\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc MustLoadConfig(confFile string) {\n\tcontent, err := ioutil.ReadFile(confFile)\n\tif err != nil {\n\t\tlogger.FatalfIf(err != nil, \"read config error: %v\", err)\n\t}\n\terr = yaml.Unmarshal(content, &Config)\n\tlogger.FatalfIf(err != nil, \"config error: %v\", err)\n}\n```", "```go\nfunc MustLoadConfig(confFile string) {\n\tcontent, err := ioutil.ReadFile(confFile)\n\tif err != nil {\n\t\tlogger.FatalfIf(err != nil, \"load config file err is %v\", err)\n\t}\n\tvar confType Type\n\terr = yaml.Unmarshal(content, &confType)\n\tlogger.FatalIf(err != nil, \"parse yaml file err is %v\", err)\n\tConfig = confType\n}\n```", "```go\nfunc MustLoadConfig(confFile string) {\n\t// Read the config file\n\tbs, err := ioutil.ReadFile(confFile)\n\tif err != nil {\n\t\tlogger.Fatalf(\"fail to read config: %v\", err)\n\t}\n\t// Unmarshal the config file into the Config variable\n\tif err := yaml.Unmarshal(bs, &Config); err != nil {\n\t\tlogger.Fatalf(\"fail to unmarshal config: %v\", err)\n\t}\n\t// If there is an environment variable for the config, use it to overwrite the config file\n\tif envConf := os.Getenv(\"CONFIG\"); envConf != \"\" {\n\t\tif err := json.Unmarshal([]byte(envConf), &Config); err != nil {\n\t\t\tlogger.Fatalf(\"fail to unmarshal env config: %v\", err)\n\t\t}\n\t}\n}\n```"]}, "dtm-client/dtmcli/dtmimp/utils.go-MustRemarshal": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/dtm/client/dtmcli/dtmimp/utils.go:\n```\n/*\n * Copyright (c) 2021 yedf. All rights reserved.\n * Use of this source code is governed by a BSD-style\n * license that can be found in the LICENSE file.\n */\n\npackage dtmimp\n\nimport (\n\t\"database/sql\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net/url\"\n\t\"os\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/dtm-labs/logger\"\n\t\"github.com/go-resty/resty/v2\"\n)\n\n// Logf an alias of Infof\n// Deprecated: use logger.Errorf\nvar Logf = logger.Infof\n\n// LogRedf an alias of Errorf\n// Deprecated: use logger.Errorf\nvar LogRedf = logger.Errorf\n\n// FatalIfError fatal if error is not nil\n// Deprecated: use logger.FatalIfError\nvar FatalIfError = logger.FatalIfError\n\n// LogIfFatalf fatal if cond is true\n// Deprecated: use logger.FatalfIf\nvar LogIfFatalf = logger.FatalfIf\n\n// AsError wrap a panic value as an error\nfunc AsError(x interface{}) error {\n\tlogger.Errorf(\"panic wrapped to error: '%v'\", x)\n\tif e, ok := x.(error); ok {\n\t\treturn e\n\t}\n\treturn fmt.Errorf(\"%v\", x)\n}\n\n// P2E panic to error\nfunc P2E(perr *error) {\n\tif x := recover(); x != nil {\n\t\t*perr = AsError(x)\n\t}\n}\n\n// E2P error to panic\nfunc E2P(err error) {\n\tif err != nil {\n\t\tpanic(err)\n\t}\n}\n\n// CatchP catch panic to error\nfunc CatchP(f func()) (rerr error) {\n\tdefer P2E(&rerr)\n\tf()\n\treturn nil\n}\n\n// PanicIf name is clear\nfunc PanicIf(cond bool, err error) {\n\tif cond {\n\t\tpanic(err)\n\t}\n}\n\n// MustAtoi is string to int\nfunc MustAtoi(s string) int {\n\tr, err := strconv.Atoi(s)\n\tif err != nil {\n\t\tE2P(errors.New(\"convert to int error: \" + s))\n\t}\n\treturn r\n}\n\n// OrString return the first not empty string\nfunc OrString(ss ...string) string {\n\tfor _, s := range ss {\n\t\tif s != \"\" {\n\t\t\treturn s\n\t\t}\n\t}\n\treturn \"\"\n}\n\n// If ternary operator\nfunc If(condition bool, trueObj interface{}, falseObj interface{}) interface{} {\n\tif condition {\n\t\treturn trueObj\n\t}\n\treturn falseObj\n}\n\n// MustMarshal checked version for marshal\nfunc MustMarshal(v interface{}) []byte {\n\tb, err := json.Marshal(v)\n\tE2P(err)\n\treturn b\n}\n\n// MustMarshalString string version of MustMarshal\nfunc MustMarshalString(v interface{}) string {\n\treturn string(MustMarshal(v))\n}\n\n// MustUnmarshal checked version for unmarshal\nfunc MustUnmarshal(b []byte, obj interface{}) {\n\terr := json.Unmarshal(b, obj)\n\tE2P(err)\n}\n\n// MustUnmarshalString string version of MustUnmarshal\nfunc MustUnmarshalString(s string, obj interface{}) {\n\tMustUnmarshal([]byte(s), obj)\n}\n\n// MustRemarshal marshal and unmarshal, and check error\n\n\n\n\n\n\n\n// GetFuncName get current call func name\nfunc GetFuncName() string {\n\tpc, _, _, _ := runtime.Caller(1)\n\tnm := runtime.FuncForPC(pc).Name()\n\treturn nm[strings.LastIndex(nm, \".\")+1:]\n}\n\n// MayReplaceLocalhost when run in docker compose, change localhost to host.docker.internal for accessing host network\nfunc MayReplaceLocalhost(host string) string {\n\tif os.Getenv(\"IS_DOCKER\") != \"\" {\n\t\treturn strings.Replace(strings.Replace(host,\n\t\t\t\"localhost\", \"host.docker.internal\", 1),\n\t\t\t\"127.0.0.1\", \"host.docker.internal\", 1)\n\t}\n\treturn host\n}\n\nvar sqlDbs = &mapCache{cache: map[string]*sql.DB{}}\n\ntype mapCache struct {\n\tmutex sync.Mutex\n\tcache map[string]*sql.DB\n}\n\nfunc (m *mapCache) LoadOrStore(conf DBConf, factory func(conf DBConf) (*sql.DB, error)) (*sql.DB, error) {\n\tm.mutex.Lock()\n\tdefer m.mutex.Unlock()\n\tdsn := GetDsn(conf)\n\tif db, ok := m.cache[dsn]; ok {\n\t\treturn db, nil\n\t}\n\tdb, err := factory(conf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tm.cache[dsn] = db\n\treturn db, nil\n}\n\n// PooledDB get pooled sql.DB\nfunc PooledDB(conf DBConf) (*sql.DB, error) {\n\treturn sqlDbs.LoadOrStore(conf, StandaloneDB)\n}\n\n// StandaloneDB get a standalone db instance\nfunc StandaloneDB(conf DBConf) (*sql.DB, error) {\n\tdsn := GetDsn(conf)\n\tlogger.Infof(\"opening standalone %s: %s\", conf.Driver, strings.Replace(dsn, conf.Password, \"****\", 1))\n\treturn sql.Open(conf.Driver, dsn)\n}\n\n// XaDB return a standalone db instance for xa\nfunc XaDB(conf DBConf) (*sql.DB, error) {\n\tdsn := GetDsn(conf)\n\tif conf.Driver == DBTypeMysql {\n\t\tdsn += \"&autocommit=0\"\n\t}\n\tlogger.Infof(\"opening xa standalone %s: %s\", conf.Driver, strings.Replace(dsn, conf.Password, \"****\", 1))\n\treturn sql.Open(conf.Driver, dsn)\n}\n\n// XaClose will log and close the db\nfunc XaClose(db *sql.DB) {\n\tlogger.Infof(\"closing xa db\")\n\t_ = db.Close()\n}\n\n// DBExec use raw db to exec\nfunc DBExec(dbType string, db DB, sql string, values ...interface{}) (affected int64, rerr error) {\n\tif sql == \"\" {\n\t\treturn 0, nil\n\t}\n\tbegan := time.Now()\n\tif len(values) > 0 {\n\t\tsql = GetDBSpecial(dbType).GetPlaceHoldSQL(sql)\n\t}\n\tr, rerr := db.Exec(sql, values...)\n\tused := time.Since(began) / time.Millisecond\n\tif rerr == nil {\n\t\taffected, rerr = r.RowsAffected()\n\t\tlogger.Debugf(\"used: %d ms affected: %d for %s %v\", used, affected, sql, values)\n\t} else {\n\t\tlogger.Errorf(\"used: %d ms exec error: %v for %s %v\", used, rerr, sql, values)\n\t}\n\treturn\n}\n\n// GetDsn get dsn from map config\nfunc GetDsn(conf DBConf) string {\n\thost := MayReplaceLocalhost(conf.Host)\n\tdriver := conf.Driver\n\tdsn := map[string]string{\n\t\t\"mysql\": fmt.Sprintf(\"%s:%s@tcp(%s:%d)/%s?charset=utf8mb4&parseTime=true&loc=Local&interpolateParams=true\",\n\t\t\tconf.User, conf.Password, host, conf.Port, conf.Db),\n\t\t\"postgres\": fmt.Sprintf(\"host=%s user=%s password=%s dbname='%s' search_path=%s port=%d sslmode=disable\",\n\t\t\thost, conf.User, conf.Password, conf.Db, conf.Schema, conf.Port),\n\t\t// sqlserver://sa:mypass@localhost:1234?database=master&connection+timeout=30\n\t\t\"sqlserver\": getSQLServerConnectionString(&conf, &host),\n\t}[driver]\n\tPanicIf(dsn == \"\", fmt.Errorf(\"unknow driver: %s\", driver))\n\treturn dsn\n}\n\nfunc getSQLServerConnectionString(conf *DBConf, host *string) string {\n\tquery := url.Values{}\n\tquery.Add(\"database\", conf.Db)\n\tu := &url.URL{\n\t\tScheme: \"sqlserver\",\n\t\tUser:   url.UserPassword(conf.User, conf.Password),\n\t\tHost:   fmt.Sprintf(\"%s:%d\", *host, conf.Port),\n\t\t// Path:  instance, // if connecting to an instance instead of a port\n\t\tRawQuery: query.Encode(),\n\t}\n\treturn u.String()\n}\n\n// RespAsErrorByJSONRPC  translate json rpc resty response to error\nfunc RespAsErrorByJSONRPC(resp *resty.Response) error {\n\tstr := resp.String()\n\tvar result map[string]interface{}\n\tMustUnmarshalString(str, &result)\n\tif result[\"error\"] != nil {\n\t\trerr := result[\"error\"].(map[string]interface{})\n\t\tif rerr[\"code\"] == JrpcCodeFailure {\n\t\t\treturn fmt.Errorf(\"%s. %w\", str, ErrFailure)\n\t\t} else if rerr[\"code\"] == JrpcCodeOngoing {\n\t\t\treturn ErrOngoing\n\t\t}\n\t\treturn errors.New(resp.String())\n\t}\n\treturn nil\n}\n\n// DeferDo a common defer do used in dtmcli/dtmgrpc\nfunc DeferDo(rerr *error, success func() error, fail func() error) {\n\tif x := recover(); x != nil {\n\t\t*rerr = AsError(x)\n\t\t_ = fail()\n\t\tpanic(x)\n\t} else if *rerr != nil {\n\t\t_ = fail()\n\t} else {\n\t\t*rerr = success()\n\t}\n}\n\n// Escape solve CodeQL reported problem\nfunc Escape(input string) string {\n\tv := strings.Replace(input, \"\\n\", \"\", -1)\n\tv = strings.Replace(v, \"\\r\", \"\", -1)\n\tv = strings.Replace(v, \";\", \"\", -1)\n\t// v = strings.Replace(v, \"'\", \"\", -1)\n\treturn v\n}\n\n// EscapeGet escape get\nfunc EscapeGet(qs url.Values, key string) string {\n\treturn Escape(qs.Get(key))\n}\n\n// InsertBarrier insert a record to barrier\nfunc InsertBarrier(tx DB, transType string, gid string, branchID string, op string, barrierID string, reason string, dbType string, barrierTableName string) (int64, error) {\n\tif op == \"\" {\n\t\treturn 0, nil\n\t}\n\tif dbType == \"\" {\n\t\tdbType = currentDBType\n\t}\n\tif barrierTableName == \"\" {\n\t\tbarrierTableName = BarrierTableName\n\t}\n\tsql := GetDBSpecial(dbType).GetInsertIgnoreTemplate(barrierTableName+\"(trans_type, gid, branch_id, op, barrier_id, reason) values(?,?,?,?,?,?)\", \"uniq_barrier\")\n\treturn DBExec(dbType, tx, sql, transType, gid, branchID, op, barrierID, reason)\n}\n\n```\n# Function Signature: func MustRemarshal(from interface{}, to interface{}) {\n# Natural Language Annotations: // MustRemarshal marshal and unmarshal, and check error\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc MustRemarshal(from interface{}, to interface{}) {\n    b, err := json.Marshal(from)\n    E2P(err)\n    err = json.Unmarshal(b, to)\n    E2P(err)\n}\n```\n\nThis function takes two parameters: `from` and `to`. It marshals the `from` value into a JSON byte slice, checks for any errors, then unmarshals the JSON byte slice into the `to` value. If any errors occur during this process, it panics with the error.", "```go\nfunc MustRemarshal(from interface{}, to interface{}) {\n\tmarshaled, err := json.Marshal(from)\n\tE2P(err)\n\terr = json.Unmarshal(marshaled, to)\n\tE2P(err)\n}\n```", "```go\nfunc MustRemarshal(from interface{}, to interface{}) {\n\tb, err := json.Marshal(from)\n\tE2P(err)\n\terr = json.Unmarshal(b, to)\n\tE2P(err)\n}\n```\nThis function takes two interfaces, marshals the first one to JSON, and then unmarshals the JSON into the second interface. If any error occurs during this process, it panics."]}, "kitex-tool/internal_pkg/generator/type.go-FixHasStreamingForExtendedService": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/tool/internal_pkg/generator/type.go:\n```\n// Copyright 2021 CloudWeGo Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generator\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"strings\"\n\t\"text/template\"\n\n\t\"github.com/cloudwego/thriftgo/generator/golang/streaming\"\n\n\t\"github.com/cloudwego/kitex/tool/internal_pkg/util\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// File .\ntype File struct {\n\tName    string\n\tContent string\n}\n\n// PackageInfo contains information to generate a package for a service.\ntype PackageInfo struct {\n\tNamespace    string            // a dot-separated string for generating service package under kitex_gen\n\tDependencies map[string]string // package name => import path, used for searching imports\n\t*ServiceInfo                   // the target service\n\n\t// the following fields will be filled and used by the generator\n\tCodec            string\n\tNoFastAPI        bool\n\tVersion          string\n\tRealServiceName  string\n\tImports          map[string]map[string]bool // import path => alias\n\tExternalKitexGen string\n\tFeatures         []feature\n\tFrugalPretouch   bool\n\tModule           string\n\tProtocol         transport.Protocol\n\tIDLName          string\n\tServerPkg        string\n}\n\n// AddImport .\nfunc (p *PackageInfo) AddImport(pkg, path string) {\n\tif p.Imports == nil {\n\t\tp.Imports = make(map[string]map[string]bool)\n\t}\n\tif pkg != \"\" {\n\t\tif p.ExternalKitexGen != \"\" && strings.Contains(path, KitexGenPath) {\n\t\t\tparts := strings.Split(path, KitexGenPath)\n\t\t\tpath = util.JoinPath(p.ExternalKitexGen, parts[len(parts)-1])\n\t\t}\n\t\tif path == pkg {\n\t\t\tp.Imports[path] = nil\n\t\t} else {\n\t\t\tif p.Imports[path] == nil {\n\t\t\t\tp.Imports[path] = make(map[string]bool)\n\t\t\t}\n\t\t\tp.Imports[path][pkg] = true\n\t\t}\n\t}\n}\n\n// AddImports .\nfunc (p *PackageInfo) AddImports(pkgs ...string) {\n\tfor _, pkg := range pkgs {\n\t\tif path, ok := p.Dependencies[pkg]; ok {\n\t\t\tp.AddImport(pkg, path)\n\t\t} else {\n\t\t\tp.AddImport(pkg, pkg)\n\t\t}\n\t}\n}\n\n// PkgInfo .\ntype PkgInfo struct {\n\tPkgName    string\n\tPkgRefName string\n\tImportPath string\n}\n\n// ServiceInfo .\ntype ServiceInfo struct {\n\tPkgInfo\n\tServiceName           string\n\tRawServiceName        string\n\tServiceTypeName       func() string\n\tBase                  *ServiceInfo\n\tMethods               []*MethodInfo\n\tCombineServices       []*ServiceInfo\n\tHasStreaming          bool\n\tServiceFilePath       string\n\tProtocol              string\n\tHandlerReturnKeepResp bool\n\tUseThriftReflection   bool\n}\n\n// AllMethods returns all methods that the service have.\nfunc (s *ServiceInfo) AllMethods() (ms []*MethodInfo) {\n\tms = append(ms, s.Methods...)\n\tfor base := s.Base; base != nil; base = base.Base {\n\t\tms = append(base.Methods, ms...)\n\t}\n\treturn ms\n}\n\n// FixHasStreamingForExtendedService updates the HasStreaming field for extended services.\n\n\n\n\n\n\n\n\n\n\n\n// HasStreamingRecursive recursively check if the service has streaming method\n\n\n\n\n\n\n\n\n\n\n// MethodInfo .\ntype MethodInfo struct {\n\tPkgInfo\n\tServiceName            string\n\tName                   string\n\tRawName                string\n\tOneway                 bool\n\tVoid                   bool\n\tArgs                   []*Parameter\n\tArgsLength             int\n\tResp                   *Parameter\n\tExceptions             []*Parameter\n\tArgStructName          string\n\tResStructName          string\n\tIsResponseNeedRedirect bool // int -> int*\n\tGenArgResultStruct     bool\n\tClientStreaming        bool\n\tServerStreaming        bool\n\tStreaming              *streaming.Streaming\n}\n\n// Parameter .\ntype Parameter struct {\n\tDeps    []PkgInfo\n\tName    string\n\tRawName string\n\tType    string // *PkgA.StructB\n}\n\nvar funcs = map[string]interface{}{\n\t\"ToLower\":       strings.ToLower,\n\t\"LowerFirst\":    util.LowerFirst,\n\t\"UpperFirst\":    util.UpperFirst,\n\t\"NotPtr\":        util.NotPtr,\n\t\"ReplaceString\": util.ReplaceString,\n\t\"SnakeString\":   util.SnakeString,\n\t\"HasFeature\":    HasFeature,\n\t\"FilterImports\": FilterImports,\n\t\"backquoted\":    BackQuoted,\n}\n\nfunc AddTemplateFunc(key string, f interface{}) {\n\tfuncs[key] = f\n}\n\nvar templateNames = []string{\n\t\"@client.go-NewClient-option\",\n\t\"@client.go-NewStreamClient-option\",\n\t\"@client.go-EOF\",\n\t\"@server.go-NewServer-option\",\n\t\"@server.go-EOF\",\n\t\"@invoker.go-NewInvoker-option\",\n\t\"@invoker.go-EOF\",\n}\n\nfunc wrapTemplate(point, content string) string {\n\treturn fmt.Sprintf(`{{define \"%s\"}}%s{{end}}`, point, content)\n}\n\nvar templateExtensions = (func() map[string]string {\n\tm := make(map[string]string)\n\tfor _, name := range templateNames {\n\t\t// create dummy templates\n\t\tm[name] = wrapTemplate(name, \"\")\n\t}\n\treturn m\n})()\n\n// SetTemplateExtension .\nfunc SetTemplateExtension(name, text string) {\n\tif _, ok := templateExtensions[name]; ok {\n\t\ttemplateExtensions[name] = text\n\t}\n}\n\nfunc applyExtension(name string, x *template.Template) (*template.Template, error) {\n\tvar err error\n\tfor _, n := range templateNames {\n\t\tx, err = x.Parse(templateExtensions[n])\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to parse extension %q for %s: %w (%#q)\",\n\t\t\t\tn, name, err, templateExtensions[n])\n\t\t}\n\t}\n\treturn x, nil\n}\n\n// Task .\ntype Task struct {\n\tName string\n\tPath string\n\tText string\n\t*template.Template\n\tExt *APIExtension\n}\n\n// Build .\nfunc (t *Task) Build() error {\n\tx, err := template.New(t.Name).Funcs(funcs).Parse(t.Text)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// old fashion\n\tx, err = applyExtension(t.Name, x)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// new fashion\n\tfor _, str := range t.makeExtension() {\n\t\tif x, err = x.Parse(str); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tt.Template = x\n\treturn nil\n}\n\nfunc fileTemplateExtension(name string) (option, eof string) {\n\tfor _, tn := range templateNames {\n\t\tif strings.HasPrefix(tn, \"@\"+name+\"-\") {\n\t\t\tif strings.HasSuffix(tn, \"-option\") {\n\t\t\t\toption = tn\n\t\t\t} else if strings.HasSuffix(tn, \"-EOF\") {\n\t\t\t\teof = tn\n\t\t\t}\n\t\t}\n\t}\n\treturn\n}\n\nfunc (t *Task) makeExtension() (res []string) {\n\tif t.Ext == nil {\n\t\treturn\n\t}\n\tp1, p2 := fileTemplateExtension(t.Name)\n\tif t.Ext.ExtendOption != \"\" {\n\t\tres = append(res, wrapTemplate(p1, t.Ext.ExtendOption))\n\t}\n\tif t.Ext.ExtendFile != \"\" {\n\t\tres = append(res, wrapTemplate(p2, t.Ext.ExtendFile))\n\t}\n\treturn\n}\n\n// Render .\nfunc (t *Task) Render(data interface{}) (*File, error) {\n\tif t.Template == nil {\n\t\terr := t.Build()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tvar buf bytes.Buffer\n\terr := t.ExecuteTemplate(&buf, t.Name, data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &File{t.Path, buf.String()}, nil\n}\n\nfunc (t *Task) RenderString(data interface{}) (string, error) {\n\tif t.Template == nil {\n\t\terr := t.Build()\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t}\n\n\tvar buf bytes.Buffer\n\terr := t.ExecuteTemplate(&buf, t.Name, data)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn buf.String(), nil\n}\n\nfunc FilterImports(Imports map[string]map[string]bool, ms []*MethodInfo) map[string]map[string]bool {\n\tres := map[string]map[string]bool{}\n\tfor _, m := range ms {\n\t\tif m.Resp != nil {\n\t\t\tfor _, dep := range m.Resp.Deps {\n\t\t\t\tif _, ok := Imports[dep.ImportPath]; ok {\n\t\t\t\t\tres[dep.ImportPath] = Imports[dep.ImportPath]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tfor _, arg := range m.Args {\n\t\t\tfor _, dep := range arg.Deps {\n\t\t\t\tif _, ok := Imports[dep.ImportPath]; ok {\n\t\t\t\t\tres[dep.ImportPath] = Imports[dep.ImportPath]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn res\n}\n\nfunc BackQuoted(s string) string {\n\treturn \"`\" + s + \"`\"\n}\n\n```\n# Function Signature: func (s *ServiceInfo) FixHasStreamingForExtendedService() {\n# Natural Language Annotations: // FixHasStreamingForExtendedService updates the HasStreaming field for extended services.\nPlease return the generated code surrounded by ```\n", "response": ["Sure, here is the generated function based on the function signature and comments given:\n\n```go\nfunc (s *ServiceInfo) FixHasStreamingForExtendedService() {\n\tfor _, base := range s.CombineServices {\n\t\tfor _, method := range base.Methods {\n\t\t\tif method.ClientStreaming || method.ServerStreaming {\n\t\t\t\ts.HasStreaming = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n}\n```\n\nThis function iterates over all combine services in the service info, and for each combine service, it checks if any of its methods have client or server streaming enabled. If it finds a method that does, it sets the HasStreaming field of the service to true and breaks out of the loop.\n\nPlease note that this function assumes that the CombineServices slice is already populated with the services that are being combined to create the current service. If this is not the case, the function will not work as expected.", "```go\nfunc (s *ServiceInfo) FixHasStreamingForExtendedService() {\n\tif s == nil {\n\t\treturn\n\t}\n\n\tallMethods := s.AllMethods()\n\tfor _, method := range allMethods {\n\t\tif method.ClientStreaming || method.ServerStreaming {\n\t\t\ts.HasStreaming = true\n\t\t\treturn\n\t\t}\n\t}\n\n\ts.HasStreaming = false\n}\n```", "```go\nfunc (s *ServiceInfo) FixHasStreamingForExtendedService() {\n\tfor _, baseService := range s.CombineServices {\n\t\tfor _, method := range baseService.Methods {\n\t\t\tif method.ClientStreaming || method.ServerStreaming {\n\t\t\t\ts.HasStreaming = true\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n}\n```"]}, "kitex-tool/internal_pkg/generator/type.go-HasStreamingRecursive": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/tool/internal_pkg/generator/type.go:\n```\n// Copyright 2021 CloudWeGo Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generator\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"strings\"\n\t\"text/template\"\n\n\t\"github.com/cloudwego/thriftgo/generator/golang/streaming\"\n\n\t\"github.com/cloudwego/kitex/tool/internal_pkg/util\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// File .\ntype File struct {\n\tName    string\n\tContent string\n}\n\n// PackageInfo contains information to generate a package for a service.\ntype PackageInfo struct {\n\tNamespace    string            // a dot-separated string for generating service package under kitex_gen\n\tDependencies map[string]string // package name => import path, used for searching imports\n\t*ServiceInfo                   // the target service\n\n\t// the following fields will be filled and used by the generator\n\tCodec            string\n\tNoFastAPI        bool\n\tVersion          string\n\tRealServiceName  string\n\tImports          map[string]map[string]bool // import path => alias\n\tExternalKitexGen string\n\tFeatures         []feature\n\tFrugalPretouch   bool\n\tModule           string\n\tProtocol         transport.Protocol\n\tIDLName          string\n\tServerPkg        string\n}\n\n// AddImport .\nfunc (p *PackageInfo) AddImport(pkg, path string) {\n\tif p.Imports == nil {\n\t\tp.Imports = make(map[string]map[string]bool)\n\t}\n\tif pkg != \"\" {\n\t\tif p.ExternalKitexGen != \"\" && strings.Contains(path, KitexGenPath) {\n\t\t\tparts := strings.Split(path, KitexGenPath)\n\t\t\tpath = util.JoinPath(p.ExternalKitexGen, parts[len(parts)-1])\n\t\t}\n\t\tif path == pkg {\n\t\t\tp.Imports[path] = nil\n\t\t} else {\n\t\t\tif p.Imports[path] == nil {\n\t\t\t\tp.Imports[path] = make(map[string]bool)\n\t\t\t}\n\t\t\tp.Imports[path][pkg] = true\n\t\t}\n\t}\n}\n\n// AddImports .\nfunc (p *PackageInfo) AddImports(pkgs ...string) {\n\tfor _, pkg := range pkgs {\n\t\tif path, ok := p.Dependencies[pkg]; ok {\n\t\t\tp.AddImport(pkg, path)\n\t\t} else {\n\t\t\tp.AddImport(pkg, pkg)\n\t\t}\n\t}\n}\n\n// PkgInfo .\ntype PkgInfo struct {\n\tPkgName    string\n\tPkgRefName string\n\tImportPath string\n}\n\n// ServiceInfo .\ntype ServiceInfo struct {\n\tPkgInfo\n\tServiceName           string\n\tRawServiceName        string\n\tServiceTypeName       func() string\n\tBase                  *ServiceInfo\n\tMethods               []*MethodInfo\n\tCombineServices       []*ServiceInfo\n\tHasStreaming          bool\n\tServiceFilePath       string\n\tProtocol              string\n\tHandlerReturnKeepResp bool\n\tUseThriftReflection   bool\n}\n\n// AllMethods returns all methods that the service have.\nfunc (s *ServiceInfo) AllMethods() (ms []*MethodInfo) {\n\tms = append(ms, s.Methods...)\n\tfor base := s.Base; base != nil; base = base.Base {\n\t\tms = append(base.Methods, ms...)\n\t}\n\treturn ms\n}\n\n// FixHasStreamingForExtendedService updates the HasStreaming field for extended services.\n\n\n\n\n\n\n\n\n\n\n\n// HasStreamingRecursive recursively check if the service has streaming method\n\n\n\n\n\n\n\n\n\n\n// MethodInfo .\ntype MethodInfo struct {\n\tPkgInfo\n\tServiceName            string\n\tName                   string\n\tRawName                string\n\tOneway                 bool\n\tVoid                   bool\n\tArgs                   []*Parameter\n\tArgsLength             int\n\tResp                   *Parameter\n\tExceptions             []*Parameter\n\tArgStructName          string\n\tResStructName          string\n\tIsResponseNeedRedirect bool // int -> int*\n\tGenArgResultStruct     bool\n\tClientStreaming        bool\n\tServerStreaming        bool\n\tStreaming              *streaming.Streaming\n}\n\n// Parameter .\ntype Parameter struct {\n\tDeps    []PkgInfo\n\tName    string\n\tRawName string\n\tType    string // *PkgA.StructB\n}\n\nvar funcs = map[string]interface{}{\n\t\"ToLower\":       strings.ToLower,\n\t\"LowerFirst\":    util.LowerFirst,\n\t\"UpperFirst\":    util.UpperFirst,\n\t\"NotPtr\":        util.NotPtr,\n\t\"ReplaceString\": util.ReplaceString,\n\t\"SnakeString\":   util.SnakeString,\n\t\"HasFeature\":    HasFeature,\n\t\"FilterImports\": FilterImports,\n\t\"backquoted\":    BackQuoted,\n}\n\nfunc AddTemplateFunc(key string, f interface{}) {\n\tfuncs[key] = f\n}\n\nvar templateNames = []string{\n\t\"@client.go-NewClient-option\",\n\t\"@client.go-NewStreamClient-option\",\n\t\"@client.go-EOF\",\n\t\"@server.go-NewServer-option\",\n\t\"@server.go-EOF\",\n\t\"@invoker.go-NewInvoker-option\",\n\t\"@invoker.go-EOF\",\n}\n\nfunc wrapTemplate(point, content string) string {\n\treturn fmt.Sprintf(`{{define \"%s\"}}%s{{end}}`, point, content)\n}\n\nvar templateExtensions = (func() map[string]string {\n\tm := make(map[string]string)\n\tfor _, name := range templateNames {\n\t\t// create dummy templates\n\t\tm[name] = wrapTemplate(name, \"\")\n\t}\n\treturn m\n})()\n\n// SetTemplateExtension .\nfunc SetTemplateExtension(name, text string) {\n\tif _, ok := templateExtensions[name]; ok {\n\t\ttemplateExtensions[name] = text\n\t}\n}\n\nfunc applyExtension(name string, x *template.Template) (*template.Template, error) {\n\tvar err error\n\tfor _, n := range templateNames {\n\t\tx, err = x.Parse(templateExtensions[n])\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to parse extension %q for %s: %w (%#q)\",\n\t\t\t\tn, name, err, templateExtensions[n])\n\t\t}\n\t}\n\treturn x, nil\n}\n\n// Task .\ntype Task struct {\n\tName string\n\tPath string\n\tText string\n\t*template.Template\n\tExt *APIExtension\n}\n\n// Build .\nfunc (t *Task) Build() error {\n\tx, err := template.New(t.Name).Funcs(funcs).Parse(t.Text)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// old fashion\n\tx, err = applyExtension(t.Name, x)\n\tif err != nil {\n\t\treturn err\n\t}\n\t// new fashion\n\tfor _, str := range t.makeExtension() {\n\t\tif x, err = x.Parse(str); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tt.Template = x\n\treturn nil\n}\n\nfunc fileTemplateExtension(name string) (option, eof string) {\n\tfor _, tn := range templateNames {\n\t\tif strings.HasPrefix(tn, \"@\"+name+\"-\") {\n\t\t\tif strings.HasSuffix(tn, \"-option\") {\n\t\t\t\toption = tn\n\t\t\t} else if strings.HasSuffix(tn, \"-EOF\") {\n\t\t\t\teof = tn\n\t\t\t}\n\t\t}\n\t}\n\treturn\n}\n\nfunc (t *Task) makeExtension() (res []string) {\n\tif t.Ext == nil {\n\t\treturn\n\t}\n\tp1, p2 := fileTemplateExtension(t.Name)\n\tif t.Ext.ExtendOption != \"\" {\n\t\tres = append(res, wrapTemplate(p1, t.Ext.ExtendOption))\n\t}\n\tif t.Ext.ExtendFile != \"\" {\n\t\tres = append(res, wrapTemplate(p2, t.Ext.ExtendFile))\n\t}\n\treturn\n}\n\n// Render .\nfunc (t *Task) Render(data interface{}) (*File, error) {\n\tif t.Template == nil {\n\t\terr := t.Build()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tvar buf bytes.Buffer\n\terr := t.ExecuteTemplate(&buf, t.Name, data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &File{t.Path, buf.String()}, nil\n}\n\nfunc (t *Task) RenderString(data interface{}) (string, error) {\n\tif t.Template == nil {\n\t\terr := t.Build()\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t}\n\n\tvar buf bytes.Buffer\n\terr := t.ExecuteTemplate(&buf, t.Name, data)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn buf.String(), nil\n}\n\nfunc FilterImports(Imports map[string]map[string]bool, ms []*MethodInfo) map[string]map[string]bool {\n\tres := map[string]map[string]bool{}\n\tfor _, m := range ms {\n\t\tif m.Resp != nil {\n\t\t\tfor _, dep := range m.Resp.Deps {\n\t\t\t\tif _, ok := Imports[dep.ImportPath]; ok {\n\t\t\t\t\tres[dep.ImportPath] = Imports[dep.ImportPath]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tfor _, arg := range m.Args {\n\t\t\tfor _, dep := range arg.Deps {\n\t\t\t\tif _, ok := Imports[dep.ImportPath]; ok {\n\t\t\t\t\tres[dep.ImportPath] = Imports[dep.ImportPath]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn res\n}\n\nfunc BackQuoted(s string) string {\n\treturn \"`\" + s + \"`\"\n}\n\n```\n# Function Signature: func (s *ServiceInfo) HasStreamingRecursive() bool {\n# Natural Language Annotations: // HasStreamingRecursive recursively check if the service has streaming method\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (s *ServiceInfo) HasStreamingRecursive() bool {\n\tif s.HasStreaming {\n\t\treturn true\n\t}\n\n\tif s.Base != nil {\n\t\treturn s.Base.HasStreamingRecursive()\n\t}\n\n\treturn false\n}\n```", "```go\nfunc (s *ServiceInfo) HasStreamingRecursive() bool {\n\tif s.ClientStreaming || s.ServerStreaming {\n\t\treturn true\n\t}\n\tfor _, method := range s.Methods {\n\t\tif method.ClientStreaming || method.ServerStreaming {\n\t\t\treturn true\n\t\t}\n\t}\n\tif s.Base != nil {\n\t\treturn s.Base.HasStreamingRecursive()\n\t}\n\treturn false\n}\n```", "```go\nfunc (s *ServiceInfo) HasStreamingRecursive() bool {\n\tfor _, m := range s.AllMethods() {\n\t\tif m.ClientStreaming || m.ServerStreaming {\n\t\t\treturn true\n\t\t}\n\t}\n\n\tif s.Base != nil {\n\t\treturn s.Base.HasStreamingRecursive()\n\t}\n\n\treturn false\n}\n```"]}, "kitex-tool/internal_pkg/generator/generator.go-Unpack": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/tool/internal_pkg/generator/generator.go:\n```\n// Copyright 2021 CloudWeGo Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package generator .\npackage generator\n\nimport (\n\t\"fmt\"\n\t\"go/token\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/tool/internal_pkg/log\"\n\t\"github.com/cloudwego/kitex/tool/internal_pkg/tpl\"\n\t\"github.com/cloudwego/kitex/tool/internal_pkg/util\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Constants .\nconst (\n\tKitexGenPath = \"kitex_gen\"\n\tDefaultCodec = \"thrift\"\n\n\tBuildFileName       = \"build.sh\"\n\tBootstrapFileName   = \"bootstrap.sh\"\n\tToolVersionFileName = \"kitex_info.yaml\"\n\tHandlerFileName     = \"handler.go\"\n\tMainFileName        = \"main.go\"\n\tClientFileName      = \"client.go\"\n\tServerFileName      = \"server.go\"\n\tInvokerFileName     = \"invoker.go\"\n\tServiceFileName     = \"*service.go\"\n\tExtensionFilename   = \"extensions.yaml\"\n\n\tDefaultThriftPluginTimeLimit = time.Minute\n)\n\nvar (\n\tkitexImportPath = \"github.com/cloudwego/kitex\"\n\n\tglobalMiddlewares  []Middleware\n\tglobalDependencies = map[string]string{\n\t\t\"kitex\":     kitexImportPath,\n\t\t\"client\":    ImportPathTo(\"client\"),\n\t\t\"server\":    ImportPathTo(\"server\"),\n\t\t\"callopt\":   ImportPathTo(\"client/callopt\"),\n\t\t\"frugal\":    \"github.com/cloudwego/frugal\",\n\t\t\"fieldmask\": \"github.com/cloudwego/thriftgo/fieldmask\",\n\t}\n)\n\n// SetKitexImportPath sets the import path of kitex.\n// Must be called before generating code.\nfunc SetKitexImportPath(path string) {\n\tfor k, v := range globalDependencies {\n\t\tglobalDependencies[k] = strings.ReplaceAll(v, kitexImportPath, path)\n\t}\n\tkitexImportPath = path\n}\n\n// ImportPathTo returns an import path to the specified package under kitex.\nfunc ImportPathTo(pkg string) string {\n\treturn util.JoinPath(kitexImportPath, pkg)\n}\n\n// AddGlobalMiddleware adds middleware for all generators\nfunc AddGlobalMiddleware(mw Middleware) {\n\tglobalMiddlewares = append(globalMiddlewares, mw)\n}\n\n// AddGlobalDependency adds dependency for all generators\nfunc AddGlobalDependency(ref, path string) bool {\n\tif _, ok := globalDependencies[ref]; !ok {\n\t\tglobalDependencies[ref] = path\n\t\treturn true\n\t}\n\treturn false\n}\n\n// Generator generates the codes of main package and scripts for building a server based on kitex.\ntype Generator interface {\n\tGenerateService(pkg *PackageInfo) ([]*File, error)\n\tGenerateMainPackage(pkg *PackageInfo) ([]*File, error)\n\tGenerateCustomPackage(pkg *PackageInfo) ([]*File, error)\n}\n\n// Config .\ntype Config struct {\n\tVerbose               bool\n\tGenerateMain          bool // whether stuff in the main package should be generated\n\tGenerateInvoker       bool // generate main.go with invoker when main package generate\n\tVersion               string\n\tNoFastAPI             bool\n\tModuleName            string\n\tServiceName           string\n\tUse                   string\n\tIDLType               string\n\tIncludes              util.StringSlice\n\tThriftOptions         util.StringSlice\n\tProtobufOptions       util.StringSlice\n\tHessian2Options       util.StringSlice\n\tIDL                   string // the IDL file passed on the command line\n\tOutputPath            string // the output path for main pkg and kitex_gen\n\tPackagePrefix         string\n\tCombineService        bool // combine services to one service\n\tCopyIDL               bool\n\tThriftPlugins         util.StringSlice\n\tProtobufPlugins       util.StringSlice\n\tFeatures              []feature\n\tFrugalPretouch        bool\n\tThriftPluginTimeLimit time.Duration\n\tCompilerPath          string // specify the path of thriftgo or protoc\n\n\tExtensionFile string\n\ttmplExt       *TemplateExtension\n\n\tRecord    bool\n\tRecordCmd []string\n\n\tTemplateDir string\n\n\tGenPath string\n\n\tDeepCopyAPI           bool\n\tProtocol              string\n\tHandlerReturnKeepResp bool\n\n\tNoDependencyCheck bool\n}\n\n// Pack packs the Config into a slice of \"key=val\" strings.\nfunc (c *Config) Pack() (res []string) {\n\tt := reflect.TypeOf(c).Elem()\n\tv := reflect.ValueOf(c).Elem()\n\tfor i := 0; i < t.NumField(); i++ {\n\t\tf := t.Field(i)\n\t\tx := v.Field(i)\n\t\tn := f.Name\n\n\t\t// skip the plugin arguments to avoid the 'strings in strings' trouble\n\t\tif f.Name == \"ThriftPlugins\" || !token.IsExported(f.Name) {\n\t\t\tcontinue\n\t\t}\n\n\t\tif str, ok := x.Interface().(interface{ String() string }); ok {\n\t\t\tres = append(res, n+\"=\"+str.String())\n\t\t\tcontinue\n\t\t}\n\n\t\tswitch x.Kind() {\n\t\tcase reflect.Bool:\n\t\t\tres = append(res, n+\"=\"+fmt.Sprint(x.Bool()))\n\t\tcase reflect.String:\n\t\t\tres = append(res, n+\"=\"+x.String())\n\t\tcase reflect.Slice:\n\t\t\tvar ss []string\n\t\t\tif x.Type().Elem().Kind() == reflect.Int {\n\t\t\t\tfor i := 0; i < x.Len(); i++ {\n\t\t\t\t\tss = append(ss, strconv.Itoa(int(x.Index(i).Int())))\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfor i := 0; i < x.Len(); i++ {\n\t\t\t\t\tss = append(ss, x.Index(i).String())\n\t\t\t\t}\n\t\t\t}\n\t\t\tres = append(res, n+\"=\"+strings.Join(ss, \";\"))\n\t\tdefault:\n\t\t\tpanic(fmt.Errorf(\"unsupported field type: %+v\", f))\n\t\t}\n\t}\n\treturn res\n}\n\n// Unpack restores the Config from a slice of \"key=val\" strings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// AddFeature add registered feature to config\nfunc (c *Config) AddFeature(key string) bool {\n\tif f, ok := getFeature(key); ok {\n\t\tc.Features = append(c.Features, f)\n\t\treturn true\n\t}\n\treturn false\n}\n\n// ApplyExtension applies template extension.\nfunc (c *Config) ApplyExtension() error {\n\ttemplateExtExist := false\n\tpath := util.JoinPath(c.TemplateDir, ExtensionFilename)\n\tif c.TemplateDir != \"\" && util.Exists(path) {\n\t\ttemplateExtExist = true\n\t}\n\n\tif c.ExtensionFile == \"\" && !templateExtExist {\n\t\treturn nil\n\t}\n\n\text := new(TemplateExtension)\n\tif c.ExtensionFile != \"\" {\n\t\tif err := ext.FromYAMLFile(c.ExtensionFile); err != nil {\n\t\t\treturn fmt.Errorf(\"read template extension %q failed: %s\", c.ExtensionFile, err.Error())\n\t\t}\n\t}\n\n\tif templateExtExist {\n\t\tyamlExt := new(TemplateExtension)\n\t\tif err := yamlExt.FromYAMLFile(path); err != nil {\n\t\t\treturn fmt.Errorf(\"read template extension %q failed: %s\", path, err.Error())\n\t\t}\n\t\text.Merge(yamlExt)\n\t}\n\n\tfor _, fn := range ext.FeatureNames {\n\t\tRegisterFeature(fn)\n\t}\n\tfor _, fn := range ext.EnableFeatures {\n\t\tc.AddFeature(fn)\n\t}\n\tfor path, alias := range ext.Dependencies {\n\t\tAddGlobalDependency(alias, path)\n\t}\n\n\tc.tmplExt = ext\n\treturn nil\n}\n\n// NewGenerator .\nfunc NewGenerator(config *Config, middlewares []Middleware) Generator {\n\tmws := append(globalMiddlewares, middlewares...)\n\tg := &generator{Config: config, middlewares: mws}\n\tif g.IDLType == \"\" {\n\t\tg.IDLType = DefaultCodec\n\t}\n\treturn g\n}\n\n// Middleware used generator\ntype Middleware func(HandleFunc) HandleFunc\n\n// HandleFunc used generator\ntype HandleFunc func(*Task, *PackageInfo) (*File, error)\n\ntype generator struct {\n\t*Config\n\tmiddlewares []Middleware\n}\n\nfunc (g *generator) chainMWs(handle HandleFunc) HandleFunc {\n\tfor i := len(g.middlewares) - 1; i > -1; i-- {\n\t\thandle = g.middlewares[i](handle)\n\t}\n\treturn handle\n}\n\nfunc (g *generator) GenerateMainPackage(pkg *PackageInfo) (fs []*File, err error) {\n\tg.updatePackageInfo(pkg)\n\n\ttasks := []*Task{\n\t\t{\n\t\t\tName: BuildFileName,\n\t\t\tPath: util.JoinPath(g.OutputPath, BuildFileName),\n\t\t\tText: tpl.BuildTpl,\n\t\t},\n\t\t{\n\t\t\tName: BootstrapFileName,\n\t\t\tPath: util.JoinPath(g.OutputPath, \"script\", BootstrapFileName),\n\t\t\tText: tpl.BootstrapTpl,\n\t\t},\n\t\t{\n\t\t\tName: ToolVersionFileName,\n\t\t\tPath: util.JoinPath(g.OutputPath, ToolVersionFileName),\n\t\t\tText: tpl.ToolVersionTpl,\n\t\t},\n\t}\n\tif !g.Config.GenerateInvoker {\n\t\ttasks = append(tasks, &Task{\n\t\t\tName: MainFileName,\n\t\t\tPath: util.JoinPath(g.OutputPath, MainFileName),\n\t\t\tText: tpl.MainTpl,\n\t\t})\n\t}\n\tfor _, t := range tasks {\n\t\tif util.Exists(t.Path) {\n\t\t\tlog.Info(t.Path, \"exists. Skipped.\")\n\t\t\tcontinue\n\t\t}\n\t\tg.setImports(t.Name, pkg)\n\t\thandle := func(task *Task, pkg *PackageInfo) (*File, error) {\n\t\t\treturn task.Render(pkg)\n\t\t}\n\t\tf, err := g.chainMWs(handle)(t, pkg)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfs = append(fs, f)\n\t}\n\n\thandlerFilePath := filepath.Join(g.OutputPath, HandlerFileName)\n\tif util.Exists(handlerFilePath) {\n\t\tcomp := newCompleter(\n\t\t\tpkg.ServiceInfo.AllMethods(),\n\t\t\thandlerFilePath,\n\t\t\tpkg.ServiceInfo.ServiceName)\n\t\tf, err := comp.CompleteMethods()\n\t\tif err != nil {\n\t\t\tif err == errNoNewMethod {\n\t\t\t\treturn fs, nil\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\t\tfs = append(fs, f)\n\t} else {\n\t\ttask := Task{\n\t\t\tName: HandlerFileName,\n\t\t\tPath: handlerFilePath,\n\t\t\tText: tpl.HandlerTpl + \"\\n\" + tpl.HandlerMethodsTpl,\n\t\t}\n\t\tg.setImports(task.Name, pkg)\n\t\thandle := func(task *Task, pkg *PackageInfo) (*File, error) {\n\t\t\treturn task.Render(pkg)\n\t\t}\n\t\tf, err := g.chainMWs(handle)(&task, pkg)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfs = append(fs, f)\n\t}\n\treturn\n}\n\nfunc (g *generator) GenerateService(pkg *PackageInfo) ([]*File, error) {\n\tg.updatePackageInfo(pkg)\n\toutput := util.JoinPath(g.OutputPath, util.CombineOutputPath(g.GenPath, pkg.Namespace))\n\tsvcPkg := strings.ToLower(pkg.ServiceName)\n\toutput = util.JoinPath(output, svcPkg)\n\text := g.tmplExt\n\tif ext == nil {\n\t\text = new(TemplateExtension)\n\t}\n\n\ttasks := []*Task{\n\t\t{\n\t\t\tName: ClientFileName,\n\t\t\tPath: util.JoinPath(output, ClientFileName),\n\t\t\tText: tpl.ClientTpl,\n\t\t\tExt:  ext.ExtendClient,\n\t\t},\n\t\t{\n\t\t\tName: ServerFileName,\n\t\t\tPath: util.JoinPath(output, ServerFileName),\n\t\t\tText: tpl.ServerTpl,\n\t\t\tExt:  ext.ExtendServer,\n\t\t},\n\t\t{\n\t\t\tName: InvokerFileName,\n\t\t\tPath: util.JoinPath(output, InvokerFileName),\n\t\t\tText: tpl.InvokerTpl,\n\t\t\tExt:  ext.ExtendInvoker,\n\t\t},\n\t\t{\n\t\t\tName: ServiceFileName,\n\t\t\tPath: util.JoinPath(output, svcPkg+\".go\"),\n\t\t\tText: tpl.ServiceTpl,\n\t\t},\n\t}\n\n\tvar fs []*File\n\tfor _, t := range tasks {\n\t\tif err := t.Build(); err != nil {\n\t\t\terr = fmt.Errorf(\"build %s failed: %w\", t.Name, err)\n\t\t\treturn nil, err\n\t\t}\n\t\tg.setImports(t.Name, pkg)\n\t\tif t.Ext != nil {\n\t\t\tfor _, path := range t.Ext.ImportPaths {\n\t\t\t\tif alias, exist := ext.Dependencies[path]; exist {\n\t\t\t\t\tpkg.AddImports(alias)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\thandle := func(task *Task, pkg *PackageInfo) (*File, error) {\n\t\t\treturn task.Render(pkg)\n\t\t}\n\t\tf, err := g.chainMWs(handle)(t, pkg)\n\t\tif err != nil {\n\t\t\terr = fmt.Errorf(\"render %s failed: %w\", t.Name, err)\n\t\t\treturn nil, err\n\t\t}\n\t\tfs = append(fs, f)\n\t}\n\treturn fs, nil\n}\n\nfunc (g *generator) updatePackageInfo(pkg *PackageInfo) {\n\tpkg.NoFastAPI = g.NoFastAPI\n\tpkg.Codec = g.IDLType\n\tpkg.Version = g.Version\n\tpkg.RealServiceName = g.ServiceName\n\tpkg.Features = g.Features\n\tpkg.ExternalKitexGen = g.Use\n\tpkg.FrugalPretouch = g.FrugalPretouch\n\tpkg.Module = g.ModuleName\n\tif strings.EqualFold(g.Protocol, transport.HESSIAN2.String()) {\n\t\tpkg.Protocol = transport.HESSIAN2\n\t}\n\tif pkg.Dependencies == nil {\n\t\tpkg.Dependencies = make(map[string]string)\n\t}\n\n\tfor ref, path := range globalDependencies {\n\t\tif _, ok := pkg.Dependencies[ref]; !ok {\n\t\t\tpkg.Dependencies[ref] = path\n\t\t}\n\t}\n}\n\nfunc (g *generator) setImports(name string, pkg *PackageInfo) {\n\tpkg.Imports = make(map[string]map[string]bool)\n\tswitch name {\n\tcase ClientFileName:\n\t\tpkg.AddImports(\"client\")\n\t\tif pkg.HasStreaming {\n\t\t\tpkg.AddImport(\"streaming\", \"github.com/cloudwego/kitex/pkg/streaming\")\n\t\t\tpkg.AddImport(\"transport\", \"github.com/cloudwego/kitex/transport\")\n\t\t}\n\t\tif len(pkg.AllMethods()) > 0 {\n\t\t\tif needCallOpt(pkg) {\n\t\t\t\tpkg.AddImports(\"callopt\")\n\t\t\t}\n\t\t\tpkg.AddImports(\"context\")\n\t\t}\n\t\tfallthrough\n\tcase HandlerFileName:\n\t\tfor _, m := range pkg.ServiceInfo.AllMethods() {\n\t\t\tif !m.ServerStreaming && !m.ClientStreaming {\n\t\t\t\tpkg.AddImports(\"context\")\n\t\t\t}\n\t\t\tfor _, a := range m.Args {\n\t\t\t\tfor _, dep := range a.Deps {\n\t\t\t\t\tpkg.AddImport(dep.PkgRefName, dep.ImportPath)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !m.Void && m.Resp != nil {\n\t\t\t\tfor _, dep := range m.Resp.Deps {\n\t\t\t\t\tpkg.AddImport(dep.PkgRefName, dep.ImportPath)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\tcase ServerFileName, InvokerFileName:\n\t\tif len(pkg.CombineServices) == 0 {\n\t\t\tpkg.AddImport(pkg.ServiceInfo.PkgRefName, pkg.ServiceInfo.ImportPath)\n\t\t}\n\t\tpkg.AddImports(\"server\")\n\tcase ServiceFileName:\n\t\tpkg.AddImports(\"errors\")\n\t\tpkg.AddImports(\"client\")\n\t\tpkg.AddImport(\"kitex\", \"github.com/cloudwego/kitex/pkg/serviceinfo\")\n\t\tpkg.AddImport(pkg.ServiceInfo.PkgRefName, pkg.ServiceInfo.ImportPath)\n\t\tif len(pkg.AllMethods()) > 0 {\n\t\t\tpkg.AddImports(\"context\")\n\t\t}\n\t\tfor _, m := range pkg.ServiceInfo.AllMethods() {\n\t\t\tif m.ClientStreaming || m.ServerStreaming {\n\t\t\t\tpkg.AddImports(\"fmt\")\n\t\t\t}\n\t\t\tif m.GenArgResultStruct {\n\t\t\t\tpkg.AddImports(\"proto\")\n\t\t\t} else {\n\t\t\t\t// for method Arg and Result\n\t\t\t\tpkg.AddImport(m.PkgRefName, m.ImportPath)\n\t\t\t}\n\t\t\tfor _, a := range m.Args {\n\t\t\t\tfor _, dep := range a.Deps {\n\t\t\t\t\tpkg.AddImport(dep.PkgRefName, dep.ImportPath)\n\t\t\t\t}\n\t\t\t}\n\t\t\tif m.Streaming.IsStreaming || pkg.Codec == \"protobuf\" {\n\t\t\t\t// protobuf handler support both PingPong and Unary (streaming) requests\n\t\t\t\tpkg.AddImport(\"streaming\", \"github.com/cloudwego/kitex/pkg/streaming\")\n\t\t\t}\n\t\t\tif !m.Void && m.Resp != nil {\n\t\t\t\tfor _, dep := range m.Resp.Deps {\n\t\t\t\t\tpkg.AddImport(dep.PkgRefName, dep.ImportPath)\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor _, e := range m.Exceptions {\n\t\t\t\tfor _, dep := range e.Deps {\n\t\t\t\t\tpkg.AddImport(dep.PkgRefName, dep.ImportPath)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif pkg.FrugalPretouch {\n\t\t\tpkg.AddImports(\"sync\")\n\t\t\tif len(pkg.AllMethods()) > 0 {\n\t\t\t\tpkg.AddImports(\"frugal\")\n\t\t\t\tpkg.AddImports(\"reflect\")\n\t\t\t}\n\t\t}\n\tcase MainFileName:\n\t\tpkg.AddImport(\"log\", \"log\")\n\t\tpkg.AddImport(pkg.PkgRefName, util.JoinPath(pkg.ImportPath, strings.ToLower(pkg.ServiceName)))\n\t}\n}\n\nfunc needCallOpt(pkg *PackageInfo) bool {\n\t// callopt is referenced only by non-streaming methods\n\tneedCallOpt := false\n\tswitch pkg.Codec {\n\tcase \"thrift\":\n\t\tfor _, m := range pkg.ServiceInfo.AllMethods() {\n\t\t\tif !m.Streaming.IsStreaming {\n\t\t\t\tneedCallOpt = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\tcase \"protobuf\":\n\t\tneedCallOpt = true\n\t}\n\treturn needCallOpt\n}\n\n```\n# Function Signature: func (c *Config) Unpack(args []string) error {\n# Natural Language Annotations: // Unpack restores the Config from a slice of \"key=val\" strings.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (c *Config) Unpack(args []string) error {\n\tt := reflect.TypeOf(c).Elem()\n\tfor _, arg := range args {\n\t\tparts := strings.SplitN(arg, \"=\", 2)\n\t\tif len(parts) != 2 {\n\t\t\treturn fmt.Errorf(\"invalid argument: %s\", arg)\n\t\t}\n\t\tname := parts[0]\n\t\tvalue := parts[1]\n\n\t\tf := t.FieldByName(name)\n\t\tif !f.IsValid() {\n\t\t\treturn fmt.Errorf(\"unknown field: %s\", name)\n\t\t}\n\n\t\tx := reflect.ValueOf(c).Elem().FieldByName(name)\n\t\tswitch f.Type.Kind() {\n\t\tcase reflect.Bool:\n\t\t\tv, err := strconv.ParseBool(value)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"invalid bool value for %s: %s\", name, value)\n\t\t\t}\n\t\t\tx.SetBool(v)\n\t\tcase reflect.String:\n\t\t\tx.SetString(value)\n\t\tcase reflect.Slice:\n\t\t\tvar ss []string\n\t\t\tif f.Type.Elem().Kind() == reflect.Int {\n\t\t\t\tfor _, str := range strings.Split(value, \";\") {\n\t\t\t\t\ti, err := strconv.Atoi(str)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn fmt.Errorf(\"invalid int value in slice for %s: %s\", name, str)\n\t\t\t\t\t}\n\t\t\t\t\tss = append(ss, strconv.Itoa(i))\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tss = strings.Split(value, \";\")\n\t\t\t}\n\t\t\tx.Set(reflect.ValueOf(ss))\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"unsupported field type: %s\", f.Type.Kind())\n\t\t}\n\t}\n\treturn nil\n}\n```", "```Go\nfunc (c *Config) Unpack(args []string) error {\n\tt := reflect.TypeOf(c).Elem()\n\tv := reflect.ValueOf(c).Elem()\n\n\tfor _, arg := range args {\n\t\ts := strings.Split(arg, \"=\")\n\t\tif len(s) != 2 {\n\t\t\treturn fmt.Errorf(\"invalid argument: %s\", arg)\n\t\t}\n\n\t\tname, value := s[0], s[1]\n\n\t\tf, ok := t.FieldByNameFunc(func(name string) bool {\n\t\t\tfield, _ := t.FieldByName(name)\n\t\t\treturn strings.ToLower(field.Name) == strings.ToLower(name)\n\t\t})\n\t\tif !ok {\n\t\t\treturn fmt.Errorf(\"unknown field: %s\", name)\n\t\t}\n\n\t\tif !v.FieldByIndex(f.Index).CanSet() {\n\t\t\treturn fmt.Errorf(\"field %s is not settable\", f.Name)\n\t\t}\n\n\t\tswitch f.Type.Kind() {\n\t\tcase reflect.Bool:\n\t\t\tb, err := strconv.ParseBool(value)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"invalid bool value for %s: %s\", name, value)\n\t\t\t}\n\t\t\tv.FieldByIndex(f.Index).SetBool(b)\n\t\tcase reflect.String:\n\t\t\tv.FieldByIndex(f.Index).SetString(value)\n\t\tcase reflect.Slice:\n\t\t\tss := strings.Split(value, \";\")\n\t\t\tslice := reflect.MakeSlice(f.Type, len(ss), len(ss))\n\t\t\tfor i, s := range ss {\n\t\t\t\tif f.Type.Elem().Kind() == reflect.Int {\n\t\t\t\t\tv, err := strconv.Atoi(s)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn fmt.Errorf(\"invalid int value for %s: %s\", name, value)\n\t\t\t\t\t}\n\t\t\t\t\tslice.Index(i).SetInt(int64(v))\n\t\t\t\t} else {\n\t\t\t\t\tslice.Index(i).SetString(s)\n\t\t\t\t}\n\t\t\t}\n\t\t\tv.FieldByIndex(f.Index).Set(slice)\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"unsupported field type: %s\", f.Type.Kind())\n\t\t}\n\t}\n\n\treturn nil\n}\n```\n\nThis function takes a slice of string arguments, splits them into key-value pairs, and then sets the corresponding field in the Config struct to the parsed value. It uses reflection to dynamically set fields based on the field name (case-insensitive) and field type. If the field is not settable or if the type is unsupported, it returns an error.", "```go\nfunc (c *Config) Unpack(args []string) error {\n\tt := reflect.TypeOf(c).Elem()\n\tv := reflect.ValueOf(c).Elem()\n\n\tfor _, arg := range args {\n\t\tparts := strings.SplitN(arg, \"=\", 2)\n\t\tif len(parts) != 2 {\n\t\t\treturn fmt.Errorf(\"invalid argument: %s\", arg)\n\t\t}\n\n\t\tkey := parts[0]\n\t\tval := parts[1]\n\n\t\tf, ok := t.FieldByName(key)\n\t\tif !ok {\n\t\t\treturn fmt.Errorf(\"unsupported field: %s\", key)\n\t\t}\n\n\t\tx := v.FieldByName(key)\n\t\tif !x.CanSet() {\n\t\t\treturn fmt.Errorf(\"field %s is not settable\", key)\n\t\t}\n\n\t\tswitch f.Type.Kind() {\n\t\tcase reflect.Bool:\n\t\t\tb, err := strconv.ParseBool(val)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"invalid bool value for %s: %s\", key, val)\n\t\t\t}\n\t\t\tx.SetBool(b)\n\t\tcase reflect.String:\n\t\t\tx.SetString(val)\n\t\tcase reflect.Slice:\n\t\t\t// split values by ';'\n\t\t\tparts := strings.Split(val, \";\")\n\t\t\tslice := reflect.MakeSlice(f.Type, len(parts), len(parts))\n\t\t\tfor i, s := range parts {\n\t\t\t\tif f.Type.Elem().Kind() == reflect.String {\n\t\t\t\t\tslice.Index(i).SetString(s)\n\t\t\t\t} else if f.Type.Elem().Kind() == reflect.Int {\n\t\t\t\t\tn, err := strconv.Atoi(s)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn fmt.Errorf(\"invalid int value for %s: %s\", key, s)\n\t\t\t\t\t}\n\t\t\t\t\tslice.Index(i).SetInt(int64(n))\n\t\t\t\t} else {\n\t\t\t\t\treturn fmt.Errorf(\"unsupported slice type: %v\", f.Type.Elem())\n\t\t\t\t}\n\t\t\t}\n\t\t\tx.Set(slice)\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"unsupported field type: %v\", f.Type)\n\t\t}\n\t}\n\n\treturn nil\n}\n```"]}, "kitex-tool/internal_pkg/util/util.go-CombineOutputPath": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/tool/internal_pkg/util/util.go:\n```\n// Copyright 2021 CloudWeGo Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage util\n\nimport (\n\t\"fmt\"\n\t\"go/build\"\n\t\"go/format\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"os\"\n\t\"os/exec\"\n\t\"os/user\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"runtime\"\n\t\"sort\"\n\t\"strings\"\n\t\"unicode\"\n)\n\n// StringSlice implements the flag.Value interface on string slices\n// to allow a flag to be set multiple times.\ntype StringSlice []string\n\nfunc (ss *StringSlice) String() string {\n\treturn fmt.Sprintf(\"%v\", *ss)\n}\n\n// Set implements the flag.Value interface.\nfunc (ss *StringSlice) Set(value string) error {\n\t*ss = append(*ss, value)\n\treturn nil\n}\n\n// FormatCode formats go source codes.\nfunc FormatCode(code []byte) ([]byte, error) {\n\tformatCode, err := format.Source(code)\n\tif err != nil {\n\t\treturn code, fmt.Errorf(\"format code error: %s\", err)\n\t}\n\treturn formatCode, nil\n}\n\n// GetGOPATH retrieves the GOPATH from environment variables or the `go env` command.\nfunc GetGOPATH() (string, error) {\n\tgoPath := os.Getenv(\"GOPATH\")\n\t// If there are many path in GOPATH, pick up the first one.\n\tif GoPaths := strings.Split(goPath, \":\"); len(GoPaths) >= 1 && strings.TrimSpace(GoPaths[0]) != \"\" {\n\t\treturn strings.TrimSpace(GoPaths[0]), nil\n\t}\n\t// GOPATH not set through environment variables, try to get one by executing \"go env GOPATH\"\n\toutput, err := exec.Command(\"go\", \"env\", \"GOPATH\").Output()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tgoPath = strings.TrimSpace(string(output))\n\tif len(goPath) == 0 {\n\t\tbuildContext := build.Default\n\t\tgoPath = buildContext.GOPATH\n\t}\n\n\tif len(goPath) == 0 {\n\t\treturn \"\", fmt.Errorf(\"GOPATH not found\")\n\t}\n\treturn goPath, nil\n}\n\n// Exists reports whether a file exists.\nfunc Exists(path string) bool {\n\tfi, err := os.Stat(path)\n\tif err != nil {\n\t\treturn os.IsExist(err)\n\t}\n\treturn !fi.IsDir()\n}\n\n// LowerFirst converts the first letter to upper case for the given string.\nfunc LowerFirst(s string) string {\n\trs := []rune(s)\n\trs[0] = unicode.ToLower(rs[0])\n\treturn string(rs)\n}\n\n// ReplaceString be used in string substitution.\nfunc ReplaceString(s, old, new string, n int) string {\n\treturn strings.Replace(s, old, new, n)\n}\n\n// SnakeString converts the string 's' to a snake string\nfunc SnakeString(s string) string {\n\tdata := make([]byte, 0, len(s)*2)\n\tj := false\n\tfor _, d := range []byte(s) {\n\t\tif d >= 'A' && d <= 'Z' {\n\t\t\tif j {\n\t\t\t\tdata = append(data, '_')\n\t\t\t\tj = false\n\t\t\t}\n\t\t} else if d != '_' {\n\t\t\tj = true\n\t\t}\n\t\tdata = append(data, d)\n\t}\n\treturn strings.ToLower(string(data))\n}\n\n// UpperFirst converts the first letter to upper case for the given string.\nfunc UpperFirst(s string) string {\n\trs := []rune(s)\n\trs[0] = unicode.ToUpper(rs[0])\n\treturn string(rs)\n}\n\n// NotPtr converts an pointer type into non-pointer type.\nfunc NotPtr(s string) string {\n\treturn strings.ReplaceAll(s, \"*\", \"\")\n}\n\n// SearchGoMod searches go.mod from the given directory (which must be an absolute path) to\n// the root directory. When the go.mod is found, its module name and path will be returned.\nfunc SearchGoMod(cwd string) (moduleName, path string, found bool) {\n\tfor {\n\t\tpath = filepath.Join(cwd, \"go.mod\")\n\t\tdata, err := ioutil.ReadFile(path)\n\t\tif err == nil {\n\t\t\tre := regexp.MustCompile(`^\\s*module\\s+(\\S+)\\s*`)\n\t\t\tfor _, line := range strings.Split(string(data), \"\\n\") {\n\t\t\t\tm := re.FindStringSubmatch(line)\n\t\t\t\tif m != nil {\n\t\t\t\t\treturn m[1], cwd, true\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn fmt.Sprintf(\"<module name not found in '%s'>\", path), path, true\n\t\t}\n\n\t\tif !os.IsNotExist(err) {\n\t\t\treturn\n\t\t}\n\t\tparentCwd := filepath.Dir(cwd)\n\t\tif parentCwd == cwd {\n\t\t\tbreak\n\t\t}\n\t\tcwd = parentCwd\n\t}\n\treturn\n}\n\nfunc RunGitCommand(gitLink string) (string, string, error) {\n\tu, err := user.Current()\n\tif err != nil {\n\t\treturn \"\", \"Failed to get home dir\", err\n\t}\n\tcachePath := JoinPath(u.HomeDir, \".kitex\", \"cache\")\n\n\tbranch := \"\"\n\tif strings.Contains(gitLink, \".git@\") {\n\t\tstrs := strings.Split(gitLink, \".git@\")\n\t\tbranch = strs[1]\n\t\tgitLink = strs[0] + \".git\"\n\t}\n\tpullLink := gitLink\n\n\tgitLink = strings.TrimPrefix(gitLink, \"git@\")\n\n\tgitLink = strings.TrimSuffix(gitLink, \".git\")\n\n\trepoLink := \"\"\n\tif strings.Contains(gitLink, \"://\") {\n\t\trepoLink = strings.Split(gitLink, \"://\")[1]\n\t} else {\n\t\trepoLink = strings.ReplaceAll(gitLink, \":\", \"/\")\n\t}\n\n\tbranchSuffix := \"\"\n\tif branch != \"\" {\n\t\tbranchSuffix = \"@\" + branch\n\t}\n\tgitPath := JoinPath(cachePath, repoLink+branchSuffix)\n\n\t_, err = os.Stat(JoinPath(gitPath, \".git\"))\n\tif err != nil && !os.IsExist(err) {\n\t\terr = os.MkdirAll(gitPath, os.ModePerm)\n\t\tif err != nil {\n\t\t\treturn \"\", \"Failed to create cache directory,please check your permission for ~/.kitex/cache\", err\n\t\t}\n\t\tcmdClone := exec.Command(\"git\", \"clone\", pullLink, \".\")\n\t\tcmdClone.Dir = gitPath\n\t\tout, gitErr := cmdClone.CombinedOutput()\n\t\tif gitErr != nil {\n\t\t\treturn \"\", string(out), gitErr\n\t\t}\n\t\tif branch != \"\" {\n\t\t\tcmdCheckout := exec.Command(\"git\", \"checkout\", branch)\n\t\t\tcmdCheckout.Dir = gitPath\n\t\t\tout, gitErr = cmdCheckout.CombinedOutput()\n\t\t\treturn gitPath, string(out), gitErr\n\t\t} else {\n\t\t\treturn gitPath, \"\", nil\n\t\t}\n\t}\n\n\tcmdPull := exec.Command(\"git\", \"pull\")\n\tcmdPull.Dir = gitPath\n\tout, gitErr := cmdPull.CombinedOutput()\n\tif gitErr != nil {\n\t\treturn \"\", string(out), gitErr\n\t}\n\n\treturn gitPath, \"\", nil\n}\n\n// CombineOutputPath read the output and path variables and render them into the final path\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// JoinPath joins dirs as golang import format, such as xx/xx/xx\nfunc JoinPath(elem ...string) string {\n\tif runtime.GOOS == \"windows\" {\n\t\treturn strings.ReplaceAll(filepath.Join(elem...), \"\\\\\", \"/\")\n\t}\n\treturn filepath.Join(elem...)\n}\n\n// DownloadFile Download file to local\nfunc DownloadFile(remotePath, localPath string) error {\n\tresp, err := http.Get(remotePath)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer resp.Body.Close()\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn fmt.Errorf(\"failed to download file, http status: %s\", resp.Status)\n\t}\n\n\tbody, err := ioutil.ReadAll(resp.Body)\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = ioutil.WriteFile(localPath, body, 0o644)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// IDLName returns the name of the IDL file.\nfunc IDLName(filename string) string {\n\treturn filepath.Base(filename)\n}\n\ntype Import struct {\n\tAlias string\n\tPath  string\n}\n\nfunc SortImports(imps map[string]string, localPrefix string) (ret []Import) {\n\tstds := make([]Import, 0, len(imps))\n\tlocals := make([]Import, 0, len(imps))\n\tthirds := make([]Import, 0, len(imps))\n\tfor path, alias := range imps {\n\t\tif strings.HasPrefix(path, localPrefix+\"/\") {\n\t\t\tlocals = append(locals, Import{alias, path})\n\t\t} else if !strings.Contains(path, \".\") {\n\t\t\tstds = append(stds, Import{alias, path})\n\t\t} else {\n\t\t\tthirds = append(thirds, Import{alias, path})\n\t\t}\n\t}\n\n\tsort.SliceStable(stds, func(i, j int) bool {\n\t\treturn stds[i].Path < stds[j].Path\n\t})\n\tret = append(ret, stds...)\n\tif len(thirds) > 0 {\n\t\tret = append(ret, Import{\"\", \"\"})\n\t}\n\tsort.SliceStable(thirds, func(i, j int) bool {\n\t\treturn thirds[i].Path < thirds[j].Path\n\t})\n\tret = append(ret, thirds...)\n\tif len(locals) > 0 {\n\t\tret = append(ret, Import{\"\", \"\"})\n\t}\n\tsort.SliceStable(locals, func(i, j int) bool {\n\t\treturn locals[i].Path < locals[j].Path\n\t})\n\tret = append(ret, locals...)\n\treturn ret\n}\n\nfunc (i Import) PackageName() string {\n\tif i.Alias != \"\" {\n\t\treturn i.Alias\n\t} else {\n\t\treturn strings.ToLower(filepath.Base(i.Path))\n\t}\n}\n\nfunc PrintlImports(imports []Import) string {\n\tbuilder := strings.Builder{}\n\tfor _, v := range imports {\n\t\tif v.Path != \"\" {\n\t\t\tbuilder.WriteString(fmt.Sprintf(\"%s %q\\n\", v.Alias, v.Path))\n\t\t} else {\n\t\t\tbuilder.WriteString(\"\\n\")\n\t\t}\n\t}\n\treturn builder.String()\n}\n\n```\n# Function Signature: func CombineOutputPath(outputPath, ns string) string {\n# Natural Language Annotations: // CombineOutputPath read the output and path variables and render them into the final path\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc CombineOutputPath(outputPath, ns string) string {\n\tif outputPath == \"\" {\n\t\treturn JoinPath(ns)\n\t}\n\tif ns == \"\" {\n\t\treturn JoinPath(outputPath)\n\t}\n\treturn JoinPath(outputPath, ns)\n}\n```", "```go\nfunc CombineOutputPath(outputPath, ns string) string {\n\t// Combine the output path and namespace to form the final path\n\tfinalPath := filepath.Join(outputPath, ns)\n\n\t// Return the final path\n\treturn finalPath\n}\n```\n\nThis code creates a function `CombineOutputPath` that takes in two parameters: `outputPath` and `ns`. It uses the `filepath.Join` function to combine `outputPath` and `ns` into `finalPath`. Finally, it returns `finalPath`, which is the final path generated by combining the `outputPath` and `ns`.", "```go\npackage util\n\nimport (\n\t\"path/filepath\"\n\t\"strings\"\n)\n\nfunc CombineOutputPath(outputPath, ns string) string {\n\tdirs := strings.Split(ns, \".\")\n\tpath := filepath.Join(outputPath, JoinPath(dirs...))\n\treturn path\n}\n```\nThis function takes in an outputPath and a namespace. It splits the namespace into individual directories, joins them with the outputPath, and then returns the final path."]}, "kitex-pkg/rpcinfo/stats_util.go-CalcEventCostUs": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/rpcinfo/stats_util.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *  http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage rpcinfo\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"runtime/debug\"\n\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n)\n\n// Record records the event to RPCStats.\nfunc Record(ctx context.Context, ri RPCInfo, event stats.Event, err error) {\n\tif ctx == nil || ri.Stats() == nil {\n\t\treturn\n\t}\n\tif err != nil {\n\t\tri.Stats().Record(ctx, event, stats.StatusError, err.Error())\n\t} else {\n\t\tri.Stats().Record(ctx, event, stats.StatusInfo, \"\")\n\t}\n}\n\n// CalcEventCostUs calculates the duration between start and end and returns in microsecond.\n\n\n\n\n\n\n\n// ClientPanicToErr to transform the panic info to error, and output the error if needed.\nfunc ClientPanicToErr(ctx context.Context, panicInfo interface{}, ri RPCInfo, logErr bool) error {\n\te := fmt.Errorf(\"KITEX: client panic, to_service=%s to_method=%s error=%v\\nstack=%s\",\n\t\tri.To().ServiceName(), ri.To().Method(), panicInfo, debug.Stack())\n\trpcStats := AsMutableRPCStats(ri.Stats())\n\trpcStats.SetPanicked(e)\n\tif logErr {\n\t\tklog.CtxErrorf(ctx, \"%s\", e.Error())\n\t}\n\treturn e\n}\n\n```\n# Function Signature: func CalcEventCostUs(start, end Event) uint64 {\n# Natural Language Annotations: // CalcEventCostUs calculates the duration between start and end and returns in microsecond.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// CalcEventCostUs calculates the duration between start and end and returns in microsecond.\nfunc CalcEventCostUs(start, end Event) uint64 {\n\t// Assuming Event has a method to get the timestamp\n\tstartTime := start.GetTimestamp()\n\tendTime := end.GetTimestamp()\n\n\t// Assuming Event is a struct with Timestamp and Status fields\n\t// Assuming Timestamp is a time.Time\n\tduration := endTime.Sub(startTime)\n\n\t// Convert duration to microseconds\n\tmicroseconds := uint64(duration.Microseconds())\n\n\treturn microseconds\n}\n```", "```go\nfunc CalcEventCostUs(start, end stats.Event) uint64 {\n    if start != nil && end != nil {\n        startTime := start.GetStartTime()\n        endTime := end.GetEndTime()\n\n        if startTime.IsZero() || endTime.IsZero() {\n            // handle error, return 0 or throw an exception according to your needs\n            return 0\n        }\n\n        duration := endTime.Sub(startTime)\n        return uint64(duration.Microseconds())\n    }\n    return 0\n}\n```\n\nIn this function, we first check if both start and end are not nil. Then, we get the start and end time from the event. After that, we validate if these times are not zero. If any of these checks fail, we return 0 or handle the error according to your needs. If everything is valid, we calculate the duration between the start and end time and return it in microseconds.", "```go\npackage rpcinfo\n\nimport (\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n)\n\n// CalcEventCostUs calculates the duration between start and end and returns in microsecond.\nfunc CalcEventCostUs(start, end stats.Event) uint64 {\n\t// Assuming start and end are timestamps and are of type time.Time\n\tstartTime := time.Unix(0, start. Nanoseconds())\n\tendTime := time.Unix(0, end.Nanoseconds())\n\n\t// Calculate the duration\n\tduration := endTime.Sub(startTime)\n\n\t// Convert the duration to microseconds and return\n\treturn uint64(duration.Microseconds())\n}\n```"]}, "kitex-pkg/rpcinfo/invocation.go-NewInvocation": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/rpcinfo/invocation.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage rpcinfo\n\nimport (\n\t\"sync\"\n\t\"sync/atomic\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\nvar (\n\t_              Invocation       = (*invocation)(nil)\n\t_              InvocationSetter = (*invocation)(nil)\n\tinvocationPool sync.Pool\n\tglobalSeqID    int32 = 0\n)\n\nfunc init() {\n\tinvocationPool.New = newInvocation\n}\n\n// InvocationSetter is used to set information about an RPC.\ntype InvocationSetter interface {\n\tSetPackageName(name string)\n\tSetServiceName(name string)\n\tSetMethodName(name string)\n\tSetSeqID(seqID int32)\n\tSetBizStatusErr(err kerrors.BizStatusErrorIface)\n\tSetExtra(key string, value interface{})\n\tReset()\n}\ntype invocation struct {\n\tpackageName string\n\tserviceName string\n\tmethodName  string\n\tseqID       int32\n\tbizErr      kerrors.BizStatusErrorIface\n\textra       map[string]interface{}\n}\n\n// NewInvocation creates a new Invocation with the given service, method and optional package.\n\n\n\n\n\n\n\n\n\n\n\n// NewServerInvocation to get Invocation for new request in server side\nfunc NewServerInvocation() Invocation {\n\tivk := invocationPool.Get().(*invocation)\n\treturn ivk\n}\n\nfunc genSeqID() int32 {\n\tid := atomic.AddInt32(&globalSeqID, 1)\n\tif id == 0 {\n\t\t// seqID is non-0 to avoid potential default value judgments leading to error handling\n\t\tid = atomic.AddInt32(&globalSeqID, 1)\n\t}\n\treturn id\n}\n\nfunc newInvocation() interface{} {\n\treturn &invocation{}\n}\n\n// SeqID implements the Invocation interface.\nfunc (i *invocation) SeqID() int32 {\n\treturn i.seqID\n}\n\n// SetSeqID implements the InvocationSetter interface.\nfunc (i *invocation) SetSeqID(seqID int32) {\n\ti.seqID = seqID\n}\n\nfunc (i *invocation) PackageName() string {\n\treturn i.packageName\n}\n\nfunc (i *invocation) SetPackageName(name string) {\n\ti.packageName = name\n}\n\nfunc (i *invocation) ServiceName() string {\n\treturn i.serviceName\n}\n\n// SetServiceName implements the InvocationSetter interface.\nfunc (i *invocation) SetServiceName(name string) {\n\ti.serviceName = name\n}\n\n// MethodName implements the Invocation interface.\nfunc (i *invocation) MethodName() string {\n\treturn i.methodName\n}\n\n// SetMethodName implements the InvocationSetter interface.\nfunc (i *invocation) SetMethodName(name string) {\n\ti.methodName = name\n}\n\n// BizStatusErr implements the Invocation interface.\nfunc (i *invocation) BizStatusErr() kerrors.BizStatusErrorIface {\n\treturn i.bizErr\n}\n\n// SetBizStatusErr implements the InvocationSetter interface.\nfunc (i *invocation) SetBizStatusErr(err kerrors.BizStatusErrorIface) {\n\ti.bizErr = err\n}\n\nfunc (i *invocation) SetExtra(key string, value interface{}) {\n\tif i.extra == nil {\n\t\ti.extra = map[string]interface{}{}\n\t}\n\ti.extra[key] = value\n}\n\nfunc (i *invocation) Extra(key string) interface{} {\n\tif i.extra == nil {\n\t\treturn nil\n\t}\n\treturn i.extra[key]\n}\n\n// Reset implements the InvocationSetter interface.\nfunc (i *invocation) Reset() {\n\ti.zero()\n}\n\n// Recycle reuses the invocation.\nfunc (i *invocation) Recycle() {\n\ti.zero()\n\tinvocationPool.Put(i)\n}\n\nfunc (i *invocation) zero() {\n\ti.seqID = 0\n\ti.packageName = \"\"\n\ti.serviceName = \"\"\n\ti.methodName = \"\"\n\ti.bizErr = nil\n\tfor key := range i.extra {\n\t\tdelete(i.extra, key)\n\t}\n}\n\n```\n# Function Signature: func NewInvocation(service, method string, pkgOpt ...string) *invocation {\n# Natural Language Annotations: // NewInvocation creates a new Invocation with the given service, method and optional package.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewInvocation(service, method string, pkgOpt ...string) *invocation {\n\tpackageName := \"\"\n\tif len(pkgOpt) > 0 {\n\t\tpackageName = pkgOpt[0]\n\t}\n\tivk := invocationPool.Get().(*invocation)\n\tivk.SetServiceName(service)\n\tivk.SetMethodName(method)\n\tivk.SetPackageName(packageName)\n\tivk.SetSeqID(genSeqID())\n\treturn ivk\n}\n```", "```go\nfunc NewInvocation(service, method string, pkgOpt ...string) *invocation {\n\tpkgName := \"\"\n\tif len(pkgOpt) > 0 {\n\t\tpkgName = pkgOpt[0]\n\t}\n\tinv := invocationPool.Get().(*invocation)\n\tinv.SetPackageName(pkgName)\n\tinv.SetServiceName(service)\n\tinv.SetMethodName(method)\n\tinv.SetSeqID(genSeqID())\n\treturn inv\n}\n```", "```go\nfunc NewInvocation(service, method string, pkgOpt ...string) *invocation {\n\tvar pkgName string\n\tif len(pkgOpt) > 0 {\n\t\tpkgName = pkgOpt[0]\n\t}\n\n\tivk := invocationPool.Get().(*invocation)\n\tivk.SetServiceName(service)\n\tivk.SetMethodName(method)\n\tivk.SetPackageName(pkgName)\n\n\treturn ivk\n}\n```"]}, "kitex-pkg/rpcinfo/tracer.go-DoFinish": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/rpcinfo/tracer.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage rpcinfo\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"runtime/debug\"\n\n\t\"github.com/cloudwego/kitex/internal\"\n\t\"github.com/cloudwego/kitex/internal/stream\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n)\n\n// StreamEventReporter should be implemented by any tracer that wants to report stream events\ntype StreamEventReporter interface {\n\t// ReportStreamEvent is for collecting Recv/Send events on stream\n\t// NOTE: The callee should NOT hold references to event, which may be recycled later\n\tReportStreamEvent(ctx context.Context, ri RPCInfo, event Event)\n}\n\n// TraceController controls tracers.\ntype TraceController struct {\n\ttracers              []stats.Tracer\n\tstreamEventReporters []StreamEventReporter\n}\n\n// Append appends a new tracer to the controller.\nfunc (c *TraceController) Append(col stats.Tracer) {\n\tc.tracers = append(c.tracers, col)\n\tif reporter, ok := col.(StreamEventReporter); ok {\n\t\tc.streamEventReporters = append(c.streamEventReporters, reporter)\n\t}\n}\n\n// DoStart starts the tracers.\nfunc (c *TraceController) DoStart(ctx context.Context, ri RPCInfo) context.Context {\n\tdefer c.tryRecover(ctx)\n\tRecord(ctx, ri, stats.RPCStart, nil)\n\n\tfor _, col := range c.tracers {\n\t\tctx = col.Start(ctx)\n\t}\n\treturn ctx\n}\n\n// DoFinish calls the tracers in reversed order.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc buildStreamingEvent(statsEvent stats.Event, err error) Event {\n\tif err == nil || err == io.EOF {\n\t\treturn NewEvent(statsEvent, stats.StatusInfo, \"\")\n\t} else {\n\t\treturn NewEvent(statsEvent, stats.StatusError, err.Error())\n\t}\n}\n\n// ReportStreamEvent is for collecting Recv/Send events on stream\nfunc (c *TraceController) ReportStreamEvent(ctx context.Context, statsEvent stats.Event, err error) {\n\tif !c.HasStreamEventReporter() {\n\t\treturn\n\t}\n\tdefer c.tryRecover(ctx)\n\tevent := buildStreamingEvent(statsEvent, err)\n\tdefer func() {\n\t\tif recyclable, ok := event.(internal.Reusable); ok {\n\t\t\trecyclable.Recycle()\n\t\t}\n\t}()\n\t// RPCInfo is likely to be used by each reporter\n\tri := GetRPCInfo(ctx)\n\tfor i := len(c.streamEventReporters) - 1; i >= 0; i-- {\n\t\tc.streamEventReporters[i].ReportStreamEvent(ctx, ri, event)\n\t}\n}\n\n// GetStreamEventHandler returns the stream event handler\n// If there's no StreamEventReporter, nil is returned for client/server to skip adding tracing middlewares\nfunc (c *TraceController) GetStreamEventHandler() stream.StreamEventHandler {\n\tif c.HasStreamEventReporter() {\n\t\treturn c.ReportStreamEvent\n\t}\n\treturn nil\n}\n\n// HasTracer reports whether there exists any tracer.\nfunc (c *TraceController) HasTracer() bool {\n\treturn c != nil && len(c.tracers) > 0\n}\n\n// HasStreamEventReporter reports whether there exists any StreamEventReporter.\nfunc (c *TraceController) HasStreamEventReporter() bool {\n\treturn c != nil && len(c.streamEventReporters) > 0\n}\n\nfunc (c *TraceController) tryRecover(ctx context.Context) {\n\tif err := recover(); err != nil {\n\t\tklog.CtxWarnf(ctx, \"Panic happened during tracer call. This doesn't affect the rpc call, but may lead to lack of monitor data such as metrics and logs: error=%s, stack=%s\", err, string(debug.Stack()))\n\t}\n}\n\n```\n# Function Signature: func (c *TraceController) DoFinish(ctx context.Context, ri RPCInfo, err error) {\n# Natural Language Annotations: // DoFinish calls the tracers in reversed order.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (c *TraceController) DoFinish(ctx context.Context, ri RPCInfo, err error) {\n\tdefer c.tryRecover(ctx)\n\tRecord(ctx, ri, stats.RPCFinish, err)\n\n\tfor i := len(c.tracers) - 1; i >= 0; i-- {\n\t\tc.tracers[i].Finish(ctx)\n\t}\n}\n```", "```go\n// DoFinish calls the tracers in reversed order.\nfunc (c *TraceController) DoFinish(ctx context.Context, ri RPCInfo, err error) {\n\tdefer c.tryRecover(ctx)\n\tRecord(ctx, ri, stats.RPCFinish, nil)\n\n\tfor i := len(c.tracers) - 1; i >= 0; i-- {\n\t\tc.tracers[i].Finish(ctx)\n\t}\n}\n```", "```go\nfunc (c *TraceController) DoFinish(ctx context.Context, ri RPCInfo, err error) {\n\tdefer c.tryRecover(ctx)\n\tRecord(ctx, ri, stats.RPCFinish, err)\n\n\tfor i := len(c.tracers) - 1; i >= 0; i-- {\n\t\tc.tracers[i].Finish(ctx)\n\t}\n}\n```"]}, "kitex-pkg/retry/retryer.go-InitWithPolicies": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/retry/retryer.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package retry implements rpc retry\npackage retry\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/bytedance/gopkg/cloud/circuitbreaker\"\n\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\n// RPCCallFunc is the definition with wrap rpc call\ntype RPCCallFunc func(context.Context, Retryer) (rpcinfo rpcinfo.RPCInfo, resp interface{}, err error)\n\n// GenRetryKeyFunc to generate retry key through rpcinfo.\n// You can customize the config key according to your config center.\ntype GenRetryKeyFunc func(ctx context.Context, ri rpcinfo.RPCInfo) string\n\n// Retryer is the interface for Retry implements\ntype Retryer interface {\n\t// AllowRetry to check if current request satisfy retry condition[eg: circuit, retry times == 0, chain stop, ddl].\n\t// If not satisfy won't execute Retryer.Do and return the reason message\n\t// Execute anyway for the first time regardless of able to retry.\n\tAllowRetry(ctx context.Context) (msg string, ok bool)\n\n\t// ShouldRetry to check if retry request can be called, it is checked in retryer.Do.\n\t// If not satisfy will return the reason message\n\tShouldRetry(ctx context.Context, err error, callTimes int, req interface{}, cbKey string) (msg string, ok bool)\n\tUpdatePolicy(policy Policy) error\n\n\t// Retry policy execute func. recycleRI is to decide if the firstRI can be recycled.\n\tDo(ctx context.Context, rpcCall RPCCallFunc, firstRI rpcinfo.RPCInfo, request interface{}) (lastRI rpcinfo.RPCInfo, recycleRI bool, err error)\n\tAppendErrMsgIfNeeded(ctx context.Context, err error, ri rpcinfo.RPCInfo, msg string)\n\n\t// Prepare to do something needed before retry call.\n\tPrepare(ctx context.Context, prevRI, retryRI rpcinfo.RPCInfo)\n\tDump() map[string]interface{}\n\tType() Type\n}\n\n// NewRetryContainerWithCB build Container that doesn't do circuit breaker statistic but get statistic result.\n// Which is used in case that circuit breaker is enabled.\n// eg:\n//\n//\t   cbs := circuitbreak.NewCBSuite(circuitbreak.RPCInfo2Key)\n//\t   retryC := retry.NewRetryContainerWithCB(cbs.ServiceControl(), cbs.ServicePanel())\n//\t\t  var opts []client.Option\n//\t\t  opts = append(opts, client.WithRetryContainer(retryC))\n//\t   // enable service circuit breaker\n//\t\t  opts = append(opts, client.WithMiddleware(cbs.ServiceCBMW()))\nfunc NewRetryContainerWithCB(cc *circuitbreak.Control, cp circuitbreaker.Panel) *Container {\n\treturn NewRetryContainer(WithContainerCBControl(cc), WithContainerCBPanel(cp))\n}\n\nfunc newCBSuite(opts []circuitbreak.CBSuiteOption) *circuitbreak.CBSuite {\n\treturn circuitbreak.NewCBSuite(circuitbreak.RPCInfo2Key, opts...)\n}\n\n// NewRetryContainerWithCBStat build Container that need to do circuit breaker statistic.\n// Which is used in case that the service CB key is customized.\n// eg:\n//\n//\tcbs := circuitbreak.NewCBSuite(YourGenServiceCBKeyFunc)\n//\tretry.NewRetryContainerWithCBStat(cbs.ServiceControl(), cbs.ServicePanel())\nfunc NewRetryContainerWithCBStat(cc *circuitbreak.Control, cp circuitbreaker.Panel) *Container {\n\treturn NewRetryContainer(WithContainerCBControl(cc), WithContainerCBPanel(cp), WithContainerCBStat())\n}\n\n// NewRetryContainerWithPercentageLimit build a Container to limiting the percentage of retry requests;\n// This is the RECOMMENDED initializer if you want to control PRECISELY the percentage of retry requests.\nfunc NewRetryContainerWithPercentageLimit() *Container {\n\treturn NewRetryContainer(WithContainerEnablePercentageLimit())\n}\n\n// ContainerOption is used when initializing a Container\ntype ContainerOption func(rc *Container)\n\n// WithContainerCBSuite specifies the CBSuite used in the retry circuitbreak\n// retryer will use its ServiceControl and ServicePanel\n// Its priority is lower than WithContainerCBControl and WithContainerCBPanel\nfunc WithContainerCBSuite(cbs *circuitbreak.CBSuite) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbSuite = cbs\n\t}\n}\n\n// WithCustomizeKeyFunc specifies the GenRetryKeyFunc to customize retry key\nfunc WithCustomizeKeyFunc(fn GenRetryKeyFunc) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.genRetryKey = fn\n\t}\n}\n\n// WithContainerCBSuiteOptions specifies the circuitbreak.CBSuiteOption for initializing circuitbreak.CBSuite\nfunc WithContainerCBSuiteOptions(opts ...circuitbreak.CBSuiteOption) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbSuiteOptions = opts\n\t}\n}\n\n// WithContainerCBControl specifies the circuitbreak.Control used in the retry circuitbreaker\n// It's user's responsibility to make sure it's paired with panel\nfunc WithContainerCBControl(ctrl *circuitbreak.Control) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbCtl = ctrl\n\t}\n}\n\n// WithContainerCBPanel specifies the circuitbreaker.Panel used in the retry circuitbreaker\n// It's user's responsibility to make sure it's paired with control\nfunc WithContainerCBPanel(panel circuitbreaker.Panel) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbPanel = panel\n\t}\n}\n\n// WithContainerCBStat instructs the circuitbreak.RecordStat is called within the retryer\nfunc WithContainerCBStat() ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbStat = true\n\t}\n}\n\n// WithContainerEnablePercentageLimit should be called for limiting the percentage of retry requests\nfunc WithContainerEnablePercentageLimit() ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.enablePercentageLimit = true\n\t}\n}\n\n// NewRetryContainer build Container that need to build circuit breaker and do circuit breaker statistic.\n// The caller is responsible for calling Container.Close() to release resources referenced.\nfunc NewRetryContainer(opts ...ContainerOption) *Container {\n\trc := &Container{\n\t\tcbContainer: &cbContainer{\n\t\t\tcbSuite: nil,\n\t\t},\n\t\tretryerMap: sync.Map{},\n\t}\n\tfor _, opt := range opts {\n\t\topt(rc)\n\t}\n\n\tif rc.cbContainer.enablePercentageLimit {\n\t\t// ignore cbSuite/cbCtl/cbPanel options\n\t\trc.cbContainer = &cbContainer{\n\t\t\tenablePercentageLimit: true,\n\t\t\tcbSuite:               newCBSuite(rc.cbContainer.cbSuiteOptions),\n\t\t\tcbSuiteOptions:        rc.cbContainer.cbSuiteOptions,\n\t\t}\n\t}\n\n\tcontainer := rc.cbContainer\n\tif container.cbCtl == nil && container.cbPanel == nil {\n\t\tif container.cbSuite == nil {\n\t\t\tcontainer.cbSuite = newCBSuite(rc.cbContainer.cbSuiteOptions)\n\t\t\tcontainer.cbStat = true\n\t\t}\n\t\tcontainer.cbCtl = container.cbSuite.ServiceControl()\n\t\tcontainer.cbPanel = container.cbSuite.ServicePanel()\n\t}\n\tif !container.IsValid() {\n\t\tpanic(\"KITEX: invalid container\")\n\t}\n\treturn rc\n}\n\nfunc defaultGenRetryKey(_ context.Context, rpcInfo rpcinfo.RPCInfo) string {\n\treturn rpcInfo.To().Method()\n}\n\n// Container is a wrapper for Retryer.\ntype Container struct {\n\thasCodeCfg  bool\n\tretryerMap  sync.Map // <method: retryer>\n\tcbContainer *cbContainer\n\tmsg         string\n\tsync.RWMutex\n\n\tgenRetryKey GenRetryKeyFunc\n\n\t// shouldResultRetry is only used with FailureRetry\n\tshouldResultRetry *ShouldResultRetry\n}\n\n// Recommended usage: NewRetryContainerWithPercentageLimit()\n// For more details, refer to the following comments for each field.\ntype cbContainer struct {\n\t// In NewRetryContainer, if cbCtrl & cbPanel are not set, Kitex will use cbSuite.ServiceControl() and\n\t// cbSuite.ServicePanel(); If cbSuite is nil, Kitex will create one.\n\tcbSuite *circuitbreak.CBSuite\n\n\t// It's more recommended to rely on the cbSuite than specifying cbCtl & cbPanel with corresponding options,\n\t// since cbCtl & cbPanel should be correctly paired, and with the cbSuite, Kitex will ensure it by using the\n\t// cbSuite.ServiceControl() and cbSuite.ServicePanel().\n\tcbCtl   *circuitbreak.Control\n\tcbPanel circuitbreaker.Panel\n\n\t// If cbStat && !enablePercentageLimit, retryer will call `circuitbreak.RecordStat` after rpcCall to record\n\t// rpc failures/timeouts, for cutting down on the retry requests when the error rate is beyond the threshold.\n\tcbStat bool\n\n\t// If enabled, Kitex will always create a cbSuite and use its cbCtl & cbPanel, and retryer will call\n\t// recordRetryStat before rpcCall, to precisely control the percentage of retry requests over all requests.\n\tenablePercentageLimit bool\n\n\t// for creating CBSuite inside NewRetryContainer\n\tcbSuiteOptions []circuitbreak.CBSuiteOption\n}\n\n// IsValid returns true when both cbCtl & cbPanel are not nil\n// It's the user's responsibility to guarantee that cbCtl & cbPanel are correctly paired.\nfunc (c *cbContainer) IsValid() bool {\n\treturn c.cbCtl != nil && c.cbPanel != nil\n}\n\n// InitWithPolicies to init Retryer with methodPolicies\n// Notice, InitWithPolicies is export func, the lock should be added inside\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// DeletePolicy to delete the method by method.\nfunc (rc *Container) DeletePolicy(key string) {\n\trc.Lock()\n\tdefer rc.Unlock()\n\trc.msg = \"\"\n\tif rc.hasCodeCfg {\n\t\t// the priority of user setup code policy is higher than remote config\n\t\treturn\n\t}\n\t_, ok := rc.retryerMap.Load(key)\n\tif ok {\n\t\trc.retryerMap.Delete(key)\n\t\trc.msg = fmt.Sprintf(\"delete retryer[%s] at %s\", key, time.Now())\n\t}\n}\n\n// NotifyPolicyChange to receive policy when it changes\nfunc (rc *Container) NotifyPolicyChange(key string, p Policy) {\n\trc.Lock()\n\tdefer rc.Unlock()\n\trc.msg = \"\"\n\tif rc.hasCodeCfg {\n\t\t// the priority of user setup code policy is higher than remote config\n\t\treturn\n\t}\n\tr, ok := rc.retryerMap.Load(key)\n\tif ok && r != nil {\n\t\tretryer, ok := r.(Retryer)\n\t\tif ok {\n\t\t\tif retryer.Type() == p.Type {\n\t\t\t\tretryer.UpdatePolicy(p)\n\t\t\t\trc.msg = fmt.Sprintf(\"update retryer[%s-%s] at %s\", key, retryer.Type(), time.Now())\n\t\t\t\treturn\n\t\t\t}\n\t\t\trc.retryerMap.Delete(key)\n\t\t\trc.msg = fmt.Sprintf(\"delete retryer[%s-%s] at %s\", key, retryer.Type(), time.Now())\n\t\t}\n\t}\n\trc.initRetryer(key, p)\n}\n\n// Init to build Retryer with code config.\nfunc (rc *Container) Init(mp map[string]Policy, rr *ShouldResultRetry) (err error) {\n\t// NotifyPolicyChange func may execute before Init func.\n\t// Because retry Container is built before Client init, NotifyPolicyChange can be triggered first\n\trc.updateRetryer(rr)\n\tif err = rc.InitWithPolicies(mp); err != nil {\n\t\treturn fmt.Errorf(\"NewRetryer in Init failed, err=%w\", err)\n\t}\n\treturn nil\n}\n\n// PrepareRetryContext adds necessary keys to context for retry\n// These keys should be added to `ctx` no matter whether there's a need to retry, to avoid sharing the same\n// object objects with another method call, since `ctx` might be reused in user-defined middlewares.\nfunc PrepareRetryContext(ctx context.Context) context.Context {\n\t// reqOp can be used to avoid multiple writes to the request object.\n\t// If a blocking write is needed, implement a lock based on it (spin-lock for example).\n\treqOp := OpNo\n\tctx = context.WithValue(ctx, CtxReqOp, &reqOp)\n\n\t// `respOp` is used to avoid concurrent write/read on the response object, especially for backup requests.\n\t// If `respOp` is modified by one request of this method call, all other requests will skip decoding.\n\trespOp := OpNo\n\tctx = context.WithValue(ctx, CtxRespOp, &respOp)\n\treturn ctx\n}\n\n// WithRetryIfNeeded to check if there is a retryer can be used and if current call can retry.\n// When the retry condition is satisfied, use retryer to call\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewRetryer build a retryer with policy\nfunc NewRetryer(p Policy, r *ShouldResultRetry, cbC *cbContainer) (retryer Retryer, err error) {\n\t// just one retry policy can be enabled at same time\n\tif p.Type == BackupType {\n\t\tretryer, err = newBackupRetryer(p, cbC)\n\t} else {\n\t\tretryer, err = newFailureRetryer(p, r, cbC)\n\t}\n\treturn\n}\n\nfunc (rc *Container) getRetryer(ctx context.Context, ri rpcinfo.RPCInfo) Retryer {\n\tkeyFunc := defaultGenRetryKey\n\tif rc.genRetryKey != nil {\n\t\tkeyFunc = rc.genRetryKey\n\t}\n\t// the priority of specific method is high\n\tr, ok := rc.retryerMap.Load(keyFunc(ctx, ri))\n\tif ok {\n\t\treturn r.(Retryer)\n\t}\n\tr, ok = rc.retryerMap.Load(Wildcard)\n\tif ok {\n\t\treturn r.(Retryer)\n\t}\n\treturn nil\n}\n\n// Dump is used to show current retry policy\nfunc (rc *Container) Dump() interface{} {\n\trc.RLock()\n\tdm := make(map[string]interface{})\n\tdm[\"has_code_cfg\"] = rc.hasCodeCfg\n\trc.retryerMap.Range(func(key, value interface{}) bool {\n\t\tif r, ok := value.(Retryer); ok {\n\t\t\tdm[key.(string)] = r.Dump()\n\t\t}\n\t\treturn true\n\t})\n\tif rc.msg != \"\" {\n\t\tdm[\"msg\"] = rc.msg\n\t}\n\trc.RUnlock()\n\treturn dm\n}\n\nfunc (rc *Container) initRetryer(method string, p Policy) error {\n\tretryer, err := NewRetryer(p, rc.shouldResultRetry, rc.cbContainer)\n\tif err != nil {\n\t\terrMsg := fmt.Sprintf(\"new retryer[%s-%s] failed, err=%s, at %s\", method, p.Type, err.Error(), time.Now())\n\t\trc.msg = errMsg\n\t\tklog.Warnf(errMsg)\n\t\treturn err\n\t}\n\n\trc.retryerMap.Store(method, retryer)\n\tif p.Enable {\n\t\trc.msg = fmt.Sprintf(\"new retryer[%s-%s] at %s\", method, retryer.Type(), time.Now())\n\t} else {\n\t\trc.msg = fmt.Sprintf(\"disable retryer[%s-%s](enable=%t) %s\", method, p.Type, p.Enable, time.Now())\n\t}\n\treturn nil\n}\n\nfunc (rc *Container) updateRetryer(rr *ShouldResultRetry) {\n\trc.Lock()\n\tdefer rc.Unlock()\n\n\trc.shouldResultRetry = rr\n\tif rc.shouldResultRetry != nil {\n\t\trc.retryerMap.Range(func(key, value interface{}) bool {\n\t\t\tif fr, ok := value.(*failureRetryer); ok {\n\t\t\t\tfr.setSpecifiedResultRetryIfNeeded(rc.shouldResultRetry)\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t}\n}\n\n// Close releases all possible resources referenced.\nfunc (rc *Container) Close() (err error) {\n\tif rc.cbContainer != nil && rc.cbContainer.cbSuite != nil {\n\t\terr = rc.cbContainer.cbSuite.Close()\n\t}\n\treturn\n}\n\n```\n# Function Signature: func (rc *Container) InitWithPolicies(methodPolicies map[string]Policy) error {\n# Natural Language Annotations: // InitWithPolicies to init Retryer with methodPolicies\n// Notice, InitWithPolicies is export func, the lock should be added inside\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (rc *Container) InitWithPolicies(methodPolicies map[string]Policy) error {\n\trc.Lock()\n\tdefer rc.Unlock()\n\n\tfor method, policy := range methodPolicies {\n\t\tretryer, ok := rc.retryerMap.Load(method)\n\t\tif ok && retryer.(Retryer).Type() == policy.Type {\n\t\t\tretryer.(Retryer).UpdatePolicy(policy)\n\t\t} else {\n\t\t\terr := rc.initRetryer(method, policy)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n```\nThis function initializes the retryer with the given method policies. It locks the container to prevent concurrent access, and defers the unlock until the function returns. It iterates over the methodPolicies map, and for each method and policy, it either updates the existing retryer with the new policy, or initializes a new retryer if the method is not already in the retryerMap. If there's an error initializing a new retryer, it returns the error.", "```go\nfunc (rc *Container) InitWithPolicies(methodPolicies map[string]Policy) error {\n\trc.Lock()\n\tdefer rc.Unlock()\n\n\tfor method, policy := range methodPolicies {\n\t\trc.initRetryer(method, policy)\n\t}\n\treturn nil\n}\n```", "```go\nfunc (rc *Container) InitWithPolicies(methodPolicies map[string]Policy) error {\n\trc.Lock()\n\tdefer rc.Unlock()\n\n\tfor method, policy := range methodPolicies {\n\t\tif re, ok := rc.retryerMap.Load(method); ok {\n\t\t\tif p, ok := re.(Retryer); ok {\n\t\t\t\tp.UpdatePolicy(policy)\n\t\t\t}\n\t\t} else {\n\t\t\tif err := rc.initRetryer(method, policy); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n```"]}, "kitex-pkg/retry/retryer.go-WithRetryIfNeeded": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/retry/retryer.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package retry implements rpc retry\npackage retry\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/bytedance/gopkg/cloud/circuitbreaker\"\n\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\n// RPCCallFunc is the definition with wrap rpc call\ntype RPCCallFunc func(context.Context, Retryer) (rpcinfo rpcinfo.RPCInfo, resp interface{}, err error)\n\n// GenRetryKeyFunc to generate retry key through rpcinfo.\n// You can customize the config key according to your config center.\ntype GenRetryKeyFunc func(ctx context.Context, ri rpcinfo.RPCInfo) string\n\n// Retryer is the interface for Retry implements\ntype Retryer interface {\n\t// AllowRetry to check if current request satisfy retry condition[eg: circuit, retry times == 0, chain stop, ddl].\n\t// If not satisfy won't execute Retryer.Do and return the reason message\n\t// Execute anyway for the first time regardless of able to retry.\n\tAllowRetry(ctx context.Context) (msg string, ok bool)\n\n\t// ShouldRetry to check if retry request can be called, it is checked in retryer.Do.\n\t// If not satisfy will return the reason message\n\tShouldRetry(ctx context.Context, err error, callTimes int, req interface{}, cbKey string) (msg string, ok bool)\n\tUpdatePolicy(policy Policy) error\n\n\t// Retry policy execute func. recycleRI is to decide if the firstRI can be recycled.\n\tDo(ctx context.Context, rpcCall RPCCallFunc, firstRI rpcinfo.RPCInfo, request interface{}) (lastRI rpcinfo.RPCInfo, recycleRI bool, err error)\n\tAppendErrMsgIfNeeded(ctx context.Context, err error, ri rpcinfo.RPCInfo, msg string)\n\n\t// Prepare to do something needed before retry call.\n\tPrepare(ctx context.Context, prevRI, retryRI rpcinfo.RPCInfo)\n\tDump() map[string]interface{}\n\tType() Type\n}\n\n// NewRetryContainerWithCB build Container that doesn't do circuit breaker statistic but get statistic result.\n// Which is used in case that circuit breaker is enabled.\n// eg:\n//\n//\t   cbs := circuitbreak.NewCBSuite(circuitbreak.RPCInfo2Key)\n//\t   retryC := retry.NewRetryContainerWithCB(cbs.ServiceControl(), cbs.ServicePanel())\n//\t\t  var opts []client.Option\n//\t\t  opts = append(opts, client.WithRetryContainer(retryC))\n//\t   // enable service circuit breaker\n//\t\t  opts = append(opts, client.WithMiddleware(cbs.ServiceCBMW()))\nfunc NewRetryContainerWithCB(cc *circuitbreak.Control, cp circuitbreaker.Panel) *Container {\n\treturn NewRetryContainer(WithContainerCBControl(cc), WithContainerCBPanel(cp))\n}\n\nfunc newCBSuite(opts []circuitbreak.CBSuiteOption) *circuitbreak.CBSuite {\n\treturn circuitbreak.NewCBSuite(circuitbreak.RPCInfo2Key, opts...)\n}\n\n// NewRetryContainerWithCBStat build Container that need to do circuit breaker statistic.\n// Which is used in case that the service CB key is customized.\n// eg:\n//\n//\tcbs := circuitbreak.NewCBSuite(YourGenServiceCBKeyFunc)\n//\tretry.NewRetryContainerWithCBStat(cbs.ServiceControl(), cbs.ServicePanel())\nfunc NewRetryContainerWithCBStat(cc *circuitbreak.Control, cp circuitbreaker.Panel) *Container {\n\treturn NewRetryContainer(WithContainerCBControl(cc), WithContainerCBPanel(cp), WithContainerCBStat())\n}\n\n// NewRetryContainerWithPercentageLimit build a Container to limiting the percentage of retry requests;\n// This is the RECOMMENDED initializer if you want to control PRECISELY the percentage of retry requests.\nfunc NewRetryContainerWithPercentageLimit() *Container {\n\treturn NewRetryContainer(WithContainerEnablePercentageLimit())\n}\n\n// ContainerOption is used when initializing a Container\ntype ContainerOption func(rc *Container)\n\n// WithContainerCBSuite specifies the CBSuite used in the retry circuitbreak\n// retryer will use its ServiceControl and ServicePanel\n// Its priority is lower than WithContainerCBControl and WithContainerCBPanel\nfunc WithContainerCBSuite(cbs *circuitbreak.CBSuite) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbSuite = cbs\n\t}\n}\n\n// WithCustomizeKeyFunc specifies the GenRetryKeyFunc to customize retry key\nfunc WithCustomizeKeyFunc(fn GenRetryKeyFunc) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.genRetryKey = fn\n\t}\n}\n\n// WithContainerCBSuiteOptions specifies the circuitbreak.CBSuiteOption for initializing circuitbreak.CBSuite\nfunc WithContainerCBSuiteOptions(opts ...circuitbreak.CBSuiteOption) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbSuiteOptions = opts\n\t}\n}\n\n// WithContainerCBControl specifies the circuitbreak.Control used in the retry circuitbreaker\n// It's user's responsibility to make sure it's paired with panel\nfunc WithContainerCBControl(ctrl *circuitbreak.Control) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbCtl = ctrl\n\t}\n}\n\n// WithContainerCBPanel specifies the circuitbreaker.Panel used in the retry circuitbreaker\n// It's user's responsibility to make sure it's paired with control\nfunc WithContainerCBPanel(panel circuitbreaker.Panel) ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbPanel = panel\n\t}\n}\n\n// WithContainerCBStat instructs the circuitbreak.RecordStat is called within the retryer\nfunc WithContainerCBStat() ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.cbStat = true\n\t}\n}\n\n// WithContainerEnablePercentageLimit should be called for limiting the percentage of retry requests\nfunc WithContainerEnablePercentageLimit() ContainerOption {\n\treturn func(rc *Container) {\n\t\trc.cbContainer.enablePercentageLimit = true\n\t}\n}\n\n// NewRetryContainer build Container that need to build circuit breaker and do circuit breaker statistic.\n// The caller is responsible for calling Container.Close() to release resources referenced.\nfunc NewRetryContainer(opts ...ContainerOption) *Container {\n\trc := &Container{\n\t\tcbContainer: &cbContainer{\n\t\t\tcbSuite: nil,\n\t\t},\n\t\tretryerMap: sync.Map{},\n\t}\n\tfor _, opt := range opts {\n\t\topt(rc)\n\t}\n\n\tif rc.cbContainer.enablePercentageLimit {\n\t\t// ignore cbSuite/cbCtl/cbPanel options\n\t\trc.cbContainer = &cbContainer{\n\t\t\tenablePercentageLimit: true,\n\t\t\tcbSuite:               newCBSuite(rc.cbContainer.cbSuiteOptions),\n\t\t\tcbSuiteOptions:        rc.cbContainer.cbSuiteOptions,\n\t\t}\n\t}\n\n\tcontainer := rc.cbContainer\n\tif container.cbCtl == nil && container.cbPanel == nil {\n\t\tif container.cbSuite == nil {\n\t\t\tcontainer.cbSuite = newCBSuite(rc.cbContainer.cbSuiteOptions)\n\t\t\tcontainer.cbStat = true\n\t\t}\n\t\tcontainer.cbCtl = container.cbSuite.ServiceControl()\n\t\tcontainer.cbPanel = container.cbSuite.ServicePanel()\n\t}\n\tif !container.IsValid() {\n\t\tpanic(\"KITEX: invalid container\")\n\t}\n\treturn rc\n}\n\nfunc defaultGenRetryKey(_ context.Context, rpcInfo rpcinfo.RPCInfo) string {\n\treturn rpcInfo.To().Method()\n}\n\n// Container is a wrapper for Retryer.\ntype Container struct {\n\thasCodeCfg  bool\n\tretryerMap  sync.Map // <method: retryer>\n\tcbContainer *cbContainer\n\tmsg         string\n\tsync.RWMutex\n\n\tgenRetryKey GenRetryKeyFunc\n\n\t// shouldResultRetry is only used with FailureRetry\n\tshouldResultRetry *ShouldResultRetry\n}\n\n// Recommended usage: NewRetryContainerWithPercentageLimit()\n// For more details, refer to the following comments for each field.\ntype cbContainer struct {\n\t// In NewRetryContainer, if cbCtrl & cbPanel are not set, Kitex will use cbSuite.ServiceControl() and\n\t// cbSuite.ServicePanel(); If cbSuite is nil, Kitex will create one.\n\tcbSuite *circuitbreak.CBSuite\n\n\t// It's more recommended to rely on the cbSuite than specifying cbCtl & cbPanel with corresponding options,\n\t// since cbCtl & cbPanel should be correctly paired, and with the cbSuite, Kitex will ensure it by using the\n\t// cbSuite.ServiceControl() and cbSuite.ServicePanel().\n\tcbCtl   *circuitbreak.Control\n\tcbPanel circuitbreaker.Panel\n\n\t// If cbStat && !enablePercentageLimit, retryer will call `circuitbreak.RecordStat` after rpcCall to record\n\t// rpc failures/timeouts, for cutting down on the retry requests when the error rate is beyond the threshold.\n\tcbStat bool\n\n\t// If enabled, Kitex will always create a cbSuite and use its cbCtl & cbPanel, and retryer will call\n\t// recordRetryStat before rpcCall, to precisely control the percentage of retry requests over all requests.\n\tenablePercentageLimit bool\n\n\t// for creating CBSuite inside NewRetryContainer\n\tcbSuiteOptions []circuitbreak.CBSuiteOption\n}\n\n// IsValid returns true when both cbCtl & cbPanel are not nil\n// It's the user's responsibility to guarantee that cbCtl & cbPanel are correctly paired.\nfunc (c *cbContainer) IsValid() bool {\n\treturn c.cbCtl != nil && c.cbPanel != nil\n}\n\n// InitWithPolicies to init Retryer with methodPolicies\n// Notice, InitWithPolicies is export func, the lock should be added inside\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// DeletePolicy to delete the method by method.\nfunc (rc *Container) DeletePolicy(key string) {\n\trc.Lock()\n\tdefer rc.Unlock()\n\trc.msg = \"\"\n\tif rc.hasCodeCfg {\n\t\t// the priority of user setup code policy is higher than remote config\n\t\treturn\n\t}\n\t_, ok := rc.retryerMap.Load(key)\n\tif ok {\n\t\trc.retryerMap.Delete(key)\n\t\trc.msg = fmt.Sprintf(\"delete retryer[%s] at %s\", key, time.Now())\n\t}\n}\n\n// NotifyPolicyChange to receive policy when it changes\nfunc (rc *Container) NotifyPolicyChange(key string, p Policy) {\n\trc.Lock()\n\tdefer rc.Unlock()\n\trc.msg = \"\"\n\tif rc.hasCodeCfg {\n\t\t// the priority of user setup code policy is higher than remote config\n\t\treturn\n\t}\n\tr, ok := rc.retryerMap.Load(key)\n\tif ok && r != nil {\n\t\tretryer, ok := r.(Retryer)\n\t\tif ok {\n\t\t\tif retryer.Type() == p.Type {\n\t\t\t\tretryer.UpdatePolicy(p)\n\t\t\t\trc.msg = fmt.Sprintf(\"update retryer[%s-%s] at %s\", key, retryer.Type(), time.Now())\n\t\t\t\treturn\n\t\t\t}\n\t\t\trc.retryerMap.Delete(key)\n\t\t\trc.msg = fmt.Sprintf(\"delete retryer[%s-%s] at %s\", key, retryer.Type(), time.Now())\n\t\t}\n\t}\n\trc.initRetryer(key, p)\n}\n\n// Init to build Retryer with code config.\nfunc (rc *Container) Init(mp map[string]Policy, rr *ShouldResultRetry) (err error) {\n\t// NotifyPolicyChange func may execute before Init func.\n\t// Because retry Container is built before Client init, NotifyPolicyChange can be triggered first\n\trc.updateRetryer(rr)\n\tif err = rc.InitWithPolicies(mp); err != nil {\n\t\treturn fmt.Errorf(\"NewRetryer in Init failed, err=%w\", err)\n\t}\n\treturn nil\n}\n\n// PrepareRetryContext adds necessary keys to context for retry\n// These keys should be added to `ctx` no matter whether there's a need to retry, to avoid sharing the same\n// object objects with another method call, since `ctx` might be reused in user-defined middlewares.\nfunc PrepareRetryContext(ctx context.Context) context.Context {\n\t// reqOp can be used to avoid multiple writes to the request object.\n\t// If a blocking write is needed, implement a lock based on it (spin-lock for example).\n\treqOp := OpNo\n\tctx = context.WithValue(ctx, CtxReqOp, &reqOp)\n\n\t// `respOp` is used to avoid concurrent write/read on the response object, especially for backup requests.\n\t// If `respOp` is modified by one request of this method call, all other requests will skip decoding.\n\trespOp := OpNo\n\tctx = context.WithValue(ctx, CtxRespOp, &respOp)\n\treturn ctx\n}\n\n// WithRetryIfNeeded to check if there is a retryer can be used and if current call can retry.\n// When the retry condition is satisfied, use retryer to call\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewRetryer build a retryer with policy\nfunc NewRetryer(p Policy, r *ShouldResultRetry, cbC *cbContainer) (retryer Retryer, err error) {\n\t// just one retry policy can be enabled at same time\n\tif p.Type == BackupType {\n\t\tretryer, err = newBackupRetryer(p, cbC)\n\t} else {\n\t\tretryer, err = newFailureRetryer(p, r, cbC)\n\t}\n\treturn\n}\n\nfunc (rc *Container) getRetryer(ctx context.Context, ri rpcinfo.RPCInfo) Retryer {\n\tkeyFunc := defaultGenRetryKey\n\tif rc.genRetryKey != nil {\n\t\tkeyFunc = rc.genRetryKey\n\t}\n\t// the priority of specific method is high\n\tr, ok := rc.retryerMap.Load(keyFunc(ctx, ri))\n\tif ok {\n\t\treturn r.(Retryer)\n\t}\n\tr, ok = rc.retryerMap.Load(Wildcard)\n\tif ok {\n\t\treturn r.(Retryer)\n\t}\n\treturn nil\n}\n\n// Dump is used to show current retry policy\nfunc (rc *Container) Dump() interface{} {\n\trc.RLock()\n\tdm := make(map[string]interface{})\n\tdm[\"has_code_cfg\"] = rc.hasCodeCfg\n\trc.retryerMap.Range(func(key, value interface{}) bool {\n\t\tif r, ok := value.(Retryer); ok {\n\t\t\tdm[key.(string)] = r.Dump()\n\t\t}\n\t\treturn true\n\t})\n\tif rc.msg != \"\" {\n\t\tdm[\"msg\"] = rc.msg\n\t}\n\trc.RUnlock()\n\treturn dm\n}\n\nfunc (rc *Container) initRetryer(method string, p Policy) error {\n\tretryer, err := NewRetryer(p, rc.shouldResultRetry, rc.cbContainer)\n\tif err != nil {\n\t\terrMsg := fmt.Sprintf(\"new retryer[%s-%s] failed, err=%s, at %s\", method, p.Type, err.Error(), time.Now())\n\t\trc.msg = errMsg\n\t\tklog.Warnf(errMsg)\n\t\treturn err\n\t}\n\n\trc.retryerMap.Store(method, retryer)\n\tif p.Enable {\n\t\trc.msg = fmt.Sprintf(\"new retryer[%s-%s] at %s\", method, retryer.Type(), time.Now())\n\t} else {\n\t\trc.msg = fmt.Sprintf(\"disable retryer[%s-%s](enable=%t) %s\", method, p.Type, p.Enable, time.Now())\n\t}\n\treturn nil\n}\n\nfunc (rc *Container) updateRetryer(rr *ShouldResultRetry) {\n\trc.Lock()\n\tdefer rc.Unlock()\n\n\trc.shouldResultRetry = rr\n\tif rc.shouldResultRetry != nil {\n\t\trc.retryerMap.Range(func(key, value interface{}) bool {\n\t\t\tif fr, ok := value.(*failureRetryer); ok {\n\t\t\t\tfr.setSpecifiedResultRetryIfNeeded(rc.shouldResultRetry)\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t}\n}\n\n// Close releases all possible resources referenced.\nfunc (rc *Container) Close() (err error) {\n\tif rc.cbContainer != nil && rc.cbContainer.cbSuite != nil {\n\t\terr = rc.cbContainer.cbSuite.Close()\n\t}\n\treturn\n}\n\n```\n# Function Signature: func (rc *Container) WithRetryIfNeeded(ctx context.Context, callOptRetry *Policy, rpcCall RPCCallFunc, ri rpcinfo.RPCInfo, request interface{}) (lastRI rpcinfo.RPCInfo, recycleRI bool, err error) {\n# Natural Language Annotations: // WithRetryIfNeeded to check if there is a retryer can be used and if current call can retry.\n// When the retry condition is satisfied, use retryer to call\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (rc *Container) WithRetryIfNeeded(ctx context.Context, callOptRetry *Policy, rpcCall RPCCallFunc, ri rpcinfo.RPCInfo, request interface{}) (lastRI rpcinfo.RPCInfo, recycleRI bool, err error) {\n\tretryer := rc.getRetryer(ctx, ri)\n\tif retryer == nil {\n\t\tif callOptRetry != nil {\n\t\t\tretryer, err = NewRetryer(*callOptRetry, rc.shouldResultRetry, rc.cbContainer)\n\t\t\tif err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\trc.retryerMap.Store(defaultGenRetryKey(ctx, ri), retryer)\n\t\t} else {\n\t\t\treturn\n\t\t}\n\t}\n\tmsg, ok := retryer.AllowRetry(ctx)\n\tif !ok {\n\t\terr = fmt.Errorf(\"retry not allowed: %s\", msg)\n\t\treturn\n\t}\n\tlastRI, recycleRI, err = retryer.Do(ctx, rpcCall, ri, request)\n\treturn\n}\n```", "Sure, here is the generated code based on the function signature and annotations provided:\n\n```go\nfunc (rc *Container) WithRetryIfNeeded(ctx context.Context, callOptRetry *Policy, rpcCall RPCCallFunc, ri rpcinfo.RPCInfo, request interface{}) (lastRI rpcinfo.RPCInfo, recycleRI bool, err error) {\n\tretryer := rc.getRetryer(ctx, ri)\n\tif retryer != nil {\n\t\tmsg, ok := retryer.AllowRetry(ctx)\n\t\tif !ok {\n\t\t\treturn ri, false, fmt.Errorf(\"retry not allowed: %s\", msg)\n\t\t}\n\n\t\tmsg, ok = retryer.ShouldRetry(ctx, nil, 0, nil, \"\")\n\t\tif !ok {\n\t\t\treturn ri, false, fmt.Errorf(\"retry not needed: %s\", msg)\n\t\t}\n\n\t\tlastRI, recycleRI, err = retryer.Do(ctx, rpcCall, ri, request)\n\t\tif err != nil {\n\t\t\terrMsg := fmt.Sprintf(\"retry failed: %s\", err.Error())\n\t\t\trc.AppendErrMsgIfNeeded(ctx, err, ri, errMsg)\n\t\t}\n\t} else {\n\t\tlastRI, recycleRI, err = rpcCall(ctx, nil)\n\t\tif err != nil {\n\t\t\terrMsg := fmt.Sprintf(\"call failed: %s\", err.Error())\n\t\t\trc.AppendErrMsgIfNeeded(ctx, err, ri, errMsg)\n\t\t}\n\t}\n\treturn lastRI, recycleRI, err\n}\n```\n\nThis function first checks if there is a retryer for the given RPCInfo. If there is, it checks if the retry is allowed and if it should be retried. If the retry is allowed and necessary, it calls the `Do` method of the retryer. If there is no retryer or if the retry is not necessary, it calls the `rpcCall` function directly. The function returns the last RPCInfo, a boolean indicating whether the RPCInfo should be recycled, and an error.", "```go\nfunc (rc *Container) WithRetryIfNeeded(ctx context.Context, callOptRetry *Policy, rpcCall RPCCallFunc, ri rpcinfo.RPCInfo, request interface{}) (lastRI rpcinfo.RPCInfo, recycleRI bool, err error) {\n\tretryer := rc.getRetryer(ctx, ri)\n\tif retryer != nil {\n\t\tmsg, ok := retryer.AllowRetry(ctx)\n\t\tif !ok {\n\t\t\terr = fmt.Errorf(\"retry not allowed: %s\", msg)\n\t\t\treturn\n\t\t}\n\t\tlastRI, recycleRI, err = retryer.Do(ctx, rpcCall, ri, request)\n\t\tif err != nil {\n\t\t\tretryer.AppendErrMsgIfNeeded(ctx, err, ri, msg)\n\t\t}\n\t} else {\n\t\tlastRI, recycleRI, err = rpcCall(ctx, nil)\n\t}\n\treturn\n}\n```"]}, "kitex-pkg/discovery/discovery.go-DefaultDiff": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/discovery/discovery.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package discovery defines interfaces for service discovery.\n// Developers that are willing to customize service discovery\n// should implement their own Resolver and supply it with the\n// option WithResolver at client's creation.\npackage discovery\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// DefaultWeight is the default weight for an instance.\nconst DefaultWeight = 10\n\n// Result contains the result of service discovery process.\n// Cacheable tells whether the instance list can/should be cached.\n// When Cacheable is true, CacheKey can be used to map the instance list in cache.\ntype Result struct {\n\tCacheable bool\n\tCacheKey  string\n\tInstances []Instance\n}\n\n// Change contains the difference between the current discovery result and the previous one.\n// It is designed for providing detail information when dispatching an event for service\n// discovery result change.\n// Since the loadbalancer may rely on caching the result of resolver to improve performance,\n// the resolver implementation should dispatch an event when result changes.\ntype Change struct {\n\tResult  Result\n\tAdded   []Instance\n\tUpdated []Instance\n\tRemoved []Instance\n}\n\n// Resolver resolves the target endpoint into a list of Instance.\ntype Resolver interface {\n\t// Target should return a description for the given target that is suitable for being a key for cache.\n\tTarget(ctx context.Context, target rpcinfo.EndpointInfo) (description string)\n\n\t// Resolve returns a list of instances for the given description of a target.\n\tResolve(ctx context.Context, desc string) (Result, error)\n\n\t// Diff computes the difference between two results.\n\t// When `next` is cacheable, the Change should be cacheable, too. And the `Result` field's CacheKey in\n\t// the return value should be set with the given cacheKey.\n\tDiff(cacheKey string, prev, next Result) (Change, bool)\n\n\t// Name returns the name of the resolver.\n\tName() string\n}\n\n// DefaultDiff provides a natural implementation for the Diff method of the Resolver interface.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype instance struct {\n\taddr   net.Addr\n\tweight int\n\ttags   map[string]string\n}\n\nfunc (i *instance) Address() net.Addr {\n\treturn i.addr\n}\n\nfunc (i *instance) Weight() int {\n\treturn i.weight\n}\n\nfunc (i *instance) Tag(key string) (value string, exist bool) {\n\tvalue, exist = i.tags[key]\n\treturn\n}\n\n// NewInstance creates a Instance using the given network, address and tags\nfunc NewInstance(network, address string, weight int, tags map[string]string) Instance {\n\treturn &instance{\n\t\taddr:   utils.NewNetAddr(network, address),\n\t\tweight: weight,\n\t\ttags:   tags,\n\t}\n}\n\n// SynthesizedResolver synthesizes a Resolver using a resolve function.\ntype SynthesizedResolver struct {\n\tTargetFunc  func(ctx context.Context, target rpcinfo.EndpointInfo) string\n\tResolveFunc func(ctx context.Context, key string) (Result, error)\n\tDiffFunc    func(key string, prev, next Result) (Change, bool)\n\tNameFunc    func() string\n}\n\n// Target implements the Resolver interface.\nfunc (sr SynthesizedResolver) Target(ctx context.Context, target rpcinfo.EndpointInfo) string {\n\tif sr.TargetFunc == nil {\n\t\treturn \"\"\n\t}\n\treturn sr.TargetFunc(ctx, target)\n}\n\n// Resolve implements the Resolver interface.\nfunc (sr SynthesizedResolver) Resolve(ctx context.Context, key string) (Result, error) {\n\treturn sr.ResolveFunc(ctx, key)\n}\n\n// Diff implements the Resolver interface.\nfunc (sr SynthesizedResolver) Diff(key string, prev, next Result) (Change, bool) {\n\tif sr.DiffFunc == nil {\n\t\treturn DefaultDiff(key, prev, next)\n\t}\n\treturn sr.DiffFunc(key, prev, next)\n}\n\n// Name implements the Resolver interface\nfunc (sr SynthesizedResolver) Name() string {\n\tif sr.NameFunc == nil {\n\t\treturn \"\"\n\t}\n\treturn sr.NameFunc()\n}\n\n// Instance contains information of an instance from the target service.\ntype Instance interface {\n\tAddress() net.Addr\n\tWeight() int\n\tTag(key string) (value string, exist bool)\n}\n\n```\n# Function Signature: func DefaultDiff(cacheKey string, prev, next Result) (Change, bool) {\n# Natural Language Annotations: // DefaultDiff provides a natural implementation for the Diff method of the Resolver interface.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc DefaultDiff(cacheKey string, prev, next Result) (Change, bool) {\n\tadded := []Instance{}\n\tupdated := []Instance{}\n\tremoved := []Instance{}\n\n\tprevMap := make(map[string]Instance)\n\tfor _, inst := range prev.Instances {\n\t\tprevMap[inst.Address().String()] = inst\n\t}\n\n\tnextMap := make(map[string]Instance)\n\tfor _, inst := range next.Instances {\n\t\tnextMap[inst.Address().String()] = inst\n\t}\n\n\tfor k, v := range prevMap {\n\t\tif _, exist := nextMap[k]; !exist {\n\t\t\tremoved = append(removed, v)\n\t\t}\n\t}\n\n\tfor k, v := range nextMap {\n\t\tif _, exist := prevMap[k]; !exist {\n\t\t\tadded = append(added, v)\n\t\t} else {\n\t\t\tupdated = append(updated, v)\n\t\t}\n\t}\n\n\treturn Change{\n\t\tResult: Result{\n\t\t\tCacheable: true,\n\t\t\tCacheKey:  cacheKey,\n\t\t\tInstances: next.Instances,\n\t\t},\n\t\tAdded:   added,\n\t\tUpdated: updated,\n\t\tRemoved: removed,\n\t}, true\n}\n```", "```go\nfunc DefaultDiff(cacheKey string, prev, next Result) (Change, bool) {\n\tadded := make([]Instance, 0)\n\tupdated := make([]Instance, 0)\n\tremoved := make([]Instance, 0)\n\n\tprevMap := make(map[string]Instance)\n\tfor _, ins := range prev.Instances {\n\t\tprevMap[ins.Address().String()] = ins\n\t}\n\n\tnextMap := make(map[string]Instance)\n\tfor _, ins := range next.Instances {\n\t\tnextMap[ins.Address().String()] = ins\n\t}\n\n\tfor k, v := range prevMap {\n\t\tif _, ok := nextMap[k]; !ok {\n\t\t\tremoved = append(removed, v)\n\t\t}\n\t}\n\n\tfor k, v := range nextMap {\n\t\tif _, ok := prevMap[k]; !ok {\n\t\t\tadded = append(added, v)\n\t\t} else {\n\t\t\t// check if the instance has been updated\n\t\t\tif prevMap[k].Weight() != v.Weight() {\n\t\t\t\tupdated = append(updated, v)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn Change{\n\t\tResult: Result{\n\t\t\tCacheable: next.Cacheable,\n\t\t\tCacheKey:  cacheKey,\n\t\t\tInstances: next.Instances,\n\t\t},\n\t\tAdded:   added,\n\t\tUpdated: updated,\n\t\tRemoved: removed,\n\t}, true\n}\n```", "```go\nfunc DefaultDiff(cacheKey string, prev, next Result) (Change, bool) {\n\tchange := Change{Result: next}\n\n\t// Find added instances\n\tfor _, nextInstance := range next.Instances {\n\t\tfound := false\n\t\tfor _, prevInstance := range prev.Instances {\n\t\t\tif nextInstance.Address().String() == prevInstance.Address().String() {\n\t\t\t\tfound = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !found {\n\t\t\tchange.Added = append(change.Added, nextInstance)\n\t\t}\n\t}\n\n\t// Find removed instances\n\tfor _, prevInstance := range prev.Instances {\n\t\tfound := false\n\t\tfor _, nextInstance := range next.Instances {\n\t\t\tif prevInstance.Address().String() == nextInstance.Address().String() {\n\t\t\t\tfound = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !found {\n\t\t\tchange.Removed = append(change.Removed, prevInstance)\n\t\t}\n\t}\n\n\t// Find updated instances\n\tfor _, nextInstance := range next.Instances {\n\t\tfor _, prevInstance := range prev.Instances {\n\t\t\tif nextInstance.Address().String() == prevInstance.Address().String() &&\n\t\t\t\t(nextInstance.Weight() != prevInstance.Weight() ||\n\t\t\t\t\tnextInstance.Tag(\"\") != prevInstance.Tag(\"\")) {\n\t\t\t\tchange.Updated = append(change.Updated, nextInstance)\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\tchange.Result.CacheKey = cacheKey\n\treturn change, change.Result.Cacheable\n}\n```"]}, "kitex-pkg/protocol/bthrift/binary.go-SetSpanCache": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/protocol/bthrift/binary.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package bthrift .\npackage bthrift\n\nimport (\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\n\t\"github.com/cloudwego/kitex/pkg/mem\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\nvar (\n\t// Binary protocol for bthrift.\n\tBinary binaryProtocol\n\t_      BTProtocol = binaryProtocol{}\n)\n\nvar allocator Allocator\n\nconst binaryInplaceThreshold = 4096 // 4k\n\ntype binaryProtocol struct{}\n\n// SetSpanCache enable/disable binary protocol bytes/string allocator\n\n\n\n\n\n\n\n\n// SetAllocator set binary protocol bytes/string allocator.\nfunc SetAllocator(alloc Allocator) {\n\tallocator = alloc\n}\n\nfunc (binaryProtocol) WriteMessageBegin(buf []byte, name string, typeID thrift.TMessageType, seqid int32) int {\n\toffset := 0\n\tversion := uint32(thrift.VERSION_1) | uint32(typeID)\n\toffset += Binary.WriteI32(buf, int32(version))\n\toffset += Binary.WriteString(buf[offset:], name)\n\toffset += Binary.WriteI32(buf[offset:], seqid)\n\treturn offset\n}\n\nfunc (binaryProtocol) WriteMessageEnd(buf []byte) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteStructBegin(buf []byte, name string) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteStructEnd(buf []byte) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteFieldBegin(buf []byte, name string, typeID thrift.TType, id int16) int {\n\treturn Binary.WriteByte(buf, int8(typeID)) + Binary.WriteI16(buf[1:], id)\n}\n\nfunc (binaryProtocol) WriteFieldEnd(buf []byte) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteFieldStop(buf []byte) int {\n\treturn Binary.WriteByte(buf, thrift.STOP)\n}\n\nfunc (binaryProtocol) WriteMapBegin(buf []byte, keyType, valueType thrift.TType, size int) int {\n\treturn Binary.WriteByte(buf, int8(keyType)) +\n\t\tBinary.WriteByte(buf[1:], int8(valueType)) +\n\t\tBinary.WriteI32(buf[2:], int32(size))\n}\n\nfunc (binaryProtocol) WriteMapEnd(buf []byte) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteListBegin(buf []byte, elemType thrift.TType, size int) int {\n\treturn Binary.WriteByte(buf, int8(elemType)) +\n\t\tBinary.WriteI32(buf[1:], int32(size))\n}\n\nfunc (binaryProtocol) WriteListEnd(buf []byte) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteSetBegin(buf []byte, elemType thrift.TType, size int) int {\n\treturn Binary.WriteByte(buf, int8(elemType)) +\n\t\tBinary.WriteI32(buf[1:], int32(size))\n}\n\nfunc (binaryProtocol) WriteSetEnd(buf []byte) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) WriteBool(buf []byte, value bool) int {\n\tif value {\n\t\treturn Binary.WriteByte(buf, 1)\n\t}\n\treturn Binary.WriteByte(buf, 0)\n}\n\nfunc (binaryProtocol) WriteByte(buf []byte, value int8) int {\n\tbuf[0] = byte(value)\n\treturn 1\n}\n\nfunc (binaryProtocol) WriteI16(buf []byte, value int16) int {\n\tbinary.BigEndian.PutUint16(buf, uint16(value))\n\treturn 2\n}\n\nfunc (binaryProtocol) WriteI32(buf []byte, value int32) int {\n\tbinary.BigEndian.PutUint32(buf, uint32(value))\n\treturn 4\n}\n\nfunc (binaryProtocol) WriteI64(buf []byte, value int64) int {\n\tbinary.BigEndian.PutUint64(buf, uint64(value))\n\treturn 8\n}\n\nfunc (binaryProtocol) WriteDouble(buf []byte, value float64) int {\n\treturn Binary.WriteI64(buf, int64(math.Float64bits(value)))\n}\n\nfunc (binaryProtocol) WriteString(buf []byte, value string) int {\n\tl := Binary.WriteI32(buf, int32(len(value)))\n\tcopy(buf[l:], value)\n\treturn l + len(value)\n}\n\nfunc (binaryProtocol) WriteBinary(buf, value []byte) int {\n\tl := Binary.WriteI32(buf, int32(len(value)))\n\tcopy(buf[l:], value)\n\treturn l + len(value)\n}\n\nfunc (binaryProtocol) WriteStringNocopy(buf []byte, binaryWriter BinaryWriter, value string) int {\n\treturn Binary.WriteBinaryNocopy(buf, binaryWriter, utils.StringToSliceByte(value))\n}\n\nfunc (binaryProtocol) WriteBinaryNocopy(buf []byte, binaryWriter BinaryWriter, value []byte) int {\n\tl := Binary.WriteI32(buf, int32(len(value)))\n\tif binaryWriter != nil && len(value) > binaryInplaceThreshold {\n\t\tbinaryWriter.WriteDirect(value, len(buf[l:]))\n\t\treturn l\n\t}\n\tcopy(buf[l:], value)\n\treturn l + len(value)\n}\n\nfunc (binaryProtocol) MessageBeginLength(name string, typeID thrift.TMessageType, seqid int32) int {\n\tversion := uint32(thrift.VERSION_1) | uint32(typeID)\n\treturn Binary.I32Length(int32(version)) + Binary.StringLength(name) + Binary.I32Length(seqid)\n}\n\nfunc (binaryProtocol) MessageEndLength() int {\n\treturn 0\n}\n\nfunc (binaryProtocol) StructBeginLength(name string) int {\n\treturn 0\n}\n\nfunc (binaryProtocol) StructEndLength() int {\n\treturn 0\n}\n\nfunc (binaryProtocol) FieldBeginLength(name string, typeID thrift.TType, id int16) int {\n\treturn Binary.ByteLength(int8(typeID)) + Binary.I16Length(id)\n}\n\nfunc (binaryProtocol) FieldEndLength() int {\n\treturn 0\n}\n\nfunc (binaryProtocol) FieldStopLength() int {\n\treturn Binary.ByteLength(thrift.STOP)\n}\n\nfunc (binaryProtocol) MapBeginLength(keyType, valueType thrift.TType, size int) int {\n\treturn Binary.ByteLength(int8(keyType)) +\n\t\tBinary.ByteLength(int8(valueType)) +\n\t\tBinary.I32Length(int32(size))\n}\n\nfunc (binaryProtocol) MapEndLength() int {\n\treturn 0\n}\n\nfunc (binaryProtocol) ListBeginLength(elemType thrift.TType, size int) int {\n\treturn Binary.ByteLength(int8(elemType)) +\n\t\tBinary.I32Length(int32(size))\n}\n\nfunc (binaryProtocol) ListEndLength() int {\n\treturn 0\n}\n\nfunc (binaryProtocol) SetBeginLength(elemType thrift.TType, size int) int {\n\treturn Binary.ByteLength(int8(elemType)) +\n\t\tBinary.I32Length(int32(size))\n}\n\nfunc (binaryProtocol) SetEndLength() int {\n\treturn 0\n}\n\nfunc (binaryProtocol) BoolLength(value bool) int {\n\tif value {\n\t\treturn Binary.ByteLength(1)\n\t}\n\treturn Binary.ByteLength(0)\n}\n\nfunc (binaryProtocol) ByteLength(value int8) int {\n\treturn 1\n}\n\nfunc (binaryProtocol) I16Length(value int16) int {\n\treturn 2\n}\n\nfunc (binaryProtocol) I32Length(value int32) int {\n\treturn 4\n}\n\nfunc (binaryProtocol) I64Length(value int64) int {\n\treturn 8\n}\n\nfunc (binaryProtocol) DoubleLength(value float64) int {\n\treturn Binary.I64Length(int64(math.Float64bits(value)))\n}\n\nfunc (binaryProtocol) StringLength(value string) int {\n\treturn Binary.I32Length(int32(len(value))) + len(value)\n}\n\nfunc (binaryProtocol) BinaryLength(value []byte) int {\n\treturn Binary.I32Length(int32(len(value))) + len(value)\n}\n\nfunc (binaryProtocol) StringLengthNocopy(value string) int {\n\treturn Binary.BinaryLengthNocopy(utils.StringToSliceByte(value))\n}\n\nfunc (binaryProtocol) BinaryLengthNocopy(value []byte) int {\n\tl := Binary.I32Length(int32(len(value)))\n\treturn l + len(value)\n}\n\nfunc (binaryProtocol) ReadMessageBegin(buf []byte) (name string, typeID thrift.TMessageType, seqid int32, length int, err error) {\n\tsize, l, e := Binary.ReadI32(buf)\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tif size > 0 {\n\t\terr = perrors.NewProtocolErrorWithType(perrors.BadVersion, \"Missing version in ReadMessageBegin\")\n\t\treturn\n\t}\n\ttypeID = thrift.TMessageType(size & 0x0ff)\n\tversion := int64(size) & thrift.VERSION_MASK\n\tif version != thrift.VERSION_1 {\n\t\terr = perrors.NewProtocolErrorWithType(perrors.BadVersion, \"Bad version in ReadMessageBegin\")\n\t\treturn\n\t}\n\tname, l, e = Binary.ReadString(buf[length:])\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tseqid, l, e = Binary.ReadI32(buf[length:])\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\treturn\n}\n\nfunc (binaryProtocol) ReadMessageEnd(buf []byte) (int, error) {\n\treturn 0, nil\n}\n\nfunc (binaryProtocol) ReadStructBegin(buf []byte) (name string, length int, err error) {\n\treturn\n}\n\nfunc (binaryProtocol) ReadStructEnd(buf []byte) (int, error) {\n\treturn 0, nil\n}\n\nfunc (binaryProtocol) ReadFieldBegin(buf []byte) (name string, typeID thrift.TType, id int16, length int, err error) {\n\tt, l, e := Binary.ReadByte(buf)\n\tlength += l\n\ttypeID = thrift.TType(t)\n\tif e != nil {\n\t\terr = e\n\t\treturn\n\t}\n\tif t != thrift.STOP {\n\t\tid, l, err = Binary.ReadI16(buf[length:])\n\t\tlength += l\n\t}\n\treturn\n}\n\nfunc (binaryProtocol) ReadFieldEnd(buf []byte) (int, error) {\n\treturn 0, nil\n}\n\nfunc (binaryProtocol) ReadMapBegin(buf []byte) (keyType, valueType thrift.TType, size, length int, err error) {\n\tk, l, e := Binary.ReadByte(buf)\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tkeyType = thrift.TType(k)\n\tv, l, e := Binary.ReadByte(buf[length:])\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tvalueType = thrift.TType(v)\n\tsize32, l, e := Binary.ReadI32(buf[length:])\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tif size32 < 0 {\n\t\terr = perrors.InvalidDataLength\n\t\treturn\n\t}\n\tsize = int(size32)\n\treturn\n}\n\nfunc (binaryProtocol) ReadMapEnd(buf []byte) (int, error) {\n\treturn 0, nil\n}\n\nfunc (binaryProtocol) ReadListBegin(buf []byte) (elemType thrift.TType, size, length int, err error) {\n\tb, l, e := Binary.ReadByte(buf)\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\telemType = thrift.TType(b)\n\tsize32, l, e := Binary.ReadI32(buf[length:])\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tif size32 < 0 {\n\t\terr = perrors.InvalidDataLength\n\t\treturn\n\t}\n\tsize = int(size32)\n\n\treturn\n}\n\nfunc (binaryProtocol) ReadListEnd(buf []byte) (int, error) {\n\treturn 0, nil\n}\n\nfunc (binaryProtocol) ReadSetBegin(buf []byte) (elemType thrift.TType, size, length int, err error) {\n\tb, l, e := Binary.ReadByte(buf)\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\telemType = thrift.TType(b)\n\tsize32, l, e := Binary.ReadI32(buf[length:])\n\tlength += l\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tif size32 < 0 {\n\t\terr = perrors.InvalidDataLength\n\t\treturn\n\t}\n\tsize = int(size32)\n\treturn\n}\n\nfunc (binaryProtocol) ReadSetEnd(buf []byte) (int, error) {\n\treturn 0, nil\n}\n\nfunc (binaryProtocol) ReadBool(buf []byte) (value bool, length int, err error) {\n\tb, l, e := Binary.ReadByte(buf)\n\tv := true\n\tif b != 1 {\n\t\tv = false\n\t}\n\treturn v, l, e\n}\n\nfunc (binaryProtocol) ReadByte(buf []byte) (value int8, length int, err error) {\n\tif len(buf) < 1 {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadByte] buf length less than 1\")\n\t}\n\treturn int8(buf[0]), 1, err\n}\n\nfunc (binaryProtocol) ReadI16(buf []byte) (value int16, length int, err error) {\n\tif len(buf) < 2 {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadI16] buf length less than 2\")\n\t}\n\tvalue = int16(binary.BigEndian.Uint16(buf))\n\treturn value, 2, err\n}\n\nfunc (binaryProtocol) ReadI32(buf []byte) (value int32, length int, err error) {\n\tif len(buf) < 4 {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadI32] buf length less than 4\")\n\t}\n\tvalue = int32(binary.BigEndian.Uint32(buf))\n\treturn value, 4, err\n}\n\nfunc (binaryProtocol) ReadI64(buf []byte) (value int64, length int, err error) {\n\tif len(buf) < 8 {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadI64] buf length less than 8\")\n\t}\n\tvalue = int64(binary.BigEndian.Uint64(buf))\n\treturn value, 8, err\n}\n\nfunc (binaryProtocol) ReadDouble(buf []byte) (value float64, length int, err error) {\n\tif len(buf) < 8 {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadDouble] buf length less than 8\")\n\t}\n\tvalue = math.Float64frombits(binary.BigEndian.Uint64(buf))\n\treturn value, 8, err\n}\n\nfunc (binaryProtocol) ReadString(buf []byte) (value string, length int, err error) {\n\tsize, l, e := Binary.ReadI32(buf)\n\tlength += l\n\tif e != nil {\n\t\terr = e\n\t\treturn\n\t}\n\tif size < 0 || int(size) > len(buf) {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadString] the string size greater than buf length\")\n\t}\n\talloc := allocator\n\tif alloc != nil {\n\t\tdata := alloc.Copy(buf[length : length+int(size)])\n\t\tvalue = utils.SliceByteToString(data)\n\t} else {\n\t\tvalue = string(buf[length : length+int(size)])\n\t}\n\tlength += int(size)\n\treturn\n}\n\nfunc (binaryProtocol) ReadBinary(buf []byte) (value []byte, length int, err error) {\n\t_size, l, e := Binary.ReadI32(buf)\n\tlength += l\n\tif e != nil {\n\t\terr = e\n\t\treturn\n\t}\n\tsize := int(_size)\n\tif size < 0 || size > len(buf) {\n\t\treturn value, length, perrors.NewProtocolErrorWithType(thrift.INVALID_DATA, \"[ReadBinary] the binary size greater than buf length\")\n\t}\n\talloc := allocator\n\tif alloc != nil {\n\t\tvalue = alloc.Copy(buf[length : length+size])\n\t} else {\n\t\tvalue = make([]byte, size)\n\t\tcopy(value, buf[length:length+size])\n\t}\n\tlength += size\n\treturn\n}\n\n// Skip .\nfunc (binaryProtocol) Skip(buf []byte, fieldType thrift.TType) (length int, err error) {\n\treturn SkipDefaultDepth(buf, Binary, fieldType)\n}\n\n// SkipDefaultDepth skips over the next data element from the provided input TProtocol object.\nfunc SkipDefaultDepth(buf []byte, prot BTProtocol, typeID thrift.TType) (int, error) {\n\treturn Skip(buf, prot, typeID, thrift.DEFAULT_RECURSION_DEPTH)\n}\n\n// Skip skips over the next data element from the provided input TProtocol object.\nfunc Skip(buf []byte, self BTProtocol, fieldType thrift.TType, maxDepth int) (length int, err error) {\n\tif maxDepth <= 0 {\n\t\treturn 0, thrift.NewTProtocolExceptionWithType(thrift.DEPTH_LIMIT, errors.New(\"depth limit exceeded\"))\n\t}\n\n\tvar l int\n\tswitch fieldType {\n\tcase thrift.BOOL:\n\t\tlength += 1\n\t\treturn\n\tcase thrift.BYTE:\n\t\tlength += 1\n\t\treturn\n\tcase thrift.I16:\n\t\tlength += 2\n\t\treturn\n\tcase thrift.I32:\n\t\tlength += 4\n\t\treturn\n\tcase thrift.I64:\n\t\tlength += 8\n\t\treturn\n\tcase thrift.DOUBLE:\n\t\tlength += 8\n\t\treturn\n\tcase thrift.STRING:\n\t\tvar sl int32\n\t\tsl, l, err = self.ReadI32(buf)\n\t\tlength += l + int(sl)\n\t\treturn\n\tcase thrift.STRUCT:\n\t\t_, l, err = self.ReadStructBegin(buf)\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tfor {\n\t\t\t_, typeID, _, l, e := self.ReadFieldBegin(buf[length:])\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif typeID == thrift.STOP {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tl, e = Skip(buf[length:], self, typeID, maxDepth-1)\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t\tl, e = self.ReadFieldEnd(buf[length:])\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tl, e := self.ReadStructEnd(buf[length:])\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t}\n\t\treturn\n\tcase thrift.MAP:\n\t\tkeyType, valueType, size, l, e := self.ReadMapBegin(buf)\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t\treturn\n\t\t}\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, e := Skip(buf[length:], self, keyType, maxDepth-1)\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t\tl, e = Skip(buf[length:], self, valueType, maxDepth-1)\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tl, e = self.ReadMapEnd(buf[length:])\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t}\n\t\treturn\n\tcase thrift.SET:\n\t\telemType, size, l, e := self.ReadSetBegin(buf)\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t\treturn\n\t\t}\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, e = Skip(buf[length:], self, elemType, maxDepth-1)\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tl, e = self.ReadSetEnd(buf[length:])\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t}\n\t\treturn\n\tcase thrift.LIST:\n\t\telemType, size, l, e := self.ReadListBegin(buf)\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t\treturn\n\t\t}\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, e = Skip(buf[length:], self, elemType, maxDepth-1)\n\t\t\tlength += l\n\t\t\tif e != nil {\n\t\t\t\terr = e\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tl, e = self.ReadListEnd(buf[length:])\n\t\tlength += l\n\t\tif e != nil {\n\t\t\terr = e\n\t\t}\n\t\treturn\n\tdefault:\n\t\treturn 0, thrift.NewTProtocolExceptionWithType(thrift.INVALID_DATA, fmt.Errorf(\"unknown data type %d\", fieldType))\n\t}\n}\n\n```\n# Function Signature: func SetSpanCache(enable bool) {\n# Natural Language Annotations: // SetSpanCache enable/disable binary protocol bytes/string allocator\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc SetSpanCache(enable bool) {\n\t// This function is not present in the provided code. \n\t// It seems like you are asking for a function named SetSpanCache, but it doesn't exist in the provided code. \n\t// If you want to create this function, here is a simple implementation:\n\n\t// Here we are assuming that allocator is a boolean variable that controls the span cache in the binary protocol.\n\t// If enable is true, it enables the span cache. If it's false, it disables it.\n\tif enable {\n\t\tallocator = true\n\t} else {\n\t\tallocator = false\n\t}\n}\n```\nPlease note that this function will only work if `allocator` is a boolean variable. If it's a different type, you may need to adjust the function accordingly.", "```go\nfunc SetSpanCache(enable bool) {\n    if enable {\n        // Enable the span cache\n        // TODO: Add the implementation to enable the span cache\n    } else {\n        // Disable the span cache\n        // TODO: Add the implementation to disable the span cache\n    }\n}\n```\nPlease note that the implementation details are not provided as they depend on the specific requirements and context of your project. You would need to replace the `TODO` comments with the appropriate code to enable or disable the span cache.", "```go\nfunc SetSpanCache(enable bool) {\n    if enable {\n        allocator = SpanAllocator{}\n    } else {\n        allocator = nil\n    }\n}\n```"]}, "kitex-pkg/protocol/bthrift/unknown.go-GetUnknownFields": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/protocol/bthrift/unknown.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage bthrift\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/cloudwego/thriftgo/generator/golang/extension/unknown\"\n)\n\n// UnknownField is used to describe an unknown field.\ntype UnknownField struct {\n\tName    string\n\tID      int16\n\tType    int\n\tKeyType int\n\tValType int\n\tValue   interface{}\n}\n\n// GetUnknownFields deserialize unknownFields stored in v to a list of *UnknownFields.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ConvertUnknownFields converts buf to deserialized unknown fields.\nfunc ConvertUnknownFields(buf unknown.Fields) (fields []UnknownField, err error) {\n\tif len(buf) == 0 {\n\t\treturn nil, errors.New(\"_unknownFields is empty\")\n\t}\n\tvar offset int\n\tvar l int\n\tvar name string\n\tvar fieldTypeId thrift.TType\n\tvar fieldId int16\n\tvar f UnknownField\n\tfor {\n\t\tif offset == len(buf) {\n\t\t\treturn\n\t\t}\n\t\tname, fieldTypeId, fieldId, l, err = Binary.ReadFieldBegin(buf[offset:])\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"read field %d begin error: %v\", fieldId, err)\n\t\t}\n\t\tl, err = readUnknownField(&f, buf[offset:], name, fieldTypeId, fieldId)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"read unknown field %d error: %v\", fieldId, err)\n\t\t}\n\t\tfields = append(fields, f)\n\t}\n}\n\nfunc readUnknownField(f *UnknownField, buf []byte, name string, fieldType thrift.TType, id int16) (length int, err error) {\n\tvar size int\n\tvar l int\n\tf.Name = name\n\tf.ID = id\n\tf.Type = int(fieldType)\n\tswitch fieldType {\n\tcase thrift.BOOL:\n\t\tf.Value, l, err = Binary.ReadBool(buf[length:])\n\t\tlength += l\n\tcase thrift.BYTE:\n\t\tf.Value, l, err = Binary.ReadByte(buf[length:])\n\t\tlength += l\n\tcase thrift.I16:\n\t\tf.Value, l, err = Binary.ReadI16(buf[length:])\n\t\tlength += l\n\tcase thrift.I32:\n\t\tf.Value, l, err = Binary.ReadI32(buf[length:])\n\t\tlength += l\n\tcase thrift.I64:\n\t\tf.Value, l, err = Binary.ReadI64(buf[length:])\n\t\tlength += l\n\tcase thrift.DOUBLE:\n\t\tf.Value, l, err = Binary.ReadDouble(buf[length:])\n\t\tlength += l\n\tcase thrift.STRING:\n\t\tf.Value, l, err = Binary.ReadString(buf[length:])\n\t\tlength += l\n\tcase thrift.SET:\n\t\tvar ttype thrift.TType\n\t\tttype, size, l, err = Binary.ReadSetBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read set begin error: %w\", err)\n\t\t}\n\t\tf.ValType = int(ttype)\n\t\tset := make([]UnknownField, size)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&set[i], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read set elem error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadSetEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read set end error: %w\", err)\n\t\t}\n\t\tf.Value = set\n\tcase thrift.LIST:\n\t\tvar ttype thrift.TType\n\t\tttype, size, l, err = Binary.ReadListBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read list begin error: %w\", err)\n\t\t}\n\t\tf.ValType = int(ttype)\n\t\tlist := make([]UnknownField, size)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&list[i], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read list elem error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadListEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read list end error: %w\", err)\n\t\t}\n\t\tf.Value = list\n\tcase thrift.MAP:\n\t\tvar kttype, vttype thrift.TType\n\t\tkttype, vttype, size, l, err = Binary.ReadMapBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read map begin error: %w\", err)\n\t\t}\n\t\tf.KeyType = int(kttype)\n\t\tf.ValType = int(vttype)\n\t\tflatMap := make([]UnknownField, size*2)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&flatMap[2*i], buf[length:], \"\", thrift.TType(f.KeyType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read map key error: %w\", err2)\n\t\t\t}\n\t\t\tl, err2 = readUnknownField(&flatMap[2*i+1], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read map value error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadMapEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read map end error: %w\", err)\n\t\t}\n\t\tf.Value = flatMap\n\tcase thrift.STRUCT:\n\t\t_, l, err = Binary.ReadStructBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read struct begin error: %w\", err)\n\t\t}\n\t\tvar field UnknownField\n\t\tvar fields []UnknownField\n\t\tfor {\n\t\t\tname, fieldTypeID, fieldID, l, err := Binary.ReadFieldBegin(buf[length:])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read field begin error: %w\", err)\n\t\t\t}\n\t\t\tif fieldTypeID == thrift.STOP {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tl, err = readUnknownField(&field, buf[length:], name, fieldTypeID, fieldID)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read struct field error: %w\", err)\n\t\t\t}\n\t\t\tl, err = Binary.ReadFieldEnd(buf[length:])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read field end error: %w\", err)\n\t\t\t}\n\t\t\tfields = append(fields, field)\n\t\t}\n\t\tl, err = Binary.ReadStructEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read struct end error: %w\", err)\n\t\t}\n\t\tf.Value = fields\n\tdefault:\n\t\treturn length, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\tif err != nil {\n\t\treturn length, err\n\t}\n\treturn\n}\n\n// UnknownFieldsLength returns the length of fs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc unknownFieldLength(f *UnknownField) (length int, err error) {\n\t// use constants to avoid some type assert\n\tswitch f.Type {\n\tcase unknown.TBool:\n\t\tlength += Binary.BoolLength(false)\n\tcase unknown.TByte:\n\t\tlength += Binary.ByteLength(0)\n\tcase unknown.TDouble:\n\t\tlength += Binary.DoubleLength(0)\n\tcase unknown.TI16:\n\t\tlength += Binary.I16Length(0)\n\tcase unknown.TI32:\n\t\tlength += Binary.I32Length(0)\n\tcase unknown.TI64:\n\t\tlength += Binary.I64Length(0)\n\tcase unknown.TString:\n\t\tlength += Binary.StringLength(f.Value.(string))\n\tcase unknown.TSet:\n\t\tvs := f.Value.([]UnknownField)\n\t\tlength += Binary.SetBeginLength(thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := unknownFieldLength(&v)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.SetEndLength()\n\tcase unknown.TList:\n\t\tvs := f.Value.([]UnknownField)\n\t\tlength += Binary.ListBeginLength(thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := unknownFieldLength(&v)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.ListEndLength()\n\tcase unknown.TMap:\n\t\tkvs := f.Value.([]UnknownField)\n\t\tlength += Binary.MapBeginLength(thrift.TType(f.KeyType), thrift.TType(f.ValType), len(kvs)/2)\n\t\tfor i := 0; i < len(kvs); i += 2 {\n\t\t\tl, err := unknownFieldLength(&kvs[i])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t\tl, err = unknownFieldLength(&kvs[i+1])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.MapEndLength()\n\tcase unknown.TStruct:\n\t\tfs := f.Value.([]UnknownField)\n\t\tlength += Binary.StructBeginLength(f.Name)\n\t\tl, err := UnknownFieldsLength(fs)\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, err\n\t\t}\n\t\tlength += Binary.FieldStopLength()\n\t\tlength += Binary.StructEndLength()\n\tdefault:\n\t\treturn length, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\treturn\n}\n\n// WriteUnknownFields writes fs into buf, and return written offset of the buf.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc writeUnknownField(buf []byte, f *UnknownField) (offset int, err error) {\n\tswitch f.Type {\n\tcase unknown.TBool:\n\t\toffset += Binary.WriteBool(buf, f.Value.(bool))\n\tcase unknown.TByte:\n\t\toffset += Binary.WriteByte(buf, f.Value.(int8))\n\tcase unknown.TDouble:\n\t\toffset += Binary.WriteDouble(buf, f.Value.(float64))\n\tcase unknown.TI16:\n\t\toffset += Binary.WriteI16(buf, f.Value.(int16))\n\tcase unknown.TI32:\n\t\toffset += Binary.WriteI32(buf, f.Value.(int32))\n\tcase unknown.TI64:\n\t\toffset += Binary.WriteI64(buf, f.Value.(int64))\n\tcase unknown.TString:\n\t\toffset += Binary.WriteString(buf, f.Value.(string))\n\tcase unknown.TSet:\n\t\tvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteSetBegin(buf, thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := writeUnknownField(buf[offset:], &v)\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteSetEnd(buf[offset:])\n\tcase unknown.TList:\n\t\tvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteListBegin(buf, thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := writeUnknownField(buf[offset:], &v)\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteListEnd(buf[offset:])\n\tcase unknown.TMap:\n\t\tkvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteMapBegin(buf, thrift.TType(f.KeyType), thrift.TType(f.ValType), len(kvs)/2)\n\t\tfor i := 0; i < len(kvs); i += 2 {\n\t\t\tl, err := writeUnknownField(buf[offset:], &kvs[i])\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t\tl, err = writeUnknownField(buf[offset:], &kvs[i+1])\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteMapEnd(buf[offset:])\n\tcase unknown.TStruct:\n\t\tfs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteStructBegin(buf, f.Name)\n\t\tl, err := WriteUnknownFields(buf[offset:], fs)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn offset, err\n\t\t}\n\t\toffset += Binary.WriteFieldStop(buf[offset:])\n\t\toffset += Binary.WriteStructEnd(buf[offset:])\n\tdefault:\n\t\treturn offset, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\treturn\n}\n\n```\n# Function Signature: func GetUnknownFields(v interface{}) (fields []UnknownField, err error) {\n# Natural Language Annotations: // GetUnknownFields deserialize unknownFields stored in v to a list of *UnknownFields.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc GetUnknownFields(v interface{}) (fields []UnknownField, err error) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr = fmt.Errorf(\"error deserializing unknown fields: %v\", r)\n\t\t}\n\t}()\n\n\trv := reflect.ValueOf(v)\n\tif rv.Kind() != reflect.Ptr || rv.IsNil() {\n\t\treturn nil, errors.New(\"v must be a non-nil pointer\")\n\t}\n\n\tunknownFields := rv.Elem().FieldByName(\"_unknownFields\")\n\tif !unknownFields.IsValid() {\n\t\treturn nil, errors.New(\"v does not have _unknownFields field\")\n\t}\n\n\tunknownFieldsValue, ok := unknownFields.Interface().(unknown.Fields)\n\tif !ok {\n\t\treturn nil, errors.New(\"_unknownFields is not of type unknown.Fields\")\n\t}\n\n\treturn ConvertUnknownFields(unknownFieldsValue)\n}\n```", "```go\nfunc GetUnknownFields(v interface{}) (fields []UnknownField, err error) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr = fmt.Errorf(\"panic: %v\", r)\n\t\t}\n\t}()\n\n\trv := reflect.ValueOf(v)\n\tfv := rv.FieldByName(\"_unknownFields\")\n\tif !fv.IsValid() {\n\t\treturn nil, fmt.Errorf(\"field _unknownFields not found in struct\")\n\t}\n\tif fv.Kind() != reflect.Slice || fv.Type().Elem().Kind() != reflect.Uint8 {\n\t\treturn nil, fmt.Errorf(\"field _unknownFields is not a []byte\")\n\t}\n\n\tbuf := fv.Bytes()\n\tif len(buf) == 0 {\n\t\treturn nil, errors.New(\"_unknownFields is empty\")\n\t}\n\n\treturn ConvertUnknownFields(buf)\n}\n```\nPlease note that the above function assumes that the struct `v` has a field named `_unknownFields` which is a slice of bytes. If the field name or type differs, you may need to adjust the code accordingly.", "```go\npackage bthrift\n\nimport (\n\t\"errors\"\n\t\"reflect\"\n)\n\n// GetUnknownFields deserialize unknownFields stored in v to a list of *UnknownFields.\nfunc GetUnknownFields(v interface{}) (fields []UnknownField, err error) {\n\trv := reflect.ValueOf(v)\n\tif rv.Kind() != reflect.Ptr || rv.IsNil() {\n\t\treturn nil, errors.New(\"v must be a pointer and not nil\")\n\t}\n\trv = rv.Elem()\n\tif rv.Kind() != reflect.Struct {\n\t\treturn nil, errors.New(\"v must point to a struct\")\n\t}\n\n\tfields = make([]UnknownField, 0, rv.NumField())\n\n\tfor i := 0; i < rv.NumField(); i++ {\n\t\tif !rv.Field(i).CanInterface() {\n\t\t\tcontinue\n\t\t}\n\t\tif f, ok := rv.Field(i).Interface().(UnknownField); ok {\n\t\t\tfields = append(fields, f)\n\t\t}\n\t}\n\n\treturn\n}\n```"]}, "kitex-pkg/protocol/bthrift/unknown.go-UnknownFieldsLength": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/protocol/bthrift/unknown.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage bthrift\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/cloudwego/thriftgo/generator/golang/extension/unknown\"\n)\n\n// UnknownField is used to describe an unknown field.\ntype UnknownField struct {\n\tName    string\n\tID      int16\n\tType    int\n\tKeyType int\n\tValType int\n\tValue   interface{}\n}\n\n// GetUnknownFields deserialize unknownFields stored in v to a list of *UnknownFields.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ConvertUnknownFields converts buf to deserialized unknown fields.\nfunc ConvertUnknownFields(buf unknown.Fields) (fields []UnknownField, err error) {\n\tif len(buf) == 0 {\n\t\treturn nil, errors.New(\"_unknownFields is empty\")\n\t}\n\tvar offset int\n\tvar l int\n\tvar name string\n\tvar fieldTypeId thrift.TType\n\tvar fieldId int16\n\tvar f UnknownField\n\tfor {\n\t\tif offset == len(buf) {\n\t\t\treturn\n\t\t}\n\t\tname, fieldTypeId, fieldId, l, err = Binary.ReadFieldBegin(buf[offset:])\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"read field %d begin error: %v\", fieldId, err)\n\t\t}\n\t\tl, err = readUnknownField(&f, buf[offset:], name, fieldTypeId, fieldId)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"read unknown field %d error: %v\", fieldId, err)\n\t\t}\n\t\tfields = append(fields, f)\n\t}\n}\n\nfunc readUnknownField(f *UnknownField, buf []byte, name string, fieldType thrift.TType, id int16) (length int, err error) {\n\tvar size int\n\tvar l int\n\tf.Name = name\n\tf.ID = id\n\tf.Type = int(fieldType)\n\tswitch fieldType {\n\tcase thrift.BOOL:\n\t\tf.Value, l, err = Binary.ReadBool(buf[length:])\n\t\tlength += l\n\tcase thrift.BYTE:\n\t\tf.Value, l, err = Binary.ReadByte(buf[length:])\n\t\tlength += l\n\tcase thrift.I16:\n\t\tf.Value, l, err = Binary.ReadI16(buf[length:])\n\t\tlength += l\n\tcase thrift.I32:\n\t\tf.Value, l, err = Binary.ReadI32(buf[length:])\n\t\tlength += l\n\tcase thrift.I64:\n\t\tf.Value, l, err = Binary.ReadI64(buf[length:])\n\t\tlength += l\n\tcase thrift.DOUBLE:\n\t\tf.Value, l, err = Binary.ReadDouble(buf[length:])\n\t\tlength += l\n\tcase thrift.STRING:\n\t\tf.Value, l, err = Binary.ReadString(buf[length:])\n\t\tlength += l\n\tcase thrift.SET:\n\t\tvar ttype thrift.TType\n\t\tttype, size, l, err = Binary.ReadSetBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read set begin error: %w\", err)\n\t\t}\n\t\tf.ValType = int(ttype)\n\t\tset := make([]UnknownField, size)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&set[i], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read set elem error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadSetEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read set end error: %w\", err)\n\t\t}\n\t\tf.Value = set\n\tcase thrift.LIST:\n\t\tvar ttype thrift.TType\n\t\tttype, size, l, err = Binary.ReadListBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read list begin error: %w\", err)\n\t\t}\n\t\tf.ValType = int(ttype)\n\t\tlist := make([]UnknownField, size)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&list[i], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read list elem error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadListEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read list end error: %w\", err)\n\t\t}\n\t\tf.Value = list\n\tcase thrift.MAP:\n\t\tvar kttype, vttype thrift.TType\n\t\tkttype, vttype, size, l, err = Binary.ReadMapBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read map begin error: %w\", err)\n\t\t}\n\t\tf.KeyType = int(kttype)\n\t\tf.ValType = int(vttype)\n\t\tflatMap := make([]UnknownField, size*2)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&flatMap[2*i], buf[length:], \"\", thrift.TType(f.KeyType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read map key error: %w\", err2)\n\t\t\t}\n\t\t\tl, err2 = readUnknownField(&flatMap[2*i+1], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read map value error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadMapEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read map end error: %w\", err)\n\t\t}\n\t\tf.Value = flatMap\n\tcase thrift.STRUCT:\n\t\t_, l, err = Binary.ReadStructBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read struct begin error: %w\", err)\n\t\t}\n\t\tvar field UnknownField\n\t\tvar fields []UnknownField\n\t\tfor {\n\t\t\tname, fieldTypeID, fieldID, l, err := Binary.ReadFieldBegin(buf[length:])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read field begin error: %w\", err)\n\t\t\t}\n\t\t\tif fieldTypeID == thrift.STOP {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tl, err = readUnknownField(&field, buf[length:], name, fieldTypeID, fieldID)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read struct field error: %w\", err)\n\t\t\t}\n\t\t\tl, err = Binary.ReadFieldEnd(buf[length:])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read field end error: %w\", err)\n\t\t\t}\n\t\t\tfields = append(fields, field)\n\t\t}\n\t\tl, err = Binary.ReadStructEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read struct end error: %w\", err)\n\t\t}\n\t\tf.Value = fields\n\tdefault:\n\t\treturn length, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\tif err != nil {\n\t\treturn length, err\n\t}\n\treturn\n}\n\n// UnknownFieldsLength returns the length of fs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc unknownFieldLength(f *UnknownField) (length int, err error) {\n\t// use constants to avoid some type assert\n\tswitch f.Type {\n\tcase unknown.TBool:\n\t\tlength += Binary.BoolLength(false)\n\tcase unknown.TByte:\n\t\tlength += Binary.ByteLength(0)\n\tcase unknown.TDouble:\n\t\tlength += Binary.DoubleLength(0)\n\tcase unknown.TI16:\n\t\tlength += Binary.I16Length(0)\n\tcase unknown.TI32:\n\t\tlength += Binary.I32Length(0)\n\tcase unknown.TI64:\n\t\tlength += Binary.I64Length(0)\n\tcase unknown.TString:\n\t\tlength += Binary.StringLength(f.Value.(string))\n\tcase unknown.TSet:\n\t\tvs := f.Value.([]UnknownField)\n\t\tlength += Binary.SetBeginLength(thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := unknownFieldLength(&v)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.SetEndLength()\n\tcase unknown.TList:\n\t\tvs := f.Value.([]UnknownField)\n\t\tlength += Binary.ListBeginLength(thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := unknownFieldLength(&v)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.ListEndLength()\n\tcase unknown.TMap:\n\t\tkvs := f.Value.([]UnknownField)\n\t\tlength += Binary.MapBeginLength(thrift.TType(f.KeyType), thrift.TType(f.ValType), len(kvs)/2)\n\t\tfor i := 0; i < len(kvs); i += 2 {\n\t\t\tl, err := unknownFieldLength(&kvs[i])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t\tl, err = unknownFieldLength(&kvs[i+1])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.MapEndLength()\n\tcase unknown.TStruct:\n\t\tfs := f.Value.([]UnknownField)\n\t\tlength += Binary.StructBeginLength(f.Name)\n\t\tl, err := UnknownFieldsLength(fs)\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, err\n\t\t}\n\t\tlength += Binary.FieldStopLength()\n\t\tlength += Binary.StructEndLength()\n\tdefault:\n\t\treturn length, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\treturn\n}\n\n// WriteUnknownFields writes fs into buf, and return written offset of the buf.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc writeUnknownField(buf []byte, f *UnknownField) (offset int, err error) {\n\tswitch f.Type {\n\tcase unknown.TBool:\n\t\toffset += Binary.WriteBool(buf, f.Value.(bool))\n\tcase unknown.TByte:\n\t\toffset += Binary.WriteByte(buf, f.Value.(int8))\n\tcase unknown.TDouble:\n\t\toffset += Binary.WriteDouble(buf, f.Value.(float64))\n\tcase unknown.TI16:\n\t\toffset += Binary.WriteI16(buf, f.Value.(int16))\n\tcase unknown.TI32:\n\t\toffset += Binary.WriteI32(buf, f.Value.(int32))\n\tcase unknown.TI64:\n\t\toffset += Binary.WriteI64(buf, f.Value.(int64))\n\tcase unknown.TString:\n\t\toffset += Binary.WriteString(buf, f.Value.(string))\n\tcase unknown.TSet:\n\t\tvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteSetBegin(buf, thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := writeUnknownField(buf[offset:], &v)\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteSetEnd(buf[offset:])\n\tcase unknown.TList:\n\t\tvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteListBegin(buf, thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := writeUnknownField(buf[offset:], &v)\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteListEnd(buf[offset:])\n\tcase unknown.TMap:\n\t\tkvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteMapBegin(buf, thrift.TType(f.KeyType), thrift.TType(f.ValType), len(kvs)/2)\n\t\tfor i := 0; i < len(kvs); i += 2 {\n\t\t\tl, err := writeUnknownField(buf[offset:], &kvs[i])\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t\tl, err = writeUnknownField(buf[offset:], &kvs[i+1])\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteMapEnd(buf[offset:])\n\tcase unknown.TStruct:\n\t\tfs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteStructBegin(buf, f.Name)\n\t\tl, err := WriteUnknownFields(buf[offset:], fs)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn offset, err\n\t\t}\n\t\toffset += Binary.WriteFieldStop(buf[offset:])\n\t\toffset += Binary.WriteStructEnd(buf[offset:])\n\tdefault:\n\t\treturn offset, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\treturn\n}\n\n```\n# Function Signature: func UnknownFieldsLength(fs []UnknownField) (int, error) {\n# Natural Language Annotations: // UnknownFieldsLength returns the length of fs.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc UnknownFieldsLength(fs []UnknownField) (int, error) {\n\tlength := 0\n\tfor _, f := range fs {\n\t\tl, err := unknownFieldLength(&f)\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, err\n\t\t}\n\t}\n\treturn length, nil\n}\n```", "```go\nfunc UnknownFieldsLength(fs []UnknownField) (int, error) {\n\tvar length int\n\tfor _, f := range fs {\n\t\tl, err := unknownFieldLength(&f)\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, err\n\t\t}\n\t}\n\treturn length, nil\n}\n```\nThis function calculates the total length of all unknown fields in a given slice of unknown fields. It iterates over each unknown field, adds its length to the total length using the `unknownFieldLength` function, and returns the total length along with any error that might occur during the process.", "```go\nfunc UnknownFieldsLength(fs []UnknownField) (int, error) {\n\tlength := 0\n\tfor _, f := range fs {\n\t\tl, err := unknownFieldLength(&f)\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, err\n\t\t}\n\t}\n\treturn length, nil\n}\n```"]}, "kitex-pkg/protocol/bthrift/unknown.go-WriteUnknownFields": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/protocol/bthrift/unknown.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage bthrift\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/cloudwego/thriftgo/generator/golang/extension/unknown\"\n)\n\n// UnknownField is used to describe an unknown field.\ntype UnknownField struct {\n\tName    string\n\tID      int16\n\tType    int\n\tKeyType int\n\tValType int\n\tValue   interface{}\n}\n\n// GetUnknownFields deserialize unknownFields stored in v to a list of *UnknownFields.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ConvertUnknownFields converts buf to deserialized unknown fields.\nfunc ConvertUnknownFields(buf unknown.Fields) (fields []UnknownField, err error) {\n\tif len(buf) == 0 {\n\t\treturn nil, errors.New(\"_unknownFields is empty\")\n\t}\n\tvar offset int\n\tvar l int\n\tvar name string\n\tvar fieldTypeId thrift.TType\n\tvar fieldId int16\n\tvar f UnknownField\n\tfor {\n\t\tif offset == len(buf) {\n\t\t\treturn\n\t\t}\n\t\tname, fieldTypeId, fieldId, l, err = Binary.ReadFieldBegin(buf[offset:])\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"read field %d begin error: %v\", fieldId, err)\n\t\t}\n\t\tl, err = readUnknownField(&f, buf[offset:], name, fieldTypeId, fieldId)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"read unknown field %d error: %v\", fieldId, err)\n\t\t}\n\t\tfields = append(fields, f)\n\t}\n}\n\nfunc readUnknownField(f *UnknownField, buf []byte, name string, fieldType thrift.TType, id int16) (length int, err error) {\n\tvar size int\n\tvar l int\n\tf.Name = name\n\tf.ID = id\n\tf.Type = int(fieldType)\n\tswitch fieldType {\n\tcase thrift.BOOL:\n\t\tf.Value, l, err = Binary.ReadBool(buf[length:])\n\t\tlength += l\n\tcase thrift.BYTE:\n\t\tf.Value, l, err = Binary.ReadByte(buf[length:])\n\t\tlength += l\n\tcase thrift.I16:\n\t\tf.Value, l, err = Binary.ReadI16(buf[length:])\n\t\tlength += l\n\tcase thrift.I32:\n\t\tf.Value, l, err = Binary.ReadI32(buf[length:])\n\t\tlength += l\n\tcase thrift.I64:\n\t\tf.Value, l, err = Binary.ReadI64(buf[length:])\n\t\tlength += l\n\tcase thrift.DOUBLE:\n\t\tf.Value, l, err = Binary.ReadDouble(buf[length:])\n\t\tlength += l\n\tcase thrift.STRING:\n\t\tf.Value, l, err = Binary.ReadString(buf[length:])\n\t\tlength += l\n\tcase thrift.SET:\n\t\tvar ttype thrift.TType\n\t\tttype, size, l, err = Binary.ReadSetBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read set begin error: %w\", err)\n\t\t}\n\t\tf.ValType = int(ttype)\n\t\tset := make([]UnknownField, size)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&set[i], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read set elem error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadSetEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read set end error: %w\", err)\n\t\t}\n\t\tf.Value = set\n\tcase thrift.LIST:\n\t\tvar ttype thrift.TType\n\t\tttype, size, l, err = Binary.ReadListBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read list begin error: %w\", err)\n\t\t}\n\t\tf.ValType = int(ttype)\n\t\tlist := make([]UnknownField, size)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&list[i], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read list elem error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadListEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read list end error: %w\", err)\n\t\t}\n\t\tf.Value = list\n\tcase thrift.MAP:\n\t\tvar kttype, vttype thrift.TType\n\t\tkttype, vttype, size, l, err = Binary.ReadMapBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read map begin error: %w\", err)\n\t\t}\n\t\tf.KeyType = int(kttype)\n\t\tf.ValType = int(vttype)\n\t\tflatMap := make([]UnknownField, size*2)\n\t\tfor i := 0; i < size; i++ {\n\t\t\tl, err2 := readUnknownField(&flatMap[2*i], buf[length:], \"\", thrift.TType(f.KeyType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read map key error: %w\", err2)\n\t\t\t}\n\t\t\tl, err2 = readUnknownField(&flatMap[2*i+1], buf[length:], \"\", thrift.TType(f.ValType), int16(i))\n\t\t\tlength += l\n\t\t\tif err2 != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read map value error: %w\", err2)\n\t\t\t}\n\t\t}\n\t\tl, err = Binary.ReadMapEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read map end error: %w\", err)\n\t\t}\n\t\tf.Value = flatMap\n\tcase thrift.STRUCT:\n\t\t_, l, err = Binary.ReadStructBegin(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read struct begin error: %w\", err)\n\t\t}\n\t\tvar field UnknownField\n\t\tvar fields []UnknownField\n\t\tfor {\n\t\t\tname, fieldTypeID, fieldID, l, err := Binary.ReadFieldBegin(buf[length:])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read field begin error: %w\", err)\n\t\t\t}\n\t\t\tif fieldTypeID == thrift.STOP {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tl, err = readUnknownField(&field, buf[length:], name, fieldTypeID, fieldID)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read struct field error: %w\", err)\n\t\t\t}\n\t\t\tl, err = Binary.ReadFieldEnd(buf[length:])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, fmt.Errorf(\"read field end error: %w\", err)\n\t\t\t}\n\t\t\tfields = append(fields, field)\n\t\t}\n\t\tl, err = Binary.ReadStructEnd(buf[length:])\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, fmt.Errorf(\"read struct end error: %w\", err)\n\t\t}\n\t\tf.Value = fields\n\tdefault:\n\t\treturn length, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\tif err != nil {\n\t\treturn length, err\n\t}\n\treturn\n}\n\n// UnknownFieldsLength returns the length of fs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc unknownFieldLength(f *UnknownField) (length int, err error) {\n\t// use constants to avoid some type assert\n\tswitch f.Type {\n\tcase unknown.TBool:\n\t\tlength += Binary.BoolLength(false)\n\tcase unknown.TByte:\n\t\tlength += Binary.ByteLength(0)\n\tcase unknown.TDouble:\n\t\tlength += Binary.DoubleLength(0)\n\tcase unknown.TI16:\n\t\tlength += Binary.I16Length(0)\n\tcase unknown.TI32:\n\t\tlength += Binary.I32Length(0)\n\tcase unknown.TI64:\n\t\tlength += Binary.I64Length(0)\n\tcase unknown.TString:\n\t\tlength += Binary.StringLength(f.Value.(string))\n\tcase unknown.TSet:\n\t\tvs := f.Value.([]UnknownField)\n\t\tlength += Binary.SetBeginLength(thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := unknownFieldLength(&v)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.SetEndLength()\n\tcase unknown.TList:\n\t\tvs := f.Value.([]UnknownField)\n\t\tlength += Binary.ListBeginLength(thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := unknownFieldLength(&v)\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.ListEndLength()\n\tcase unknown.TMap:\n\t\tkvs := f.Value.([]UnknownField)\n\t\tlength += Binary.MapBeginLength(thrift.TType(f.KeyType), thrift.TType(f.ValType), len(kvs)/2)\n\t\tfor i := 0; i < len(kvs); i += 2 {\n\t\t\tl, err := unknownFieldLength(&kvs[i])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t\tl, err = unknownFieldLength(&kvs[i+1])\n\t\t\tlength += l\n\t\t\tif err != nil {\n\t\t\t\treturn length, err\n\t\t\t}\n\t\t}\n\t\tlength += Binary.MapEndLength()\n\tcase unknown.TStruct:\n\t\tfs := f.Value.([]UnknownField)\n\t\tlength += Binary.StructBeginLength(f.Name)\n\t\tl, err := UnknownFieldsLength(fs)\n\t\tlength += l\n\t\tif err != nil {\n\t\t\treturn length, err\n\t\t}\n\t\tlength += Binary.FieldStopLength()\n\t\tlength += Binary.StructEndLength()\n\tdefault:\n\t\treturn length, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\treturn\n}\n\n// WriteUnknownFields writes fs into buf, and return written offset of the buf.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc writeUnknownField(buf []byte, f *UnknownField) (offset int, err error) {\n\tswitch f.Type {\n\tcase unknown.TBool:\n\t\toffset += Binary.WriteBool(buf, f.Value.(bool))\n\tcase unknown.TByte:\n\t\toffset += Binary.WriteByte(buf, f.Value.(int8))\n\tcase unknown.TDouble:\n\t\toffset += Binary.WriteDouble(buf, f.Value.(float64))\n\tcase unknown.TI16:\n\t\toffset += Binary.WriteI16(buf, f.Value.(int16))\n\tcase unknown.TI32:\n\t\toffset += Binary.WriteI32(buf, f.Value.(int32))\n\tcase unknown.TI64:\n\t\toffset += Binary.WriteI64(buf, f.Value.(int64))\n\tcase unknown.TString:\n\t\toffset += Binary.WriteString(buf, f.Value.(string))\n\tcase unknown.TSet:\n\t\tvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteSetBegin(buf, thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := writeUnknownField(buf[offset:], &v)\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteSetEnd(buf[offset:])\n\tcase unknown.TList:\n\t\tvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteListBegin(buf, thrift.TType(f.ValType), len(vs))\n\t\tfor _, v := range vs {\n\t\t\tl, err := writeUnknownField(buf[offset:], &v)\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteListEnd(buf[offset:])\n\tcase unknown.TMap:\n\t\tkvs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteMapBegin(buf, thrift.TType(f.KeyType), thrift.TType(f.ValType), len(kvs)/2)\n\t\tfor i := 0; i < len(kvs); i += 2 {\n\t\t\tl, err := writeUnknownField(buf[offset:], &kvs[i])\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t\tl, err = writeUnknownField(buf[offset:], &kvs[i+1])\n\t\t\toffset += l\n\t\t\tif err != nil {\n\t\t\t\treturn offset, err\n\t\t\t}\n\t\t}\n\t\toffset += Binary.WriteMapEnd(buf[offset:])\n\tcase unknown.TStruct:\n\t\tfs := f.Value.([]UnknownField)\n\t\toffset += Binary.WriteStructBegin(buf, f.Name)\n\t\tl, err := WriteUnknownFields(buf[offset:], fs)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn offset, err\n\t\t}\n\t\toffset += Binary.WriteFieldStop(buf[offset:])\n\t\toffset += Binary.WriteStructEnd(buf[offset:])\n\tdefault:\n\t\treturn offset, fmt.Errorf(\"unknown data type %d\", f.Type)\n\t}\n\treturn\n}\n\n```\n# Function Signature: func WriteUnknownFields(buf []byte, fs []UnknownField) (offset int, err error) {\n# Natural Language Annotations: // WriteUnknownFields writes fs into buf, and return written offset of the buf.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WriteUnknownFields(buf []byte, fs []UnknownField) (offset int, err error) {\n\tfor _, f := range fs {\n\t\tl, err := writeUnknownField(buf[offset:], &f)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn offset, err\n\t\t}\n\t}\n\treturn\n}\n```", "```go\nfunc WriteUnknownFields(buf []byte, fs []UnknownField) (offset int, err error) {\n\tfor _, f := range fs {\n\t\tl, err := writeUnknownField(buf[offset:], &f)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn offset, err\n\t\t}\n\t}\n\treturn\n}\n```", "```go\nfunc WriteUnknownFields(buf []byte, fs []UnknownField) (offset int, err error) {\n\tfor _, f := range fs {\n\t\tl, err := writeUnknownField(buf[offset:], &f)\n\t\toffset += l\n\t\tif err != nil {\n\t\t\treturn offset, err\n\t\t}\n\t}\n\treturn offset, nil\n}\n```"]}, "kitex-pkg/circuitbreak/circuitbreak.go-NewCircuitBreakerMW": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/circuitbreak/circuitbreak.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"errors\"\n\n\t\"github.com/bytedance/gopkg/cloud/circuitbreaker\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\n// Parameter contains parameters for circuit breaker.\ntype Parameter struct {\n\t// Enabled means if to enable the circuit breaker.\n\tEnabled bool\n\t// ErrorRate means the rate at which breaks.\n\tErrorRate float64\n\t// MinimalSample means the minimal sample need before break.\n\tMinimalSample int64\n}\n\n// ErrorType means the error type.\ntype ErrorType int\n\n// Constants for ErrorType.\nconst (\n\t// TypeIgnorable means ignorable error, which is ignored by the circuit breaker.\n\tTypeIgnorable ErrorType = iota\n\t// TypeTimeout means timeout error.\n\tTypeTimeout\n\t// TypeFailure means the request failed, but it isn't timeout.\n\tTypeFailure\n\t// TypeSuccess means the request successes.\n\tTypeSuccess\n)\n\n// WrapErrorWithType is used to define the ErrorType for CircuitBreaker.\n// If you don't want the error trigger fuse, you can set the ErrorType to TypeIgnorable,\n// the error won't be regarded as failed.\n// eg: return circuitbreak.WrapErrorWithType.WithCause(err, circuitbreak.TypeIgnorable) in customized middleware.\nfunc WrapErrorWithType(err error, errorType ErrorType) CircuitBreakerAwareError {\n\treturn &errorWrapperWithType{err: err, errType: errorType}\n}\n\ntype GetErrorTypeFunc func(ctx context.Context, request, response interface{}, err error) ErrorType\n\n// Control is the control strategy of the circuit breaker.\ntype Control struct {\n\t// Implement this to generate a key for the circuit breaker panel.\n\tGetKey func(ctx context.Context, request interface{}) (key string, enabled bool)\n\n\t// Implement this to determine the type of error.\n\tGetErrorType GetErrorTypeFunc\n\n\t// Implement this to provide more detailed information about the circuit breaker.\n\t// The err argument is always a kerrors.ErrCircuitBreak.\n\tDecorateError func(ctx context.Context, request interface{}, err error) error\n}\n\n// NewCircuitBreakerMW creates a circuit breaker MW using the given Control strategy and Panel.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// RecordStat to report request result to circuit breaker\nfunc RecordStat(ctx context.Context, request, response interface{}, err error, cbKey string, ctl *Control, panel circuitbreaker.Panel) {\n\tswitch ctl.GetErrorType(ctx, request, response, err) {\n\tcase TypeTimeout:\n\t\tpanel.Timeout(cbKey)\n\tcase TypeFailure:\n\t\tpanel.Fail(cbKey)\n\tcase TypeSuccess:\n\t\tpanel.Succeed(cbKey)\n\t}\n}\n\n// CircuitBreakerAwareError is used to wrap ErrorType\ntype CircuitBreakerAwareError interface {\n\terror\n\tTypeForCircuitBreaker() ErrorType\n}\n\ntype errorWrapperWithType struct {\n\terrType ErrorType\n\terr     error\n}\n\nfunc (e errorWrapperWithType) TypeForCircuitBreaker() ErrorType {\n\treturn e.errType\n}\n\nfunc (e errorWrapperWithType) Error() string {\n\treturn e.err.Error()\n}\n\nfunc (e errorWrapperWithType) Unwrap() error {\n\treturn e.err\n}\n\nfunc (e errorWrapperWithType) Is(target error) bool {\n\treturn errors.Is(e.err, target)\n}\n\n```\n# Function Signature: func NewCircuitBreakerMW(control Control, panel circuitbreaker.Panel) endpoint.Middleware {\n# Natural Language Annotations: // NewCircuitBreakerMW creates a circuit breaker MW using the given Control strategy and Panel.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"github.com/bytedance/gopkg/cloud/circuitbreaker\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\nfunc NewCircuitBreakerMW(control Control, panel circuitbreaker.Panel) endpoint.Middleware {\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) (err error) {\n\t\t\tkey, enabled := control.GetKey(ctx, request)\n\t\t\tif !enabled {\n\t\t\t\treturn next(ctx, request, response)\n\t\t\t}\n\n\t\t\t// Check the circuit breaker status\n\t\t\tif panel.IsTriggered(key) {\n\t\t\t\treturn control.DecorateError(ctx, request, kerrors.ErrCircuitBreak)\n\t\t\t}\n\n\t\t\terr = next(ctx, request, response)\n\t\t\tRecordStat(ctx, request, response, err, key, &control, panel)\n\t\t\treturn err\n\t\t}\n\t}\n}\n```", "```go\nfunc NewCircuitBreakerMW(control Control, panel circuitbreaker.Panel) endpoint.Middleware {\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) (err error) {\n\t\t\tcbKey, enabled := control.GetKey(ctx, request)\n\t\t\tif enabled && panel.IsOpen(cbKey) {\n\t\t\t\tdecoratedErr := control.DecorateError(ctx, request, kerrors.ErrCircuitBreak)\n\t\t\t\treturn decoratedErr\n\t\t\t}\n\n\t\t\tdefer func() {\n\t\t\t\tif control.GetErrorType(ctx, request, response, err) == TypeFailure {\n\t\t\t\t\tpanel.Fail(cbKey)\n\t\t\t\t} else {\n\t\t\t\t\tpanel.Succeed(cbKey)\n\t\t\t\t}\n\t\t\t}()\n\n\t\t\terr = next(ctx, request, response)\n\t\t\treturn err\n\t\t}\n\t}\n}\n```", "```go\nfunc NewCircuitBreakerMW(control Control, panel circuitbreaker.Panel) endpoint.Middleware {\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) (err error) {\n\t\t\tcbKey, enabled := control.GetKey(ctx, request)\n\t\t\tif !enabled {\n\t\t\t\treturn next(ctx, request, response)\n\t\t\t}\n\n\t\t\terr = panel.Call(cbKey, func() error {\n\t\t\t\treturn next(ctx, request, response)\n\t\t\t})\n\n\t\t\tif err != nil {\n\t\t\t\tif errors.Is(err, circuitbreaker.ErrCircuitOpen) {\n\t\t\t\t\terr = kerrors.ErrCircuitBreak\n\t\t\t\t}\n\n\t\t\t\terr = control.DecorateError(ctx, request, err)\n\t\t\t}\n\n\t\t\tRecordStat(ctx, request, response, err, cbKey, &control, panel)\n\n\t\t\treturn err\n\t\t}\n\t}\n}\n```"]}, "kitex-pkg/circuitbreak/cbsuite.go-ServiceCBMW": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/circuitbreak/cbsuite.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/bytedance/gopkg/cloud/circuitbreaker\"\n\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\nconst (\n\tserviceCBKey  = \"service\"\n\tinstanceCBKey = \"instance\"\n\tcbConfig      = \"cb_config\"\n)\n\nvar defaultCBConfig = CBConfig{Enable: true, ErrRate: 0.5, MinSample: 200}\n\n// GetDefaultCBConfig return defaultConfig of CircuitBreaker.\nfunc GetDefaultCBConfig() CBConfig {\n\treturn defaultCBConfig\n}\n\n// CBConfig is policy config of CircuitBreaker.\n// DON'T FORGET to update DeepCopy() and Equals() if you add new fields.\ntype CBConfig struct {\n\tEnable    bool    `json:\"enable\"`\n\tErrRate   float64 `json:\"err_rate\"`\n\tMinSample int64   `json:\"min_sample\"`\n}\n\n// DeepCopy returns a full copy of CBConfig.\nfunc (c *CBConfig) DeepCopy() *CBConfig {\n\tif c == nil {\n\t\treturn nil\n\t}\n\treturn &CBConfig{\n\t\tEnable:    c.Enable,\n\t\tErrRate:   c.ErrRate,\n\t\tMinSample: c.MinSample,\n\t}\n}\n\nfunc (c *CBConfig) Equals(other *CBConfig) bool {\n\tif c == nil && other == nil {\n\t\treturn true\n\t}\n\tif c == nil || other == nil {\n\t\treturn false\n\t}\n\treturn c.Enable == other.Enable && c.ErrRate == other.ErrRate && c.MinSample == other.MinSample\n}\n\n// GenServiceCBKeyFunc to generate circuit breaker key through rpcinfo.\n// You can customize the config key according to your config center.\ntype GenServiceCBKeyFunc func(ri rpcinfo.RPCInfo) string\n\ntype instanceCBConfig struct {\n\tCBConfig\n\tsync.RWMutex\n}\n\n// CBSuite is default wrapper of CircuitBreaker. If you don't have customized policy, you can specify CircuitBreaker\n// middlewares like this:\n//\n//\tcbs := NewCBSuite(GenServiceCBKeyFunc)\n//\topts = append(opts, client.WithCircuitBreaker(cbs))\ntype CBSuite struct {\n\tservicePanel    circuitbreaker.Panel\n\tserviceControl  *Control\n\tinstancePanel   circuitbreaker.Panel\n\tinstanceControl *Control\n\n\tgenServiceCBKey GenServiceCBKeyFunc\n\tserviceCBConfig sync.Map // map[serviceCBKey]CBConfig\n\n\tinstanceCBConfig instanceCBConfig\n\n\tevents event.Queue\n\n\tconfig CBSuiteConfig\n}\n\n// NewCBSuite to build a new CBSuite.\n// Notice: Should NewCBSuite for every client in this version,\n// because event.Queue and event.Bus are not shared with all clients now.\nfunc NewCBSuite(genKey GenServiceCBKeyFunc, options ...CBSuiteOption) *CBSuite {\n\ts := &CBSuite{\n\t\tgenServiceCBKey: genKey,\n\t\tinstanceCBConfig: instanceCBConfig{\n\t\t\tCBConfig: defaultCBConfig,\n\t\t},\n\t\tconfig: CBSuiteConfig{\n\t\t\tserviceGetErrorTypeFunc:  ErrorTypeOnServiceLevel,\n\t\t\tinstanceGetErrorTypeFunc: ErrorTypeOnInstanceLevel,\n\t\t},\n\t}\n\tfor _, option := range options {\n\t\toption(&s.config)\n\t}\n\treturn s\n}\n\n// ServiceCBMW return a new service level CircuitBreakerMW.\n\n\n\n\n\n\n\n\n// InstanceCBMW return a new instance level CircuitBreakerMW.\n\n\n\n\n\n\n\n\n// ServicePanel return cb Panel of service\nfunc (s *CBSuite) ServicePanel() circuitbreaker.Panel {\n\tif s.servicePanel == nil {\n\t\ts.initServiceCB()\n\t}\n\treturn s.servicePanel\n}\n\n// ServiceControl return cb Control of service\nfunc (s *CBSuite) ServiceControl() *Control {\n\tif s.serviceControl == nil {\n\t\ts.initServiceCB()\n\t}\n\treturn s.serviceControl\n}\n\n// UpdateServiceCBConfig is to update service CircuitBreaker config.\n// This func is suggested to be called in remote config module.\nfunc (s *CBSuite) UpdateServiceCBConfig(key string, cfg CBConfig) {\n\ts.serviceCBConfig.Store(key, cfg)\n}\n\n// UpdateInstanceCBConfig is to update instance CircuitBreaker param.\n// This func is suggested to be called in remote config module.\nfunc (s *CBSuite) UpdateInstanceCBConfig(cfg CBConfig) {\n\ts.instanceCBConfig.Lock()\n\ts.instanceCBConfig.CBConfig = cfg\n\ts.instanceCBConfig.Unlock()\n}\n\n// SetEventBusAndQueue is to make CircuitBreaker relate to event change.\nfunc (s *CBSuite) SetEventBusAndQueue(bus event.Bus, events event.Queue) {\n\ts.events = events\n\tif bus != nil {\n\t\tbus.Watch(discovery.ChangeEventName, s.discoveryChangeHandler)\n\t}\n}\n\n// Dump is to dump CircuitBreaker info for debug query.\n\n\n\n\n\n\n\n\n// Close circuitbreaker.Panel to release associated resources.\nfunc (s *CBSuite) Close() error {\n\tif s.servicePanel != nil {\n\t\ts.servicePanel.Close()\n\t\ts.servicePanel = nil\n\t\ts.serviceControl = nil\n\t}\n\tif s.instancePanel != nil {\n\t\ts.instancePanel.Close()\n\t\ts.instancePanel = nil\n\t\ts.instanceControl = nil\n\t}\n\treturn nil\n}\n\nfunc (s *CBSuite) initServiceCB() {\n\tif s.servicePanel != nil && s.serviceControl != nil {\n\t\treturn\n\t}\n\tif s.genServiceCBKey == nil {\n\t\ts.genServiceCBKey = RPCInfo2Key\n\t}\n\topts := circuitbreaker.Options{\n\t\tShouldTripWithKey: s.svcTripFunc,\n\t}\n\ts.servicePanel, _ = circuitbreaker.NewPanel(s.onServiceStateChange, opts)\n\n\tsvcKey := func(ctx context.Context, request interface{}) (serviceCBKey string, enabled bool) {\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tserviceCBKey = s.genServiceCBKey(ri)\n\t\tcbConfig, _ := s.serviceCBConfig.LoadOrStore(serviceCBKey, defaultCBConfig)\n\t\tenabled = cbConfig.(CBConfig).Enable\n\t\treturn\n\t}\n\ts.serviceControl = &Control{\n\t\tGetKey:       svcKey,\n\t\tGetErrorType: s.config.serviceGetErrorTypeFunc,\n\t\tDecorateError: func(ctx context.Context, request interface{}, err error) error {\n\t\t\treturn kerrors.ErrServiceCircuitBreak\n\t\t},\n\t}\n}\n\nfunc (s *CBSuite) initInstanceCB() {\n\tif s.instancePanel != nil && s.instanceControl != nil {\n\t\treturn\n\t}\n\topts := circuitbreaker.Options{\n\t\tShouldTripWithKey: s.insTripFunc,\n\t}\n\ts.instancePanel, _ = circuitbreaker.NewPanel(s.onInstanceStateChange, opts)\n\n\tinstanceKey := func(ctx context.Context, request interface{}) (instCBKey string, enabled bool) {\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tinstCBKey = ri.To().Address().String()\n\t\ts.instanceCBConfig.RLock()\n\t\tenabled = s.instanceCBConfig.Enable\n\t\ts.instanceCBConfig.RUnlock()\n\t\treturn\n\t}\n\ts.instanceControl = &Control{\n\t\tGetKey:       instanceKey,\n\t\tGetErrorType: s.config.instanceGetErrorTypeFunc,\n\t\tDecorateError: func(ctx context.Context, request interface{}, err error) error {\n\t\t\treturn kerrors.ErrInstanceCircuitBreak\n\t\t},\n\t}\n}\n\nfunc (s *CBSuite) onStateChange(level, key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\tif s.events == nil {\n\t\treturn\n\t}\n\tsuccesses, failures, timeouts := m.Counts()\n\tvar errRate float64\n\tif sum := successes + failures + timeouts; sum > 0 {\n\t\terrRate = float64(failures+timeouts) / float64(sum)\n\t}\n\ts.events.Push(&event.Event{\n\t\tName: level + \"_cb\",\n\t\tTime: time.Now(),\n\t\tDetail: fmt.Sprintf(\"%s: %s -> %s, (succ: %d, err: %d, timeout: %d, rate: %f)\",\n\t\t\tkey, oldState, newState, successes, failures, timeouts, errRate),\n\t})\n}\n\nfunc (s *CBSuite) onServiceStateChange(key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\ts.onStateChange(serviceCBKey, key, oldState, newState, m)\n}\n\nfunc (s *CBSuite) onInstanceStateChange(key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\ts.onStateChange(instanceCBKey, key, oldState, newState, m)\n}\n\nfunc (s *CBSuite) discoveryChangeHandler(e *event.Event) {\n\tif s.instancePanel == nil {\n\t\treturn\n\t}\n\textra := e.Extra.(*discovery.Change)\n\tfor i := range extra.Removed {\n\t\tinstCBKey := extra.Removed[i].Address().String()\n\t\ts.instancePanel.RemoveBreaker(instCBKey)\n\t}\n}\n\nfunc (s *CBSuite) svcTripFunc(key string) circuitbreaker.TripFunc {\n\tpi, _ := s.serviceCBConfig.LoadOrStore(key, defaultCBConfig)\n\tp := pi.(CBConfig)\n\treturn circuitbreaker.RateTripFunc(p.ErrRate, p.MinSample)\n}\n\nfunc (s *CBSuite) insTripFunc(key string) circuitbreaker.TripFunc {\n\ts.instanceCBConfig.RLock()\n\terrRate := s.instanceCBConfig.ErrRate\n\tminSample := s.instanceCBConfig.MinSample\n\ts.instanceCBConfig.RUnlock()\n\treturn circuitbreaker.RateTripFunc(errRate, minSample)\n}\n\nfunc cbDebugInfo(panel circuitbreaker.Panel) map[string]interface{} {\n\tdumper, ok := panel.(interface {\n\t\tDumpBreakers() map[string]circuitbreaker.Breaker\n\t})\n\tif !ok {\n\t\treturn nil\n\t}\n\tcbMap := make(map[string]interface{})\n\tfor key, breaker := range dumper.DumpBreakers() {\n\t\tcbState := breaker.State()\n\t\tif cbState == circuitbreaker.Closed {\n\t\t\tcontinue\n\t\t}\n\t\tcbMap[key] = map[string]interface{}{\n\t\t\t\"state\":             cbState,\n\t\t\t\"successes in 10s\":  breaker.Metricer().Successes(),\n\t\t\t\"failures in 10s\":   breaker.Metricer().Failures(),\n\t\t\t\"timeouts in 10s\":   breaker.Metricer().Timeouts(),\n\t\t\t\"error rate in 10s\": breaker.Metricer().ErrorRate(),\n\t\t}\n\t}\n\tif len(cbMap) == 0 {\n\t\tcbMap[\"msg\"] = \"all circuit breakers are in closed state\"\n\t}\n\treturn cbMap\n}\n\nfunc (s *CBSuite) configInfo() map[string]interface{} {\n\tsvcCBMap := make(map[string]interface{})\n\ts.serviceCBConfig.Range(func(key, value interface{}) bool {\n\t\tsvcCBMap[key.(string)] = value\n\t\treturn true\n\t})\n\ts.instanceCBConfig.RLock()\n\tinstCBConfig := s.instanceCBConfig.CBConfig\n\ts.instanceCBConfig.RUnlock()\n\n\tcbMap := make(map[string]interface{}, 2)\n\tcbMap[serviceCBKey] = svcCBMap\n\tcbMap[instanceCBKey] = instCBConfig\n\treturn cbMap\n}\n\n// RPCInfo2Key is to generate circuit breaker key through rpcinfo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (s *CBSuite) ServiceCBMW() endpoint.Middleware {\n# Natural Language Annotations: // ServiceCBMW return a new service level CircuitBreakerMW.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (s *CBSuite) ServiceCBMW() endpoint.Middleware {\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) (err error) {\n\t\t\ts.initServiceCB()\n\n\t\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\t\tserviceCBKey := s.genServiceCBKey(ri)\n\t\t\tserviceCBConfig, _ := s.serviceCBConfig.LoadOrStore(serviceCBKey, defaultCBConfig)\n\t\t\tserviceCB := s.ServicePanel().Get(serviceCBKey)\n\n\t\t\tif !serviceCB.Allow() {\n\t\t\t\treturn kerrors.ErrServiceCircuitBreak\n\t\t\t}\n\n\t\t\terr = next(ctx, request, response)\n\t\t\tserviceCB.Observe(err)\n\t\t\treturn\n\t\t}\n\t}\n}\n```", "```go\nfunc (s *CBSuite) ServiceCBMW() endpoint.Middleware {\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\t\tserviceCBKey := s.genServiceCBKey(ri)\n\t\t\tcbConfig, _ := s.serviceCBConfig.LoadOrStore(serviceCBKey, defaultCBConfig)\n\n\t\t\tcb, _ := s.ServicePanel().GetBreaker(serviceCBKey)\n\t\t\tif cb.Allow() {\n\t\t\t\terr := next(ctx, request, response)\n\t\t\t\tif err != nil {\n\t\t\t\t\tcb.MarkFail()\n\t\t\t\t} else {\n\t\t\t\t\tcb.MarkSuccess()\n\t\t\t\t}\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\treturn kerrors.ErrServiceCircuitBreak\n\t\t}\n\t}\n}\n```\nIn this code, the ServiceCBMW is a function that returns a Middleware. This Middleware is an endpoint.Endpoint that wraps the original endpoint.Endpoint and adds the CircuitBreaker functionality. If the CircuitBreaker allows the request to pass, it calls the original endpoint. If there's an error, it marks the request as failed. If the CircuitBreaker doesn't allow the request to pass, it returns a service-level CircuitBreak error.", "```go\nfunc (s *CBSuite) ServiceCBMW() endpoint.Middleware {\n    s.initServiceCB()\n    return func(next endpoint.Endpoint) endpoint.Endpoint {\n        return func(ctx context.Context, request, response interface{}) error {\n            ri := rpcinfo.GetRPCInfo(ctx)\n            ctrl := s.ServiceControl()\n            if ctrl == nil {\n                return next(ctx, request, response)\n            }\n            key, enabled := ctrl.GetKey(ctx, request)\n            if !enabled {\n                return next(ctx, request, response)\n            }\n            cb := s.ServicePanel().GetBreaker(key)\n            if err := cb.Call(func() error {\n                return next(ctx, request, response)\n            }, 0); err != nil {\n                return ctrl.DecorateError(ctx, request, err)\n            }\n            return nil\n        }\n    }\n}\n```"]}, "kitex-pkg/circuitbreak/cbsuite.go-InstanceCBMW": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/circuitbreak/cbsuite.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/bytedance/gopkg/cloud/circuitbreaker\"\n\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\nconst (\n\tserviceCBKey  = \"service\"\n\tinstanceCBKey = \"instance\"\n\tcbConfig      = \"cb_config\"\n)\n\nvar defaultCBConfig = CBConfig{Enable: true, ErrRate: 0.5, MinSample: 200}\n\n// GetDefaultCBConfig return defaultConfig of CircuitBreaker.\nfunc GetDefaultCBConfig() CBConfig {\n\treturn defaultCBConfig\n}\n\n// CBConfig is policy config of CircuitBreaker.\n// DON'T FORGET to update DeepCopy() and Equals() if you add new fields.\ntype CBConfig struct {\n\tEnable    bool    `json:\"enable\"`\n\tErrRate   float64 `json:\"err_rate\"`\n\tMinSample int64   `json:\"min_sample\"`\n}\n\n// DeepCopy returns a full copy of CBConfig.\nfunc (c *CBConfig) DeepCopy() *CBConfig {\n\tif c == nil {\n\t\treturn nil\n\t}\n\treturn &CBConfig{\n\t\tEnable:    c.Enable,\n\t\tErrRate:   c.ErrRate,\n\t\tMinSample: c.MinSample,\n\t}\n}\n\nfunc (c *CBConfig) Equals(other *CBConfig) bool {\n\tif c == nil && other == nil {\n\t\treturn true\n\t}\n\tif c == nil || other == nil {\n\t\treturn false\n\t}\n\treturn c.Enable == other.Enable && c.ErrRate == other.ErrRate && c.MinSample == other.MinSample\n}\n\n// GenServiceCBKeyFunc to generate circuit breaker key through rpcinfo.\n// You can customize the config key according to your config center.\ntype GenServiceCBKeyFunc func(ri rpcinfo.RPCInfo) string\n\ntype instanceCBConfig struct {\n\tCBConfig\n\tsync.RWMutex\n}\n\n// CBSuite is default wrapper of CircuitBreaker. If you don't have customized policy, you can specify CircuitBreaker\n// middlewares like this:\n//\n//\tcbs := NewCBSuite(GenServiceCBKeyFunc)\n//\topts = append(opts, client.WithCircuitBreaker(cbs))\ntype CBSuite struct {\n\tservicePanel    circuitbreaker.Panel\n\tserviceControl  *Control\n\tinstancePanel   circuitbreaker.Panel\n\tinstanceControl *Control\n\n\tgenServiceCBKey GenServiceCBKeyFunc\n\tserviceCBConfig sync.Map // map[serviceCBKey]CBConfig\n\n\tinstanceCBConfig instanceCBConfig\n\n\tevents event.Queue\n\n\tconfig CBSuiteConfig\n}\n\n// NewCBSuite to build a new CBSuite.\n// Notice: Should NewCBSuite for every client in this version,\n// because event.Queue and event.Bus are not shared with all clients now.\nfunc NewCBSuite(genKey GenServiceCBKeyFunc, options ...CBSuiteOption) *CBSuite {\n\ts := &CBSuite{\n\t\tgenServiceCBKey: genKey,\n\t\tinstanceCBConfig: instanceCBConfig{\n\t\t\tCBConfig: defaultCBConfig,\n\t\t},\n\t\tconfig: CBSuiteConfig{\n\t\t\tserviceGetErrorTypeFunc:  ErrorTypeOnServiceLevel,\n\t\t\tinstanceGetErrorTypeFunc: ErrorTypeOnInstanceLevel,\n\t\t},\n\t}\n\tfor _, option := range options {\n\t\toption(&s.config)\n\t}\n\treturn s\n}\n\n// ServiceCBMW return a new service level CircuitBreakerMW.\n\n\n\n\n\n\n\n\n// InstanceCBMW return a new instance level CircuitBreakerMW.\n\n\n\n\n\n\n\n\n// ServicePanel return cb Panel of service\nfunc (s *CBSuite) ServicePanel() circuitbreaker.Panel {\n\tif s.servicePanel == nil {\n\t\ts.initServiceCB()\n\t}\n\treturn s.servicePanel\n}\n\n// ServiceControl return cb Control of service\nfunc (s *CBSuite) ServiceControl() *Control {\n\tif s.serviceControl == nil {\n\t\ts.initServiceCB()\n\t}\n\treturn s.serviceControl\n}\n\n// UpdateServiceCBConfig is to update service CircuitBreaker config.\n// This func is suggested to be called in remote config module.\nfunc (s *CBSuite) UpdateServiceCBConfig(key string, cfg CBConfig) {\n\ts.serviceCBConfig.Store(key, cfg)\n}\n\n// UpdateInstanceCBConfig is to update instance CircuitBreaker param.\n// This func is suggested to be called in remote config module.\nfunc (s *CBSuite) UpdateInstanceCBConfig(cfg CBConfig) {\n\ts.instanceCBConfig.Lock()\n\ts.instanceCBConfig.CBConfig = cfg\n\ts.instanceCBConfig.Unlock()\n}\n\n// SetEventBusAndQueue is to make CircuitBreaker relate to event change.\nfunc (s *CBSuite) SetEventBusAndQueue(bus event.Bus, events event.Queue) {\n\ts.events = events\n\tif bus != nil {\n\t\tbus.Watch(discovery.ChangeEventName, s.discoveryChangeHandler)\n\t}\n}\n\n// Dump is to dump CircuitBreaker info for debug query.\n\n\n\n\n\n\n\n\n// Close circuitbreaker.Panel to release associated resources.\nfunc (s *CBSuite) Close() error {\n\tif s.servicePanel != nil {\n\t\ts.servicePanel.Close()\n\t\ts.servicePanel = nil\n\t\ts.serviceControl = nil\n\t}\n\tif s.instancePanel != nil {\n\t\ts.instancePanel.Close()\n\t\ts.instancePanel = nil\n\t\ts.instanceControl = nil\n\t}\n\treturn nil\n}\n\nfunc (s *CBSuite) initServiceCB() {\n\tif s.servicePanel != nil && s.serviceControl != nil {\n\t\treturn\n\t}\n\tif s.genServiceCBKey == nil {\n\t\ts.genServiceCBKey = RPCInfo2Key\n\t}\n\topts := circuitbreaker.Options{\n\t\tShouldTripWithKey: s.svcTripFunc,\n\t}\n\ts.servicePanel, _ = circuitbreaker.NewPanel(s.onServiceStateChange, opts)\n\n\tsvcKey := func(ctx context.Context, request interface{}) (serviceCBKey string, enabled bool) {\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tserviceCBKey = s.genServiceCBKey(ri)\n\t\tcbConfig, _ := s.serviceCBConfig.LoadOrStore(serviceCBKey, defaultCBConfig)\n\t\tenabled = cbConfig.(CBConfig).Enable\n\t\treturn\n\t}\n\ts.serviceControl = &Control{\n\t\tGetKey:       svcKey,\n\t\tGetErrorType: s.config.serviceGetErrorTypeFunc,\n\t\tDecorateError: func(ctx context.Context, request interface{}, err error) error {\n\t\t\treturn kerrors.ErrServiceCircuitBreak\n\t\t},\n\t}\n}\n\nfunc (s *CBSuite) initInstanceCB() {\n\tif s.instancePanel != nil && s.instanceControl != nil {\n\t\treturn\n\t}\n\topts := circuitbreaker.Options{\n\t\tShouldTripWithKey: s.insTripFunc,\n\t}\n\ts.instancePanel, _ = circuitbreaker.NewPanel(s.onInstanceStateChange, opts)\n\n\tinstanceKey := func(ctx context.Context, request interface{}) (instCBKey string, enabled bool) {\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tinstCBKey = ri.To().Address().String()\n\t\ts.instanceCBConfig.RLock()\n\t\tenabled = s.instanceCBConfig.Enable\n\t\ts.instanceCBConfig.RUnlock()\n\t\treturn\n\t}\n\ts.instanceControl = &Control{\n\t\tGetKey:       instanceKey,\n\t\tGetErrorType: s.config.instanceGetErrorTypeFunc,\n\t\tDecorateError: func(ctx context.Context, request interface{}, err error) error {\n\t\t\treturn kerrors.ErrInstanceCircuitBreak\n\t\t},\n\t}\n}\n\nfunc (s *CBSuite) onStateChange(level, key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\tif s.events == nil {\n\t\treturn\n\t}\n\tsuccesses, failures, timeouts := m.Counts()\n\tvar errRate float64\n\tif sum := successes + failures + timeouts; sum > 0 {\n\t\terrRate = float64(failures+timeouts) / float64(sum)\n\t}\n\ts.events.Push(&event.Event{\n\t\tName: level + \"_cb\",\n\t\tTime: time.Now(),\n\t\tDetail: fmt.Sprintf(\"%s: %s -> %s, (succ: %d, err: %d, timeout: %d, rate: %f)\",\n\t\t\tkey, oldState, newState, successes, failures, timeouts, errRate),\n\t})\n}\n\nfunc (s *CBSuite) onServiceStateChange(key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\ts.onStateChange(serviceCBKey, key, oldState, newState, m)\n}\n\nfunc (s *CBSuite) onInstanceStateChange(key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\ts.onStateChange(instanceCBKey, key, oldState, newState, m)\n}\n\nfunc (s *CBSuite) discoveryChangeHandler(e *event.Event) {\n\tif s.instancePanel == nil {\n\t\treturn\n\t}\n\textra := e.Extra.(*discovery.Change)\n\tfor i := range extra.Removed {\n\t\tinstCBKey := extra.Removed[i].Address().String()\n\t\ts.instancePanel.RemoveBreaker(instCBKey)\n\t}\n}\n\nfunc (s *CBSuite) svcTripFunc(key string) circuitbreaker.TripFunc {\n\tpi, _ := s.serviceCBConfig.LoadOrStore(key, defaultCBConfig)\n\tp := pi.(CBConfig)\n\treturn circuitbreaker.RateTripFunc(p.ErrRate, p.MinSample)\n}\n\nfunc (s *CBSuite) insTripFunc(key string) circuitbreaker.TripFunc {\n\ts.instanceCBConfig.RLock()\n\terrRate := s.instanceCBConfig.ErrRate\n\tminSample := s.instanceCBConfig.MinSample\n\ts.instanceCBConfig.RUnlock()\n\treturn circuitbreaker.RateTripFunc(errRate, minSample)\n}\n\nfunc cbDebugInfo(panel circuitbreaker.Panel) map[string]interface{} {\n\tdumper, ok := panel.(interface {\n\t\tDumpBreakers() map[string]circuitbreaker.Breaker\n\t})\n\tif !ok {\n\t\treturn nil\n\t}\n\tcbMap := make(map[string]interface{})\n\tfor key, breaker := range dumper.DumpBreakers() {\n\t\tcbState := breaker.State()\n\t\tif cbState == circuitbreaker.Closed {\n\t\t\tcontinue\n\t\t}\n\t\tcbMap[key] = map[string]interface{}{\n\t\t\t\"state\":             cbState,\n\t\t\t\"successes in 10s\":  breaker.Metricer().Successes(),\n\t\t\t\"failures in 10s\":   breaker.Metricer().Failures(),\n\t\t\t\"timeouts in 10s\":   breaker.Metricer().Timeouts(),\n\t\t\t\"error rate in 10s\": breaker.Metricer().ErrorRate(),\n\t\t}\n\t}\n\tif len(cbMap) == 0 {\n\t\tcbMap[\"msg\"] = \"all circuit breakers are in closed state\"\n\t}\n\treturn cbMap\n}\n\nfunc (s *CBSuite) configInfo() map[string]interface{} {\n\tsvcCBMap := make(map[string]interface{})\n\ts.serviceCBConfig.Range(func(key, value interface{}) bool {\n\t\tsvcCBMap[key.(string)] = value\n\t\treturn true\n\t})\n\ts.instanceCBConfig.RLock()\n\tinstCBConfig := s.instanceCBConfig.CBConfig\n\ts.instanceCBConfig.RUnlock()\n\n\tcbMap := make(map[string]interface{}, 2)\n\tcbMap[serviceCBKey] = svcCBMap\n\tcbMap[instanceCBKey] = instCBConfig\n\treturn cbMap\n}\n\n// RPCInfo2Key is to generate circuit breaker key through rpcinfo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (s *CBSuite) InstanceCBMW() endpoint.Middleware {\n# Natural Language Annotations: // InstanceCBMW return a new instance level CircuitBreakerMW.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (s *CBSuite) InstanceCBMW() endpoint.Middleware {\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\tif s.instancePanel == nil {\n\t\t\t\ts.initInstanceCB()\n\t\t\t}\n\t\t\tcb := s.instancePanel.GetBreaker(s.instanceControl.GetKey)\n\t\t\tcb.Protect(func() error {\n\t\t\t\treturn next(ctx, request, response)\n\t\t\t})\n\n\t\t\treturn nil\n\t\t}\n\t}\n}\n```", "```go\nfunc (s *CBSuite) InstanceCBMW() endpoint.Middleware {\n\ts.initInstanceCB()\n\treturn NewCircuitBreakerMW(s.instancePanel, s.instanceControl)\n}\n```", "```go\nfunc (s *CBSuite) InstanceCBMW() endpoint.Middleware {\n\ts.initInstanceCB()\n\tcb := circuitbreaker.NewCircuitBreaker(s.instancePanel, s.instanceControl)\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\terr := cb.Execute(ctx, func() error {\n\t\t\t\treturn next(ctx, request, response)\n\t\t\t}, nil)\n\t\t\treturn err\n\t\t}\n\t}\n}\n```"]}, "kitex-pkg/circuitbreak/cbsuite.go-Dump": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/circuitbreak/cbsuite.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/bytedance/gopkg/cloud/circuitbreaker\"\n\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\nconst (\n\tserviceCBKey  = \"service\"\n\tinstanceCBKey = \"instance\"\n\tcbConfig      = \"cb_config\"\n)\n\nvar defaultCBConfig = CBConfig{Enable: true, ErrRate: 0.5, MinSample: 200}\n\n// GetDefaultCBConfig return defaultConfig of CircuitBreaker.\nfunc GetDefaultCBConfig() CBConfig {\n\treturn defaultCBConfig\n}\n\n// CBConfig is policy config of CircuitBreaker.\n// DON'T FORGET to update DeepCopy() and Equals() if you add new fields.\ntype CBConfig struct {\n\tEnable    bool    `json:\"enable\"`\n\tErrRate   float64 `json:\"err_rate\"`\n\tMinSample int64   `json:\"min_sample\"`\n}\n\n// DeepCopy returns a full copy of CBConfig.\nfunc (c *CBConfig) DeepCopy() *CBConfig {\n\tif c == nil {\n\t\treturn nil\n\t}\n\treturn &CBConfig{\n\t\tEnable:    c.Enable,\n\t\tErrRate:   c.ErrRate,\n\t\tMinSample: c.MinSample,\n\t}\n}\n\nfunc (c *CBConfig) Equals(other *CBConfig) bool {\n\tif c == nil && other == nil {\n\t\treturn true\n\t}\n\tif c == nil || other == nil {\n\t\treturn false\n\t}\n\treturn c.Enable == other.Enable && c.ErrRate == other.ErrRate && c.MinSample == other.MinSample\n}\n\n// GenServiceCBKeyFunc to generate circuit breaker key through rpcinfo.\n// You can customize the config key according to your config center.\ntype GenServiceCBKeyFunc func(ri rpcinfo.RPCInfo) string\n\ntype instanceCBConfig struct {\n\tCBConfig\n\tsync.RWMutex\n}\n\n// CBSuite is default wrapper of CircuitBreaker. If you don't have customized policy, you can specify CircuitBreaker\n// middlewares like this:\n//\n//\tcbs := NewCBSuite(GenServiceCBKeyFunc)\n//\topts = append(opts, client.WithCircuitBreaker(cbs))\ntype CBSuite struct {\n\tservicePanel    circuitbreaker.Panel\n\tserviceControl  *Control\n\tinstancePanel   circuitbreaker.Panel\n\tinstanceControl *Control\n\n\tgenServiceCBKey GenServiceCBKeyFunc\n\tserviceCBConfig sync.Map // map[serviceCBKey]CBConfig\n\n\tinstanceCBConfig instanceCBConfig\n\n\tevents event.Queue\n\n\tconfig CBSuiteConfig\n}\n\n// NewCBSuite to build a new CBSuite.\n// Notice: Should NewCBSuite for every client in this version,\n// because event.Queue and event.Bus are not shared with all clients now.\nfunc NewCBSuite(genKey GenServiceCBKeyFunc, options ...CBSuiteOption) *CBSuite {\n\ts := &CBSuite{\n\t\tgenServiceCBKey: genKey,\n\t\tinstanceCBConfig: instanceCBConfig{\n\t\t\tCBConfig: defaultCBConfig,\n\t\t},\n\t\tconfig: CBSuiteConfig{\n\t\t\tserviceGetErrorTypeFunc:  ErrorTypeOnServiceLevel,\n\t\t\tinstanceGetErrorTypeFunc: ErrorTypeOnInstanceLevel,\n\t\t},\n\t}\n\tfor _, option := range options {\n\t\toption(&s.config)\n\t}\n\treturn s\n}\n\n// ServiceCBMW return a new service level CircuitBreakerMW.\n\n\n\n\n\n\n\n\n// InstanceCBMW return a new instance level CircuitBreakerMW.\n\n\n\n\n\n\n\n\n// ServicePanel return cb Panel of service\nfunc (s *CBSuite) ServicePanel() circuitbreaker.Panel {\n\tif s.servicePanel == nil {\n\t\ts.initServiceCB()\n\t}\n\treturn s.servicePanel\n}\n\n// ServiceControl return cb Control of service\nfunc (s *CBSuite) ServiceControl() *Control {\n\tif s.serviceControl == nil {\n\t\ts.initServiceCB()\n\t}\n\treturn s.serviceControl\n}\n\n// UpdateServiceCBConfig is to update service CircuitBreaker config.\n// This func is suggested to be called in remote config module.\nfunc (s *CBSuite) UpdateServiceCBConfig(key string, cfg CBConfig) {\n\ts.serviceCBConfig.Store(key, cfg)\n}\n\n// UpdateInstanceCBConfig is to update instance CircuitBreaker param.\n// This func is suggested to be called in remote config module.\nfunc (s *CBSuite) UpdateInstanceCBConfig(cfg CBConfig) {\n\ts.instanceCBConfig.Lock()\n\ts.instanceCBConfig.CBConfig = cfg\n\ts.instanceCBConfig.Unlock()\n}\n\n// SetEventBusAndQueue is to make CircuitBreaker relate to event change.\nfunc (s *CBSuite) SetEventBusAndQueue(bus event.Bus, events event.Queue) {\n\ts.events = events\n\tif bus != nil {\n\t\tbus.Watch(discovery.ChangeEventName, s.discoveryChangeHandler)\n\t}\n}\n\n// Dump is to dump CircuitBreaker info for debug query.\n\n\n\n\n\n\n\n\n// Close circuitbreaker.Panel to release associated resources.\nfunc (s *CBSuite) Close() error {\n\tif s.servicePanel != nil {\n\t\ts.servicePanel.Close()\n\t\ts.servicePanel = nil\n\t\ts.serviceControl = nil\n\t}\n\tif s.instancePanel != nil {\n\t\ts.instancePanel.Close()\n\t\ts.instancePanel = nil\n\t\ts.instanceControl = nil\n\t}\n\treturn nil\n}\n\nfunc (s *CBSuite) initServiceCB() {\n\tif s.servicePanel != nil && s.serviceControl != nil {\n\t\treturn\n\t}\n\tif s.genServiceCBKey == nil {\n\t\ts.genServiceCBKey = RPCInfo2Key\n\t}\n\topts := circuitbreaker.Options{\n\t\tShouldTripWithKey: s.svcTripFunc,\n\t}\n\ts.servicePanel, _ = circuitbreaker.NewPanel(s.onServiceStateChange, opts)\n\n\tsvcKey := func(ctx context.Context, request interface{}) (serviceCBKey string, enabled bool) {\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tserviceCBKey = s.genServiceCBKey(ri)\n\t\tcbConfig, _ := s.serviceCBConfig.LoadOrStore(serviceCBKey, defaultCBConfig)\n\t\tenabled = cbConfig.(CBConfig).Enable\n\t\treturn\n\t}\n\ts.serviceControl = &Control{\n\t\tGetKey:       svcKey,\n\t\tGetErrorType: s.config.serviceGetErrorTypeFunc,\n\t\tDecorateError: func(ctx context.Context, request interface{}, err error) error {\n\t\t\treturn kerrors.ErrServiceCircuitBreak\n\t\t},\n\t}\n}\n\nfunc (s *CBSuite) initInstanceCB() {\n\tif s.instancePanel != nil && s.instanceControl != nil {\n\t\treturn\n\t}\n\topts := circuitbreaker.Options{\n\t\tShouldTripWithKey: s.insTripFunc,\n\t}\n\ts.instancePanel, _ = circuitbreaker.NewPanel(s.onInstanceStateChange, opts)\n\n\tinstanceKey := func(ctx context.Context, request interface{}) (instCBKey string, enabled bool) {\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tinstCBKey = ri.To().Address().String()\n\t\ts.instanceCBConfig.RLock()\n\t\tenabled = s.instanceCBConfig.Enable\n\t\ts.instanceCBConfig.RUnlock()\n\t\treturn\n\t}\n\ts.instanceControl = &Control{\n\t\tGetKey:       instanceKey,\n\t\tGetErrorType: s.config.instanceGetErrorTypeFunc,\n\t\tDecorateError: func(ctx context.Context, request interface{}, err error) error {\n\t\t\treturn kerrors.ErrInstanceCircuitBreak\n\t\t},\n\t}\n}\n\nfunc (s *CBSuite) onStateChange(level, key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\tif s.events == nil {\n\t\treturn\n\t}\n\tsuccesses, failures, timeouts := m.Counts()\n\tvar errRate float64\n\tif sum := successes + failures + timeouts; sum > 0 {\n\t\terrRate = float64(failures+timeouts) / float64(sum)\n\t}\n\ts.events.Push(&event.Event{\n\t\tName: level + \"_cb\",\n\t\tTime: time.Now(),\n\t\tDetail: fmt.Sprintf(\"%s: %s -> %s, (succ: %d, err: %d, timeout: %d, rate: %f)\",\n\t\t\tkey, oldState, newState, successes, failures, timeouts, errRate),\n\t})\n}\n\nfunc (s *CBSuite) onServiceStateChange(key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\ts.onStateChange(serviceCBKey, key, oldState, newState, m)\n}\n\nfunc (s *CBSuite) onInstanceStateChange(key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\ts.onStateChange(instanceCBKey, key, oldState, newState, m)\n}\n\nfunc (s *CBSuite) discoveryChangeHandler(e *event.Event) {\n\tif s.instancePanel == nil {\n\t\treturn\n\t}\n\textra := e.Extra.(*discovery.Change)\n\tfor i := range extra.Removed {\n\t\tinstCBKey := extra.Removed[i].Address().String()\n\t\ts.instancePanel.RemoveBreaker(instCBKey)\n\t}\n}\n\nfunc (s *CBSuite) svcTripFunc(key string) circuitbreaker.TripFunc {\n\tpi, _ := s.serviceCBConfig.LoadOrStore(key, defaultCBConfig)\n\tp := pi.(CBConfig)\n\treturn circuitbreaker.RateTripFunc(p.ErrRate, p.MinSample)\n}\n\nfunc (s *CBSuite) insTripFunc(key string) circuitbreaker.TripFunc {\n\ts.instanceCBConfig.RLock()\n\terrRate := s.instanceCBConfig.ErrRate\n\tminSample := s.instanceCBConfig.MinSample\n\ts.instanceCBConfig.RUnlock()\n\treturn circuitbreaker.RateTripFunc(errRate, minSample)\n}\n\nfunc cbDebugInfo(panel circuitbreaker.Panel) map[string]interface{} {\n\tdumper, ok := panel.(interface {\n\t\tDumpBreakers() map[string]circuitbreaker.Breaker\n\t})\n\tif !ok {\n\t\treturn nil\n\t}\n\tcbMap := make(map[string]interface{})\n\tfor key, breaker := range dumper.DumpBreakers() {\n\t\tcbState := breaker.State()\n\t\tif cbState == circuitbreaker.Closed {\n\t\t\tcontinue\n\t\t}\n\t\tcbMap[key] = map[string]interface{}{\n\t\t\t\"state\":             cbState,\n\t\t\t\"successes in 10s\":  breaker.Metricer().Successes(),\n\t\t\t\"failures in 10s\":   breaker.Metricer().Failures(),\n\t\t\t\"timeouts in 10s\":   breaker.Metricer().Timeouts(),\n\t\t\t\"error rate in 10s\": breaker.Metricer().ErrorRate(),\n\t\t}\n\t}\n\tif len(cbMap) == 0 {\n\t\tcbMap[\"msg\"] = \"all circuit breakers are in closed state\"\n\t}\n\treturn cbMap\n}\n\nfunc (s *CBSuite) configInfo() map[string]interface{} {\n\tsvcCBMap := make(map[string]interface{})\n\ts.serviceCBConfig.Range(func(key, value interface{}) bool {\n\t\tsvcCBMap[key.(string)] = value\n\t\treturn true\n\t})\n\ts.instanceCBConfig.RLock()\n\tinstCBConfig := s.instanceCBConfig.CBConfig\n\ts.instanceCBConfig.RUnlock()\n\n\tcbMap := make(map[string]interface{}, 2)\n\tcbMap[serviceCBKey] = svcCBMap\n\tcbMap[instanceCBKey] = instCBConfig\n\treturn cbMap\n}\n\n// RPCInfo2Key is to generate circuit breaker key through rpcinfo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (s *CBSuite) Dump() interface{} {\n# Natural Language Annotations: // Dump is to dump CircuitBreaker info for debug query.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (s *CBSuite) Dump() interface{} {\n\tdump := make(map[string]interface{})\n\tdump[\"service_cb\"] = cbDebugInfo(s.ServicePanel())\n\tdump[\"instance_cb\"] = cbDebugInfo(s.InstancePanel())\n\tdump[\"config\"] = s.configInfo()\n\treturn dump\n}\n```", "```go\nfunc (s *CBSuite) Dump() map[string]interface{} {\n\tinfo := make(map[string]interface{})\n\tinfo[\"config\"] = s.configInfo()\n\tinfo[\"service_cb\"] = cbDebugInfo(s.ServicePanel())\n\tinfo[\"instance_cb\"] = cbDebugInfo(s.InstancePanel())\n\treturn info\n}\n```", "```go\nfunc (s *CBSuite) Dump() interface{} {\n\tdump := make(map[string]interface{})\n\tif s.servicePanel != nil {\n\t\tdump[serviceCBKey] = cbDebugInfo(s.servicePanel)\n\t}\n\tif s.instancePanel != nil {\n\t\tdump[instanceCBKey] = cbDebugInfo(s.instancePanel)\n\t}\n\tdump[cbConfig] = s.configInfo()\n\treturn dump\n}\n```"]}, "kitex-pkg/circuitbreak/cbsuite.go-RPCInfo2Key": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/circuitbreak/cbsuite.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/bytedance/gopkg/cloud/circuitbreaker\"\n\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\nconst (\n\tserviceCBKey  = \"service\"\n\tinstanceCBKey = \"instance\"\n\tcbConfig      = \"cb_config\"\n)\n\nvar defaultCBConfig = CBConfig{Enable: true, ErrRate: 0.5, MinSample: 200}\n\n// GetDefaultCBConfig return defaultConfig of CircuitBreaker.\nfunc GetDefaultCBConfig() CBConfig {\n\treturn defaultCBConfig\n}\n\n// CBConfig is policy config of CircuitBreaker.\n// DON'T FORGET to update DeepCopy() and Equals() if you add new fields.\ntype CBConfig struct {\n\tEnable    bool    `json:\"enable\"`\n\tErrRate   float64 `json:\"err_rate\"`\n\tMinSample int64   `json:\"min_sample\"`\n}\n\n// DeepCopy returns a full copy of CBConfig.\nfunc (c *CBConfig) DeepCopy() *CBConfig {\n\tif c == nil {\n\t\treturn nil\n\t}\n\treturn &CBConfig{\n\t\tEnable:    c.Enable,\n\t\tErrRate:   c.ErrRate,\n\t\tMinSample: c.MinSample,\n\t}\n}\n\nfunc (c *CBConfig) Equals(other *CBConfig) bool {\n\tif c == nil && other == nil {\n\t\treturn true\n\t}\n\tif c == nil || other == nil {\n\t\treturn false\n\t}\n\treturn c.Enable == other.Enable && c.ErrRate == other.ErrRate && c.MinSample == other.MinSample\n}\n\n// GenServiceCBKeyFunc to generate circuit breaker key through rpcinfo.\n// You can customize the config key according to your config center.\ntype GenServiceCBKeyFunc func(ri rpcinfo.RPCInfo) string\n\ntype instanceCBConfig struct {\n\tCBConfig\n\tsync.RWMutex\n}\n\n// CBSuite is default wrapper of CircuitBreaker. If you don't have customized policy, you can specify CircuitBreaker\n// middlewares like this:\n//\n//\tcbs := NewCBSuite(GenServiceCBKeyFunc)\n//\topts = append(opts, client.WithCircuitBreaker(cbs))\ntype CBSuite struct {\n\tservicePanel    circuitbreaker.Panel\n\tserviceControl  *Control\n\tinstancePanel   circuitbreaker.Panel\n\tinstanceControl *Control\n\n\tgenServiceCBKey GenServiceCBKeyFunc\n\tserviceCBConfig sync.Map // map[serviceCBKey]CBConfig\n\n\tinstanceCBConfig instanceCBConfig\n\n\tevents event.Queue\n\n\tconfig CBSuiteConfig\n}\n\n// NewCBSuite to build a new CBSuite.\n// Notice: Should NewCBSuite for every client in this version,\n// because event.Queue and event.Bus are not shared with all clients now.\nfunc NewCBSuite(genKey GenServiceCBKeyFunc, options ...CBSuiteOption) *CBSuite {\n\ts := &CBSuite{\n\t\tgenServiceCBKey: genKey,\n\t\tinstanceCBConfig: instanceCBConfig{\n\t\t\tCBConfig: defaultCBConfig,\n\t\t},\n\t\tconfig: CBSuiteConfig{\n\t\t\tserviceGetErrorTypeFunc:  ErrorTypeOnServiceLevel,\n\t\t\tinstanceGetErrorTypeFunc: ErrorTypeOnInstanceLevel,\n\t\t},\n\t}\n\tfor _, option := range options {\n\t\toption(&s.config)\n\t}\n\treturn s\n}\n\n// ServiceCBMW return a new service level CircuitBreakerMW.\n\n\n\n\n\n\n\n\n// InstanceCBMW return a new instance level CircuitBreakerMW.\n\n\n\n\n\n\n\n\n// ServicePanel return cb Panel of service\nfunc (s *CBSuite) ServicePanel() circuitbreaker.Panel {\n\tif s.servicePanel == nil {\n\t\ts.initServiceCB()\n\t}\n\treturn s.servicePanel\n}\n\n// ServiceControl return cb Control of service\nfunc (s *CBSuite) ServiceControl() *Control {\n\tif s.serviceControl == nil {\n\t\ts.initServiceCB()\n\t}\n\treturn s.serviceControl\n}\n\n// UpdateServiceCBConfig is to update service CircuitBreaker config.\n// This func is suggested to be called in remote config module.\nfunc (s *CBSuite) UpdateServiceCBConfig(key string, cfg CBConfig) {\n\ts.serviceCBConfig.Store(key, cfg)\n}\n\n// UpdateInstanceCBConfig is to update instance CircuitBreaker param.\n// This func is suggested to be called in remote config module.\nfunc (s *CBSuite) UpdateInstanceCBConfig(cfg CBConfig) {\n\ts.instanceCBConfig.Lock()\n\ts.instanceCBConfig.CBConfig = cfg\n\ts.instanceCBConfig.Unlock()\n}\n\n// SetEventBusAndQueue is to make CircuitBreaker relate to event change.\nfunc (s *CBSuite) SetEventBusAndQueue(bus event.Bus, events event.Queue) {\n\ts.events = events\n\tif bus != nil {\n\t\tbus.Watch(discovery.ChangeEventName, s.discoveryChangeHandler)\n\t}\n}\n\n// Dump is to dump CircuitBreaker info for debug query.\n\n\n\n\n\n\n\n\n// Close circuitbreaker.Panel to release associated resources.\nfunc (s *CBSuite) Close() error {\n\tif s.servicePanel != nil {\n\t\ts.servicePanel.Close()\n\t\ts.servicePanel = nil\n\t\ts.serviceControl = nil\n\t}\n\tif s.instancePanel != nil {\n\t\ts.instancePanel.Close()\n\t\ts.instancePanel = nil\n\t\ts.instanceControl = nil\n\t}\n\treturn nil\n}\n\nfunc (s *CBSuite) initServiceCB() {\n\tif s.servicePanel != nil && s.serviceControl != nil {\n\t\treturn\n\t}\n\tif s.genServiceCBKey == nil {\n\t\ts.genServiceCBKey = RPCInfo2Key\n\t}\n\topts := circuitbreaker.Options{\n\t\tShouldTripWithKey: s.svcTripFunc,\n\t}\n\ts.servicePanel, _ = circuitbreaker.NewPanel(s.onServiceStateChange, opts)\n\n\tsvcKey := func(ctx context.Context, request interface{}) (serviceCBKey string, enabled bool) {\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tserviceCBKey = s.genServiceCBKey(ri)\n\t\tcbConfig, _ := s.serviceCBConfig.LoadOrStore(serviceCBKey, defaultCBConfig)\n\t\tenabled = cbConfig.(CBConfig).Enable\n\t\treturn\n\t}\n\ts.serviceControl = &Control{\n\t\tGetKey:       svcKey,\n\t\tGetErrorType: s.config.serviceGetErrorTypeFunc,\n\t\tDecorateError: func(ctx context.Context, request interface{}, err error) error {\n\t\t\treturn kerrors.ErrServiceCircuitBreak\n\t\t},\n\t}\n}\n\nfunc (s *CBSuite) initInstanceCB() {\n\tif s.instancePanel != nil && s.instanceControl != nil {\n\t\treturn\n\t}\n\topts := circuitbreaker.Options{\n\t\tShouldTripWithKey: s.insTripFunc,\n\t}\n\ts.instancePanel, _ = circuitbreaker.NewPanel(s.onInstanceStateChange, opts)\n\n\tinstanceKey := func(ctx context.Context, request interface{}) (instCBKey string, enabled bool) {\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tinstCBKey = ri.To().Address().String()\n\t\ts.instanceCBConfig.RLock()\n\t\tenabled = s.instanceCBConfig.Enable\n\t\ts.instanceCBConfig.RUnlock()\n\t\treturn\n\t}\n\ts.instanceControl = &Control{\n\t\tGetKey:       instanceKey,\n\t\tGetErrorType: s.config.instanceGetErrorTypeFunc,\n\t\tDecorateError: func(ctx context.Context, request interface{}, err error) error {\n\t\t\treturn kerrors.ErrInstanceCircuitBreak\n\t\t},\n\t}\n}\n\nfunc (s *CBSuite) onStateChange(level, key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\tif s.events == nil {\n\t\treturn\n\t}\n\tsuccesses, failures, timeouts := m.Counts()\n\tvar errRate float64\n\tif sum := successes + failures + timeouts; sum > 0 {\n\t\terrRate = float64(failures+timeouts) / float64(sum)\n\t}\n\ts.events.Push(&event.Event{\n\t\tName: level + \"_cb\",\n\t\tTime: time.Now(),\n\t\tDetail: fmt.Sprintf(\"%s: %s -> %s, (succ: %d, err: %d, timeout: %d, rate: %f)\",\n\t\t\tkey, oldState, newState, successes, failures, timeouts, errRate),\n\t})\n}\n\nfunc (s *CBSuite) onServiceStateChange(key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\ts.onStateChange(serviceCBKey, key, oldState, newState, m)\n}\n\nfunc (s *CBSuite) onInstanceStateChange(key string, oldState, newState circuitbreaker.State, m circuitbreaker.Metricer) {\n\ts.onStateChange(instanceCBKey, key, oldState, newState, m)\n}\n\nfunc (s *CBSuite) discoveryChangeHandler(e *event.Event) {\n\tif s.instancePanel == nil {\n\t\treturn\n\t}\n\textra := e.Extra.(*discovery.Change)\n\tfor i := range extra.Removed {\n\t\tinstCBKey := extra.Removed[i].Address().String()\n\t\ts.instancePanel.RemoveBreaker(instCBKey)\n\t}\n}\n\nfunc (s *CBSuite) svcTripFunc(key string) circuitbreaker.TripFunc {\n\tpi, _ := s.serviceCBConfig.LoadOrStore(key, defaultCBConfig)\n\tp := pi.(CBConfig)\n\treturn circuitbreaker.RateTripFunc(p.ErrRate, p.MinSample)\n}\n\nfunc (s *CBSuite) insTripFunc(key string) circuitbreaker.TripFunc {\n\ts.instanceCBConfig.RLock()\n\terrRate := s.instanceCBConfig.ErrRate\n\tminSample := s.instanceCBConfig.MinSample\n\ts.instanceCBConfig.RUnlock()\n\treturn circuitbreaker.RateTripFunc(errRate, minSample)\n}\n\nfunc cbDebugInfo(panel circuitbreaker.Panel) map[string]interface{} {\n\tdumper, ok := panel.(interface {\n\t\tDumpBreakers() map[string]circuitbreaker.Breaker\n\t})\n\tif !ok {\n\t\treturn nil\n\t}\n\tcbMap := make(map[string]interface{})\n\tfor key, breaker := range dumper.DumpBreakers() {\n\t\tcbState := breaker.State()\n\t\tif cbState == circuitbreaker.Closed {\n\t\t\tcontinue\n\t\t}\n\t\tcbMap[key] = map[string]interface{}{\n\t\t\t\"state\":             cbState,\n\t\t\t\"successes in 10s\":  breaker.Metricer().Successes(),\n\t\t\t\"failures in 10s\":   breaker.Metricer().Failures(),\n\t\t\t\"timeouts in 10s\":   breaker.Metricer().Timeouts(),\n\t\t\t\"error rate in 10s\": breaker.Metricer().ErrorRate(),\n\t\t}\n\t}\n\tif len(cbMap) == 0 {\n\t\tcbMap[\"msg\"] = \"all circuit breakers are in closed state\"\n\t}\n\treturn cbMap\n}\n\nfunc (s *CBSuite) configInfo() map[string]interface{} {\n\tsvcCBMap := make(map[string]interface{})\n\ts.serviceCBConfig.Range(func(key, value interface{}) bool {\n\t\tsvcCBMap[key.(string)] = value\n\t\treturn true\n\t})\n\ts.instanceCBConfig.RLock()\n\tinstCBConfig := s.instanceCBConfig.CBConfig\n\ts.instanceCBConfig.RUnlock()\n\n\tcbMap := make(map[string]interface{}, 2)\n\tcbMap[serviceCBKey] = svcCBMap\n\tcbMap[instanceCBKey] = instCBConfig\n\treturn cbMap\n}\n\n// RPCInfo2Key is to generate circuit breaker key through rpcinfo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func RPCInfo2Key(ri rpcinfo.RPCInfo) string {\n# Natural Language Annotations: // RPCInfo2Key is to generate circuit breaker key through rpcinfo\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc RPCInfo2Key(ri rpcinfo.RPCInfo) string {\n\treturn ri.To().ServiceName()\n}\n```", "```go\nfunc RPCInfo2Key(ri rpcinfo.RPCInfo) string {\n\tserviceName := ri.To().ServiceName()\n\tmethod := ri.To().Method()\n\treturn fmt.Sprintf(\"%s::%s\", serviceName, method)\n}\n```", "```go\nfunc RPCInfo2Key(ri rpcinfo.RPCInfo) string {\n\tif ri.Invocation().PackageName() == \"\" {\n\t\treturn ri.To().ServiceName()\n\t}\n\treturn ri.Invocation().PackageName() + \".\" + ri.Invocation().ServiceName()\n}\n```"]}, "kitex-pkg/circuitbreak/default.go-ErrorTypeOnServiceLevel": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/circuitbreak/default.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"errors\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\n// some types of error won't trigger circuit breaker\nvar ignoreErrTypes = map[error]ErrorType{\n\tkerrors.ErrInternalException: TypeIgnorable,\n\tkerrors.ErrServiceDiscovery:  TypeIgnorable,\n\tkerrors.ErrACL:               TypeIgnorable,\n\tkerrors.ErrLoadbalance:       TypeIgnorable,\n\tkerrors.ErrRPCFinish:         TypeIgnorable,\n}\n\n// ErrorTypeOnServiceLevel determines the error type with a service level criteria.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ErrorTypeOnInstanceLevel determines the error type with an instance level criteria.\n// Basically, it treats only the connection error as failure.\n\n\n\n\n\n\n\n// FailIfError return TypeFailure if err is not nil, otherwise TypeSuccess.\nfunc FailIfError(ctx context.Context, request, response interface{}, err error) ErrorType {\n\tif err != nil {\n\t\treturn TypeFailure\n\t}\n\treturn TypeSuccess\n}\n\n// NoDecoration returns the original err.\nfunc NoDecoration(ctx context.Context, request interface{}, err error) error {\n\treturn err\n}\n\n```\n# Function Signature: func ErrorTypeOnServiceLevel(ctx context.Context, request, response interface{}, err error) ErrorType {\n# Natural Language Annotations: // ErrorTypeOnServiceLevel determines the error type with a service level criteria.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ErrorTypeOnServiceLevel(ctx context.Context, request, response interface{}, err error) ErrorType {\n\t// ErrorTypeOnServiceLevel determines the error type with a service level criteria.\n\t// It will check the ignoreErrTypes map to see if the error type is ignorable.\n\t// If the error type is found in the map, it will return TypeIgnorable.\n\t// Otherwise, it will return the error type based on the FailIfError function.\n\n\t// If err is nil, return TypeSuccess\n\tif err == nil {\n\t\treturn TypeSuccess\n\t}\n\n\t// Check if the error type is ignorable\n\tif _, ok := ignoreErrTypes[err]; ok {\n\t\treturn TypeIgnorable\n\t}\n\n\t// If the error type is not ignorable, return the error type based on FailIfError\n\treturn FailIfError(ctx, request, response, err)\n}\n```", "```go\npackage circuitbreak\n\nimport (\n\t\"context\"\n)\n\n// ErrorTypeOnServiceLevel determines the error type with a service level criteria.\nfunc ErrorTypeOnServiceLevel(ctx context.Context, request, response interface{}, err error) ErrorType {\n\t// The service level criteria could be based on the error type, service name, etc.\n\t// Here is a placeholder implementation. For actual implementation, you need to decide the criteria.\n\tif err != nil {\n\t\treturn TypeFailure\n\t}\n\treturn TypeSuccess\n}\n```", "```go\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"errors\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\n// ErrorTypeOnServiceLevel determines the error type with a service level criteria.\nfunc ErrorTypeOnServiceLevel(ctx context.Context, request, response interface{}, err error) ErrorType {\n\t// Implement your logic here.\n\t// This is a placeholder.\n\n\t// For now, let's assume that we'll treat all errors as TypeFailure\n\t// In the future, you might want to handle certain types of errors differently\n\t// based on your service-level criteria.\n\treturn TypeFailure\n}\n```"]}, "kitex-pkg/circuitbreak/default.go-ErrorTypeOnInstanceLevel": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/circuitbreak/default.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"errors\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\n// some types of error won't trigger circuit breaker\nvar ignoreErrTypes = map[error]ErrorType{\n\tkerrors.ErrInternalException: TypeIgnorable,\n\tkerrors.ErrServiceDiscovery:  TypeIgnorable,\n\tkerrors.ErrACL:               TypeIgnorable,\n\tkerrors.ErrLoadbalance:       TypeIgnorable,\n\tkerrors.ErrRPCFinish:         TypeIgnorable,\n}\n\n// ErrorTypeOnServiceLevel determines the error type with a service level criteria.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ErrorTypeOnInstanceLevel determines the error type with an instance level criteria.\n// Basically, it treats only the connection error as failure.\n\n\n\n\n\n\n\n// FailIfError return TypeFailure if err is not nil, otherwise TypeSuccess.\nfunc FailIfError(ctx context.Context, request, response interface{}, err error) ErrorType {\n\tif err != nil {\n\t\treturn TypeFailure\n\t}\n\treturn TypeSuccess\n}\n\n// NoDecoration returns the original err.\nfunc NoDecoration(ctx context.Context, request interface{}, err error) error {\n\treturn err\n}\n\n```\n# Function Signature: func ErrorTypeOnInstanceLevel(ctx context.Context, request, response interface{}, err error) ErrorType {\n# Natural Language Annotations: // ErrorTypeOnInstanceLevel determines the error type with an instance level criteria.\n// Basically, it treats only the connection error as failure.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage circuitbreak\n\nimport (\n\t\"context\"\n\t\"errors\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\n// ErrorTypeOnInstanceLevel determines the error type with an instance level criteria.\n// Basically, it treats only the connection error as failure.\nfunc ErrorTypeOnInstanceLevel(ctx context.Context, request, response interface{}, err error) ErrorType {\n\t// Check if the error is a connection error\n\tif errors.Is(err, kerrors.ErrConn) {\n\t\treturn TypeFailure\n\t}\n\treturn TypeSuccess\n}\n```", "```go\nfunc ErrorTypeOnInstanceLevel(ctx context.Context, request interface{}, response interface{}, err error) ErrorType {\n\tif err != nil {\n\t\t// Basically, it treats only the connection error as failure.\n\t\tif _, ok := ignoreErrTypes[err]; ok {\n\t\t\treturn TypeIgnorable\n\t\t}\n\t\treturn TypeFailure\n\t}\n\treturn TypeSuccess\n}\n```", "```go\nfunc ErrorTypeOnInstanceLevel(ctx context.Context, request interface{}, response interface{}, err error) ErrorType {\n    if err != nil {\n        // Basically, it treats only the connection error as failure.\n        // Let's assume that 'connection error' means an error returned from a network operation.\n        // We can check if the error is a network error by comparing it with net.Error.\n        var netError net.Error\n        if errors.As(err, &netError) {\n            return TypeFailure\n        }\n    }\n    return TypeSuccess\n}\n```"]}, "kitex-pkg/limiter/qps_limiter.go-UpdateLimit": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/limiter/qps_limiter.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage limiter\n\nimport (\n\t\"context\"\n\t\"sync/atomic\"\n\t\"time\"\n)\n\nvar fixedWindowTime = time.Second\n\n// qpsLimiter implements the RateLimiter interface.\ntype qpsLimiter struct {\n\tlimit      int32\n\ttokens     int32\n\tinterval   time.Duration\n\tonce       int32\n\tticker     *time.Ticker\n\ttickerDone chan bool\n}\n\n// NewQPSLimiter creates qpsLimiter.\nfunc NewQPSLimiter(interval time.Duration, limit int) RateLimiter {\n\tonce := calcOnce(interval, limit)\n\tl := &qpsLimiter{\n\t\tlimit:    int32(limit),\n\t\tinterval: interval,\n\t\ttokens:   once,\n\t\tonce:     once,\n\t}\n\tgo l.startTicker(interval)\n\treturn l\n}\n\n// UpdateLimit update limitation of QPS. It is **not** concurrent-safe.\n\n\n\n\n\n\n\n// UpdateQPSLimit update the interval and limit. It is **not** concurrent-safe.\n\n\n\n\n\n\n\n\n\n\n\n\n// Acquire one token.\nfunc (l *qpsLimiter) Acquire(ctx context.Context) bool {\n\tif atomic.LoadInt32(&l.limit) <= 0 {\n\t\treturn true\n\t}\n\tif atomic.LoadInt32(&l.tokens) <= 0 {\n\t\treturn false\n\t}\n\treturn atomic.AddInt32(&l.tokens, -1) >= 0\n}\n\n// Status returns the current status.\nfunc (l *qpsLimiter) Status(ctx context.Context) (max, cur int, interval time.Duration) {\n\tmax = int(atomic.LoadInt32(&l.limit))\n\tcur = int(atomic.LoadInt32(&l.tokens))\n\tinterval = l.interval\n\treturn\n}\n\nfunc (l *qpsLimiter) startTicker(interval time.Duration) {\n\tl.ticker = time.NewTicker(interval)\n\tdefer l.ticker.Stop()\n\tl.tickerDone = make(chan bool, 1)\n\ttc := l.ticker.C\n\ttd := l.tickerDone\n\t// ticker and tickerDone can be reset, cannot use l.ticker or l.tickerDone directly\n\tfor {\n\t\tselect {\n\t\tcase <-tc:\n\t\t\tl.updateToken()\n\t\tcase <-td:\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (l *qpsLimiter) stopTicker() {\n\tif l.tickerDone == nil {\n\t\treturn\n\t}\n\tselect {\n\tcase l.tickerDone <- true:\n\tdefault:\n\t}\n}\n\n// Some deviation is allowed here to gain better performance.\nfunc (l *qpsLimiter) updateToken() {\n\tif atomic.LoadInt32(&l.limit) < atomic.LoadInt32(&l.tokens) {\n\t\treturn\n\t}\n\n\tonce := atomic.LoadInt32(&l.once)\n\n\tdelta := atomic.LoadInt32(&l.limit) - atomic.LoadInt32(&l.tokens)\n\n\tif delta > once || delta < 0 {\n\t\tdelta = once\n\t}\n\n\tnewTokens := atomic.AddInt32(&l.tokens, delta)\n\tif newTokens < once {\n\t\tatomic.StoreInt32(&l.tokens, once)\n\t}\n}\n\nfunc calcOnce(interval time.Duration, limit int) int32 {\n\tif interval > time.Second {\n\t\tinterval = time.Second\n\t}\n\tonce := int32(float64(limit) / (fixedWindowTime.Seconds() / interval.Seconds()))\n\tif once < 0 {\n\t\tonce = 0\n\t}\n\treturn once\n}\n\nfunc (l *qpsLimiter) resetTokens(once int32) {\n\tif atomic.LoadInt32(&l.tokens) > once {\n\t\tatomic.StoreInt32(&l.tokens, once)\n\t}\n}\n\n```\n# Function Signature: func (l *qpsLimiter) UpdateLimit(limit int) {\n# Natural Language Annotations: // UpdateLimit update limitation of QPS. It is **not** concurrent-safe.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (l *qpsLimiter) UpdateLimit(limit int) {\n\tl.limit = int32(limit)\n}\n```\nPlease note that this function is not concurrent-safe as it directly modifies the `limit` field. If you need to update the `limit` field concurrently, you need to implement a proper lock or use atomic operations.", "```go\nfunc (l *qpsLimiter) UpdateLimit(limit int) {\n\tl.limit = int32(limit)\n\tl.once = calcOnce(l.interval, limit)\n\tl.resetTokens(l.once)\n}\n```", "```go\nfunc (l *qpsLimiter) UpdateLimit(limit int) {\n    atomic.StoreInt32(&l.limit, int32(limit))\n    once := calcOnce(l.interval, limit)\n    atomic.StoreInt32(&l.once, once)\n    l.resetTokens(once)\n}\n```\nThis function is used to update the limit of the QPSLimiter. It first atomically stores the new limit. Then, it calculates the value of `once` based on the interval and the new limit. The `once` value is then atomically stored to update the `once` value. Finally, the tokens are reset to the new `once` value. This function is **not** concurrent-safe because it uses atomic operations, which ensure that the operation is atomic and cannot be interrupted by other goroutines."]}, "kitex-pkg/limiter/qps_limiter.go-UpdateQPSLimit": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/limiter/qps_limiter.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage limiter\n\nimport (\n\t\"context\"\n\t\"sync/atomic\"\n\t\"time\"\n)\n\nvar fixedWindowTime = time.Second\n\n// qpsLimiter implements the RateLimiter interface.\ntype qpsLimiter struct {\n\tlimit      int32\n\ttokens     int32\n\tinterval   time.Duration\n\tonce       int32\n\tticker     *time.Ticker\n\ttickerDone chan bool\n}\n\n// NewQPSLimiter creates qpsLimiter.\nfunc NewQPSLimiter(interval time.Duration, limit int) RateLimiter {\n\tonce := calcOnce(interval, limit)\n\tl := &qpsLimiter{\n\t\tlimit:    int32(limit),\n\t\tinterval: interval,\n\t\ttokens:   once,\n\t\tonce:     once,\n\t}\n\tgo l.startTicker(interval)\n\treturn l\n}\n\n// UpdateLimit update limitation of QPS. It is **not** concurrent-safe.\n\n\n\n\n\n\n\n// UpdateQPSLimit update the interval and limit. It is **not** concurrent-safe.\n\n\n\n\n\n\n\n\n\n\n\n\n// Acquire one token.\nfunc (l *qpsLimiter) Acquire(ctx context.Context) bool {\n\tif atomic.LoadInt32(&l.limit) <= 0 {\n\t\treturn true\n\t}\n\tif atomic.LoadInt32(&l.tokens) <= 0 {\n\t\treturn false\n\t}\n\treturn atomic.AddInt32(&l.tokens, -1) >= 0\n}\n\n// Status returns the current status.\nfunc (l *qpsLimiter) Status(ctx context.Context) (max, cur int, interval time.Duration) {\n\tmax = int(atomic.LoadInt32(&l.limit))\n\tcur = int(atomic.LoadInt32(&l.tokens))\n\tinterval = l.interval\n\treturn\n}\n\nfunc (l *qpsLimiter) startTicker(interval time.Duration) {\n\tl.ticker = time.NewTicker(interval)\n\tdefer l.ticker.Stop()\n\tl.tickerDone = make(chan bool, 1)\n\ttc := l.ticker.C\n\ttd := l.tickerDone\n\t// ticker and tickerDone can be reset, cannot use l.ticker or l.tickerDone directly\n\tfor {\n\t\tselect {\n\t\tcase <-tc:\n\t\t\tl.updateToken()\n\t\tcase <-td:\n\t\t\treturn\n\t\t}\n\t}\n}\n\nfunc (l *qpsLimiter) stopTicker() {\n\tif l.tickerDone == nil {\n\t\treturn\n\t}\n\tselect {\n\tcase l.tickerDone <- true:\n\tdefault:\n\t}\n}\n\n// Some deviation is allowed here to gain better performance.\nfunc (l *qpsLimiter) updateToken() {\n\tif atomic.LoadInt32(&l.limit) < atomic.LoadInt32(&l.tokens) {\n\t\treturn\n\t}\n\n\tonce := atomic.LoadInt32(&l.once)\n\n\tdelta := atomic.LoadInt32(&l.limit) - atomic.LoadInt32(&l.tokens)\n\n\tif delta > once || delta < 0 {\n\t\tdelta = once\n\t}\n\n\tnewTokens := atomic.AddInt32(&l.tokens, delta)\n\tif newTokens < once {\n\t\tatomic.StoreInt32(&l.tokens, once)\n\t}\n}\n\nfunc calcOnce(interval time.Duration, limit int) int32 {\n\tif interval > time.Second {\n\t\tinterval = time.Second\n\t}\n\tonce := int32(float64(limit) / (fixedWindowTime.Seconds() / interval.Seconds()))\n\tif once < 0 {\n\t\tonce = 0\n\t}\n\treturn once\n}\n\nfunc (l *qpsLimiter) resetTokens(once int32) {\n\tif atomic.LoadInt32(&l.tokens) > once {\n\t\tatomic.StoreInt32(&l.tokens, once)\n\t}\n}\n\n```\n# Function Signature: func (l *qpsLimiter) UpdateQPSLimit(interval time.Duration, limit int) {\n# Natural Language Annotations: // UpdateQPSLimit update the interval and limit. It is **not** concurrent-safe.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (l *qpsLimiter) UpdateQPSLimit(interval time.Duration, limit int) {\n\tif l.interval != interval || l.limit != int32(limit) {\n\t\tl.interval = interval\n\t\tl.limit = int32(limit)\n\t\tl.once = calcOnce(interval, limit)\n\t}\n}\n```\nThis function `UpdateQPSLimit` updates the interval and limit of the QPSLimiter. It's important to note that this is not a concurrent-safe operation, meaning it should only be called from a single goroutine at a time. If it's called concurrently from multiple goroutines, it might lead to race conditions.", "```go\nfunc (l *qpsLimiter) UpdateQPSLimit(interval time.Duration, limit int) {\n    l.interval = interval\n    l.limit = int32(limit)\n    once := calcOnce(interval, limit)\n    l.once = once\n    l.resetTokens(once)\n}\n```\nThis function `UpdateQPSLimit` updates the interval and limit of the QPSLimiter. It should be noted that it is not concurrent-safe, meaning it should not be called from multiple goroutines simultaneously. This is because it updates shared state (`interval`, `limit`, and `once`) without any synchronization.", "```go\nfunc (l *qpsLimiter) UpdateQPSLimit(interval time.Duration, limit int) {\n\tl.stopTicker()\n\tl.interval = interval\n\tl.limit = int32(limit)\n\tonce := calcOnce(interval, limit)\n\tl.once = once\n\tl.resetTokens(once)\n\tgo l.startTicker(interval)\n}\n```"]}, "kitex-pkg/utils/ring.go-NewRing": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/ring.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"errors\"\n\t\"runtime\"\n)\n\n// ErrRingFull means the ring is full.\nvar ErrRingFull = errors.New(\"ring is full\")\n\n// Deprecated: it's not used by kitex anymore.\n// NewRing creates a ringbuffer with fixed size.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Deprecated: it's not used by kitex anymore.\n// Ring implements a fixed size hash list to manage data\ntype Ring struct {\n\tlength int\n\trings  []*ring\n}\n\n// Push appends item to the ring.\nfunc (r *Ring) Push(obj interface{}) error {\n\tif r.length == 1 {\n\t\treturn r.rings[0].Push(obj)\n\t}\n\n\tidx := getGoroutineID() % r.length\n\tfor i := 0; i < r.length; i, idx = i+1, (idx+1)%r.length {\n\t\terr := r.rings[idx].Push(obj)\n\t\tif err == nil {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn ErrRingFull\n}\n\n// Pop returns the last item and removes it from the ring.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Dump dumps the data in the ring.\nfunc (r *Ring) Dump() interface{} {\n\tm := &ringDump{}\n\tdumpList := make([]*ringDump, 0, r.length)\n\tidx := getGoroutineID() % r.length\n\tfor i := 0; i < r.length; i, idx = i+1, (idx+1)%r.length {\n\t\tcurDump := &ringDump{}\n\t\tr.rings[idx].Dump(curDump)\n\t\tdumpList = append(dumpList, curDump)\n\t\tm.Cap += curDump.Cap\n\t\tm.Len += curDump.Len\n\t}\n\tm.Array = make([]interface{}, 0, m.Len)\n\tfor _, shardData := range dumpList {\n\t\tfor i := 0; i < shardData.Len; i++ {\n\t\t\tm.Array = append(m.Array, shardData.Array[i])\n\t\t}\n\t}\n\treturn m\n}\n\n```\n# Function Signature: func NewRing(size int) *Ring {\n# Natural Language Annotations: // Deprecated: it's not used by kitex anymore.\n// NewRing creates a ringbuffer with fixed size.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewRing(size int) *Ring {\n    rings := make([]*ring, size)\n    for i := 0; i < size; i++ {\n        rings[i] = newRing(0)\n    }\n    return &Ring{\n        length: size,\n        rings:  rings,\n    }\n}\n```", "```go\nfunc NewRing(size int) *Ring {\n\trings := make([]*ring, size)\n\tfor i := range rings {\n\t\trings[i] = newRing()\n\t}\n\treturn &Ring{\n\t\tlength: size,\n\t\trings:  rings,\n\t}\n}\n```", "```go\nfunc NewRing(size int) *Ring {\n\tr := &Ring{\n\t\tlength: size,\n\t\trings:  make([]*ring, size),\n\t}\n\n\tfor i := 0; i < size; i++ {\n\t\tr.rings[i] = newRing(0)\n\t}\n\n\treturn r\n}\n```"]}, "kitex-pkg/utils/ring.go-Pop": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/ring.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"errors\"\n\t\"runtime\"\n)\n\n// ErrRingFull means the ring is full.\nvar ErrRingFull = errors.New(\"ring is full\")\n\n// Deprecated: it's not used by kitex anymore.\n// NewRing creates a ringbuffer with fixed size.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Deprecated: it's not used by kitex anymore.\n// Ring implements a fixed size hash list to manage data\ntype Ring struct {\n\tlength int\n\trings  []*ring\n}\n\n// Push appends item to the ring.\nfunc (r *Ring) Push(obj interface{}) error {\n\tif r.length == 1 {\n\t\treturn r.rings[0].Push(obj)\n\t}\n\n\tidx := getGoroutineID() % r.length\n\tfor i := 0; i < r.length; i, idx = i+1, (idx+1)%r.length {\n\t\terr := r.rings[idx].Push(obj)\n\t\tif err == nil {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn ErrRingFull\n}\n\n// Pop returns the last item and removes it from the ring.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Dump dumps the data in the ring.\nfunc (r *Ring) Dump() interface{} {\n\tm := &ringDump{}\n\tdumpList := make([]*ringDump, 0, r.length)\n\tidx := getGoroutineID() % r.length\n\tfor i := 0; i < r.length; i, idx = i+1, (idx+1)%r.length {\n\t\tcurDump := &ringDump{}\n\t\tr.rings[idx].Dump(curDump)\n\t\tdumpList = append(dumpList, curDump)\n\t\tm.Cap += curDump.Cap\n\t\tm.Len += curDump.Len\n\t}\n\tm.Array = make([]interface{}, 0, m.Len)\n\tfor _, shardData := range dumpList {\n\t\tfor i := 0; i < shardData.Len; i++ {\n\t\t\tm.Array = append(m.Array, shardData.Array[i])\n\t\t}\n\t}\n\treturn m\n}\n\n```\n# Function Signature: func (r *Ring) Pop() interface{} {\n# Natural Language Annotations: // Pop returns the last item and removes it from the ring.\nPlease return the generated code surrounded by ```\n", "response": ["Sure, here is the code generated based on the function signature and annotations you provided.\n\n```go\nfunc (r *Ring) Pop() interface{} {\n\tif r.length == 1 {\n\t\treturn r.rings[0].Pop()\n\t}\n\n\tidx := getGoroutineID() % r.length\n\tfor i := 0; i < r.length; i, idx = i+1, (idx+1)%r.length {\n\t\titem, err := r.rings[idx].Pop()\n\t\tif err == nil {\n\t\t\treturn item, nil\n\t\t}\n\t}\n\treturn nil, ErrRingFull\n}\n```\nThis function attempts to pop the last item from the ring. If the ring only contains one element, it simply pops from the only ring in the rings slice. If the ring contains multiple elements, it attempts to pop an item from each ring in the rings slice in a round-robin manner. If it successfully pops an item, it returns the item. If none of the rings contain any items, it returns an error indicating that the ring is full.", "```go\nfunc (r *Ring) Pop() interface{} {\n\tif r.length == 1 {\n\t\treturn r.rings[0].Pop()\n\t}\n\n\tidx := getGoroutineID() % r.length\n\tfor i := 0; i < r.length; i, idx = i+1, (idx+1)%r.length {\n\t\titem := r.rings[idx].Pop()\n\t\tif item != nil {\n\t\t\treturn item\n\t\t}\n\t}\n\treturn nil\n}\n```", "```go\npackage utils\n\nimport (\n\t\"errors\"\n\t\"runtime\"\n)\n\n// ErrRingFull means the ring is full.\nvar ErrRingFull = errors.New(\"ring is full\")\n\n// Deprecated: it's not used by kitex anymore.\n// NewRing creates a ringbuffer with fixed size.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Deprecated: it's not used by kitex anymore.\n// Ring implements a fixed size hash list to manage data\ntype Ring struct {\n\tlength int\n\trings  []*ring\n}\n\n// Push appends item to the ring.\nfunc (r *Ring) Push(obj interface{}) error {\n\tif r.length == 1 {\n\t\treturn r.rings[0].Push(obj)\n\t}\n\n\tidx := getGoroutineID() % r.length\n\tfor i := 0; i < r.length; i, idx = i+1, (idx+1)%r.length {\n\t\terr := r.rings[idx].Push(obj)\n\t\tif err == nil {\n\t\t\treturn nil\n\t\t}\n\t}\n\treturn ErrRingFull\n}\n\n// Pop returns the last item and removes it from the ring.\nfunc (r *Ring) Pop() interface{} {\n\tif r.length == 1 {\n\t\treturn r.rings[0].Pop()\n\t}\n\n\tidx := getGoroutineID() % r.length\n\tfor i := 0; i < r.length; i, idx = i+1, (idx+1)%r.length {\n\t\titem := r.rings[idx].Pop()\n\t\tif item != nil {\n\t\t\treturn item\n\t\t}\n\t}\n\treturn nil\n}\n\n// Dump dumps the data in the ring.\nfunc (r *Ring) Dump() interface{} {\n\tm := &ringDump{}\n\tdumpList := make([]*ringDump, 0, r.length)\n\tidx := getGoroutineID() % r.length\n\tfor i := 0; i < r.length; i, idx = i+1, (idx+1)%r.length {\n\t\tcurDump := &ringDump{}\n\t\tr.rings[idx].Dump(curDump)\n\t\tdumpList = append(dumpList, curDump)\n\t\tm.Cap += curDump.Cap\n\t\tm.Len += curDump.Len\n\t}\n\tm.Array = make([]interface{}, 0, m.Len)\n\tfor _, shardData := range dumpList {\n\t\tfor i := 0; i < shardData.Len; i++ {\n\t\t\tm.Array = append(m.Array, shardData.Array[i])\n\t\t}\n\t}\n\treturn m\n}\n```"]}, "kitex-pkg/utils/thrift.go-Encode": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/thrift.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n)\n\n// ThriftMessageCodec is used to codec thrift messages.\ntype ThriftMessageCodec struct {\n\ttb    *thrift.TMemoryBuffer\n\ttProt thrift.TProtocol\n}\n\n// NewThriftMessageCodec creates a new ThriftMessageCodec.\nfunc NewThriftMessageCodec() *ThriftMessageCodec {\n\ttransport := thrift.NewTMemoryBufferLen(1024)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\n\treturn &ThriftMessageCodec{\n\t\ttb:    transport,\n\t\ttProt: tProt,\n\t}\n}\n\n// Encode do thrift message encode.\n// Notice! msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n// Notice! seqID will be reset in kitex if the buffer is used for generic call in client side, set seqID=0 is suggested\n// when you call this method as client.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Decode do thrift message decode, notice: msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Serialize serialize message into bytes. This is normal thrift serialize func.\n// Notice: Binary generic use Encode instead of Serialize.\n\n\n\n\n\n\n\n\n\n\n// Deserialize deserialize bytes into message. This is normal thrift deserialize func.\n// Notice: Binary generic use Decode instead of Deserialize.\n\n\n\n\n\n\n\n\n\n\n\n// MarshalError convert go error to thrift exception, and encode exception over buffered binary transport.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalError decode binary and return error message\nfunc UnmarshalError(b []byte) error {\n\ttrans := thrift.NewStreamTransportR(bytes.NewReader(b))\n\tproto := thrift.NewTBinaryProtocolTransport(trans)\n\tif _, _, _, err := proto.ReadMessageBegin(); err != nil {\n\t\treturn fmt.Errorf(\"read message begin error: %w\", err)\n\t}\n\te := thrift.NewTApplicationException(0, \"\")\n\tif err := e.Read(proto); err != nil {\n\t\treturn fmt.Errorf(\"read exception error: %w\", err)\n\t}\n\tif err := proto.ReadMessageEnd(); err != nil {\n\t\treturn fmt.Errorf(\"read message end error: %w\", err)\n\t}\n\treturn e\n}\n\n```\n# Function Signature: func (t *ThriftMessageCodec) Encode(method string, msgType thrift.TMessageType, seqID int32, msg thrift.TStruct) (b []byte, err error) {\n# Natural Language Annotations: // Encode do thrift message encode.\n// Notice! msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n// Notice! seqID will be reset in kitex if the buffer is used for generic call in client side, set seqID=0 is suggested\n// when you call this method as client.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (t *ThriftMessageCodec) Encode(method string, msgType thrift.TMessageType, seqID int32, msg thrift.TStruct) (b []byte, err error) {\n\tt.tb.Reset()\n\tif err = t.tProt.WriteMessageBegin(method, msgType, seqID); err != nil {\n\t\treturn nil, fmt.Errorf(\"write message begin error: %w\", err)\n\t}\n\tif err = msg.Write(t.tProt); err != nil {\n\t\treturn nil, fmt.Errorf(\"write struct error: %w\", err)\n\t}\n\tif err = t.tProt.WriteMessageEnd(); err != nil {\n\t\treturn nil, fmt.Errorf(\"write message end error: %w\", err)\n\t}\n\tif msgType == thrift.REPLY {\n\t\treturn t.tb.Bytes()[4:], nil\n\t}\n\treturn t.tb.Bytes(), nil\n}\n```", "```go\nfunc (t *ThriftMessageCodec) Encode(method string, msgType thrift.TMessageType, seqID int32, msg thrift.TStruct) (b []byte, err error) {\n\tif msg == nil {\n\t\treturn nil, errors.New(\"message is nil\")\n\t}\n\tif err := t.tb.Flush(); err != nil {\n\t\treturn nil, fmt.Errorf(\"flush memory buffer error: %w\", err)\n\t}\n\tt.tb.Reset()\n\tif err := t.tProt.WriteMessageBegin(context.Background(), method, msgType, seqID); err != nil {\n\t\treturn nil, fmt.Errorf(\"write message begin error: %w\", err)\n\t}\n\tif err := msg.Write(t.tProt); err != nil {\n\t\treturn nil, fmt.Errorf(\"write message error: %w\", err)\n\t}\n\tif err := t.tProt.WriteMessageEnd(); err != nil {\n\t\treturn nil, fmt.Errorf(\"write message end error: %w\", err)\n\t}\n\treturn t.tb.Bytes(), nil\n}\n```\nPlease note that the `Encode` function will reset the memory buffer before encoding the message. This is because Kitex will reset the sequence ID if the buffer is used for generic call in client side. Therefore, to avoid this, you should set the sequence ID to 0 when calling this method as a client.", "```go\nfunc (t *ThriftMessageCodec) Encode(method string, msgType thrift.TMessageType, seqID int32, msg thrift.TStruct) (b []byte, err error) {\n\t// Notice: msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n\t// Notice: seqID will be reset in kitex if the buffer is used for generic call in client side, set seqID=0 is suggested\n\t// when you call this method as client.\n\tt.tb.Reset() // Reset the buffer\n\tif err = t.tProt.WriteMessageBegin(context.Background(), method, msgType, seqID); err != nil {\n\t\treturn nil, fmt.Errorf(\"write message begin error: %w\", err)\n\t}\n\tif err = msg.Write(t.tProt); err != nil {\n\t\treturn nil, fmt.Errorf(\"write thrift struct error: %w\", err)\n\t}\n\tif err = t.tProt.WriteMessageEnd(); err != nil {\n\t\treturn nil, fmt.Errorf(\"write message end error: %w\", err)\n\t}\n\tif err = t.tProt.Flush(context.Background()); err != nil {\n\t\treturn nil, fmt.Errorf(\"flush error: %w\", err)\n\t}\n\n\treturn t.tb.Bytes(), nil\n}\n```"]}, "kitex-pkg/utils/thrift.go-Decode": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/thrift.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n)\n\n// ThriftMessageCodec is used to codec thrift messages.\ntype ThriftMessageCodec struct {\n\ttb    *thrift.TMemoryBuffer\n\ttProt thrift.TProtocol\n}\n\n// NewThriftMessageCodec creates a new ThriftMessageCodec.\nfunc NewThriftMessageCodec() *ThriftMessageCodec {\n\ttransport := thrift.NewTMemoryBufferLen(1024)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\n\treturn &ThriftMessageCodec{\n\t\ttb:    transport,\n\t\ttProt: tProt,\n\t}\n}\n\n// Encode do thrift message encode.\n// Notice! msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n// Notice! seqID will be reset in kitex if the buffer is used for generic call in client side, set seqID=0 is suggested\n// when you call this method as client.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Decode do thrift message decode, notice: msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Serialize serialize message into bytes. This is normal thrift serialize func.\n// Notice: Binary generic use Encode instead of Serialize.\n\n\n\n\n\n\n\n\n\n\n// Deserialize deserialize bytes into message. This is normal thrift deserialize func.\n// Notice: Binary generic use Decode instead of Deserialize.\n\n\n\n\n\n\n\n\n\n\n\n// MarshalError convert go error to thrift exception, and encode exception over buffered binary transport.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalError decode binary and return error message\nfunc UnmarshalError(b []byte) error {\n\ttrans := thrift.NewStreamTransportR(bytes.NewReader(b))\n\tproto := thrift.NewTBinaryProtocolTransport(trans)\n\tif _, _, _, err := proto.ReadMessageBegin(); err != nil {\n\t\treturn fmt.Errorf(\"read message begin error: %w\", err)\n\t}\n\te := thrift.NewTApplicationException(0, \"\")\n\tif err := e.Read(proto); err != nil {\n\t\treturn fmt.Errorf(\"read exception error: %w\", err)\n\t}\n\tif err := proto.ReadMessageEnd(); err != nil {\n\t\treturn fmt.Errorf(\"read message end error: %w\", err)\n\t}\n\treturn e\n}\n\n```\n# Function Signature: func (t *ThriftMessageCodec) Decode(b []byte, msg thrift.TStruct) (method string, seqID int32, err error) {\n# Natural Language Annotations: // Decode do thrift message decode, notice: msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (t *ThriftMessageCodec) Decode(b []byte, msg thrift.TStruct) (method string, seqID int32, err error) {\n\tt.tb.Reset()\n\tt.tProt.Reset()\n\n\t_, err = t.tb.Write(b)\n\tif err != nil {\n\t\treturn \"\", 0, err\n\t}\n\n\tname, typeID, seqID, err := t.tProt.ReadMessageBegin()\n\tif err != nil {\n\t\treturn \"\", 0, err\n\t}\n\n\tif typeID != thrift.REPLY {\n\t\treturn \"\", 0, fmt.Errorf(\"expected reply message, but got %v message\", typeID)\n\t}\n\n\terr = msg.Read(t.tProt)\n\tif err != nil {\n\t\treturn \"\", 0, err\n\t}\n\n\terr = t.tProt.ReadMessageEnd()\n\tif err != nil {\n\t\treturn \"\", 0, err\n\t}\n\n\treturn name, seqID, nil\n}\n```", "```go\n// Decode do thrift message decode, notice: msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\nfunc (t *ThriftMessageCodec) Decode(b []byte, msg thrift.TStruct) (method string, seqID int32, err error) {\n\tt.tb.Reset()\n\tt.tProt.Reset()\n\t_, err = t.tb.Write(b)\n\tif err != nil {\n\t\treturn \"\", 0, err\n\t}\n\tname, typeID, seqID, err := t.tProt.ReadMessageBegin()\n\tif err != nil {\n\t\treturn \"\", 0, err\n\t}\n\tif typeID != thrift.REPLY {\n\t\treturn \"\", 0, errors.New(\"unexpected message type\")\n\t}\n\terr = msg.Read(t.tProt)\n\tif err != nil {\n\t\treturn \"\", 0, err\n\t}\n\terr = t.tProt.ReadMessageEnd()\n\tif err != nil {\n\t\treturn \"\", 0, err\n\t}\n\treturn name, seqID, nil\n}\n```", "```go\nfunc (t *ThriftMessageCodec) Decode(b []byte, msg thrift.TStruct) (method string, seqID int32, err error) {\n\t// Reset the underlying buffer to zero to prevent any existing data.\n\tt.tb.Reset()\n\n\t// Write the incoming byte data into the buffer.\n\t_, err = t.tb.Write(b)\n\tif err != nil {\n\t\treturn \"\", 0, err\n\t}\n\n\t// Decode the method name from the buffer.\n\tmethod, _, seqID, err = t.tProt.ReadMessageBegin()\n\tif err != nil {\n\t\treturn \"\", 0, err\n\t}\n\n\t// Decode the message from the buffer into the provided struct.\n\terr = msg.Read(t.tProt)\n\tif err != nil {\n\t\treturn \"\", 0, err\n\t}\n\n\t// Read the end of the message from the buffer.\n\terr = t.tProt.ReadMessageEnd()\n\tif err != nil {\n\t\treturn \"\", 0, err\n\t}\n\n\t// Return the method name, sequence ID, and any error that occurred during decoding.\n\treturn method, seqID, err\n}\n```\nThis function decodes a thrift message from a byte slice into the provided Thrift struct. It first resets the underlying buffer, then writes the incoming byte data into the buffer. It then decodes the method name and sequence ID from the buffer and decodes the message from the buffer into the provided struct. Finally, it reads the end of the message from the buffer and returns the method name, sequence ID, and any error that occurred during decoding."]}, "kitex-pkg/utils/thrift.go-Serialize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/thrift.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n)\n\n// ThriftMessageCodec is used to codec thrift messages.\ntype ThriftMessageCodec struct {\n\ttb    *thrift.TMemoryBuffer\n\ttProt thrift.TProtocol\n}\n\n// NewThriftMessageCodec creates a new ThriftMessageCodec.\nfunc NewThriftMessageCodec() *ThriftMessageCodec {\n\ttransport := thrift.NewTMemoryBufferLen(1024)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\n\treturn &ThriftMessageCodec{\n\t\ttb:    transport,\n\t\ttProt: tProt,\n\t}\n}\n\n// Encode do thrift message encode.\n// Notice! msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n// Notice! seqID will be reset in kitex if the buffer is used for generic call in client side, set seqID=0 is suggested\n// when you call this method as client.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Decode do thrift message decode, notice: msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Serialize serialize message into bytes. This is normal thrift serialize func.\n// Notice: Binary generic use Encode instead of Serialize.\n\n\n\n\n\n\n\n\n\n\n// Deserialize deserialize bytes into message. This is normal thrift deserialize func.\n// Notice: Binary generic use Decode instead of Deserialize.\n\n\n\n\n\n\n\n\n\n\n\n// MarshalError convert go error to thrift exception, and encode exception over buffered binary transport.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalError decode binary and return error message\nfunc UnmarshalError(b []byte) error {\n\ttrans := thrift.NewStreamTransportR(bytes.NewReader(b))\n\tproto := thrift.NewTBinaryProtocolTransport(trans)\n\tif _, _, _, err := proto.ReadMessageBegin(); err != nil {\n\t\treturn fmt.Errorf(\"read message begin error: %w\", err)\n\t}\n\te := thrift.NewTApplicationException(0, \"\")\n\tif err := e.Read(proto); err != nil {\n\t\treturn fmt.Errorf(\"read exception error: %w\", err)\n\t}\n\tif err := proto.ReadMessageEnd(); err != nil {\n\t\treturn fmt.Errorf(\"read message end error: %w\", err)\n\t}\n\treturn e\n}\n\n```\n# Function Signature: func (t *ThriftMessageCodec) Serialize(msg thrift.TStruct) (b []byte, err error) {\n# Natural Language Annotations: // Serialize serialize message into bytes. This is normal thrift serialize func.\n// Notice: Binary generic use Encode instead of Serialize.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (t *ThriftMessageCodec) Serialize(msg thrift.TStruct) (b []byte, err error) {\n\tt.tb.Reset()\n\tif err := msg.Write(t.tProt); err != nil {\n\t\treturn nil, err\n\t}\n\tb = t.tb.Bytes()\n\treturn b, nil\n}\n```", "```go\nfunc (t *ThriftMessageCodec) Serialize(msg thrift.TStruct) (b []byte, err error) {\n\tt.tb.Reset()\n\tif err := msg.Write(t.tProt); err != nil {\n\t\treturn nil, fmt.Errorf(\"write error: %w\", err)\n\t}\n\treturn t.tb.Bytes(), nil\n}\n```\nThe code above defines a function `Serialize` that takes an instance of `thrift.TStruct` and serializes it into a byte array. It uses the thrift protocol to write the struct into a memory buffer, then returns the byte array. If there is an error during the writing process, it returns an error.", "```go\nfunc (t *ThriftMessageCodec) Serialize(msg thrift.TStruct) (b []byte, err error) {\n\tt.tb.Reset()\n\tif err := msg.Write(t.tProt); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to serialize thrift message: %w\", err)\n\t}\n\treturn t.tb.Bytes(), nil\n}\n```"]}, "kitex-pkg/utils/thrift.go-Deserialize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/thrift.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n)\n\n// ThriftMessageCodec is used to codec thrift messages.\ntype ThriftMessageCodec struct {\n\ttb    *thrift.TMemoryBuffer\n\ttProt thrift.TProtocol\n}\n\n// NewThriftMessageCodec creates a new ThriftMessageCodec.\nfunc NewThriftMessageCodec() *ThriftMessageCodec {\n\ttransport := thrift.NewTMemoryBufferLen(1024)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\n\treturn &ThriftMessageCodec{\n\t\ttb:    transport,\n\t\ttProt: tProt,\n\t}\n}\n\n// Encode do thrift message encode.\n// Notice! msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n// Notice! seqID will be reset in kitex if the buffer is used for generic call in client side, set seqID=0 is suggested\n// when you call this method as client.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Decode do thrift message decode, notice: msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Serialize serialize message into bytes. This is normal thrift serialize func.\n// Notice: Binary generic use Encode instead of Serialize.\n\n\n\n\n\n\n\n\n\n\n// Deserialize deserialize bytes into message. This is normal thrift deserialize func.\n// Notice: Binary generic use Decode instead of Deserialize.\n\n\n\n\n\n\n\n\n\n\n\n// MarshalError convert go error to thrift exception, and encode exception over buffered binary transport.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalError decode binary and return error message\nfunc UnmarshalError(b []byte) error {\n\ttrans := thrift.NewStreamTransportR(bytes.NewReader(b))\n\tproto := thrift.NewTBinaryProtocolTransport(trans)\n\tif _, _, _, err := proto.ReadMessageBegin(); err != nil {\n\t\treturn fmt.Errorf(\"read message begin error: %w\", err)\n\t}\n\te := thrift.NewTApplicationException(0, \"\")\n\tif err := e.Read(proto); err != nil {\n\t\treturn fmt.Errorf(\"read exception error: %w\", err)\n\t}\n\tif err := proto.ReadMessageEnd(); err != nil {\n\t\treturn fmt.Errorf(\"read message end error: %w\", err)\n\t}\n\treturn e\n}\n\n```\n# Function Signature: func (t *ThriftMessageCodec) Deserialize(msg thrift.TStruct, b []byte) (err error) {\n# Natural Language Annotations: // Deserialize deserialize bytes into message. This is normal thrift deserialize func.\n// Notice: Binary generic use Decode instead of Deserialize.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (t *ThriftMessageCodec) Deserialize(msg thrift.TStruct, b []byte) (err error) {\n\ttrans := thrift.NewStreamTransportR(bytes.NewReader(b))\n\tproto := thrift.NewTBinaryProtocolTransport(trans)\n\tif err := msg.Read(proto); err != nil {\n\t\treturn fmt.Errorf(\"read message error: %w\", err)\n\t}\n\treturn nil\n}\n```", "```go\nfunc (t *ThriftMessageCodec) Deserialize(msg thrift.TStruct, b []byte) (err error) {\n\tt.tb.Reset()\n\tif _, err := t.tb.Write(b); err != nil {\n\t\treturn err\n\t}\n\treturn t.tProt.Read(context.Background(), msg.TStruct, t.tProt.ProtocolId())\n}\n```", "```go\nfunc (t *ThriftMessageCodec) Deserialize(msg thrift.TStruct, b []byte) (err error) {\n\tt.tb.Reset()\n\tif _, err := t.tb.Write(b); err != nil {\n\t\treturn fmt.Errorf(\"write bytes error: %w\", err)\n\t}\n\tif err := msg.Read(t.tProt); err != nil {\n\t\treturn fmt.Errorf(\"read struct error: %w\", err)\n\t}\n\treturn nil\n}\n```"]}, "kitex-pkg/utils/thrift.go-MarshalError": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/thrift.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n)\n\n// ThriftMessageCodec is used to codec thrift messages.\ntype ThriftMessageCodec struct {\n\ttb    *thrift.TMemoryBuffer\n\ttProt thrift.TProtocol\n}\n\n// NewThriftMessageCodec creates a new ThriftMessageCodec.\nfunc NewThriftMessageCodec() *ThriftMessageCodec {\n\ttransport := thrift.NewTMemoryBufferLen(1024)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\n\treturn &ThriftMessageCodec{\n\t\ttb:    transport,\n\t\ttProt: tProt,\n\t}\n}\n\n// Encode do thrift message encode.\n// Notice! msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n// Notice! seqID will be reset in kitex if the buffer is used for generic call in client side, set seqID=0 is suggested\n// when you call this method as client.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Decode do thrift message decode, notice: msg must be XXXArgs/XXXResult that the wrap struct for args and result, not the actual args or result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Serialize serialize message into bytes. This is normal thrift serialize func.\n// Notice: Binary generic use Encode instead of Serialize.\n\n\n\n\n\n\n\n\n\n\n// Deserialize deserialize bytes into message. This is normal thrift deserialize func.\n// Notice: Binary generic use Decode instead of Deserialize.\n\n\n\n\n\n\n\n\n\n\n\n// MarshalError convert go error to thrift exception, and encode exception over buffered binary transport.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalError decode binary and return error message\nfunc UnmarshalError(b []byte) error {\n\ttrans := thrift.NewStreamTransportR(bytes.NewReader(b))\n\tproto := thrift.NewTBinaryProtocolTransport(trans)\n\tif _, _, _, err := proto.ReadMessageBegin(); err != nil {\n\t\treturn fmt.Errorf(\"read message begin error: %w\", err)\n\t}\n\te := thrift.NewTApplicationException(0, \"\")\n\tif err := e.Read(proto); err != nil {\n\t\treturn fmt.Errorf(\"read exception error: %w\", err)\n\t}\n\tif err := proto.ReadMessageEnd(); err != nil {\n\t\treturn fmt.Errorf(\"read message end error: %w\", err)\n\t}\n\treturn e\n}\n\n```\n# Function Signature: func MarshalError(method string, err error) []byte {\n# Natural Language Annotations: // MarshalError convert go error to thrift exception, and encode exception over buffered binary transport.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc MarshalError(method string, err error) []byte {\n\ttrans := thrift.NewStreamTransportR(bytes.NewReader([]byte{}))\n\tproto := thrift.NewTBinaryProtocolTransport(trans)\n\n\te := thrift.NewTApplicationException(0, err.Error())\n\tproto.WriteMessageBegin(method, thrift.EXCEPTION, 0)\n\te.Write(proto)\n\tproto.WriteMessageEnd()\n\n\tif buffer, ok := proto.Transport().(*thrift.TMemoryBuffer); ok {\n\t\treturn buffer.Bytes()\n\t}\n\treturn nil\n}\n```", "```go\n// MarshalError convert go error to thrift exception, and encode exception over buffered binary transport.\nfunc MarshalError(method string, err error) []byte {\n\ttb := thrift.NewTMemoryBufferLen(1024)\n\ttProt := thrift.NewTBinaryProtocol(tb, true, true)\n\n\t// Create a new application exception\n\te := thrift.NewTApplicationException(thrift.INTERNAL_ERROR, err.Error())\n\te.SetClass(method)\n\n\t// Write the exception to the protocol\n\ttProt.WriteMessageBegin(context.Background(), \"exception\", thrift.EXCEPTION, 0)\n\te.Write(tProt)\n\ttProt.WriteMessageEnd(context.Background())\n\n\t// Get the buffer's contents\n\tbytes := make([]byte, tb.Len())\n\t_, _ = tb.Read(bytes)\n\n\treturn bytes\n}\n```", "```go\nfunc MarshalError(method string, err error) []byte {\n\ttrans := thrift.NewTMemoryBufferLen(1024)\n\tproto := thrift.NewTBinaryProtocol(trans, true, true)\n\tif err := proto.WriteMessageBegin(method, thrift.Exception, 0); err != nil {\n\t\tpanic(fmt.Errorf(\"write message begin error: %w\", err))\n\t}\n\te := thrift.NewTApplicationException(thrift.INTERNAL_ERROR, err.Error())\n\tif err := e.Write(proto); err != nil {\n\t\tpanic(fmt.Errorf(\"write exception error: %w\", err))\n\t}\n\tif err := proto.WriteMessageEnd(); err != nil {\n\t\tpanic(fmt.Errorf(\"write message end error: %w\", err))\n\t}\n\treturn trans.Bytes()\n}\n```"]}, "kitex-pkg/utils/strbuf.go-WriteInt64ToStringBuilder": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/strbuf.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n//lint:file-ignore SA6002 allocations cannot be avoided\n\npackage utils\n\nimport (\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n)\n\nvar intBytesPool = sync.Pool{\n\tNew: func() interface{} {\n\t\treturn make([]byte, 0, 20)\n\t},\n}\n\n// WriteInt64ToStringBuilder writes a int64 to a strings.Builder.\n\n\n\n\n\n\n\n```\n# Function Signature: func WriteInt64ToStringBuilder(sb *strings.Builder, value int64) {\n# Natural Language Annotations: // WriteInt64ToStringBuilder writes a int64 to a strings.Builder.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WriteInt64ToStringBuilder(sb *strings.Builder, value int64) {\n\t// Get a byte slice from the pool\n\tbuf := intBytesPool.Get().([]byte)\n\t// Convert the int64 to a string\n\tbuf = strconv.AppendInt(buf, value, 10)\n\t// Write the string to the strings.Builder\n\tsb.Write(buf)\n\t// Reset the slice\n\tbuf = buf[:0]\n\t// Put the slice back into the pool\n\tintBytesPool.Put(buf)\n}\n```\nThis function writes an `int64` value to a `strings.Builder`. It gets a byte slice from a sync pool, converts the `int64` value to a string using `strconv.AppendInt`, writes the string to the `strings.Builder` using `strings.Builder.Write`, resets the slice, and then puts it back into the pool. This reuses buffers to reduce memory allocations.", "```go\nfunc WriteInt64ToStringBuilder(sb *strings.Builder, value int64) {\n\tif sb.Cap() > sb.Len() {\n\t\tsb.WriteString(strconv.FormatInt(value, 10))\n\t} else {\n\t\tsb.Grow(10)\n\t\tsb.WriteString(strconv.FormatInt(value, 10))\n\t}\n}\n```", "```go\nfunc WriteInt64ToStringBuilder(sb *strings.Builder, value int64) {\n    b := intBytesPool.Get().([]byte)\n    b = strconv.AppendInt(b, value, 10)\n    sb.Grow(len(b))\n    sb.Write(b)\n    intBytesPool.Put(b[:0])\n}\n```"]}, "kitex-pkg/utils/rpcstats.go-CalculateEventCost": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/rpcstats.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage utils\n\nimport (\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n)\n\n// CalculateEventCost get events from rpcstats, and calculates the time duration of (end - start).\n// It returns 0 when get nil rpcinfo event of either stats event.\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func CalculateEventCost(rpcstats rpcinfo.RPCStats, start, end stats.Event) time.Duration {\n# Natural Language Annotations: // CalculateEventCost get events from rpcstats, and calculates the time duration of (end - start).\n// It returns 0 when get nil rpcinfo event of either stats event.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage utils\n\nimport (\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n)\n\n// CalculateEventCost get events from rpcstats, and calculates the time duration of (end - start).\n// It returns 0 when get nil rpcinfo event of either stats event.\nfunc CalculateEventCost(rpcstats rpcinfo.RPCStats, start, end stats.Event) time.Duration {\n\tstartTime, startExists := rpcstats.GetEvent(start)\n\tendTime, endExists := rpcstats.GetEvent(end)\n\n\tif !startExists || !endExists {\n\t\treturn 0\n\t}\n\n\treturn endTime.Sub(startTime)\n}\n```", "```go\nfunc CalculateEventCost(rpcstats rpcinfo.RPCStats, start, end stats.Event) time.Duration {\n\tstartEvent := rpcstats.GetEvent(start)\n\tendEvent := rpcstats.GetEvent(end)\n\n\tif startEvent == nil || endEvent == nil {\n\t\treturn 0\n\t}\n\n\tstartTime := startEvent.Time()\n\tendTime := endEvent.Time()\n\n\treturn endTime.Sub(startTime)\n}\n```", "```go\nfunc CalculateEventCost(rpcstats rpcinfo.RPCStats, start, end stats.Event) time.Duration {\n\tstartEvent := rpcstats.GetEvent(start)\n\tendEvent := rpcstats.GetEvent(end)\n\n\tif startEvent == nil || endEvent == nil {\n\t\treturn 0\n\t}\n\n\treturn endEvent.Time().Sub(startEvent.Time())\n}\n```"]}, "kitex-pkg/utils/json.go-_Map2JSONStr": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/json.go:\n```\n/*\n * Copyright 2024 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * MIT License\n *\n * Copyright (c) 2016 json-iterator\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in all\n * copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n * The source code of this file is written based on json-iterator,\n * all modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage utils\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\t\"unicode/utf16\"\n\t\"unicode/utf8\"\n\t\"unsafe\"\n)\n\n// const of json keyword char\nconst (\n\tEmptyJSON  = \"{}\"\n\tComma      = ','\n\tColon      = ':'\n\tDQuotation = '\"'\n\tLeftBrace  = '{'\n\tRightBrace = '}'\n)\n\nconst (\n\tt1 = 0x00 // 0000 0000\n\ttx = 0x80 // 1000 0000\n\tt2 = 0xC0 // 1100 0000\n\tt3 = 0xE0 // 1110 0000\n\tt4 = 0xF0 // 1111 0000\n\tt5 = 0xF8 // 1111 1000\n\n\tmaskx = 0x3F // 0011 1111\n\n\trune1Max = 1<<7 - 1\n\trune2Max = 1<<11 - 1\n\trune3Max = 1<<16 - 1\n\n\tsurrogateMin = 0xD800\n\tsurrogateMax = 0xDFFF\n\n\tmaxRune   = '\\U0010FFFF' // Maximum valid Unicode code point.\n\truneError = '\\uFFFD'     // the \"error\" Rune or \"Unicode replacement character\"\n\n\thex = \"0123456789abcdef\"\n)\n\n// Map2JSONStr transform map[string]string to json str, perf is better than use json lib directly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// JSONStr2Map transform json str to map[string]string, perf is better than use json lib directly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc readString(buf []byte, idx, lastIdx int) (string, int, error) {\n\tvar err error\n\tvar c byte\n\tvar isNull bool\n\tif c, idx, err = nextToken(buf, idx, lastIdx); err != nil {\n\t\treturn \"\", idx, err\n\t}\n\tvar str []byte\n\tif c == '\"' {\n\t\tstart := idx\n\t\tnoESC := true\n\t\tfor idx <= lastIdx {\n\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn \"\", idx, err\n\t\t\t}\n\t\t\tswitch c {\n\t\t\tcase '\"':\n\t\t\t\tif start < idx-1 {\n\t\t\t\t\tif noESC {\n\t\t\t\t\t\tstr = buf[start : idx-1]\n\t\t\t\t\t} else {\n\t\t\t\t\t\tstr = append(str, buf[start:idx-1]...)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn *(*string)(unsafe.Pointer(&str)), idx, nil\n\t\t\tcase '\\\\':\n\t\t\t\tif start < idx-1 {\n\t\t\t\t\tif noESC {\n\t\t\t\t\t\tstr = buf[start : idx-1]\n\t\t\t\t\t} else {\n\t\t\t\t\t\tstr = append(str, buf[start:idx-1]...)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\t\treturn \"\", idx, err\n\t\t\t\t}\n\t\t\t\tif str, idx, err = readEscapedChar(c, buf, idx, str, lastIdx); err != nil {\n\t\t\t\t\treturn \"\", 0, err\n\t\t\t\t}\n\t\t\t\tstart = idx\n\t\t\t\tnoESC = false\n\t\t\t}\n\t\t}\n\t} else if idx, isNull = checkNull(c, buf, idx, lastIdx); isNull {\n\t\treturn \"\", idx, nil\n\t}\n\terr = fmt.Errorf(\"json str is invalid, expects '\\\"' or n, but found %s\", string(c))\n\treturn *(*string)(unsafe.Pointer(&str)), idx, err\n}\n\nfunc readByte(buf []byte, idx, lastIdx int) (byte, int, error) {\n\tif lastIdx < idx {\n\t\treturn 0, -1, fmt.Errorf(\"readByte no more data\")\n\t}\n\tc := buf[idx]\n\tidx++\n\treturn c, idx, nil\n}\n\nfunc nextToken(buf []byte, idx, lastIdx int) (byte, int, error) {\n\tif lastIdx < idx {\n\t\treturn 0, -1, errors.New(\"nextToken no more data\")\n\t}\n\tvar c byte\n\tfor idx <= lastIdx {\n\t\tc = buf[idx]\n\t\tidx++\n\t\tswitch c {\n\t\tcase ' ', '\\n', '\\t', '\\r':\n\t\t\tcontinue\n\t\t}\n\t\treturn c, idx, nil\n\t}\n\treturn c, idx, nil\n}\n\nfunc checkNull(c byte, data []byte, idx, lastIdx int) (int, bool) {\n\tif c == 'n' {\n\t\tch, idx, _ := readByte(data, idx, lastIdx)\n\t\tif ch != 'u' {\n\t\t\tidx--\n\t\t\treturn idx, false\n\t\t}\n\t\tch, idx, _ = readByte(data, idx, lastIdx)\n\t\tif ch != 'l' {\n\t\t\tidx--\n\t\t\treturn idx, false\n\t\t}\n\t\tch, idx, _ = readByte(data, idx, lastIdx)\n\t\tif ch != 'l' {\n\t\t\tidx--\n\t\t\treturn idx, false\n\t\t}\n\t\treturn idx, true\n\t}\n\treturn idx, false\n}\n\nfunc readU4(buf []byte, idx, lastIdx int) (rune, int, error) {\n\tvar err error\n\tvar ret rune\n\tfor i := 0; i < 4; i++ {\n\t\tvar c byte\n\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\treturn ret, idx, err\n\t\t}\n\t\tif c >= '0' && c <= '9' {\n\t\t\tret = ret*16 + rune(c-'0')\n\t\t} else if c >= 'a' && c <= 'f' {\n\t\t\tret = ret*16 + rune(c-'a'+10)\n\t\t} else if c >= 'A' && c <= 'F' {\n\t\t\tret = ret*16 + rune(c-'A'+10)\n\t\t} else {\n\t\t\treturn ret, idx, fmt.Errorf(\"unicode invalid: expects 0~9 or a~f, but found %v\", string([]byte{c}))\n\t\t}\n\t}\n\treturn ret, idx, nil\n}\n\n// refer to json-iterator/go/iter_str readEscapedChar\nfunc readEscapedChar(c byte, buf []byte, idx int, str []byte, lastIdx int) ([]byte, int, error) {\n\tvar err error\n\tswitch c {\n\tcase 'u':\n\t\tvar r rune\n\t\tif r, idx, err = readU4(buf, idx, lastIdx); err != nil {\n\t\t\treturn str, idx, err\n\t\t}\n\t\t// \u662f\u5426\u662f\u6269\u5c55\u5b57\u7b26\n\t\tif utf16.IsSurrogate(r) {\n\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn str, idx, err\n\t\t\t}\n\t\t\tif c != '\\\\' {\n\t\t\t\tidx--\n\t\t\t\tstr = appendRune(str, r)\n\t\t\t\treturn str, idx, nil\n\t\t\t}\n\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn str, idx, err\n\t\t\t}\n\t\t\tif c != 'u' {\n\t\t\t\tstr = appendRune(str, r)\n\t\t\t\treturn readEscapedChar(c, buf, idx, str, lastIdx)\n\t\t\t}\n\t\t\tvar r2 rune\n\t\t\tif r2, idx, err = readU4(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn str, idx, err\n\t\t\t}\n\t\t\tcombined := utf16.DecodeRune(r, r2)\n\t\t\tif combined == '\\uFFFD' {\n\t\t\t\tstr = appendRune(str, r)\n\t\t\t\tstr = appendRune(str, r2)\n\t\t\t} else {\n\t\t\t\tstr = appendRune(str, combined)\n\t\t\t}\n\t\t} else {\n\t\t\tstr = appendRune(str, r)\n\t\t}\n\tcase '\"':\n\t\tstr = append(str, '\"')\n\tcase '\\\\':\n\t\tstr = append(str, '\\\\')\n\tcase '/':\n\t\tstr = append(str, '/')\n\tcase 'b':\n\t\tstr = append(str, '\\b')\n\tcase 'f':\n\t\tstr = append(str, '\\f')\n\tcase 'n':\n\t\tstr = append(str, '\\n')\n\tcase 'r':\n\t\tstr = append(str, '\\r')\n\tcase 't':\n\t\tstr = append(str, '\\t')\n\tdefault:\n\t\treturn str, idx, errors.New(\"invalid escape char after \\\\\")\n\t}\n\treturn str, idx, nil\n}\n\n// refer to json-iterator/go/stream_str writeStringSlowPath\nfunc wrapStrWithQuotation(s string, strBuilder *strings.Builder) {\n\tstrBuilder.WriteByte(DQuotation)\n\tvalLen := len(s)\n\ti := 0\n\tstart := i\n\tfor i < valLen {\n\t\tc := s[i]\n\t\tif c < utf8.RuneSelf && htmlSafeSet[c] {\n\t\t\ti++\n\t\t\tcontinue\n\t\t} else {\n\t\t\tif b := s[i]; b < utf8.RuneSelf {\n\t\t\t\tif start < i {\n\t\t\t\t\tstrBuilder.WriteString(s[start:i])\n\t\t\t\t}\n\t\t\t\tswitch b {\n\t\t\t\tcase '\\\\', '\"':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte(b)\n\t\t\t\tcase '\\n':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte('n')\n\t\t\t\tcase '\\r':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte('r')\n\t\t\t\tcase '\\t':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte('t')\n\t\t\t\tdefault:\n\t\t\t\t\t// This encodes bytes < 0x20 except for \\t, \\n and \\r.\n\t\t\t\t\t// If escapeHTML is set, it also escapes <, >, and &\n\t\t\t\t\t// because they can lead to security holes when\n\t\t\t\t\t// user-controlled strings are rendered into JSON\n\t\t\t\t\t// and served to some browsers.\n\t\t\t\t\tstrBuilder.WriteString(`\\u00`)\n\t\t\t\t\tstrBuilder.WriteByte(hex[b>>4])\n\t\t\t\t\tstrBuilder.WriteByte(hex[b&0xF])\n\t\t\t\t}\n\t\t\t\ti++\n\t\t\t\tstart = i\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc, size := utf8.DecodeRuneInString(s[i:])\n\t\t\tif c == utf8.RuneError && size == 1 {\n\t\t\t\tif start < i {\n\t\t\t\t\tstrBuilder.WriteString(s[start:i])\n\t\t\t\t}\n\t\t\t\tstrBuilder.WriteString(`\\ufffd`)\n\t\t\t\ti++\n\t\t\t\tstart = i\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// U+2028 is LINE SEPARATOR.\n\t\t\t// U+2029 is PARAGRAPH SEPARATOR.\n\t\t\t// They are both technically valid characters in JSON strings,\n\t\t\t// but don't work in JSONP, which has to be evaluated as JavaScript,\n\t\t\t// and can lead to security holes there. It is valid JSON to\n\t\t\t// escape them, so we do so unconditionally.\n\t\t\t// See http://timelessrepo.com/json-isnt-a-javascript-subset for discussion.\n\t\t\tif c == '\\u2028' || c == '\\u2029' {\n\t\t\t\tif start < i {\n\t\t\t\t\tstrBuilder.WriteString(s[start:i])\n\t\t\t\t}\n\t\t\t\tstrBuilder.WriteString(`\\u202`)\n\t\t\t\tstrBuilder.WriteByte(hex[c&0xF])\n\t\t\t\ti += size\n\t\t\t\tstart = i\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ti += size\n\t\t}\n\t}\n\tif start < valLen {\n\t\tstrBuilder.WriteString(s[start:])\n\t}\n\tstrBuilder.WriteByte(DQuotation)\n}\n\n// refer to json-iterator/go/iter_str appendRune\nfunc appendRune(p []byte, r rune) []byte {\n\t// Negative values are erroneous. Making it unsigned addresses the problem.\n\tswitch i := uint32(r); {\n\tcase i <= rune1Max:\n\t\tp = append(p, byte(r))\n\t\treturn p\n\tcase i <= rune2Max:\n\t\tp = append(p, t2|byte(r>>6))\n\t\tp = append(p, tx|byte(r)&maskx)\n\t\treturn p\n\tcase i > maxRune, surrogateMin <= i && i <= surrogateMax:\n\t\tr = runeError\n\t\tfallthrough\n\tcase i <= rune3Max:\n\t\tp = append(p, t3|byte(r>>12))\n\t\tp = append(p, tx|byte(r>>6)&maskx)\n\t\tp = append(p, tx|byte(r)&maskx)\n\t\treturn p\n\tdefault:\n\t\tp = append(p, t4|byte(r>>18))\n\t\tp = append(p, tx|byte(r>>12)&maskx)\n\t\tp = append(p, tx|byte(r>>6)&maskx)\n\t\tp = append(p, tx|byte(r)&maskx)\n\t\treturn p\n\t}\n}\n\nvar htmlSafeSet = [utf8.RuneSelf]bool{\n\t' ':      true,\n\t'!':      true,\n\t'\"':      false,\n\t'#':      true,\n\t'$':      true,\n\t'%':      true,\n\t'&':      false,\n\t'\\'':     true,\n\t'(':      true,\n\t')':      true,\n\t'*':      true,\n\t'+':      true,\n\t',':      true,\n\t'-':      true,\n\t'.':      true,\n\t'/':      true,\n\t'0':      true,\n\t'1':      true,\n\t'2':      true,\n\t'3':      true,\n\t'4':      true,\n\t'5':      true,\n\t'6':      true,\n\t'7':      true,\n\t'8':      true,\n\t'9':      true,\n\t':':      true,\n\t';':      true,\n\t'<':      false,\n\t'=':      true,\n\t'>':      false,\n\t'?':      true,\n\t'@':      true,\n\t'A':      true,\n\t'B':      true,\n\t'C':      true,\n\t'D':      true,\n\t'E':      true,\n\t'F':      true,\n\t'G':      true,\n\t'H':      true,\n\t'I':      true,\n\t'J':      true,\n\t'K':      true,\n\t'L':      true,\n\t'M':      true,\n\t'N':      true,\n\t'O':      true,\n\t'P':      true,\n\t'Q':      true,\n\t'R':      true,\n\t'S':      true,\n\t'T':      true,\n\t'U':      true,\n\t'V':      true,\n\t'W':      true,\n\t'X':      true,\n\t'Y':      true,\n\t'Z':      true,\n\t'[':      true,\n\t'\\\\':     false,\n\t']':      true,\n\t'^':      true,\n\t'_':      true,\n\t'`':      true,\n\t'a':      true,\n\t'b':      true,\n\t'c':      true,\n\t'd':      true,\n\t'e':      true,\n\t'f':      true,\n\t'g':      true,\n\t'h':      true,\n\t'i':      true,\n\t'j':      true,\n\t'k':      true,\n\t'l':      true,\n\t'm':      true,\n\t'n':      true,\n\t'o':      true,\n\t'p':      true,\n\t'q':      true,\n\t'r':      true,\n\t's':      true,\n\t't':      true,\n\t'u':      true,\n\t'v':      true,\n\t'w':      true,\n\t'x':      true,\n\t'y':      true,\n\t'z':      true,\n\t'{':      true,\n\t'|':      true,\n\t'}':      true,\n\t'~':      true,\n\t'\\u007f': true,\n}\n\n```\n# Function Signature: func _Map2JSONStr(mapInfo map[string]string) (str string, err error) {\n# Natural Language Annotations: // Map2JSONStr transform map[string]string to json str, perf is better than use json lib directly\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc _Map2JSONStr(mapInfo map[string]string) (str string, err error) {\n\tvar strBuilder strings.Builder\n\tstrBuilder.WriteByte(LeftBrace)\n\ti := 0\n\tfor key, value := range mapInfo {\n\t\tif i > 0 {\n\t\t\tstrBuilder.WriteByte(Comma)\n\t\t}\n\t\twrapStrWithQuotation(key, &strBuilder)\n\t\tstrBuilder.WriteByte(Colon)\n\t\twrapStrWithQuotation(value, &strBuilder)\n\t\ti++\n\t}\n\tstrBuilder.WriteByte(RightBrace)\n\treturn strBuilder.String(), nil\n}\n```", "```go\nfunc _Map2JSONStr(mapInfo map[string]string) (str string, err error) {\n\tstrBuilder := &strings.Builder{}\n\tstrBuilder.WriteByte(LeftBrace)\n\ti := 0\n\tfor k, v := range mapInfo {\n\t\tif i > 0 {\n\t\t\tstrBuilder.WriteByte(Comma)\n\t\t}\n\t\twrapStrWithQuotation(k, strBuilder)\n\t\tstrBuilder.WriteByte(Colon)\n\t\twrapStrWithQuotation(v, strBuilder)\n\t\ti++\n\t}\n\tstrBuilder.WriteByte(RightBrace)\n\treturn strBuilder.String(), nil\n}\n```", "```go\nfunc Map2JSONStr(mapInfo map[string]string) (str string, err error) {\n\tstr = \"{\"\n\ti := 0\n\tfor k, v := range mapInfo {\n\t\tif i > 0 {\n\t\t\tstr += \",\"\n\t\t}\n\t\tstr += \"\\\"\" + k + \"\\\":\"\n\t\tstr += \"\\\"\" + v + \"\\\"\"\n\t\ti++\n\t}\n\tstr += \"}\"\n\treturn str, nil\n}\n```"]}, "kitex-pkg/utils/json.go-_JSONStr2Map": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/json.go:\n```\n/*\n * Copyright 2024 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * MIT License\n *\n * Copyright (c) 2016 json-iterator\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy\n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights\n * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n * copies of the Software, and to permit persons to whom the Software is\n * furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in all\n * copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n * SOFTWARE.\n *\n * The source code of this file is written based on json-iterator,\n * all modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage utils\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\t\"unicode/utf16\"\n\t\"unicode/utf8\"\n\t\"unsafe\"\n)\n\n// const of json keyword char\nconst (\n\tEmptyJSON  = \"{}\"\n\tComma      = ','\n\tColon      = ':'\n\tDQuotation = '\"'\n\tLeftBrace  = '{'\n\tRightBrace = '}'\n)\n\nconst (\n\tt1 = 0x00 // 0000 0000\n\ttx = 0x80 // 1000 0000\n\tt2 = 0xC0 // 1100 0000\n\tt3 = 0xE0 // 1110 0000\n\tt4 = 0xF0 // 1111 0000\n\tt5 = 0xF8 // 1111 1000\n\n\tmaskx = 0x3F // 0011 1111\n\n\trune1Max = 1<<7 - 1\n\trune2Max = 1<<11 - 1\n\trune3Max = 1<<16 - 1\n\n\tsurrogateMin = 0xD800\n\tsurrogateMax = 0xDFFF\n\n\tmaxRune   = '\\U0010FFFF' // Maximum valid Unicode code point.\n\truneError = '\\uFFFD'     // the \"error\" Rune or \"Unicode replacement character\"\n\n\thex = \"0123456789abcdef\"\n)\n\n// Map2JSONStr transform map[string]string to json str, perf is better than use json lib directly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// JSONStr2Map transform json str to map[string]string, perf is better than use json lib directly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc readString(buf []byte, idx, lastIdx int) (string, int, error) {\n\tvar err error\n\tvar c byte\n\tvar isNull bool\n\tif c, idx, err = nextToken(buf, idx, lastIdx); err != nil {\n\t\treturn \"\", idx, err\n\t}\n\tvar str []byte\n\tif c == '\"' {\n\t\tstart := idx\n\t\tnoESC := true\n\t\tfor idx <= lastIdx {\n\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn \"\", idx, err\n\t\t\t}\n\t\t\tswitch c {\n\t\t\tcase '\"':\n\t\t\t\tif start < idx-1 {\n\t\t\t\t\tif noESC {\n\t\t\t\t\t\tstr = buf[start : idx-1]\n\t\t\t\t\t} else {\n\t\t\t\t\t\tstr = append(str, buf[start:idx-1]...)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn *(*string)(unsafe.Pointer(&str)), idx, nil\n\t\t\tcase '\\\\':\n\t\t\t\tif start < idx-1 {\n\t\t\t\t\tif noESC {\n\t\t\t\t\t\tstr = buf[start : idx-1]\n\t\t\t\t\t} else {\n\t\t\t\t\t\tstr = append(str, buf[start:idx-1]...)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\t\treturn \"\", idx, err\n\t\t\t\t}\n\t\t\t\tif str, idx, err = readEscapedChar(c, buf, idx, str, lastIdx); err != nil {\n\t\t\t\t\treturn \"\", 0, err\n\t\t\t\t}\n\t\t\t\tstart = idx\n\t\t\t\tnoESC = false\n\t\t\t}\n\t\t}\n\t} else if idx, isNull = checkNull(c, buf, idx, lastIdx); isNull {\n\t\treturn \"\", idx, nil\n\t}\n\terr = fmt.Errorf(\"json str is invalid, expects '\\\"' or n, but found %s\", string(c))\n\treturn *(*string)(unsafe.Pointer(&str)), idx, err\n}\n\nfunc readByte(buf []byte, idx, lastIdx int) (byte, int, error) {\n\tif lastIdx < idx {\n\t\treturn 0, -1, fmt.Errorf(\"readByte no more data\")\n\t}\n\tc := buf[idx]\n\tidx++\n\treturn c, idx, nil\n}\n\nfunc nextToken(buf []byte, idx, lastIdx int) (byte, int, error) {\n\tif lastIdx < idx {\n\t\treturn 0, -1, errors.New(\"nextToken no more data\")\n\t}\n\tvar c byte\n\tfor idx <= lastIdx {\n\t\tc = buf[idx]\n\t\tidx++\n\t\tswitch c {\n\t\tcase ' ', '\\n', '\\t', '\\r':\n\t\t\tcontinue\n\t\t}\n\t\treturn c, idx, nil\n\t}\n\treturn c, idx, nil\n}\n\nfunc checkNull(c byte, data []byte, idx, lastIdx int) (int, bool) {\n\tif c == 'n' {\n\t\tch, idx, _ := readByte(data, idx, lastIdx)\n\t\tif ch != 'u' {\n\t\t\tidx--\n\t\t\treturn idx, false\n\t\t}\n\t\tch, idx, _ = readByte(data, idx, lastIdx)\n\t\tif ch != 'l' {\n\t\t\tidx--\n\t\t\treturn idx, false\n\t\t}\n\t\tch, idx, _ = readByte(data, idx, lastIdx)\n\t\tif ch != 'l' {\n\t\t\tidx--\n\t\t\treturn idx, false\n\t\t}\n\t\treturn idx, true\n\t}\n\treturn idx, false\n}\n\nfunc readU4(buf []byte, idx, lastIdx int) (rune, int, error) {\n\tvar err error\n\tvar ret rune\n\tfor i := 0; i < 4; i++ {\n\t\tvar c byte\n\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\treturn ret, idx, err\n\t\t}\n\t\tif c >= '0' && c <= '9' {\n\t\t\tret = ret*16 + rune(c-'0')\n\t\t} else if c >= 'a' && c <= 'f' {\n\t\t\tret = ret*16 + rune(c-'a'+10)\n\t\t} else if c >= 'A' && c <= 'F' {\n\t\t\tret = ret*16 + rune(c-'A'+10)\n\t\t} else {\n\t\t\treturn ret, idx, fmt.Errorf(\"unicode invalid: expects 0~9 or a~f, but found %v\", string([]byte{c}))\n\t\t}\n\t}\n\treturn ret, idx, nil\n}\n\n// refer to json-iterator/go/iter_str readEscapedChar\nfunc readEscapedChar(c byte, buf []byte, idx int, str []byte, lastIdx int) ([]byte, int, error) {\n\tvar err error\n\tswitch c {\n\tcase 'u':\n\t\tvar r rune\n\t\tif r, idx, err = readU4(buf, idx, lastIdx); err != nil {\n\t\t\treturn str, idx, err\n\t\t}\n\t\t// \u662f\u5426\u662f\u6269\u5c55\u5b57\u7b26\n\t\tif utf16.IsSurrogate(r) {\n\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn str, idx, err\n\t\t\t}\n\t\t\tif c != '\\\\' {\n\t\t\t\tidx--\n\t\t\t\tstr = appendRune(str, r)\n\t\t\t\treturn str, idx, nil\n\t\t\t}\n\t\t\tif c, idx, err = readByte(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn str, idx, err\n\t\t\t}\n\t\t\tif c != 'u' {\n\t\t\t\tstr = appendRune(str, r)\n\t\t\t\treturn readEscapedChar(c, buf, idx, str, lastIdx)\n\t\t\t}\n\t\t\tvar r2 rune\n\t\t\tif r2, idx, err = readU4(buf, idx, lastIdx); err != nil {\n\t\t\t\treturn str, idx, err\n\t\t\t}\n\t\t\tcombined := utf16.DecodeRune(r, r2)\n\t\t\tif combined == '\\uFFFD' {\n\t\t\t\tstr = appendRune(str, r)\n\t\t\t\tstr = appendRune(str, r2)\n\t\t\t} else {\n\t\t\t\tstr = appendRune(str, combined)\n\t\t\t}\n\t\t} else {\n\t\t\tstr = appendRune(str, r)\n\t\t}\n\tcase '\"':\n\t\tstr = append(str, '\"')\n\tcase '\\\\':\n\t\tstr = append(str, '\\\\')\n\tcase '/':\n\t\tstr = append(str, '/')\n\tcase 'b':\n\t\tstr = append(str, '\\b')\n\tcase 'f':\n\t\tstr = append(str, '\\f')\n\tcase 'n':\n\t\tstr = append(str, '\\n')\n\tcase 'r':\n\t\tstr = append(str, '\\r')\n\tcase 't':\n\t\tstr = append(str, '\\t')\n\tdefault:\n\t\treturn str, idx, errors.New(\"invalid escape char after \\\\\")\n\t}\n\treturn str, idx, nil\n}\n\n// refer to json-iterator/go/stream_str writeStringSlowPath\nfunc wrapStrWithQuotation(s string, strBuilder *strings.Builder) {\n\tstrBuilder.WriteByte(DQuotation)\n\tvalLen := len(s)\n\ti := 0\n\tstart := i\n\tfor i < valLen {\n\t\tc := s[i]\n\t\tif c < utf8.RuneSelf && htmlSafeSet[c] {\n\t\t\ti++\n\t\t\tcontinue\n\t\t} else {\n\t\t\tif b := s[i]; b < utf8.RuneSelf {\n\t\t\t\tif start < i {\n\t\t\t\t\tstrBuilder.WriteString(s[start:i])\n\t\t\t\t}\n\t\t\t\tswitch b {\n\t\t\t\tcase '\\\\', '\"':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte(b)\n\t\t\t\tcase '\\n':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte('n')\n\t\t\t\tcase '\\r':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte('r')\n\t\t\t\tcase '\\t':\n\t\t\t\t\tstrBuilder.WriteByte('\\\\')\n\t\t\t\t\tstrBuilder.WriteByte('t')\n\t\t\t\tdefault:\n\t\t\t\t\t// This encodes bytes < 0x20 except for \\t, \\n and \\r.\n\t\t\t\t\t// If escapeHTML is set, it also escapes <, >, and &\n\t\t\t\t\t// because they can lead to security holes when\n\t\t\t\t\t// user-controlled strings are rendered into JSON\n\t\t\t\t\t// and served to some browsers.\n\t\t\t\t\tstrBuilder.WriteString(`\\u00`)\n\t\t\t\t\tstrBuilder.WriteByte(hex[b>>4])\n\t\t\t\t\tstrBuilder.WriteByte(hex[b&0xF])\n\t\t\t\t}\n\t\t\t\ti++\n\t\t\t\tstart = i\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc, size := utf8.DecodeRuneInString(s[i:])\n\t\t\tif c == utf8.RuneError && size == 1 {\n\t\t\t\tif start < i {\n\t\t\t\t\tstrBuilder.WriteString(s[start:i])\n\t\t\t\t}\n\t\t\t\tstrBuilder.WriteString(`\\ufffd`)\n\t\t\t\ti++\n\t\t\t\tstart = i\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// U+2028 is LINE SEPARATOR.\n\t\t\t// U+2029 is PARAGRAPH SEPARATOR.\n\t\t\t// They are both technically valid characters in JSON strings,\n\t\t\t// but don't work in JSONP, which has to be evaluated as JavaScript,\n\t\t\t// and can lead to security holes there. It is valid JSON to\n\t\t\t// escape them, so we do so unconditionally.\n\t\t\t// See http://timelessrepo.com/json-isnt-a-javascript-subset for discussion.\n\t\t\tif c == '\\u2028' || c == '\\u2029' {\n\t\t\t\tif start < i {\n\t\t\t\t\tstrBuilder.WriteString(s[start:i])\n\t\t\t\t}\n\t\t\t\tstrBuilder.WriteString(`\\u202`)\n\t\t\t\tstrBuilder.WriteByte(hex[c&0xF])\n\t\t\t\ti += size\n\t\t\t\tstart = i\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ti += size\n\t\t}\n\t}\n\tif start < valLen {\n\t\tstrBuilder.WriteString(s[start:])\n\t}\n\tstrBuilder.WriteByte(DQuotation)\n}\n\n// refer to json-iterator/go/iter_str appendRune\nfunc appendRune(p []byte, r rune) []byte {\n\t// Negative values are erroneous. Making it unsigned addresses the problem.\n\tswitch i := uint32(r); {\n\tcase i <= rune1Max:\n\t\tp = append(p, byte(r))\n\t\treturn p\n\tcase i <= rune2Max:\n\t\tp = append(p, t2|byte(r>>6))\n\t\tp = append(p, tx|byte(r)&maskx)\n\t\treturn p\n\tcase i > maxRune, surrogateMin <= i && i <= surrogateMax:\n\t\tr = runeError\n\t\tfallthrough\n\tcase i <= rune3Max:\n\t\tp = append(p, t3|byte(r>>12))\n\t\tp = append(p, tx|byte(r>>6)&maskx)\n\t\tp = append(p, tx|byte(r)&maskx)\n\t\treturn p\n\tdefault:\n\t\tp = append(p, t4|byte(r>>18))\n\t\tp = append(p, tx|byte(r>>12)&maskx)\n\t\tp = append(p, tx|byte(r>>6)&maskx)\n\t\tp = append(p, tx|byte(r)&maskx)\n\t\treturn p\n\t}\n}\n\nvar htmlSafeSet = [utf8.RuneSelf]bool{\n\t' ':      true,\n\t'!':      true,\n\t'\"':      false,\n\t'#':      true,\n\t'$':      true,\n\t'%':      true,\n\t'&':      false,\n\t'\\'':     true,\n\t'(':      true,\n\t')':      true,\n\t'*':      true,\n\t'+':      true,\n\t',':      true,\n\t'-':      true,\n\t'.':      true,\n\t'/':      true,\n\t'0':      true,\n\t'1':      true,\n\t'2':      true,\n\t'3':      true,\n\t'4':      true,\n\t'5':      true,\n\t'6':      true,\n\t'7':      true,\n\t'8':      true,\n\t'9':      true,\n\t':':      true,\n\t';':      true,\n\t'<':      false,\n\t'=':      true,\n\t'>':      false,\n\t'?':      true,\n\t'@':      true,\n\t'A':      true,\n\t'B':      true,\n\t'C':      true,\n\t'D':      true,\n\t'E':      true,\n\t'F':      true,\n\t'G':      true,\n\t'H':      true,\n\t'I':      true,\n\t'J':      true,\n\t'K':      true,\n\t'L':      true,\n\t'M':      true,\n\t'N':      true,\n\t'O':      true,\n\t'P':      true,\n\t'Q':      true,\n\t'R':      true,\n\t'S':      true,\n\t'T':      true,\n\t'U':      true,\n\t'V':      true,\n\t'W':      true,\n\t'X':      true,\n\t'Y':      true,\n\t'Z':      true,\n\t'[':      true,\n\t'\\\\':     false,\n\t']':      true,\n\t'^':      true,\n\t'_':      true,\n\t'`':      true,\n\t'a':      true,\n\t'b':      true,\n\t'c':      true,\n\t'd':      true,\n\t'e':      true,\n\t'f':      true,\n\t'g':      true,\n\t'h':      true,\n\t'i':      true,\n\t'j':      true,\n\t'k':      true,\n\t'l':      true,\n\t'm':      true,\n\t'n':      true,\n\t'o':      true,\n\t'p':      true,\n\t'q':      true,\n\t'r':      true,\n\t's':      true,\n\t't':      true,\n\t'u':      true,\n\t'v':      true,\n\t'w':      true,\n\t'x':      true,\n\t'y':      true,\n\t'z':      true,\n\t'{':      true,\n\t'|':      true,\n\t'}':      true,\n\t'~':      true,\n\t'\\u007f': true,\n}\n\n```\n# Function Signature: func _JSONStr2Map(jsonStr string) (mapInfo map[string]string, err error) {\n# Natural Language Annotations: // JSONStr2Map transform json str to map[string]string, perf is better than use json lib directly\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc _JSONStr2Map(jsonStr string) (mapInfo map[string]string, err error) {\n\tmapInfo = make(map[string]string)\n\tstr := []byte(jsonStr)\n\tlastIdx := len(str) - 1\n\tidx := 0\n\tvar key string\n\tvar value string\n\tfor idx <= lastIdx {\n\t\tif key, idx, err = readString(str, idx, lastIdx); err != nil {\n\t\t\treturn\n\t\t}\n\t\tif idx > lastIdx {\n\t\t\terr = fmt.Errorf(\"json str is invalid, expects ':', but found nothing\")\n\t\t\treturn\n\t\t}\n\t\tif str[idx] != Colon {\n\t\t\terr = fmt.Errorf(\"json str is invalid, expects ':', but found %s\", string(str[idx]))\n\t\t\treturn\n\t\t}\n\t\tidx++\n\t\tif value, idx, err = readString(str, idx, lastIdx); err != nil {\n\t\t\treturn\n\t\t}\n\t\tmapInfo[key] = value\n\t\tif idx > lastIdx {\n\t\t\terr = fmt.Errorf(\"json str is invalid, expects ',', but found nothing\")\n\t\t\treturn\n\t\t}\n\t\tif str[idx] != Comma && str[idx] != RightBrace {\n\t\t\terr = fmt.Errorf(\"json str is invalid, expects ',', but found %s\", string(str[idx]))\n\t\t\treturn\n\t\t}\n\t\tif str[idx] == Comma {\n\t\t\tidx++\n\t\t}\n\t}\n\treturn\n}\n```", "```go\nfunc _JSONStr2Map(jsonStr string) (mapInfo map[string]string, err error) {\n\tmapInfo = make(map[string]string)\n\tlength := len(jsonStr)\n\tif length < 2 || jsonStr[0] != '{' || jsonStr[length-1] != '}' {\n\t\treturn nil, errors.New(\"invalid json string\")\n\t}\n\tjsonStr = jsonStr[1 : length-1]\n\tkey := \"\"\n\tval := \"\"\n\tisKey := true\n\tidx := 0\n\tfor idx < len(jsonStr) {\n\t\tc := jsonStr[idx]\n\t\tswitch c {\n\t\tcase '\"':\n\t\t\tif isKey {\n\t\t\t\tkey, idx, err = readString([]byte(jsonStr), idx, len(jsonStr))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tval, idx, err = readString([]byte(jsonStr), idx, len(jsonStr))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tmapInfo[key] = val\n\t\t\t}\n\t\t\tisKey = !isKey\n\t\tcase ':':\n\t\t\tidx++\n\t\tcase ',':\n\t\t\tidx++\n\t\t\tisKey = !isKey\n\t\tcase ' ', '\\n', '\\t', '\\r':\n\t\t\tidx++\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"json str is invalid, expects '\\\"' or n, but found %s\", string(c))\n\t\t}\n\t}\n\treturn mapInfo, nil\n}\n```", "```go\nfunc _JSONStr2Map(jsonStr string) (mapInfo map[string]string, err error) {\n\tbuf := []byte(jsonStr)\n\tlength := len(buf)\n\tmapInfo = make(map[string]string)\n\n\tvar c byte\n\tvar idx int\n\tvar key, value string\n\n\terr = func() error {\n\t\t// start with '{'\n\t\tif c, idx, err = readByte(buf, idx, length-1); err != nil || c != '{' {\n\t\t\treturn fmt.Errorf(\"json str is invalid, expects '{', but found %s\", string(c))\n\t\t}\n\n\t\tfor idx <= length-1 {\n\t\t\tif c, idx, err = readByte(buf, idx, length-1); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif c == '}' { // end with '}'\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tif c != '\"' {\n\t\t\t\treturn fmt.Errorf(\"json str is invalid, expects '\\\"', but found %s\", string(c))\n\t\t\t}\n\n\t\t\t// read key\n\t\t\tif key, idx, err = readString(buf, idx, length-1); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tif c, idx, err = readByte(buf, idx, length-1); err != nil || c != ':' {\n\t\t\t\treturn fmt.Errorf(\"json str is invalid, expects ':', but found %s\", string(c))\n\t\t\t}\n\n\t\t\t// read value\n\t\t\tif value, idx, err = readString(buf, idx, length-1); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tmapInfo[key] = value\n\n\t\t\tif c, idx, err = readByte(buf, idx, length-1); err != nil || c != ',' {\n\t\t\t\treturn fmt.Errorf(\"json str is invalid, expects ',', but found %s\", string(c))\n\t\t\t}\n\t\t}\n\n\t\treturn fmt.Errorf(\"json str is invalid, expects '}', but found EOF\")\n\t}()\n\n\treturn\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetCaller": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetCaller(ctx context.Context) (string, bool) {\n# Natural Language Annotations: // GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc GetCaller(ctx context.Context) (string, bool) {\n\tri := rpcinfo.GetRPCInfo(ctx)\n\tif ri == nil {\n\t\treturn \"\", false\n\t}\n\treturn ri.From().ServiceName(), true\n}\n```", "```go\nfunc GetCaller(ctx context.Context) (string, bool) {\n\tri := rpcinfo.GetRPCInfo(ctx)\n\tif ri == nil {\n\t\treturn \"\", false\n\t}\n\treturn ri.From().ServiceName(), true\n}\n```", "Here is the generated code:\n```go\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\nfunc GetCaller(ctx context.Context) (string, bool) {\n\tri := rpcinfo.GetRPCInfo(ctx)\n\tif ri == nil {\n\t\treturn \"\", false\n\t}\n\treturn ri.From().ServiceName(), true\n}\n```\nThis function first retrieves the RPCInfo from the context. If it is not found, it returns an empty string and false. If it is found, it returns the Service Name from the RPCInfo and true."]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetMethod": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetMethod(ctx context.Context) (string, bool) {\n# Natural Language Annotations: // GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc GetMethod(ctx context.Context) (string, bool) {\n\tri := rpcinfo.GetRPCInfo(ctx)\n\tif ri == nil {\n\t\treturn \"\", false\n\t}\n\treturn ri.Invocation().Method(), true\n}\n```", "```go\nfunc GetMethod(ctx context.Context) (string, bool) {\n    ri := rpcinfo.GetRPCInfo(ctx)\n    if ri == nil {\n        return \"\", false\n    }\n    return ri.Invocation().Method(), true\n}\n```", "```go\nfunc GetMethod(ctx context.Context) (string, bool) {\n    ri := rpcinfo.GetRPCInfo(ctx)\n    if ri == nil {\n        return \"\", false\n    }\n    return ri.Invocation().Method(), true\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetCallerHandlerMethod": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetCallerHandlerMethod(ctx context.Context) (string, bool) {\n# Natural Language Annotations: // GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc GetCallerHandlerMethod(ctx context.Context) (string, bool) {\n\tri := rpcinfo.GetRPCInfo(ctx)\n\tif ri == nil {\n\t\treturn \"\", false\n\t}\n\n\tmethod := ri.Invocation().Method()\n\tif method == \"\" {\n\t\treturn \"\", false\n\t}\n\n\treturn method, true\n}\n```", "```go\nfunc GetCallerHandlerMethod(ctx context.Context) (string, bool) {\n    rpcInfo, ok := rpcinfo.GetRPCInfo(ctx)\n    if !ok {\n        return \"\", false\n    }\n    serverInfo := rpcInfo.From()\n\n    method := serverInfo.Method()\n    if method == \"\" {\n        return \"\", false\n    }\n\n    return method, true\n}\n```\nThis function retrieves the RPCInfo from the context, checks if it's a server, and if so, retrieves the method name. If either of these steps fails, it returns an empty string and false.", "```go\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\nfunc GetCallerHandlerMethod(ctx context.Context) (string, bool) {\n\tri := rpcinfo.GetRPCInfo(ctx)\n\tif ri == nil {\n\t\treturn \"\", false\n\t}\n\tmethod := ri.Invocation().Method()\n\tif method == \"\" {\n\t\treturn \"\", false\n\t}\n\treturn method, true\n}\n```\n\nThe `GetCallerHandlerMethod` function is used to get the method name of caller from the `context.Context`. It uses the `rpcinfo.GetRPCInfo` function to get the `RPCInfo` from the `context.Context`. If the `RPCInfo` is `nil`, it returns an empty string and `false` to indicate failure. If the method name is empty, it also returns an empty string and `false` to indicate failure. Otherwise, it returns the method name and `true` to indicate success."]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetIDLServiceName": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetIDLServiceName(ctx context.Context) (string, bool) {\n# Natural Language Annotations: // GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc GetIDLServiceName(ctx context.Context) (string, bool) {\n    ri := rpcinfo.GetRPCInfo(ctx)\n    if ri == nil {\n        return \"\", false\n    }\n    srv := ri.To()\n    if srv == nil {\n        return \"\", false\n    }\n    return srv.ServiceName(), true\n}\n```", "```go\nfunc GetIDLServiceName(ctx context.Context) (string, bool) {\n\tri := rpcinfo.GetRPCInfo(ctx)\n\tif ri == nil {\n\t\treturn \"\", false\n\t}\n\treturn ri.Invocation().PackageName(), true\n}\n```\nThis function `GetIDLServiceName()` gets the service name which is defined in the IDL (Interface Definition Language) using the context provided. It gets the RPCInfo from the context using the `rpcinfo.GetRPCInfo()` function. If the RPCInfo is not nil, it returns the package name of the RPC invocation, which should be the IDL service name, and returns `true` to indicate success. If the RPCInfo is nil, it returns an empty string and `false` to indicate failure.", "```go\nfunc GetIDLServiceName(ctx context.Context) (string, bool) {\n    ri := rpcinfo.GetRPCInfo(ctx)\n    if ri == nil {\n        return \"\", false\n    }\n    return ri.To().ServiceName(), true\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetCallerAddr": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetCallerAddr(ctx context.Context) (net.Addr, bool) {\n# Natural Language Annotations: // GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\nfunc GetCallerAddr(ctx context.Context) (net.Addr, bool) {\n\tri := rpcinfo.GetRPCInfo(ctx)\n\tif ri == nil || ri.From() == nil {\n\t\treturn nil, false\n\t}\n\treturn ri.From().Addr, true\n}\n```", "```go\nfunc GetCallerAddr(ctx context.Context) (net.Addr, bool) {\n\tri := rpcinfo.GetRPCInfo(ctx)\n\tif ri == nil {\n\t\treturn nil, false\n\t}\n\taddr := ri.From()\n\tif addr == nil {\n\t\treturn nil, false\n\t}\n\treturn addr, true\n}\n```", "```go\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\nfunc GetCallerAddr(ctx context.Context) (net.Addr, bool) {\n\tri := rpcinfo.GetRPCInfo(ctx)\n\tif ri == nil {\n\t\treturn nil, false\n\t}\n\ta := ri.From()\n\tif a == nil {\n\t\treturn nil, false\n\t}\n\treturn a, true\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetCallerIP": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetCallerIP(ctx context.Context) (string, bool) {\n# Natural Language Annotations: // GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\nfunc GetCallerIP(ctx context.Context) (string, bool) {\n\tri := rpcinfo.GetRPCInfo(ctx)\n\tif ri == nil {\n\t\treturn \"\", false\n\t}\n\tip, _, err := net.SplitHostPort(ri.From().Address().String())\n\tif err != nil {\n\t\treturn \"\", false\n\t}\n\treturn ip, true\n}\n```\n\nThis function first retrieves the RPCInfo from the context using the `rpcinfo.GetRPCInfo` method. If the RPCInfo is nil, the function returns an empty string and false.\n\nNext, the function uses the `net.SplitHostPort` function to separate the host and port from the address string. If an error occurs during this process, the function again returns an empty string and false.\n\nOtherwise, the function returns the IP address as a string and true.", "```go\nfunc GetCallerIP(ctx context.Context) (string, bool) {\n    ri := rpcinfo.GetRPCInfo(ctx)\n    if ri == nil {\n        return \"\", false\n    }\n\n    host, port, err := net.SplitHostPort(ri.From().Address())\n    if err != nil {\n        return \"\", false\n    }\n\n    return host, port != \"\"\n}\n```\nThis function `GetCallerIP` gets the caller's IP address from the context. It first gets the `rpcinfo.RPCInfo` from the context. If the `rpcinfo.RPCInfo` is nil, it returns an empty string and `false` because it failed to get the information. If it's not nil, it splits the address into host and port. If there's an error during the split, it returns an empty string and `false`. If there's no error, it returns the host and `true` to indicate success.", "```go\nfunc GetCallerIP(ctx context.Context) (string, bool) {\n\tr := rpcinfo.GetRPCInfo(ctx)\n\tif r == nil {\n\t\treturn \"\", false\n\t}\n\n\tip, port, err := net.SplitHostPort(r.From().Address())\n\tif err != nil {\n\t\treturn \"\", false\n\t}\n\n\treturn ip, true\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetTransportProtocol": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetTransportProtocol(ctx context.Context) (string, bool) {\n# Natural Language Annotations: // GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc GetTransportProtocol(ctx context.Context) (string, bool) {\n\tri := rpcinfo.GetRPCInfo(ctx)\n\tif ri == nil {\n\t\treturn \"\", false\n\t}\n\treturn ri.From().TransportProtocol(), true\n}\n```\nPlease note that this function assumes that the `rpcinfo.RPCInfo` object is available in the context. If it is not, the function will return false.", "```go\nfunc GetTransportProtocol(ctx context.Context) (string, bool) {\n    ri := rpcinfo.GetRPCInfo(ctx)\n    if ri == nil {\n        return \"\", false\n    }\n    return ri.Invocation().Protocol().Name(), true\n}\n```", "```go\nfunc GetTransportProtocol(ctx context.Context) (string, bool) {\n\tri := rpcinfo.GetRPCInfo(ctx)\n\tif ri == nil {\n\t\treturn \"\", false\n\t}\n\n\treturn ri.Invocation().Protocol(), true\n}\n```"]}, "kitex-pkg/utils/kitexutil/kitexutil.go-GetRPCInfo": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/utils/kitexutil/kitexutil.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package kitexutil provides some util methods to get RPC information\npackage kitexutil\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// GetCaller is used to get the Service Name of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetMethod is used to get the current RPC Method name.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerHandlerMethod is used to get the method name of caller.\n// Only the caller is a Kitex server will have this method information by default, or you can set K_METHOD into context.Context then kitex will get it.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetIDLServiceName gets the serviceName which defined in IDL.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerAddr is used for the server to get the Address of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetCallerIP is used for the server to get the IP of the caller.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GetTransportProtocol gets the transport protocol of the request.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\n\n\n\n\n\n\n\n\n\n// GetRealReqFromKitexArgs assert the req to be KitexArgs and return the real request if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealReqFromKitexArgs(req interface{}) interface{} {\n\tif arg, ok := req.(utils.KitexArgs); ok {\n\t\treturn arg.GetFirstArgument()\n\t}\n\treturn nil\n}\n\n// GetRealRespFromKitexResult assert the req to be KitexResult and return the real response if succeeded, otherwise return nil.\n// This method should be used in the middleware.\nfunc GetRealRespFromKitexResult(resp interface{}) interface{} {\n\tif res, ok := resp.(utils.KitexResult); ok {\n\t\treturn res.GetResult()\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func GetRPCInfo(ctx context.Context) (rpcinfo.RPCInfo, bool) {\n# Natural Language Annotations: // GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage kitexutil\n\nimport (\n\t\"context\"\n\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\nfunc GetRPCInfo(ctx context.Context) (rpcinfo.RPCInfo, bool) {\n\tri := rpcinfo.GetRPCInfo(ctx)\n\tif ri == nil {\n\t\treturn nil, false\n\t}\n\treturn ri, true\n}\n```\nThis function uses the `GetRPCInfo` method from the `rpcinfo` package to get the RPCInfo from the given context. If the RPCInfo is nil, it returns false. Otherwise, it returns the RPCInfo and true.", "```go\nfunc GetRPCInfo(ctx context.Context) (rpcinfo.RPCInfo, bool) {\n\tri := rpcinfo.GetRPCInfo(ctx)\n\tif ri != nil {\n\t\treturn ri, true\n\t}\n\treturn nil, false\n}\n```", "```go\n// GetRPCInfo gets the RPCInfo in ctx.\n// Return false if failed to get the information.\nfunc GetRPCInfo(ctx context.Context) (rpcinfo.RPCInfo, bool) {\n\tri := rpcinfo.GetRPCInfo(ctx)\n\tif ri == nil {\n\t\treturn nil, false\n\t}\n\treturn ri, true\n}\n```"]}, "kitex-pkg/remote/trans_errors.go-NewTransError": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans_errors.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage remote\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\n// corresponding with thrift TApplicationException, cannot change it\nconst (\n\tUnknownApplicationException = 0\n\tUnknownMethod               = 1\n\tInvalidMessageTypeException = 2\n\tWrongMethodName             = 3\n\tBadSequenceID               = 4\n\tMissingResult               = 5\n\tInternalError               = 6\n\tProtocolError               = 7\n\tInvalidTransform            = 8\n\tInvalidProtocol             = 9\n\tUnsupportedClientType       = 10\n\t// kitex's own type id from number 20\n\tUnknownService = 20\n\tNoServiceName  = 21\n)\n\nvar defaultTransErrorMessage = map[int32]string{\n\tUnknownApplicationException: \"unknown application exception\",\n\tUnknownMethod:               \"unknown method\",\n\tInvalidMessageTypeException: \"invalid message type\",\n\tWrongMethodName:             \"wrong method name\",\n\tBadSequenceID:               \"bad sequence ID\",\n\tMissingResult:               \"missing result\",\n\tInternalError:               \"unknown internal error\",\n\tProtocolError:               \"unknown protocol error\",\n\tInvalidTransform:            \"Invalid transform\",\n\tInvalidProtocol:             \"Invalid protocol\",\n\tUnsupportedClientType:       \"Unsupported client type\",\n\tUnknownService:              \"unknown service\",\n}\n\n// TransError is the error that can be transmitted, it corresponds to TApplicationException in Thrift\ntype TransError struct {\n\tmessage string\n\ttypeID  int32\n\trawErr  error\n}\n\n// Error implements the error interface.\nfunc (e TransError) Error() string {\n\tif e.message != \"\" {\n\t\treturn e.message\n\t}\n\treturn defaultTransErrorMessage[e.typeID]\n}\n\n// TypeID return err type id\nfunc (e TransError) TypeID() int32 {\n\treturn e.typeID\n}\n\n// Unwrap the transError to expose raw error\nfunc (e TransError) Unwrap() error {\n\treturn e.rawErr\n}\n\n// Is to check if inner error that transError wrap is target error\nfunc (e TransError) Is(target error) bool {\n\treturn e == target || errors.Is(e.rawErr, target)\n}\n\n// AppendMessage append extra msg for TransError\nfunc (e TransError) AppendMessage(extraMsg string) *TransError {\n\tif extraMsg == \"\" {\n\t\treturn &e\n\t}\n\tmsg := fmt.Sprintf(\"%s %s\", e.message, extraMsg)\n\t// should not modify origin error\n\treturn &TransError{message: msg, typeID: e.typeID, rawErr: e.rawErr}\n}\n\n// NewTransErrorWithMsg to build TransError with typeID and errMsg\nfunc NewTransErrorWithMsg(typeID int32, message string) *TransError {\n\treturn &TransError{message: message, typeID: typeID}\n}\n\n// NewTransError to build TransError with typeID and rawErr.\n// rawErr can be used by errors.Is(target) to check err type, like read timeout.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// TypeId is used to assert Error with has 'TypeID() int32'\ntype TypeID interface {\n\tTypeID() int32\n}\n\n// TypeId is used to assert Error with has 'TypeId() int32' like TApplicationException\ntype TypeId interface {\n\tTypeId() int32\n}\n\n```\n# Function Signature: func NewTransError(typeID int32, err error) *TransError {\n# Natural Language Annotations: // NewTransError to build TransError with typeID and rawErr.\n// rawErr can be used by errors.Is(target) to check err type, like read timeout.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewTransError(typeID int32, err error) *TransError {\n\treturn &TransError{message: defaultTransErrorMessage[typeID], typeID: typeID, rawErr: err}\n}\n```", "```go\nfunc NewTransError(typeID int32, err error) *TransError {\n\treturn &TransError{\n\t\tmessage: defaultTransErrorMessage[typeID],\n\t\ttypeID:  typeID,\n\t\trawErr:  err,\n\t}\n}\n```", "```go\nfunc NewTransError(typeID int32, err error) *TransError {\n    return &TransError{\n        message: defaultTransErrorMessage[typeID],\n        typeID:  typeID,\n        rawErr:  err,\n    }\n}\n```"]}, "kitex-pkg/remote/trans/gonet/bytebuffer.go-NewBufferReader": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/gonet/bytebuffer.go:\n```\n/*\n * Copyright 2022 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage gonet\n\nimport (\n\t\"errors\"\n\t\"io\"\n\t\"sync\"\n\n\t\"github.com/cloudwego/netpoll\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n)\n\nvar rwPool sync.Pool\n\nfunc init() {\n\trwPool.New = newBufferReadWriter\n}\n\nvar _ remote.ByteBuffer = &bufferReadWriter{}\n\ntype bufferReadWriter struct {\n\treader netpoll.Reader\n\twriter netpoll.Writer\n\n\tioReader io.Reader\n\tioWriter io.Writer\n\n\treadSize int\n\tstatus   int\n}\n\nfunc newBufferReadWriter() interface{} {\n\treturn &bufferReadWriter{}\n}\n\n// NewBufferReader creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReader.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewBufferWriter creates a new remote.ByteBuffer using the given netpoll.ZeroCopyWriter.\nfunc NewBufferWriter(iw io.Writer) remote.ByteBuffer {\n\trw := rwPool.Get().(*bufferReadWriter)\n\trw.writer = netpoll.NewWriter(iw)\n\trw.ioWriter = iw\n\trw.status = remote.BitWritable\n\treturn rw\n}\n\n// NewBufferReadWriter creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReadWriter.\nfunc NewBufferReadWriter(irw io.ReadWriter) remote.ByteBuffer {\n\trw := rwPool.Get().(*bufferReadWriter)\n\trw.writer = netpoll.NewWriter(irw)\n\trw.reader = netpoll.NewReader(irw)\n\trw.ioWriter = irw\n\trw.ioReader = irw\n\trw.status = remote.BitWritable | remote.BitReadable\n\treturn rw\n}\n\nfunc (rw *bufferReadWriter) readable() bool {\n\treturn rw.status&remote.BitReadable != 0\n}\n\nfunc (rw *bufferReadWriter) writable() bool {\n\treturn rw.status&remote.BitWritable != 0\n}\n\nfunc (rw *bufferReadWriter) Next(n int) (p []byte, err error) {\n\tif !rw.readable() {\n\t\treturn nil, errors.New(\"unreadable buffer, cannot support Next\")\n\t}\n\tif p, err = rw.reader.Next(n); err == nil {\n\t\trw.readSize += n\n\t}\n\treturn\n}\n\nfunc (rw *bufferReadWriter) Peek(n int) (buf []byte, err error) {\n\tif !rw.readable() {\n\t\treturn nil, errors.New(\"unreadable buffer, cannot support Peek\")\n\t}\n\treturn rw.reader.Peek(n)\n}\n\nfunc (rw *bufferReadWriter) Skip(n int) (err error) {\n\tif !rw.readable() {\n\t\treturn errors.New(\"unreadable buffer, cannot support Skip\")\n\t}\n\treturn rw.reader.Skip(n)\n}\n\nfunc (rw *bufferReadWriter) ReadableLen() (n int) {\n\tif !rw.readable() {\n\t\treturn -1\n\t}\n\treturn rw.reader.Len()\n}\n\nfunc (rw *bufferReadWriter) ReadString(n int) (s string, err error) {\n\tif !rw.readable() {\n\t\treturn \"\", errors.New(\"unreadable buffer, cannot support ReadString\")\n\t}\n\tif s, err = rw.reader.ReadString(n); err == nil {\n\t\trw.readSize += n\n\t}\n\treturn\n}\n\nfunc (rw *bufferReadWriter) ReadBinary(n int) (p []byte, err error) {\n\tif !rw.readable() {\n\t\treturn p, errors.New(\"unreadable buffer, cannot support ReadBinary\")\n\t}\n\tif p, err = rw.reader.ReadBinary(n); err == nil {\n\t\trw.readSize += n\n\t}\n\treturn\n}\n\nfunc (rw *bufferReadWriter) Read(p []byte) (n int, err error) {\n\tif !rw.readable() {\n\t\treturn -1, errors.New(\"unreadable buffer, cannot support Read\")\n\t}\n\tif rw.ioReader != nil {\n\t\treturn rw.ioReader.Read(p)\n\t}\n\treturn -1, errors.New(\"ioReader is nil\")\n}\n\nfunc (rw *bufferReadWriter) ReadLen() (n int) {\n\treturn rw.readSize\n}\n\nfunc (rw *bufferReadWriter) Malloc(n int) (buf []byte, err error) {\n\tif !rw.writable() {\n\t\treturn nil, errors.New(\"unwritable buffer, cannot support Malloc\")\n\t}\n\treturn rw.writer.Malloc(n)\n}\n\nfunc (rw *bufferReadWriter) MallocLen() (length int) {\n\tif !rw.writable() {\n\t\treturn -1\n\t}\n\treturn rw.writer.MallocLen()\n}\n\nfunc (rw *bufferReadWriter) WriteString(s string) (n int, err error) {\n\tif !rw.writable() {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support WriteString\")\n\t}\n\treturn rw.writer.WriteString(s)\n}\n\nfunc (rw *bufferReadWriter) WriteBinary(b []byte) (n int, err error) {\n\tif !rw.writable() {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support WriteBinary\")\n\t}\n\treturn rw.writer.WriteBinary(b)\n}\n\nfunc (rw *bufferReadWriter) Flush() (err error) {\n\tif !rw.writable() {\n\t\treturn errors.New(\"unwritable buffer, cannot support Flush\")\n\t}\n\treturn rw.writer.Flush()\n}\n\nfunc (rw *bufferReadWriter) Write(p []byte) (n int, err error) {\n\tif !rw.writable() {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support Write\")\n\t}\n\tif rw.ioWriter != nil {\n\t\treturn rw.ioWriter.Write(p)\n\t}\n\treturn -1, errors.New(\"ioWriter is nil\")\n}\n\nfunc (rw *bufferReadWriter) Release(e error) (err error) {\n\tif rw.reader != nil {\n\t\terr = rw.reader.Release()\n\t}\n\trw.zero()\n\trwPool.Put(rw)\n\treturn\n}\n\n// WriteDirect is a way to write []byte without copying, and splits the original buffer.\n\n\n\n\n\n\n\nfunc (rw *bufferReadWriter) AppendBuffer(buf remote.ByteBuffer) (err error) {\n\tsubBuf, ok := buf.(*bufferReadWriter)\n\tif !ok {\n\t\treturn errors.New(\"AppendBuffer failed, Buffer is not bufferReadWriter\")\n\t}\n\tif err = rw.writer.Append(subBuf.writer); err != nil {\n\t\treturn\n\t}\n\treturn buf.Release(nil)\n}\n\n// NewBuffer returns a new writable remote.ByteBuffer.\nfunc (rw *bufferReadWriter) NewBuffer() remote.ByteBuffer {\n\tpanic(\"unimplemented\")\n}\n\nfunc (rw *bufferReadWriter) Bytes() (buf []byte, err error) {\n\tpanic(\"unimplemented\")\n}\n\nfunc (rw *bufferReadWriter) zero() {\n\trw.reader = nil\n\trw.writer = nil\n\trw.ioReader = nil\n\trw.ioWriter = nil\n\trw.readSize = 0\n\trw.status = 0\n}\n\n```\n# Function Signature: func NewBufferReader(ir io.Reader) remote.ByteBuffer {\n# Natural Language Annotations: // NewBufferReader creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReader.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewBufferReader(ir io.Reader) remote.ByteBuffer {\n\trw := rwPool.Get().(*bufferReadWriter)\n\trw.reader = netpoll.NewReader(ir)\n\trw.ioReader = ir\n\trw.status = remote.BitReadable\n\treturn rw\n}\n```", "```go\nfunc NewBufferReader(ir io.Reader) remote.ByteBuffer {\n\trw := rwPool.Get().(*bufferReadWriter)\n\trw.reader = netpoll.NewReader(ir)\n\trw.ioReader = ir\n\trw.status = remote.BitReadable\n\treturn rw\n}\n```", "```go\nfunc NewBufferReader(ir io.Reader) remote.ByteBuffer {\n\trw := rwPool.Get().(*bufferReadWriter)\n\trw.reader = netpoll.NewReader(ir)\n\trw.ioReader = ir\n\trw.status = remote.BitReadable\n\treturn rw\n}\n```"]}, "kitex-pkg/remote/trans/gonet/bytebuffer.go-WriteDirect": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/gonet/bytebuffer.go:\n```\n/*\n * Copyright 2022 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage gonet\n\nimport (\n\t\"errors\"\n\t\"io\"\n\t\"sync\"\n\n\t\"github.com/cloudwego/netpoll\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n)\n\nvar rwPool sync.Pool\n\nfunc init() {\n\trwPool.New = newBufferReadWriter\n}\n\nvar _ remote.ByteBuffer = &bufferReadWriter{}\n\ntype bufferReadWriter struct {\n\treader netpoll.Reader\n\twriter netpoll.Writer\n\n\tioReader io.Reader\n\tioWriter io.Writer\n\n\treadSize int\n\tstatus   int\n}\n\nfunc newBufferReadWriter() interface{} {\n\treturn &bufferReadWriter{}\n}\n\n// NewBufferReader creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReader.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewBufferWriter creates a new remote.ByteBuffer using the given netpoll.ZeroCopyWriter.\nfunc NewBufferWriter(iw io.Writer) remote.ByteBuffer {\n\trw := rwPool.Get().(*bufferReadWriter)\n\trw.writer = netpoll.NewWriter(iw)\n\trw.ioWriter = iw\n\trw.status = remote.BitWritable\n\treturn rw\n}\n\n// NewBufferReadWriter creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReadWriter.\nfunc NewBufferReadWriter(irw io.ReadWriter) remote.ByteBuffer {\n\trw := rwPool.Get().(*bufferReadWriter)\n\trw.writer = netpoll.NewWriter(irw)\n\trw.reader = netpoll.NewReader(irw)\n\trw.ioWriter = irw\n\trw.ioReader = irw\n\trw.status = remote.BitWritable | remote.BitReadable\n\treturn rw\n}\n\nfunc (rw *bufferReadWriter) readable() bool {\n\treturn rw.status&remote.BitReadable != 0\n}\n\nfunc (rw *bufferReadWriter) writable() bool {\n\treturn rw.status&remote.BitWritable != 0\n}\n\nfunc (rw *bufferReadWriter) Next(n int) (p []byte, err error) {\n\tif !rw.readable() {\n\t\treturn nil, errors.New(\"unreadable buffer, cannot support Next\")\n\t}\n\tif p, err = rw.reader.Next(n); err == nil {\n\t\trw.readSize += n\n\t}\n\treturn\n}\n\nfunc (rw *bufferReadWriter) Peek(n int) (buf []byte, err error) {\n\tif !rw.readable() {\n\t\treturn nil, errors.New(\"unreadable buffer, cannot support Peek\")\n\t}\n\treturn rw.reader.Peek(n)\n}\n\nfunc (rw *bufferReadWriter) Skip(n int) (err error) {\n\tif !rw.readable() {\n\t\treturn errors.New(\"unreadable buffer, cannot support Skip\")\n\t}\n\treturn rw.reader.Skip(n)\n}\n\nfunc (rw *bufferReadWriter) ReadableLen() (n int) {\n\tif !rw.readable() {\n\t\treturn -1\n\t}\n\treturn rw.reader.Len()\n}\n\nfunc (rw *bufferReadWriter) ReadString(n int) (s string, err error) {\n\tif !rw.readable() {\n\t\treturn \"\", errors.New(\"unreadable buffer, cannot support ReadString\")\n\t}\n\tif s, err = rw.reader.ReadString(n); err == nil {\n\t\trw.readSize += n\n\t}\n\treturn\n}\n\nfunc (rw *bufferReadWriter) ReadBinary(n int) (p []byte, err error) {\n\tif !rw.readable() {\n\t\treturn p, errors.New(\"unreadable buffer, cannot support ReadBinary\")\n\t}\n\tif p, err = rw.reader.ReadBinary(n); err == nil {\n\t\trw.readSize += n\n\t}\n\treturn\n}\n\nfunc (rw *bufferReadWriter) Read(p []byte) (n int, err error) {\n\tif !rw.readable() {\n\t\treturn -1, errors.New(\"unreadable buffer, cannot support Read\")\n\t}\n\tif rw.ioReader != nil {\n\t\treturn rw.ioReader.Read(p)\n\t}\n\treturn -1, errors.New(\"ioReader is nil\")\n}\n\nfunc (rw *bufferReadWriter) ReadLen() (n int) {\n\treturn rw.readSize\n}\n\nfunc (rw *bufferReadWriter) Malloc(n int) (buf []byte, err error) {\n\tif !rw.writable() {\n\t\treturn nil, errors.New(\"unwritable buffer, cannot support Malloc\")\n\t}\n\treturn rw.writer.Malloc(n)\n}\n\nfunc (rw *bufferReadWriter) MallocLen() (length int) {\n\tif !rw.writable() {\n\t\treturn -1\n\t}\n\treturn rw.writer.MallocLen()\n}\n\nfunc (rw *bufferReadWriter) WriteString(s string) (n int, err error) {\n\tif !rw.writable() {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support WriteString\")\n\t}\n\treturn rw.writer.WriteString(s)\n}\n\nfunc (rw *bufferReadWriter) WriteBinary(b []byte) (n int, err error) {\n\tif !rw.writable() {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support WriteBinary\")\n\t}\n\treturn rw.writer.WriteBinary(b)\n}\n\nfunc (rw *bufferReadWriter) Flush() (err error) {\n\tif !rw.writable() {\n\t\treturn errors.New(\"unwritable buffer, cannot support Flush\")\n\t}\n\treturn rw.writer.Flush()\n}\n\nfunc (rw *bufferReadWriter) Write(p []byte) (n int, err error) {\n\tif !rw.writable() {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support Write\")\n\t}\n\tif rw.ioWriter != nil {\n\t\treturn rw.ioWriter.Write(p)\n\t}\n\treturn -1, errors.New(\"ioWriter is nil\")\n}\n\nfunc (rw *bufferReadWriter) Release(e error) (err error) {\n\tif rw.reader != nil {\n\t\terr = rw.reader.Release()\n\t}\n\trw.zero()\n\trwPool.Put(rw)\n\treturn\n}\n\n// WriteDirect is a way to write []byte without copying, and splits the original buffer.\n\n\n\n\n\n\n\nfunc (rw *bufferReadWriter) AppendBuffer(buf remote.ByteBuffer) (err error) {\n\tsubBuf, ok := buf.(*bufferReadWriter)\n\tif !ok {\n\t\treturn errors.New(\"AppendBuffer failed, Buffer is not bufferReadWriter\")\n\t}\n\tif err = rw.writer.Append(subBuf.writer); err != nil {\n\t\treturn\n\t}\n\treturn buf.Release(nil)\n}\n\n// NewBuffer returns a new writable remote.ByteBuffer.\nfunc (rw *bufferReadWriter) NewBuffer() remote.ByteBuffer {\n\tpanic(\"unimplemented\")\n}\n\nfunc (rw *bufferReadWriter) Bytes() (buf []byte, err error) {\n\tpanic(\"unimplemented\")\n}\n\nfunc (rw *bufferReadWriter) zero() {\n\trw.reader = nil\n\trw.writer = nil\n\trw.ioReader = nil\n\trw.ioWriter = nil\n\trw.readSize = 0\n\trw.status = 0\n}\n\n```\n# Function Signature: func (rw *bufferReadWriter) WriteDirect(p []byte, remainCap int) error {\n# Natural Language Annotations: // WriteDirect is a way to write []byte without copying, and splits the original buffer.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (rw *bufferReadWriter) WriteDirect(p []byte, remainCap int) error {\n\tif !rw.writable() {\n\t\treturn errors.New(\"unwritable buffer, cannot support WriteDirect\")\n\t}\n\tif err := rw.writer.WriteDirect(p, remainCap); err != nil {\n\t\treturn err\n\t}\n\trw.readSize += len(p)\n\treturn nil\n}\n```\nThis function `WriteDirect` is a way to write `[]byte` without copying, and splits the original buffer. It takes `[]byte` and `remainCap` as parameters and writes the byte slice to the buffer directly. It returns an error if the buffer is not writable. After writing, it updates the read size.", "```go\nfunc (rw *bufferReadWriter) WriteDirect(p []byte, remainCap int) error {\n\tif !rw.writable() {\n\t\treturn errors.New(\"unwritable buffer, cannot support WriteDirect\")\n\t}\n\n\tif len(p) > rw.writer.Len() {\n\t\treturn errors.New(\"buffer is not large enough\")\n\t}\n\n\trw.writer.Grow(len(p))\n\tcopy(rw.writer.Bytes(), p)\n\trw.writer.Chop(len(p), remainCap)\n\n\treturn nil\n}\n```", "```go\nfunc (rw *bufferReadWriter) WriteDirect(p []byte, remainCap int) error {\n    if !rw.writable() {\n        return errors.New(\"unwritable buffer, cannot support WriteDirect\")\n    }\n    if len(p) > remainCap {\n        return errors.New(\"provided byte slice is larger than remaining buffer capacity\")\n    }\n    // The original buffer is split here\n    if err := rw.writer.Append(p[:remainCap]); err != nil {\n        return err\n    }\n    // The original buffer is split and the byte slice is written without copying\n    return nil\n}\n```\nThis function is used to write a byte slice into the buffer without copying, and the original buffer is split at the `remainCap` index. If the length of the byte slice `p` is larger than `remainCap`, an error is returned. If writing to the buffer fails, the function also returns an error."]}, "kitex-pkg/remote/trans/netpollmux/mux_transport.go-parseHeader": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/netpollmux/mux_transport.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage netpollmux\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\n\t\"github.com/cloudwego/netpoll\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/codec\"\n)\n\n// 0-4Byte length, 4-8Byte version check, 8-12Byte seqID\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func parseHeader(reader netpoll.Reader) (length int, seqID int32, err error) {\n# Natural Language Annotations: // 0-4Byte length, 4-8Byte version check, 8-12Byte seqID\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc parseHeader(reader netpoll.Reader) (length int, seqID int32, err error) {\n    header := make([]byte, 12)\n    _, err = reader.Read(header)\n    if err != nil {\n        return 0, 0, fmt.Errorf(\"failed to read header: %v\", err)\n    }\n\n    length = int(binary.BigEndian.Uint32(header[:4]))\n    version := binary.BigEndian.Uint32(header[4:8])\n    seqID = int32(binary.BigEndian.Uint32(header[8:12]))\n\n    if version != codec.MagicVersion2 {\n        return 0, 0, fmt.Errorf(\"invalid version: %d, expected: %d\", version, codec.MagicVersion2)\n    }\n\n    return length, seqID, nil\n}\n```", "```go\nfunc parseHeader(reader netpoll.Reader) (length int, seqID int32, err error) {\n\theader := make([]byte, 12)\n\tif _, err := reader.Read(header); err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\tlength = int(binary.BigEndian.Uint32(header[:4]))\n\tversion := binary.BigEndian.Uint32(header[4:8])\n\tseqID = int32(binary.BigEndian.Uint32(header[8:12]))\n\n\t// check version\n\tif version != codec.MagicVersion {\n\t\treturn 0, 0, fmt.Errorf(\"version mismatch, expected: %d, got: %d\", codec.MagicVersion, version)\n\t}\n\n\treturn length, seqID, nil\n}\n```", "```go\nfunc parseHeader(reader netpoll.Reader) (length int, seqID int32, err error) {\n\theader := make([]byte, 12)\n\tif _, err = reader.Read(header); err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\tlength = int(binary.BigEndian.Uint32(header[:4]))\n\tversion := binary.BigEndian.Uint32(header[4:8])\n\tif version != codec.VERSION {\n\t\treturn 0, 0, fmt.Errorf(\"invalid version: %v, should be: %v\", version, codec.VERSION)\n\t}\n\n\tseqID = int32(binary.BigEndian.Uint32(header[8:12]))\n\treturn length, seqID, nil\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/client_conn.go-fullMethodName": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/client_conn.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage nphttp2\n\nimport (\n\t\"context\"\n\t\"encoding/binary\"\n\t\"io\"\n\t\"net\"\n\t\"net/url\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/bytedance/gopkg/lang/dirtmake\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/codes\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n)\n\nconst (\n\tcontentSubTypeThrift   = \"thrift\"\n\tcontentSubTypeProtobuf = \"protobuf\"\n)\n\ntype streamDesc struct {\n\tisStreaming bool\n}\n\ntype clientConn struct {\n\ttr   grpc.ClientTransport\n\ts    *grpc.Stream\n\tdesc *streamDesc\n}\n\nvar _ GRPCConn = (*clientConn)(nil)\n\nfunc (c *clientConn) ReadFrame() (hdr, data []byte, err error) {\n\thdr = dirtmake.Bytes(5, 5)\n\t_, err = c.Read(hdr)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tdLen := int(binary.BigEndian.Uint32(hdr[1:]))\n\tdata = dirtmake.Bytes(dLen, dLen)\n\t_, err = c.Read(data)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn hdr, data, nil\n}\n\nfunc getContentSubType(codec serviceinfo.PayloadCodec) string {\n\tswitch codec {\n\tcase serviceinfo.Thrift:\n\t\treturn contentSubTypeThrift\n\tcase serviceinfo.Protobuf:\n\t\tfallthrough\n\tdefault:\n\t\treturn \"\" // default is protobuf, keep for backward compatibility\n\t}\n}\n\nfunc newClientConn(ctx context.Context, tr grpc.ClientTransport, addr string) (*clientConn, error) {\n\tri := rpcinfo.GetRPCInfo(ctx)\n\thost := ri.To().ServiceName()\n\tif rawURL, ok := ri.To().Tag(rpcinfo.HTTPURL); ok {\n\t\tu, err := url.Parse(rawURL)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\thost = u.Host\n\t}\n\tisStreaming := ri.Config().InteractionMode() == rpcinfo.Streaming\n\tinvocation := ri.Invocation()\n\tcallHdr := &grpc.CallHdr{\n\t\tHost: host,\n\t\t// grpc method format /package.Service/Method\n\t\tMethod:         fullMethodName(invocation.PackageName(), invocation.ServiceName(), invocation.MethodName()),\n\t\tSendCompress:   remote.GetSendCompressor(ri),\n\t\tContentSubtype: getContentSubType(ri.Config().PayloadCodec()),\n\t}\n\n\ts, err := tr.NewStream(ctx, callHdr)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &clientConn{\n\t\ttr:   tr,\n\t\ts:    s,\n\t\tdesc: &streamDesc{isStreaming: isStreaming},\n\t}, nil\n}\n\n// fullMethodName returns in the format of \"/[$pkg.]$svc/$methodName\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// impl net.Conn\nfunc (c *clientConn) Read(b []byte) (n int, err error) {\n\tn, err = c.s.Read(b)\n\tif err == io.EOF {\n\t\tif status := c.s.Status(); status.Code() != codes.OK {\n\t\t\tif bizStatusErr := c.s.BizStatusErr(); bizStatusErr != nil {\n\t\t\t\terr = bizStatusErr\n\t\t\t} else {\n\t\t\t\terr = status.Err()\n\t\t\t}\n\t\t}\n\t}\n\treturn n, err\n}\n\nfunc (c *clientConn) Write(b []byte) (n int, err error) {\n\tif len(b) < 5 {\n\t\treturn 0, io.ErrShortWrite\n\t}\n\treturn c.WriteFrame(b[:5], b[5:])\n}\n\nfunc (c *clientConn) WriteFrame(hdr, data []byte) (n int, err error) {\n\tgrpcConnOpt := &grpc.Options{Last: !c.desc.isStreaming}\n\terr = c.tr.Write(c.s, hdr, data, grpcConnOpt)\n\treturn len(hdr) + len(data), err\n}\n\nfunc (c *clientConn) LocalAddr() net.Addr                { return c.tr.LocalAddr() }\nfunc (c *clientConn) RemoteAddr() net.Addr               { return c.tr.RemoteAddr() }\nfunc (c *clientConn) SetDeadline(t time.Time) error      { return nil }\nfunc (c *clientConn) SetReadDeadline(t time.Time) error  { return nil }\nfunc (c *clientConn) SetWriteDeadline(t time.Time) error { return nil }\nfunc (c *clientConn) Close() error {\n\tc.tr.Write(c.s, nil, nil, &grpc.Options{Last: true})\n\t// Always return nil; io.EOF is the only error that might make sense\n\t// instead, but there is no need to signal the client to call Read\n\t// as the only use left for the stream after Close is to call\n\t// Read. This also matches historical behavior.\n\treturn nil\n}\n\nfunc (c *clientConn) Header() (metadata.MD, error) { return c.s.Header() }\nfunc (c *clientConn) Trailer() metadata.MD         { return c.s.Trailer() }\nfunc (c *clientConn) GetRecvCompress() string      { return c.s.RecvCompress() }\n\ntype hasGetRecvCompress interface {\n\tGetRecvCompress() string\n}\n\ntype hasHeader interface {\n\tHeader() (metadata.MD, error)\n}\n\ntype hasTrailer interface {\n\tTrailer() metadata.MD\n}\n\n```\n# Function Signature: func fullMethodName(pkg, svc, method string) string {\n# Natural Language Annotations: // fullMethodName returns in the format of \"/[$pkg.]$svc/$methodName\".\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc fullMethodName(pkg, svc, method string) string {\n\tif pkg == \"\" {\n\t\treturn \"/\" + svc + \"/\" + method\n\t} else {\n\t\treturn \"/\" + pkg + \".\" + svc + \"/\" + method\n\t}\n}\n```", "```go\nfunc fullMethodName(pkg, svc, method string) string {\n    fullMethod := \"/\"\n    if pkg != \"\" {\n        fullMethod += pkg + \".\"\n    }\n    fullMethod += svc + \"/\" + method\n    return fullMethod\n}\n```", "```go\nfunc fullMethodName(pkg, svc, method string) string {\n\t// fullMethodName returns in the format of \"/[$pkg.]$svc/$methodName\".\n\tif pkg != \"\" {\n\t\treturn \"/\" + pkg + \".\" + svc + \"/\" + method\n\t}\n\treturn \"/\" + svc + \"/\" + method\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/server_handler.go-invokeStreamUnaryHandler": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/server_handler.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage nphttp2\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"runtime/debug\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/netpoll\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/gofunc\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/codes\"\n\tgrpcTransport \"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/status\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\ntype svrTransHandlerFactory struct{}\n\n// NewSvrTransHandlerFactory ...\nfunc NewSvrTransHandlerFactory() remote.ServerTransHandlerFactory {\n\treturn &svrTransHandlerFactory{}\n}\n\nfunc (f *svrTransHandlerFactory) NewTransHandler(opt *remote.ServerOption) (remote.ServerTransHandler, error) {\n\treturn newSvrTransHandler(opt)\n}\n\nfunc newSvrTransHandler(opt *remote.ServerOption) (*svrTransHandler, error) {\n\treturn &svrTransHandler{\n\t\topt:          opt,\n\t\tsvcSearchMap: opt.SvcSearchMap,\n\t\tcodec:        grpc.NewGRPCCodec(grpc.WithThriftCodec(opt.PayloadCodec)),\n\t}, nil\n}\n\nvar _ remote.ServerTransHandler = &svrTransHandler{}\n\ntype svrTransHandler struct {\n\topt          *remote.ServerOption\n\tsvcSearchMap map[string]*serviceinfo.ServiceInfo\n\tinkHdlFunc   endpoint.Endpoint\n\tcodec        remote.Codec\n}\n\nvar prefaceReadAtMost = func() int {\n\t// min(len(ClientPreface), len(flagBuf))\n\t// len(flagBuf) = 2 * codec.Size32\n\tif 2*codec.Size32 < grpcTransport.ClientPrefaceLen {\n\t\treturn 2 * codec.Size32\n\t}\n\treturn grpcTransport.ClientPrefaceLen\n}()\n\nfunc (t *svrTransHandler) ProtocolMatch(ctx context.Context, conn net.Conn) (err error) {\n\t// Check the validity of client preface.\n\tnpReader := conn.(interface{ Reader() netpoll.Reader }).Reader()\n\t// read at most avoid block\n\tpreface, err := npReader.Peek(prefaceReadAtMost)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif bytes.Equal(preface[:prefaceReadAtMost], grpcTransport.ClientPreface[:prefaceReadAtMost]) {\n\t\treturn nil\n\t}\n\treturn errors.New(\"error protocol not match\")\n}\n\nfunc (t *svrTransHandler) Write(ctx context.Context, conn net.Conn, msg remote.Message) (nctx context.Context, err error) {\n\tbuf := newBuffer(conn.(*serverConn))\n\tdefer buf.Release(err)\n\n\tif err = t.codec.Encode(ctx, msg, buf); err != nil {\n\t\treturn ctx, err\n\t}\n\treturn ctx, buf.Flush()\n}\n\nfunc (t *svrTransHandler) Read(ctx context.Context, conn net.Conn, msg remote.Message) (nctx context.Context, err error) {\n\tbuf := newBuffer(conn.(*serverConn))\n\tdefer buf.Release(err)\n\n\terr = t.codec.Decode(ctx, msg, buf)\n\treturn ctx, err\n}\n\n// \u53ea return write err\nfunc (t *svrTransHandler) OnRead(ctx context.Context, conn net.Conn) error {\n\tsvrTrans := ctx.Value(ctxKeySvrTransport).(*SvrTrans)\n\ttr := svrTrans.tr\n\n\ttr.HandleStreams(func(s *grpcTransport.Stream) {\n\t\tgofunc.GoFunc(ctx, func() {\n\t\t\tri := svrTrans.pool.Get().(rpcinfo.RPCInfo)\n\t\t\trCtx := rpcinfo.NewCtxWithRPCInfo(s.Context(), ri)\n\t\t\tdefer func() {\n\t\t\t\t// reset rpcinfo for performance (PR #584)\n\t\t\t\tif rpcinfo.PoolEnabled() {\n\t\t\t\t\tri = t.opt.InitOrResetRPCInfoFunc(ri, conn.RemoteAddr())\n\t\t\t\t\tsvrTrans.pool.Put(ri)\n\t\t\t\t}\n\t\t\t}()\n\n\t\t\tink := ri.Invocation().(rpcinfo.InvocationSetter)\n\t\t\tsm := s.Method()\n\t\t\tif sm != \"\" && sm[0] == '/' {\n\t\t\t\tsm = sm[1:]\n\t\t\t}\n\t\t\tpos := strings.LastIndex(sm, \"/\")\n\t\t\tif pos == -1 {\n\t\t\t\terrDesc := fmt.Sprintf(\"malformed method name, method=%q\", s.Method())\n\t\t\t\ttr.WriteStatus(s, status.New(codes.Internal, errDesc))\n\t\t\t\treturn\n\t\t\t}\n\t\t\tmethodName := sm[pos+1:]\n\t\t\tink.SetMethodName(methodName)\n\n\t\t\tif mutableTo := rpcinfo.AsMutableEndpointInfo(ri.To()); mutableTo != nil {\n\t\t\t\tif err := mutableTo.SetMethod(methodName); err != nil {\n\t\t\t\t\terrDesc := fmt.Sprintf(\"setMethod failed in streaming, method=%s, error=%s\", methodName, err.Error())\n\t\t\t\t\t_ = tr.WriteStatus(s, status.New(codes.Internal, errDesc))\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tvar serviceName string\n\t\t\tidx := strings.LastIndex(sm[:pos], \".\")\n\t\t\tif idx == -1 {\n\t\t\t\tink.SetPackageName(\"\")\n\t\t\t\tserviceName = sm[0:pos]\n\t\t\t} else {\n\t\t\t\tink.SetPackageName(sm[:idx])\n\t\t\t\tserviceName = sm[idx+1 : pos]\n\t\t\t}\n\t\t\tink.SetServiceName(serviceName)\n\n\t\t\t// set grpc transport flag before execute metahandler\n\t\t\trpcinfo.AsMutableRPCConfig(ri.Config()).SetTransportProtocol(transport.GRPC)\n\t\t\tvar err error\n\t\t\tfor _, shdlr := range t.opt.StreamingMetaHandlers {\n\t\t\t\trCtx, err = shdlr.OnReadStream(rCtx)\n\t\t\t\tif err != nil {\n\t\t\t\t\ttr.WriteStatus(s, convertStatus(err))\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t\trCtx = t.startTracer(rCtx, ri)\n\t\t\tdefer func() {\n\t\t\t\tpanicErr := recover()\n\t\t\t\tif panicErr != nil {\n\t\t\t\t\tif conn != nil {\n\t\t\t\t\t\tklog.CtxErrorf(rCtx, \"KITEX: gRPC panic happened, close conn, remoteAddress=%s, error=%s\\nstack=%s\", conn.RemoteAddr(), panicErr, string(debug.Stack()))\n\t\t\t\t\t} else {\n\t\t\t\t\t\tklog.CtxErrorf(rCtx, \"KITEX: gRPC panic happened, error=%v\\nstack=%s\", panicErr, string(debug.Stack()))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tt.finishTracer(rCtx, ri, err, panicErr)\n\t\t\t}()\n\n\t\t\t// set recv grpc compressor at server to decode the pack from client\n\t\t\tremote.SetRecvCompressor(ri, s.RecvCompress())\n\t\t\t// set send grpc compressor at server to encode reply pack\n\t\t\tremote.SetSendCompressor(ri, s.SendCompress())\n\n\t\t\tsvcInfo := t.svcSearchMap[remote.BuildMultiServiceKey(serviceName, methodName)]\n\t\t\tvar methodInfo serviceinfo.MethodInfo\n\t\t\tif svcInfo != nil {\n\t\t\t\tmethodInfo = svcInfo.MethodInfo(methodName)\n\t\t\t}\n\n\t\t\trawStream := NewStream(rCtx, svcInfo, newServerConn(tr, s), t)\n\t\t\tst := newStreamWithMiddleware(rawStream, t.opt.RecvEndpoint, t.opt.SendEndpoint)\n\n\t\t\t// bind stream into ctx, in order to let user set header and trailer by provided api in meta_api.go\n\t\t\trCtx = streaming.NewCtxWithStream(rCtx, st)\n\n\t\t\tif methodInfo == nil {\n\t\t\t\tunknownServiceHandlerFunc := t.opt.GRPCUnknownServiceHandler\n\t\t\t\tif unknownServiceHandlerFunc != nil {\n\t\t\t\t\trpcinfo.Record(rCtx, ri, stats.ServerHandleStart, nil)\n\t\t\t\t\terr = unknownServiceHandlerFunc(rCtx, methodName, st)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\terr = kerrors.ErrBiz.WithCause(err)\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tif svcInfo == nil {\n\t\t\t\t\t\terr = remote.NewTransErrorWithMsg(remote.UnknownService, fmt.Sprintf(\"unknown service %s\", serviceName))\n\t\t\t\t\t} else {\n\t\t\t\t\t\terr = remote.NewTransErrorWithMsg(remote.UnknownMethod, fmt.Sprintf(\"unknown method %s\", methodName))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif streaming.UnaryCompatibleMiddleware(methodInfo.StreamingMode(), t.opt.CompatibleMiddlewareForUnary) {\n\t\t\t\t\t// making streaming unary APIs capable of using the same server middleware as non-streaming APIs\n\t\t\t\t\t// note: rawStream skips recv/send middleware for unary API requests to avoid confusion\n\t\t\t\t\terr = invokeStreamUnaryHandler(rCtx, rawStream, methodInfo, t.inkHdlFunc, ri)\n\t\t\t\t} else {\n\t\t\t\t\terr = t.inkHdlFunc(rCtx, &streaming.Args{Stream: st}, nil)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif err != nil {\n\t\t\t\ttr.WriteStatus(s, convertStatus(err))\n\t\t\t\tt.OnError(rCtx, err, conn)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif bizStatusErr := ri.Invocation().BizStatusErr(); bizStatusErr != nil {\n\t\t\t\tvar st *status.Status\n\t\t\t\tif sterr, ok := bizStatusErr.(status.Iface); ok {\n\t\t\t\t\tst = sterr.GRPCStatus()\n\t\t\t\t} else {\n\t\t\t\t\tst = status.New(codes.Internal, bizStatusErr.BizMessage())\n\t\t\t\t}\n\t\t\t\ts.SetBizStatusErr(bizStatusErr)\n\t\t\t\ttr.WriteStatus(s, st)\n\t\t\t\treturn\n\t\t\t}\n\t\t\ttr.WriteStatus(s, status.New(codes.OK, \"\"))\n\t\t})\n\t}, func(ctx context.Context, method string) context.Context {\n\t\treturn ctx\n\t})\n\treturn nil\n}\n\n// invokeStreamUnaryHandler allows unary APIs over HTTP2 to use the same server middleware as non-streaming APIs.\n// For thrift unary APIs over HTTP2, it's enabled by default.\n// For grpc(protobuf) unary APIs, it's disabled by default to keep backward compatibility.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// msg \u662f\u89e3\u7801\u540e\u7684\u5b9e\u4f8b\uff0c\u5982 Arg \u6216 Result, \u89e6\u53d1\u4e0a\u5c42\u5904\u7406\uff0c\u7528\u4e8e\u5f02\u6b65 \u548c \u670d\u52a1\u7aef\u5904\u7406\nfunc (t *svrTransHandler) OnMessage(ctx context.Context, args, result remote.Message) (context.Context, error) {\n\tpanic(\"unimplemented\")\n}\n\ntype svrTransKey int\n\nconst ctxKeySvrTransport svrTransKey = 1\n\ntype SvrTrans struct {\n\ttr   grpcTransport.ServerTransport\n\tpool *sync.Pool // value is rpcInfo\n}\n\n// \u65b0\u8fde\u63a5\u5efa\u7acb\u65f6\u89e6\u53d1\uff0c\u4e3b\u8981\u7528\u4e8e\u670d\u52a1\u7aef\uff0c\u5bf9\u5e94 netpoll onPrepare\nfunc (t *svrTransHandler) OnActive(ctx context.Context, conn net.Conn) (context.Context, error) {\n\t// set readTimeout to infinity to avoid streaming break\n\t// use keepalive to check the health of connection\n\tif npConn, ok := conn.(netpoll.Connection); ok {\n\t\tnpConn.SetReadTimeout(grpcTransport.Infinity)\n\t} else {\n\t\tconn.SetReadDeadline(time.Now().Add(grpcTransport.Infinity))\n\t}\n\n\ttr, err := grpcTransport.NewServerTransport(ctx, conn, t.opt.GRPCCfg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpool := &sync.Pool{\n\t\tNew: func() interface{} {\n\t\t\t// init rpcinfo\n\t\t\tri := t.opt.InitOrResetRPCInfoFunc(nil, conn.RemoteAddr())\n\t\t\treturn ri\n\t\t},\n\t}\n\tctx = context.WithValue(ctx, ctxKeySvrTransport, &SvrTrans{tr: tr, pool: pool})\n\treturn ctx, nil\n}\n\n// \u8fde\u63a5\u5173\u95ed\u65f6\u56de\u8c03\nfunc (t *svrTransHandler) OnInactive(ctx context.Context, conn net.Conn) {\n\ttr := ctx.Value(ctxKeySvrTransport).(*SvrTrans).tr\n\ttr.Close()\n}\n\n// \u4f20\u8f93\u5c42 error \u56de\u8c03\nfunc (t *svrTransHandler) OnError(ctx context.Context, err error, conn net.Conn) {\n\tvar de *kerrors.DetailedError\n\tif ok := errors.As(err, &de); ok && de.Stack() != \"\" {\n\t\tklog.CtxErrorf(ctx, \"KITEX: processing gRPC request error, remoteAddr=%s, error=%s\\nstack=%s\", conn.RemoteAddr(), err.Error(), de.Stack())\n\t} else {\n\t\tklog.CtxErrorf(ctx, \"KITEX: processing gRPC request error, remoteAddr=%s, error=%s\", conn.RemoteAddr(), err.Error())\n\t}\n}\n\nfunc (t *svrTransHandler) SetInvokeHandleFunc(inkHdlFunc endpoint.Endpoint) {\n\tt.inkHdlFunc = inkHdlFunc\n}\n\nfunc (t *svrTransHandler) SetPipeline(p *remote.TransPipeline) {\n}\n\nfunc (t *svrTransHandler) startTracer(ctx context.Context, ri rpcinfo.RPCInfo) context.Context {\n\tc := t.opt.TracerCtl.DoStart(ctx, ri)\n\treturn c\n}\n\nfunc (t *svrTransHandler) finishTracer(ctx context.Context, ri rpcinfo.RPCInfo, err error, panicErr interface{}) {\n\trpcStats := rpcinfo.AsMutableRPCStats(ri.Stats())\n\tif rpcStats == nil {\n\t\treturn\n\t}\n\tif panicErr != nil {\n\t\trpcStats.SetPanicked(panicErr)\n\t}\n\tt.opt.TracerCtl.DoFinish(ctx, ri, err)\n\trpcStats.Reset()\n}\n\n```\n# Function Signature: func invokeStreamUnaryHandler(ctx context.Context, st streaming.Stream, mi serviceinfo.MethodInfo,\n# Natural Language Annotations: // invokeStreamUnaryHandler allows unary APIs over HTTP2 to use the same server middleware as non-streaming APIs.\n// For thrift unary APIs over HTTP2, it's enabled by default.\n// For grpc(protobuf) unary APIs, it's disabled by default to keep backward compatibility.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc invokeStreamUnaryHandler(ctx context.Context, st streaming.Stream, mi serviceinfo.MethodInfo, endpoint endpoint.Endpoint) error {\n\treq := &streaming.Args{Stream: st}\n\trsp := &streaming.Args{Stream: st}\n\n\tif err := endpoint(ctx, req, rsp); err != nil {\n\t\treturn err\n\t}\n\n\targs, _ := mi.ArgsType().New().(streaming.Args)\n\targs = rsp\n\n\treturn streaming.NewStreamHandlerFunc(mi.Handler())(ctx, &args)\n}\n```\nThis function `invokeStreamUnaryHandler` is an implementation for handling unary APIs over HTTP2 using the same server middleware as non-streaming APIs. It creates a new `streaming.Args` instance, calls the endpoint with the request and response arguments, and then calls the handler with the new arguments.", "```go\nfunc invokeStreamUnaryHandler(ctx context.Context, st streaming.Stream, mi serviceinfo.MethodInfo, inkHdlFunc endpoint.Endpoint) error {\n\treq := new(streaming.Args)\n\treq.Stream = st\n\n\treply := new(streaming.Result)\n\treply.Stream = st\n\n\terr := inkHdlFunc(ctx, req, reply)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n```", "Here's the generated code for the `invokeStreamUnaryHandler` function:\n\n```go\nfunc invokeStreamUnaryHandler(ctx context.Context, st streaming.Stream, mi serviceinfo.MethodInfo, inkHdlFunc endpoint.Endpoint) error {\n\tinkCtx, err := rpcinfo.GetCtx(ctx)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treq, err := st.RecvMsg()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tres, err := inkHdlFunc(inkCtx, &streaming.Args{Stream: st, Msg: req})\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn st.SendMsg(res)\n}\n```\nThis function is used to handle unary APIs over HTTP2. It receives a gRPC stream, a method info, and an endpoint. It then receives a message from the stream, invokes the endpoint with the message and context, and sends the response back to the stream."]}, "kitex-pkg/remote/trans/nphttp2/metadata/metadata.go-FromOutgoingContext": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/metadata/metadata.go:\n```\n/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\n// Package metadata define the structure of the metadata supported by gRPC library.\n// Please refer to https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md\n// for more information about custom-metadata.\npackage metadata\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"strings\"\n)\n\n// DecodeKeyValue returns k, v, nil.\n//\n// Deprecated: use k and v directly instead.\nfunc DecodeKeyValue(k, v string) (string, string, error) {\n\treturn k, v, nil\n}\n\n// MD is a mapping from metadata keys to values. Users should use the following\n// two convenience functions New and Pairs to generate MD.\ntype MD map[string][]string\n\n// New creates an MD from a given key-value map.\n//\n// Only the following ASCII characters are allowed in keys:\n//   - digits: 0-9\n//   - uppercase letters: A-Z (normalized to lower)\n//   - lowercase letters: a-z\n//   - special characters: -_.\n//\n// Uppercase letters are automatically converted to lowercase.\n//\n// Keys beginning with \"grpc-\" are reserved for grpc-internal use only and may\n// result in errors if set in metadata.\nfunc New(m map[string]string) MD {\n\tmd := MD{}\n\tfor k, val := range m {\n\t\tkey := strings.ToLower(k)\n\t\tmd[key] = append(md[key], val)\n\t}\n\treturn md\n}\n\n// Pairs returns an MD formed by the mapping of key, value ...\n// Pairs panics if len(kv) is odd.\n//\n// Only the following ASCII characters are allowed in keys:\n//   - digits: 0-9\n//   - uppercase letters: A-Z (normalized to lower)\n//   - lowercase letters: a-z\n//   - special characters: -_.\n//\n// Uppercase letters are automatically converted to lowercase.\n//\n// Keys beginning with \"grpc-\" are reserved for grpc-internal use only and may\n// result in errors if set in metadata.\nfunc Pairs(kv ...string) MD {\n\tif len(kv)%2 == 1 {\n\t\tpanic(fmt.Sprintf(\"metadata: Pairs got the odd number of input pairs for metadata: %d\", len(kv)))\n\t}\n\tmd := MD{}\n\tvar key string\n\tfor i, s := range kv {\n\t\tif i%2 == 0 {\n\t\t\tkey = strings.ToLower(s)\n\t\t\tcontinue\n\t\t}\n\t\tmd[key] = append(md[key], s)\n\t}\n\treturn md\n}\n\n// Len returns the number of items in md.\nfunc (md MD) Len() int {\n\treturn len(md)\n}\n\n// Copy returns a copy of md.\nfunc (md MD) Copy() MD {\n\tresult := make(MD, len(md))\n\tfor k, v := range md {\n\t\tvalues := make([]string, len(v))\n\t\tcopy(values, v)\n\t\tresult[k] = values\n\t}\n\treturn result\n}\n\n// Get obtains the values for a given key.\nfunc (md MD) Get(k string) []string {\n\tk = strings.ToLower(k)\n\treturn md[k]\n}\n\n// Set sets the value of a given key with a slice of values.\nfunc (md MD) Set(k string, vals ...string) {\n\tif len(vals) == 0 {\n\t\treturn\n\t}\n\tk = strings.ToLower(k)\n\tmd[k] = vals\n}\n\n// Append adds the values to key k, not overwriting what was already stored at that key.\nfunc (md MD) Append(k string, vals ...string) {\n\tif len(vals) == 0 {\n\t\treturn\n\t}\n\tk = strings.ToLower(k)\n\tmd[k] = append(md[k], vals...)\n}\n\n// Join joins any number of mds into a single MD.\n// The order of values for each key is determined by the order in which\n// the mds containing those values are presented to Join.\nfunc Join(mds ...MD) MD {\n\tn := 0\n\tfor _, md := range mds {\n\t\tn += len(md)\n\t}\n\tout := make(MD, n)\n\tfor _, md := range mds {\n\t\tfor k, v := range md {\n\t\t\tout[k] = append(out[k], v...)\n\t\t}\n\t}\n\treturn out\n}\n\n// AppendMD appends other into md, merging values of the same key.\nfunc AppendMD(md, other MD) MD {\n\tif md == nil {\n\t\tmd = make(MD, len(other))\n\t}\n\tfor k, v := range other {\n\t\tmd[k] = append(md[k], v...)\n\t}\n\treturn md\n}\n\ntype (\n\tmdIncomingKey struct{}\n\tmdOutgoingKey struct{}\n)\n\n// NewIncomingContext creates a new context with incoming md attached.\nfunc NewIncomingContext(ctx context.Context, md MD) context.Context {\n\treturn context.WithValue(ctx, mdIncomingKey{}, md)\n}\n\n// NewOutgoingContext creates a new context with outgoing md attached. If used\n// in conjunction with AppendToOutgoingContext, NewOutgoingContext will\n// overwrite any previously-appended metadata.\nfunc NewOutgoingContext(ctx context.Context, md MD) context.Context {\n\treturn context.WithValue(ctx, mdOutgoingKey{}, rawMD{md: md})\n}\n\n// AppendToOutgoingContext returns a new context with the provided kv merged\n// with any existing metadata in the context. Please refer to the\n// documentation of Pairs for a description of kv.\nfunc AppendToOutgoingContext(ctx context.Context, kv ...string) context.Context {\n\tif len(kv)%2 == 1 {\n\t\tpanic(fmt.Sprintf(\"metadata: AppendToOutgoingContext got an odd number of input pairs for metadata: %d\", len(kv)))\n\t}\n\tmd, _ := ctx.Value(mdOutgoingKey{}).(rawMD)\n\tadded := make([][]string, len(md.added)+1)\n\tcopy(added, md.added)\n\tadded[len(added)-1] = make([]string, len(kv))\n\tcopy(added[len(added)-1], kv)\n\treturn context.WithValue(ctx, mdOutgoingKey{}, rawMD{md: md.md, added: added})\n}\n\n// FromIncomingContext returns the incoming metadata in ctx if it exists.  The\n// returned MD should not be modified. Writing to it may cause races.\n// Modification should be made to copies of the returned MD.\nfunc FromIncomingContext(ctx context.Context) (md MD, ok bool) {\n\tmd, ok = ctx.Value(mdIncomingKey{}).(MD)\n\treturn\n}\n\n// FromOutgoingContextRaw returns the un-merged, intermediary contents\n// of rawMD. Remember to perform strings.ToLower on the keys. The returned\n// MD should not be modified. Writing to it may cause races. Modification\n// should be made to copies of the returned MD.\n//\n// This is intended for gRPC-internal use ONLY.\nfunc FromOutgoingContextRaw(ctx context.Context) (MD, [][]string, bool) {\n\traw, ok := ctx.Value(mdOutgoingKey{}).(rawMD)\n\tif !ok {\n\t\treturn nil, nil, false\n\t}\n\n\treturn raw.md, raw.added, true\n}\n\n// FromOutgoingContext returns the outgoing metadata in ctx if it exists.  The\n// returned MD should not be modified. Writing to it may cause races.\n// Modification should be made to copies of the returned MD.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype rawMD struct {\n\tmd    MD\n\tadded [][]string\n}\n\n```\n# Function Signature: func FromOutgoingContext(ctx context.Context) (MD, bool) {\n# Natural Language Annotations: // FromOutgoingContext returns the outgoing metadata in ctx if it exists.  The\n// returned MD should not be modified. Writing to it may cause races.\n// Modification should be made to copies of the returned MD.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc FromOutgoingContext(ctx context.Context) (MD, bool) {\n\traw, ok := ctx.Value(mdOutgoingKey{}).(rawMD)\n\tif !ok {\n\t\treturn nil, false\n\t}\n\n\tmd := make(MD, len(raw.md))\n\tfor k, v := range raw.md {\n\t\tmd[k] = append([]string(nil), v...)\n\t}\n\tfor _, added := range raw.added {\n\t\tfor i := 0; i < len(added); i += 2 {\n\t\t\tk, v := strings.ToLower(added[i]), added[i+1]\n\t\t\tmd[k] = append(md[k], v)\n\t\t}\n\t}\n\n\treturn md, true\n}\n```", "```go\nfunc FromOutgoingContext(ctx context.Context) (MD, bool) {\n\traw, ok := ctx.Value(mdOutgoingKey{}).(rawMD)\n\tif !ok {\n\t\treturn nil, false\n\t}\n\n\tvar md MD\n\tfor k, v := range raw.md {\n\t\tmd = AppendMD(md, MD{k: v})\n\t}\n\tfor _, kv := range raw.added {\n\t\tfor i := 0; i < len(kv); i += 2 {\n\t\t\tmd = AppendMD(md, MD{kv[i]: []string{kv[i+1]}})\n\t\t}\n\t}\n\treturn md, true\n}\n```", "```go\nfunc FromOutgoingContext(ctx context.Context) (MD, bool) {\n\traw, ok := ctx.Value(mdOutgoingKey{}).(rawMD)\n\tif !ok {\n\t\treturn nil, false\n\t}\n\n\tif raw.md == nil {\n\t\traw.md = MD{}\n\t}\n\n\tif len(raw.added) > 0 {\n\t\tfor _, kv := range raw.added {\n\t\t\tif len(kv) < 2 {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tk := strings.ToLower(kv[0])\n\t\t\traw.md[k] = append(raw.md[k], kv[1:]...)\n\t\t}\n\t}\n\n\treturn raw.md, true\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/status/status.go-WithDetails": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/status/status.go:\n```\n/*\n *\n * Copyright 2020 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\n// Package status implements errors returned by gRPC. These errors are\n// serialized and transmitted on the wire between server and client, and allow\n// for additional data to be transmitted via the Details field in the status\n// proto. gRPC service handlers should return an error created by this\n// package, and gRPC clients should expect a corresponding error to be\n// returned from the RPC call.\n//\n// This package upholds the invariants that a non-nil error may not\n// contain an OK code, and an OK code must result in a nil error.\npackage status\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\tspb \"google.golang.org/genproto/googleapis/rpc/status\"\n\t\"google.golang.org/protobuf/proto\"\n\t\"google.golang.org/protobuf/types/known/anypb\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/codes\"\n)\n\ntype Iface interface {\n\tGRPCStatus() *Status\n}\n\n// Status represents an RPC status code, message, and details.  It is immutable\n// and should be created with New, Newf, or FromProto.\ntype Status struct {\n\ts *spb.Status\n}\n\n// New returns a Status representing c and msg.\nfunc New(c codes.Code, msg string) *Status {\n\treturn &Status{s: &spb.Status{Code: int32(c), Message: msg}}\n}\n\n// Newf returns New(c, fmt.Sprintf(format, a...)).\nfunc Newf(c codes.Code, format string, a ...interface{}) *Status {\n\treturn New(c, fmt.Sprintf(format, a...))\n}\n\n// ErrorProto returns an error representing s.  If s.Code is OK, returns nil.\nfunc ErrorProto(s *spb.Status) error {\n\treturn FromProto(s).Err()\n}\n\n// FromProto returns a Status representing s.\nfunc FromProto(s *spb.Status) *Status {\n\treturn &Status{s: proto.Clone(s).(*spb.Status)}\n}\n\n// Err returns an error representing c and msg.  If c is OK, returns nil.\nfunc Err(c codes.Code, msg string) error {\n\treturn New(c, msg).Err()\n}\n\n// Errorf returns Error(c, fmt.Sprintf(format, a...)).\nfunc Errorf(c codes.Code, format string, a ...interface{}) error {\n\treturn Err(c, fmt.Sprintf(format, a...))\n}\n\n// Code returns the status code contained in s.\nfunc (s *Status) Code() codes.Code {\n\tif s == nil || s.s == nil {\n\t\treturn codes.OK\n\t}\n\treturn codes.Code(s.s.Code)\n}\n\n// Message returns the message contained in s.\nfunc (s *Status) Message() string {\n\tif s == nil || s.s == nil {\n\t\treturn \"\"\n\t}\n\treturn s.s.Message\n}\n\n// AppendMessage append extra msg for Status\nfunc (s *Status) AppendMessage(extraMsg string) *Status {\n\tif s == nil || s.s == nil || extraMsg == \"\" {\n\t\treturn s\n\t}\n\ts.s.Message = fmt.Sprintf(\"%s %s\", s.s.Message, extraMsg)\n\treturn s\n}\n\n// Proto returns s's status as an spb.Status proto message.\nfunc (s *Status) Proto() *spb.Status {\n\tif s == nil {\n\t\treturn nil\n\t}\n\treturn proto.Clone(s.s).(*spb.Status)\n}\n\n// Err returns an immutable error representing s; returns nil if s.Code() is OK.\nfunc (s *Status) Err() error {\n\tif s.Code() == codes.OK {\n\t\treturn nil\n\t}\n\treturn &Error{e: s.Proto()}\n}\n\n// WithDetails returns a new status with the provided details messages appended to the status.\n// If any errors are encountered, it returns nil and the first error encountered.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Details returns a slice of details messages attached to the status.\n// If a detail cannot be decoded, the error is returned in place of the detail.\nfunc (s *Status) Details() []interface{} {\n\tif s == nil || s.s == nil {\n\t\treturn nil\n\t}\n\tdetails := make([]interface{}, 0, len(s.s.Details))\n\tfor _, any := range s.s.Details {\n\t\tdetail, err := any.UnmarshalNew()\n\t\tif err != nil {\n\t\t\tdetails = append(details, err)\n\t\t\tcontinue\n\t\t}\n\t\tdetails = append(details, detail)\n\t}\n\treturn details\n}\n\n// Error wraps a pointer of a status proto. It implements error and Status,\n// and a nil *Error should never be returned by this package.\ntype Error struct {\n\te *spb.Status\n}\n\nfunc (e *Error) Error() string {\n\treturn fmt.Sprintf(\"rpc error: code = %d desc = %s\", codes.Code(e.e.GetCode()), e.e.GetMessage())\n}\n\n// GRPCStatus returns the Status represented by se.\nfunc (e *Error) GRPCStatus() *Status {\n\treturn FromProto(e.e)\n}\n\n// Is implements future error.Is functionality.\n// A Error is equivalent if the code and message are identical.\nfunc (e *Error) Is(target error) bool {\n\ttse, ok := target.(*Error)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn proto.Equal(e.e, tse.e)\n}\n\n// FromError returns a Status representing err if it was produced from this\n// package or has a method `GRPCStatus() *Status`. Otherwise, ok is false and a\n// Status is returned with codes.Unknown and the original error message.\n\n\n\n\n\n\n\n\n\n\n\n// Convert is a convenience function which removes the need to handle the\n// boolean return value from FromError.\nfunc Convert(err error) *Status {\n\ts, _ := FromError(err)\n\treturn s\n}\n\n// Code returns the Code of the error if it is a Status error, codes.OK if err\n// is nil, or codes.Unknown otherwise.\nfunc Code(err error) codes.Code {\n\t// Don't use FromError to avoid allocation of OK status.\n\tif err == nil {\n\t\treturn codes.OK\n\t}\n\tvar se Iface\n\tif errors.As(err, &se) {\n\t\treturn se.GRPCStatus().Code()\n\t}\n\treturn codes.Unknown\n}\n\n// FromContextError converts a context error into a Status.  It returns a\n// Status with codes.OK if err is nil, or a Status with codes.Unknown if err is\n// non-nil and not a context error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (s *Status) WithDetails(details ...proto.Message) (*Status, error) {\n# Natural Language Annotations: // WithDetails returns a new status with the provided details messages appended to the status.\n// If any errors are encountered, it returns nil and the first error encountered.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (s *Status) WithDetails(details ...proto.Message) (*Status, error) {\n\tif s == nil || s.s == nil {\n\t\treturn nil, nil\n\t}\n\tfor _, detail := range details {\n\t\tany, err := anypb.New(detail)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\ts.s.Details = append(s.s.Details, any)\n\t}\n\treturn s, nil\n}\n```", "```go\nfunc (s *Status) WithDetails(details ...proto.Message) (*Status, error) {\n\tif s == nil || s.s == nil {\n\t\treturn nil, nil\n\t}\n\n\tfor _, detail := range details {\n\t\tany, err := anypb.New(detail)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to pack detail to any: %v\", err)\n\t\t}\n\t\ts.s.Details = append(s.s.Details, any)\n\t}\n\n\treturn s, nil\n}\n```", "```go\nfunc (s *Status) WithDetails(details ...proto.Message) (*Status, error) {\n\tif s == nil {\n\t\treturn nil, Err(codes.InvalidArgument, \"status is nil\")\n\t}\n\tnewStatus := FromProto(s.Proto())\n\tfor _, detail := range details {\n\t\tany, err := anypb.New(detail)\n\t\tif err != nil {\n\t\t\treturn nil, Err(codes.InvalidArgument, fmt.Sprintf(\"failed to pack detail message to any: %v\", err))\n\t\t}\n\t\tnewStatus.s.Details = append(newStatus.s.Details, any)\n\t}\n\treturn newStatus, nil\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/status/status.go-FromError": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/status/status.go:\n```\n/*\n *\n * Copyright 2020 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\n// Package status implements errors returned by gRPC. These errors are\n// serialized and transmitted on the wire between server and client, and allow\n// for additional data to be transmitted via the Details field in the status\n// proto. gRPC service handlers should return an error created by this\n// package, and gRPC clients should expect a corresponding error to be\n// returned from the RPC call.\n//\n// This package upholds the invariants that a non-nil error may not\n// contain an OK code, and an OK code must result in a nil error.\npackage status\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\tspb \"google.golang.org/genproto/googleapis/rpc/status\"\n\t\"google.golang.org/protobuf/proto\"\n\t\"google.golang.org/protobuf/types/known/anypb\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/codes\"\n)\n\ntype Iface interface {\n\tGRPCStatus() *Status\n}\n\n// Status represents an RPC status code, message, and details.  It is immutable\n// and should be created with New, Newf, or FromProto.\ntype Status struct {\n\ts *spb.Status\n}\n\n// New returns a Status representing c and msg.\nfunc New(c codes.Code, msg string) *Status {\n\treturn &Status{s: &spb.Status{Code: int32(c), Message: msg}}\n}\n\n// Newf returns New(c, fmt.Sprintf(format, a...)).\nfunc Newf(c codes.Code, format string, a ...interface{}) *Status {\n\treturn New(c, fmt.Sprintf(format, a...))\n}\n\n// ErrorProto returns an error representing s.  If s.Code is OK, returns nil.\nfunc ErrorProto(s *spb.Status) error {\n\treturn FromProto(s).Err()\n}\n\n// FromProto returns a Status representing s.\nfunc FromProto(s *spb.Status) *Status {\n\treturn &Status{s: proto.Clone(s).(*spb.Status)}\n}\n\n// Err returns an error representing c and msg.  If c is OK, returns nil.\nfunc Err(c codes.Code, msg string) error {\n\treturn New(c, msg).Err()\n}\n\n// Errorf returns Error(c, fmt.Sprintf(format, a...)).\nfunc Errorf(c codes.Code, format string, a ...interface{}) error {\n\treturn Err(c, fmt.Sprintf(format, a...))\n}\n\n// Code returns the status code contained in s.\nfunc (s *Status) Code() codes.Code {\n\tif s == nil || s.s == nil {\n\t\treturn codes.OK\n\t}\n\treturn codes.Code(s.s.Code)\n}\n\n// Message returns the message contained in s.\nfunc (s *Status) Message() string {\n\tif s == nil || s.s == nil {\n\t\treturn \"\"\n\t}\n\treturn s.s.Message\n}\n\n// AppendMessage append extra msg for Status\nfunc (s *Status) AppendMessage(extraMsg string) *Status {\n\tif s == nil || s.s == nil || extraMsg == \"\" {\n\t\treturn s\n\t}\n\ts.s.Message = fmt.Sprintf(\"%s %s\", s.s.Message, extraMsg)\n\treturn s\n}\n\n// Proto returns s's status as an spb.Status proto message.\nfunc (s *Status) Proto() *spb.Status {\n\tif s == nil {\n\t\treturn nil\n\t}\n\treturn proto.Clone(s.s).(*spb.Status)\n}\n\n// Err returns an immutable error representing s; returns nil if s.Code() is OK.\nfunc (s *Status) Err() error {\n\tif s.Code() == codes.OK {\n\t\treturn nil\n\t}\n\treturn &Error{e: s.Proto()}\n}\n\n// WithDetails returns a new status with the provided details messages appended to the status.\n// If any errors are encountered, it returns nil and the first error encountered.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Details returns a slice of details messages attached to the status.\n// If a detail cannot be decoded, the error is returned in place of the detail.\nfunc (s *Status) Details() []interface{} {\n\tif s == nil || s.s == nil {\n\t\treturn nil\n\t}\n\tdetails := make([]interface{}, 0, len(s.s.Details))\n\tfor _, any := range s.s.Details {\n\t\tdetail, err := any.UnmarshalNew()\n\t\tif err != nil {\n\t\t\tdetails = append(details, err)\n\t\t\tcontinue\n\t\t}\n\t\tdetails = append(details, detail)\n\t}\n\treturn details\n}\n\n// Error wraps a pointer of a status proto. It implements error and Status,\n// and a nil *Error should never be returned by this package.\ntype Error struct {\n\te *spb.Status\n}\n\nfunc (e *Error) Error() string {\n\treturn fmt.Sprintf(\"rpc error: code = %d desc = %s\", codes.Code(e.e.GetCode()), e.e.GetMessage())\n}\n\n// GRPCStatus returns the Status represented by se.\nfunc (e *Error) GRPCStatus() *Status {\n\treturn FromProto(e.e)\n}\n\n// Is implements future error.Is functionality.\n// A Error is equivalent if the code and message are identical.\nfunc (e *Error) Is(target error) bool {\n\ttse, ok := target.(*Error)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn proto.Equal(e.e, tse.e)\n}\n\n// FromError returns a Status representing err if it was produced from this\n// package or has a method `GRPCStatus() *Status`. Otherwise, ok is false and a\n// Status is returned with codes.Unknown and the original error message.\n\n\n\n\n\n\n\n\n\n\n\n// Convert is a convenience function which removes the need to handle the\n// boolean return value from FromError.\nfunc Convert(err error) *Status {\n\ts, _ := FromError(err)\n\treturn s\n}\n\n// Code returns the Code of the error if it is a Status error, codes.OK if err\n// is nil, or codes.Unknown otherwise.\nfunc Code(err error) codes.Code {\n\t// Don't use FromError to avoid allocation of OK status.\n\tif err == nil {\n\t\treturn codes.OK\n\t}\n\tvar se Iface\n\tif errors.As(err, &se) {\n\t\treturn se.GRPCStatus().Code()\n\t}\n\treturn codes.Unknown\n}\n\n// FromContextError converts a context error into a Status.  It returns a\n// Status with codes.OK if err is nil, or a Status with codes.Unknown if err is\n// non-nil and not a context error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func FromError(err error) (s *Status, ok bool) {\n# Natural Language Annotations: // FromError returns a Status representing err if it was produced from this\n// package or has a method `GRPCStatus() *Status`. Otherwise, ok is false and a\n// Status is returned with codes.Unknown and the original error message.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc FromError(err error) (s *Status, ok bool) {\n\tvar se Iface\n\tif errors.As(err, &se) {\n\t\treturn se.GRPCStatus(), true\n\t}\n\treturn New(codes.Unknown, err.Error()), false\n}\n```", "```go\nfunc FromError(err error) (s *Status, ok bool) {\n\tvar se Iface\n\tif errors.As(err, &se) {\n\t\treturn se.GRPCStatus(), true\n\t}\n\treturn New(codes.Unknown, err.Error()), false\n}\n```", "```go\nfunc FromError(err error) (s *Status, ok bool) {\n\tif err == nil {\n\t\treturn nil, true\n\t}\n\tvar se Iface\n\tif errors.As(err, &se) {\n\t\treturn se.GRPCStatus(), true\n\t}\n\treturn New(codes.Unknown, err.Error()), false\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/status/status.go-FromContextError": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/status/status.go:\n```\n/*\n *\n * Copyright 2020 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\n// Package status implements errors returned by gRPC. These errors are\n// serialized and transmitted on the wire between server and client, and allow\n// for additional data to be transmitted via the Details field in the status\n// proto. gRPC service handlers should return an error created by this\n// package, and gRPC clients should expect a corresponding error to be\n// returned from the RPC call.\n//\n// This package upholds the invariants that a non-nil error may not\n// contain an OK code, and an OK code must result in a nil error.\npackage status\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\n\tspb \"google.golang.org/genproto/googleapis/rpc/status\"\n\t\"google.golang.org/protobuf/proto\"\n\t\"google.golang.org/protobuf/types/known/anypb\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/codes\"\n)\n\ntype Iface interface {\n\tGRPCStatus() *Status\n}\n\n// Status represents an RPC status code, message, and details.  It is immutable\n// and should be created with New, Newf, or FromProto.\ntype Status struct {\n\ts *spb.Status\n}\n\n// New returns a Status representing c and msg.\nfunc New(c codes.Code, msg string) *Status {\n\treturn &Status{s: &spb.Status{Code: int32(c), Message: msg}}\n}\n\n// Newf returns New(c, fmt.Sprintf(format, a...)).\nfunc Newf(c codes.Code, format string, a ...interface{}) *Status {\n\treturn New(c, fmt.Sprintf(format, a...))\n}\n\n// ErrorProto returns an error representing s.  If s.Code is OK, returns nil.\nfunc ErrorProto(s *spb.Status) error {\n\treturn FromProto(s).Err()\n}\n\n// FromProto returns a Status representing s.\nfunc FromProto(s *spb.Status) *Status {\n\treturn &Status{s: proto.Clone(s).(*spb.Status)}\n}\n\n// Err returns an error representing c and msg.  If c is OK, returns nil.\nfunc Err(c codes.Code, msg string) error {\n\treturn New(c, msg).Err()\n}\n\n// Errorf returns Error(c, fmt.Sprintf(format, a...)).\nfunc Errorf(c codes.Code, format string, a ...interface{}) error {\n\treturn Err(c, fmt.Sprintf(format, a...))\n}\n\n// Code returns the status code contained in s.\nfunc (s *Status) Code() codes.Code {\n\tif s == nil || s.s == nil {\n\t\treturn codes.OK\n\t}\n\treturn codes.Code(s.s.Code)\n}\n\n// Message returns the message contained in s.\nfunc (s *Status) Message() string {\n\tif s == nil || s.s == nil {\n\t\treturn \"\"\n\t}\n\treturn s.s.Message\n}\n\n// AppendMessage append extra msg for Status\nfunc (s *Status) AppendMessage(extraMsg string) *Status {\n\tif s == nil || s.s == nil || extraMsg == \"\" {\n\t\treturn s\n\t}\n\ts.s.Message = fmt.Sprintf(\"%s %s\", s.s.Message, extraMsg)\n\treturn s\n}\n\n// Proto returns s's status as an spb.Status proto message.\nfunc (s *Status) Proto() *spb.Status {\n\tif s == nil {\n\t\treturn nil\n\t}\n\treturn proto.Clone(s.s).(*spb.Status)\n}\n\n// Err returns an immutable error representing s; returns nil if s.Code() is OK.\nfunc (s *Status) Err() error {\n\tif s.Code() == codes.OK {\n\t\treturn nil\n\t}\n\treturn &Error{e: s.Proto()}\n}\n\n// WithDetails returns a new status with the provided details messages appended to the status.\n// If any errors are encountered, it returns nil and the first error encountered.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Details returns a slice of details messages attached to the status.\n// If a detail cannot be decoded, the error is returned in place of the detail.\nfunc (s *Status) Details() []interface{} {\n\tif s == nil || s.s == nil {\n\t\treturn nil\n\t}\n\tdetails := make([]interface{}, 0, len(s.s.Details))\n\tfor _, any := range s.s.Details {\n\t\tdetail, err := any.UnmarshalNew()\n\t\tif err != nil {\n\t\t\tdetails = append(details, err)\n\t\t\tcontinue\n\t\t}\n\t\tdetails = append(details, detail)\n\t}\n\treturn details\n}\n\n// Error wraps a pointer of a status proto. It implements error and Status,\n// and a nil *Error should never be returned by this package.\ntype Error struct {\n\te *spb.Status\n}\n\nfunc (e *Error) Error() string {\n\treturn fmt.Sprintf(\"rpc error: code = %d desc = %s\", codes.Code(e.e.GetCode()), e.e.GetMessage())\n}\n\n// GRPCStatus returns the Status represented by se.\nfunc (e *Error) GRPCStatus() *Status {\n\treturn FromProto(e.e)\n}\n\n// Is implements future error.Is functionality.\n// A Error is equivalent if the code and message are identical.\nfunc (e *Error) Is(target error) bool {\n\ttse, ok := target.(*Error)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn proto.Equal(e.e, tse.e)\n}\n\n// FromError returns a Status representing err if it was produced from this\n// package or has a method `GRPCStatus() *Status`. Otherwise, ok is false and a\n// Status is returned with codes.Unknown and the original error message.\n\n\n\n\n\n\n\n\n\n\n\n// Convert is a convenience function which removes the need to handle the\n// boolean return value from FromError.\nfunc Convert(err error) *Status {\n\ts, _ := FromError(err)\n\treturn s\n}\n\n// Code returns the Code of the error if it is a Status error, codes.OK if err\n// is nil, or codes.Unknown otherwise.\nfunc Code(err error) codes.Code {\n\t// Don't use FromError to avoid allocation of OK status.\n\tif err == nil {\n\t\treturn codes.OK\n\t}\n\tvar se Iface\n\tif errors.As(err, &se) {\n\t\treturn se.GRPCStatus().Code()\n\t}\n\treturn codes.Unknown\n}\n\n// FromContextError converts a context error into a Status.  It returns a\n// Status with codes.OK if err is nil, or a Status with codes.Unknown if err is\n// non-nil and not a context error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func FromContextError(err error) *Status {\n# Natural Language Annotations: // FromContextError converts a context error into a Status.  It returns a\n// Status with codes.OK if err is nil, or a Status with codes.Unknown if err is\n// non-nil and not a context error.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc FromContextError(err error) *Status {\n\tif err == nil {\n\t\treturn New(codes.OK, \"\")\n\t}\n\tvar se Iface\n\tif errors.As(err, &se) {\n\t\treturn se.GRPCStatus()\n\t}\n\treturn New(codes.Unknown, err.Error())\n}\n```", "```go\nfunc FromContextError(err error) *Status {\n\tif err == nil {\n\t\treturn New(codes.OK, \"\")\n\t}\n\n\tif errors.Is(err, context.DeadlineExceeded) {\n\t\treturn New(codes.DeadlineExceeded, err.Error())\n\t}\n\n\tif errors.Is(err, context.Canceled) {\n\t\treturn New(codes.Canceled, err.Error())\n\t}\n\n\treturn New(codes.Unknown, err.Error())\n}\n```\nThe code above checks if the error is a context error (context deadline exceeded or context canceled). If it is, it creates a new status with the appropriate code. If not, it creates a status with codes.Unknown.", "```go\nfunc FromContextError(err error) *Status {\n\tif err == nil {\n\t\treturn New(codes.OK, \"\")\n\t}\n\tif errors.Is(err, context.Canceled) {\n\t\treturn New(codes.Canceled, err.Error())\n\t}\n\tif errors.Is(err, context.DeadlineExceeded) {\n\t\treturn New(codes.DeadlineExceeded, err.Error())\n\t}\n\treturn New(codes.Unknown, err.Error())\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/grpc/bdp_estimator.go-calculate": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/grpc/bdp_estimator.go:\n```\n/*\n *\n * Copyright 2017 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage grpc\n\nimport (\n\t\"sync\"\n\t\"time\"\n)\n\nconst (\n\t// bdpLimit is the maximum value the flow control windows will be increased\n\t// to.  TCP typically limits this to 4MB, but some systems go up to 16MB.\n\t// Since this is only a limit, it is safe to make it optimistic.\n\tbdpLimit = (1 << 20) * 16\n\t// alpha is a constant factor used to keep a moving average\n\t// of RTTs.\n\talpha = 0.9\n\t// If the current bdp sample is greater than or equal to\n\t// our beta * our estimated bdp and the current bandwidth\n\t// sample is the maximum bandwidth observed so far, we\n\t// increase our bbp estimate by a factor of gamma.\n\tbeta = 0.66\n\t// To put our bdp to be smaller than or equal to twice the real BDP,\n\t// we should multiply our current sample with 4/3, however to round things out\n\t// we use 2 as the multiplication factor.\n\tgamma = 2\n)\n\n// Adding arbitrary data to ping so that its ack can be identified.\n// Easter-egg: what does the ping message say?\nvar bdpPing = &ping{data: [8]byte{2, 4, 16, 16, 9, 14, 7, 7}}\n\ntype bdpEstimator struct {\n\t// sentAt is the time when the ping was sent.\n\tsentAt time.Time\n\n\tmu sync.Mutex\n\t// bdp is the current bdp estimate.\n\tbdp uint32\n\t// sample is the number of bytes received in one measurement cycle.\n\tsample uint32\n\t// bwMax is the maximum bandwidth noted so far (bytes/sec).\n\tbwMax float64\n\t// bool to keep track of the beginning of a new measurement cycle.\n\tisSent bool\n\t// Callback to update the window sizes.\n\tupdateFlowControl func(n uint32)\n\t// sampleCount is the number of samples taken so far.\n\tsampleCount uint64\n\t// round trip time (seconds)\n\trtt float64\n}\n\n// timesnap registers the time bdp ping was sent out so that\n// network rtt can be calculated when its ack is received.\n// It is called (by controller) when the bdpPing is\n// being written on the wire.\nfunc (b *bdpEstimator) timesnap(d [8]byte) {\n\tif bdpPing.data != d {\n\t\treturn\n\t}\n\t// Locking here is to avoid DATA RACE in the unittest.\n\t// In fact, it would not bring the concurrency problem.\n\tb.mu.Lock()\n\tb.sentAt = time.Now()\n\tb.mu.Unlock()\n}\n\n// add adds bytes to the current sample for calculating bdp.\n// It returns true only if a ping must be sent. This can be used\n// by the caller (handleData) to make decision about batching\n// a window update with it.\nfunc (b *bdpEstimator) add(n uint32) bool {\n\tb.mu.Lock()\n\tdefer b.mu.Unlock()\n\tif b.bdp == bdpLimit {\n\t\treturn false\n\t}\n\tif !b.isSent {\n\t\tb.isSent = true\n\t\tb.sample = n\n\t\tb.sentAt = time.Time{}\n\t\tb.sampleCount++\n\t\treturn true\n\t}\n\tb.sample += n\n\treturn false\n}\n\n// calculate is called when an ack for a bdp ping is received.\n// Here we calculate the current bdp and bandwidth sample and\n// decide if the flow control windows should go up.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (b *bdpEstimator) calculate(d [8]byte) {\n# Natural Language Annotations: // calculate is called when an ack for a bdp ping is received.\n// Here we calculate the current bdp and bandwidth sample and\n// decide if the flow control windows should go up.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (b *bdpEstimator) calculate(d [8]byte) {\n\tif bdpPing.data != d {\n\t\treturn\n\t}\n\n\tb.mu.Lock()\n\tdefer b.mu.Unlock()\n\n\t// calculate the round trip time\n\trtt := time.Since(b.sentAt).Seconds()\n\tb.rtt = b.rtt*alpha + rtt*(1-alpha)\n\n\t// calculate the bandwidth sample\n\tbwSample := float64(b.sample) / (b.rtt * float64(time.Second))\n\tb.bwMax = math.Max(b.bwMax, bwSample)\n\n\t// calculate the current bdp\n\tbdp := uint32(b.bwMax * b.rtt * float64(time.Second))\n\n\t// decide if the flow control windows should go up\n\tif b.bdp < bdp {\n\t\tb.bdp = uint32(math.Min(float64(bdpLimit), float64(bdp)*gamma))\n\t\tb.updateFlowControl(b.bdp)\n\t}\n\n\t// reset the sample and isSent flag\n\tb.sample = 0\n\tb.isSent = false\n}\n```", "```go\nfunc (b *bdpEstimator) calculate(d [8]byte) {\n\tif bdpPing.data != d {\n\t\treturn\n\t}\n\n\tb.mu.Lock()\n\tdefer b.mu.Unlock()\n\n\t// Calculate the round trip time.\n\trtt := time.Since(b.sentAt).Seconds()\n\t// Update the moving average of the round trip time.\n\tb.rtt = b.rtt*alpha + rtt*(1-alpha)\n\n\t// Calculate the bandwidth sample.\n\tbw := float64(b.sample) / rtt\n\t// Update the maximum bandwidth observed so far.\n\tif b.bwMax < bw {\n\t\tb.bwMax = bw\n\t}\n\n\t// Calculate the new bdp estimate.\n\tbdp := uint32(b.rtt * b.bwMax)\n\n\t// If the current bdp sample is greater than or equal to\n\t// our beta * our estimated bdp and the current bandwidth\n\t// sample is the maximum bandwidth observed so far, we\n\t// increase our bbp estimate by a factor of gamma.\n\tif b.bdp < bdp && bdp >= beta*float64(b.bdp) && bw == b.bwMax {\n\t\tb.bdp = uint32(gamma * float64(b.bdp))\n\t\tif b.bdp > bdpLimit {\n\t\t\tb.bdp = bdpLimit\n\t\t}\n\t\t// Update the flow control windows.\n\t\tb.updateFlowControl(b.bdp)\n\t}\n\n\t// Reset for the next measurement cycle.\n\tb.sample = 0\n\tb.isSent = false\n}\n```", "```go\nfunc (b *bdpEstimator) calculate(d [8]byte) {\n\tif bdpPing.data != d {\n\t\treturn\n\t}\n\tb.mu.Lock()\n\tdefer b.mu.Unlock()\n\n\t// time since the ping was sent.\n\trtt := time.Since(b.sentAt).Seconds()\n\n\t// update the rtt moving average.\n\tb.rtt = b.rtt*alpha + rtt*(1-alpha)\n\n\t// calculate the bandwidth (bytes per second).\n\tbw := float64(b.sample) / rtt\n\n\t// update the maximum bandwidth.\n\tif b.bwMax < bw {\n\t\tb.bwMax = bw\n\t}\n\n\t// calculate the bandwidth delay product.\n\tbdp := bw * b.rtt\n\n\t// update the bandwidth delay product.\n\tif b.bdp < uint32(bdp) {\n\t\tb.bdp = uint32(bdp)\n\t}\n\n\t// if the current bdp sample is greater than or equal to\n\t// our beta * our estimated bdp and the current bandwidth\n\t// sample is the maximum bandwidth observed so far, we\n\t// increase our bbp estimate by a factor of gamma.\n\tif bdp >= beta*float64(b.bdp) && b.bwMax == bw {\n\t\tb.bdp = uint32(bdp * gamma)\n\t}\n\n\t// reset for the next measurement cycle.\n\tb.sample = 0\n\tb.isSent = false\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/grpc/http2_server.go-finishStream": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/grpc/http2_server.go:\n```\n/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage grpc\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"math/rand\"\n\t\"net\"\n\t\"strconv\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf/encoding\"\n\n\t\"github.com/cloudwego/netpoll\"\n\t\"golang.org/x/net/http2\"\n\t\"golang.org/x/net/http2/hpack\"\n\t\"google.golang.org/protobuf/proto\"\n\n\t\"github.com/cloudwego/kitex/pkg/gofunc\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc/grpcframe\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/status\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\nvar (\n\t// ErrIllegalHeaderWrite indicates that setting header is illegal because of\n\t// the stream's state.\n\tErrIllegalHeaderWrite = errors.New(\"transport: the stream is done or WriteHeader was already called\")\n\t// ErrHeaderListSizeLimitViolation indicates that the header list size is larger\n\t// than the limit set by peer.\n\tErrHeaderListSizeLimitViolation = errors.New(\"transport: trying to send header list size larger than the limit set by peer\")\n)\n\nfunc init() {\n\trand.Seed(time.Now().UnixNano())\n}\n\n// http2Server implements the ServerTransport interface with HTTP2.\ntype http2Server struct {\n\tlastRead    int64\n\tctx         context.Context\n\tdone        chan struct{}\n\tconn        net.Conn\n\tloopy       *loopyWriter\n\treaderDone  chan struct{} // sync point to enable testing.\n\twriterDone  chan struct{} // sync point to enable testing.\n\tremoteAddr  net.Addr\n\tlocalAddr   net.Addr\n\tmaxStreamID uint32 // max stream ID ever seen\n\tframer      *framer\n\t// The max number of concurrent streams.\n\tmaxStreams uint32\n\t// controlBuf delivers all the control related tasks (e.g., window\n\t// updates, reset streams, and various settings) to the controller.\n\tcontrolBuf *controlBuffer\n\tfc         *trInFlow\n\t// Keepalive and max-age parameters for the server.\n\tkp ServerKeepalive\n\t// Keepalive enforcement policy.\n\tkep EnforcementPolicy\n\t// The time instance last ping was received.\n\tlastPingAt time.Time\n\t// Number of times the client has violated keepalive ping policy so far.\n\tpingStrikes uint8\n\t// Flag to signify that number of ping strikes should be reset to 0.\n\t// This is set whenever data or header frames are sent.\n\t// 1 means yes.\n\tresetPingStrikes      uint32 // Accessed atomically.\n\tinitialWindowSize     int32\n\tbdpEst                *bdpEstimator\n\tmaxSendHeaderListSize *uint32\n\n\tmu sync.Mutex // guard the following\n\t// drainChan is initialized when drain(...) is called the first time.\n\t// After which the server writes out the first GoAway(with ID 2^31-1) frame.\n\t// Then an independent goroutine will be launched to later send the second GoAway.\n\t// During this time we don't want to write another first GoAway(with ID 2^31 -1) frame.\n\t// Thus call to drain(...) will be a no-op if drainChan is already initialized since draining is\n\t// already underway.\n\tdrainChan     chan struct{}\n\tstate         transportState\n\tactiveStreams map[uint32]*Stream\n\t// idle is the time instant when the connection went idle.\n\t// This is either the beginning of the connection or when the number of\n\t// RPCs go down to 0.\n\t// When the connection is busy, this value is set to 0.\n\tidle time.Time\n\n\tbufferPool *bufferPool\n}\n\n// newHTTP2Server constructs a ServerTransport based on HTTP2. ConnectionError is\n// returned if something goes wrong.\nfunc newHTTP2Server(ctx context.Context, conn net.Conn, config *ServerConfig) (_ ServerTransport, err error) {\n\tmaxHeaderListSize := defaultServerMaxHeaderListSize\n\tif config.MaxHeaderListSize != nil {\n\t\tmaxHeaderListSize = *config.MaxHeaderListSize\n\t}\n\n\tframer := newFramer(conn, config.WriteBufferSize, config.ReadBufferSize, maxHeaderListSize)\n\t// Send initial settings as connection preface to client.\n\tisettings := []http2.Setting{{\n\t\tID:  http2.SettingMaxFrameSize,\n\t\tVal: http2MaxFrameLen,\n\t}}\n\n\t// 0 is permitted in the HTTP2 spec.\n\tmaxStreams := config.MaxStreams\n\tif maxStreams == 0 {\n\t\tmaxStreams = math.MaxUint32\n\t} else {\n\t\tisettings = append(isettings, http2.Setting{\n\t\t\tID:  http2.SettingMaxConcurrentStreams,\n\t\t\tVal: maxStreams,\n\t\t})\n\t}\n\n\tdynamicWindow := true\n\tiwz := initialWindowSize\n\tif config.InitialWindowSize >= defaultWindowSize {\n\t\tiwz = config.InitialWindowSize\n\t\tdynamicWindow = false\n\n\t\tisettings = append(isettings, http2.Setting{\n\t\t\tID:  http2.SettingInitialWindowSize,\n\t\t\tVal: iwz,\n\t\t})\n\t}\n\ticwz := initialWindowSize\n\tif config.InitialConnWindowSize >= defaultWindowSize {\n\t\ticwz = config.InitialConnWindowSize\n\t\tdynamicWindow = false\n\t}\n\tif config.MaxHeaderListSize != nil {\n\t\tisettings = append(isettings, http2.Setting{\n\t\t\tID:  http2.SettingMaxHeaderListSize,\n\t\t\tVal: *config.MaxHeaderListSize,\n\t\t})\n\t}\n\n\tif err := framer.WriteSettings(isettings...); err != nil {\n\t\treturn nil, connectionErrorf(false, err, \"transport: %v\", err)\n\t}\n\n\t// Adjust the connection flow control window if needed.\n\tif icwz > defaultWindowSize {\n\t\tif delta := icwz - defaultWindowSize; delta > 0 {\n\t\t\tif err := framer.WriteWindowUpdate(0, delta); err != nil {\n\t\t\t\treturn nil, connectionErrorf(false, err, \"transport: %v\", err)\n\t\t\t}\n\t\t}\n\t}\n\tkp := config.KeepaliveParams\n\tif kp.MaxConnectionIdle == 0 {\n\t\tkp.MaxConnectionIdle = defaultMaxConnectionIdle\n\t}\n\tif kp.MaxConnectionAge == 0 {\n\t\tkp.MaxConnectionAge = defaultMaxConnectionAge\n\t}\n\tif kp.MaxConnectionAgeGrace == 0 {\n\t\tkp.MaxConnectionAgeGrace = defaultMaxConnectionAgeGrace\n\t}\n\tif kp.Time == 0 {\n\t\tkp.Time = defaultServerKeepaliveTime\n\t}\n\tif kp.Timeout == 0 {\n\t\tkp.Timeout = defaultServerKeepaliveTimeout\n\t}\n\tkep := config.KeepaliveEnforcementPolicy\n\tif kep.MinTime == 0 {\n\t\tkep.MinTime = defaultKeepalivePolicyMinTime\n\t}\n\n\tdone := make(chan struct{})\n\tt := &http2Server{\n\t\tctx:               ctx,\n\t\tdone:              done,\n\t\tconn:              conn,\n\t\tremoteAddr:        conn.RemoteAddr(),\n\t\tlocalAddr:         conn.LocalAddr(),\n\t\tframer:            framer,\n\t\treaderDone:        make(chan struct{}),\n\t\twriterDone:        make(chan struct{}),\n\t\tmaxStreams:        math.MaxUint32,\n\t\tfc:                &trInFlow{limit: icwz},\n\t\tstate:             reachable,\n\t\tactiveStreams:     make(map[uint32]*Stream),\n\t\tkp:                kp,\n\t\tkep:               kep,\n\t\tidle:              time.Now(),\n\t\tinitialWindowSize: int32(iwz),\n\t\tbufferPool:        newBufferPool(),\n\t}\n\tt.controlBuf = newControlBuffer(t.done)\n\tif dynamicWindow {\n\t\tt.bdpEst = &bdpEstimator{\n\t\t\tbdp:               initialWindowSize,\n\t\t\tupdateFlowControl: t.updateFlowControl,\n\t\t}\n\t}\n\n\tt.framer.writer.Flush()\n\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tt.Close()\n\t\t}\n\t}()\n\n\t// Check the validity of client preface.\n\tpreface := make([]byte, len(ClientPreface))\n\tif _, err := io.ReadFull(t.conn, preface); err != nil {\n\t\t// In deployments where a gRPC server runs behind a cloud load balancer\n\t\t// which performs regular TCP level health checks, the connection is\n\t\t// closed immediately by the latter.  Returning io.EOF here allows the\n\t\t// grpc server implementation to recognize this scenario and suppress\n\t\t// logging to reduce spam.\n\t\tif err == io.EOF {\n\t\t\treturn nil, io.EOF\n\t\t}\n\t\treturn nil, connectionErrorf(false, err, \"transport: http2Server.HandleStreams failed to receive the preface from client: %v\", err)\n\t}\n\tif !bytes.Equal(preface, ClientPreface) {\n\t\treturn nil, connectionErrorf(false, nil, \"transport: http2Server.HandleStreams received bogus greeting from client: %q\", preface)\n\t}\n\n\tframe, err := t.framer.ReadFrame()\n\tif err == io.EOF || err == io.ErrUnexpectedEOF {\n\t\treturn nil, err\n\t}\n\tif err != nil {\n\t\treturn nil, connectionErrorf(false, err, \"transport: http2Server.HandleStreams failed to read initial settings frame: %v\", err)\n\t}\n\tatomic.StoreInt64(&t.lastRead, time.Now().UnixNano())\n\tsf, ok := frame.(*grpcframe.SettingsFrame)\n\tif !ok {\n\t\treturn nil, connectionErrorf(false, nil, \"transport: http2Server.HandleStreams saw invalid preface type %T from client\", frame)\n\t}\n\tt.handleSettings(sf)\n\n\tgofunc.RecoverGoFuncWithInfo(ctx, func() {\n\t\tt.loopy = newLoopyWriter(serverSide, t.framer, t.controlBuf, t.bdpEst)\n\t\tt.loopy.ssGoAwayHandler = t.outgoingGoAwayHandler\n\t\tif err := t.loopy.run(conn.RemoteAddr().String()); err != nil {\n\t\t\tklog.CtxErrorf(ctx, \"KITEX: grpc server loopyWriter.run returning, error=%v\", err)\n\t\t}\n\t\tt.conn.Close()\n\t\tclose(t.writerDone)\n\t}, gofunc.NewBasicInfo(\"\", conn.RemoteAddr().String()))\n\n\tgofunc.RecoverGoFuncWithInfo(ctx, t.keepalive, gofunc.NewBasicInfo(\"\", conn.RemoteAddr().String()))\n\treturn t, nil\n}\n\n// operateHeader takes action on the decoded headers.\nfunc (t *http2Server) operateHeaders(frame *grpcframe.MetaHeadersFrame, handle func(*Stream), traceCtx func(context.Context, string) context.Context) (fatal bool) {\n\tstreamID := frame.Header().StreamID\n\tstate := &decodeState{\n\t\tserverSide: true,\n\t}\n\tif err := state.decodeHeader(frame); err != nil {\n\t\tif se, ok := status.FromError(err); ok {\n\t\t\tt.controlBuf.put(&cleanupStream{\n\t\t\t\tstreamID: streamID,\n\t\t\t\trst:      true,\n\t\t\t\trstCode:  statusCodeConvTab[se.Code()],\n\t\t\t\tonWrite:  func() {},\n\t\t\t})\n\t\t}\n\t\treturn false\n\t}\n\n\tbuf := newRecvBuffer()\n\ts := &Stream{\n\t\tid:             streamID,\n\t\tst:             t,\n\t\tbuf:            buf,\n\t\tfc:             &inFlow{limit: uint32(t.initialWindowSize)},\n\t\trecvCompress:   state.data.encoding,\n\t\tsendCompress:   state.data.acceptEncoding,\n\t\tmethod:         state.data.method,\n\t\tcontentSubtype: state.data.contentSubtype,\n\t}\n\tif frame.StreamEnded() {\n\t\t// s is just created by the caller. No lock needed.\n\t\ts.state = streamReadDone\n\t}\n\tif state.data.timeoutSet {\n\t\ts.ctx, s.cancel = context.WithTimeout(t.ctx, state.data.timeout)\n\t} else {\n\t\ts.ctx, s.cancel = context.WithCancel(t.ctx)\n\t}\n\t// Attach the received metadata to the context.\n\tif len(state.data.mdata) > 0 {\n\t\ts.ctx = metadata.NewIncomingContext(s.ctx, state.data.mdata)\n\t}\n\n\tt.mu.Lock()\n\tif t.state != reachable {\n\t\tt.mu.Unlock()\n\t\ts.cancel()\n\t\treturn false\n\t}\n\tif uint32(len(t.activeStreams)) >= t.maxStreams {\n\t\tt.mu.Unlock()\n\t\tt.controlBuf.put(&cleanupStream{\n\t\t\tstreamID: streamID,\n\t\t\trst:      true,\n\t\t\trstCode:  http2.ErrCodeRefusedStream,\n\t\t\tonWrite:  func() {},\n\t\t})\n\t\ts.cancel()\n\t\treturn false\n\t}\n\tif streamID%2 != 1 || streamID <= t.maxStreamID {\n\t\tt.mu.Unlock()\n\t\t// illegal gRPC stream id.\n\t\tklog.CtxErrorf(s.ctx, \"transport: http2Server.HandleStreams received an illegal stream id: %v\", streamID)\n\t\ts.cancel()\n\t\treturn true\n\t}\n\tt.maxStreamID = streamID\n\tt.activeStreams[streamID] = s\n\tif len(t.activeStreams) == 1 {\n\t\tt.idle = time.Time{}\n\t}\n\tt.mu.Unlock()\n\ts.requestRead = func(n int) {\n\t\tt.adjustWindow(s, uint32(n))\n\t}\n\ts.ctx = traceCtx(s.ctx, s.method)\n\ts.ctxDone = s.ctx.Done()\n\ts.wq = newWriteQuota(defaultWriteQuota, s.ctxDone)\n\ts.trReader = &transportReader{\n\t\treader: &recvBufferReader{\n\t\t\tctx:        s.ctx,\n\t\t\tctxDone:    s.ctxDone,\n\t\t\trecv:       s.buf,\n\t\t\tfreeBuffer: t.bufferPool.put,\n\t\t},\n\t\twindowHandler: func(n int) {\n\t\t\tt.updateWindow(s, uint32(n))\n\t\t},\n\t}\n\t// Register the stream with loopy.\n\tt.controlBuf.put(&registerStream{\n\t\tstreamID: s.id,\n\t\twq:       s.wq,\n\t})\n\thandle(s)\n\treturn false\n}\n\n// HandleStreams receives incoming streams using the given handler. This is\n// typically run in a separate goroutine.\n// traceCtx attaches trace to ctx and returns the new context.\nfunc (t *http2Server) HandleStreams(handle func(*Stream), traceCtx func(context.Context, string) context.Context) {\n\tdefer close(t.readerDone)\n\tfor {\n\t\tt.controlBuf.throttle()\n\t\tframe, err := t.framer.ReadFrame()\n\t\tatomic.StoreInt64(&t.lastRead, time.Now().UnixNano())\n\t\tif err != nil {\n\t\t\tif se, ok := err.(http2.StreamError); ok {\n\t\t\t\tklog.CtxWarnf(t.ctx, \"transport: http2Server.HandleStreams encountered http2.StreamError: %v\", se)\n\t\t\t\tt.mu.Lock()\n\t\t\t\ts := t.activeStreams[se.StreamID]\n\t\t\t\tt.mu.Unlock()\n\t\t\t\tif s != nil {\n\t\t\t\t\tt.closeStream(s, true, se.Code, false)\n\t\t\t\t} else {\n\t\t\t\t\tt.controlBuf.put(&cleanupStream{\n\t\t\t\t\t\tstreamID: se.StreamID,\n\t\t\t\t\t\trst:      true,\n\t\t\t\t\t\trstCode:  se.Code,\n\t\t\t\t\t\tonWrite:  func() {},\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err == io.EOF || err == io.ErrUnexpectedEOF || errors.Is(err, netpoll.ErrEOF) {\n\t\t\t\tt.Close()\n\t\t\t\treturn\n\t\t\t}\n\t\t\tklog.CtxWarnf(t.ctx, \"transport: http2Server.HandleStreams failed to read frame: %v\", err)\n\t\t\tt.Close()\n\t\t\treturn\n\t\t}\n\t\tswitch frame := frame.(type) {\n\t\tcase *grpcframe.MetaHeadersFrame:\n\t\t\tif t.operateHeaders(frame, handle, traceCtx) {\n\t\t\t\tt.Close()\n\t\t\t\tbreak\n\t\t\t}\n\t\tcase *grpcframe.DataFrame:\n\t\t\tt.handleData(frame)\n\t\tcase *http2.RSTStreamFrame:\n\t\t\tt.handleRSTStream(frame)\n\t\tcase *grpcframe.SettingsFrame:\n\t\t\tt.handleSettings(frame)\n\t\tcase *http2.PingFrame:\n\t\t\tt.handlePing(frame)\n\t\tcase *http2.WindowUpdateFrame:\n\t\t\tt.handleWindowUpdate(frame)\n\t\tcase *grpcframe.GoAwayFrame:\n\t\t\t// TODO: Handle GoAway from the client appropriately.\n\t\tdefault:\n\t\t\tklog.CtxErrorf(t.ctx, \"transport: http2Server.HandleStreams found unhandled frame type %v.\", frame)\n\t\t}\n\t\tt.framer.reader.Release()\n\t}\n}\n\nfunc (t *http2Server) getStream(f http2.Frame) (*Stream, bool) {\n\tt.mu.Lock()\n\tdefer t.mu.Unlock()\n\tif t.activeStreams == nil {\n\t\t// The transport is closing.\n\t\treturn nil, false\n\t}\n\ts, ok := t.activeStreams[f.Header().StreamID]\n\tif !ok {\n\t\t// The stream is already done.\n\t\treturn nil, false\n\t}\n\treturn s, true\n}\n\n// adjustWindow sends out extra window update over the initial window size\n// of stream if the application is requesting data larger in size than\n// the window.\nfunc (t *http2Server) adjustWindow(s *Stream, n uint32) {\n\tif w := s.fc.maybeAdjust(n); w > 0 {\n\t\tt.controlBuf.put(&outgoingWindowUpdate{streamID: s.id, increment: w})\n\t}\n}\n\n// updateFlowControl updates the incoming flow control windows\n// for the transport and the stream based on the current bdp\n// estimation.\nfunc (t *http2Server) updateFlowControl(n uint32) {\n\tt.mu.Lock()\n\tfor _, s := range t.activeStreams {\n\t\ts.fc.newLimit(n)\n\t}\n\tt.initialWindowSize = int32(n)\n\tt.mu.Unlock()\n\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\tstreamID:  0,\n\t\tincrement: t.fc.newLimit(n),\n\t})\n\tt.controlBuf.put(&outgoingSettings{\n\t\tss: []http2.Setting{\n\t\t\t{\n\t\t\t\tID:  http2.SettingInitialWindowSize,\n\t\t\t\tVal: n,\n\t\t\t},\n\t\t},\n\t})\n}\n\n// updateWindow adjusts the inbound quota for the stream and the transport.\n// Window updates will deliver to the controller for sending when\n// the cumulative quota exceeds the corresponding threshold.\nfunc (t *http2Server) updateWindow(s *Stream, n uint32) {\n\tif w := s.fc.onRead(n); w > 0 {\n\t\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\t\tstreamID:  s.id,\n\t\t\tincrement: w,\n\t\t})\n\t}\n}\n\nfunc (t *http2Server) handleData(f *grpcframe.DataFrame) {\n\tsize := f.Header().Length\n\tvar sendBDPPing bool\n\tif t.bdpEst != nil {\n\t\tsendBDPPing = t.bdpEst.add(size)\n\t}\n\t// Decouple connection's flow control from application's read.\n\t// An update on connection's flow control should not depend on\n\t// whether user application has read the data or not. Such a\n\t// restriction is already imposed on the stream's flow control,\n\t// and therefore the sender will be blocked anyways.\n\t// Decoupling the connection flow control will prevent other\n\t// active(fast) streams from starving in presence of slow or\n\t// inactive streams.\n\tif w := t.fc.onData(size); w > 0 {\n\t\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\t\tstreamID:  0,\n\t\t\tincrement: w,\n\t\t})\n\t}\n\tif sendBDPPing {\n\t\t// Avoid excessive ping detection (e.g. in an L7 proxy)\n\t\t// by sending a window update prior to the BDP ping.\n\t\tif w := t.fc.reset(); w > 0 {\n\t\t\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\t\t\tstreamID:  0,\n\t\t\t\tincrement: w,\n\t\t\t})\n\t\t}\n\t\tt.controlBuf.put(bdpPing)\n\t}\n\t// Select the right stream to dispatch.\n\ts, ok := t.getStream(f)\n\tif !ok {\n\t\treturn\n\t}\n\tif size > 0 {\n\t\tif err := s.fc.onData(size); err != nil {\n\t\t\tt.closeStream(s, true, http2.ErrCodeFlowControl, false)\n\t\t\treturn\n\t\t}\n\t\tif f.Header().Flags.Has(http2.FlagDataPadded) {\n\t\t\tif w := s.fc.onRead(size - uint32(len(f.Data()))); w > 0 {\n\t\t\t\tt.controlBuf.put(&outgoingWindowUpdate{s.id, w})\n\t\t\t}\n\t\t}\n\t\t// TODO(bradfitz, zhaoq): A copy is required here because there is no\n\t\t// guarantee f.Data() is consumed before the arrival of next frame.\n\t\t// Can this copy be eliminated?\n\t\tif len(f.Data()) > 0 {\n\t\t\tbuffer := t.bufferPool.get()\n\t\t\tbuffer.Reset()\n\t\t\tbuffer.Write(f.Data())\n\t\t\ts.write(recvMsg{buffer: buffer})\n\t\t}\n\t}\n\tif f.Header().Flags.Has(http2.FlagDataEndStream) {\n\t\t// Received the end of stream from the client.\n\t\ts.compareAndSwapState(streamActive, streamReadDone)\n\t\ts.write(recvMsg{err: io.EOF})\n\t}\n}\n\nfunc (t *http2Server) handleRSTStream(f *http2.RSTStreamFrame) {\n\t// If the stream is not deleted from the transport's active streams map, then do a regular close stream.\n\tif s, ok := t.getStream(f); ok {\n\t\tt.closeStream(s, false, 0, false)\n\t\treturn\n\t}\n\t// If the stream is already deleted from the active streams map, then put a cleanupStream item into controlbuf to delete the stream from loopy writer's established streams map.\n\tt.controlBuf.put(&cleanupStream{\n\t\tstreamID: f.Header().StreamID,\n\t\trst:      false,\n\t\trstCode:  0,\n\t\tonWrite:  func() {},\n\t})\n}\n\nfunc (t *http2Server) handleSettings(f *grpcframe.SettingsFrame) {\n\tif f.IsAck() {\n\t\treturn\n\t}\n\tvar ss []http2.Setting\n\tvar updateFuncs []func()\n\tf.ForeachSetting(func(s http2.Setting) error {\n\t\tswitch s.ID {\n\t\tcase http2.SettingMaxHeaderListSize:\n\t\t\tupdateFuncs = append(updateFuncs, func() {\n\t\t\t\tt.maxSendHeaderListSize = new(uint32)\n\t\t\t\t*t.maxSendHeaderListSize = s.Val\n\t\t\t})\n\t\tdefault:\n\t\t\tss = append(ss, s)\n\t\t}\n\t\treturn nil\n\t})\n\tt.controlBuf.executeAndPut(func(interface{}) bool {\n\t\tfor _, f := range updateFuncs {\n\t\t\tf()\n\t\t}\n\t\treturn true\n\t}, &incomingSettings{\n\t\tss: ss,\n\t})\n}\n\nconst (\n\tmaxPingStrikes\n```\n# Function Signature: func (t *http2Server) finishStream(s *Stream, rst bool, rstCode http2.ErrCode, hdr *headerFrame, eosReceived bool) {\n# Natural Language Annotations: // finishStream closes the stream and puts the trailing headerFrame into controlbuf.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (t *http2Server) finishStream(s *Stream, rst bool, rstCode http2.ErrCode, hdr *headerFrame, eosReceived bool) {\n\tt.mu.Lock()\n\tdefer t.mu.Unlock()\n\tif s.compareAndSwapState(streamActive, streamDone) {\n\t\tt.controlBuf.put(&cleanupStream{\n\t\t\tstreamID: s.id,\n\t\t\trst:      rst,\n\t\t\trstCode:  rstCode,\n\t\t\tonWrite: func() {\n\t\t\t\tif hdr != nil {\n\t\t\t\t\ts.write(hdr)\n\t\t\t\t}\n\t\t\t},\n\t\t})\n\t}\n\tif eosReceived {\n\t\t// Reset stream.\n\t\tt.handleRSTStream(&http2.RSTStreamFrame{\n\t\t\tStreamID:  s.id,\n\t\t\tErrCode:   http2.ErrCodeNo,\n\t\t\tData:      nil,\n\t\t})\n\t}\n}\n```", "```go\nfunc (t *http2Server) finishStream(s *Stream, rst bool, rstCode http2.ErrCode, hdr *headerFrame, eosReceived bool) {\n\tt.mu.Lock()\n\tif _, ok := t.activeStreams[s.id]; ok {\n\t\tdelete(t.activeStreams, s.id)\n\t\tt.controlBuf.put(&cleanupStream{\n\t\t\tstreamID: s.id,\n\t\t\trst:      rst,\n\t\t\trstCode:  rstCode,\n\t\t\tonWrite:  func() {},\n\t\t})\n\t\tif len(t.activeStreams) == 0 {\n\t\t\tt.idle = time.Now()\n\t\t}\n\t}\n\tt.mu.Unlock()\n\tif hdr != nil {\n\t\tt.controlBuf.put(hdr)\n\t}\n\tif eosReceived {\n\t\tt.controlBuf.put(&http2.DataFrame{\n\t\t\tDataFrameHeader: http2.DataFrameHeader{\n\t\t\t\tStreamID:      s.id,\n\t\t\t\tFlags:         http2.FlagDataEndStream,\n\t\t\t\tLength:        0,\n\t\t\t},\n\t\t})\n\t}\n}\n```", "```go\nfunc (t *http2Server) finishStream(s *Stream, rst bool, rstCode http2.ErrCode, hdr *headerFrame, eosReceived bool) {\n\tif rst {\n\t\tt.controlBuf.put(&cleanupStream{\n\t\t\tstreamID: s.id,\n\t\t\trst:      true,\n\t\t\trstCode:  rstCode,\n\t\t\tonWrite:  func() {},\n\t\t})\n\t} else {\n\t\tt.mu.Lock()\n\t\ts.state = streamDone\n\t\tt.mu.Unlock()\n\t\tif hdr != nil {\n\t\t\tt.controlBuf.put(hdr)\n\t\t}\n\t}\n\tif eosReceived {\n\t\ts.write(recvMsg{err: io.EOF})\n\t}\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/grpc/http2_server.go-closeStream": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/grpc/http2_server.go:\n```\n/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage grpc\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\t\"math/rand\"\n\t\"net\"\n\t\"strconv\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf/encoding\"\n\n\t\"github.com/cloudwego/netpoll\"\n\t\"golang.org/x/net/http2\"\n\t\"golang.org/x/net/http2/hpack\"\n\t\"google.golang.org/protobuf/proto\"\n\n\t\"github.com/cloudwego/kitex/pkg/gofunc\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc/grpcframe\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/status\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\nvar (\n\t// ErrIllegalHeaderWrite indicates that setting header is illegal because of\n\t// the stream's state.\n\tErrIllegalHeaderWrite = errors.New(\"transport: the stream is done or WriteHeader was already called\")\n\t// ErrHeaderListSizeLimitViolation indicates that the header list size is larger\n\t// than the limit set by peer.\n\tErrHeaderListSizeLimitViolation = errors.New(\"transport: trying to send header list size larger than the limit set by peer\")\n)\n\nfunc init() {\n\trand.Seed(time.Now().UnixNano())\n}\n\n// http2Server implements the ServerTransport interface with HTTP2.\ntype http2Server struct {\n\tlastRead    int64\n\tctx         context.Context\n\tdone        chan struct{}\n\tconn        net.Conn\n\tloopy       *loopyWriter\n\treaderDone  chan struct{} // sync point to enable testing.\n\twriterDone  chan struct{} // sync point to enable testing.\n\tremoteAddr  net.Addr\n\tlocalAddr   net.Addr\n\tmaxStreamID uint32 // max stream ID ever seen\n\tframer      *framer\n\t// The max number of concurrent streams.\n\tmaxStreams uint32\n\t// controlBuf delivers all the control related tasks (e.g., window\n\t// updates, reset streams, and various settings) to the controller.\n\tcontrolBuf *controlBuffer\n\tfc         *trInFlow\n\t// Keepalive and max-age parameters for the server.\n\tkp ServerKeepalive\n\t// Keepalive enforcement policy.\n\tkep EnforcementPolicy\n\t// The time instance last ping was received.\n\tlastPingAt time.Time\n\t// Number of times the client has violated keepalive ping policy so far.\n\tpingStrikes uint8\n\t// Flag to signify that number of ping strikes should be reset to 0.\n\t// This is set whenever data or header frames are sent.\n\t// 1 means yes.\n\tresetPingStrikes      uint32 // Accessed atomically.\n\tinitialWindowSize     int32\n\tbdpEst                *bdpEstimator\n\tmaxSendHeaderListSize *uint32\n\n\tmu sync.Mutex // guard the following\n\t// drainChan is initialized when drain(...) is called the first time.\n\t// After which the server writes out the first GoAway(with ID 2^31-1) frame.\n\t// Then an independent goroutine will be launched to later send the second GoAway.\n\t// During this time we don't want to write another first GoAway(with ID 2^31 -1) frame.\n\t// Thus call to drain(...) will be a no-op if drainChan is already initialized since draining is\n\t// already underway.\n\tdrainChan     chan struct{}\n\tstate         transportState\n\tactiveStreams map[uint32]*Stream\n\t// idle is the time instant when the connection went idle.\n\t// This is either the beginning of the connection or when the number of\n\t// RPCs go down to 0.\n\t// When the connection is busy, this value is set to 0.\n\tidle time.Time\n\n\tbufferPool *bufferPool\n}\n\n// newHTTP2Server constructs a ServerTransport based on HTTP2. ConnectionError is\n// returned if something goes wrong.\nfunc newHTTP2Server(ctx context.Context, conn net.Conn, config *ServerConfig) (_ ServerTransport, err error) {\n\tmaxHeaderListSize := defaultServerMaxHeaderListSize\n\tif config.MaxHeaderListSize != nil {\n\t\tmaxHeaderListSize = *config.MaxHeaderListSize\n\t}\n\n\tframer := newFramer(conn, config.WriteBufferSize, config.ReadBufferSize, maxHeaderListSize)\n\t// Send initial settings as connection preface to client.\n\tisettings := []http2.Setting{{\n\t\tID:  http2.SettingMaxFrameSize,\n\t\tVal: http2MaxFrameLen,\n\t}}\n\n\t// 0 is permitted in the HTTP2 spec.\n\tmaxStreams := config.MaxStreams\n\tif maxStreams == 0 {\n\t\tmaxStreams = math.MaxUint32\n\t} else {\n\t\tisettings = append(isettings, http2.Setting{\n\t\t\tID:  http2.SettingMaxConcurrentStreams,\n\t\t\tVal: maxStreams,\n\t\t})\n\t}\n\n\tdynamicWindow := true\n\tiwz := initialWindowSize\n\tif config.InitialWindowSize >= defaultWindowSize {\n\t\tiwz = config.InitialWindowSize\n\t\tdynamicWindow = false\n\n\t\tisettings = append(isettings, http2.Setting{\n\t\t\tID:  http2.SettingInitialWindowSize,\n\t\t\tVal: iwz,\n\t\t})\n\t}\n\ticwz := initialWindowSize\n\tif config.InitialConnWindowSize >= defaultWindowSize {\n\t\ticwz = config.InitialConnWindowSize\n\t\tdynamicWindow = false\n\t}\n\tif config.MaxHeaderListSize != nil {\n\t\tisettings = append(isettings, http2.Setting{\n\t\t\tID:  http2.SettingMaxHeaderListSize,\n\t\t\tVal: *config.MaxHeaderListSize,\n\t\t})\n\t}\n\n\tif err := framer.WriteSettings(isettings...); err != nil {\n\t\treturn nil, connectionErrorf(false, err, \"transport: %v\", err)\n\t}\n\n\t// Adjust the connection flow control window if needed.\n\tif icwz > defaultWindowSize {\n\t\tif delta := icwz - defaultWindowSize; delta > 0 {\n\t\t\tif err := framer.WriteWindowUpdate(0, delta); err != nil {\n\t\t\t\treturn nil, connectionErrorf(false, err, \"transport: %v\", err)\n\t\t\t}\n\t\t}\n\t}\n\tkp := config.KeepaliveParams\n\tif kp.MaxConnectionIdle == 0 {\n\t\tkp.MaxConnectionIdle = defaultMaxConnectionIdle\n\t}\n\tif kp.MaxConnectionAge == 0 {\n\t\tkp.MaxConnectionAge = defaultMaxConnectionAge\n\t}\n\tif kp.MaxConnectionAgeGrace == 0 {\n\t\tkp.MaxConnectionAgeGrace = defaultMaxConnectionAgeGrace\n\t}\n\tif kp.Time == 0 {\n\t\tkp.Time = defaultServerKeepaliveTime\n\t}\n\tif kp.Timeout == 0 {\n\t\tkp.Timeout = defaultServerKeepaliveTimeout\n\t}\n\tkep := config.KeepaliveEnforcementPolicy\n\tif kep.MinTime == 0 {\n\t\tkep.MinTime = defaultKeepalivePolicyMinTime\n\t}\n\n\tdone := make(chan struct{})\n\tt := &http2Server{\n\t\tctx:               ctx,\n\t\tdone:              done,\n\t\tconn:              conn,\n\t\tremoteAddr:        conn.RemoteAddr(),\n\t\tlocalAddr:         conn.LocalAddr(),\n\t\tframer:            framer,\n\t\treaderDone:        make(chan struct{}),\n\t\twriterDone:        make(chan struct{}),\n\t\tmaxStreams:        math.MaxUint32,\n\t\tfc:                &trInFlow{limit: icwz},\n\t\tstate:             reachable,\n\t\tactiveStreams:     make(map[uint32]*Stream),\n\t\tkp:                kp,\n\t\tkep:               kep,\n\t\tidle:              time.Now(),\n\t\tinitialWindowSize: int32(iwz),\n\t\tbufferPool:        newBufferPool(),\n\t}\n\tt.controlBuf = newControlBuffer(t.done)\n\tif dynamicWindow {\n\t\tt.bdpEst = &bdpEstimator{\n\t\t\tbdp:               initialWindowSize,\n\t\t\tupdateFlowControl: t.updateFlowControl,\n\t\t}\n\t}\n\n\tt.framer.writer.Flush()\n\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tt.Close()\n\t\t}\n\t}()\n\n\t// Check the validity of client preface.\n\tpreface := make([]byte, len(ClientPreface))\n\tif _, err := io.ReadFull(t.conn, preface); err != nil {\n\t\t// In deployments where a gRPC server runs behind a cloud load balancer\n\t\t// which performs regular TCP level health checks, the connection is\n\t\t// closed immediately by the latter.  Returning io.EOF here allows the\n\t\t// grpc server implementation to recognize this scenario and suppress\n\t\t// logging to reduce spam.\n\t\tif err == io.EOF {\n\t\t\treturn nil, io.EOF\n\t\t}\n\t\treturn nil, connectionErrorf(false, err, \"transport: http2Server.HandleStreams failed to receive the preface from client: %v\", err)\n\t}\n\tif !bytes.Equal(preface, ClientPreface) {\n\t\treturn nil, connectionErrorf(false, nil, \"transport: http2Server.HandleStreams received bogus greeting from client: %q\", preface)\n\t}\n\n\tframe, err := t.framer.ReadFrame()\n\tif err == io.EOF || err == io.ErrUnexpectedEOF {\n\t\treturn nil, err\n\t}\n\tif err != nil {\n\t\treturn nil, connectionErrorf(false, err, \"transport: http2Server.HandleStreams failed to read initial settings frame: %v\", err)\n\t}\n\tatomic.StoreInt64(&t.lastRead, time.Now().UnixNano())\n\tsf, ok := frame.(*grpcframe.SettingsFrame)\n\tif !ok {\n\t\treturn nil, connectionErrorf(false, nil, \"transport: http2Server.HandleStreams saw invalid preface type %T from client\", frame)\n\t}\n\tt.handleSettings(sf)\n\n\tgofunc.RecoverGoFuncWithInfo(ctx, func() {\n\t\tt.loopy = newLoopyWriter(serverSide, t.framer, t.controlBuf, t.bdpEst)\n\t\tt.loopy.ssGoAwayHandler = t.outgoingGoAwayHandler\n\t\tif err := t.loopy.run(conn.RemoteAddr().String()); err != nil {\n\t\t\tklog.CtxErrorf(ctx, \"KITEX: grpc server loopyWriter.run returning, error=%v\", err)\n\t\t}\n\t\tt.conn.Close()\n\t\tclose(t.writerDone)\n\t}, gofunc.NewBasicInfo(\"\", conn.RemoteAddr().String()))\n\n\tgofunc.RecoverGoFuncWithInfo(ctx, t.keepalive, gofunc.NewBasicInfo(\"\", conn.RemoteAddr().String()))\n\treturn t, nil\n}\n\n// operateHeader takes action on the decoded headers.\nfunc (t *http2Server) operateHeaders(frame *grpcframe.MetaHeadersFrame, handle func(*Stream), traceCtx func(context.Context, string) context.Context) (fatal bool) {\n\tstreamID := frame.Header().StreamID\n\tstate := &decodeState{\n\t\tserverSide: true,\n\t}\n\tif err := state.decodeHeader(frame); err != nil {\n\t\tif se, ok := status.FromError(err); ok {\n\t\t\tt.controlBuf.put(&cleanupStream{\n\t\t\t\tstreamID: streamID,\n\t\t\t\trst:      true,\n\t\t\t\trstCode:  statusCodeConvTab[se.Code()],\n\t\t\t\tonWrite:  func() {},\n\t\t\t})\n\t\t}\n\t\treturn false\n\t}\n\n\tbuf := newRecvBuffer()\n\ts := &Stream{\n\t\tid:             streamID,\n\t\tst:             t,\n\t\tbuf:            buf,\n\t\tfc:             &inFlow{limit: uint32(t.initialWindowSize)},\n\t\trecvCompress:   state.data.encoding,\n\t\tsendCompress:   state.data.acceptEncoding,\n\t\tmethod:         state.data.method,\n\t\tcontentSubtype: state.data.contentSubtype,\n\t}\n\tif frame.StreamEnded() {\n\t\t// s is just created by the caller. No lock needed.\n\t\ts.state = streamReadDone\n\t}\n\tif state.data.timeoutSet {\n\t\ts.ctx, s.cancel = context.WithTimeout(t.ctx, state.data.timeout)\n\t} else {\n\t\ts.ctx, s.cancel = context.WithCancel(t.ctx)\n\t}\n\t// Attach the received metadata to the context.\n\tif len(state.data.mdata) > 0 {\n\t\ts.ctx = metadata.NewIncomingContext(s.ctx, state.data.mdata)\n\t}\n\n\tt.mu.Lock()\n\tif t.state != reachable {\n\t\tt.mu.Unlock()\n\t\ts.cancel()\n\t\treturn false\n\t}\n\tif uint32(len(t.activeStreams)) >= t.maxStreams {\n\t\tt.mu.Unlock()\n\t\tt.controlBuf.put(&cleanupStream{\n\t\t\tstreamID: streamID,\n\t\t\trst:      true,\n\t\t\trstCode:  http2.ErrCodeRefusedStream,\n\t\t\tonWrite:  func() {},\n\t\t})\n\t\ts.cancel()\n\t\treturn false\n\t}\n\tif streamID%2 != 1 || streamID <= t.maxStreamID {\n\t\tt.mu.Unlock()\n\t\t// illegal gRPC stream id.\n\t\tklog.CtxErrorf(s.ctx, \"transport: http2Server.HandleStreams received an illegal stream id: %v\", streamID)\n\t\ts.cancel()\n\t\treturn true\n\t}\n\tt.maxStreamID = streamID\n\tt.activeStreams[streamID] = s\n\tif len(t.activeStreams) == 1 {\n\t\tt.idle = time.Time{}\n\t}\n\tt.mu.Unlock()\n\ts.requestRead = func(n int) {\n\t\tt.adjustWindow(s, uint32(n))\n\t}\n\ts.ctx = traceCtx(s.ctx, s.method)\n\ts.ctxDone = s.ctx.Done()\n\ts.wq = newWriteQuota(defaultWriteQuota, s.ctxDone)\n\ts.trReader = &transportReader{\n\t\treader: &recvBufferReader{\n\t\t\tctx:        s.ctx,\n\t\t\tctxDone:    s.ctxDone,\n\t\t\trecv:       s.buf,\n\t\t\tfreeBuffer: t.bufferPool.put,\n\t\t},\n\t\twindowHandler: func(n int) {\n\t\t\tt.updateWindow(s, uint32(n))\n\t\t},\n\t}\n\t// Register the stream with loopy.\n\tt.controlBuf.put(&registerStream{\n\t\tstreamID: s.id,\n\t\twq:       s.wq,\n\t})\n\thandle(s)\n\treturn false\n}\n\n// HandleStreams receives incoming streams using the given handler. This is\n// typically run in a separate goroutine.\n// traceCtx attaches trace to ctx and returns the new context.\nfunc (t *http2Server) HandleStreams(handle func(*Stream), traceCtx func(context.Context, string) context.Context) {\n\tdefer close(t.readerDone)\n\tfor {\n\t\tt.controlBuf.throttle()\n\t\tframe, err := t.framer.ReadFrame()\n\t\tatomic.StoreInt64(&t.lastRead, time.Now().UnixNano())\n\t\tif err != nil {\n\t\t\tif se, ok := err.(http2.StreamError); ok {\n\t\t\t\tklog.CtxWarnf(t.ctx, \"transport: http2Server.HandleStreams encountered http2.StreamError: %v\", se)\n\t\t\t\tt.mu.Lock()\n\t\t\t\ts := t.activeStreams[se.StreamID]\n\t\t\t\tt.mu.Unlock()\n\t\t\t\tif s != nil {\n\t\t\t\t\tt.closeStream(s, true, se.Code, false)\n\t\t\t\t} else {\n\t\t\t\t\tt.controlBuf.put(&cleanupStream{\n\t\t\t\t\t\tstreamID: se.StreamID,\n\t\t\t\t\t\trst:      true,\n\t\t\t\t\t\trstCode:  se.Code,\n\t\t\t\t\t\tonWrite:  func() {},\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif err == io.EOF || err == io.ErrUnexpectedEOF || errors.Is(err, netpoll.ErrEOF) {\n\t\t\t\tt.Close()\n\t\t\t\treturn\n\t\t\t}\n\t\t\tklog.CtxWarnf(t.ctx, \"transport: http2Server.HandleStreams failed to read frame: %v\", err)\n\t\t\tt.Close()\n\t\t\treturn\n\t\t}\n\t\tswitch frame := frame.(type) {\n\t\tcase *grpcframe.MetaHeadersFrame:\n\t\t\tif t.operateHeaders(frame, handle, traceCtx) {\n\t\t\t\tt.Close()\n\t\t\t\tbreak\n\t\t\t}\n\t\tcase *grpcframe.DataFrame:\n\t\t\tt.handleData(frame)\n\t\tcase *http2.RSTStreamFrame:\n\t\t\tt.handleRSTStream(frame)\n\t\tcase *grpcframe.SettingsFrame:\n\t\t\tt.handleSettings(frame)\n\t\tcase *http2.PingFrame:\n\t\t\tt.handlePing(frame)\n\t\tcase *http2.WindowUpdateFrame:\n\t\t\tt.handleWindowUpdate(frame)\n\t\tcase *grpcframe.GoAwayFrame:\n\t\t\t// TODO: Handle GoAway from the client appropriately.\n\t\tdefault:\n\t\t\tklog.CtxErrorf(t.ctx, \"transport: http2Server.HandleStreams found unhandled frame type %v.\", frame)\n\t\t}\n\t\tt.framer.reader.Release()\n\t}\n}\n\nfunc (t *http2Server) getStream(f http2.Frame) (*Stream, bool) {\n\tt.mu.Lock()\n\tdefer t.mu.Unlock()\n\tif t.activeStreams == nil {\n\t\t// The transport is closing.\n\t\treturn nil, false\n\t}\n\ts, ok := t.activeStreams[f.Header().StreamID]\n\tif !ok {\n\t\t// The stream is already done.\n\t\treturn nil, false\n\t}\n\treturn s, true\n}\n\n// adjustWindow sends out extra window update over the initial window size\n// of stream if the application is requesting data larger in size than\n// the window.\nfunc (t *http2Server) adjustWindow(s *Stream, n uint32) {\n\tif w := s.fc.maybeAdjust(n); w > 0 {\n\t\tt.controlBuf.put(&outgoingWindowUpdate{streamID: s.id, increment: w})\n\t}\n}\n\n// updateFlowControl updates the incoming flow control windows\n// for the transport and the stream based on the current bdp\n// estimation.\nfunc (t *http2Server) updateFlowControl(n uint32) {\n\tt.mu.Lock()\n\tfor _, s := range t.activeStreams {\n\t\ts.fc.newLimit(n)\n\t}\n\tt.initialWindowSize = int32(n)\n\tt.mu.Unlock()\n\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\tstreamID:  0,\n\t\tincrement: t.fc.newLimit(n),\n\t})\n\tt.controlBuf.put(&outgoingSettings{\n\t\tss: []http2.Setting{\n\t\t\t{\n\t\t\t\tID:  http2.SettingInitialWindowSize,\n\t\t\t\tVal: n,\n\t\t\t},\n\t\t},\n\t})\n}\n\n// updateWindow adjusts the inbound quota for the stream and the transport.\n// Window updates will deliver to the controller for sending when\n// the cumulative quota exceeds the corresponding threshold.\nfunc (t *http2Server) updateWindow(s *Stream, n uint32) {\n\tif w := s.fc.onRead(n); w > 0 {\n\t\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\t\tstreamID:  s.id,\n\t\t\tincrement: w,\n\t\t})\n\t}\n}\n\nfunc (t *http2Server) handleData(f *grpcframe.DataFrame) {\n\tsize := f.Header().Length\n\tvar sendBDPPing bool\n\tif t.bdpEst != nil {\n\t\tsendBDPPing = t.bdpEst.add(size)\n\t}\n\t// Decouple connection's flow control from application's read.\n\t// An update on connection's flow control should not depend on\n\t// whether user application has read the data or not. Such a\n\t// restriction is already imposed on the stream's flow control,\n\t// and therefore the sender will be blocked anyways.\n\t// Decoupling the connection flow control will prevent other\n\t// active(fast) streams from starving in presence of slow or\n\t// inactive streams.\n\tif w := t.fc.onData(size); w > 0 {\n\t\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\t\tstreamID:  0,\n\t\t\tincrement: w,\n\t\t})\n\t}\n\tif sendBDPPing {\n\t\t// Avoid excessive ping detection (e.g. in an L7 proxy)\n\t\t// by sending a window update prior to the BDP ping.\n\t\tif w := t.fc.reset(); w > 0 {\n\t\t\tt.controlBuf.put(&outgoingWindowUpdate{\n\t\t\t\tstreamID:  0,\n\t\t\t\tincrement: w,\n\t\t\t})\n\t\t}\n\t\tt.controlBuf.put(bdpPing)\n\t}\n\t// Select the right stream to dispatch.\n\ts, ok := t.getStream(f)\n\tif !ok {\n\t\treturn\n\t}\n\tif size > 0 {\n\t\tif err := s.fc.onData(size); err != nil {\n\t\t\tt.closeStream(s, true, http2.ErrCodeFlowControl, false)\n\t\t\treturn\n\t\t}\n\t\tif f.Header().Flags.Has(http2.FlagDataPadded) {\n\t\t\tif w := s.fc.onRead(size - uint32(len(f.Data()))); w > 0 {\n\t\t\t\tt.controlBuf.put(&outgoingWindowUpdate{s.id, w})\n\t\t\t}\n\t\t}\n\t\t// TODO(bradfitz, zhaoq): A copy is required here because there is no\n\t\t// guarantee f.Data() is consumed before the arrival of next frame.\n\t\t// Can this copy be eliminated?\n\t\tif len(f.Data()) > 0 {\n\t\t\tbuffer := t.bufferPool.get()\n\t\t\tbuffer.Reset()\n\t\t\tbuffer.Write(f.Data())\n\t\t\ts.write(recvMsg{buffer: buffer})\n\t\t}\n\t}\n\tif f.Header().Flags.Has(http2.FlagDataEndStream) {\n\t\t// Received the end of stream from the client.\n\t\ts.compareAndSwapState(streamActive, streamReadDone)\n\t\ts.write(recvMsg{err: io.EOF})\n\t}\n}\n\nfunc (t *http2Server) handleRSTStream(f *http2.RSTStreamFrame) {\n\t// If the stream is not deleted from the transport's active streams map, then do a regular close stream.\n\tif s, ok := t.getStream(f); ok {\n\t\tt.closeStream(s, false, 0, false)\n\t\treturn\n\t}\n\t// If the stream is already deleted from the active streams map, then put a cleanupStream item into controlbuf to delete the stream from loopy writer's established streams map.\n\tt.controlBuf.put(&cleanupStream{\n\t\tstreamID: f.Header().StreamID,\n\t\trst:      false,\n\t\trstCode:  0,\n\t\tonWrite:  func() {},\n\t})\n}\n\nfunc (t *http2Server) handleSettings(f *grpcframe.SettingsFrame) {\n\tif f.IsAck() {\n\t\treturn\n\t}\n\tvar ss []http2.Setting\n\tvar updateFuncs []func()\n\tf.ForeachSetting(func(s http2.Setting) error {\n\t\tswitch s.ID {\n\t\tcase http2.SettingMaxHeaderListSize:\n\t\t\tupdateFuncs = append(updateFuncs, func() {\n\t\t\t\tt.maxSendHeaderListSize = new(uint32)\n\t\t\t\t*t.maxSendHeaderListSize = s.Val\n\t\t\t})\n\t\tdefault:\n\t\t\tss = append(ss, s)\n\t\t}\n\t\treturn nil\n\t})\n\tt.controlBuf.executeAndPut(func(interface{}) bool {\n\t\tfor _, f := range updateFuncs {\n\t\t\tf()\n\t\t}\n\t\treturn true\n\t}, &incomingSettings{\n\t\tss: ss,\n\t})\n}\n\nconst (\n\tmaxPingStrikes\n```\n# Function Signature: func (t *http2Server) closeStream(s *Stream, rst bool, rstCode http2.ErrCode, eosReceived bool) {\n# Natural Language Annotations: // closeStream clears the footprint of a stream when the stream is not needed any more.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (t *http2Server) closeStream(s *Stream, rst bool, rstCode http2.ErrCode, eosReceived bool) {\n\tt.mu.Lock()\n\tdefer t.mu.Unlock()\n\tif s == nil || !s.compareAndSwapState(streamActive, streamClosed) || rst {\n\t\treturn\n\t}\n\tif eosReceived {\n\t\ts.compareAndSwapState(streamReadDone, streamClosed)\n\t}\n\tif s.state == streamClosed {\n\t\tdelete(t.activeStreams, s.id)\n\t}\n}\n```", "```go\nfunc (t *http2Server) closeStream(s *Stream, rst bool, rstCode http2.ErrCode, eosReceived bool) {\n\tt.controlBuf.put(&cleanupStream{\n\t\tstreamID: s.id,\n\t\trst:      rst,\n\t\trstCode:  rstCode,\n\t\tonWrite: func() {\n\t\t\tt.mu.Lock()\n\t\t\tdelete(t.activeStreams, s.id)\n\t\t\tt.mu.Unlock()\n\t\t\ts.cancel()\n\t\t\tif eosReceived {\n\t\t\t\tt.controlBuf.executeAndPut(func(interface{}) bool {\n\t\t\t\t\tt.loopy.done()\n\t\t\t\t\treturn true\n\t\t\t\t}, nil)\n\t\t\t}\n\t\t},\n\t})\n}\n```", "Sure, here is the code for the `closeStream` function based on the function signature and natural language annotations:\n\n```go\nfunc (t *http2Server) closeStream(s *Stream, rst bool, rstCode http2.ErrCode, eosReceived bool) {\n\ts.mu.Lock()\n\tif s.state == streamDone {\n\t\ts.mu.Unlock()\n\t\treturn\n\t}\n\tframe := &http2.RSTStreamFrame{\n\t\tStreamID: s.id,\n\t\tErrCode:  rstCode,\n\t}\n\ts.state = streamDone\n\tif rst {\n\t\tt.controlBuf.put(&outgoingFrame{\n\t\t\tframe: frame,\n\t\t\tonWrite: func() {\n\t\t\t\tt.mu.Lock()\n\t\t\t\tdelete(t.activeStreams, s.id)\n\t\t\t\tt.mu.Unlock()\n\t\t\t},\n\t\t})\n\t} else if eosReceived {\n\t\tt.controlBuf.put(&outgoingFrame{\n\t\t\tframe: frame,\n\t\t\tonWrite: func() {\n\t\t\t\tt.mu.Lock()\n\t\t\t\tdelete(t.activeStreams, s.id)\n\t\t\t\tt.mu.Unlock()\n\t\t\t},\n\t\t})\n\t} else {\n\t\tt.controlBuf.put(&outgoingFrame{frame: frame})\n\t}\n\ts.mu.Unlock()\n}\n```\nThis function is responsible for clearing the footprint of a stream when it is not needed any more. It locks the stream to ensure it is not modified while being cleared. It then creates an RST_STREAM frame with the provided error code. If the reset flag (rst) is true, it adds the frame to the outgoing control buffer with an onWrite function that deletes the stream from the active streams map. If the end of stream flag (eosReceived) is true, it also adds the frame to the outgoing control buffer with the same onWrite function. If neither flag is true, it simply adds the frame to the outgoing control buffer without an onWrite function."]}, "kitex-pkg/remote/trans/nphttp2/grpc/transport.go-TLSConfig": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/grpc/transport.go:\n```\n/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\n// Package grpc defines and implements message oriented communication\n// channel to complete various transactions (e.g., an RPC).  It is meant for\n// grpc-internal usage and is not intended to be imported directly by users.\npackage grpc\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto/tls\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/codes\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/status\"\n)\n\ntype bufferPool struct {\n\tpool sync.Pool\n}\n\nfunc newBufferPool() *bufferPool {\n\treturn &bufferPool{\n\t\tpool: sync.Pool{\n\t\t\tNew: func() interface{} {\n\t\t\t\treturn new(bytes.Buffer)\n\t\t\t},\n\t\t},\n\t}\n}\n\nfunc (p *bufferPool) get() *bytes.Buffer {\n\treturn p.pool.Get().(*bytes.Buffer)\n}\n\nfunc (p *bufferPool) put(b *bytes.Buffer) {\n\tp.pool.Put(b)\n}\n\n// recvMsg represents the received msg from the transport. All transport\n// protocol specific info has been removed.\ntype recvMsg struct {\n\tbuffer *bytes.Buffer\n\t// nil: received some data\n\t// io.EOF: stream is completed. data is nil.\n\t// other non-nil error: transport failure. data is nil.\n\terr error\n}\n\n// recvBuffer is an unbounded channel of recvMsg structs.\n//\n// Note: recvBuffer differs from buffer.Unbounded only in the fact that it\n// holds a channel of recvMsg structs instead of objects implementing \"item\"\n// interface. recvBuffer is written to much more often and using strict recvMsg\n// structs helps avoid allocation in \"recvBuffer.put\"\ntype recvBuffer struct {\n\tc       chan recvMsg\n\tmu      sync.Mutex\n\tbacklog []recvMsg\n\terr     error\n}\n\nfunc newRecvBuffer() *recvBuffer {\n\tb := &recvBuffer{\n\t\tc: make(chan recvMsg, 1),\n\t}\n\treturn b\n}\n\nfunc (b *recvBuffer) put(r recvMsg) {\n\tb.mu.Lock()\n\tif b.err != nil {\n\t\tb.mu.Unlock()\n\t\t// An error had occurred earlier, don't accept more\n\t\t// data or errors.\n\t\treturn\n\t}\n\tb.err = r.err\n\tif len(b.backlog) == 0 {\n\t\tselect {\n\t\tcase b.c <- r:\n\t\t\tb.mu.Unlock()\n\t\t\treturn\n\t\tdefault:\n\t\t}\n\t}\n\tb.backlog = append(b.backlog, r)\n\tb.mu.Unlock()\n}\n\nfunc (b *recvBuffer) load() {\n\tb.mu.Lock()\n\tif len(b.backlog) > 0 {\n\t\tselect {\n\t\tcase b.c <- b.backlog[0]:\n\t\t\tb.backlog[0] = recvMsg{}\n\t\t\tb.backlog = b.backlog[1:]\n\t\tdefault:\n\t\t}\n\t}\n\tb.mu.Unlock()\n}\n\n// get returns the channel that receives a recvMsg in the buffer.\n//\n// Upon receipt of a recvMsg, the caller should call load to send another\n// recvMsg onto the channel if there is any.\nfunc (b *recvBuffer) get() <-chan recvMsg {\n\treturn b.c\n}\n\n// recvBufferReader implements io.Reader interface to read the data from\n// recvBuffer.\ntype recvBufferReader struct {\n\tcloseStream func(error) // Closes the client transport stream with the given error and nil trailer metadata.\n\tctx         context.Context\n\tctxDone     <-chan struct{} // cache of ctx.Done() (for performance).\n\trecv        *recvBuffer\n\tlast        *bytes.Buffer // Stores the remaining data in the previous calls.\n\terr         error\n\tfreeBuffer  func(*bytes.Buffer)\n}\n\n// Read reads the next len(p) bytes from last. If last is drained, it tries to\n// read additional data from recv. It blocks if there no additional data available\n// in recv. If Read returns any non-nil error, it will continue to return that error.\nfunc (r *recvBufferReader) Read(p []byte) (n int, err error) {\n\tif r.err != nil {\n\t\treturn 0, r.err\n\t}\n\tif r.last != nil {\n\t\t// Read remaining data left in last call.\n\t\tcopied, _ := r.last.Read(p)\n\t\tif r.last.Len() == 0 {\n\t\t\tr.freeBuffer(r.last)\n\t\t\tr.last = nil\n\t\t}\n\t\treturn copied, nil\n\t}\n\tif r.closeStream != nil {\n\t\tn, r.err = r.readClient(p)\n\t} else {\n\t\tn, r.err = r.read(p)\n\t}\n\treturn n, r.err\n}\n\nfunc (r *recvBufferReader) read(p []byte) (n int, err error) {\n\tselect {\n\tcase <-r.ctxDone:\n\t\treturn 0, ContextErr(r.ctx.Err())\n\tcase m := <-r.recv.get():\n\t\treturn r.readAdditional(m, p)\n\t}\n}\n\nfunc (r *recvBufferReader) readClient(p []byte) (n int, err error) {\n\t// If the context is canceled, then closes the stream with nil metadata.\n\t// closeStream writes its error parameter to r.recv as a recvMsg.\n\t// r.readAdditional acts on that message and returns the necessary error.\n\tselect {\n\tcase <-r.ctxDone:\n\t\t// Note that this adds the ctx error to the end of recv buffer, and\n\t\t// reads from the head. This will delay the error until recv buffer is\n\t\t// empty, thus will delay ctx cancellation in Recv().\n\t\t//\n\t\t// It's done this way to fix a race between ctx cancel and trailer. The\n\t\t// race was, stream.Recv() may return ctx error if ctxDone wins the\n\t\t// race, but stream.Trailer() may return a non-nil md because the stream\n\t\t// was not marked as done when trailer is received. This closeStream\n\t\t// call will mark stream as done, thus fix the race.\n\t\t//\n\t\t// TODO: delaying ctx error seems like a unnecessary side effect. What\n\t\t// we really want is to mark the stream as done, and return ctx error\n\t\t// faster.\n\t\tr.closeStream(ContextErr(r.ctx.Err()))\n\t\tm := <-r.recv.get()\n\t\treturn r.readAdditional(m, p)\n\tcase m := <-r.recv.get():\n\t\treturn r.readAdditional(m, p)\n\t}\n}\n\nfunc (r *recvBufferReader) readAdditional(m recvMsg, p []byte) (n int, err error) {\n\tr.recv.load()\n\tif m.err != nil {\n\t\treturn 0, m.err\n\t}\n\tcopied, _ := m.buffer.Read(p)\n\tif m.buffer.Len() == 0 {\n\t\tr.freeBuffer(m.buffer)\n\t\tr.last = nil\n\t} else {\n\t\tr.last = m.buffer\n\t}\n\treturn copied, nil\n}\n\ntype streamState uint32\n\nconst (\n\tstreamActive    streamState = iota\n\tstreamWriteDone             // EndStream sent\n\tstreamReadDone              // EndStream received\n\tstreamDone                  // the entire stream is finished.\n)\n\n// Stream represents an RPC in the transport layer.\ntype Stream struct {\n\tid           uint32\n\tst           ServerTransport    // nil for client side Stream\n\tct           *http2Client       // nil for server side Stream\n\tctx          context.Context    // the associated context of the stream\n\tcancel       context.CancelFunc // always nil for client side Stream\n\tdone         chan struct{}      // closed at the end of stream to unblock writers. On the client side.\n\tctxDone      <-chan struct{}    // same as done chan but for server side. Cache of ctx.Done() (for performance)\n\tmethod       string             // the associated RPC method of the stream\n\trecvCompress string\n\tsendCompress string\n\tbuf          *recvBuffer\n\ttrReader     io.Reader\n\tfc           *inFlow\n\twq           *writeQuota\n\n\t// Callback to state application's intentions to read data. This\n\t// is used to adjust flow control, if needed.\n\trequestRead func(int)\n\n\theaderChan       chan struct{} // closed to indicate the end of header metadata.\n\theaderChanClosed uint32        // set when headerChan is closed. Used to avoid closing headerChan multiple times.\n\t// headerValid indicates whether a valid header was received.  Only\n\t// meaningful after headerChan is closed (always call waitOnHeader() before\n\t// reading its value).  Not valid on server side.\n\theaderValid bool\n\n\t// hdrMu protects header and trailer metadata on the server-side.\n\thdrMu sync.Mutex\n\t// On client side, header keeps the received header metadata.\n\t//\n\t// On server side, header keeps the header set by SetHeader(). The complete\n\t// header will merged into this after t.WriteHeader() is called.\n\theader  metadata.MD\n\ttrailer metadata.MD // the key-value map of trailer metadata.\n\n\tnoHeaders bool // set if the client never received headers (set only after the stream is done).\n\n\t// On the server-side, headerSent is atomically set to 1 when the headers are sent out.\n\theaderSent uint32\n\n\tstate streamState\n\n\t// On client-side it is the status error received from the server.\n\t// On server-side it is unused.\n\tstatus       *status.Status\n\tbizStatusErr kerrors.BizStatusErrorIface\n\n\tbytesReceived uint32 // indicates whether any bytes have been received on this stream\n\tunprocessed   uint32 // set if the server sends a refused stream or GOAWAY including this stream\n\n\t// contentSubtype is the content-subtype for requests.\n\t// this must be lowercase or the behavior is undefined.\n\tcontentSubtype string\n}\n\n// isHeaderSent is only valid on the server-side.\nfunc (s *Stream) isHeaderSent() bool {\n\treturn atomic.LoadUint32(&s.headerSent) == 1\n}\n\n// updateHeaderSent updates headerSent and returns true\n// if it was already set. It is valid only on server-side.\nfunc (s *Stream) updateHeaderSent() bool {\n\treturn atomic.SwapUint32(&s.headerSent, 1) == 1\n}\n\nfunc (s *Stream) swapState(st streamState) streamState {\n\treturn streamState(atomic.SwapUint32((*uint32)(&s.state), uint32(st)))\n}\n\nfunc (s *Stream) compareAndSwapState(oldState, newState streamState) bool {\n\treturn atomic.CompareAndSwapUint32((*uint32)(&s.state), uint32(oldState), uint32(newState))\n}\n\nfunc (s *Stream) getState() streamState {\n\treturn streamState(atomic.LoadUint32((*uint32)(&s.state)))\n}\n\nfunc (s *Stream) waitOnHeader() {\n\tif s.headerChan == nil {\n\t\t// On the server headerChan is always nil since a stream originates\n\t\t// only after having received headers.\n\t\treturn\n\t}\n\tselect {\n\tcase <-s.ctx.Done():\n\t\t// Close the stream to prevent headers/trailers from changing after\n\t\t// this function returns.\n\t\ts.ct.CloseStream(s, ContextErr(s.ctx.Err()))\n\t\t// headerChan could possibly not be closed yet if closeStream raced\n\t\t// with operateHeaders; wait until it is closed explicitly here.\n\t\t<-s.headerChan\n\tcase <-s.headerChan:\n\t}\n}\n\n// RecvCompress returns the compression algorithm applied to the inbound\n// message. It is empty string if there is no compression applied.\nfunc (s *Stream) RecvCompress() string {\n\ts.waitOnHeader()\n\treturn s.recvCompress\n}\n\n// SendCompress returns the compression algorithm applied to the outbound\n// message. It is empty string if there is no compression applied.\nfunc (s *Stream) SendCompress() string {\n\ts.waitOnHeader()\n\treturn s.sendCompress\n}\n\n// SetSendCompress sets the compression algorithm to the stream.\nfunc (s *Stream) SetSendCompress(str string) {\n\ts.sendCompress = str\n}\n\n// Done returns a channel which is closed when it receives the final status\n// from the server.\nfunc (s *Stream) Done() <-chan struct{} {\n\treturn s.done\n}\n\n// Header returns the header metadata of the stream.\n//\n// On client side, it acquires the key-value pairs of header metadata once it is\n// available. It blocks until i) the metadata is ready or ii) there is no header\n// metadata or iii) the stream is canceled/expired.\n//\n// On server side, it returns the out header after t.WriteHeader is called.  It\n// does not block and must not be called until after WriteHeader.\nfunc (s *Stream) Header() (metadata.MD, error) {\n\tif s.headerChan == nil {\n\t\t// On server side, return the header in stream. It will be the out\n\t\t// header after t.WriteHeader is called.\n\t\treturn s.header.Copy(), nil\n\t}\n\ts.waitOnHeader()\n\tif !s.headerValid {\n\t\treturn nil, s.status.Err()\n\t}\n\treturn s.header.Copy(), nil\n}\n\n// TrailersOnly blocks until a header or trailers-only frame is received and\n// then returns true if the stream was trailers-only.  If the stream ends\n// before headers are received, returns true, nil.  Client-side only.\nfunc (s *Stream) TrailersOnly() bool {\n\ts.waitOnHeader()\n\treturn s.noHeaders\n}\n\n// Trailer returns the cached trailer metedata. Note that if it is not called\n// after the entire stream is done, it could return an empty MD. Client\n// side only.\n// It can be safely read only after stream has ended that is either read\n// or write have returned io.EOF.\nfunc (s *Stream) Trailer() metadata.MD {\n\tc := s.trailer.Copy()\n\treturn c\n}\n\n// ContentSubtype returns the content-subtype for a request. For example, a\n// content-subtype of \"proto\" will result in a content-type of\n// \"application/grpc+proto\". This will always be lowercase.  See\n// https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests for\n// more details.\nfunc (s *Stream) ContentSubtype() string {\n\treturn s.contentSubtype\n}\n\n// Context returns the context of the stream.\nfunc (s *Stream) Context() context.Context {\n\treturn s.ctx\n}\n\n// Method returns the method for the stream.\nfunc (s *Stream) Method() string {\n\treturn s.method\n}\n\n// Status returns the status received from the server.\n// Status can be read safely only after the stream has ended,\n// that is, after Done() is closed.\nfunc (s *Stream) Status() *status.Status {\n\treturn s.status\n}\n\nfunc (s *Stream) SetBizStatusErr(bizStatusErr kerrors.BizStatusErrorIface) {\n\ts.bizStatusErr = bizStatusErr\n}\n\nfunc (s *Stream) BizStatusErr() kerrors.BizStatusErrorIface {\n\treturn s.bizStatusErr\n}\n\n// SetHeader sets the header metadata. This can be called multiple times.\n// Server side only.\n// This should not be called in parallel to other data writes.\nfunc (s *Stream) SetHeader(md metadata.MD) error {\n\tif md.Len() == 0 {\n\t\treturn nil\n\t}\n\tif s.isHeaderSent() || s.getState() == streamDone {\n\t\treturn ErrIllegalHeaderWrite\n\t}\n\ts.hdrMu.Lock()\n\ts.header = metadata.AppendMD(s.header, md)\n\ts.hdrMu.Unlock()\n\treturn nil\n}\n\n// SendHeader sends the given header metadata. The given metadata is\n// combined with any metadata set by previous calls to SetHeader and\n// then written to the transport stream.\nfunc (s *Stream) SendHeader(md metadata.MD) error {\n\treturn s.st.WriteHeader(s, md)\n}\n\n// SetTrailer sets the trailer metadata which will be sent with the RPC status\n// by the server. This can be called multiple times. Server side only.\n// This should not be called parallel to other data writes.\nfunc (s *Stream) SetTrailer(md metadata.MD) error {\n\tif md.Len() == 0 {\n\t\treturn nil\n\t}\n\tif s.getState() == streamDone {\n\t\treturn ErrIllegalHeaderWrite\n\t}\n\ts.hdrMu.Lock()\n\ts.trailer = metadata.AppendMD(s.trailer, md)\n\ts.hdrMu.Unlock()\n\treturn nil\n}\n\nfunc (s *Stream) write(m recvMsg) {\n\ts.buf.put(m)\n}\n\n// Read reads all p bytes from the wire for this stream.\nfunc (s *Stream) Read(p []byte) (n int, err error) {\n\t// Don't request a read if there was an error earlier\n\tif er := s.trReader.(*transportReader).er; er != nil {\n\t\treturn 0, er\n\t}\n\ts.requestRead(len(p))\n\treturn io.ReadFull(s.trReader, p)\n}\n\n// StreamWrite only used for unit test\nfunc StreamWrite(s *Stream, buffer *bytes.Buffer) {\n\ts.write(recvMsg{buffer: buffer})\n}\n\n// CreateStream only used for unit test. Create an independent stream out of http2client / http2server\nfunc CreateStream(id uint32, requestRead func(i int)) *Stream {\n\trecvBuffer := newRecvBuffer()\n\ttrReader := &transportReader{\n\t\treader: &recvBufferReader{\n\t\t\trecv: recvBuffer,\n\t\t\tfreeBuffer: func(buffer *bytes.Buffer) {\n\t\t\t\tbuffer.Reset()\n\t\t\t},\n\t\t},\n\t\twindowHandler: func(i int) {},\n\t}\n\n\tstream := &Stream{\n\t\tid:          id,\n\t\tbuf:         recvBuffer,\n\t\ttrReader:    trReader,\n\t\twq:          newWriteQuota(defaultWriteQuota, nil),\n\t\trequestRead: requestRead,\n\t\thdrMu:       sync.Mutex{},\n\t}\n\n\treturn stream\n}\n\n// transportReader reads all the data available for this Stream from the transport and\n// passes them into the decoder, which converts them into a gRPC message stream.\n// The error is io.EOF when the stream is done or another non-nil error if\n// the stream broke.\ntype transportReader struct {\n\treader io.Reader\n\t// The handler to control the window update procedure for both this\n\t// particular stream and the associated transport.\n\twindowHandler func(int)\n\ter            error\n}\n\nfunc (t *transportReader) Read(p []byte) (n int, err error) {\n\tn, err = t.reader.Read(p)\n\tif err != nil {\n\t\tt.er = err\n\t\treturn\n\t}\n\tt.windowHandler(n)\n\treturn\n}\n\n// BytesReceived indicates whether any bytes have been received on this stream.\nfunc (s *Stream) BytesReceived() bool {\n\treturn atomic.LoadUint32(&s.bytesReceived) == 1\n}\n\n// Unprocessed indicates whether the server did not process this stream --\n// i.e. it sent a refused stream or GOAWAY including this stream ID.\nfunc (s *Stream) Unprocessed() bool {\n\treturn atomic.LoadUint32(&s.unprocessed) == 1\n}\n\n// state of transport\ntype transportState int\n\nconst (\n\treachable transportState = iota\n\tclosing\n\tdraining\n)\n\n// ServerConfig consists of all the configurations to establish a server transport.\ntype ServerConfig struct {\n\tMaxStreams                 uint32\n\tKeepaliveParams            ServerKeepalive\n\tKeepaliveEnforcementPolicy EnforcementPolicy\n\tInitialWindowSize          uint32\n\tInitialConnWindowSize      uint32\n\tWriteBufferSize            uint32\n\tReadBufferSize             uint32\n\tMaxHeaderListSize          *uint32\n}\n\nfunc DefaultServerConfig() *ServerConfig {\n\treturn &ServerConfig{\n\t\tWriteBufferSize: defaultWriteBufferSize,\n\t\tReadBufferSize:  defaultReadBufferSize,\n\t}\n}\n\n// ConnectOptions covers all relevant options for communicating with the server.\ntype ConnectOptions struct {\n\t// KeepaliveParams stores the keepalive parameters.\n\tKeepaliveParams ClientKeepalive\n\t// InitialWindowSize sets the initial window size for a stream.\n\tInitialWindowSize uint32\n\t// InitialConnWindowSize sets the initial window size for a connection.\n\tInitialConnWindowSize uint32\n\t// WriteBufferSize sets the size of write buffer which in turn determines how much data can be batched before it's written on the wire.\n\tWriteBufferSize uint32\n\t// ReadBufferSize sets the size of read buffer, which in turn determines how much data can be read at most for one read syscall.\n\tReadBufferSize uint32\n\t// MaxHeaderListSize sets the max (uncompressed) size of header list that is prepared to be received.\n\tMaxHeaderListSize *uint32\n\t// ShortConn indicates whether the connection will be reused from grpc conn pool\n\tShortConn bool\n\t// TLSConfig\n\tTLSConfig *tls.Config\n}\n\n// NewServerTransport creates a ServerTransport with conn or non-nil error\n// if it fails.\nfunc NewServerTransport(ctx context.Context, conn net.Conn, cfg *ServerConfig) (ServerTransport, error) {\n\treturn newHTTP2Server(ctx, conn, cfg)\n}\n\n// NewClientTransport establishes the transport with the required ConnectOptions\n// and returns it to the caller.\nfunc NewClientTransport(ctx context.Context, conn net.Conn, opts ConnectOptions,\n\tremoteService string, onGoAway func(GoAwayReason), onClose func(),\n) (ClientTransport, error) {\n\treturn newHTTP2Client(ctx, conn, opts, remoteService, onGoAway, onClose)\n}\n\n// Options provides additional hints and information for message\n// transmission.\ntype Options struct {\n\t// Last indicates whether this write is the last piece for\n\t// this stream.\n\tLast bool\n}\n\n// CallHdr carries the information of a particular RPC.\ntype CallHdr struct {\n\t// Host specifies the peer's host.\n\tHost string\n\n\t// Method specifies the operation to perform.\n\tMethod string\n\n\t// SendCompress specifies the compression algorithm applied on\n\t// outbound message.\n\tSendCompress string\n\n\t// ContentSubtype specifies the content-subtype for a request. For example, a\n\t// content-subtype of \"proto\" will result in a content-type of\n\t// \"application/grpc+proto\". The value of ContentSubtype must be all\n\t// lowercase, otherwise the behavior is undefined. See\n\t// https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\n\t// for more details.\n\tContentSubtype string\n\n\tPreviousAttempts int // value of grpc-previous-rpc-attempts header to set\n}\n\n// IsActive is the interface that exposing the underlying connection's active status.\ntype IsActive interface {\n\tIsActive() bool\n}\n\n// ClientTransport is the common interface for all gRPC client-side transport\n// implementations.\ntype ClientTransport interface {\n\t// Close tears down this transport. Once it returns, the transport\n\t// should not be accessed any more. The caller must make sure this\n\t// is called only once.\n\tClose() error\n\n\t// GracefulClose starts to tear down\n```\n# Function Signature: func TLSConfig(tlsConfig *tls.Config) *tls.Config {\n# Natural Language Annotations: // TLSConfig checks and supplement the tls config provided by user.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc TLSConfig(tlsConfig *tls.Config) *tls.Config {\n\tif tlsConfig == nil {\n\t\ttlsConfig = &tls.Config{}\n\t}\n\tif tlsConfig.MinVersion == 0 {\n\t\ttlsConfig.MinVersion = tls.VersionTLS10\n\t}\n\tif tlsConfig.MaxVersion == 0 {\n\t\ttlsConfig.MaxVersion = tls.VersionTLS13\n\t}\n\tif tlsConfig.CipherSuites == nil {\n\t\ttlsConfig.CipherSuites = []uint16{\n\t\t\ttls.TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,\n\t\t\ttls.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,\n\t\t\ttls.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,\n\t\t\ttls.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,\n\t\t\ttls.TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,\n\t\t\ttls.TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,\n\t\t}\n\t}\n\treturn tlsConfig\n}\n```", "```go\nfunc TLSConfig(tlsConfig *tls.Config) *tls.Config {\n\tif tlsConfig == nil {\n\t\ttlsConfig = &tls.Config{}\n\t}\n\tif len(tlsConfig.NextProtos) == 0 {\n\t\ttlsConfig.NextProtos = []string{\"h2\"}\n\t}\n\treturn tlsConfig\n}\n```", "```go\nfunc TLSConfig(tlsConfig *tls.Config) *tls.Config {\n\tif tlsConfig == nil {\n\t\ttlsConfig = &tls.Config{}\n\t}\n\n\tif tlsConfig.MinVersion == 0 {\n\t\ttlsConfig.MinVersion = tls.VersionTLS13\n\t}\n\n\tif tlsConfig.CurvePreferences == nil {\n\t\ttlsConfig.CurvePreferences = []tls.CurveID{tls.CurveP521, tls.CurveP384, tls.CurveP256}\n\t}\n\n\tif tlsConfig.CipherSuites == nil {\n\t\ttlsConfig.CipherSuites = []uint16{\n\t\t\ttls.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,\n\t\t\ttls.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,\n\t\t\ttls.TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,\n\t\t\ttls.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,\n\t\t}\n\t}\n\n\treturn tlsConfig\n}\n```"]}, "kitex-pkg/remote/trans/nphttp2/grpc/http_util.go-encodeTimeout": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/nphttp2/grpc/http_util.go:\n```\n/*\n *\n * Copyright 2014 gRPC authors.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage grpc\n\nimport (\n\t\"bytes\"\n\t\"encoding/base64\"\n\t\"fmt\"\n\t\"math\"\n\t\"net/http\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\t\"unicode/utf8\"\n\n\t\"golang.org/x/net/http2\"\n\t\"golang.org/x/net/http2/hpack\"\n\tspb \"google.golang.org/genproto/googleapis/rpc/status\"\n\t\"google.golang.org/protobuf/proto\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/codes\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc/grpcframe\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/status\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\nconst (\n\t// http2MaxFrameLen specifies the max length of a HTTP2 frame.\n\thttp2MaxFrameLen = 16384 // 16KB frame\n\t// http://http2.github.io/http2-spec/#SettingValues\n\thttp2InitHeaderTableSize = 4096\n\t// baseContentType is the base content-type for gRPC.  This is a valid\n\t// content-type on it's own, but can also include a content-subtype such as\n\t// \"proto\" as a suffix after \"+\" or \";\".  See\n\t// https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\n\t// for more details.\n\tbaseContentType = \"application/grpc\"\n)\n\nvar (\n\t// ClientPreface http2 preface message\n\tClientPreface = []byte(http2.ClientPreface)\n\t// ClientPrefaceLen preface length\n\tClientPrefaceLen = len(ClientPreface)\n\thttp2ErrConvTab  = map[http2.ErrCode]codes.Code{\n\t\thttp2.ErrCodeNo:                 codes.Internal,\n\t\thttp2.ErrCodeProtocol:           codes.Internal,\n\t\thttp2.ErrCodeInternal:           codes.Internal,\n\t\thttp2.ErrCodeFlowControl:        codes.ResourceExhausted,\n\t\thttp2.ErrCodeSettingsTimeout:    codes.Internal,\n\t\thttp2.ErrCodeStreamClosed:       codes.Internal,\n\t\thttp2.ErrCodeFrameSize:          codes.Internal,\n\t\thttp2.ErrCodeRefusedStream:      codes.Unavailable,\n\t\thttp2.ErrCodeCancel:             codes.Canceled,\n\t\thttp2.ErrCodeCompression:        codes.Internal,\n\t\thttp2.ErrCodeConnect:            codes.Internal,\n\t\thttp2.ErrCodeEnhanceYourCalm:    codes.ResourceExhausted,\n\t\thttp2.ErrCodeInadequateSecurity: codes.PermissionDenied,\n\t\thttp2.ErrCodeHTTP11Required:     codes.Internal,\n\t}\n\tstatusCodeConvTab = map[codes.Code]http2.ErrCode{\n\t\tcodes.Internal:          http2.ErrCodeInternal,\n\t\tcodes.Canceled:          http2.ErrCodeCancel,\n\t\tcodes.Unavailable:       http2.ErrCodeRefusedStream,\n\t\tcodes.ResourceExhausted: http2.ErrCodeEnhanceYourCalm,\n\t\tcodes.PermissionDenied:  http2.ErrCodeInadequateSecurity,\n\t}\n\t// HTTPStatusConvTab is the HTTP status code to gRPC error code conversion table.\n\tHTTPStatusConvTab = map[int]codes.Code{\n\t\t// 400 Bad Request - INTERNAL.\n\t\thttp.StatusBadRequest: codes.Internal,\n\t\t// 401 Unauthorized  - UNAUTHENTICATED.\n\t\thttp.StatusUnauthorized: codes.Unauthenticated,\n\t\t// 403 Forbidden - PERMISSION_DENIED.\n\t\thttp.StatusForbidden: codes.PermissionDenied,\n\t\t// 404 Not Found - UNIMPLEMENTED.\n\t\thttp.StatusNotFound: codes.Unimplemented,\n\t\t// 429 Too Many Requests - UNAVAILABLE.\n\t\thttp.StatusTooManyRequests: codes.Unavailable,\n\t\t// 502 Bad Gateway - UNAVAILABLE.\n\t\thttp.StatusBadGateway: codes.Unavailable,\n\t\t// 503 Service Unavailable - UNAVAILABLE.\n\t\thttp.StatusServiceUnavailable: codes.Unavailable,\n\t\t// 504 Gateway timeout - UNAVAILABLE.\n\t\thttp.StatusGatewayTimeout: codes.Unavailable,\n\t}\n)\n\ntype parsedHeaderData struct {\n\tencoding       string\n\tacceptEncoding string\n\t// statusGen caches the stream status received from the trailer the server\n\t// sent.  Client side only.  Do not access directly.  After all trailers are\n\t// parsed, use the status method to retrieve the status.\n\tstatusGen    *status.Status\n\tbizStatusErr kerrors.BizStatusErrorIface\n\t// rawStatusCode and rawStatusMsg are set from the raw trailer fields and are not\n\t// intended for direct access outside of parsing.\n\trawStatusCode  *int\n\trawStatusMsg   string\n\tbizStatusCode  *int\n\tbizStatusExtra map[string]string\n\thttpStatus     *int\n\t// Server side only fields.\n\ttimeoutSet bool\n\ttimeout    time.Duration\n\tmethod     string\n\t// key-value metadata map from the peer.\n\tmdata          map[string][]string\n\tstatsTags      []byte\n\tstatsTrace     []byte\n\tcontentSubtype string\n\n\t// isGRPC field indicates whether the peer is speaking gRPC (otherwise HTTP).\n\t//\n\t// We are in gRPC mode (peer speaking gRPC) if:\n\t// \t* We are client side and have already received a HEADER frame that indicates gRPC peer.\n\t//  * The header contains valid  a content-type, i.e. a string starts with \"application/grpc\"\n\t// And we should handle error specific to gRPC.\n\t//\n\t// Otherwise (i.e. a content-type string starts without \"application/grpc\", or does not exist), we\n\t// are in HTTP fallback mode, and should handle error specific to HTTP.\n\tisGRPC         bool\n\tgrpcErr        error\n\thttpErr        error\n\tcontentTypeErr string\n}\n\n// decodeState configures decoding criteria and records the decoded data.\ntype decodeState struct {\n\t// whether decoding on server side or not\n\tserverSide bool\n\n\t// Records the states during HPACK decoding. It will be filled with info parsed from HTTP HEADERS\n\t// frame once decodeHeader function has been invoked and returned.\n\tdata parsedHeaderData\n}\n\n// isReservedHeader checks whether hdr belongs to HTTP2 headers\n// reserved by gRPC protocol. Any other headers are classified as the\n// user-specified metadata.\nfunc isReservedHeader(hdr string) bool {\n\tif hdr != \"\" && hdr[0] == ':' {\n\t\treturn true\n\t}\n\tswitch hdr {\n\tcase \"content-type\",\n\t\t\"user-agent\",\n\t\t\"grpc-message-type\",\n\t\t\"grpc-encoding\",\n\t\t\"grpc-message\",\n\t\t\"grpc-status\",\n\t\t\"grpc-timeout\",\n\t\t\"grpc-status-details-bin\",\n\t\t// Intentionally exclude grpc-previous-rpc-attempts and\n\t\t// grpc-retry-pushback-ms, which are \"reserved\", but their API\n\t\t// intentionally works via metadata.\n\t\t\"te\":\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// isWhitelistedHeader checks whether hdr should be propagated into metadata\n// visible to users, even though it is classified as \"reserved\", above.\nfunc isWhitelistedHeader(hdr string) bool {\n\tswitch hdr {\n\tcase \":authority\", \"user-agent\":\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// contentSubtype returns the content-subtype for the given content-type.  The\n// given content-type must be a valid content-type that starts with\n// \"application/grpc\". A content-subtype will follow \"application/grpc\" after a\n// \"+\" or \";\". See\n// https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests for\n// more details.\n//\n// If contentType is not a valid content-type for gRPC, the boolean\n// will be false, otherwise true. If content-type == \"application/grpc\",\n// \"application/grpc+\", or \"application/grpc;\", the boolean will be true,\n// but no content-subtype will be returned.\n//\n// contentType is assumed to be lowercase already.\nfunc contentSubtype(contentType string) (string, bool) {\n\tif contentType == baseContentType {\n\t\treturn \"\", true\n\t}\n\tif !strings.HasPrefix(contentType, baseContentType) {\n\t\treturn \"\", false\n\t}\n\t// guaranteed since != baseContentType and has baseContentType prefix\n\tswitch contentType[len(baseContentType)] {\n\tcase '+', ';':\n\t\t// this will return true for \"application/grpc+\" or \"application/grpc;\"\n\t\t// which the previous validContentType function tested to be valid, so we\n\t\t// just say that no content-subtype is specified in this case\n\t\treturn contentType[len(baseContentType)+1:], true\n\tdefault:\n\t\treturn \"\", false\n\t}\n}\n\n// contentSubtype is assumed to be lowercase\nfunc contentType(contentSubtype string) string {\n\tif contentSubtype == \"\" {\n\t\treturn baseContentType\n\t}\n\treturn baseContentType + \"+\" + contentSubtype\n}\n\nfunc (d *decodeState) status() *status.Status {\n\tif d.data.statusGen == nil {\n\t\t// No status-details were provided; generate status using code/msg.\n\t\td.data.statusGen = status.New(codes.Code(safeCastInt32(*(d.data.rawStatusCode))), d.data.rawStatusMsg)\n\t}\n\treturn d.data.statusGen\n}\n\nfunc (d *decodeState) bizStatusErr() kerrors.BizStatusErrorIface {\n\tif d.data.bizStatusErr == nil && d.data.bizStatusCode != nil {\n\t\td.data.bizStatusErr = kerrors.NewGRPCBizStatusErrorWithExtra(\n\t\t\tsafeCastInt32(*(d.data.bizStatusCode)), d.data.rawStatusMsg, d.data.bizStatusExtra)\n\t\tif st, ok := d.data.bizStatusErr.(kerrors.GRPCStatusIface); ok {\n\t\t\tst.SetGRPCStatus(d.status())\n\t\t}\n\t}\n\treturn d.data.bizStatusErr\n}\n\n// safeCastInt32 casts the number from int to int32 in safety.\nfunc safeCastInt32(n int) int32 {\n\tif n > math.MaxInt32 || n < math.MinInt32 {\n\t\tpanic(fmt.Sprintf(\"Cast int to int32 failed, due to overflow, n=%d\", n))\n\t}\n\treturn int32(n)\n}\n\nconst binHdrSuffix = \"-bin\"\n\nfunc encodeBinHeader(v []byte) string {\n\treturn base64.RawStdEncoding.EncodeToString(v)\n}\n\nfunc decodeBinHeader(v string) ([]byte, error) {\n\tif len(v)%4 == 0 {\n\t\t// Input was padded, or padding was not necessary.\n\t\treturn base64.StdEncoding.DecodeString(v)\n\t}\n\treturn base64.RawStdEncoding.DecodeString(v)\n}\n\nfunc encodeMetadataHeader(k, v string) string {\n\tif strings.HasSuffix(k, binHdrSuffix) {\n\t\treturn encodeBinHeader(([]byte)(v))\n\t}\n\treturn v\n}\n\nfunc decodeMetadataHeader(k, v string) (string, error) {\n\tif strings.HasSuffix(k, binHdrSuffix) {\n\t\tb, err := decodeBinHeader(v)\n\t\treturn string(b), err\n\t}\n\treturn v, nil\n}\n\nfunc (d *decodeState) decodeHeader(frame *grpcframe.MetaHeadersFrame) error {\n\t// frame.Truncated is set to true when framer detects that the current header\n\t// list size hits MaxHeaderListSize limit.\n\tif frame.Truncated {\n\t\treturn status.New(codes.Internal, \"peer header list size exceeded limit\").Err()\n\t}\n\n\tfor _, hf := range frame.Fields {\n\t\td.processHeaderField(hf)\n\t}\n\n\tif d.data.isGRPC {\n\t\tif d.data.grpcErr != nil {\n\t\t\treturn d.data.grpcErr\n\t\t}\n\t\tif d.serverSide {\n\t\t\treturn nil\n\t\t}\n\t\tif d.data.rawStatusCode == nil && d.data.statusGen == nil {\n\t\t\t// gRPC status doesn't exist.\n\t\t\t// Set rawStatusCode to be unknown and return nil error.\n\t\t\t// So that, if the stream has ended this Unknown status\n\t\t\t// will be propagated to the user.\n\t\t\t// Otherwise, it will be ignored. In which case, status from\n\t\t\t// a later trailer, that has StreamEnded flag set, is propagated.\n\t\t\tcode := int(codes.Unknown)\n\t\t\td.data.rawStatusCode = &code\n\t\t}\n\t\treturn nil\n\t}\n\n\t// HTTP fallback mode\n\tif d.data.httpErr != nil {\n\t\treturn d.data.httpErr\n\t}\n\n\tvar (\n\t\tcode = codes.Internal // when header does not include HTTP status, return INTERNAL\n\t\tok   bool\n\t)\n\n\tif d.data.httpStatus != nil {\n\t\tcode, ok = HTTPStatusConvTab[*(d.data.httpStatus)]\n\t\tif !ok {\n\t\t\tcode = codes.Unknown\n\t\t}\n\t}\n\n\treturn status.New(code, d.constructHTTPErrMsg()).Err()\n}\n\n// constructErrMsg constructs error message to be returned in HTTP fallback mode.\n// Format: HTTP status code and its corresponding message + content-type error message.\nfunc (d *decodeState) constructHTTPErrMsg() string {\n\tvar errMsgs []string\n\n\tif d.data.httpStatus == nil {\n\t\terrMsgs = append(errMsgs, \"malformed header: missing HTTP status\")\n\t} else {\n\t\terrMsgs = append(errMsgs, fmt.Sprintf(\"%s: HTTP status code %d\", http.StatusText(*(d.data.httpStatus)), *d.data.httpStatus))\n\t}\n\n\tif d.data.contentTypeErr == \"\" {\n\t\terrMsgs = append(errMsgs, \"transport: missing content-type field\")\n\t} else {\n\t\terrMsgs = append(errMsgs, d.data.contentTypeErr)\n\t}\n\n\treturn strings.Join(errMsgs, \"; \")\n}\n\nfunc (d *decodeState) addMetadata(k, v string) {\n\tif d.data.mdata == nil {\n\t\td.data.mdata = make(map[string][]string)\n\t}\n\td.data.mdata[k] = append(d.data.mdata[k], v)\n}\n\nfunc (d *decodeState) processHeaderField(f hpack.HeaderField) {\n\tswitch f.Name {\n\tcase \"content-type\":\n\t\tcontentSubtype, validContentType := contentSubtype(f.Value)\n\t\tif !validContentType {\n\t\t\td.data.contentTypeErr = fmt.Sprintf(\"transport: received the unexpected content-type %q\", f.Value)\n\t\t\treturn\n\t\t}\n\t\td.data.contentSubtype = contentSubtype\n\t\t// TODO: do we want to propagate the whole content-type in the metadata,\n\t\t// or come up with a way to just propagate the content-subtype if it was set?\n\t\t// ie {\"content-type\": \"application/grpc+proto\"} or {\"content-subtype\": \"proto\"}\n\t\t// in the metadata?\n\t\td.addMetadata(f.Name, f.Value)\n\t\td.data.isGRPC = true\n\tcase \"grpc-encoding\":\n\t\td.data.encoding = f.Value\n\tcase \"grpc-accept-encoding\":\n\t\td.data.acceptEncoding = f.Value\n\tcase \"grpc-status\":\n\t\tcode, err := strconv.Atoi(f.Value)\n\t\tif err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed grpc-status: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.rawStatusCode = &code\n\tcase \"grpc-message\":\n\t\td.data.rawStatusMsg = decodeGrpcMessage(f.Value)\n\tcase \"biz-status\":\n\t\tcode, err := strconv.Atoi(f.Value)\n\t\tif err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed biz-status: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.bizStatusCode = &code\n\tcase \"biz-extra\":\n\t\textra, err := utils.JSONStr2Map(f.Value)\n\t\tif err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed biz-extra: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.bizStatusExtra = extra\n\tcase \"grpc-status-details-bin\":\n\t\tv, err := decodeBinHeader(f.Value)\n\t\tif err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed grpc-status-details-bin: %v\", err)\n\t\t\treturn\n\t\t}\n\t\ts := &spb.Status{}\n\t\tif err := proto.Unmarshal(v, s); err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed grpc-status-details-bin: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.statusGen = status.FromProto(s)\n\tcase \"grpc-timeout\":\n\t\td.data.timeoutSet = true\n\t\tvar err error\n\t\tif d.data.timeout, err = decodeTimeout(f.Value); err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed time-out: %v\", err)\n\t\t}\n\tcase \":path\":\n\t\td.data.method = f.Value\n\tcase \":status\":\n\t\tcode, err := strconv.Atoi(f.Value)\n\t\tif err != nil {\n\t\t\td.data.httpErr = status.Errorf(codes.Internal, \"transport: malformed http-status: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.httpStatus = &code\n\tcase \"grpc-tags-bin\":\n\t\tv, err := decodeBinHeader(f.Value)\n\t\tif err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed grpc-tags-bin: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.statsTags = v\n\t\td.addMetadata(f.Name, string(v))\n\tcase \"grpc-trace-bin\":\n\t\tv, err := decodeBinHeader(f.Value)\n\t\tif err != nil {\n\t\t\td.data.grpcErr = status.Errorf(codes.Internal, \"transport: malformed grpc-trace-bin: %v\", err)\n\t\t\treturn\n\t\t}\n\t\td.data.statsTrace = v\n\t\td.addMetadata(f.Name, string(v))\n\tdefault:\n\t\tif isReservedHeader(f.Name) && !isWhitelistedHeader(f.Name) {\n\t\t\tbreak\n\t\t}\n\t\tv, err := decodeMetadataHeader(f.Name, f.Value)\n\t\tif err != nil {\n\t\t\tklog.Errorf(\"Failed to decode metadata header (%q, %q): %v\", f.Name, f.Value, err)\n\t\t\treturn\n\t\t}\n\t\td.addMetadata(f.Name, v)\n\t}\n}\n\ntype timeoutUnit uint8\n\nconst (\n\thour        timeoutUnit = 'H'\n\tminute      timeoutUnit = 'M'\n\tsecond      timeoutUnit = 'S'\n\tmillisecond timeoutUnit = 'm'\n\tmicrosecond timeoutUnit = 'u'\n\tnanosecond  timeoutUnit = 'n'\n)\n\nfunc timeoutUnitToDuration(u timeoutUnit) (d time.Duration, ok bool) {\n\tswitch u {\n\tcase hour:\n\t\treturn time.Hour, true\n\tcase minute:\n\t\treturn time.Minute, true\n\tcase second:\n\t\treturn time.Second, true\n\tcase millisecond:\n\t\treturn time.Millisecond, true\n\tcase microsecond:\n\t\treturn time.Microsecond, true\n\tcase nanosecond:\n\t\treturn time.Nanosecond, true\n\tdefault:\n\t}\n\treturn\n}\n\nconst maxTimeoutValue int64 = 100000000 - 1\n\n// div does integer division and round-up the result. Note that this is\n// equivalent to (d+r-1)/r but has less chance to overflow.\nfunc div(d, r time.Duration) int64 {\n\tif m := d % r; m > 0 {\n\t\treturn int64(d/r + 1)\n\t}\n\treturn int64(d / r)\n}\n\n// TODO(zhaoq): It is the simplistic and not bandwidth efficient. Improve it.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc decodeTimeout(s string) (time.Duration, error) {\n\tsize := len(s)\n\tif size < 2 {\n\t\treturn 0, fmt.Errorf(\"transport: timeout string is too short: %q\", s)\n\t}\n\tif size > 9 {\n\t\t// Spec allows for 8 digits plus the unit.\n\t\treturn 0, fmt.Errorf(\"transport: timeout string is too long: %q\", s)\n\t}\n\tunit := timeoutUnit(s[size-1])\n\td, ok := timeoutUnitToDuration(unit)\n\tif !ok {\n\t\treturn 0, fmt.Errorf(\"transport: timeout unit is not recognized: %q\", s)\n\t}\n\tt, err := strconv.ParseInt(s[:size-1], 10, 64)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tconst maxHours = math.MaxInt64 / int64(time.Hour)\n\tif d == time.Hour && t > maxHours {\n\t\t// This timeout would overflow math.MaxInt64; clamp it.\n\t\treturn time.Duration(math.MaxInt64), nil\n\t}\n\treturn d * time.Duration(t), nil\n}\n\nconst (\n\tspaceByte   = ' '\n\ttildeByte   = '~'\n\tpercentByte = '%'\n)\n\n// encodeGrpcMessage is used to encode status code in header field\n// \"grpc-message\". It does percent encoding and also replaces invalid utf-8\n// characters with Unicode replacement character.\n//\n// It checks to see if each individual byte in msg is an allowable byte, and\n// then either percent encoding or passing it through. When percent encoding,\n// the byte is converted into hexadecimal notation with a '%' prepended.\nfunc encodeGrpcMessage(msg string) string {\n\tif msg == \"\" {\n\t\treturn \"\"\n\t}\n\tlenMsg := len(msg)\n\tfor i := 0; i < lenMsg; i++ {\n\t\tc := msg[i]\n\t\tif !(c >= spaceByte && c <= tildeByte && c != percentByte) {\n\t\t\treturn encodeGrpcMessageUnchecked(msg)\n\t\t}\n\t}\n\treturn msg\n}\n\nfunc encodeGrpcMessageUnchecked(msg string) string {\n\tvar buf bytes.Buffer\n\tfor len(msg) > 0 {\n\t\tr, size := utf8.DecodeRuneInString(msg)\n\t\tfor _, b := range []byte(string(r)) {\n\t\t\tif size > 1 {\n\t\t\t\t// If size > 1, r is not ascii. Always do percent encoding.\n\t\t\t\tbuf.WriteString(fmt.Sprintf(\"%%%02X\", b))\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// The for loop is necessary even if size == 1. r could be\n\t\t\t// utf8.RuneError.\n\t\t\t//\n\t\t\t// fmt.Sprintf(\"%%%02X\", utf8.RuneError) gives \"%FFFD\".\n\t\t\tif b >= spaceByte && b <= tildeByte && b != percentByte {\n\t\t\t\tbuf.WriteByte(b)\n\t\t\t} else {\n\t\t\t\tbuf.WriteString(fmt.Sprintf(\"%%%02X\", b))\n\t\t\t}\n\t\t}\n\t\tmsg = msg[size:]\n\t}\n\treturn buf.String()\n}\n\n// decodeGrpcMessage decodes the msg encoded by encodeGrpcMessage.\nfunc decodeGrpcMessage(msg string) string {\n\tif msg == \"\" {\n\t\treturn \"\"\n\t}\n\tlenMsg := len(msg)\n\tfor i := 0; i < lenMsg; i++ {\n\t\tif msg[i] == percentByte && i+2 < lenMsg {\n\t\t\treturn decodeGrpcMessageUnchecked(msg)\n\t\t}\n\t}\n\treturn msg\n}\n\nfunc decodeGrpcMessageUnchecked(msg string) string {\n\tvar buf bytes.Buffer\n\tlenMsg := len(msg)\n\tfor i := 0; i < lenMsg\n```\n# Function Signature: func encodeTimeout(t time.Duration) string {\n# Natural Language Annotations: // TODO(zhaoq): It is the simplistic and not bandwidth efficient. Improve it.\nPlease return the generated code surrounded by ```\n", "response": ["Here is the generated code for the function `encodeTimeout`:\n\n```go\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"time\"\n)\n\nfunc encodeTimeout(t time.Duration) string {\n\tif t == 0 {\n\t\treturn \"0S\"\n\t}\n\n\tvar result string\n\tif t >= time.Hour {\n\t\thours := t / time.Hour\n\t\tt -= hours * time.Hour\n\t\tresult += fmt.Sprintf(\"%dH\", hours)\n\t}\n\tif t >= time.Minute {\n\t\tminutes := t / time.Minute\n\t\tt -= minutes * time.Minute\n\t\tresult += fmt.Sprintf(\"%dM\", minutes)\n\t}\n\tif t >= time.Second {\n\t\tseconds := t / time.Second\n\t\tt -= seconds * time.Second\n\t\tresult += fmt.Sprintf(\"%dS\", seconds)\n\t}\n\tif t >= time.Millisecond {\n\t\tmilliseconds := t / time.Millisecond\n\t\tt -= milliseconds * time.Millisecond\n\t\tresult += fmt.Sprintf(\"%dM\", milliseconds)\n\t}\n\tif t >= time.Microsecond {\n\t\tmicroseconds := t / time.Microsecond\n\t\tt -= microseconds * time.Microsecond\n\t\tresult += fmt.Sprintf(\"%dU\", microseconds)\n\t}\n\tif t >= time.Nanosecond {\n\t\tnanoseconds := t / time.Nanosecond\n\t\tresult += fmt.Sprintf(\"%dN\", nanoseconds)\n\t}\n\treturn result\n}\n```\n\nThis function converts a time duration into a string representation that represents the duration in terms of hours (H), minutes (M), seconds (S), milliseconds (m), microseconds (u), and nanoseconds (n).", "```go\nfunc encodeTimeout(t time.Duration) string {\n\tif t == 0 {\n\t\treturn \"\"\n\t}\n\n\tvar buf bytes.Buffer\n\tif t >= time.Hour {\n\t\th := t / time.Hour\n\t\tt %= time.Hour\n\t\tbuf.WriteString(fmt.Sprintf(\"%dH\", h))\n\t}\n\tif t >= time.Minute {\n\t\tm := t / time.Minute\n\t\tt %= time.Minute\n\t\tbuf.WriteString(fmt.Sprintf(\"%dM\", m))\n\t}\n\tif t >= time.Second {\n\t\ts := t / time.Second\n\t\tt %= time.Second\n\t\tbuf.WriteString(fmt.Sprintf(\"%dS\", s))\n\t}\n\tif t >= time.Millisecond {\n\t\tms := t / time.Millisecond\n\t\tt %= time.Millisecond\n\t\tbuf.WriteString(fmt.Sprintf(\"%dm\", ms))\n\t}\n\tif t >= time.Microsecond {\n\t\tus := t / time.Microsecond\n\t\tt %= time.Microsecond\n\t\tbuf.WriteString(fmt.Sprintf(\"%du\", us))\n\t}\n\tif t >= time.Nanosecond {\n\t\tns := t / time.Nanosecond\n\t\tbuf.WriteString(fmt.Sprintf(\"%dn\", ns))\n\t}\n\n\treturn buf.String()\n}\n```", "```go\nfunc encodeTimeout(t time.Duration) string {\n\tconst maxHours = math.MaxInt64 / int64(time.Hour)\n\tif t > maxHours {\n\t\t// This timeout would overflow math.MaxInt64; clamp it.\n\t\tt = time.Duration(math.MaxInt64)\n\t}\n\tunits := []time.Duration{time.Hour, time.Minute, time.Second, time.Millisecond, time.Microsecond, time.Nanosecond}\n\tnames := []string{\"H\", \"M\", \"S\", \"m\", \"u\", \"n\"}\n\tvar u time.Duration\n\tvar name string\n\tfor i, unit := range units {\n\t\tif t%unit == 0 {\n\t\t\tu = unit\n\t\t\tname = names[i]\n\t\t\tbreak\n\t\t}\n\t}\n\tval := t / u\n\tval = ((val + u - 1) / u)\n\treturn strconv.FormatInt(int64(val), 10) + name\n}\n```"]}, "kitex-pkg/remote/trans/netpoll/bytebuf.go-AppendBuffer": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/trans/netpoll/bytebuf.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage netpoll\n\nimport (\n\t\"errors\"\n\t\"io\"\n\t\"sync\"\n\n\t\"github.com/cloudwego/netpoll\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n)\n\nvar bytebufPool sync.Pool\n\nfunc init() {\n\tbytebufPool.New = newNetpollByteBuffer\n}\n\n// NewReaderByteBuffer creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReader.\nfunc NewReaderByteBuffer(r netpoll.Reader) remote.ByteBuffer {\n\tbytebuf := bytebufPool.Get().(*netpollByteBuffer)\n\tbytebuf.reader = r\n\t// TODO(wangtieju): fix me when netpoll support netpoll.Reader\n\t// and LinkBuffer not support io.Reader, type assertion would fail when r is from NewBuffer\n\tif ir, ok := r.(io.Reader); ok {\n\t\tbytebuf.ioReader = ir\n\t}\n\tbytebuf.status = remote.BitReadable\n\tbytebuf.readSize = 0\n\treturn bytebuf\n}\n\n// NewWriterByteBuffer creates a new remote.ByteBuffer using the given netpoll.ZeroCopyWriter.\nfunc NewWriterByteBuffer(w netpoll.Writer) remote.ByteBuffer {\n\tbytebuf := bytebufPool.Get().(*netpollByteBuffer)\n\tbytebuf.writer = w\n\t// TODO(wangtieju): fix me when netpoll support netpoll.Writer\n\t// and LinkBuffer not support io.Reader, type assertion would fail when w is from NewBuffer\n\tif iw, ok := w.(io.Writer); ok {\n\t\tbytebuf.ioWriter = iw\n\t}\n\tbytebuf.status = remote.BitWritable\n\treturn bytebuf\n}\n\n// NewReaderWriterByteBuffer creates a new remote.ByteBuffer using the given netpoll.ZeroCopyReadWriter.\nfunc NewReaderWriterByteBuffer(rw netpoll.ReadWriter) remote.ByteBuffer {\n\tbytebuf := bytebufPool.Get().(*netpollByteBuffer)\n\tbytebuf.writer = rw\n\tbytebuf.reader = rw\n\t// TODO(wangtieju): fix me when netpoll support netpoll.ReadWriter\n\t// and LinkBuffer not support io.ReadWriter, type assertion would fail when rw is from NewBuffer\n\tif irw, ok := rw.(io.ReadWriter); ok {\n\t\tbytebuf.ioReader = irw\n\t\tbytebuf.ioWriter = irw\n\t}\n\tbytebuf.status = remote.BitWritable | remote.BitReadable\n\treturn bytebuf\n}\n\nfunc newNetpollByteBuffer() interface{} {\n\treturn &netpollByteBuffer{}\n}\n\ntype netpollByteBuffer struct {\n\twriter   netpoll.Writer\n\treader   netpoll.Reader\n\tioReader io.Reader\n\tioWriter io.Writer\n\tstatus   int\n\treadSize int\n}\n\nvar _ remote.ByteBuffer = &netpollByteBuffer{}\n\n// Next reads n bytes sequentially, returns the original address.\nfunc (b *netpollByteBuffer) Next(n int) (p []byte, err error) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn nil, errors.New(\"unreadable buffer, cannot support Next\")\n\t}\n\tif p, err = b.reader.Next(n); err == nil {\n\t\tb.readSize += n\n\t}\n\treturn\n}\n\n// Peek returns the next n bytes without advancing the reader.\nfunc (b *netpollByteBuffer) Peek(n int) (buf []byte, err error) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn nil, errors.New(\"unreadable buffer, cannot support Peek\")\n\t}\n\treturn b.reader.Peek(n)\n}\n\n// Skip is used to skip n bytes, it's much faster than Next.\n// Skip will not cause release.\nfunc (b *netpollByteBuffer) Skip(n int) (err error) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn errors.New(\"unreadable buffer, cannot support Skip\")\n\t}\n\treturn b.reader.Skip(n)\n}\n\n// ReadableLen returns the total length of readable buffer.\nfunc (b *netpollByteBuffer) ReadableLen() (n int) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn -1\n\t}\n\treturn b.reader.Len()\n}\n\n// Read implement io.Reader\nfunc (b *netpollByteBuffer) Read(p []byte) (n int, err error) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn -1, errors.New(\"unreadable buffer, cannot support Read\")\n\t}\n\tif b.ioReader != nil {\n\t\treturn b.ioReader.Read(p)\n\t}\n\treturn -1, errors.New(\"ioReader is nil\")\n}\n\n// ReadString is a more efficient way to read string than Next.\nfunc (b *netpollByteBuffer) ReadString(n int) (s string, err error) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn \"\", errors.New(\"unreadable buffer, cannot support ReadString\")\n\t}\n\tif s, err = b.reader.ReadString(n); err == nil {\n\t\tb.readSize += n\n\t}\n\treturn\n}\n\n// ReadBinary like ReadString.\n// Returns a copy of original buffer.\nfunc (b *netpollByteBuffer) ReadBinary(n int) (p []byte, err error) {\n\tif b.status&remote.BitReadable == 0 {\n\t\treturn p, errors.New(\"unreadable buffer, cannot support ReadBinary\")\n\t}\n\tif p, err = b.reader.ReadBinary(n); err == nil {\n\t\tb.readSize += n\n\t}\n\treturn\n}\n\n// Malloc n bytes sequentially in the writer buffer.\nfunc (b *netpollByteBuffer) Malloc(n int) (buf []byte, err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn nil, errors.New(\"unwritable buffer, cannot support Malloc\")\n\t}\n\treturn b.writer.Malloc(n)\n}\n\n// MallocAck n bytes in the writer buffer.\nfunc (b *netpollByteBuffer) MallocAck(n int) (err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn errors.New(\"unwritable buffer, cannot support MallocAck\")\n\t}\n\treturn b.writer.MallocAck(n)\n}\n\n// MallocLen returns the total length of the buffer malloced.\nfunc (b *netpollByteBuffer) MallocLen() (length int) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn -1\n\t}\n\treturn b.writer.MallocLen()\n}\n\n// Write implement io.Writer\nfunc (b *netpollByteBuffer) Write(p []byte) (n int, err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support Write\")\n\t}\n\tif b.ioWriter != nil {\n\t\treturn b.ioWriter.Write(p)\n\t}\n\treturn -1, errors.New(\"ioWriter is nil\")\n}\n\n// WriteString is a more efficient way to write string, using the unsafe method to convert the string to []byte.\nfunc (b *netpollByteBuffer) WriteString(s string) (n int, err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support WriteString\")\n\t}\n\treturn b.writer.WriteString(s)\n}\n\n// WriteBinary writes the []byte directly. Callers must guarantee that the []byte doesn't change.\nfunc (b *netpollByteBuffer) WriteBinary(p []byte) (n int, err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn -1, errors.New(\"unwritable buffer, cannot support WriteBinary\")\n\t}\n\treturn b.writer.WriteBinary(p)\n}\n\n// WriteDirect is a way to write []byte without copying, and splits the original buffer.\nfunc (b *netpollByteBuffer) WriteDirect(p []byte, remainCap int) error {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn errors.New(\"unwritable buffer, cannot support WriteBinary\")\n\t}\n\treturn b.writer.WriteDirect(p, remainCap)\n}\n\n// ReadLen returns the size already read.\nfunc (b *netpollByteBuffer) ReadLen() (n int) {\n\treturn b.readSize\n}\n\n// Flush writes any malloc data to the underlying io.Writer.\n// The malloced buffer must be set correctly.\nfunc (b *netpollByteBuffer) Flush() (err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn errors.New(\"unwritable buffer, cannot support Flush\")\n\t}\n\treturn b.writer.Flush()\n}\n\n// NewBuffer returns a new writable remote.ByteBuffer.\nfunc (b *netpollByteBuffer) NewBuffer() remote.ByteBuffer {\n\treturn NewWriterByteBuffer(netpoll.NewLinkBuffer())\n}\n\n// AppendBuffer appends buf to the original buffer.\n\n\n\n\n\n\n\n// Bytes are not supported in netpoll bytebuf.\nfunc (b *netpollByteBuffer) Bytes() (buf []byte, err error) {\n\tif b.reader != nil {\n\t\treturn b.reader.Peek(b.reader.Len())\n\t}\n\treturn nil, errors.New(\"method Bytes() not support in netpoll bytebuf\")\n}\n\n// Release will free the buffer already read.\n// After release, buffer read by Next/Skip/Peek is invalid.\nfunc (b *netpollByteBuffer) Release(e error) (err error) {\n\tif b.reader != nil {\n\t\t// \u91cd\u590d\u6267\u884cRelease nil panic\n\t\terr = b.reader.Release()\n\t}\n\tb.zero()\n\tbytebufPool.Put(b)\n\treturn\n}\n\nfunc (b *netpollByteBuffer) zero() {\n\tb.writer = nil\n\tb.reader = nil\n\tb.ioReader = nil\n\tb.ioWriter = nil\n\tb.status = 0\n\tb.readSize = 0\n}\n\n```\n# Function Signature: func (b *netpollByteBuffer) AppendBuffer(buf remote.ByteBuffer) (err error) {\n# Natural Language Annotations: // AppendBuffer appends buf to the original buffer.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (b *netpollByteBuffer) AppendBuffer(buf remote.ByteBuffer) (err error) {\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn errors.New(\"unwritable buffer, cannot support AppendBuffer\")\n\t}\n\n\tif netpollBuf, ok := buf.(*netpollByteBuffer); ok {\n\t\t// For simplicity, we assume netpollBuf.writer is a netpoll.Writer\n\t\tif err := netpollBuf.Flush(); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif _, err := b.Write(netpollBuf.writer.Bytes()); err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\treturn errors.New(\"unsupported buffer type, AppendBuffer only supports netpollByteBuffer\")\n\t}\n\n\treturn nil\n}\n```", "```go\nfunc (b *netpollByteBuffer) AppendBuffer(buf remote.ByteBuffer) (err error) {\n\toriginalBuf, ok := buf.(*netpollByteBuffer)\n\tif !ok {\n\t\treturn errors.New(\"input buf is not a netpollByteBuffer\")\n\t}\n\n\tif originalBuf.status&remote.BitReadable == 0 {\n\t\treturn errors.New(\"input buf is not readable\")\n\t}\n\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn errors.New(\"original buf is not writable\")\n\t}\n\n\t// Get the length of the original buffer\n\toriginalBufLen := originalBuf.ReadableLen()\n\n\t// Malloc the space in the original buffer to store the appended buffer\n\tappendedBuf, err := b.Malloc(originalBufLen)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Copy the appended buffer to the malloced space\n\t_, err = originalBuf.Read(appendedBuf)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Release the appended buffer\n\terr = originalBuf.Release(nil)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Flush the original buffer\n\terr = b.Flush()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n```", "```go\nfunc (b *netpollByteBuffer) AppendBuffer(buf remote.ByteBuffer) (err error) {\n\t// Check if the original buffer is writable\n\tif b.status&remote.BitWritable == 0 {\n\t\treturn errors.New(\"unwritable buffer, cannot support AppendBuffer\")\n\t}\n\n\t// Check if the buffer to be appended is readable\n\tif buf.Status()&remote.BitReadable == 0 {\n\t\treturn errors.New(\"unreadable buffer, cannot support AppendBuffer\")\n\t}\n\n\t// Check if the buffer to be appended is a netpollByteBuffer\n\tnbb, ok := buf.(*netpollByteBuffer)\n\tif !ok {\n\t\treturn errors.New(\"unsupported buffer type, cannot support AppendBuffer\")\n\t}\n\n\t// Check if the reader in the buffer to be appended is not nil\n\tif nbb.reader == nil {\n\t\treturn errors.New(\"unreadable buffer, cannot support AppendBuffer\")\n\t}\n\n\t// Get the available length of the buffer to be appended\n\tn := nbb.reader.Len()\n\n\t// Malloc n bytes in the writer buffer\n\t_, err = b.writer.Malloc(n)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Write the buffer to be appended to the writer buffer\n\t_, err = b.writer.Write(nbb.reader.Peek(n))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Release the buffer to be appended\n\terr = nbb.reader.Release()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n```\nThis function appends a `remote.ByteBuffer` to the original `netpollByteBuffer`. It first checks if the original buffer and the buffer to be appended are writable and readable. Then it malloc the size of the buffer to be appended in the writer buffer, write the buffer to be appended to the writer buffer, and release the buffer to be appended."]}, "kitex-pkg/remote/codec/util.go-SetOrCheckMethodName": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/util.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage codec\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\nconst (\n\t// FrontMask is used in protocol sniffing.\n\tFrontMask = 0x0000ffff\n)\n\n// SetOrCheckMethodName is used to set method name to invocation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// SetOrCheckSeqID is used to check the sequence ID.\nfunc SetOrCheckSeqID(seqID int32, message remote.Message) error {\n\tswitch message.MessageType() {\n\tcase remote.Call, remote.Oneway:\n\t\tif ink, ok := message.RPCInfo().Invocation().(rpcinfo.InvocationSetter); ok {\n\t\t\tink.SetSeqID(seqID)\n\t\t} else {\n\t\t\treturn errors.New(\"the interface Invocation doesn't implement InvocationSetter\")\n\t\t}\n\tcase remote.Reply:\n\t\texpectSeqID := message.RPCInfo().Invocation().SeqID()\n\t\tif expectSeqID != seqID {\n\t\t\tmethodName := message.RPCInfo().Invocation().MethodName()\n\t\t\treturn remote.NewTransErrorWithMsg(remote.BadSequenceID, fmt.Sprintf(\"method[%s] out of order sequence response, expect[%d], receive[%d]\", methodName, expectSeqID, seqID))\n\t\t}\n\tcase remote.Exception:\n\t\t// don't check, proxy may build Exception with seqID = 0\n\t\t// thrift 0.13 check seqID for Exception but thrift 0.9.2 doesn't check\n\t}\n\treturn nil\n}\n\n// UpdateMsgType updates msg type.\nfunc UpdateMsgType(msgType uint32, message remote.Message) error {\n\trpcRole := message.RPCRole()\n\tmt := remote.MessageType(msgType)\n\tif mt == message.MessageType() {\n\t\treturn nil\n\t}\n\tif rpcRole == remote.Server {\n\t\tif mt != remote.Call && mt != remote.Oneway && mt != remote.Stream {\n\t\t\treturn remote.NewTransErrorWithMsg(remote.InvalidMessageTypeException, fmt.Sprintf(\"server side, invalid message type %d\", mt))\n\t\t}\n\t} else {\n\t\tif mt != remote.Reply && mt != remote.Exception && mt != remote.Stream {\n\t\t\treturn remote.NewTransErrorWithMsg(remote.InvalidMessageTypeException, fmt.Sprintf(\"client side, invalid message type %d\", mt))\n\t\t}\n\t}\n\n\tmessage.SetMessageType(mt)\n\treturn nil\n}\n\n// NewDataIfNeeded is used to create the data if not exist.\nfunc NewDataIfNeeded(method string, message remote.Message) error {\n\tif message.Data() != nil {\n\t\treturn nil\n\t}\n\tif message.NewData(method) {\n\t\treturn nil\n\t}\n\treturn remote.NewTransErrorWithMsg(remote.InternalError, \"message data for codec is nil\")\n}\n\n```\n# Function Signature: func SetOrCheckMethodName(methodName string, message remote.Message) error {\n# Natural Language Annotations: // SetOrCheckMethodName is used to set method name to invocation.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// SetOrCheckMethodName is used to set method name to invocation.\nfunc SetOrCheckMethodName(methodName string, message remote.Message) error {\n\t// Check if the interface Invocation implements the InvocationSetter interface\n\tif ink, ok := message.RPCInfo().Invocation().(rpcinfo.InvocationSetter); ok {\n\t\tink.SetMethodName(methodName)\n\t} else {\n\t\treturn errors.New(\"the interface Invocation doesn't implement InvocationSetter\")\n\t}\n\treturn nil\n}\n```", "```go\n// SetOrCheckMethodName is used to set method name to invocation.\nfunc SetOrCheckMethodName(methodName string, message remote.Message) error {\n\tinvocation := message.RPCInfo().Invocation()\n\tinvocationSetter, ok := invocation.(rpcinfo.InvocationSetter)\n\tif !ok {\n\t\treturn errors.New(\"the interface Invocation doesn't implement InvocationSetter\")\n\t}\n\tinvocationSetter.SetMethodName(methodName)\n\treturn nil\n}\n```", "```go\n// SetOrCheckMethodName is used to set method name to invocation.\nfunc SetOrCheckMethodName(methodName string, message remote.Message) error {\n\tif ink, ok := message.RPCInfo().Invocation().(rpcinfo.InvocationSetter); ok {\n\t\tink.SetMethodName(methodName)\n\t} else {\n\t\treturn errors.New(\"the interface Invocation doesn't implement InvocationSetter\")\n\t}\n\treturn nil\n}\n```"]}, "kitex-pkg/remote/codec/default_codec.go-Decode": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/default_codec.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage codec\n\nimport (\n\t\"context\"\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"sync/atomic\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// The byte count of 32 and 16 integer values.\nconst (\n\tSize32 = 4\n\tSize16 = 2\n)\n\nconst (\n\t// ThriftV1Magic is the magic code for thrift.VERSION_1\n\tThriftV1Magic = 0x80010000\n\t// ProtobufV1Magic is the magic code for kitex protobuf\n\tProtobufV1Magic = 0x90010000\n\n\t// MagicMask is bit mask for checking version.\n\tMagicMask = 0xffff0000\n)\n\nvar (\n\tttHeaderCodec   = ttHeader{}\n\tmeshHeaderCodec = meshHeader{}\n\n\t_ remote.Codec       = (*defaultCodec)(nil)\n\t_ remote.MetaDecoder = (*defaultCodec)(nil)\n)\n\n// NewDefaultCodec creates the default protocol sniffing codec supporting thrift and protobuf.\nfunc NewDefaultCodec() remote.Codec {\n\t// No size limit by default\n\treturn &defaultCodec{\n\t\tmaxSize: 0,\n\t}\n}\n\n// NewDefaultCodecWithSizeLimit creates the default protocol sniffing codec supporting thrift and protobuf but with size limit.\n// maxSize is in bytes\nfunc NewDefaultCodecWithSizeLimit(maxSize int) remote.Codec {\n\treturn &defaultCodec{\n\t\tmaxSize: maxSize,\n\t}\n}\n\ntype defaultCodec struct {\n\t// maxSize limits the max size of the payload\n\tmaxSize int\n}\n\n// EncodePayload encode payload\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// EncodeMetaAndPayload encode meta and payload\nfunc (c *defaultCodec) EncodeMetaAndPayload(ctx context.Context, message remote.Message, out remote.ByteBuffer, me remote.MetaEncoder) error {\n\tvar err error\n\tvar totalLenField []byte\n\ttp := message.ProtocolInfo().TransProto\n\n\t// 1. encode header and return totalLenField if needed\n\t// totalLenField will be filled after payload encoded\n\tif tp&transport.TTHeader == transport.TTHeader {\n\t\tif totalLenField, err = ttHeaderCodec.encode(ctx, message, out); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\t// 2. encode payload\n\tif err = me.EncodePayload(ctx, message, out); err != nil {\n\t\treturn err\n\t}\n\t// 3. fill totalLen field for header if needed\n\tif tp&transport.TTHeader == transport.TTHeader {\n\t\tif totalLenField == nil {\n\t\t\treturn perrors.NewProtocolErrorWithMsg(\"no buffer allocated for the header length field\")\n\t\t}\n\t\tpayloadLen := out.MallocLen() - Size32\n\t\tbinary.BigEndian.PutUint32(totalLenField, uint32(payloadLen))\n\t}\n\treturn nil\n}\n\n// Encode implements the remote.Codec interface, it does complete message encode include header and payload.\nfunc (c *defaultCodec) Encode(ctx context.Context, message remote.Message, out remote.ByteBuffer) (err error) {\n\treturn c.EncodeMetaAndPayload(ctx, message, out, c)\n}\n\n// DecodeMeta decode header\nfunc (c *defaultCodec) DecodeMeta(ctx context.Context, message remote.Message, in remote.ByteBuffer) (err error) {\n\tvar flagBuf []byte\n\tif flagBuf, err = in.Peek(2 * Size32); err != nil {\n\t\treturn perrors.NewProtocolErrorWithErrMsg(err, fmt.Sprintf(\"default codec read failed: %s\", err.Error()))\n\t}\n\n\tif err = checkRPCState(ctx, message); err != nil {\n\t\t// there is one call has finished in retry task, it doesn't need to do decode for this call\n\t\treturn err\n\t}\n\tisTTHeader := IsTTHeader(flagBuf)\n\t// 1. decode header\n\tif isTTHeader {\n\t\t// TTHeader\n\t\tif err = ttHeaderCodec.decode(ctx, message, in); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif flagBuf, err = in.Peek(2 * Size32); err != nil {\n\t\t\treturn perrors.NewProtocolErrorWithErrMsg(err, fmt.Sprintf(\"ttheader read payload first 8 byte failed: %s\", err.Error()))\n\t\t}\n\t} else if isMeshHeader(flagBuf) {\n\t\tmessage.Tags()[remote.MeshHeader] = true\n\t\t// MeshHeader\n\t\tif err = meshHeaderCodec.decode(ctx, message, in); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif flagBuf, err = in.Peek(2 * Size32); err != nil {\n\t\t\treturn perrors.NewProtocolErrorWithErrMsg(err, fmt.Sprintf(\"meshHeader read payload first 8 byte failed: %s\", err.Error()))\n\t\t}\n\t}\n\treturn checkPayload(flagBuf, message, in, isTTHeader, c.maxSize)\n}\n\n// DecodePayload decode payload\nfunc (c *defaultCodec) DecodePayload(ctx context.Context, message remote.Message, in remote.ByteBuffer) error {\n\tdefer func() {\n\t\tif ri := message.RPCInfo(); ri != nil {\n\t\t\tif ms := rpcinfo.AsMutableRPCStats(ri.Stats()); ms != nil {\n\t\t\t\tms.SetRecvSize(uint64(in.ReadLen()))\n\t\t\t}\n\t\t}\n\t}()\n\n\thasRead := in.ReadLen()\n\tpCodec, err := remote.GetPayloadCodec(message)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = pCodec.Unmarshal(ctx, message, in); err != nil {\n\t\treturn err\n\t}\n\tif message.PayloadLen() == 0 {\n\t\t// if protocol is PurePayload, should set payload length after decoded\n\t\tmessage.SetPayloadLen(in.ReadLen() - hasRead)\n\t}\n\treturn nil\n}\n\n// Decode implements the remote.Codec interface, it does complete message decode include header and payload.\n\n\n\n\n\n\n\n\n\n\nfunc (c *defaultCodec) Name() string {\n\treturn \"default\"\n}\n\n// Select to use thrift or protobuf according to the protocol.\nfunc (c *defaultCodec) encodePayload(ctx context.Context, message remote.Message, out remote.ByteBuffer) error {\n\tpCodec, err := remote.GetPayloadCodec(message)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn pCodec.Marshal(ctx, message, out)\n}\n\n/**\n * +------------------------------------------------------------+\n * |                  4Byte                 |       2Byte       |\n * +------------------------------------------------------------+\n * |   \t\t\t     Length\t\t\t    \t|   HEADER MAGIC    |\n * +------------------------------------------------------------+\n */\nfunc IsTTHeader(flagBuf []byte) bool {\n\treturn binary.BigEndian.Uint32(flagBuf[Size32:])&MagicMask == TTHeaderMagic\n}\n\n/**\n * +----------------------------------------+\n * |       2Byte        |       2Byte       |\n * +----------------------------------------+\n * |    HEADER MAGIC    |   HEADER SIZE     |\n * +----------------------------------------+\n */\nfunc isMeshHeader(flagBuf []byte) bool {\n\treturn binary.BigEndian.Uint32(flagBuf[:Size32])&MagicMask == MeshHeaderMagic\n}\n\n/**\n * Kitex protobuf has framed field\n * +------------------------------------------------------------+\n * |                  4Byte                 |       2Byte       |\n * +------------------------------------------------------------+\n * |   \t\t\t     Length\t\t\t    \t|   HEADER MAGIC    |\n * +------------------------------------------------------------+\n */\nfunc isProtobufKitex(flagBuf []byte) bool {\n\treturn binary.BigEndian.Uint32(flagBuf[Size32:])&MagicMask == ProtobufV1Magic\n}\n\n/**\n * +-------------------+\n * |       2Byte       |\n * +-------------------+\n * |   HEADER MAGIC    |\n * +-------------------\n */\nfunc isThriftBinary(flagBuf []byte) bool {\n\treturn binary.BigEndian.Uint32(flagBuf[:Size32])&MagicMask == ThriftV1Magic\n}\n\n/**\n * +------------------------------------------------------------+\n * |                  4Byte                 |       2Byte       |\n * +------------------------------------------------------------+\n * |   \t\t\t     Length\t\t\t    \t|   HEADER MAGIC    |\n * +------------------------------------------------------------+\n */\nfunc isThriftFramedBinary(flagBuf []byte) bool {\n\treturn binary.BigEndian.Uint32(flagBuf[Size32:])&MagicMask == ThriftV1Magic\n}\n\nfunc checkRPCState(ctx context.Context, message remote.Message) error {\n\tif message.RPCRole() == remote.Server {\n\t\treturn nil\n\t}\n\tif ctx.Err() == context.DeadlineExceeded || ctx.Err() == context.Canceled {\n\t\treturn kerrors.ErrRPCFinish\n\t}\n\tif respOp, ok := ctx.Value(retry.CtxRespOp).(*int32); ok {\n\t\tif !atomic.CompareAndSwapInt32(respOp, retry.OpNo, retry.OpDoing) {\n\t\t\t// previous call is being handling or done\n\t\t\t// this flag is used to check request status in retry(backup request) scene\n\t\t\treturn kerrors.ErrRPCFinish\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc checkPayload(flagBuf []byte, message remote.Message, in remote.ByteBuffer, isTTHeader bool, maxPayloadSize int) error {\n\tvar transProto transport.Protocol\n\tvar codecType serviceinfo.PayloadCodec\n\tif isThriftBinary(flagBuf) {\n\t\tcodecType = serviceinfo.Thrift\n\t\tif isTTHeader {\n\t\t\ttransProto = transport.TTHeader\n\t\t} else {\n\t\t\ttransProto = transport.PurePayload\n\t\t}\n\t} else if isThriftFramedBinary(flagBuf) {\n\t\tcodecType = serviceinfo.Thrift\n\t\tif isTTHeader {\n\t\t\ttransProto = transport.TTHeaderFramed\n\t\t} else {\n\t\t\ttransProto = transport.Framed\n\t\t}\n\t\tpayloadLen := binary.BigEndian.Uint32(flagBuf[:Size32])\n\t\tmessage.SetPayloadLen(int(payloadLen))\n\t\tif err := in.Skip(Size32); err != nil {\n\t\t\treturn err\n\t\t}\n\t} else if isProtobufKitex(flagBuf) {\n\t\tcodecType = serviceinfo.Protobuf\n\t\tif isTTHeader {\n\t\t\ttransProto = transport.TTHeaderFramed\n\t\t} else {\n\t\t\ttransProto = transport.Framed\n\t\t}\n\t\tpayloadLen := binary.BigEndian.Uint32(flagBuf[:Size32])\n\t\tmessage.SetPayloadLen(int(payloadLen))\n\t\tif err := in.Skip(Size32); err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\tfirst4Bytes := binary.BigEndian.Uint32(flagBuf[:Size32])\n\t\tsecond4Bytes := binary.BigEndian.Uint32(flagBuf[Size32:])\n\t\t// 0xfff4fffd is the interrupt message of telnet\n\t\terr := perrors.NewProtocolErrorWithMsg(fmt.Sprintf(\"invalid payload (first4Bytes=%#x, second4Bytes=%#x)\", first4Bytes, second4Bytes))\n\t\treturn err\n\t}\n\tif err := checkPayloadSize(message.PayloadLen(), maxPayloadSize); err != nil {\n\t\treturn err\n\t}\n\tmessage.SetProtocolInfo(remote.NewProtocolInfo(transProto, codecType))\n\tcfg := rpcinfo.AsMutableRPCConfig(message.RPCInfo().Config())\n\tif cfg != nil {\n\t\ttp := message.ProtocolInfo().TransProto\n\t\tcfg.SetTransportProtocol(tp)\n\t}\n\treturn nil\n}\n\nfunc checkPayloadSize(payloadLen, maxSize int) error {\n\tif maxSize > 0 && payloadLen > 0 && payloadLen > maxSize {\n\t\treturn perrors.NewProtocolErrorWithType(\n\t\t\tperrors.InvalidData,\n\t\t\tfmt.Sprintf(\"invalid data: payload size(%d) larger than the limit(%d)\", payloadLen, maxSize),\n\t\t)\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func (c *defaultCodec) Decode(ctx context.Context, message remote.Message, in remote.ByteBuffer) (err error) {\n# Natural Language Annotations: // Decode implements the remote.Codec interface, it does complete message decode include header and payload.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (c *defaultCodec) Decode(ctx context.Context, message remote.Message, in remote.ByteBuffer) (err error) {\n\t// 1. decode header\n\tif err = c.DecodeMeta(ctx, message, in); err != nil {\n\t\treturn err\n\t}\n\t// 2. decode payload\n\tif err = c.DecodePayload(ctx, message, in); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n```", "```go\nfunc (c *defaultCodec) Decode(ctx context.Context, message remote.Message, in remote.ByteBuffer) (err error) {\n\t// 1. decode meta\n\tif err = c.DecodeMeta(ctx, message, in); err != nil {\n\t\treturn err\n\t}\n\t// 2. decode payload\n\tif err = c.DecodePayload(ctx, message, in); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n```", "```go\nfunc (c *defaultCodec) Decode(ctx context.Context, message remote.Message, in remote.ByteBuffer) (err error) {\n\tdefer func() {\n\t\tif ri := message.RPCInfo(); ri != nil {\n\t\t\tif ms := rpcinfo.AsMutableRPCStats(ri.Stats()); ms != nil {\n\t\t\t\tms.SetRecvSize(uint64(in.ReadLen()))\n\t\t\t}\n\t\t}\n\t}()\n\n\thasRead := in.ReadLen()\n\tpCodec, err := remote.GetPayloadCodec(message)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = pCodec.Unmarshal(ctx, message, in); err != nil {\n\t\treturn err\n\t}\n\tif message.PayloadLen() == 0 {\n\t\tmessage.SetPayloadLen(in.ReadLen() - hasRead)\n\t}\n\treturn nil\n}\n```"]}, "kitex-pkg/remote/codec/thrift/binary_protocol.go-WriteMessageBegin": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/thrift/binary_protocol.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"encoding/binary\"\n\t\"math\"\n\t\"sync\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\n// must be strict read & strict write\nvar (\n\tbpPool sync.Pool\n\t_      thrift.TProtocol = (*BinaryProtocol)(nil)\n)\n\nfunc init() {\n\tbpPool.New = newBP\n}\n\nfunc newBP() interface{} {\n\treturn &BinaryProtocol{}\n}\n\n// NewBinaryProtocol ...\nfunc NewBinaryProtocol(t remote.ByteBuffer) *BinaryProtocol {\n\tbp := bpPool.Get().(*BinaryProtocol)\n\tbp.trans = t\n\treturn bp\n}\n\n// BinaryProtocol ...\ntype BinaryProtocol struct {\n\ttrans remote.ByteBuffer\n}\n\n// Recycle ...\nfunc (p *BinaryProtocol) Recycle() {\n\tp.trans = nil\n\tbpPool.Put(p)\n}\n\n/**\n * Writing Methods\n */\n\n// WriteMessageBegin ...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WriteMessageEnd ...\nfunc (p *BinaryProtocol) WriteMessageEnd() error {\n\treturn nil\n}\n\n// WriteStructBegin ...\nfunc (p *BinaryProtocol) WriteStructBegin(name string) error {\n\treturn nil\n}\n\n// WriteStructEnd ...\nfunc (p *BinaryProtocol) WriteStructEnd() error {\n\treturn nil\n}\n\n// WriteFieldBegin ...\nfunc (p *BinaryProtocol) WriteFieldBegin(name string, typeID thrift.TType, id int16) error {\n\te := p.WriteByte(int8(typeID))\n\tif e != nil {\n\t\treturn e\n\t}\n\te = p.WriteI16(id)\n\treturn e\n}\n\n// WriteFieldEnd ...\nfunc (p *BinaryProtocol) WriteFieldEnd() error {\n\treturn nil\n}\n\n// WriteFieldStop ...\nfunc (p *BinaryProtocol) WriteFieldStop() error {\n\te := p.WriteByte(thrift.STOP)\n\treturn e\n}\n\n// WriteMapBegin ...\nfunc (p *BinaryProtocol) WriteMapBegin(keyType, valueType thrift.TType, size int) error {\n\te := p.WriteByte(int8(keyType))\n\tif e != nil {\n\t\treturn e\n\t}\n\te = p.WriteByte(int8(valueType))\n\tif e != nil {\n\t\treturn e\n\t}\n\te = p.WriteI32(int32(size))\n\treturn e\n}\n\n// WriteMapEnd ...\nfunc (p *BinaryProtocol) WriteMapEnd() error {\n\treturn nil\n}\n\n// WriteListBegin ...\nfunc (p *BinaryProtocol) WriteListBegin(elemType thrift.TType, size int) error {\n\te := p.WriteByte(int8(elemType))\n\tif e != nil {\n\t\treturn e\n\t}\n\te = p.WriteI32(int32(size))\n\treturn e\n}\n\n// WriteListEnd ...\nfunc (p *BinaryProtocol) WriteListEnd() error {\n\treturn nil\n}\n\n// WriteSetBegin ...\nfunc (p *BinaryProtocol) WriteSetBegin(elemType thrift.TType, size int) error {\n\te := p.WriteByte(int8(elemType))\n\tif e != nil {\n\t\treturn e\n\t}\n\te = p.WriteI32(int32(size))\n\treturn e\n}\n\n// WriteSetEnd ...\nfunc (p *BinaryProtocol) WriteSetEnd() error {\n\treturn nil\n}\n\n// WriteBool ...\nfunc (p *BinaryProtocol) WriteBool(value bool) error {\n\tif value {\n\t\treturn p.WriteByte(1)\n\t}\n\treturn p.WriteByte(0)\n}\n\n// WriteByte ...\nfunc (p *BinaryProtocol) WriteByte(value int8) error {\n\tv, err := p.malloc(1)\n\tif err != nil {\n\t\treturn err\n\t}\n\tv[0] = byte(value)\n\treturn err\n}\n\n// WriteI16 ...\nfunc (p *BinaryProtocol) WriteI16(value int16) error {\n\tv, err := p.malloc(2)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbinary.BigEndian.PutUint16(v, uint16(value))\n\treturn err\n}\n\n// WriteI32 ...\nfunc (p *BinaryProtocol) WriteI32(value int32) error {\n\tv, err := p.malloc(4)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbinary.BigEndian.PutUint32(v, uint32(value))\n\treturn err\n}\n\n// WriteI64 ...\nfunc (p *BinaryProtocol) WriteI64(value int64) error {\n\tv, err := p.malloc(8)\n\tif err != nil {\n\t\treturn err\n\t}\n\tbinary.BigEndian.PutUint64(v, uint64(value))\n\treturn err\n}\n\n// WriteDouble ...\nfunc (p *BinaryProtocol) WriteDouble(value float64) error {\n\treturn p.WriteI64(int64(math.Float64bits(value)))\n}\n\n// WriteString ...\nfunc (p *BinaryProtocol) WriteString(value string) error {\n\tlen := len(value)\n\te := p.WriteI32(int32(len))\n\tif e != nil {\n\t\treturn e\n\t}\n\t_, e = p.trans.WriteString(value)\n\treturn e\n}\n\n// WriteBinary ...\nfunc (p *BinaryProtocol) WriteBinary(value []byte) error {\n\te := p.WriteI32(int32(len(value)))\n\tif e != nil {\n\t\treturn e\n\t}\n\t_, e = p.trans.WriteBinary(value)\n\treturn e\n}\n\n// malloc ...\nfunc (p *BinaryProtocol) malloc(size int) ([]byte, error) {\n\tbuf, err := p.trans.Malloc(size)\n\tif err != nil {\n\t\treturn buf, perrors.NewProtocolError(err)\n\t}\n\treturn buf, nil\n}\n\n/**\n * Reading methods\n */\n\n// ReadMessageBegin ...\nfunc (p *BinaryProtocol) ReadMessageBegin() (name string, typeID thrift.TMessageType, seqID int32, err error) {\n\tsize, e := p.ReadI32()\n\tif e != nil {\n\t\treturn \"\", typeID, 0, perrors.NewProtocolError(e)\n\t}\n\tif size > 0 {\n\t\treturn name, typeID, seqID, perrors.NewProtocolErrorWithType(perrors.BadVersion, \"Missing version in ReadMessageBegin\")\n\t}\n\ttypeID = thrift.TMessageType(size & 0x0ff)\n\tversion := int64(int64(size) & thrift.VERSION_MASK)\n\tif version != thrift.VERSION_1 {\n\t\treturn name, typeID, seqID, perrors.NewProtocolErrorWithType(perrors.BadVersion, \"Bad version in ReadMessageBegin\")\n\t}\n\tname, e = p.ReadString()\n\tif e != nil {\n\t\treturn name, typeID, seqID, perrors.NewProtocolError(e)\n\t}\n\tseqID, e = p.ReadI32()\n\tif e != nil {\n\t\treturn name, typeID, seqID, perrors.NewProtocolError(e)\n\t}\n\treturn name, typeID, seqID, nil\n}\n\n// ReadMessageEnd ...\nfunc (p *BinaryProtocol) ReadMessageEnd() error {\n\treturn nil\n}\n\n// ReadStructBegin ...\nfunc (p *BinaryProtocol) ReadStructBegin() (name string, err error) {\n\treturn\n}\n\n// ReadStructEnd ...\nfunc (p *BinaryProtocol) ReadStructEnd() error {\n\treturn nil\n}\n\n// ReadFieldBegin ...\nfunc (p *BinaryProtocol) ReadFieldBegin() (name string, typeID thrift.TType, id int16, err error) {\n\tt, err := p.ReadByte()\n\ttypeID = thrift.TType(t)\n\tif err != nil {\n\t\treturn name, typeID, id, err\n\t}\n\tif t != thrift.STOP {\n\t\tid, err = p.ReadI16()\n\t}\n\treturn name, typeID, id, err\n}\n\n// ReadFieldEnd ...\nfunc (p *BinaryProtocol) ReadFieldEnd() error {\n\treturn nil\n}\n\n// ReadMapBegin ...\nfunc (p *BinaryProtocol) ReadMapBegin() (kType, vType thrift.TType, size int, err error) {\n\tk, e := p.ReadByte()\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tkType = thrift.TType(k)\n\tv, e := p.ReadByte()\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tvType = thrift.TType(v)\n\tsize32, e := p.ReadI32()\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tif size32 < 0 {\n\t\terr = perrors.InvalidDataLength\n\t\treturn\n\t}\n\tsize = int(size32)\n\treturn kType, vType, size, nil\n}\n\n// ReadMapEnd ...\nfunc (p *BinaryProtocol) ReadMapEnd() error {\n\treturn nil\n}\n\n// ReadListBegin ...\nfunc (p *BinaryProtocol) ReadListBegin() (elemType thrift.TType, size int, err error) {\n\tb, e := p.ReadByte()\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\telemType = thrift.TType(b)\n\tsize32, e := p.ReadI32()\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tif size32 < 0 {\n\t\terr = perrors.InvalidDataLength\n\t\treturn\n\t}\n\tsize = int(size32)\n\n\treturn\n}\n\n// ReadListEnd ...\nfunc (p *BinaryProtocol) ReadListEnd() error {\n\treturn nil\n}\n\n// ReadSetBegin ...\nfunc (p *BinaryProtocol) ReadSetBegin() (elemType thrift.TType, size int, err error) {\n\tb, e := p.ReadByte()\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\telemType = thrift.TType(b)\n\tsize32, e := p.ReadI32()\n\tif e != nil {\n\t\terr = perrors.NewProtocolError(e)\n\t\treturn\n\t}\n\tif size32 < 0 {\n\t\terr = perrors.InvalidDataLength\n\t\treturn\n\t}\n\tsize = int(size32)\n\treturn elemType, size, nil\n}\n\n// ReadSetEnd ...\nfunc (p *BinaryProtocol) ReadSetEnd() error {\n\treturn nil\n}\n\n// ReadBool ...\nfunc (p *BinaryProtocol) ReadBool() (bool, error) {\n\tb, e := p.ReadByte()\n\tv := true\n\tif b != 1 {\n\t\tv = false\n\t}\n\treturn v, e\n}\n\n// ReadByte ...\nfunc (p *BinaryProtocol) ReadByte() (value int8, err error) {\n\tbuf, err := p.next(1)\n\tif err != nil {\n\t\treturn value, err\n\t}\n\treturn int8(buf[0]), err\n}\n\n// ReadI16 ...\nfunc (p *BinaryProtocol) ReadI16() (value int16, err error) {\n\tbuf, err := p.next(2)\n\tif err != nil {\n\t\treturn value, err\n\t}\n\tvalue = int16(binary.BigEndian.Uint16(buf))\n\treturn value, err\n}\n\n// ReadI32 ...\nfunc (p *BinaryProtocol) ReadI32() (value int32, err error) {\n\tbuf, err := p.next(4)\n\tif err != nil {\n\t\treturn value, err\n\t}\n\tvalue = int32(binary.BigEndian.Uint32(buf))\n\treturn value, err\n}\n\n// ReadI64 ...\nfunc (p *BinaryProtocol) ReadI64() (value int64, err error) {\n\tbuf, err := p.next(8)\n\tif err != nil {\n\t\treturn value, err\n\t}\n\tvalue = int64(binary.BigEndian.Uint64(buf))\n\treturn value, err\n}\n\n// ReadDouble ...\nfunc (p *BinaryProtocol) ReadDouble() (value float64, err error) {\n\tbuf, err := p.next(8)\n\tif err != nil {\n\t\treturn value, err\n\t}\n\tvalue = math.Float64frombits(binary.BigEndian.Uint64(buf))\n\treturn value, err\n}\n\n// ReadString ...\nfunc (p *BinaryProtocol) ReadString() (value string, err error) {\n\tsize, e := p.ReadI32()\n\tif e != nil {\n\t\treturn \"\", e\n\t}\n\tif size < 0 {\n\t\terr = perrors.InvalidDataLength\n\t\treturn\n\t}\n\tvalue, err = p.trans.ReadString(int(size))\n\tif err != nil {\n\t\treturn value, perrors.NewProtocolError(err)\n\t}\n\treturn value, nil\n}\n\n// ReadBinary ...\nfunc (p *BinaryProtocol) ReadBinary() ([]byte, error) {\n\tsize, e := p.ReadI32()\n\tif e != nil {\n\t\treturn nil, e\n\t}\n\tif size < 0 {\n\t\treturn nil, perrors.InvalidDataLength\n\t}\n\treturn p.trans.ReadBinary(int(size))\n}\n\n// Flush ...\nfunc (p *BinaryProtocol) Flush(ctx context.Context) (err error) {\n\terr = p.trans.Flush()\n\tif err != nil {\n\t\treturn perrors.NewProtocolError(err)\n\t}\n\treturn nil\n}\n\n// Skip ...\nfunc (p *BinaryProtocol) Skip(fieldType thrift.TType) (err error) {\n\treturn thrift.SkipDefaultDepth(p, fieldType)\n}\n\n// Transport ...\nfunc (p *BinaryProtocol) Transport() thrift.TTransport {\n\t// not support\n\treturn nil\n}\n\n// ByteBuffer ...\nfunc (p *BinaryProtocol) ByteBuffer() remote.ByteBuffer {\n\treturn p.trans\n}\n\n// next ...\nfunc (p *BinaryProtocol) next(size int) ([]byte, error) {\n\tbuf, err := p.trans.Next(size)\n\tif err != nil {\n\t\treturn buf, perrors.NewProtocolError(err)\n\t}\n\treturn buf, nil\n}\n\n```\n# Function Signature: func (p *BinaryProtocol) WriteMessageBegin(name string, typeID thrift.TMessageType, seqID int32) error {\n# Natural Language Annotations: /**\n * Writing Methods\n */\n// WriteMessageBegin ...\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (p *BinaryProtocol) WriteMessageBegin(name string, typeID thrift.TMessageType, seqID int32) error {\n\te := p.WriteByte(int8(typeID))\n\tif e != nil {\n\t\treturn e\n\t}\n\te = p.WriteI32(seqID)\n\tif e != nil {\n\t\treturn e\n\t}\n\te = p.WriteString(name)\n\treturn e\n}\n```", "```go\nfunc (p *BinaryProtocol) WriteMessageBegin(name string, typeID thrift.TMessageType, seqID int32) error {\n\te := p.WriteI32(int32(thrift.VERSION_1 | int64(typeID)))\n\tif e != nil {\n\t\treturn e\n\t}\n\te = p.WriteString(name)\n\tif e != nil {\n\t\treturn e\n\t}\n\te = p.WriteI32(seqID)\n\treturn e\n}\n```", "```go\nfunc (p *BinaryProtocol) WriteMessageBegin(name string, typeID thrift.TMessageType, seqID int32) error {\n\tvar version int64\n\tversion = version | thrift.VERSION_1\n\tversion = int64(typeID) | version<<8\n\te := p.WriteI32(-int32(version))\n\tif e != nil {\n\t\treturn e\n\t}\n\te = p.WriteString(name)\n\tif e != nil {\n\t\treturn e\n\t}\n\te = p.WriteI32(seqID)\n\tif e != nil {\n\t\treturn e\n\t}\n\treturn e\n}\n```"]}, "kitex-pkg/remote/codec/thrift/thrift_data.go-MarshalThriftData": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/thrift/thrift_data.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/bytedance/gopkg/lang/mcache\"\n\n\t\"github.com/cloudwego/kitex/pkg/protocol/bthrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\nconst marshalThriftBufferSize = 1024\n\n// MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\n\n\n\n\n\n\n\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It will allocate a new buffer and encode to it\nfunc (c thriftCodec) marshalThriftData(ctx context.Context, data interface{}) ([]byte, error) {\n\t// encode with hyper codec\n\t// NOTE: to ensure hyperMarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif c.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn c.hyperMarshalBody(data)\n\t}\n\n\t// encode with FastWrite\n\tif c.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif c.CodecType != Basic && hyperMarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn c.hyperMarshalBody(data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n\n// verifyMarshalBasicThriftDataType verifies whether data could be marshaled by old thrift way\nfunc verifyMarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageWriter:\n\tcase MessageWriterWithContext:\n\tdefault:\n\t\treturn errEncodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) fastMessageUnmarshalEnabled() bool {\n\treturn c.CodecType&FastRead != 0\n}\n\nfunc (c thriftCodec) fastMessageUnmarshalAvailable(data interface{}, payloadLen int) bool {\n\tif payloadLen == 0 && c.CodecType&EnableSkipDecoder == 0 {\n\t\treturn false\n\t}\n\t_, ok := data.(ThriftMsgFastCodec)\n\treturn ok\n}\n\nfunc (c thriftCodec) fastUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tmsg := data.(ThriftMsgFastCodec)\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\t_, err = msg.FastRead(buf)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = msg.FastRead(buf)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in FastCodec using SkipDecoder Buffer\")\n\t}\n\treturn err\n}\n\n// unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) hyperUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen - bthrift.Binary.MessageEndLength())\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in Frugal using SkipDecoder Buffer\")\n\t}\n\n\treturn nil\n}\n\n// verifyUnmarshalBasicThriftDataType verifies whether data could be unmarshal by old thrift way\nfunc verifyUnmarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageReader:\n\tcase MessageReaderWithMethodWithContext:\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// decodeBasicThriftData decode thrift body the old way (slow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getSkippedStructBuffer(tProt *BinaryProtocol) ([]byte, error) {\n\tsd := skipDecoder{ByteBuffer: tProt.trans}\n\tbuf, err := sd.NextStruct()\n\tif err != nil {\n\t\treturn nil, remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in SkipDecoder NextStruct phase\")\n\t}\n\treturn buf, nil\n}\n\n```\n# Function Signature: func MarshalThriftData(ctx context.Context, codec remote.PayloadCodec, data interface{}) ([]byte, error) {\n# Natural Language Annotations: // MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc MarshalThriftData(ctx context.Context, codec remote.PayloadCodec, data interface{}) ([]byte, error) {\n\tc, ok := codec.(thriftCodec)\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"expected codec to be of type thriftCodec\")\n\t}\n\treturn c.marshalThriftData(ctx, data)\n}\n```", "```go\nfunc MarshalThriftData(ctx context.Context, codec remote.PayloadCodec, data interface{}) ([]byte, error) {\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif codec.CodecType() != remote.Basic && hyperMarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn codec.MarshalThriftData(ctx, data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n```\nThis function `MarshalThriftData` is based on the `MarshalThriftData` function from the `thriftCodec` struct. It first checks if the data type is suitable for the basic marshaling. If it's not, it checks if the codec type is not `remote.Basic` and if frugal marshaling is available for the data type. If both conditions are true, it falls back to frugal marshaling. If not, it returns an error. If the data type is suitable for basic marshaling, it marshals the data in the old Thrift way.", "```go\nfunc MarshalThriftData(ctx context.Context, codec remote.PayloadCodec, data interface{}) ([]byte, error) {\n\tc, ok := codec.(thriftCodec)\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"invalid codec type, expected thriftCodec\")\n\t}\n\treturn c.marshalThriftData(ctx, data)\n}\n```"]}, "kitex-pkg/remote/codec/thrift/thrift_data.go-marshalBasicThriftData": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/thrift/thrift_data.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/bytedance/gopkg/lang/mcache\"\n\n\t\"github.com/cloudwego/kitex/pkg/protocol/bthrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\nconst marshalThriftBufferSize = 1024\n\n// MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\n\n\n\n\n\n\n\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It will allocate a new buffer and encode to it\nfunc (c thriftCodec) marshalThriftData(ctx context.Context, data interface{}) ([]byte, error) {\n\t// encode with hyper codec\n\t// NOTE: to ensure hyperMarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif c.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn c.hyperMarshalBody(data)\n\t}\n\n\t// encode with FastWrite\n\tif c.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif c.CodecType != Basic && hyperMarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn c.hyperMarshalBody(data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n\n// verifyMarshalBasicThriftDataType verifies whether data could be marshaled by old thrift way\nfunc verifyMarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageWriter:\n\tcase MessageWriterWithContext:\n\tdefault:\n\t\treturn errEncodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) fastMessageUnmarshalEnabled() bool {\n\treturn c.CodecType&FastRead != 0\n}\n\nfunc (c thriftCodec) fastMessageUnmarshalAvailable(data interface{}, payloadLen int) bool {\n\tif payloadLen == 0 && c.CodecType&EnableSkipDecoder == 0 {\n\t\treturn false\n\t}\n\t_, ok := data.(ThriftMsgFastCodec)\n\treturn ok\n}\n\nfunc (c thriftCodec) fastUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tmsg := data.(ThriftMsgFastCodec)\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\t_, err = msg.FastRead(buf)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = msg.FastRead(buf)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in FastCodec using SkipDecoder Buffer\")\n\t}\n\treturn err\n}\n\n// unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) hyperUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen - bthrift.Binary.MessageEndLength())\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in Frugal using SkipDecoder Buffer\")\n\t}\n\n\treturn nil\n}\n\n// verifyUnmarshalBasicThriftDataType verifies whether data could be unmarshal by old thrift way\nfunc verifyUnmarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageReader:\n\tcase MessageReaderWithMethodWithContext:\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// decodeBasicThriftData decode thrift body the old way (slow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getSkippedStructBuffer(tProt *BinaryProtocol) ([]byte, error) {\n\tsd := skipDecoder{ByteBuffer: tProt.trans}\n\tbuf, err := sd.NextStruct()\n\tif err != nil {\n\t\treturn nil, remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in SkipDecoder NextStruct phase\")\n\t}\n\treturn buf, nil\n}\n\n```\n# Function Signature: func marshalBasicThriftData(ctx context.Context, tProt thrift.TProtocol, data interface{}) error {\n# Natural Language Annotations: // marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc marshalBasicThriftData(ctx context.Context, tProt *thrift.TBinaryProtocol, data interface{}) error {\n\tswitch msg := data.(type) {\n\tcase MessageWriter:\n\t\treturn msg.Write(tProt)\n\tcase MessageWriterWithContext:\n\t\treturn msg.Write(ctx, tProt)\n\tdefault:\n\t\treturn fmt.Errorf(\"unexpected type: %T\", data)\n\t}\n}\n```\n\nThis function takes a context, a thrift protocol, and a data object. It uses a type switch to determine which specific type the data object is. If the data object is of type `MessageWriter`, it calls the Write method on the data object using the provided protocol. If the data object is of type `MessageWriterWithContext`, it calls the Write method with the context and protocol. If the data object does not match either type, it returns an error.", "```go\nfunc marshalBasicThriftData(ctx context.Context, tProt *thrift.TProtocol, data interface{}) error {\n\tswitch msg := data.(type) {\n\tcase MessageWriter:\n\t\tif err := msg.Write(tProt); err != nil {\n\t\t\treturn err\n\t\t}\n\tcase MessageWriterWithContext:\n\t\tif err := msg.Write(tProt); err != nil {\n\t\t\treturn err\n\t\t}\n\tdefault:\n\t\treturn fmt.Errorf(\"marshalBasicThriftData: unsupported type %T\", data)\n\t}\n\treturn nil\n}\n```", "```go\nfunc marshalBasicThriftData(ctx context.Context, tProt *thrift.TProtocol, data interface{}) error {\n\tswitch msg := data.(type) {\n\tcase MessageWriter:\n\t\treturn msg.Write(tProt)\n\tcase MessageWriterWithContext:\n\t\treturn msg.Write(ctx, tProt)\n\t}\n\treturn fmt.Errorf(\"unsupported message type\")\n}\n```"]}, "kitex-pkg/remote/codec/thrift/thrift_data.go-UnmarshalThriftException": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/thrift/thrift_data.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/bytedance/gopkg/lang/mcache\"\n\n\t\"github.com/cloudwego/kitex/pkg/protocol/bthrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\nconst marshalThriftBufferSize = 1024\n\n// MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\n\n\n\n\n\n\n\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It will allocate a new buffer and encode to it\nfunc (c thriftCodec) marshalThriftData(ctx context.Context, data interface{}) ([]byte, error) {\n\t// encode with hyper codec\n\t// NOTE: to ensure hyperMarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif c.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn c.hyperMarshalBody(data)\n\t}\n\n\t// encode with FastWrite\n\tif c.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif c.CodecType != Basic && hyperMarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn c.hyperMarshalBody(data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n\n// verifyMarshalBasicThriftDataType verifies whether data could be marshaled by old thrift way\nfunc verifyMarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageWriter:\n\tcase MessageWriterWithContext:\n\tdefault:\n\t\treturn errEncodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) fastMessageUnmarshalEnabled() bool {\n\treturn c.CodecType&FastRead != 0\n}\n\nfunc (c thriftCodec) fastMessageUnmarshalAvailable(data interface{}, payloadLen int) bool {\n\tif payloadLen == 0 && c.CodecType&EnableSkipDecoder == 0 {\n\t\treturn false\n\t}\n\t_, ok := data.(ThriftMsgFastCodec)\n\treturn ok\n}\n\nfunc (c thriftCodec) fastUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tmsg := data.(ThriftMsgFastCodec)\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\t_, err = msg.FastRead(buf)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = msg.FastRead(buf)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in FastCodec using SkipDecoder Buffer\")\n\t}\n\treturn err\n}\n\n// unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) hyperUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen - bthrift.Binary.MessageEndLength())\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in Frugal using SkipDecoder Buffer\")\n\t}\n\n\treturn nil\n}\n\n// verifyUnmarshalBasicThriftDataType verifies whether data could be unmarshal by old thrift way\nfunc verifyUnmarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageReader:\n\tcase MessageReaderWithMethodWithContext:\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// decodeBasicThriftData decode thrift body the old way (slow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getSkippedStructBuffer(tProt *BinaryProtocol) ([]byte, error) {\n\tsd := skipDecoder{ByteBuffer: tProt.trans}\n\tbuf, err := sd.NextStruct()\n\tif err != nil {\n\t\treturn nil, remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in SkipDecoder NextStruct phase\")\n\t}\n\treturn buf, nil\n}\n\n```\n# Function Signature: func UnmarshalThriftException(tProt thrift.TProtocol) error {\n# Natural Language Annotations: // UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc UnmarshalThriftException(tProt *thrift.TProtocol) error {\n\t_, err := tProt.ReadString() // We skip the name field\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = tProt.ReadByte() // We skip the type field\n\tif err != nil {\n\t\treturn err\n\t}\n\tseqID, err := tProt.ReadI32() // We skip the sequence ID field\n\tif err != nil {\n\t\treturn err\n\t}\n\tif seqID != 0 {\n\t\treturn fmt.Errorf(\"unexpected sequence ID for exception: %d\", seqID)\n\t}\n\n\texceptionName, err := tProt.ReadString()\n\tif err != nil {\n\t\treturn err\n\t}\n\texceptionMessage, err := tProt.ReadString()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn fmt.Errorf(\"%s: %s\", exceptionName, exceptionMessage)\n}\n```", "```go\nfunc UnmarshalThriftException(tProt thrift.TProtocol) error {\n\tif err := tProt.Skip(thrift.STRUCT_SIZE+thrift.INT_SIZE); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t}\n\n\tvar name string\n\tif v, err := tProt.ReadString(); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t} else {\n\t\tname = v\n\t}\n\n\tif err := tProt.Skip(thrift.INT_SIZE); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t}\n\n\tvar message string\n\tif v, err := tProt.ReadString(); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t} else {\n\t\tmessage = v\n\t}\n\n\treturn fmt.Errorf(\"ThriftException: %s, %s\", name, message)\n}\n```\nPlease note that this function assumes that the `TProtocol` is currently positioned at the start of a Thrift Exception. The function reads and skips the Thrift Exception's structure, field names, and field types, then reads the `name` and `message` fields of the Thrift Exception.\n\nIf your input is a `[]byte`, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))` before calling this function.", "```go\nfunc UnmarshalThriftException(tProt thrift.TProtocol) error {\n\tvar errMsg string\n\terr := tProt.ReadString(&errMsg)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t}\n\n\treturn remote.NewTransErrorWithMsg(remote.CodecException, errMsg)\n}\n```"]}, "kitex-pkg/remote/codec/thrift/thrift_data.go-UnmarshalThriftData": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/thrift/thrift_data.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/bytedance/gopkg/lang/mcache\"\n\n\t\"github.com/cloudwego/kitex/pkg/protocol/bthrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\nconst marshalThriftBufferSize = 1024\n\n// MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\n\n\n\n\n\n\n\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It will allocate a new buffer and encode to it\nfunc (c thriftCodec) marshalThriftData(ctx context.Context, data interface{}) ([]byte, error) {\n\t// encode with hyper codec\n\t// NOTE: to ensure hyperMarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif c.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn c.hyperMarshalBody(data)\n\t}\n\n\t// encode with FastWrite\n\tif c.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif c.CodecType != Basic && hyperMarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn c.hyperMarshalBody(data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n\n// verifyMarshalBasicThriftDataType verifies whether data could be marshaled by old thrift way\nfunc verifyMarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageWriter:\n\tcase MessageWriterWithContext:\n\tdefault:\n\t\treturn errEncodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) fastMessageUnmarshalEnabled() bool {\n\treturn c.CodecType&FastRead != 0\n}\n\nfunc (c thriftCodec) fastMessageUnmarshalAvailable(data interface{}, payloadLen int) bool {\n\tif payloadLen == 0 && c.CodecType&EnableSkipDecoder == 0 {\n\t\treturn false\n\t}\n\t_, ok := data.(ThriftMsgFastCodec)\n\treturn ok\n}\n\nfunc (c thriftCodec) fastUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tmsg := data.(ThriftMsgFastCodec)\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\t_, err = msg.FastRead(buf)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = msg.FastRead(buf)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in FastCodec using SkipDecoder Buffer\")\n\t}\n\treturn err\n}\n\n// unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) hyperUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen - bthrift.Binary.MessageEndLength())\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in Frugal using SkipDecoder Buffer\")\n\t}\n\n\treturn nil\n}\n\n// verifyUnmarshalBasicThriftDataType verifies whether data could be unmarshal by old thrift way\nfunc verifyUnmarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageReader:\n\tcase MessageReaderWithMethodWithContext:\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// decodeBasicThriftData decode thrift body the old way (slow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getSkippedStructBuffer(tProt *BinaryProtocol) ([]byte, error) {\n\tsd := skipDecoder{ByteBuffer: tProt.trans}\n\tbuf, err := sd.NextStruct()\n\tif err != nil {\n\t\treturn nil, remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in SkipDecoder NextStruct phase\")\n\t}\n\treturn buf, nil\n}\n\n```\n# Function Signature: func UnmarshalThriftData(ctx context.Context, codec remote.PayloadCodec, method string, buf []byte, data interface{}) error {\n# Natural Language Annotations: // UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc UnmarshalThriftData(ctx context.Context, codec remote.PayloadCodec, method string, buf []byte, data interface{}) error {\n\t// decode with hyper codec\n\t// NOTE: to ensure hyperUnmarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif codec.hyperUnmarshalEnabled() && codec.hyperUnmarshalAvailable(data, len(buf)) {\n\t\treturn codec.hyperUnmarshal(tProt, data, len(buf))\n\t}\n\n\t// decode with FastRead\n\tif codec.CodecType&FastRead != 0 && codec.fastMessageUnmarshalAvailable(data, len(buf)) {\n\t\treturn codec.fastUnmarshal(tProt, data, len(buf))\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferWithBytes(buf)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := codec.verifyUnmarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif codec.CodecType != Basic && codec.hyperUnmarshalAvailable(data, len(buf)) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn codec.hyperUnmarshal(tProt, data, len(buf))\n\t\t}\n\t\treturn err\n\t}\n\n\tif err := codec.decodeBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n```", "```go\nfunc UnmarshalThriftData(ctx context.Context, codec remote.PayloadCodec, method string, buf []byte, data interface{}) error {\n\tdataLen := len(buf)\n\ttransport := thrift.NewTMemoryBufferWithBuffer(buf)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\n\tswitch c := codec.(type) {\n\tcase thriftCodec:\n\t\tif c.fastMessageUnmarshalEnabled() && c.fastMessageUnmarshalAvailable(data, dataLen) {\n\t\t\treturn c.fastUnmarshal(tProt, data, dataLen)\n\t\t}\n\n\t\tif c.hyperUnmarshalEnabled() {\n\t\t\treturn c.hyperUnmarshal(tProt, data, dataLen)\n\t\t}\n\n\t\tif err := verifyUnmarshalBasicThriftDataType(data); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n```", "```go\nfunc UnmarshalThriftData(ctx context.Context, codec remote.PayloadCodec, method string, buf []byte, data interface{}) error {\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\t_, err := transport.Write(buf)\n\tif err != nil {\n\t\treturn err\n\t}\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tname, typeId, _ := tProt.ReadMessageBegin()\n\tif typeId != thrift.REPLY {\n\t\treturn fmt.Errorf(\"expected REPLY, but got %v\", typeId)\n\t}\n\tif name != method {\n\t\treturn fmt.Errorf(\"expected method name %v, but got %v\", method, name)\n\t}\n\terr = codec.Unmarshal(buf, data)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n```"]}, "kitex-pkg/remote/codec/thrift/thrift_data.go-unmarshalThriftData": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/thrift/thrift_data.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/bytedance/gopkg/lang/mcache\"\n\n\t\"github.com/cloudwego/kitex/pkg/protocol/bthrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\nconst marshalThriftBufferSize = 1024\n\n// MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\n\n\n\n\n\n\n\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It will allocate a new buffer and encode to it\nfunc (c thriftCodec) marshalThriftData(ctx context.Context, data interface{}) ([]byte, error) {\n\t// encode with hyper codec\n\t// NOTE: to ensure hyperMarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif c.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn c.hyperMarshalBody(data)\n\t}\n\n\t// encode with FastWrite\n\tif c.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif c.CodecType != Basic && hyperMarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn c.hyperMarshalBody(data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n\n// verifyMarshalBasicThriftDataType verifies whether data could be marshaled by old thrift way\nfunc verifyMarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageWriter:\n\tcase MessageWriterWithContext:\n\tdefault:\n\t\treturn errEncodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) fastMessageUnmarshalEnabled() bool {\n\treturn c.CodecType&FastRead != 0\n}\n\nfunc (c thriftCodec) fastMessageUnmarshalAvailable(data interface{}, payloadLen int) bool {\n\tif payloadLen == 0 && c.CodecType&EnableSkipDecoder == 0 {\n\t\treturn false\n\t}\n\t_, ok := data.(ThriftMsgFastCodec)\n\treturn ok\n}\n\nfunc (c thriftCodec) fastUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tmsg := data.(ThriftMsgFastCodec)\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\t_, err = msg.FastRead(buf)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = msg.FastRead(buf)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in FastCodec using SkipDecoder Buffer\")\n\t}\n\treturn err\n}\n\n// unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) hyperUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen - bthrift.Binary.MessageEndLength())\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in Frugal using SkipDecoder Buffer\")\n\t}\n\n\treturn nil\n}\n\n// verifyUnmarshalBasicThriftDataType verifies whether data could be unmarshal by old thrift way\nfunc verifyUnmarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageReader:\n\tcase MessageReaderWithMethodWithContext:\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// decodeBasicThriftData decode thrift body the old way (slow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getSkippedStructBuffer(tProt *BinaryProtocol) ([]byte, error) {\n\tsd := skipDecoder{ByteBuffer: tProt.trans}\n\tbuf, err := sd.NextStruct()\n\tif err != nil {\n\t\treturn nil, remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in SkipDecoder NextStruct phase\")\n\t}\n\treturn buf, nil\n}\n\n```\n# Function Signature: func (c thriftCodec) unmarshalThriftData(ctx context.Context, tProt *BinaryProtocol, method string, data interface{}, dataLen int) error {\n# Natural Language Annotations: // unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (c thriftCodec) unmarshalThriftData(ctx context.Context, tProt *BinaryProtocol, method string, data interface{}, dataLen int) error {\n\tif c.fastMessageUnmarshalEnabled() && c.fastMessageUnmarshalAvailable(data, dataLen) {\n\t\treturn c.fastUnmarshal(tProt, data, dataLen)\n\t}\n\n\tif err := verifyUnmarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif c.CodecType != Basic && hyperUnmarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn c.hyperUnmarshal(tProt, data, dataLen)\n\t\t}\n\t\treturn err\n\t}\n\n\t// fallback to old thrift way (slow)\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err := decodeBasicThriftData(ctx, tProt, data, buf); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t}\n\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err := decodeBasicThriftData(ctx, tProt, data, buf); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in old thrift way using SkipDecoder Buffer\")\n\t}\n\treturn nil\n}\n```", "```go\nfunc (c thriftCodec) unmarshalThriftData(ctx context.Context, tProt *BinaryProtocol, method string, data interface{}, dataLen int) error {\n\tif c.fastMessageUnmarshalEnabled() && c.fastMessageUnmarshalAvailable(data, dataLen) {\n\t\treturn c.fastUnmarshal(tProt, data, dataLen)\n\t}\n\n\tif err := verifyUnmarshalBasicThriftDataType(data); err != nil {\n\t\tif c.CodecType != Basic && hyperUnmarshalAvailable(data) {\n\t\t\treturn c.hyperUnmarshal(tProt, data, dataLen)\n\t\t}\n\t\treturn err\n\t}\n\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen - bthrift.Binary.MessageEndLength())\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in Frugal using SkipDecoder Buffer\")\n\t}\n\n\treturn nil\n}\n```", "```go\nfunc (c thriftCodec) unmarshalThriftData(ctx context.Context, tProt *BinaryProtocol, method string, data interface{}, dataLen int) error {\n\t// decode with hyper codec\n\t// NOTE: to ensure hyperUnmarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif c.hyperUnmarshalEnabled() && c.fastMessageUnmarshalAvailable(data, dataLen) {\n\t\treturn c.fastUnmarshal(tProt, data, dataLen)\n\t}\n\n\t// decode with FastRead\n\tif c.fastMessageUnmarshalEnabled() && c.fastMessageUnmarshalAvailable(data, dataLen) {\n\t\treturn c.fastUnmarshal(tProt, data, dataLen)\n\t}\n\n\tif err := verifyUnmarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif c.CodecType != Basic && c.fastMessageUnmarshalAvailable(data, dataLen) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn c.fastUnmarshal(tProt, data, dataLen)\n\t\t}\n\t\treturn err\n\t}\n\n\t// fallback to old thrift way (slow)\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err = decodeBasicThriftData(ctx, tProt, buf, data); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t}\n\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = decodeBasicThriftData(ctx, tProt, buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in FastCodec using SkipDecoder Buffer\")\n\t}\n\treturn nil\n}\n```"]}, "kitex-pkg/remote/codec/thrift/thrift_data.go-decodeBasicThriftData": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/codec/thrift/thrift_data.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/bytedance/gopkg/lang/mcache\"\n\n\t\"github.com/cloudwego/kitex/pkg/protocol/bthrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\nconst marshalThriftBufferSize = 1024\n\n// MarshalThriftData only encodes the data (without the prepending methodName, msgType, seqId)\n// It will allocate a new buffer and encode to it\n\n\n\n\n\n\n\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It will allocate a new buffer and encode to it\nfunc (c thriftCodec) marshalThriftData(ctx context.Context, data interface{}) ([]byte, error) {\n\t// encode with hyper codec\n\t// NOTE: to ensure hyperMarshalEnabled is inlined so split the check logic, or it may cause performance loss\n\tif c.hyperMarshalEnabled() && hyperMarshalAvailable(data) {\n\t\treturn c.hyperMarshalBody(data)\n\t}\n\n\t// encode with FastWrite\n\tif c.CodecType&FastWrite != 0 {\n\t\tif msg, ok := data.(ThriftMsgFastCodec); ok {\n\t\t\tpayloadSize := msg.BLength()\n\t\t\tpayload := mcache.Malloc(payloadSize)\n\t\t\tmsg.FastWriteNocopy(payload, nil)\n\t\t\treturn payload, nil\n\t\t}\n\t}\n\n\tif err := verifyMarshalBasicThriftDataType(data); err != nil {\n\t\t// Basic can be used for disabling frugal, we need to check it\n\t\tif c.CodecType != Basic && hyperMarshalAvailable(data) {\n\t\t\t// fallback to frugal when the generated code is using slim template\n\t\t\treturn c.hyperMarshalBody(data)\n\t\t}\n\t\treturn nil, err\n\t}\n\n\t// fallback to old thrift way (slow)\n\ttransport := thrift.NewTMemoryBufferLen(marshalThriftBufferSize)\n\ttProt := thrift.NewTBinaryProtocol(transport, true, true)\n\tif err := marshalBasicThriftData(ctx, tProt, data); err != nil {\n\t\treturn nil, err\n\t}\n\treturn transport.Bytes(), nil\n}\n\n// verifyMarshalBasicThriftDataType verifies whether data could be marshaled by old thrift way\nfunc verifyMarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageWriter:\n\tcase MessageWriterWithContext:\n\tdefault:\n\t\treturn errEncodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// marshalBasicThriftData only encodes the data (without the prepending method, msgType, seqId)\n// It uses the old thrift way which is much slower than FastCodec and Frugal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftException decode thrift exception from tProt\n// If your input is []byte, you can wrap it with `NewBinaryProtocol(remote.NewReaderBuffer(buf))`\n\n\n\n\n\n\n\n\n\n\n\n// UnmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// It will decode from the given buffer.\n// Note:\n// 1. `method` is only used for generic calls\n// 2. if the buf contains an exception, you should call UnmarshalThriftException instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) fastMessageUnmarshalEnabled() bool {\n\treturn c.CodecType&FastRead != 0\n}\n\nfunc (c thriftCodec) fastMessageUnmarshalAvailable(data interface{}, payloadLen int) bool {\n\tif payloadLen == 0 && c.CodecType&EnableSkipDecoder == 0 {\n\t\treturn false\n\t}\n\t_, ok := data.(ThriftMsgFastCodec)\n\treturn ok\n}\n\nfunc (c thriftCodec) fastUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tmsg := data.(ThriftMsgFastCodec)\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\t_, err = msg.FastRead(buf)\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\t_, err = msg.FastRead(buf)\n\tif err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in FastCodec using SkipDecoder Buffer\")\n\t}\n\treturn err\n}\n\n// unmarshalThriftData only decodes the data (after methodName, msgType and seqId)\n// method is only used for generic calls\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c thriftCodec) hyperUnmarshal(tProt *BinaryProtocol, data interface{}, dataLen int) error {\n\tif dataLen > 0 {\n\t\tbuf, err := tProt.next(dataLen - bthrift.Binary.MessageEndLength())\n\t\tif err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\t\treturn remote.NewTransError(remote.ProtocolError, err)\n\t\t}\n\t\treturn nil\n\t}\n\tbuf, err := getSkippedStructBuffer(tProt)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err = c.hyperMessageUnmarshal(buf, data); err != nil {\n\t\treturn remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in Frugal using SkipDecoder Buffer\")\n\t}\n\n\treturn nil\n}\n\n// verifyUnmarshalBasicThriftDataType verifies whether data could be unmarshal by old thrift way\nfunc verifyUnmarshalBasicThriftDataType(data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageReader:\n\tcase MessageReaderWithMethodWithContext:\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n\treturn nil\n}\n\n// decodeBasicThriftData decode thrift body the old way (slow)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getSkippedStructBuffer(tProt *BinaryProtocol) ([]byte, error) {\n\tsd := skipDecoder{ByteBuffer: tProt.trans}\n\tbuf, err := sd.NextStruct()\n\tif err != nil {\n\t\treturn nil, remote.NewTransError(remote.ProtocolError, err).AppendMessage(\"caught in SkipDecoder NextStruct phase\")\n\t}\n\treturn buf, nil\n}\n\n```\n# Function Signature: func decodeBasicThriftData(ctx context.Context, tProt thrift.TProtocol, method string, data interface{}) error {\n# Natural Language Annotations: // decodeBasicThriftData decode thrift body the old way (slow)\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc decodeBasicThriftData(ctx context.Context, tProt thrift.TProtocol, method string, data interface{}) error {\n\tswitch dt := data.(type) {\n\tcase MessageReader:\n\t\tif err := dt.ReadMessage(ctx, tProt); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\tcase MessageReaderWithMethod:\n\t\tif err := dt.ReadMessageWithMethod(ctx, tProt, method); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n}\n```", "```go\nfunc decodeBasicThriftData(ctx context.Context, tProt thrift.TProtocol, method string, data interface{}) error {\n\tswitch data.(type) {\n\tcase MessageReader:\n\t\tif _, ok := data.(MessageReader); ok {\n\t\t\tif err := data.(MessageReader).Read(tProt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\tcase MessageReaderWithMethod:\n\t\tif _, ok := data.(MessageReaderWithMethod); ok {\n\t\t\tif err := data.(MessageReaderWithMethod).Read(tProt, method); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n\n\treturn nil\n}\n```\nThis function decodes the thrift data using the old thrift way, which is slower than FastCodec and Frugal. It takes in the context, thrift protocol, method, and data as parameters. It checks the type of the data and calls the appropriate Read method based on the type. If the type is not recognized, it returns an error.", "```go\nfunc decodeBasicThriftData(ctx context.Context, tProt thrift.TProtocol, method string, data interface{}) error {\n\tswitch t := data.(type) {\n\tcase MessageReader:\n\t\treturn t.Read(ctx, tProt)\n\tcase MessageReaderWithMethodWithContext:\n\t\treturn t.ReadMethodWithContext(ctx, method, tProt)\n\tdefault:\n\t\treturn errDecodeMismatchMsgType\n\t}\n}\n```"]}, "kitex-pkg/remote/remotecli/conn_wrapper.go-GetConn": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/remotecli/conn_wrapper.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage remotecli\n\nimport (\n\t\"context\"\n\t\"net\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n)\n\nvar connWrapperPool sync.Pool\n\nfunc init() {\n\tconnWrapperPool.New = newConnWrapper\n}\n\nvar _ ConnReleaser = &ConnWrapper{}\n\n// ConnReleaser helps to release the raw connection.\ntype ConnReleaser interface {\n\tReleaseConn(err error, ri rpcinfo.RPCInfo)\n}\n\n// ConnWrapper wraps a connection.\ntype ConnWrapper struct {\n\tconnPool remote.ConnPool\n\tconn     net.Conn\n}\n\n// NewConnWrapper returns a new ConnWrapper using the given connPool and logger.\nfunc NewConnWrapper(connPool remote.ConnPool) *ConnWrapper {\n\tcm := connWrapperPool.Get().(*ConnWrapper)\n\tcm.connPool = connPool\n\treturn cm\n}\n\n// GetConn returns a connection using the given Dialer and RPCInfo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ReleaseConn should notice that ri may nil when oneway\n// TODO duplicate release may cause problem?\nfunc (cm *ConnWrapper) ReleaseConn(err error, ri rpcinfo.RPCInfo) {\n\tif cm.conn == nil {\n\t\treturn\n\t}\n\tif cm.connPool != nil {\n\t\tif err == nil {\n\t\t\t_, ok := ri.To().Tag(rpcinfo.ConnResetTag)\n\t\t\tif ok || ri.Config().InteractionMode() == rpcinfo.Oneway {\n\t\t\t\tcm.connPool.Discard(cm.conn)\n\t\t\t} else {\n\t\t\t\tcm.connPool.Put(cm.conn)\n\t\t\t}\n\t\t} else {\n\t\t\tcm.connPool.Discard(cm.conn)\n\t\t}\n\t} else {\n\t\tcm.conn.Close()\n\t}\n\n\tcm.zero()\n\tconnWrapperPool.Put(cm)\n}\n\nfunc newConnWrapper() interface{} {\n\treturn &ConnWrapper{}\n}\n\nfunc (cm *ConnWrapper) zero() {\n\tcm.connPool = nil\n\tcm.conn = nil\n}\n\nfunc (cm *ConnWrapper) getConnWithPool(ctx context.Context, cp remote.ConnPool, d remote.Dialer,\n\ttimeout time.Duration, ri rpcinfo.RPCInfo,\n) (net.Conn, error) {\n\taddr := ri.To().Address()\n\tif addr == nil {\n\t\treturn nil, kerrors.ErrNoDestAddress\n\t}\n\topt := remote.ConnOption{Dialer: d, ConnectTimeout: timeout}\n\tri.Stats().Record(ctx, stats.ClientConnStart, stats.StatusInfo, \"\")\n\tconn, err := cp.Get(ctx, addr.Network(), addr.String(), opt)\n\tif err != nil {\n\t\tri.Stats().Record(ctx, stats.ClientConnFinish, stats.StatusError, err.Error())\n\t\treturn nil, kerrors.ErrGetConnection.WithCause(err)\n\t}\n\tri.Stats().Record(ctx, stats.ClientConnFinish, stats.StatusInfo, \"\")\n\treturn conn, nil\n}\n\nfunc (cm *ConnWrapper) getConnWithDialer(ctx context.Context, d remote.Dialer,\n\ttimeout time.Duration, ri rpcinfo.RPCInfo,\n) (net.Conn, error) {\n\taddr := ri.To().Address()\n\tif addr == nil {\n\t\treturn nil, kerrors.ErrNoDestAddress\n\t}\n\n\tri.Stats().Record(ctx, stats.ClientConnStart, stats.StatusInfo, \"\")\n\tconn, err := d.DialTimeout(addr.Network(), addr.String(), timeout)\n\tif err != nil {\n\t\tri.Stats().Record(ctx, stats.ClientConnFinish, stats.StatusError, err.Error())\n\t\treturn nil, kerrors.ErrGetConnection.WithCause(err)\n\t}\n\tri.Stats().Record(ctx, stats.ClientConnFinish, stats.StatusInfo, \"\")\n\treturn conn, nil\n}\n\n```\n# Function Signature: func (cm *ConnWrapper) GetConn(ctx context.Context, d remote.Dialer, ri rpcinfo.RPCInfo) (net.Conn, error) {\n# Natural Language Annotations: // GetConn returns a connection using the given Dialer and RPCInfo.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (cm *ConnWrapper) GetConn(ctx context.Context, d remote.Dialer, ri rpcinfo.RPCInfo) (net.Conn, error) {\n\tif cm.connPool != nil {\n\t\treturn cm.getConnWithPool(ctx, cm.connPool, d, ri.Config().RPCTimeout(), ri)\n\t}\n\treturn cm.getConnWithDialer(ctx, d, ri.Config().RPCTimeout(), ri)\n}\n```\nThis function retrieves a connection using the provided Dialer and RPCInfo. If a connection pool is available, it uses the pool to get a connection. Otherwise, it directly dials to get a connection. The connection is returned along with any errors encountered.", "```go\nfunc (cm *ConnWrapper) GetConn(ctx context.Context, d remote.Dialer, ri rpcinfo.RPCInfo) (net.Conn, error) {\n\taddr := ri.To().Address()\n\tif addr == nil {\n\t\treturn nil, kerrors.ErrNoDestAddress\n\t}\n\n\ttimeout := ri.Config().RPCTimeout()\n\n\tif cm.connPool != nil {\n\t\treturn cm.getConnWithPool(ctx, cm.connPool, d, timeout, ri)\n\t}\n\n\treturn cm.getConnWithDialer(ctx, d, timeout, ri)\n}\n```", "```go\nfunc (cm *ConnWrapper) GetConn(ctx context.Context, d remote.Dialer, ri rpcinfo.RPCInfo) (net.Conn, error) {\n\tif cm.connPool != nil {\n\t\treturn cm.getConnWithPool(ctx, cm.connPool, d, ri.Config().RPCTimeout(), ri)\n\t}\n\treturn cm.getConnWithDialer(ctx, d, ri.Config().RPCTimeout(), ri)\n}\n```"]}, "kitex-pkg/remote/remotecli/client.go-NewClient": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/remotecli/client.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage remotecli\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"net\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\n// Client implementation may be different between mux and non-mux clients.\ntype Client interface {\n\t// RPCInfo as param just avoid to get it from ctx\n\tSend(ctx context.Context, ri rpcinfo.RPCInfo, req remote.Message) (err error)\n\tRecv(ctx context.Context, ri rpcinfo.RPCInfo, resp remote.Message) (err error)\n\tRecycle()\n}\n\nvar clientPool = &sync.Pool{\n\tNew: func() interface{} {\n\t\treturn new(client)\n\t},\n}\n\ntype client struct {\n\ttransHdlr   remote.TransHandler\n\tconnManager *ConnWrapper\n\tconn        net.Conn\n}\n\n// NewClient creates a new Client using the given params.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c *client) Recycle() {\n\tc.transHdlr = nil\n\tc.connManager = nil\n\tc.conn = nil\n\tclientPool.Put(c)\n}\n\nfunc (c *client) init(handler remote.TransHandler, cm *ConnWrapper, conn net.Conn) {\n\tc.transHdlr = handler\n\tc.connManager = cm\n\tc.conn = conn\n}\n\n// Send is blocked.\nfunc (c *client) Send(ctx context.Context, ri rpcinfo.RPCInfo, req remote.Message) (err error) {\n\t_, err = c.transHdlr.Write(ctx, c.conn, req)\n\tif err != nil {\n\t\tc.connManager.ReleaseConn(err, ri)\n\t}\n\treturn err\n}\n\n// Recv is blocked.\nfunc (c *client) Recv(ctx context.Context, ri rpcinfo.RPCInfo, resp remote.Message) (err error) {\n\t// resp is nil means oneway\n\tif resp != nil {\n\t\tctx, err = c.transHdlr.Read(ctx, c.conn, resp)\n\t\tc.transHdlr.OnMessage(ctx, nil, resp)\n\t} else {\n\t\t// Wait for the request to be flushed out before closing the connection.\n\t\t// 500us is an acceptable duration to keep a minimal loss rate at best effort.\n\t\ttime.Sleep(time.Millisecond / 2)\n\t}\n\n\tc.connManager.ReleaseConn(err, ri)\n\treturn err\n}\n\n```\n# Function Signature: func NewClient(ctx context.Context, ri rpcinfo.RPCInfo, handler remote.TransHandler, opt *remote.ClientOption) (Client, error) {\n# Natural Language Annotations: // NewClient creates a new Client using the given params.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewClient(ctx context.Context, ri rpcinfo.RPCInfo, handler remote.TransHandler, opt *remote.ClientOption) (Client, error) {\n\tcm := GetConnWrapper(ri)\n\tconn, err := cm.GetConn(ctx, ri)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tc := clientPool.Get().(*client)\n\tc.init(handler, cm, conn)\n\treturn c, nil\n}\n```", "```go\nfunc NewClient(ctx context.Context, ri rpcinfo.RPCInfo, handler remote.TransHandler, opt *remote.ClientOption) (Client, error) {\n    c := clientPool.Get().(*client)\n    cm := GetConnWrapper(ri)\n    conn, err := cm.GetConn(ctx, ri, c.transHdl, opt)\n    if err != nil {\n        cm.PutConn(ri)\n        return nil, err\n    }\n    c.init(handler, cm, conn)\n    return c, nil\n}\n```", "```go\nfunc NewClient(ctx context.Context, ri rpcinfo.RPCInfo, handler remote.TransHandler, opt *remote.ClientOption) (Client, error) {\n\tif ri == nil {\n\t\treturn nil, errors.New(\"rpcinfo.RPCInfo is nil\")\n\t}\n\tif handler == nil {\n\t\treturn nil, errors.New(\"remote.TransHandler is nil\")\n\t}\n\tif opt == nil {\n\t\treturn nil, errors.New(\"remote.ClientOption is nil\")\n\t}\n\t\n\t// Create a new client from the pool\n\tc := clientPool.Get().(*client)\n\t\n\t// Initialize the client with the given params\n\tc.init(handler, cm, conn)\n\t\n\treturn c, nil\n}\n```\n\nPlease note that you need to define and initialize the `cm` (ConnWrapper) and `conn` (net.Conn) variables in the context where `NewClient` is called. The `cm` and `conn` variables are needed to create a new client and they are not provided as parameters in the `NewClient` signature."]}, "kitex-pkg/remote/connpool/long_pool.go-IsActive": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/connpool/long_pool.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package connpool provide short connection and long connection pool.\npackage connpool\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n)\n\nvar (\n\t_ net.Conn            = &longConn{}\n\t_ remote.LongConnPool = &LongPool{}\n\n\t// global shared tickers for different LongPool\n\tsharedTickers sync.Map\n)\n\nconst (\n\tconfigDumpKey = \"idle_config\"\n)\n\nfunc getSharedTicker(p *LongPool, refreshInterval time.Duration) *utils.SharedTicker {\n\tsti, ok := sharedTickers.Load(refreshInterval)\n\tif ok {\n\t\tst := sti.(*utils.SharedTicker)\n\t\tst.Add(p)\n\t\treturn st\n\t}\n\tsti, _ = sharedTickers.LoadOrStore(refreshInterval, utils.NewSharedTicker(refreshInterval))\n\tst := sti.(*utils.SharedTicker)\n\tst.Add(p)\n\treturn st\n}\n\n// netAddr implements the net.Addr interface and comparability.\ntype netAddr struct {\n\tnetwork string\n\taddress string\n}\n\n// Network implements the net.Addr interface.\nfunc (na netAddr) Network() string { return na.network }\n\n// String implements the net.Addr interface.\nfunc (na netAddr) String() string { return na.address }\n\n// longConn implements the net.Conn interface.\ntype longConn struct {\n\tnet.Conn\n\tsync.RWMutex\n\tdeadline time.Time\n\taddress  string\n}\n\n// Close implements the net.Conn interface.\nfunc (c *longConn) Close() error {\n\treturn c.Conn.Close()\n}\n\n// RawConn returns the real underlying net.Conn.\nfunc (c *longConn) RawConn() net.Conn {\n\treturn c.Conn\n}\n\n// IsActive indicates whether the connection is active.\n\n\n\n\n\n\n\n\n// Expired checks the deadline of the connection.\nfunc (c *longConn) Expired() bool {\n\treturn time.Now().After(c.deadline)\n}\n\ntype PoolDump struct {\n\tIdleNum       int         `json:\"idle_num\"`\n\tConnsDeadline []time.Time `json:\"conns_deadline\"`\n}\n\nfunc newPool(minIdle, maxIdle int, maxIdleTimeout time.Duration) *pool {\n\tp := &pool{\n\t\tidleList:       make([]*longConn, 0, maxIdle),\n\t\tminIdle:        minIdle,\n\t\tmaxIdle:        maxIdle,\n\t\tmaxIdleTimeout: maxIdleTimeout,\n\t}\n\treturn p\n}\n\n// pool implements a pool of long connections.\ntype pool struct {\n\tidleList []*longConn // idleList Get/Put by FILO(stack) but Evict by FIFO(queue)\n\tmu       sync.RWMutex\n\t// config\n\tminIdle        int\n\tmaxIdle        int           // currIdle <= maxIdle.\n\tmaxIdleTimeout time.Duration // the idle connection will be cleaned if the idle time exceeds maxIdleTimeout.\n}\n\n// Get gets the first active connection from the idleList. Return the number of connections decreased during the Get.\nfunc (p *pool) Get() (*longConn, bool, int) {\n\tp.mu.Lock()\n\t// Get the first active one\n\tn := len(p.idleList)\n\tselected := n - 1\n\tfor ; selected >= 0; selected-- {\n\t\to := p.idleList[selected]\n\t\t// reset slice element to nil, active conn object only could be hold reference by user function\n\t\tp.idleList[selected] = nil\n\t\tif o.IsActive() {\n\t\t\tp.idleList = p.idleList[:selected]\n\t\t\tp.mu.Unlock()\n\t\t\treturn o, true, n - selected\n\t\t}\n\t\t// inactive object\n\t\to.Close()\n\t}\n\t// in case all objects are inactive\n\tif selected < 0 {\n\t\tselected = 0\n\t}\n\tp.idleList = p.idleList[:selected]\n\tp.mu.Unlock()\n\treturn nil, false, n - selected\n}\n\n// Put puts back a connection to the pool.\n\n\n\n\n\n\n\n\n\n\n\n\n// Evict cleanups the expired connections.\n// Evict returns how many connections has been evicted.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Len returns the length of the pool.\nfunc (p *pool) Len() int {\n\tp.mu.RLock()\n\tl := len(p.idleList)\n\tp.mu.RUnlock()\n\treturn l\n}\n\n// Close closes the pool and all the objects in the pool.\nfunc (p *pool) Close() int {\n\tp.mu.Lock()\n\tnum := len(p.idleList)\n\tfor i := 0; i < num; i++ {\n\t\tp.idleList[i].Close()\n\t}\n\tp.idleList = nil\n\n\tp.mu.Unlock()\n\treturn num\n}\n\n// Dump dumps the info of all the objects in the pool.\nfunc (p *pool) Dump() PoolDump {\n\tp.mu.RLock()\n\tidleNum := len(p.idleList)\n\tconnsDeadline := make([]time.Time, idleNum)\n\tfor i := 0; i < idleNum; i++ {\n\t\tconnsDeadline[i] = p.idleList[i].deadline\n\t}\n\ts := PoolDump{\n\t\tIdleNum:       idleNum,\n\t\tConnsDeadline: connsDeadline,\n\t}\n\tp.mu.RUnlock()\n\treturn s\n}\n\nfunc newPeer(\n\tserviceName string,\n\taddr net.Addr,\n\tminIdle int,\n\tmaxIdle int,\n\tmaxIdleTimeout time.Duration,\n\tglobalIdle *utils.MaxCounter,\n) *peer {\n\treturn &peer{\n\t\tserviceName: serviceName,\n\t\taddr:        addr,\n\t\tglobalIdle:  globalIdle,\n\t\tpool:        newPool(minIdle, maxIdle, maxIdleTimeout),\n\t}\n}\n\n// peer has one address, it manages all connections base on this address\ntype peer struct {\n\t// info\n\tserviceName string\n\taddr        net.Addr\n\tglobalIdle  *utils.MaxCounter\n\t// pool\n\tpool *pool\n}\n\n// Get gets a connection with dialer and timeout. Dial a new connection if no idle connection in pool is available.\nfunc (p *peer) Get(d remote.Dialer, timeout time.Duration, reporter Reporter, addr string) (net.Conn, error) {\n\tvar c net.Conn\n\tc, reused, decNum := p.pool.Get()\n\tp.globalIdle.DecN(int64(decNum))\n\tif reused {\n\t\treporter.ReuseSucceed(Long, p.serviceName, p.addr)\n\t\treturn c, nil\n\t}\n\t// dial a new connection\n\tc, err := d.DialTimeout(p.addr.Network(), p.addr.String(), timeout)\n\tif err != nil {\n\t\treporter.ConnFailed(Long, p.serviceName, p.addr)\n\t\treturn nil, err\n\t}\n\treporter.ConnSucceed(Long, p.serviceName, p.addr)\n\treturn &longConn{\n\t\tConn:    c,\n\t\taddress: addr,\n\t}, nil\n}\n\n// Put puts a connection back to the peer.\nfunc (p *peer) Put(c *longConn) error {\n\tif !p.globalIdle.Inc() {\n\t\treturn c.Close()\n\t}\n\tif !p.pool.Put(c) {\n\t\tp.globalIdle.Dec()\n\t\treturn c.Close()\n\t}\n\treturn nil\n}\n\nfunc (p *peer) Len() int {\n\treturn p.pool.Len()\n}\n\nfunc (p *peer) Evict() {\n\tn := p.pool.Evict()\n\tp.globalIdle.DecN(int64(n))\n}\n\n// Close closes the peer and all the connections in the ring.\nfunc (p *peer) Close() {\n\tn := p.pool.Close()\n\tp.globalIdle.DecN(int64(n))\n}\n\n// NewLongPool creates a long pool using the given IdleConfig.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// LongPool manages a pool of long connections.\ntype LongPool struct {\n\treporter     Reporter\n\tpeerMap      sync.Map\n\tnewPeer      func(net.Addr) *peer\n\tglobalIdle   *utils.MaxCounter\n\tidleConfig   connpool.IdleConfig\n\tsharedTicker *utils.SharedTicker\n\tclosed       int32 // active: 0, closed: 1\n}\n\n// Get pick or generate a net.Conn and return\n// The context is not used but leave it for now.\nfunc (lp *LongPool) Get(ctx context.Context, network, address string, opt remote.ConnOption) (net.Conn, error) {\n\taddr := netAddr{network, address}\n\tp := lp.getPeer(addr)\n\treturn p.Get(opt.Dialer, opt.ConnectTimeout, lp.reporter, address)\n}\n\n// Put implements the ConnPool interface.\nfunc (lp *LongPool) Put(conn net.Conn) error {\n\tc, ok := conn.(*longConn)\n\tif !ok {\n\t\treturn conn.Close()\n\t}\n\n\taddr := conn.RemoteAddr()\n\tna := netAddr{addr.Network(), c.address}\n\tp, ok := lp.peerMap.Load(na)\n\tif ok {\n\t\tp.(*peer).Put(c)\n\t\treturn nil\n\t}\n\treturn c.Conn.Close()\n}\n\n// Discard implements the ConnPool interface.\nfunc (lp *LongPool) Discard(conn net.Conn) error {\n\tc, ok := conn.(*longConn)\n\tif ok {\n\t\treturn c.Close()\n\t}\n\treturn conn.Close()\n}\n\n// Clean implements the LongConnPool interface.\nfunc (lp *LongPool) Clean(network, address string) {\n\tna := netAddr{network, address}\n\tif p, ok := lp.peerMap.Load(na); ok {\n\t\tlp.peerMap.Delete(na)\n\t\tgo p.(*peer).Close()\n\t}\n}\n\n// Dump is used to dump current long pool info when needed, like debug query.\nfunc (lp *LongPool) Dump() interface{} {\n\tm := make(map[string]interface{})\n\tm[configDumpKey] = lp.idleConfig\n\tlp.peerMap.Range(func(key, value interface{}) bool {\n\t\tt := value.(*peer).pool.Dump()\n\t\tm[key.(netAddr).String()] = t\n\t\treturn true\n\t})\n\treturn m\n}\n\n// Close releases all peers in the pool, it is executed when client is closed.\nfunc (lp *LongPool) Close() error {\n\tif !atomic.CompareAndSwapInt32(&lp.closed, 0, 1) {\n\t\treturn fmt.Errorf(\"long pool is already closed\")\n\t}\n\t// close all peers\n\tlp.peerMap.Range(func(addr, value interface{}) bool {\n\t\tlp.peerMap.Delete(addr)\n\t\tv := value.(*peer)\n\t\tv.Close()\n\t\treturn true\n\t})\n\t// remove from the shared ticker\n\tlp.sharedTicker.Delete(lp)\n\treturn nil\n}\n\n// EnableReporter enable reporter for long connection pool.\nfunc (lp *LongPool) EnableReporter() {\n\tlp.reporter = GetCommonReporter()\n}\n\n// WarmUp implements the warmup.Pool interface.\nfunc (lp *LongPool) WarmUp(eh warmup.ErrorHandling, wuo *warmup.PoolOption, co remote.ConnOption) error {\n\th := &warmup.PoolHelper{ErrorHandling: eh}\n\treturn h.WarmUp(wuo, lp, co)\n}\n\n// Evict cleanups the idle connections in peers.\nfunc (lp *LongPool) Evict() {\n\tif atomic.LoadInt32(&lp.closed) == 0 {\n\t\t// Evict idle connections\n\t\tlp.peerMap.Range(func(key, value interface{}) bool {\n\t\t\tp := value.(*peer)\n\t\t\tp.Evict()\n\t\t\treturn true\n\t\t})\n\t}\n}\n\n// Tick implements the interface utils.TickerTask.\nfunc (lp *LongPool) Tick() {\n\tlp.Evict()\n}\n\n// getPeer gets a peer from the pool based on the addr, or create a new one if not exist.\nfunc (lp *LongPool) getPeer(addr netAddr) *peer {\n\tp, ok := lp.peerMap.Load(addr)\n\tif ok {\n\t\treturn p.(*peer)\n\t}\n\tp, _ = lp.peerMap.LoadOrStore(addr, lp.newPeer(addr))\n\treturn p.(*peer)\n}\n\n```\n# Function Signature: func (c *longConn) IsActive() bool {\n# Natural Language Annotations: // IsActive indicates whether the connection is active.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (c *longConn) IsActive() bool {\n\tif c.Conn == nil {\n\t\treturn false\n\t}\n\tone := []byte{}\n\tc.Conn.SetReadDeadline(time.Now())\n\tif _, err := c.Conn.Read(one); err == io.EOF {\n\t\tc.Conn.Close()\n\t\tc.Conn = nil\n\t\treturn false\n\t}\n\tvar zero time.Time\n\tif c.deadline != zero && c.deadline.Before(time.Now()) {\n\t\tc.Conn.Close()\n\t\tc.Conn = nil\n\t\treturn false\n\t}\n\tc.Conn.SetReadDeadline(time.Time{})\n\treturn true\n}\n```", "```go\nfunc (c *longConn) IsActive() bool {\n\tif c.Expired() {\n\t\treturn false\n\t}\n\t// Check if the connection is still active. In the context of a long connection, it's active if it is not expired.\n\t// However, since the long connection is not actively maintained, we can't provide a more accurate check here.\n\t// You might want to add a more specific check depending on your context.\n\treturn true\n}\n```", "```go\nfunc (c *longConn) IsActive() bool {\n\tc.RLock()\n\tdefer c.RUnlock()\n\n\t// Check the connection status\n\tif c.Conn != nil {\n\t\treturn true\n\t}\n\treturn false\n}\n```\nThis function checks if the connection is active by checking if the `net.Conn` field is not nil. It uses a read lock to ensure thread safety."]}, "kitex-pkg/remote/connpool/long_pool.go-Evict": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/connpool/long_pool.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package connpool provide short connection and long connection pool.\npackage connpool\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n)\n\nvar (\n\t_ net.Conn            = &longConn{}\n\t_ remote.LongConnPool = &LongPool{}\n\n\t// global shared tickers for different LongPool\n\tsharedTickers sync.Map\n)\n\nconst (\n\tconfigDumpKey = \"idle_config\"\n)\n\nfunc getSharedTicker(p *LongPool, refreshInterval time.Duration) *utils.SharedTicker {\n\tsti, ok := sharedTickers.Load(refreshInterval)\n\tif ok {\n\t\tst := sti.(*utils.SharedTicker)\n\t\tst.Add(p)\n\t\treturn st\n\t}\n\tsti, _ = sharedTickers.LoadOrStore(refreshInterval, utils.NewSharedTicker(refreshInterval))\n\tst := sti.(*utils.SharedTicker)\n\tst.Add(p)\n\treturn st\n}\n\n// netAddr implements the net.Addr interface and comparability.\ntype netAddr struct {\n\tnetwork string\n\taddress string\n}\n\n// Network implements the net.Addr interface.\nfunc (na netAddr) Network() string { return na.network }\n\n// String implements the net.Addr interface.\nfunc (na netAddr) String() string { return na.address }\n\n// longConn implements the net.Conn interface.\ntype longConn struct {\n\tnet.Conn\n\tsync.RWMutex\n\tdeadline time.Time\n\taddress  string\n}\n\n// Close implements the net.Conn interface.\nfunc (c *longConn) Close() error {\n\treturn c.Conn.Close()\n}\n\n// RawConn returns the real underlying net.Conn.\nfunc (c *longConn) RawConn() net.Conn {\n\treturn c.Conn\n}\n\n// IsActive indicates whether the connection is active.\n\n\n\n\n\n\n\n\n// Expired checks the deadline of the connection.\nfunc (c *longConn) Expired() bool {\n\treturn time.Now().After(c.deadline)\n}\n\ntype PoolDump struct {\n\tIdleNum       int         `json:\"idle_num\"`\n\tConnsDeadline []time.Time `json:\"conns_deadline\"`\n}\n\nfunc newPool(minIdle, maxIdle int, maxIdleTimeout time.Duration) *pool {\n\tp := &pool{\n\t\tidleList:       make([]*longConn, 0, maxIdle),\n\t\tminIdle:        minIdle,\n\t\tmaxIdle:        maxIdle,\n\t\tmaxIdleTimeout: maxIdleTimeout,\n\t}\n\treturn p\n}\n\n// pool implements a pool of long connections.\ntype pool struct {\n\tidleList []*longConn // idleList Get/Put by FILO(stack) but Evict by FIFO(queue)\n\tmu       sync.RWMutex\n\t// config\n\tminIdle        int\n\tmaxIdle        int           // currIdle <= maxIdle.\n\tmaxIdleTimeout time.Duration // the idle connection will be cleaned if the idle time exceeds maxIdleTimeout.\n}\n\n// Get gets the first active connection from the idleList. Return the number of connections decreased during the Get.\nfunc (p *pool) Get() (*longConn, bool, int) {\n\tp.mu.Lock()\n\t// Get the first active one\n\tn := len(p.idleList)\n\tselected := n - 1\n\tfor ; selected >= 0; selected-- {\n\t\to := p.idleList[selected]\n\t\t// reset slice element to nil, active conn object only could be hold reference by user function\n\t\tp.idleList[selected] = nil\n\t\tif o.IsActive() {\n\t\t\tp.idleList = p.idleList[:selected]\n\t\t\tp.mu.Unlock()\n\t\t\treturn o, true, n - selected\n\t\t}\n\t\t// inactive object\n\t\to.Close()\n\t}\n\t// in case all objects are inactive\n\tif selected < 0 {\n\t\tselected = 0\n\t}\n\tp.idleList = p.idleList[:selected]\n\tp.mu.Unlock()\n\treturn nil, false, n - selected\n}\n\n// Put puts back a connection to the pool.\n\n\n\n\n\n\n\n\n\n\n\n\n// Evict cleanups the expired connections.\n// Evict returns how many connections has been evicted.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Len returns the length of the pool.\nfunc (p *pool) Len() int {\n\tp.mu.RLock()\n\tl := len(p.idleList)\n\tp.mu.RUnlock()\n\treturn l\n}\n\n// Close closes the pool and all the objects in the pool.\nfunc (p *pool) Close() int {\n\tp.mu.Lock()\n\tnum := len(p.idleList)\n\tfor i := 0; i < num; i++ {\n\t\tp.idleList[i].Close()\n\t}\n\tp.idleList = nil\n\n\tp.mu.Unlock()\n\treturn num\n}\n\n// Dump dumps the info of all the objects in the pool.\nfunc (p *pool) Dump() PoolDump {\n\tp.mu.RLock()\n\tidleNum := len(p.idleList)\n\tconnsDeadline := make([]time.Time, idleNum)\n\tfor i := 0; i < idleNum; i++ {\n\t\tconnsDeadline[i] = p.idleList[i].deadline\n\t}\n\ts := PoolDump{\n\t\tIdleNum:       idleNum,\n\t\tConnsDeadline: connsDeadline,\n\t}\n\tp.mu.RUnlock()\n\treturn s\n}\n\nfunc newPeer(\n\tserviceName string,\n\taddr net.Addr,\n\tminIdle int,\n\tmaxIdle int,\n\tmaxIdleTimeout time.Duration,\n\tglobalIdle *utils.MaxCounter,\n) *peer {\n\treturn &peer{\n\t\tserviceName: serviceName,\n\t\taddr:        addr,\n\t\tglobalIdle:  globalIdle,\n\t\tpool:        newPool(minIdle, maxIdle, maxIdleTimeout),\n\t}\n}\n\n// peer has one address, it manages all connections base on this address\ntype peer struct {\n\t// info\n\tserviceName string\n\taddr        net.Addr\n\tglobalIdle  *utils.MaxCounter\n\t// pool\n\tpool *pool\n}\n\n// Get gets a connection with dialer and timeout. Dial a new connection if no idle connection in pool is available.\nfunc (p *peer) Get(d remote.Dialer, timeout time.Duration, reporter Reporter, addr string) (net.Conn, error) {\n\tvar c net.Conn\n\tc, reused, decNum := p.pool.Get()\n\tp.globalIdle.DecN(int64(decNum))\n\tif reused {\n\t\treporter.ReuseSucceed(Long, p.serviceName, p.addr)\n\t\treturn c, nil\n\t}\n\t// dial a new connection\n\tc, err := d.DialTimeout(p.addr.Network(), p.addr.String(), timeout)\n\tif err != nil {\n\t\treporter.ConnFailed(Long, p.serviceName, p.addr)\n\t\treturn nil, err\n\t}\n\treporter.ConnSucceed(Long, p.serviceName, p.addr)\n\treturn &longConn{\n\t\tConn:    c,\n\t\taddress: addr,\n\t}, nil\n}\n\n// Put puts a connection back to the peer.\nfunc (p *peer) Put(c *longConn) error {\n\tif !p.globalIdle.Inc() {\n\t\treturn c.Close()\n\t}\n\tif !p.pool.Put(c) {\n\t\tp.globalIdle.Dec()\n\t\treturn c.Close()\n\t}\n\treturn nil\n}\n\nfunc (p *peer) Len() int {\n\treturn p.pool.Len()\n}\n\nfunc (p *peer) Evict() {\n\tn := p.pool.Evict()\n\tp.globalIdle.DecN(int64(n))\n}\n\n// Close closes the peer and all the connections in the ring.\nfunc (p *peer) Close() {\n\tn := p.pool.Close()\n\tp.globalIdle.DecN(int64(n))\n}\n\n// NewLongPool creates a long pool using the given IdleConfig.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// LongPool manages a pool of long connections.\ntype LongPool struct {\n\treporter     Reporter\n\tpeerMap      sync.Map\n\tnewPeer      func(net.Addr) *peer\n\tglobalIdle   *utils.MaxCounter\n\tidleConfig   connpool.IdleConfig\n\tsharedTicker *utils.SharedTicker\n\tclosed       int32 // active: 0, closed: 1\n}\n\n// Get pick or generate a net.Conn and return\n// The context is not used but leave it for now.\nfunc (lp *LongPool) Get(ctx context.Context, network, address string, opt remote.ConnOption) (net.Conn, error) {\n\taddr := netAddr{network, address}\n\tp := lp.getPeer(addr)\n\treturn p.Get(opt.Dialer, opt.ConnectTimeout, lp.reporter, address)\n}\n\n// Put implements the ConnPool interface.\nfunc (lp *LongPool) Put(conn net.Conn) error {\n\tc, ok := conn.(*longConn)\n\tif !ok {\n\t\treturn conn.Close()\n\t}\n\n\taddr := conn.RemoteAddr()\n\tna := netAddr{addr.Network(), c.address}\n\tp, ok := lp.peerMap.Load(na)\n\tif ok {\n\t\tp.(*peer).Put(c)\n\t\treturn nil\n\t}\n\treturn c.Conn.Close()\n}\n\n// Discard implements the ConnPool interface.\nfunc (lp *LongPool) Discard(conn net.Conn) error {\n\tc, ok := conn.(*longConn)\n\tif ok {\n\t\treturn c.Close()\n\t}\n\treturn conn.Close()\n}\n\n// Clean implements the LongConnPool interface.\nfunc (lp *LongPool) Clean(network, address string) {\n\tna := netAddr{network, address}\n\tif p, ok := lp.peerMap.Load(na); ok {\n\t\tlp.peerMap.Delete(na)\n\t\tgo p.(*peer).Close()\n\t}\n}\n\n// Dump is used to dump current long pool info when needed, like debug query.\nfunc (lp *LongPool) Dump() interface{} {\n\tm := make(map[string]interface{})\n\tm[configDumpKey] = lp.idleConfig\n\tlp.peerMap.Range(func(key, value interface{}) bool {\n\t\tt := value.(*peer).pool.Dump()\n\t\tm[key.(netAddr).String()] = t\n\t\treturn true\n\t})\n\treturn m\n}\n\n// Close releases all peers in the pool, it is executed when client is closed.\nfunc (lp *LongPool) Close() error {\n\tif !atomic.CompareAndSwapInt32(&lp.closed, 0, 1) {\n\t\treturn fmt.Errorf(\"long pool is already closed\")\n\t}\n\t// close all peers\n\tlp.peerMap.Range(func(addr, value interface{}) bool {\n\t\tlp.peerMap.Delete(addr)\n\t\tv := value.(*peer)\n\t\tv.Close()\n\t\treturn true\n\t})\n\t// remove from the shared ticker\n\tlp.sharedTicker.Delete(lp)\n\treturn nil\n}\n\n// EnableReporter enable reporter for long connection pool.\nfunc (lp *LongPool) EnableReporter() {\n\tlp.reporter = GetCommonReporter()\n}\n\n// WarmUp implements the warmup.Pool interface.\nfunc (lp *LongPool) WarmUp(eh warmup.ErrorHandling, wuo *warmup.PoolOption, co remote.ConnOption) error {\n\th := &warmup.PoolHelper{ErrorHandling: eh}\n\treturn h.WarmUp(wuo, lp, co)\n}\n\n// Evict cleanups the idle connections in peers.\nfunc (lp *LongPool) Evict() {\n\tif atomic.LoadInt32(&lp.closed) == 0 {\n\t\t// Evict idle connections\n\t\tlp.peerMap.Range(func(key, value interface{}) bool {\n\t\t\tp := value.(*peer)\n\t\t\tp.Evict()\n\t\t\treturn true\n\t\t})\n\t}\n}\n\n// Tick implements the interface utils.TickerTask.\nfunc (lp *LongPool) Tick() {\n\tlp.Evict()\n}\n\n// getPeer gets a peer from the pool based on the addr, or create a new one if not exist.\nfunc (lp *LongPool) getPeer(addr netAddr) *peer {\n\tp, ok := lp.peerMap.Load(addr)\n\tif ok {\n\t\treturn p.(*peer)\n\t}\n\tp, _ = lp.peerMap.LoadOrStore(addr, lp.newPeer(addr))\n\treturn p.(*peer)\n}\n\n```\n# Function Signature: func (p *pool) Evict() (evicted int) {\n# Natural Language Annotations: // Evict cleanups the expired connections.\n// Evict returns how many connections has been evicted.\nPlease return the generated code surrounded by ```\n", "response": ["Sure, here is the generated code:\n\n```go\nfunc (p *pool) Evict() (evicted int) {\n\tp.mu.Lock()\n\tnum := len(p.idleList)\n\tevicted = 0\n\tfor i := 0; i < num; i++ {\n\t\to := p.idleList[i]\n\t\tif o.Expired() {\n\t\t\t// remove from idelist and cleanup\n\t\t\tp.idleList = append(p.idleList[:i], p.idleList[i+1:]...)\n\t\t\to.Close()\n\t\t\tevicted++\n\t\t\tnum--\n\t\t\ti--\n\t\t}\n\t}\n\tp.mu.Unlock()\n\treturn evicted\n}\n```\n\nThis function `Evict()` removes and closes all the expired connections in the pool. It returns the number of connections that have been evicted. The connections are removed from the `idleList` and closed. The `Evict()` function is called when the connection is no longer being used and can be safely removed from the pool.", "```go\nfunc (p *pool) Evict() (evicted int) {\n\tp.mu.Lock()\n\tn := len(p.idleList)\n\tevicted = 0\n\ti := 0\n\tfor ; i < n; i++ {\n\t\tif p.idleList[i].Expired() {\n\t\t\tp.idleList[i].Close()\n\t\t\tp.idleList[i] = nil\n\t\t\tevicted++\n\t\t} else {\n\t\t\tbreak\n\t\t}\n\t}\n\tp.idleList = p.idleList[i:]\n\tp.mu.Unlock()\n\treturn evicted\n}\n```", "```go\nfunc (p *pool) Evict() (evicted int) {\n\tp.mu.Lock()\n\tevicted = 0\n\tcurrentTime := time.Now()\n\tfor i := 0; i < len(p.idleList); i++ {\n\t\tif currentTime.After(p.idleList[i].deadline) {\n\t\t\tp.idleList[i].Close()\n\t\t\tp.idleList = append(p.idleList[:i], p.idleList[i+1:]...)\n\t\t\tevicted++\n\t\t\ti-- // since we just removed an element, we need to decrement i to account for the new position of the next element\n\t\t}\n\t}\n\tp.mu.Unlock()\n\treturn evicted\n}\n```"]}, "kitex-pkg/remote/connpool/long_pool.go-NewLongPool": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/remote/connpool/long_pool.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package connpool provide short connection and long connection pool.\npackage connpool\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n)\n\nvar (\n\t_ net.Conn            = &longConn{}\n\t_ remote.LongConnPool = &LongPool{}\n\n\t// global shared tickers for different LongPool\n\tsharedTickers sync.Map\n)\n\nconst (\n\tconfigDumpKey = \"idle_config\"\n)\n\nfunc getSharedTicker(p *LongPool, refreshInterval time.Duration) *utils.SharedTicker {\n\tsti, ok := sharedTickers.Load(refreshInterval)\n\tif ok {\n\t\tst := sti.(*utils.SharedTicker)\n\t\tst.Add(p)\n\t\treturn st\n\t}\n\tsti, _ = sharedTickers.LoadOrStore(refreshInterval, utils.NewSharedTicker(refreshInterval))\n\tst := sti.(*utils.SharedTicker)\n\tst.Add(p)\n\treturn st\n}\n\n// netAddr implements the net.Addr interface and comparability.\ntype netAddr struct {\n\tnetwork string\n\taddress string\n}\n\n// Network implements the net.Addr interface.\nfunc (na netAddr) Network() string { return na.network }\n\n// String implements the net.Addr interface.\nfunc (na netAddr) String() string { return na.address }\n\n// longConn implements the net.Conn interface.\ntype longConn struct {\n\tnet.Conn\n\tsync.RWMutex\n\tdeadline time.Time\n\taddress  string\n}\n\n// Close implements the net.Conn interface.\nfunc (c *longConn) Close() error {\n\treturn c.Conn.Close()\n}\n\n// RawConn returns the real underlying net.Conn.\nfunc (c *longConn) RawConn() net.Conn {\n\treturn c.Conn\n}\n\n// IsActive indicates whether the connection is active.\n\n\n\n\n\n\n\n\n// Expired checks the deadline of the connection.\nfunc (c *longConn) Expired() bool {\n\treturn time.Now().After(c.deadline)\n}\n\ntype PoolDump struct {\n\tIdleNum       int         `json:\"idle_num\"`\n\tConnsDeadline []time.Time `json:\"conns_deadline\"`\n}\n\nfunc newPool(minIdle, maxIdle int, maxIdleTimeout time.Duration) *pool {\n\tp := &pool{\n\t\tidleList:       make([]*longConn, 0, maxIdle),\n\t\tminIdle:        minIdle,\n\t\tmaxIdle:        maxIdle,\n\t\tmaxIdleTimeout: maxIdleTimeout,\n\t}\n\treturn p\n}\n\n// pool implements a pool of long connections.\ntype pool struct {\n\tidleList []*longConn // idleList Get/Put by FILO(stack) but Evict by FIFO(queue)\n\tmu       sync.RWMutex\n\t// config\n\tminIdle        int\n\tmaxIdle        int           // currIdle <= maxIdle.\n\tmaxIdleTimeout time.Duration // the idle connection will be cleaned if the idle time exceeds maxIdleTimeout.\n}\n\n// Get gets the first active connection from the idleList. Return the number of connections decreased during the Get.\nfunc (p *pool) Get() (*longConn, bool, int) {\n\tp.mu.Lock()\n\t// Get the first active one\n\tn := len(p.idleList)\n\tselected := n - 1\n\tfor ; selected >= 0; selected-- {\n\t\to := p.idleList[selected]\n\t\t// reset slice element to nil, active conn object only could be hold reference by user function\n\t\tp.idleList[selected] = nil\n\t\tif o.IsActive() {\n\t\t\tp.idleList = p.idleList[:selected]\n\t\t\tp.mu.Unlock()\n\t\t\treturn o, true, n - selected\n\t\t}\n\t\t// inactive object\n\t\to.Close()\n\t}\n\t// in case all objects are inactive\n\tif selected < 0 {\n\t\tselected = 0\n\t}\n\tp.idleList = p.idleList[:selected]\n\tp.mu.Unlock()\n\treturn nil, false, n - selected\n}\n\n// Put puts back a connection to the pool.\n\n\n\n\n\n\n\n\n\n\n\n\n// Evict cleanups the expired connections.\n// Evict returns how many connections has been evicted.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Len returns the length of the pool.\nfunc (p *pool) Len() int {\n\tp.mu.RLock()\n\tl := len(p.idleList)\n\tp.mu.RUnlock()\n\treturn l\n}\n\n// Close closes the pool and all the objects in the pool.\nfunc (p *pool) Close() int {\n\tp.mu.Lock()\n\tnum := len(p.idleList)\n\tfor i := 0; i < num; i++ {\n\t\tp.idleList[i].Close()\n\t}\n\tp.idleList = nil\n\n\tp.mu.Unlock()\n\treturn num\n}\n\n// Dump dumps the info of all the objects in the pool.\nfunc (p *pool) Dump() PoolDump {\n\tp.mu.RLock()\n\tidleNum := len(p.idleList)\n\tconnsDeadline := make([]time.Time, idleNum)\n\tfor i := 0; i < idleNum; i++ {\n\t\tconnsDeadline[i] = p.idleList[i].deadline\n\t}\n\ts := PoolDump{\n\t\tIdleNum:       idleNum,\n\t\tConnsDeadline: connsDeadline,\n\t}\n\tp.mu.RUnlock()\n\treturn s\n}\n\nfunc newPeer(\n\tserviceName string,\n\taddr net.Addr,\n\tminIdle int,\n\tmaxIdle int,\n\tmaxIdleTimeout time.Duration,\n\tglobalIdle *utils.MaxCounter,\n) *peer {\n\treturn &peer{\n\t\tserviceName: serviceName,\n\t\taddr:        addr,\n\t\tglobalIdle:  globalIdle,\n\t\tpool:        newPool(minIdle, maxIdle, maxIdleTimeout),\n\t}\n}\n\n// peer has one address, it manages all connections base on this address\ntype peer struct {\n\t// info\n\tserviceName string\n\taddr        net.Addr\n\tglobalIdle  *utils.MaxCounter\n\t// pool\n\tpool *pool\n}\n\n// Get gets a connection with dialer and timeout. Dial a new connection if no idle connection in pool is available.\nfunc (p *peer) Get(d remote.Dialer, timeout time.Duration, reporter Reporter, addr string) (net.Conn, error) {\n\tvar c net.Conn\n\tc, reused, decNum := p.pool.Get()\n\tp.globalIdle.DecN(int64(decNum))\n\tif reused {\n\t\treporter.ReuseSucceed(Long, p.serviceName, p.addr)\n\t\treturn c, nil\n\t}\n\t// dial a new connection\n\tc, err := d.DialTimeout(p.addr.Network(), p.addr.String(), timeout)\n\tif err != nil {\n\t\treporter.ConnFailed(Long, p.serviceName, p.addr)\n\t\treturn nil, err\n\t}\n\treporter.ConnSucceed(Long, p.serviceName, p.addr)\n\treturn &longConn{\n\t\tConn:    c,\n\t\taddress: addr,\n\t}, nil\n}\n\n// Put puts a connection back to the peer.\nfunc (p *peer) Put(c *longConn) error {\n\tif !p.globalIdle.Inc() {\n\t\treturn c.Close()\n\t}\n\tif !p.pool.Put(c) {\n\t\tp.globalIdle.Dec()\n\t\treturn c.Close()\n\t}\n\treturn nil\n}\n\nfunc (p *peer) Len() int {\n\treturn p.pool.Len()\n}\n\nfunc (p *peer) Evict() {\n\tn := p.pool.Evict()\n\tp.globalIdle.DecN(int64(n))\n}\n\n// Close closes the peer and all the connections in the ring.\nfunc (p *peer) Close() {\n\tn := p.pool.Close()\n\tp.globalIdle.DecN(int64(n))\n}\n\n// NewLongPool creates a long pool using the given IdleConfig.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// LongPool manages a pool of long connections.\ntype LongPool struct {\n\treporter     Reporter\n\tpeerMap      sync.Map\n\tnewPeer      func(net.Addr) *peer\n\tglobalIdle   *utils.MaxCounter\n\tidleConfig   connpool.IdleConfig\n\tsharedTicker *utils.SharedTicker\n\tclosed       int32 // active: 0, closed: 1\n}\n\n// Get pick or generate a net.Conn and return\n// The context is not used but leave it for now.\nfunc (lp *LongPool) Get(ctx context.Context, network, address string, opt remote.ConnOption) (net.Conn, error) {\n\taddr := netAddr{network, address}\n\tp := lp.getPeer(addr)\n\treturn p.Get(opt.Dialer, opt.ConnectTimeout, lp.reporter, address)\n}\n\n// Put implements the ConnPool interface.\nfunc (lp *LongPool) Put(conn net.Conn) error {\n\tc, ok := conn.(*longConn)\n\tif !ok {\n\t\treturn conn.Close()\n\t}\n\n\taddr := conn.RemoteAddr()\n\tna := netAddr{addr.Network(), c.address}\n\tp, ok := lp.peerMap.Load(na)\n\tif ok {\n\t\tp.(*peer).Put(c)\n\t\treturn nil\n\t}\n\treturn c.Conn.Close()\n}\n\n// Discard implements the ConnPool interface.\nfunc (lp *LongPool) Discard(conn net.Conn) error {\n\tc, ok := conn.(*longConn)\n\tif ok {\n\t\treturn c.Close()\n\t}\n\treturn conn.Close()\n}\n\n// Clean implements the LongConnPool interface.\nfunc (lp *LongPool) Clean(network, address string) {\n\tna := netAddr{network, address}\n\tif p, ok := lp.peerMap.Load(na); ok {\n\t\tlp.peerMap.Delete(na)\n\t\tgo p.(*peer).Close()\n\t}\n}\n\n// Dump is used to dump current long pool info when needed, like debug query.\nfunc (lp *LongPool) Dump() interface{} {\n\tm := make(map[string]interface{})\n\tm[configDumpKey] = lp.idleConfig\n\tlp.peerMap.Range(func(key, value interface{}) bool {\n\t\tt := value.(*peer).pool.Dump()\n\t\tm[key.(netAddr).String()] = t\n\t\treturn true\n\t})\n\treturn m\n}\n\n// Close releases all peers in the pool, it is executed when client is closed.\nfunc (lp *LongPool) Close() error {\n\tif !atomic.CompareAndSwapInt32(&lp.closed, 0, 1) {\n\t\treturn fmt.Errorf(\"long pool is already closed\")\n\t}\n\t// close all peers\n\tlp.peerMap.Range(func(addr, value interface{}) bool {\n\t\tlp.peerMap.Delete(addr)\n\t\tv := value.(*peer)\n\t\tv.Close()\n\t\treturn true\n\t})\n\t// remove from the shared ticker\n\tlp.sharedTicker.Delete(lp)\n\treturn nil\n}\n\n// EnableReporter enable reporter for long connection pool.\nfunc (lp *LongPool) EnableReporter() {\n\tlp.reporter = GetCommonReporter()\n}\n\n// WarmUp implements the warmup.Pool interface.\nfunc (lp *LongPool) WarmUp(eh warmup.ErrorHandling, wuo *warmup.PoolOption, co remote.ConnOption) error {\n\th := &warmup.PoolHelper{ErrorHandling: eh}\n\treturn h.WarmUp(wuo, lp, co)\n}\n\n// Evict cleanups the idle connections in peers.\nfunc (lp *LongPool) Evict() {\n\tif atomic.LoadInt32(&lp.closed) == 0 {\n\t\t// Evict idle connections\n\t\tlp.peerMap.Range(func(key, value interface{}) bool {\n\t\t\tp := value.(*peer)\n\t\t\tp.Evict()\n\t\t\treturn true\n\t\t})\n\t}\n}\n\n// Tick implements the interface utils.TickerTask.\nfunc (lp *LongPool) Tick() {\n\tlp.Evict()\n}\n\n// getPeer gets a peer from the pool based on the addr, or create a new one if not exist.\nfunc (lp *LongPool) getPeer(addr netAddr) *peer {\n\tp, ok := lp.peerMap.Load(addr)\n\tif ok {\n\t\treturn p.(*peer)\n\t}\n\tp, _ = lp.peerMap.LoadOrStore(addr, lp.newPeer(addr))\n\treturn p.(*peer)\n}\n\n```\n# Function Signature: func NewLongPool(serviceName string, idlConfig connpool.IdleConfig) *LongPool {\n# Natural Language Annotations: // NewLongPool creates a long pool using the given IdleConfig.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewLongPool(serviceName string, idlConfig connpool.IdleConfig) *LongPool {\n    // Create a new long pool\n    lp := &LongPool{\n        serviceName: serviceName,\n        idleConfig:  idlConfig,\n        globalIdle:  utils.NewMaxCounter(),\n    }\n\n    // Create a new peer\n    lp.newPeer = func(addr net.Addr) *peer {\n        return newPeer(\n            serviceName,\n            addr,\n            idlConfig.MinIdle,\n            idlConfig.MaxIdle,\n            time.Duration(idlConfig.MaxIdleTimeout)*time.Second,\n            lp.globalIdle,\n        )\n    }\n\n    // Get shared ticker\n    lp.sharedTicker = getSharedTicker(lp, time.Duration(idlConfig.RefreshInterval)*time.Second)\n\n    return lp\n}\n```", "```go\nfunc NewLongPool(serviceName string, idlConfig connpool.IdleConfig) *LongPool {\n\tglobalIdle := utils.NewMaxCounter()\n\tnewPeer := func(addr net.Addr) *peer {\n\t\treturn newPeer(\n\t\t\tserviceName,\n\t\t\taddr,\n\t\t\tidlConfig.MinIdle,\n\t\t\tidlConfig.MaxIdle,\n\t\t\tidlConfig.MaxIdleTimeout,\n\t\t\tglobalIdle,\n\t\t)\n\t}\n\tsharedTicker := getSharedTicker(&LongPool{}, idlConfig.RefreshInterval)\n\treturn &LongPool{\n\t\treporter:     nil,\n\t\tpeerMap:      sync.Map{},\n\t\tnewPeer:      newPeer,\n\t\tglobalIdle:   globalIdle,\n\t\tidleConfig:   idlConfig,\n\t\tsharedTicker: sharedTicker,\n\t\tclosed:       0,\n\t}\n}\n```", "```go\nfunc NewLongPool(serviceName string, idleConfig connpool.IdleConfig) *LongPool {\n    globalIdle := utils.NewMaxCounter()\n\n    sharedTicker := getSharedTicker(&LongPool{}, idleConfig.RefreshInterval)\n\n    newPeer := func(addr net.Addr) *peer {\n        return newPeer(\n            serviceName,\n            addr,\n            idleConfig.MinIdle,\n            idleConfig.MaxIdle,\n            idleConfig.MaxIdleTimeout,\n            globalIdle,\n        )\n    }\n\n    return &LongPool{\n        reporter:     nil,\n        peerMap:      sync.Map{},\n        newPeer:      newPeer,\n        globalIdle:   globalIdle,\n        idleConfig:   idleConfig,\n        sharedTicker: sharedTicker,\n        closed:       0,\n    }\n}\n```"]}, "kitex-pkg/loadbalance/lbcache/cache.go-NewBalancerFactory": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/loadbalance/lbcache/cache.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package lbcache combine balancer with resolver and cache the resolve result\npackage lbcache\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"golang.org/x/sync/singleflight\"\n\n\t\"github.com/cloudwego/kitex/pkg/diagnosis\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\nconst (\n\tdefaultRefreshInterval = 5 * time.Second\n\tdefaultExpireInterval  = 15 * time.Second\n)\n\nvar (\n\tbalancerFactories    sync.Map // key: resolver name + loadbalance name\n\tbalancerFactoriesSfg singleflight.Group\n)\n\n// Options for create builder\ntype Options struct {\n\t// refresh discovery result timely\n\tRefreshInterval time.Duration\n\n\t// Balancer expire check interval\n\t// we need remove idle Balancers for resource saving\n\tExpireInterval time.Duration\n\n\t// DiagnosisService is used register info for diagnosis\n\tDiagnosisService diagnosis.Service\n\n\t// Cacheable is used to indicate that if the factory could be shared between multi clients\n\tCacheable bool\n}\n\nfunc (v *Options) check() {\n\tif v.RefreshInterval <= 0 {\n\t\tv.RefreshInterval = defaultRefreshInterval\n\t}\n\tif v.ExpireInterval <= 0 {\n\t\tv.ExpireInterval = defaultExpireInterval\n\t}\n}\n\n// Hookable add hook for rebalancer events\ntype Hookable interface {\n\t// register loadbalance rebalance hook for Rebalance events\n\tRegisterRebalanceHook(func(ch *discovery.Change)) (index int)\n\tDeregisterRebalanceHook(index int)\n\t// register loadbalance delete hook for Delete events\n\tRegisterDeleteHook(func(ch *discovery.Change)) (index int)\n\tDeregisterDeleteHook(index int)\n}\n\n// BalancerFactory get or create a balancer with given target\n// if it has the same key(reslover.Target(target)), we will cache and reuse the Balance\ntype BalancerFactory struct {\n\tHookable\n\topts       Options\n\tcache      sync.Map // key -> LoadBalancer\n\tresolver   discovery.Resolver\n\tbalancer   loadbalance.Loadbalancer\n\trebalancer loadbalance.Rebalancer\n\tsfg        singleflight.Group\n}\n\nfunc cacheKey(resolver, balancer string, opts Options) string {\n\treturn fmt.Sprintf(\"%s|%s|{%s %s}\", resolver, balancer, opts.RefreshInterval, opts.ExpireInterval)\n}\n\nfunc newBalancerFactory(resolver discovery.Resolver, balancer loadbalance.Loadbalancer, opts Options) *BalancerFactory {\n\tb := &BalancerFactory{\n\t\topts:     opts,\n\t\tresolver: resolver,\n\t\tbalancer: balancer,\n\t}\n\tif rb, ok := balancer.(loadbalance.Rebalancer); ok {\n\t\thrb := newHookRebalancer(rb)\n\t\tb.rebalancer = hrb\n\t\tb.Hookable = hrb\n\t} else {\n\t\tb.Hookable = noopHookRebalancer{}\n\t}\n\tgo b.watcher()\n\treturn b\n}\n\n// NewBalancerFactory get or create a balancer factory for balancer instance\n// cache key with resolver name, balancer name and options\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// watch expired balancer\nfunc (b *BalancerFactory) watcher() {\n\tfor range time.Tick(b.opts.ExpireInterval) {\n\t\tb.cache.Range(func(key, value interface{}) bool {\n\t\t\tbl := value.(*Balancer)\n\t\t\tif atomic.CompareAndSwapInt32(&bl.expire, 0, 1) {\n\t\t\t\t// 1. set expire flag\n\t\t\t\t// 2. wait next ticker for collect, maybe the balancer is used again\n\t\t\t\t// (avoid being immediate delete the balancer which had been created recently)\n\t\t\t} else {\n\t\t\t\tb.cache.Delete(key)\n\t\t\t\tbl.close()\n\t\t\t}\n\t\t\treturn true\n\t\t})\n\t}\n}\n\n// cache key with resolver name prefix avoid conflict for balancer\nfunc renameResultCacheKey(res *discovery.Result, resolverName string) {\n\tres.CacheKey = resolverName + \":\" + res.CacheKey\n}\n\n// Get create a new balancer if not exists\nfunc (b *BalancerFactory) Get(ctx context.Context, target rpcinfo.EndpointInfo) (*Balancer, error) {\n\tdesc := b.resolver.Target(ctx, target)\n\tval, ok := b.cache.Load(desc)\n\tif ok {\n\t\treturn val.(*Balancer), nil\n\t}\n\tval, err, _ := b.sfg.Do(desc, func() (interface{}, error) {\n\t\tres, err := b.resolver.Resolve(ctx, desc)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\trenameResultCacheKey(&res, b.resolver.Name())\n\t\tbl := &Balancer{\n\t\t\tb:      b,\n\t\t\ttarget: desc,\n\t\t}\n\t\tbl.res.Store(res)\n\t\tbl.sharedTicker = getSharedTicker(bl, b.opts.RefreshInterval)\n\t\tb.cache.Store(desc, bl)\n\t\treturn bl, nil\n\t})\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn val.(*Balancer), nil\n}\n\n// Balancer same with loadbalance.Loadbalancer but without resolver.Result that\n// has been cached\ntype Balancer struct {\n\tb            *BalancerFactory\n\ttarget       string       // a description returned from the resolver's Target method\n\tres          atomic.Value // newest and previous discovery result\n\texpire       int32        // 0 = normal, 1 = expire and collect next ticker\n\tsharedTicker *utils.SharedTicker\n}\n\nfunc (bl *Balancer) Refresh() {\n\tres, err := bl.b.resolver.Resolve(context.Background(), bl.target)\n\tif err != nil {\n\t\tklog.Warnf(\"KITEX: resolver refresh failed, key=%s error=%s\", bl.target, err.Error())\n\t\treturn\n\t}\n\trenameResultCacheKey(&res, bl.b.resolver.Name())\n\tprev := bl.res.Load().(discovery.Result)\n\tif bl.b.rebalancer != nil {\n\t\tif ch, ok := bl.b.resolver.Diff(res.CacheKey, prev, res); ok {\n\t\t\tbl.b.rebalancer.Rebalance(ch)\n\t\t}\n\t}\n\t// replace previous result\n\tbl.res.Store(res)\n}\n\n// Tick implements the interface utils.TickerTask.\nfunc (bl *Balancer) Tick() {\n\tbl.Refresh()\n}\n\n// GetResult returns the discovery result that the Balancer holds.\nfunc (bl *Balancer) GetResult() (res discovery.Result, ok bool) {\n\tif v := bl.res.Load(); v != nil {\n\t\treturn v.(discovery.Result), true\n\t}\n\treturn\n}\n\n// GetPicker equal to loadbalance.Balancer without pass discovery.Result, because we cache the result\nfunc (bl *Balancer) GetPicker() loadbalance.Picker {\n\tatomic.StoreInt32(&bl.expire, 0)\n\tres := bl.res.Load().(discovery.Result)\n\treturn bl.b.balancer.GetPicker(res)\n}\n\nfunc (bl *Balancer) close() {\n\t// notice the under rebalancer\n\tif rb, ok := bl.b.balancer.(loadbalance.Rebalancer); ok {\n\t\t// notice to rebalancing\n\t\trb.Delete(discovery.Change{\n\t\t\tResult: discovery.Result{\n\t\t\t\tCacheable: true,\n\t\t\t\tCacheKey:  bl.res.Load().(discovery.Result).CacheKey,\n\t\t\t},\n\t\t})\n\t}\n\t// delete from sharedTicker\n\tbl.sharedTicker.Delete(bl)\n}\n\nconst unknown = \"unknown\"\n\nfunc Dump() interface{} {\n\ttype instInfo struct {\n\t\tAddress string\n\t\tWeight  int\n\t}\n\tcacheDump := make(map[string]interface{})\n\tbalancerFactories.Range(func(key, val interface{}) bool {\n\t\tcacheKey := key.(string)\n\t\tif bf, ok := val.(*BalancerFactory); ok {\n\t\t\trouteMap := make(map[string]interface{})\n\t\t\tcacheDump[cacheKey] = routeMap\n\t\t\tbf.cache.Range(func(k, v interface{}) bool {\n\t\t\t\trouteKey := k.(string)\n\t\t\t\tif bl, ok := v.(*Balancer); ok {\n\t\t\t\t\tif dr, ok := bl.res.Load().(discovery.Result); ok {\n\t\t\t\t\t\tinsts := make([]instInfo, 0, len(dr.Instances))\n\t\t\t\t\t\tfor i := range dr.Instances {\n\t\t\t\t\t\t\tinst := dr.Instances[i]\n\t\t\t\t\t\t\taddr := fmt.Sprintf(\"%s://%s\", inst.Address().Network(), inst.Address().String())\n\t\t\t\t\t\t\tinsts = append(insts, instInfo{Address: addr, Weight: inst.Weight()})\n\t\t\t\t\t\t}\n\t\t\t\t\t\trouteMap[routeKey] = insts\n\t\t\t\t\t} else {\n\t\t\t\t\t\trouteMap[routeKey] = unknown\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\trouteMap[routeKey] = unknown\n\t\t\t\t}\n\t\t\t\treturn true\n\t\t\t})\n\t\t} else {\n\t\t\tcacheDump[cacheKey] = unknown\n\t\t}\n\t\treturn true\n\t})\n\treturn cacheDump\n}\n\n```\n# Function Signature: func NewBalancerFactory(resolver discovery.Resolver, balancer loadbalance.Loadbalancer, opts Options) *BalancerFactory {\n# Natural Language Annotations: // NewBalancerFactory get or create a balancer factory for balancer instance\n// cache key with resolver name, balancer name and options\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewBalancerFactory(resolver discovery.Resolver, balancer loadbalance.Loadbalancer, opts Options) *BalancerFactory {\n\tcacheKey := cacheKey(resolver.Name(), balancer.Name(), opts)\n\tval, ok := balancerFactories.Load(cacheKey)\n\tif ok {\n\t\treturn val.(*BalancerFactory)\n\t}\n\tval, err, _ := balancerFactoriesSfg.Do(cacheKey, func() (interface{}, error) {\n\t\tb := newBalancerFactory(resolver, balancer, opts)\n\t\tbalancerFactories.Store(cacheKey, b)\n\t\treturn b, nil\n\t})\n\tif err != nil {\n\t\treturn nil\n\t}\n\treturn val.(*BalancerFactory)\n}\n```", "```go\nfunc NewBalancerFactory(resolver discovery.Resolver, balancer loadbalance.Loadbalancer, opts Options) *BalancerFactory {\n\topts.check()\n\tcacheKey := cacheKey(resolver.Name(), balancer.Name(), opts)\n\n\t// Check if a balancer factory for this key already exists in the cache\n\tif val, ok := balancerFactories.Load(cacheKey); ok {\n\t\treturn val.(*BalancerFactory)\n\t}\n\n\t// If not, create a new balancer factory and store it in the cache\n\tbalancerFactory := newBalancerFactory(resolver, balancer, opts)\n\tbalancerFactories.Store(cacheKey, balancerFactory)\n\n\treturn balancerFactory\n}\n```", "```go\nfunc NewBalancerFactory(resolver discovery.Resolver, balancer loadbalance.Loadbalancer, opts Options) *BalancerFactory {\n\topts.check()\n\tcacheKey := cacheKey(resolver.Name(), balancer.Name(), opts)\n\tval, ok := balancerFactories.Load(cacheKey)\n\tif ok {\n\t\treturn val.(*BalancerFactory)\n\t}\n\tval, err, _ := balancerFactoriesSfg.Do(cacheKey, func() (interface{}, error) {\n\t\tb := newBalancerFactory(resolver, balancer, opts)\n\t\tbalancerFactories.Store(cacheKey, b)\n\t\treturn b, nil\n\t})\n\tif err != nil {\n\t\tklog.Errorf(\"KITEX: newBalancerFactory failed, key=%s error=%s\", cacheKey, err.Error())\n\t\treturn nil\n\t}\n\treturn val.(*BalancerFactory)\n}\n```"]}, "kitex-pkg/stats/event.go-DefineNewEvent": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/stats/event.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage stats\n\nimport (\n\t\"errors\"\n\t\"sync\"\n\t\"sync/atomic\"\n)\n\n// EventIndex indicates a unique event.\ntype EventIndex int\n\n// Level sets the record level.\ntype Level int\n\n// Event levels.\nconst (\n\tLevelDisabled Level = iota\n\tLevelBase\n\tLevelDetailed\n)\n\n// Event is used to indicate a specific event.\ntype Event interface {\n\tIndex() EventIndex\n\tLevel() Level\n}\n\ntype event struct {\n\tidx   EventIndex\n\tlevel Level\n}\n\n// Index implements the Event interface.\nfunc (e event) Index() EventIndex {\n\treturn e.idx\n}\n\n// Level implements the Event interface.\nfunc (e event) Level() Level {\n\treturn e.level\n}\n\nconst (\n\t_ EventIndex = iota\n\tserverHandleStart\n\tserverHandleFinish\n\tclientConnStart\n\tclientConnFinish\n\trpcStart\n\trpcFinish\n\treadStart\n\treadFinish\n\twaitReadStart\n\twaitReadFinish\n\twriteStart\n\twriteFinish\n\tstreamRecv\n\tstreamSend\n\n\t// NOTE: add new events before this line\n\tpredefinedEventNum\n)\n\n// Predefined events.\nvar (\n\tRPCStart  = newEvent(rpcStart, LevelBase)\n\tRPCFinish = newEvent(rpcFinish, LevelBase)\n\n\tServerHandleStart  = newEvent(serverHandleStart, LevelDetailed)\n\tServerHandleFinish = newEvent(serverHandleFinish, LevelDetailed)\n\tClientConnStart    = newEvent(clientConnStart, LevelDetailed)\n\tClientConnFinish   = newEvent(clientConnFinish, LevelDetailed)\n\tReadStart          = newEvent(readStart, LevelDetailed)\n\tReadFinish         = newEvent(readFinish, LevelDetailed)\n\tWaitReadStart      = newEvent(waitReadStart, LevelDetailed)\n\tWaitReadFinish     = newEvent(waitReadFinish, LevelDetailed)\n\tWriteStart         = newEvent(writeStart, LevelDetailed)\n\tWriteFinish        = newEvent(writeFinish, LevelDetailed)\n\n\t// Streaming Events\n\tStreamRecv = newEvent(streamRecv, LevelDetailed)\n\tStreamSend = newEvent(streamSend, LevelDetailed)\n)\n\n// errors\nvar (\n\tErrNotAllowed = errors.New(\"event definition is not allowed after initialization\")\n\tErrDuplicated = errors.New(\"event name is already defined\")\n)\n\nvar (\n\tlock        sync.RWMutex\n\tinited      int32\n\tuserDefined = make(map[string]Event)\n\tmaxEventNum = int(predefinedEventNum)\n)\n\n// FinishInitialization freezes all events defined and prevents further definitions to be added.\nfunc FinishInitialization() {\n\tlock.Lock()\n\tdefer lock.Unlock()\n\tatomic.StoreInt32(&inited, 1)\n}\n\n// DefineNewEvent allows user to add event definitions during program initialization.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// MaxEventNum returns the number of event defined.\nfunc MaxEventNum() int {\n\tlock.RLock()\n\tdefer lock.RUnlock()\n\treturn maxEventNum\n}\n\n// PredefinedEventNum returns the number of predefined events of kitex.\nfunc PredefinedEventNum() int {\n\treturn int(predefinedEventNum)\n}\n\nfunc newEvent(idx EventIndex, level Level) Event {\n\treturn event{\n\t\tidx:   idx,\n\t\tlevel: level,\n\t}\n}\n\n```\n# Function Signature: func DefineNewEvent(name string, level Level) (Event, error) {\n# Natural Language Annotations: // DefineNewEvent allows user to add event definitions during program initialization.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc DefineNewEvent(name string, level Level) (Event, error) {\n\tlock.Lock()\n\tdefer lock.Unlock()\n\n\tif atomic.LoadInt32(&inited) == 1 {\n\t\treturn nil, ErrNotAllowed\n\t}\n\n\tif _, exists := userDefined[name]; exists {\n\t\treturn nil, ErrDuplicated\n\t}\n\n\tnewEvent := event{\n\t\tidx:   EventIndex(maxEventNum),\n\t\tlevel: level,\n\t}\n\n\tuserDefined[name] = newEvent\n\tmaxEventNum++\n\n\treturn newEvent, nil\n}\n```", "```go\nfunc DefineNewEvent(name string, level Level) (Event, error) {\n\tlock.Lock()\n\tdefer lock.Unlock()\n\n\tif atomic.LoadInt32(&inited) == 1 {\n\t\treturn nil, ErrNotAllowed\n\t}\n\n\tif _, ok := userDefined[name]; ok {\n\t\treturn nil, ErrDuplicated\n\t}\n\n\tnewIdx := EventIndex(maxEventNum)\n\tmaxEventNum++\n\n\te := event{\n\t\tidx:   newIdx,\n\t\tlevel: level,\n\t}\n\tuserDefined[name] = e\n\n\treturn e, nil\n}\n```", "```go\nfunc DefineNewEvent(name string, level Level) (Event, error) {\n\tlock.Lock()\n\tdefer lock.Unlock()\n\n\tif atomic.LoadInt32(&inited) == 1 {\n\t\treturn nil, ErrNotAllowed\n\t}\n\n\tif _, ok := userDefined[name]; ok {\n\t\treturn nil, ErrDuplicated\n\t}\n\n\tnewIdx := EventIndex(maxEventNum)\n\tmaxEventNum++\n\tnewEvent := event{\n\t\tidx:   newIdx,\n\t\tlevel: level,\n\t}\n\n\tuserDefined[name] = newEvent\n\n\treturn newEvent, nil\n}\n```\n\nThis function `DefineNewEvent` allows user to add event definitions during program initialization. It takes a `name` and `level` as inputs, and returns an `Event` and an `error`. The input `name` is the name of the new event, and the input `level` is the level of the new event. \n\nThe function first checks if the initialization has been finished. If yes, it returns an error indicating that event definition is not allowed. \n\nThen it checks if the given name has already been defined. If yes, it returns an error indicating that the event name is already defined.\n\nIf the name has not been defined, it generates a new event index, increases the maximum event number, creates a new event, stores the new event in the `userDefined` map with the given name, and returns the new event."]}, "kitex-pkg/generic/thriftidl_provider.go-NewThriftFileProvider": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/thriftidl_provider.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage generic\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"path/filepath\"\n\t\"sync\"\n\n\t\"github.com/cloudwego/dynamicgo/meta\"\n\tdthrift \"github.com/cloudwego/dynamicgo/thrift\"\n\t\"github.com/cloudwego/thriftgo/parser\"\n\n\t\"github.com/cloudwego/kitex/pkg/generic/descriptor\"\n\t\"github.com/cloudwego/kitex/pkg/generic/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n)\n\nvar (\n\t_ Closer = &ThriftContentProvider{}\n\t_ Closer = &ThriftContentWithAbsIncludePathProvider{}\n)\n\ntype thriftFileProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\n// NewThriftFileProvider create a ThriftIDLProvider by given path and include dirs\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewThriftFileProviderWithDynamicGo create a ThriftIDLProvider with dynamicgo by given path and include dirs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc newServiceDescriptorFromPath(path string, includeDirs ...string) (*descriptor.ServiceDescriptor, error) {\n\ttree, err := parser.ParseFile(path, includeDirs, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn svc, nil\n}\n\n// maybe watch the file change and reparse\n// p.thrifts <- some change\n// func (p *thriftFileProvider) watchAndUpdate() {}\n\nfunc (p *thriftFileProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *thriftFileProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\nfunc (p *thriftFileProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\n// ThriftContentProvider provide descriptor from contents\ntype ThriftContentProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\nvar _ DescriptorProvider = (*ThriftContentProvider)(nil)\n\nconst defaultMainIDLPath = \"main.thrift\"\n\n// NewThriftContentProvider builder\nfunc NewThriftContentProvider(main string, includes map[string]string) (*ThriftContentProvider, error) {\n\tp := &ThriftContentProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: false},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\n// NewThriftContentProviderWithDynamicGo builder\nfunc NewThriftContentProviderWithDynamicGo(main string, includes map[string]string) (*ThriftContentProvider, error) {\n\tp := &ThriftContentProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: true},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.newDynamicGoDsc(svc, defaultMainIDLPath, main, includes)\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\n// UpdateIDL ...\nfunc (p *ThriftContentProvider) UpdateIDL(main string, includes map[string]string) error {\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, defaultMainIDLPath, main, includes)\n\t}\n\n\tselect {\n\tcase <-p.svcs:\n\tdefault:\n\t}\n\tselect {\n\tcase p.svcs <- svc:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// Provide ...\nfunc (p *ThriftContentProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *ThriftContentProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\n// Option ...\nfunc (p *ThriftContentProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\nfunc (p *ThriftContentProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string) {\n\tif err := newDynamicGoDscFromContent(svc, path, content, includes, false); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc parseIncludes(tree *parser.Thrift, parsed map[string]*parser.Thrift, sources map[string]string, isAbsIncludePath bool) (err error) {\n\tfor _, i := range tree.Includes {\n\t\tp := i.Path\n\t\tif isAbsIncludePath {\n\t\t\tp = absPath(tree.Filename, i.Path)\n\t\t}\n\t\tref, ok := parsed[p] // avoid infinite recursion\n\t\tif ok {\n\t\t\ti.Reference = ref\n\t\t\tcontinue\n\t\t}\n\t\tif src, ok := sources[p]; !ok {\n\t\t\treturn fmt.Errorf(\"miss include path: %s for file: %s\", p, tree.Filename)\n\t\t} else {\n\t\t\tif ref, err = parser.ParseString(p, src); err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tparsed[p] = ref\n\t\ti.Reference = ref\n\t\tif err = parseIncludes(ref, parsed, sources, isAbsIncludePath); err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\treturn nil\n}\n\n// path := /a/b/c.thrift\n// includePath := ../d.thrift\n// result := /a/d.thrift\nfunc absPath(path, includePath string) string {\n\tif filepath.IsAbs(includePath) {\n\t\treturn includePath\n\t}\n\treturn filepath.Join(filepath.Dir(path), includePath)\n}\n\n// ParseContent parses the IDL from path and content using provided includes\nfunc ParseContent(path, content string, includes map[string]string, isAbsIncludePath bool) (*parser.Thrift, error) {\n\tif src := includes[path]; src != \"\" && src != content {\n\t\treturn nil, fmt.Errorf(\"provided main IDL content conflicts with includes: %q\", path)\n\t}\n\ttree, err := parser.ParseString(path, content)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tparsed := make(map[string]*parser.Thrift)\n\tparsed[path] = tree\n\tif err := parseIncludes(tree, parsed, includes, isAbsIncludePath); err != nil {\n\t\treturn nil, err\n\t}\n\tif cir := parser.CircleDetect(tree); cir != \"\" {\n\t\treturn tree, fmt.Errorf(\"IDL circular dependency: %s\", cir)\n\t}\n\treturn tree, nil\n}\n\n// ThriftContentWithAbsIncludePathProvider ...\ntype ThriftContentWithAbsIncludePathProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\nvar _ DescriptorProvider = (*ThriftContentWithAbsIncludePathProvider)(nil)\n\n// NewThriftContentWithAbsIncludePathProvider create abs include path DescriptorProvider\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewThriftContentWithAbsIncludePathProviderWithDynamicGo create abs include path DescriptorProvider with dynamicgo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UpdateIDL update idl by given args\nfunc (p *ThriftContentWithAbsIncludePathProvider) UpdateIDL(mainIDLPath string, includes map[string]string) error {\n\tmainIDLContent, ok := includes[mainIDLPath]\n\tif !ok {\n\t\treturn fmt.Errorf(\"miss main IDL content for main IDL path: %s\", mainIDLPath)\n\t}\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(mainIDLPath, mainIDLContent, includes, true)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, mainIDLPath, mainIDLContent, includes)\n\t}\n\n\t// drain the channel\n\tselect {\n\tcase <-p.svcs:\n\tdefault:\n\t}\n\tselect {\n\tcase p.svcs <- svc:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// Provide ...\nfunc (p *ThriftContentWithAbsIncludePathProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *ThriftContentWithAbsIncludePathProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\n// Option ...\nfunc (p *ThriftContentWithAbsIncludePathProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\nfunc (p *ThriftContentWithAbsIncludePathProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string) {\n\tif err := newDynamicGoDscFromContent(svc, path, content, includes, true); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc newServiceDescriptorFromContent(path, content string, includes map[string]string, isAbsIncludePath bool) (*descriptor.ServiceDescriptor, error) {\n\ttree, err := ParseContent(path, content, includes, isAbsIncludePath)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn svc, nil\n}\n\nfunc newDynamicGoDscFromContent(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string, isAbsIncludePath bool) error {\n\t// ServiceDescriptor of dynamicgo\n\tdOpts := dthrift.Options{EnableThriftBase: true}\n\tdsvc, err := dOpts.NewDescritorFromContent(context.Background(), path, content, includes, isAbsIncludePath)\n\tif err != nil {\n\t\tklog.CtxWarnf(context.Background(), \"KITEX: failed to get dynamicgo service descriptor, fall back to the original way, error=%s\", err)\n\t\treturn err\n\t}\n\tsvc.DynamicGoDsc = dsvc\n\treturn nil\n}\n\n```\n# Function Signature: func NewThriftFileProvider(path string, includeDirs ...string) (DescriptorProvider, error) {\n# Natural Language Annotations: // NewThriftFileProvider create a ThriftIDLProvider by given path and include dirs\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewThriftFileProvider(path string, includeDirs ...string) (DescriptorProvider, error) {\n\tp := &thriftFileProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: false},\n\t}\n\tsvc, err := newServiceDescriptorFromPath(path, includeDirs...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp.svcs <- svc\n\treturn p, nil\n}\n```", "```go\nfunc NewThriftFileProvider(path string, includeDirs ...string) (DescriptorProvider, error) {\n\tp := &thriftFileProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: false},\n\t}\n\tsvc, err := newServiceDescriptorFromPath(path, includeDirs...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n```", "```go\nfunc NewThriftFileProvider(path string, includeDirs ...string) (DescriptorProvider, error) {\n\tp := &thriftFileProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1),\n\t\topts: &ProviderOption{DynamicGoEnabled: false},\n\t}\n\tsvc, err := newServiceDescriptorFromPath(path, includeDirs...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n```"]}, "kitex-pkg/generic/thriftidl_provider.go-NewThriftFileProviderWithDynamicGo": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/thriftidl_provider.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage generic\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"path/filepath\"\n\t\"sync\"\n\n\t\"github.com/cloudwego/dynamicgo/meta\"\n\tdthrift \"github.com/cloudwego/dynamicgo/thrift\"\n\t\"github.com/cloudwego/thriftgo/parser\"\n\n\t\"github.com/cloudwego/kitex/pkg/generic/descriptor\"\n\t\"github.com/cloudwego/kitex/pkg/generic/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n)\n\nvar (\n\t_ Closer = &ThriftContentProvider{}\n\t_ Closer = &ThriftContentWithAbsIncludePathProvider{}\n)\n\ntype thriftFileProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\n// NewThriftFileProvider create a ThriftIDLProvider by given path and include dirs\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewThriftFileProviderWithDynamicGo create a ThriftIDLProvider with dynamicgo by given path and include dirs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc newServiceDescriptorFromPath(path string, includeDirs ...string) (*descriptor.ServiceDescriptor, error) {\n\ttree, err := parser.ParseFile(path, includeDirs, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn svc, nil\n}\n\n// maybe watch the file change and reparse\n// p.thrifts <- some change\n// func (p *thriftFileProvider) watchAndUpdate() {}\n\nfunc (p *thriftFileProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *thriftFileProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\nfunc (p *thriftFileProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\n// ThriftContentProvider provide descriptor from contents\ntype ThriftContentProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\nvar _ DescriptorProvider = (*ThriftContentProvider)(nil)\n\nconst defaultMainIDLPath = \"main.thrift\"\n\n// NewThriftContentProvider builder\nfunc NewThriftContentProvider(main string, includes map[string]string) (*ThriftContentProvider, error) {\n\tp := &ThriftContentProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: false},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\n// NewThriftContentProviderWithDynamicGo builder\nfunc NewThriftContentProviderWithDynamicGo(main string, includes map[string]string) (*ThriftContentProvider, error) {\n\tp := &ThriftContentProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: true},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.newDynamicGoDsc(svc, defaultMainIDLPath, main, includes)\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\n// UpdateIDL ...\nfunc (p *ThriftContentProvider) UpdateIDL(main string, includes map[string]string) error {\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, defaultMainIDLPath, main, includes)\n\t}\n\n\tselect {\n\tcase <-p.svcs:\n\tdefault:\n\t}\n\tselect {\n\tcase p.svcs <- svc:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// Provide ...\nfunc (p *ThriftContentProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *ThriftContentProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\n// Option ...\nfunc (p *ThriftContentProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\nfunc (p *ThriftContentProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string) {\n\tif err := newDynamicGoDscFromContent(svc, path, content, includes, false); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc parseIncludes(tree *parser.Thrift, parsed map[string]*parser.Thrift, sources map[string]string, isAbsIncludePath bool) (err error) {\n\tfor _, i := range tree.Includes {\n\t\tp := i.Path\n\t\tif isAbsIncludePath {\n\t\t\tp = absPath(tree.Filename, i.Path)\n\t\t}\n\t\tref, ok := parsed[p] // avoid infinite recursion\n\t\tif ok {\n\t\t\ti.Reference = ref\n\t\t\tcontinue\n\t\t}\n\t\tif src, ok := sources[p]; !ok {\n\t\t\treturn fmt.Errorf(\"miss include path: %s for file: %s\", p, tree.Filename)\n\t\t} else {\n\t\t\tif ref, err = parser.ParseString(p, src); err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tparsed[p] = ref\n\t\ti.Reference = ref\n\t\tif err = parseIncludes(ref, parsed, sources, isAbsIncludePath); err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\treturn nil\n}\n\n// path := /a/b/c.thrift\n// includePath := ../d.thrift\n// result := /a/d.thrift\nfunc absPath(path, includePath string) string {\n\tif filepath.IsAbs(includePath) {\n\t\treturn includePath\n\t}\n\treturn filepath.Join(filepath.Dir(path), includePath)\n}\n\n// ParseContent parses the IDL from path and content using provided includes\nfunc ParseContent(path, content string, includes map[string]string, isAbsIncludePath bool) (*parser.Thrift, error) {\n\tif src := includes[path]; src != \"\" && src != content {\n\t\treturn nil, fmt.Errorf(\"provided main IDL content conflicts with includes: %q\", path)\n\t}\n\ttree, err := parser.ParseString(path, content)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tparsed := make(map[string]*parser.Thrift)\n\tparsed[path] = tree\n\tif err := parseIncludes(tree, parsed, includes, isAbsIncludePath); err != nil {\n\t\treturn nil, err\n\t}\n\tif cir := parser.CircleDetect(tree); cir != \"\" {\n\t\treturn tree, fmt.Errorf(\"IDL circular dependency: %s\", cir)\n\t}\n\treturn tree, nil\n}\n\n// ThriftContentWithAbsIncludePathProvider ...\ntype ThriftContentWithAbsIncludePathProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\nvar _ DescriptorProvider = (*ThriftContentWithAbsIncludePathProvider)(nil)\n\n// NewThriftContentWithAbsIncludePathProvider create abs include path DescriptorProvider\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewThriftContentWithAbsIncludePathProviderWithDynamicGo create abs include path DescriptorProvider with dynamicgo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UpdateIDL update idl by given args\nfunc (p *ThriftContentWithAbsIncludePathProvider) UpdateIDL(mainIDLPath string, includes map[string]string) error {\n\tmainIDLContent, ok := includes[mainIDLPath]\n\tif !ok {\n\t\treturn fmt.Errorf(\"miss main IDL content for main IDL path: %s\", mainIDLPath)\n\t}\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(mainIDLPath, mainIDLContent, includes, true)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, mainIDLPath, mainIDLContent, includes)\n\t}\n\n\t// drain the channel\n\tselect {\n\tcase <-p.svcs:\n\tdefault:\n\t}\n\tselect {\n\tcase p.svcs <- svc:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// Provide ...\nfunc (p *ThriftContentWithAbsIncludePathProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *ThriftContentWithAbsIncludePathProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\n// Option ...\nfunc (p *ThriftContentWithAbsIncludePathProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\nfunc (p *ThriftContentWithAbsIncludePathProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string) {\n\tif err := newDynamicGoDscFromContent(svc, path, content, includes, true); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc newServiceDescriptorFromContent(path, content string, includes map[string]string, isAbsIncludePath bool) (*descriptor.ServiceDescriptor, error) {\n\ttree, err := ParseContent(path, content, includes, isAbsIncludePath)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn svc, nil\n}\n\nfunc newDynamicGoDscFromContent(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string, isAbsIncludePath bool) error {\n\t// ServiceDescriptor of dynamicgo\n\tdOpts := dthrift.Options{EnableThriftBase: true}\n\tdsvc, err := dOpts.NewDescritorFromContent(context.Background(), path, content, includes, isAbsIncludePath)\n\tif err != nil {\n\t\tklog.CtxWarnf(context.Background(), \"KITEX: failed to get dynamicgo service descriptor, fall back to the original way, error=%s\", err)\n\t\treturn err\n\t}\n\tsvc.DynamicGoDsc = dsvc\n\treturn nil\n}\n\n```\n# Function Signature: func NewThriftFileProviderWithDynamicGo(path string, includeDirs ...string) (DescriptorProvider, error) {\n# Natural Language Annotations: // NewThriftFileProviderWithDynamicGo create a ThriftIDLProvider with dynamicgo by given path and include dirs\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewThriftFileProviderWithDynamicGo(path string, includeDirs ...string) (DescriptorProvider, error) {\n\tp := &ThriftFileProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: true},\n\t}\n\tsvc, err := newServiceDescriptorFromPath(path, includeDirs...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.newDynamicGoDsc(svc, path, includeDirs...)\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n```", "Here is the generated code:\n\n```go\n// NewThriftFileProviderWithDynamicGo create a ThriftIDLProvider with dynamicgo by given path and include dirs\nfunc NewThriftFileProviderWithDynamicGo(path string, includeDirs ...string) (DescriptorProvider, error) {\n\tp := &thriftFileProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: true},\n\t}\n\tsvc, err := newServiceDescriptorFromPath(path, includeDirs...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.newDynamicGoDsc(svc, path, includeDirs)\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\nfunc (p *thriftFileProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path string, includeDirs []string) {\n\tif err := newDynamicGoDscFromPath(svc, path, includeDirs); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc newDynamicGoDscFromPath(svc *descriptor.ServiceDescriptor, path string, includeDirs []string) error {\n\t// ServiceDescriptor of dynamicgo\n\tdOpts := dthrift.Options{EnableThriftBase: true}\n\tdsvc, err := dOpts.NewDescritorFromPath(context.Background(), path, includeDirs)\n\tif err != nil {\n\t\tklog.CtxWarnf(context.Background(), \"KITEX: failed to get dynamicgo service descriptor, fall back to the original way, error=%s\", err)\n\t\treturn err\n\t}\n\tsvc.DynamicGoDsc = dsvc\n\treturn nil\n}\n```", "```go\nfunc NewThriftFileProviderWithDynamicGo(path string, includeDirs ...string) (DescriptorProvider, error) {\n\tp := &ThriftFileProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: true},\n\t}\n\tsvc, err := newServiceDescriptorFromPath(path, includeDirs...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.newDynamicGoDsc(svc, path, includeDirs)\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n```"]}, "kitex-pkg/generic/thriftidl_provider.go-NewThriftContentWithAbsIncludePathProvider": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/thriftidl_provider.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage generic\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"path/filepath\"\n\t\"sync\"\n\n\t\"github.com/cloudwego/dynamicgo/meta\"\n\tdthrift \"github.com/cloudwego/dynamicgo/thrift\"\n\t\"github.com/cloudwego/thriftgo/parser\"\n\n\t\"github.com/cloudwego/kitex/pkg/generic/descriptor\"\n\t\"github.com/cloudwego/kitex/pkg/generic/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n)\n\nvar (\n\t_ Closer = &ThriftContentProvider{}\n\t_ Closer = &ThriftContentWithAbsIncludePathProvider{}\n)\n\ntype thriftFileProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\n// NewThriftFileProvider create a ThriftIDLProvider by given path and include dirs\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewThriftFileProviderWithDynamicGo create a ThriftIDLProvider with dynamicgo by given path and include dirs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc newServiceDescriptorFromPath(path string, includeDirs ...string) (*descriptor.ServiceDescriptor, error) {\n\ttree, err := parser.ParseFile(path, includeDirs, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn svc, nil\n}\n\n// maybe watch the file change and reparse\n// p.thrifts <- some change\n// func (p *thriftFileProvider) watchAndUpdate() {}\n\nfunc (p *thriftFileProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *thriftFileProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\nfunc (p *thriftFileProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\n// ThriftContentProvider provide descriptor from contents\ntype ThriftContentProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\nvar _ DescriptorProvider = (*ThriftContentProvider)(nil)\n\nconst defaultMainIDLPath = \"main.thrift\"\n\n// NewThriftContentProvider builder\nfunc NewThriftContentProvider(main string, includes map[string]string) (*ThriftContentProvider, error) {\n\tp := &ThriftContentProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: false},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\n// NewThriftContentProviderWithDynamicGo builder\nfunc NewThriftContentProviderWithDynamicGo(main string, includes map[string]string) (*ThriftContentProvider, error) {\n\tp := &ThriftContentProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: true},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.newDynamicGoDsc(svc, defaultMainIDLPath, main, includes)\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\n// UpdateIDL ...\nfunc (p *ThriftContentProvider) UpdateIDL(main string, includes map[string]string) error {\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, defaultMainIDLPath, main, includes)\n\t}\n\n\tselect {\n\tcase <-p.svcs:\n\tdefault:\n\t}\n\tselect {\n\tcase p.svcs <- svc:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// Provide ...\nfunc (p *ThriftContentProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *ThriftContentProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\n// Option ...\nfunc (p *ThriftContentProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\nfunc (p *ThriftContentProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string) {\n\tif err := newDynamicGoDscFromContent(svc, path, content, includes, false); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc parseIncludes(tree *parser.Thrift, parsed map[string]*parser.Thrift, sources map[string]string, isAbsIncludePath bool) (err error) {\n\tfor _, i := range tree.Includes {\n\t\tp := i.Path\n\t\tif isAbsIncludePath {\n\t\t\tp = absPath(tree.Filename, i.Path)\n\t\t}\n\t\tref, ok := parsed[p] // avoid infinite recursion\n\t\tif ok {\n\t\t\ti.Reference = ref\n\t\t\tcontinue\n\t\t}\n\t\tif src, ok := sources[p]; !ok {\n\t\t\treturn fmt.Errorf(\"miss include path: %s for file: %s\", p, tree.Filename)\n\t\t} else {\n\t\t\tif ref, err = parser.ParseString(p, src); err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tparsed[p] = ref\n\t\ti.Reference = ref\n\t\tif err = parseIncludes(ref, parsed, sources, isAbsIncludePath); err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\treturn nil\n}\n\n// path := /a/b/c.thrift\n// includePath := ../d.thrift\n// result := /a/d.thrift\nfunc absPath(path, includePath string) string {\n\tif filepath.IsAbs(includePath) {\n\t\treturn includePath\n\t}\n\treturn filepath.Join(filepath.Dir(path), includePath)\n}\n\n// ParseContent parses the IDL from path and content using provided includes\nfunc ParseContent(path, content string, includes map[string]string, isAbsIncludePath bool) (*parser.Thrift, error) {\n\tif src := includes[path]; src != \"\" && src != content {\n\t\treturn nil, fmt.Errorf(\"provided main IDL content conflicts with includes: %q\", path)\n\t}\n\ttree, err := parser.ParseString(path, content)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tparsed := make(map[string]*parser.Thrift)\n\tparsed[path] = tree\n\tif err := parseIncludes(tree, parsed, includes, isAbsIncludePath); err != nil {\n\t\treturn nil, err\n\t}\n\tif cir := parser.CircleDetect(tree); cir != \"\" {\n\t\treturn tree, fmt.Errorf(\"IDL circular dependency: %s\", cir)\n\t}\n\treturn tree, nil\n}\n\n// ThriftContentWithAbsIncludePathProvider ...\ntype ThriftContentWithAbsIncludePathProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\nvar _ DescriptorProvider = (*ThriftContentWithAbsIncludePathProvider)(nil)\n\n// NewThriftContentWithAbsIncludePathProvider create abs include path DescriptorProvider\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewThriftContentWithAbsIncludePathProviderWithDynamicGo create abs include path DescriptorProvider with dynamicgo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UpdateIDL update idl by given args\nfunc (p *ThriftContentWithAbsIncludePathProvider) UpdateIDL(mainIDLPath string, includes map[string]string) error {\n\tmainIDLContent, ok := includes[mainIDLPath]\n\tif !ok {\n\t\treturn fmt.Errorf(\"miss main IDL content for main IDL path: %s\", mainIDLPath)\n\t}\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(mainIDLPath, mainIDLContent, includes, true)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, mainIDLPath, mainIDLContent, includes)\n\t}\n\n\t// drain the channel\n\tselect {\n\tcase <-p.svcs:\n\tdefault:\n\t}\n\tselect {\n\tcase p.svcs <- svc:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// Provide ...\nfunc (p *ThriftContentWithAbsIncludePathProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *ThriftContentWithAbsIncludePathProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\n// Option ...\nfunc (p *ThriftContentWithAbsIncludePathProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\nfunc (p *ThriftContentWithAbsIncludePathProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string) {\n\tif err := newDynamicGoDscFromContent(svc, path, content, includes, true); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc newServiceDescriptorFromContent(path, content string, includes map[string]string, isAbsIncludePath bool) (*descriptor.ServiceDescriptor, error) {\n\ttree, err := ParseContent(path, content, includes, isAbsIncludePath)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn svc, nil\n}\n\nfunc newDynamicGoDscFromContent(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string, isAbsIncludePath bool) error {\n\t// ServiceDescriptor of dynamicgo\n\tdOpts := dthrift.Options{EnableThriftBase: true}\n\tdsvc, err := dOpts.NewDescritorFromContent(context.Background(), path, content, includes, isAbsIncludePath)\n\tif err != nil {\n\t\tklog.CtxWarnf(context.Background(), \"KITEX: failed to get dynamicgo service descriptor, fall back to the original way, error=%s\", err)\n\t\treturn err\n\t}\n\tsvc.DynamicGoDsc = dsvc\n\treturn nil\n}\n\n```\n# Function Signature: func NewThriftContentWithAbsIncludePathProvider(mainIDLPath string, includes map[string]string) (*ThriftContentWithAbsIncludePathProvider, error) {\n# Natural Language Annotations: // NewThriftContentWithAbsIncludePathProvider create abs include path DescriptorProvider\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewThriftContentWithAbsIncludePathProvider(mainIDLPath string, includes map[string]string) (*ThriftContentWithAbsIncludePathProvider, error) {\n\tp := &ThriftContentWithAbsIncludePathProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: false},\n\t}\n\n\tmainIDLContent, ok := includes[mainIDLPath]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"miss main IDL content for main IDL path: %s\", mainIDLPath)\n\t}\n\tsvc, err := newServiceDescriptorFromContent(mainIDLPath, mainIDLContent, includes, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n```", "```go\nfunc NewThriftContentWithAbsIncludePathProvider(mainIDLPath string, includes map[string]string) (*ThriftContentWithAbsIncludePathProvider, error) {\n\tp := &ThriftContentWithAbsIncludePathProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: false},\n\t}\n\n\tmainIDLContent, ok := includes[mainIDLPath]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"miss main IDL content for main IDL path: %s\", mainIDLPath)\n\t}\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(mainIDLPath, mainIDLContent, includes, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n```\nThis function creates a new instance of the `ThriftContentWithAbsIncludePathProvider` struct. It initializes a buffered channel for the service descriptors, sets the `DynamicGoEnabled` option to `false`, and returns the provider along with an error (if any).", "```go\nfunc NewThriftContentWithAbsIncludePathProvider(mainIDLPath string, includes map[string]string) (*ThriftContentWithAbsIncludePathProvider, error) {\n\tp := &ThriftContentWithAbsIncludePathProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: false},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(mainIDLPath, includes[mainIDLPath], includes, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n```"]}, "kitex-pkg/generic/thriftidl_provider.go-NewThriftContentWithAbsIncludePathProviderWithDynamicGo": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/thriftidl_provider.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage generic\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"path/filepath\"\n\t\"sync\"\n\n\t\"github.com/cloudwego/dynamicgo/meta\"\n\tdthrift \"github.com/cloudwego/dynamicgo/thrift\"\n\t\"github.com/cloudwego/thriftgo/parser\"\n\n\t\"github.com/cloudwego/kitex/pkg/generic/descriptor\"\n\t\"github.com/cloudwego/kitex/pkg/generic/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n)\n\nvar (\n\t_ Closer = &ThriftContentProvider{}\n\t_ Closer = &ThriftContentWithAbsIncludePathProvider{}\n)\n\ntype thriftFileProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\n// NewThriftFileProvider create a ThriftIDLProvider by given path and include dirs\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewThriftFileProviderWithDynamicGo create a ThriftIDLProvider with dynamicgo by given path and include dirs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc newServiceDescriptorFromPath(path string, includeDirs ...string) (*descriptor.ServiceDescriptor, error) {\n\ttree, err := parser.ParseFile(path, includeDirs, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn svc, nil\n}\n\n// maybe watch the file change and reparse\n// p.thrifts <- some change\n// func (p *thriftFileProvider) watchAndUpdate() {}\n\nfunc (p *thriftFileProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *thriftFileProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\nfunc (p *thriftFileProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\n// ThriftContentProvider provide descriptor from contents\ntype ThriftContentProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\nvar _ DescriptorProvider = (*ThriftContentProvider)(nil)\n\nconst defaultMainIDLPath = \"main.thrift\"\n\n// NewThriftContentProvider builder\nfunc NewThriftContentProvider(main string, includes map[string]string) (*ThriftContentProvider, error) {\n\tp := &ThriftContentProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: false},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\n// NewThriftContentProviderWithDynamicGo builder\nfunc NewThriftContentProviderWithDynamicGo(main string, includes map[string]string) (*ThriftContentProvider, error) {\n\tp := &ThriftContentProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: true},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.newDynamicGoDsc(svc, defaultMainIDLPath, main, includes)\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n\n// UpdateIDL ...\nfunc (p *ThriftContentProvider) UpdateIDL(main string, includes map[string]string) error {\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(defaultMainIDLPath, main, includes, false)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, defaultMainIDLPath, main, includes)\n\t}\n\n\tselect {\n\tcase <-p.svcs:\n\tdefault:\n\t}\n\tselect {\n\tcase p.svcs <- svc:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// Provide ...\nfunc (p *ThriftContentProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *ThriftContentProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\n// Option ...\nfunc (p *ThriftContentProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\nfunc (p *ThriftContentProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string) {\n\tif err := newDynamicGoDscFromContent(svc, path, content, includes, false); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc parseIncludes(tree *parser.Thrift, parsed map[string]*parser.Thrift, sources map[string]string, isAbsIncludePath bool) (err error) {\n\tfor _, i := range tree.Includes {\n\t\tp := i.Path\n\t\tif isAbsIncludePath {\n\t\t\tp = absPath(tree.Filename, i.Path)\n\t\t}\n\t\tref, ok := parsed[p] // avoid infinite recursion\n\t\tif ok {\n\t\t\ti.Reference = ref\n\t\t\tcontinue\n\t\t}\n\t\tif src, ok := sources[p]; !ok {\n\t\t\treturn fmt.Errorf(\"miss include path: %s for file: %s\", p, tree.Filename)\n\t\t} else {\n\t\t\tif ref, err = parser.ParseString(p, src); err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\tparsed[p] = ref\n\t\ti.Reference = ref\n\t\tif err = parseIncludes(ref, parsed, sources, isAbsIncludePath); err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\treturn nil\n}\n\n// path := /a/b/c.thrift\n// includePath := ../d.thrift\n// result := /a/d.thrift\nfunc absPath(path, includePath string) string {\n\tif filepath.IsAbs(includePath) {\n\t\treturn includePath\n\t}\n\treturn filepath.Join(filepath.Dir(path), includePath)\n}\n\n// ParseContent parses the IDL from path and content using provided includes\nfunc ParseContent(path, content string, includes map[string]string, isAbsIncludePath bool) (*parser.Thrift, error) {\n\tif src := includes[path]; src != \"\" && src != content {\n\t\treturn nil, fmt.Errorf(\"provided main IDL content conflicts with includes: %q\", path)\n\t}\n\ttree, err := parser.ParseString(path, content)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tparsed := make(map[string]*parser.Thrift)\n\tparsed[path] = tree\n\tif err := parseIncludes(tree, parsed, includes, isAbsIncludePath); err != nil {\n\t\treturn nil, err\n\t}\n\tif cir := parser.CircleDetect(tree); cir != \"\" {\n\t\treturn tree, fmt.Errorf(\"IDL circular dependency: %s\", cir)\n\t}\n\treturn tree, nil\n}\n\n// ThriftContentWithAbsIncludePathProvider ...\ntype ThriftContentWithAbsIncludePathProvider struct {\n\tcloseOnce sync.Once\n\tsvcs      chan *descriptor.ServiceDescriptor\n\topts      *ProviderOption\n}\n\nvar _ DescriptorProvider = (*ThriftContentWithAbsIncludePathProvider)(nil)\n\n// NewThriftContentWithAbsIncludePathProvider create abs include path DescriptorProvider\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewThriftContentWithAbsIncludePathProviderWithDynamicGo create abs include path DescriptorProvider with dynamicgo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// UpdateIDL update idl by given args\nfunc (p *ThriftContentWithAbsIncludePathProvider) UpdateIDL(mainIDLPath string, includes map[string]string) error {\n\tmainIDLContent, ok := includes[mainIDLPath]\n\tif !ok {\n\t\treturn fmt.Errorf(\"miss main IDL content for main IDL path: %s\", mainIDLPath)\n\t}\n\tvar svc *descriptor.ServiceDescriptor\n\ttree, err := ParseContent(mainIDLPath, mainIDLContent, includes, true)\n\tif err != nil {\n\t\treturn err\n\t}\n\tsvc, err = thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif p.opts.DynamicGoEnabled {\n\t\tp.newDynamicGoDsc(svc, mainIDLPath, mainIDLContent, includes)\n\t}\n\n\t// drain the channel\n\tselect {\n\tcase <-p.svcs:\n\tdefault:\n\t}\n\tselect {\n\tcase p.svcs <- svc:\n\tdefault:\n\t}\n\treturn nil\n}\n\n// Provide ...\nfunc (p *ThriftContentWithAbsIncludePathProvider) Provide() <-chan *descriptor.ServiceDescriptor {\n\treturn p.svcs\n}\n\n// Close the sending chan.\nfunc (p *ThriftContentWithAbsIncludePathProvider) Close() error {\n\tp.closeOnce.Do(func() {\n\t\tclose(p.svcs)\n\t})\n\treturn nil\n}\n\n// Option ...\nfunc (p *ThriftContentWithAbsIncludePathProvider) Option() ProviderOption {\n\treturn *p.opts\n}\n\nfunc (p *ThriftContentWithAbsIncludePathProvider) newDynamicGoDsc(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string) {\n\tif err := newDynamicGoDscFromContent(svc, path, content, includes, true); err != nil {\n\t\tp.opts.DynamicGoEnabled = false\n\t}\n}\n\nfunc newServiceDescriptorFromContent(path, content string, includes map[string]string, isAbsIncludePath bool) (*descriptor.ServiceDescriptor, error) {\n\ttree, err := ParseContent(path, content, includes, isAbsIncludePath)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsvc, err := thrift.Parse(tree, thrift.DefaultParseMode())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn svc, nil\n}\n\nfunc newDynamicGoDscFromContent(svc *descriptor.ServiceDescriptor, path, content string, includes map[string]string, isAbsIncludePath bool) error {\n\t// ServiceDescriptor of dynamicgo\n\tdOpts := dthrift.Options{EnableThriftBase: true}\n\tdsvc, err := dOpts.NewDescritorFromContent(context.Background(), path, content, includes, isAbsIncludePath)\n\tif err != nil {\n\t\tklog.CtxWarnf(context.Background(), \"KITEX: failed to get dynamicgo service descriptor, fall back to the original way, error=%s\", err)\n\t\treturn err\n\t}\n\tsvc.DynamicGoDsc = dsvc\n\treturn nil\n}\n\n```\n# Function Signature: func NewThriftContentWithAbsIncludePathProviderWithDynamicGo(mainIDLPath string, includes map[string]string) (*ThriftContentWithAbsIncludePathProvider, error) {\n# Natural Language Annotations: // NewThriftContentWithAbsIncludePathProviderWithDynamicGo create abs include path DescriptorProvider with dynamicgo\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewThriftContentWithAbsIncludePathProviderWithDynamicGo(mainIDLPath string, includes map[string]string) (*ThriftContentWithAbsIncludePathProvider, error) {\n\tp := &ThriftContentWithAbsIncludePathProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: true},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(mainIDLPath, includes[mainIDLPath], includes, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.newDynamicGoDsc(svc, mainIDLPath, includes[mainIDLPath], includes)\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n```", "```go\nfunc NewThriftContentWithAbsIncludePathProviderWithDynamicGo(mainIDLPath string, includes map[string]string) (*ThriftContentWithAbsIncludePathProvider, error) {\n\tp := &ThriftContentWithAbsIncludePathProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: true},\n\t}\n\tsvc, err := newServiceDescriptorFromContent(mainIDLPath, includes[mainIDLPath], includes, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.newDynamicGoDsc(svc, mainIDLPath, includes[mainIDLPath], includes)\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n```", "```go\nfunc NewThriftContentWithAbsIncludePathProviderWithDynamicGo(mainIDLPath string, includes map[string]string) (*ThriftContentWithAbsIncludePathProvider, error) {\n\tp := &ThriftContentWithAbsIncludePathProvider{\n\t\tsvcs: make(chan *descriptor.ServiceDescriptor, 1), // unblock with buffered channel\n\t\topts: &ProviderOption{DynamicGoEnabled: true},\n\t}\n\tmainIDLContent, ok := includes[mainIDLPath]\n\tif !ok {\n\t\treturn nil, fmt.Errorf(\"miss main IDL content for main IDL path: %s\", mainIDLPath)\n\t}\n\tsvc, err := newServiceDescriptorFromContent(mainIDLPath, mainIDLContent, includes, true)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tp.newDynamicGoDsc(svc, mainIDLPath, mainIDLContent, includes)\n\n\tp.svcs <- svc\n\treturn p, nil\n}\n```\nThis function creates a new ThriftContentWithAbsIncludePathProvider with dynamicgo enabled. It takes a mainIDLPath and a map of includes as arguments. It checks if the mainIDLContent exists in the includes map. If it doesn't exist, it returns an error. It then calls newServiceDescriptorFromContent with the mainIDLPath, mainIDLContent, includes and true (indicating absolute include paths) to get a service descriptor. If there is an error, it returns the error. It then calls p.newDynamicGoDsc to add the dynamicgo service descriptor. Finally, it sends the service descriptor to the svcs channel and returns the provider."]}, "kitex-pkg/generic/generic.go-MapThriftGeneric": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/generic.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package generic ...\npackage generic\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/cloudwego/dynamicgo/conv\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n)\n\n// Generic ...\ntype Generic interface {\n\tCloser\n\t// PayloadCodec return codec implement\n\tPayloadCodec() remote.PayloadCodec\n\t// PayloadCodecType return the type of codec\n\tPayloadCodecType() serviceinfo.PayloadCodec\n\t// RawThriftBinaryGeneric must be framed\n\tFramed() bool\n\t// GetMethod to get method name if need\n\tGetMethod(req interface{}, method string) (*Method, error)\n}\n\n// Method information\ntype Method struct {\n\tName   string\n\tOneway bool\n}\n\n// BinaryThriftGeneric raw thrift binary Generic\nfunc BinaryThriftGeneric() Generic {\n\treturn &binaryThriftGeneric{}\n}\n\n// MapThriftGeneric map mapping generic\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.MapThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n//\n// String value is returned for binary field by default. You can change the return value to []byte for binary field with SetBinaryWithByteSlice.\n// eg:\n//\n//\tSetBinaryWithByteSlice(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc MapThriftGenericForJSON(p DescriptorProvider) (Generic, error) {\n\tcodec, err := newMapThriftCodecForJSON(p, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &mapThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// HTTPThriftGeneric http mapping Generic.\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.HTTPThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc HTTPPbThriftGeneric(p DescriptorProvider, pbp PbDescriptorProvider) (Generic, error) {\n\tcodec, err := newHTTPPbThriftCodec(p, pbp, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &httpPbThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// JSONThriftGeneric json mapping generic.\n// Base64 codec for binary field is enabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.JSONThriftGeneric(p)\n//\tSetBinaryWithBase64(g, false)\n\n\n\n\n\n\n\n\n\n\n// JSONPbGeneric json mapping generic.\n// Uses dynamicgo for json to protobufs conversion, so DynamicGo field is true by default.\n\n\n\n\n\n\n\n\n\n\n\n// SetBinaryWithBase64 enable/disable Base64 codec for binary field.\nfunc SetBinaryWithBase64(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *httpThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t}\n\tcase *jsonThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithException.NoBase64Binary = !enable\n\t\t}\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"Base64Binary is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetBinaryWithByteSlice enable/disable returning []byte for binary field.\nfunc SetBinaryWithByteSlice(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithByteSlice = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"returning []byte for binary fields is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetFieldsForEmptyStructMode is a enum for EnableSetFieldsForEmptyStruct()\ntype SetFieldsForEmptyStructMode uint8\n\nconst (\n\t// NotSetFields means disable\n\tNotSetFields SetFieldsForEmptyStructMode = iota\n\t// SetNonOptiontionalFields means only set required and default fields\n\tSetNonOptiontionalFields\n\t// SetAllFields means set all fields\n\tSetAllFields\n)\n\n// EnableSetFieldsForEmptyStruct enable/disable set all fields of a struct even if it is empty.\n// This option is only applicable to map-generic response (reading) now.\n//\n//\tmode == 0 means disable\n//\tmode == 1 means only set required and default fields\n//\tmode == 2 means set all fields\nfunc EnableSetFieldsForEmptyStruct(g Generic, mode SetFieldsForEmptyStructMode) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.setFieldsForEmptyStruct = uint8(mode)\n\tdefault:\n\t\treturn fmt.Errorf(\"SetFieldsForEmptyStruct only supports map-generic at present\")\n\t}\n\treturn nil\n}\n\nvar thriftCodec = thrift.NewThriftCodec()\n\nvar pbCodec = protobuf.NewProtobufCodec()\n\ntype binaryThriftGeneric struct{}\n\nfunc (g *binaryThriftGeneric) Framed() bool {\n\treturn true\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\tpc := &binaryThriftCodec{thriftCodec}\n\treturn pc\n}\n\nfunc (g *binaryThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn &Method{method, false}, nil\n}\n\nfunc (g *binaryThriftGeneric) Close() error {\n\treturn nil\n}\n\ntype mapThriftGeneric struct {\n\tcodec *mapThriftCodec\n}\n\nfunc (g *mapThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *mapThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *mapThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *mapThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *mapThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonThriftGeneric struct {\n\tcodec *jsonThriftCodec\n}\n\nfunc (g *jsonThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonPbGeneric struct {\n\tcodec *jsonPbCodec\n}\n\nfunc (g *jsonPbGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *jsonPbGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Protobuf\n}\n\nfunc (g *jsonPbGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonPbGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonPbGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpThriftGeneric struct {\n\tcodec *httpThriftCodec\n}\n\nfunc (g *httpThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *httpThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpPbThriftGeneric struct {\n\tcodec *httpPbThriftCodec\n}\n\nfunc (g *httpPbThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpPbThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpPbThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\n```\n# Function Signature: func MapThriftGeneric(p DescriptorProvider) (Generic, error) {\n# Natural Language Annotations: // MapThriftGeneric map mapping generic\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.MapThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n//\n// String value is returned for binary field by default. You can change the return value to []byte for binary field with SetBinaryWithByteSlice.\n// eg:\n//\n//\tSetBinaryWithByteSlice(g, true)\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc MapThriftGeneric(p DescriptorProvider) (Generic, error) {\n\tcodec, err := newMapThriftCodec(p, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &mapThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n```", "```go\nfunc MapThriftGeneric(p DescriptorProvider) (Generic, error) {\n\tcodec, err := newMapThriftCodec(p, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &mapThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n```", "```go\nfunc MapThriftGeneric(p DescriptorProvider) (Generic, error) {\n\tcodec, err := newMapThriftCodec(p, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &mapThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n```"]}, "kitex-pkg/generic/generic.go-HTTPThriftGeneric": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/generic.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package generic ...\npackage generic\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/cloudwego/dynamicgo/conv\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n)\n\n// Generic ...\ntype Generic interface {\n\tCloser\n\t// PayloadCodec return codec implement\n\tPayloadCodec() remote.PayloadCodec\n\t// PayloadCodecType return the type of codec\n\tPayloadCodecType() serviceinfo.PayloadCodec\n\t// RawThriftBinaryGeneric must be framed\n\tFramed() bool\n\t// GetMethod to get method name if need\n\tGetMethod(req interface{}, method string) (*Method, error)\n}\n\n// Method information\ntype Method struct {\n\tName   string\n\tOneway bool\n}\n\n// BinaryThriftGeneric raw thrift binary Generic\nfunc BinaryThriftGeneric() Generic {\n\treturn &binaryThriftGeneric{}\n}\n\n// MapThriftGeneric map mapping generic\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.MapThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n//\n// String value is returned for binary field by default. You can change the return value to []byte for binary field with SetBinaryWithByteSlice.\n// eg:\n//\n//\tSetBinaryWithByteSlice(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc MapThriftGenericForJSON(p DescriptorProvider) (Generic, error) {\n\tcodec, err := newMapThriftCodecForJSON(p, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &mapThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// HTTPThriftGeneric http mapping Generic.\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.HTTPThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc HTTPPbThriftGeneric(p DescriptorProvider, pbp PbDescriptorProvider) (Generic, error) {\n\tcodec, err := newHTTPPbThriftCodec(p, pbp, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &httpPbThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// JSONThriftGeneric json mapping generic.\n// Base64 codec for binary field is enabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.JSONThriftGeneric(p)\n//\tSetBinaryWithBase64(g, false)\n\n\n\n\n\n\n\n\n\n\n// JSONPbGeneric json mapping generic.\n// Uses dynamicgo for json to protobufs conversion, so DynamicGo field is true by default.\n\n\n\n\n\n\n\n\n\n\n\n// SetBinaryWithBase64 enable/disable Base64 codec for binary field.\nfunc SetBinaryWithBase64(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *httpThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t}\n\tcase *jsonThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithException.NoBase64Binary = !enable\n\t\t}\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"Base64Binary is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetBinaryWithByteSlice enable/disable returning []byte for binary field.\nfunc SetBinaryWithByteSlice(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithByteSlice = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"returning []byte for binary fields is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetFieldsForEmptyStructMode is a enum for EnableSetFieldsForEmptyStruct()\ntype SetFieldsForEmptyStructMode uint8\n\nconst (\n\t// NotSetFields means disable\n\tNotSetFields SetFieldsForEmptyStructMode = iota\n\t// SetNonOptiontionalFields means only set required and default fields\n\tSetNonOptiontionalFields\n\t// SetAllFields means set all fields\n\tSetAllFields\n)\n\n// EnableSetFieldsForEmptyStruct enable/disable set all fields of a struct even if it is empty.\n// This option is only applicable to map-generic response (reading) now.\n//\n//\tmode == 0 means disable\n//\tmode == 1 means only set required and default fields\n//\tmode == 2 means set all fields\nfunc EnableSetFieldsForEmptyStruct(g Generic, mode SetFieldsForEmptyStructMode) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.setFieldsForEmptyStruct = uint8(mode)\n\tdefault:\n\t\treturn fmt.Errorf(\"SetFieldsForEmptyStruct only supports map-generic at present\")\n\t}\n\treturn nil\n}\n\nvar thriftCodec = thrift.NewThriftCodec()\n\nvar pbCodec = protobuf.NewProtobufCodec()\n\ntype binaryThriftGeneric struct{}\n\nfunc (g *binaryThriftGeneric) Framed() bool {\n\treturn true\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\tpc := &binaryThriftCodec{thriftCodec}\n\treturn pc\n}\n\nfunc (g *binaryThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn &Method{method, false}, nil\n}\n\nfunc (g *binaryThriftGeneric) Close() error {\n\treturn nil\n}\n\ntype mapThriftGeneric struct {\n\tcodec *mapThriftCodec\n}\n\nfunc (g *mapThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *mapThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *mapThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *mapThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *mapThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonThriftGeneric struct {\n\tcodec *jsonThriftCodec\n}\n\nfunc (g *jsonThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonPbGeneric struct {\n\tcodec *jsonPbCodec\n}\n\nfunc (g *jsonPbGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *jsonPbGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Protobuf\n}\n\nfunc (g *jsonPbGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonPbGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonPbGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpThriftGeneric struct {\n\tcodec *httpThriftCodec\n}\n\nfunc (g *httpThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *httpThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpPbThriftGeneric struct {\n\tcodec *httpPbThriftCodec\n}\n\nfunc (g *httpPbThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpPbThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpPbThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\n```\n# Function Signature: func HTTPThriftGeneric(p DescriptorProvider, opts ...Option) (Generic, error) {\n# Natural Language Annotations: // HTTPThriftGeneric http mapping Generic.\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.HTTPThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\nPlease return the generated code surrounded by ```\n", "response": ["Sure, here is the generated code for the `HTTPThriftGeneric` function:\n\n```go\nfunc HTTPThriftGeneric(p DescriptorProvider, opts ...Option) (Generic, error) {\n\tcodec, err := newHTTPThriftCodec(p, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tg := &httpThriftGeneric{\n\t\tcodec: codec,\n\t}\n\n\t// Apply options\n\tfor _, opt := range opts {\n\t\tif err := opt(g); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\treturn g, nil\n}\n```\n\nThis function creates a new `httpThriftGeneric` instance with a `httpThriftCodec`. It also applies any provided options.\n\nNote: The `newHTTPThriftCodec` function and the `Option` type are not defined in the provided code. You would need to define these to fully implement this function.", "```go\nfunc HTTPThriftGeneric(p DescriptorProvider, opts ...Option) (Generic, error) {\n\tcodec, err := newHTTPThriftCodec(p, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tg := &httpThriftGeneric{codec: codec}\n\n\tfor _, opt := range opts {\n\t\topt(g)\n\t}\n\n\treturn g, nil\n}\n\ntype Option func(*httpThriftGeneric)\n\nfunc SetBinaryWithBase64Option(enable bool) Option {\n\treturn func(g *httpThriftGeneric) {\n\t\tg.codec.binaryWithBase64 = enable\n\t\tif g.codec.dynamicgoEnabled {\n\t\t\tg.codec.convOpts.NoBase64Binary = !enable\n\t\t\tg.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t}\n\t}\n}\n```", "```go\nfunc HTTPThriftGeneric(p DescriptorProvider, opts ...Option) (Generic, error) {\n\tcodec, err := newHTTPThriftCodec(p, thriftCodec, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &httpThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n```"]}, "kitex-pkg/generic/generic.go-JSONThriftGeneric": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/generic.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package generic ...\npackage generic\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/cloudwego/dynamicgo/conv\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n)\n\n// Generic ...\ntype Generic interface {\n\tCloser\n\t// PayloadCodec return codec implement\n\tPayloadCodec() remote.PayloadCodec\n\t// PayloadCodecType return the type of codec\n\tPayloadCodecType() serviceinfo.PayloadCodec\n\t// RawThriftBinaryGeneric must be framed\n\tFramed() bool\n\t// GetMethod to get method name if need\n\tGetMethod(req interface{}, method string) (*Method, error)\n}\n\n// Method information\ntype Method struct {\n\tName   string\n\tOneway bool\n}\n\n// BinaryThriftGeneric raw thrift binary Generic\nfunc BinaryThriftGeneric() Generic {\n\treturn &binaryThriftGeneric{}\n}\n\n// MapThriftGeneric map mapping generic\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.MapThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n//\n// String value is returned for binary field by default. You can change the return value to []byte for binary field with SetBinaryWithByteSlice.\n// eg:\n//\n//\tSetBinaryWithByteSlice(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc MapThriftGenericForJSON(p DescriptorProvider) (Generic, error) {\n\tcodec, err := newMapThriftCodecForJSON(p, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &mapThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// HTTPThriftGeneric http mapping Generic.\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.HTTPThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc HTTPPbThriftGeneric(p DescriptorProvider, pbp PbDescriptorProvider) (Generic, error) {\n\tcodec, err := newHTTPPbThriftCodec(p, pbp, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &httpPbThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// JSONThriftGeneric json mapping generic.\n// Base64 codec for binary field is enabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.JSONThriftGeneric(p)\n//\tSetBinaryWithBase64(g, false)\n\n\n\n\n\n\n\n\n\n\n// JSONPbGeneric json mapping generic.\n// Uses dynamicgo for json to protobufs conversion, so DynamicGo field is true by default.\n\n\n\n\n\n\n\n\n\n\n\n// SetBinaryWithBase64 enable/disable Base64 codec for binary field.\nfunc SetBinaryWithBase64(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *httpThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t}\n\tcase *jsonThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithException.NoBase64Binary = !enable\n\t\t}\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"Base64Binary is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetBinaryWithByteSlice enable/disable returning []byte for binary field.\nfunc SetBinaryWithByteSlice(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithByteSlice = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"returning []byte for binary fields is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetFieldsForEmptyStructMode is a enum for EnableSetFieldsForEmptyStruct()\ntype SetFieldsForEmptyStructMode uint8\n\nconst (\n\t// NotSetFields means disable\n\tNotSetFields SetFieldsForEmptyStructMode = iota\n\t// SetNonOptiontionalFields means only set required and default fields\n\tSetNonOptiontionalFields\n\t// SetAllFields means set all fields\n\tSetAllFields\n)\n\n// EnableSetFieldsForEmptyStruct enable/disable set all fields of a struct even if it is empty.\n// This option is only applicable to map-generic response (reading) now.\n//\n//\tmode == 0 means disable\n//\tmode == 1 means only set required and default fields\n//\tmode == 2 means set all fields\nfunc EnableSetFieldsForEmptyStruct(g Generic, mode SetFieldsForEmptyStructMode) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.setFieldsForEmptyStruct = uint8(mode)\n\tdefault:\n\t\treturn fmt.Errorf(\"SetFieldsForEmptyStruct only supports map-generic at present\")\n\t}\n\treturn nil\n}\n\nvar thriftCodec = thrift.NewThriftCodec()\n\nvar pbCodec = protobuf.NewProtobufCodec()\n\ntype binaryThriftGeneric struct{}\n\nfunc (g *binaryThriftGeneric) Framed() bool {\n\treturn true\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\tpc := &binaryThriftCodec{thriftCodec}\n\treturn pc\n}\n\nfunc (g *binaryThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn &Method{method, false}, nil\n}\n\nfunc (g *binaryThriftGeneric) Close() error {\n\treturn nil\n}\n\ntype mapThriftGeneric struct {\n\tcodec *mapThriftCodec\n}\n\nfunc (g *mapThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *mapThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *mapThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *mapThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *mapThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonThriftGeneric struct {\n\tcodec *jsonThriftCodec\n}\n\nfunc (g *jsonThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonPbGeneric struct {\n\tcodec *jsonPbCodec\n}\n\nfunc (g *jsonPbGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *jsonPbGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Protobuf\n}\n\nfunc (g *jsonPbGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonPbGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonPbGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpThriftGeneric struct {\n\tcodec *httpThriftCodec\n}\n\nfunc (g *httpThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *httpThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpPbThriftGeneric struct {\n\tcodec *httpPbThriftCodec\n}\n\nfunc (g *httpPbThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpPbThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpPbThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\n```\n# Function Signature: func JSONThriftGeneric(p DescriptorProvider, opts ...Option) (Generic, error) {\n# Natural Language Annotations: // JSONThriftGeneric json mapping generic.\n// Base64 codec for binary field is enabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.JSONThriftGeneric(p)\n//\tSetBinaryWithBase64(g, false)\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc JSONThriftGeneric(p DescriptorProvider, opts ...Option) (Generic, error) {\n\tcodec, err := newJSONThriftCodec(p, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tg := &jsonThriftGeneric{\n\t\tcodec: codec,\n\t}\n\tfor _, opt := range opts {\n\t\topt(g.codec)\n\t}\n\treturn g, nil\n}\n```", "```go\nfunc JSONThriftGeneric(p DescriptorProvider, opts ...Option) (Generic, error) {\n\tcodec, err := newJSONThriftCodec(p, thriftCodec, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &jsonThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n```", "```go\nfunc JSONThriftGeneric(p DescriptorProvider, opts ...Option) (Generic, error) {\n\tcodecOpts := &conv.Options{\n\t\tNoBase64Binary: false,\n\t}\n\n\tfor _, opt := range opts {\n\t\topt(codecOpts)\n\t}\n\n\tcodec, err := newJsonThriftCodec(p, thriftCodec, codecOpts)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &jsonThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n```\nThis function creates a new JSONThriftGeneric which is a concrete implementation of the Generic interface. It uses the newJsonThriftCodec function to create a new jsonThriftCodec, which is a struct that implements the remote.PayloadCodec interface. The newJsonThriftCodec function takes in a DescriptorProvider (which provides type information for the codec), a remote.PayloadCodec (which is used to encode/decode Thrift messages), and a conv.Options (which contains codec options). The function returns a pointer to a jsonThriftGeneric struct which contains the jsonThriftCodec. If there is an error creating the jsonThriftCodec, the function returns nil and the error."]}, "kitex-pkg/generic/generic.go-JSONPbGeneric": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/generic.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package generic ...\npackage generic\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/cloudwego/dynamicgo/conv\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n)\n\n// Generic ...\ntype Generic interface {\n\tCloser\n\t// PayloadCodec return codec implement\n\tPayloadCodec() remote.PayloadCodec\n\t// PayloadCodecType return the type of codec\n\tPayloadCodecType() serviceinfo.PayloadCodec\n\t// RawThriftBinaryGeneric must be framed\n\tFramed() bool\n\t// GetMethod to get method name if need\n\tGetMethod(req interface{}, method string) (*Method, error)\n}\n\n// Method information\ntype Method struct {\n\tName   string\n\tOneway bool\n}\n\n// BinaryThriftGeneric raw thrift binary Generic\nfunc BinaryThriftGeneric() Generic {\n\treturn &binaryThriftGeneric{}\n}\n\n// MapThriftGeneric map mapping generic\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.MapThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n//\n// String value is returned for binary field by default. You can change the return value to []byte for binary field with SetBinaryWithByteSlice.\n// eg:\n//\n//\tSetBinaryWithByteSlice(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc MapThriftGenericForJSON(p DescriptorProvider) (Generic, error) {\n\tcodec, err := newMapThriftCodecForJSON(p, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &mapThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// HTTPThriftGeneric http mapping Generic.\n// Base64 codec for binary field is disabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.HTTPThriftGeneric(p)\n//\tSetBinaryWithBase64(g, true)\n\n\n\n\n\n\n\n\n\n\nfunc HTTPPbThriftGeneric(p DescriptorProvider, pbp PbDescriptorProvider) (Generic, error) {\n\tcodec, err := newHTTPPbThriftCodec(p, pbp, thriftCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &httpPbThriftGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n\n// JSONThriftGeneric json mapping generic.\n// Base64 codec for binary field is enabled by default. You can change this option with SetBinaryWithBase64.\n// eg:\n//\n//\tg, err := generic.JSONThriftGeneric(p)\n//\tSetBinaryWithBase64(g, false)\n\n\n\n\n\n\n\n\n\n\n// JSONPbGeneric json mapping generic.\n// Uses dynamicgo for json to protobufs conversion, so DynamicGo field is true by default.\n\n\n\n\n\n\n\n\n\n\n\n// SetBinaryWithBase64 enable/disable Base64 codec for binary field.\nfunc SetBinaryWithBase64(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *httpThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t}\n\tcase *jsonThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\t\tif c.codec.dynamicgoEnabled {\n\t\t\tc.codec.convOpts.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithThriftBase.NoBase64Binary = !enable\n\t\t\tc.codec.convOptsWithException.NoBase64Binary = !enable\n\t\t}\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithBase64 = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"Base64Binary is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetBinaryWithByteSlice enable/disable returning []byte for binary field.\nfunc SetBinaryWithByteSlice(g Generic, enable bool) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.binaryWithByteSlice = enable\n\tdefault:\n\t\treturn fmt.Errorf(\"returning []byte for binary fields is unavailable for %#v\", g)\n\t}\n\treturn nil\n}\n\n// SetFieldsForEmptyStructMode is a enum for EnableSetFieldsForEmptyStruct()\ntype SetFieldsForEmptyStructMode uint8\n\nconst (\n\t// NotSetFields means disable\n\tNotSetFields SetFieldsForEmptyStructMode = iota\n\t// SetNonOptiontionalFields means only set required and default fields\n\tSetNonOptiontionalFields\n\t// SetAllFields means set all fields\n\tSetAllFields\n)\n\n// EnableSetFieldsForEmptyStruct enable/disable set all fields of a struct even if it is empty.\n// This option is only applicable to map-generic response (reading) now.\n//\n//\tmode == 0 means disable\n//\tmode == 1 means only set required and default fields\n//\tmode == 2 means set all fields\nfunc EnableSetFieldsForEmptyStruct(g Generic, mode SetFieldsForEmptyStructMode) error {\n\tswitch c := g.(type) {\n\tcase *mapThriftGeneric:\n\t\tif c.codec == nil {\n\t\t\treturn fmt.Errorf(\"empty codec for %#v\", c)\n\t\t}\n\t\tc.codec.setFieldsForEmptyStruct = uint8(mode)\n\tdefault:\n\t\treturn fmt.Errorf(\"SetFieldsForEmptyStruct only supports map-generic at present\")\n\t}\n\treturn nil\n}\n\nvar thriftCodec = thrift.NewThriftCodec()\n\nvar pbCodec = protobuf.NewProtobufCodec()\n\ntype binaryThriftGeneric struct{}\n\nfunc (g *binaryThriftGeneric) Framed() bool {\n\treturn true\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *binaryThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\tpc := &binaryThriftCodec{thriftCodec}\n\treturn pc\n}\n\nfunc (g *binaryThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn &Method{method, false}, nil\n}\n\nfunc (g *binaryThriftGeneric) Close() error {\n\treturn nil\n}\n\ntype mapThriftGeneric struct {\n\tcodec *mapThriftCodec\n}\n\nfunc (g *mapThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *mapThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *mapThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *mapThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *mapThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonThriftGeneric struct {\n\tcodec *jsonThriftCodec\n}\n\nfunc (g *jsonThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *jsonThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype jsonPbGeneric struct {\n\tcodec *jsonPbCodec\n}\n\nfunc (g *jsonPbGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *jsonPbGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Protobuf\n}\n\nfunc (g *jsonPbGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *jsonPbGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req, method)\n}\n\nfunc (g *jsonPbGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpThriftGeneric struct {\n\tcodec *httpThriftCodec\n}\n\nfunc (g *httpThriftGeneric) Framed() bool {\n\treturn g.codec.dynamicgoEnabled\n}\n\nfunc (g *httpThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\ntype httpPbThriftGeneric struct {\n\tcodec *httpPbThriftCodec\n}\n\nfunc (g *httpPbThriftGeneric) Framed() bool {\n\treturn false\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodecType() serviceinfo.PayloadCodec {\n\treturn serviceinfo.Thrift\n}\n\nfunc (g *httpPbThriftGeneric) PayloadCodec() remote.PayloadCodec {\n\treturn g.codec\n}\n\nfunc (g *httpPbThriftGeneric) GetMethod(req interface{}, method string) (*Method, error) {\n\treturn g.codec.getMethod(req)\n}\n\nfunc (g *httpPbThriftGeneric) Close() error {\n\treturn g.codec.Close()\n}\n\n```\n# Function Signature: func JSONPbGeneric(p PbDescriptorProviderDynamicGo, opts ...Option) (Generic, error) {\n# Natural Language Annotations: // JSONPbGeneric json mapping generic.\n// Uses dynamicgo for json to protobufs conversion, so DynamicGo field is true by default.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc JSONPbGeneric(p PbDescriptorProvider, opts ...Option) (Generic, error) {\n\tcodec, err := newJSONPbCodec(p, pbCodec, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &jsonPbGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n```", "```go\nfunc JSONPbGeneric(p PbDescriptorProviderDynamicGo, opts ...Option) (Generic, error) {\n\tcodec, err := newJSONPbCodec(p, pbCodec, opts...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &jsonPbGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n```\n\nThis function creates a new `jsonPbGeneric` instance which implements the `Generic` interface. The `jsonPbGeneric` instance uses a `jsonPbCodec` for encoding and decoding JSON to and from Protocol Buffers. This codec is created using the `newJSONPbCodec` function, which takes a `PbDescriptorProviderDynamicGo` and a `protobuf.PayloadCodec` as arguments. The function returns a `Generic` and an `error`. If there is an error during the creation of the `jsonPbCodec`, the function returns the error. Otherwise, it returns a new `jsonPbGeneric` instance containing the `jsonPbCodec`.", "```go\nfunc JSONPbGeneric(p PbDescriptorProviderDynamicGo, opts ...Option) (Generic, error) {\n\tcodec, err := newJSONPbCodec(p, pbCodec)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, opt := range opts {\n\t\topt(codec)\n\t}\n\treturn &jsonPbGeneric{\n\t\tcodec: codec,\n\t}, nil\n}\n```"]}, "kitex-pkg/generic/thrift/write.go-writeStruct": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/thrift/write.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage thrift\n\nimport (\n\t\"context\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"fmt\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\t\"github.com/tidwall/gjson\"\n\n\t\"github.com/cloudwego/kitex/pkg/generic/descriptor\"\n\t\"github.com/cloudwego/kitex/pkg/generic/proto\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n)\n\ntype writerOption struct {\n\trequestBase *Base // request base from metahandler\n\t// decoding Base64 to binary\n\tbinaryWithBase64 bool\n}\n\ntype writer func(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error\n\ntype fieldGetter func(val interface{}, field *descriptor.FieldDescriptor) (interface{}, bool)\n\nvar mapGetter fieldGetter = func(val interface{}, field *descriptor.FieldDescriptor) (interface{}, bool) {\n\tst := val.(map[string]interface{})\n\tret, ok := st[field.FieldName()]\n\treturn ret, ok\n}\n\nvar pbGetter fieldGetter = func(val interface{}, field *descriptor.FieldDescriptor) (interface{}, bool) {\n\tst := val.(proto.Message)\n\tret, err := st.TryGetFieldByNumber(int(field.ID))\n\treturn ret, err == nil\n}\n\nfunc typeOf(sample interface{}, t *descriptor.TypeDescriptor, opt *writerOption) (descriptor.Type, writer, error) {\n\ttt := t.Type\n\tswitch sample.(type) {\n\tcase bool:\n\t\treturn descriptor.BOOL, writeBool, nil\n\tcase int8, byte:\n\t\tswitch tt {\n\t\tcase descriptor.I08, descriptor.I16, descriptor.I32, descriptor.I64:\n\t\t\treturn tt, writeInt8, nil\n\t\t}\n\tcase int16:\n\t\tswitch tt {\n\t\tcase descriptor.I08, descriptor.I16, descriptor.I32, descriptor.I64:\n\t\t\treturn tt, writeInt16, nil\n\t\t}\n\tcase int32:\n\t\tswitch tt {\n\t\tcase descriptor.I08, descriptor.I16, descriptor.I32, descriptor.I64:\n\t\t\treturn tt, writeInt32, nil\n\t\t}\n\tcase int64:\n\t\tswitch tt {\n\t\tcase descriptor.I08, descriptor.I16, descriptor.I32, descriptor.I64:\n\t\t\treturn tt, writeInt64, nil\n\t\t}\n\tcase float64:\n\t\t// maybe come from json decode\n\t\tswitch tt {\n\t\tcase descriptor.I08, descriptor.I16, descriptor.I32, descriptor.I64, descriptor.DOUBLE:\n\t\t\treturn tt, writeJSONFloat64, nil\n\t\t}\n\tcase json.Number:\n\t\tswitch tt {\n\t\tcase descriptor.I08, descriptor.I16, descriptor.I32, descriptor.I64, descriptor.DOUBLE:\n\t\t\treturn tt, writeJSONNumber, nil\n\t\t}\n\tcase string:\n\t\t// maybe a base64 string encoded from binary\n\t\tif t.Name == \"binary\" && opt.binaryWithBase64 {\n\t\t\treturn descriptor.STRING, writeBase64Binary, nil\n\t\t}\n\t\t// maybe a json number string\n\t\treturn descriptor.STRING, writeString, nil\n\tcase []byte:\n\t\tif tt == descriptor.LIST {\n\t\t\treturn descriptor.LIST, writeBinaryList, nil\n\t\t}\n\t\treturn descriptor.STRING, writeBinary, nil\n\tcase []interface{}:\n\t\treturn descriptor.LIST, writeList, nil\n\tcase map[interface{}]interface{}:\n\t\treturn descriptor.MAP, writeInterfaceMap, nil\n\tcase map[string]interface{}:\n\t\t//  4: optional map<i64, ReqItem> req_items (api.body='req_items')\n\t\t// need parse string into int64\n\t\tswitch tt {\n\t\tcase descriptor.STRUCT:\n\t\t\treturn descriptor.STRUCT, writeStruct, nil\n\t\tcase descriptor.MAP:\n\t\t\treturn descriptor.MAP, writeStringMap, nil\n\t\t}\n\tcase proto.Message:\n\t\treturn descriptor.STRUCT, writeStruct, nil\n\tcase *descriptor.HTTPRequest:\n\t\treturn descriptor.STRUCT, writeHTTPRequest, nil\n\tcase *gjson.Result:\n\t\treturn descriptor.STRUCT, writeJSON, nil\n\tcase nil, descriptor.Void: // nil and Void\n\t\treturn descriptor.VOID, writeVoid, nil\n\t}\n\treturn 0, nil, fmt.Errorf(\"unsupported type:%T, expected type:%s\", sample, tt)\n}\n\nfunc typeJSONOf(data *gjson.Result, t *descriptor.TypeDescriptor, opt *writerOption) (v interface{}, w writer, err error) {\n\ttt := t.Type\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr = perrors.NewProtocolErrorWithType(perrors.InvalidData, fmt.Sprintf(\"json convert error:%#+v\", r))\n\t\t}\n\t}()\n\tswitch tt {\n\tcase descriptor.BOOL:\n\t\tv = data.Bool()\n\t\tw = writeBool\n\t\treturn\n\tcase descriptor.I08:\n\t\tv = int8(data.Int())\n\t\tw = writeInt8\n\t\treturn\n\tcase descriptor.I16:\n\t\tv = int16(data.Int())\n\t\tw = writeInt16\n\t\treturn\n\tcase descriptor.I32:\n\t\tv = int32(data.Int())\n\t\tw = writeInt32\n\t\treturn\n\tcase descriptor.I64:\n\t\tv = data.Int()\n\t\tw = writeInt64\n\t\treturn\n\tcase descriptor.DOUBLE:\n\t\tv = data.Float()\n\t\tw = writeJSONFloat64\n\t\treturn\n\tcase descriptor.STRING:\n\t\tv = data.String()\n\t\tif t.Name == \"binary\" && opt.binaryWithBase64 {\n\t\t\tw = writeBase64Binary\n\t\t} else {\n\t\t\tw = writeString\n\t\t}\n\t\treturn\n\t// case descriptor.BINARY:\n\t//\treturn writeBinary, nil\n\tcase descriptor.SET, descriptor.LIST:\n\t\tv = data.Array()\n\t\tw = writeJSONList\n\t\treturn\n\tcase descriptor.MAP:\n\t\tv = data.Map()\n\t\tw = writeStringJSONMap\n\t\treturn\n\tcase descriptor.STRUCT:\n\t\tv = data\n\t\tw = writeJSON\n\t\treturn\n\tcase descriptor.VOID: // nil and Void\n\t\tv = data\n\t\tw = writeVoid\n\t\treturn\n\t}\n\treturn 0, nil, fmt.Errorf(\"data:%#v, expected type:%s, err:%#v\", data, tt, err)\n}\n\nfunc nextWriter(sample interface{}, t *descriptor.TypeDescriptor, opt *writerOption) (writer, error) {\n\ttt, fn, err := typeOf(sample, t, opt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif t.Type == thrift.SET && tt == thrift.LIST {\n\t\ttt = thrift.SET\n\t}\n\treturn fn, assertType(t.Type, tt)\n}\n\nfunc nextJSONWriter(data *gjson.Result, t *descriptor.TypeDescriptor, opt *writerOption) (interface{}, writer, error) {\n\tv, fn, err := typeJSONOf(data, t, opt)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn v, fn, nil\n}\n\nfunc writeEmptyValue(out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tswitch t.Type {\n\tcase descriptor.BOOL:\n\t\treturn out.WriteBool(false)\n\tcase descriptor.I08:\n\t\treturn out.WriteByte(0)\n\tcase descriptor.I16:\n\t\treturn out.WriteI16(0)\n\tcase descriptor.I32:\n\t\treturn out.WriteI32(0)\n\tcase descriptor.I64:\n\t\treturn out.WriteI64(0)\n\tcase descriptor.DOUBLE:\n\t\treturn out.WriteDouble(0)\n\tcase descriptor.STRING:\n\t\tif t.Name == \"binary\" && opt.binaryWithBase64 {\n\t\t\treturn out.WriteBinary([]byte{})\n\t\t} else {\n\t\t\treturn out.WriteString(\"\")\n\t\t}\n\tcase descriptor.LIST, descriptor.SET:\n\t\tif err := out.WriteListBegin(t.Elem.Type.ToThriftTType(), 0); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn out.WriteListEnd()\n\tcase descriptor.MAP:\n\t\tif err := out.WriteMapBegin(t.Key.Type.ToThriftTType(), t.Elem.Type.ToThriftTType(), 0); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn out.WriteMapEnd()\n\tcase descriptor.STRUCT:\n\t\tif err := out.WriteStructBegin(t.Name); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := out.WriteFieldStop(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn out.WriteStructEnd()\n\tcase descriptor.VOID:\n\t\treturn nil\n\t}\n\treturn fmt.Errorf(\"unsupported type:%T\", t)\n}\n\nfunc wrapStructWriter(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tif err := out.WriteStructBegin(t.Struct.Name); err != nil {\n\t\treturn err\n\t}\n\tfor name, field := range t.Struct.FieldsByName {\n\t\tif field.IsException {\n\t\t\t// generic server ignore the exception, because no description for exception\n\t\t\t// generic handler just return error\n\t\t\tcontinue\n\t\t}\n\t\tif val != nil {\n\t\t\tif err := out.WriteFieldBegin(field.Name, field.Type.Type.ToThriftTType(), int16(field.ID)); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\twriter, err := nextWriter(val, field.Type, opt)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"nextWriter of field[%s] error %w\", name, err)\n\t\t\t}\n\t\t\tif err := writer(ctx, val, out, field.Type, opt); err != nil {\n\t\t\t\treturn fmt.Errorf(\"writer of field[%s] error %w\", name, err)\n\t\t\t}\n\t\t\tif err := out.WriteFieldEnd(); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\tif err := out.WriteFieldStop(); err != nil {\n\t\treturn err\n\t}\n\treturn out.WriteStructEnd()\n}\n\nfunc wrapJSONWriter(ctx context.Context, val *gjson.Result, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tif err := out.WriteStructBegin(t.Struct.Name); err != nil {\n\t\treturn err\n\t}\n\tfor name, field := range t.Struct.FieldsByName {\n\t\tif field.IsException {\n\t\t\t// generic server ignore the exception, because no description for exception\n\t\t\t// generic handler just return error\n\t\t\tcontinue\n\t\t}\n\t\tif err := out.WriteFieldBegin(field.Name, field.Type.Type.ToThriftTType(), int16(field.ID)); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tv, writer, err := nextJSONWriter(val, field.Type, opt)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"nextJSONWriter of field[%s] error %w\", name, err)\n\t\t}\n\t\tif err := writer(ctx, v, out, field.Type, opt); err != nil {\n\t\t\treturn fmt.Errorf(\"writer of field[%s] error %w\", name, err)\n\t\t}\n\t\tif err := out.WriteFieldEnd(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif err := out.WriteFieldStop(); err != nil {\n\t\treturn err\n\t}\n\treturn out.WriteStructEnd()\n}\n\nfunc writeVoid(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\treturn writeStruct(ctx, map[string]interface{}{}, out, t, opt)\n}\n\nfunc writeBool(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\treturn out.WriteBool(val.(bool))\n}\n\nfunc writeInt8(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tvar i int8\n\tswitch val := val.(type) {\n\tcase int8:\n\t\ti = val\n\tcase uint8:\n\t\ti = int8(val)\n\tdefault:\n\t\treturn fmt.Errorf(\"unsupported type: %T\", val)\n\t}\n\t// compatible with lossless conversion\n\tswitch t.Type {\n\tcase thrift.I08:\n\t\treturn out.WriteByte(i)\n\tcase thrift.I16:\n\t\treturn out.WriteI16(int16(i))\n\tcase thrift.I32:\n\t\treturn out.WriteI32(int32(i))\n\tcase thrift.I64:\n\t\treturn out.WriteI64(int64(i))\n\t}\n\treturn fmt.Errorf(\"need int type, but got: %s\", t.Type)\n}\n\nfunc writeInt16(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\t// compatible with lossless conversion\n\ti := val.(int16)\n\tswitch t.Type {\n\tcase thrift.I08:\n\t\tif i&0xff != i {\n\t\t\treturn fmt.Errorf(\"value is beyond range of i8: %v\", i)\n\t\t}\n\t\treturn out.WriteByte(int8(i))\n\tcase thrift.I16:\n\t\treturn out.WriteI16(i)\n\tcase thrift.I32:\n\t\treturn out.WriteI32(int32(i))\n\tcase thrift.I64:\n\t\treturn out.WriteI64(int64(i))\n\t}\n\treturn fmt.Errorf(\"need int type, but got: %s\", t.Type)\n}\n\nfunc writeInt32(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\t// compatible with lossless conversion\n\ti := val.(int32)\n\tswitch t.Type {\n\tcase thrift.I08:\n\t\tif i&0xff != i {\n\t\t\treturn fmt.Errorf(\"value is beyond range of i8: %v\", i)\n\t\t}\n\t\treturn out.WriteByte(int8(i))\n\tcase thrift.I16:\n\t\tif i&0xffff != i {\n\t\t\treturn fmt.Errorf(\"value is beyond range of i16: %v\", i)\n\t\t}\n\t\treturn out.WriteI16(int16(i))\n\tcase thrift.I32:\n\t\treturn out.WriteI32(i)\n\tcase thrift.I64:\n\t\treturn out.WriteI64(int64(i))\n\t}\n\treturn fmt.Errorf(\"need int type, but got: %s\", t.Type)\n}\n\nfunc writeInt64(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\t// compatible with lossless conversion\n\ti := val.(int64)\n\tswitch t.Type {\n\tcase thrift.I08:\n\t\tif i&0xff != i {\n\t\t\treturn fmt.Errorf(\"value is beyond range of i8: %v\", i)\n\t\t}\n\t\treturn out.WriteByte(int8(i))\n\tcase thrift.I16:\n\t\tif i&0xffff != i {\n\t\t\treturn fmt.Errorf(\"value is beyond range of i16: %v\", i)\n\t\t}\n\t\treturn out.WriteI16(int16(i))\n\tcase thrift.I32:\n\t\tif i&0xffffffff != i {\n\t\t\treturn fmt.Errorf(\"value is beyond range of i32: %v\", i)\n\t\t}\n\t\treturn out.WriteI32(int32(i))\n\tcase thrift.I64:\n\t\treturn out.WriteI64(i)\n\t}\n\treturn fmt.Errorf(\"need int type, but got: %s\", t.Type)\n}\n\nfunc writeJSONNumber(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tjn := val.(json.Number)\n\tswitch t.Type {\n\tcase thrift.I08:\n\t\ti, err := jn.Int64()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn writeInt8(ctx, int8(i), out, t, opt)\n\tcase thrift.I16:\n\t\ti, err := jn.Int64()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn writeInt16(ctx, int16(i), out, t, opt)\n\tcase thrift.I32:\n\t\ti, err := jn.Int64()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn writeInt32(ctx, int32(i), out, t, opt)\n\tcase thrift.I64:\n\t\ti, err := jn.Int64()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn writeInt64(ctx, i, out, t, opt)\n\tcase thrift.DOUBLE:\n\t\ti, err := jn.Float64()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn writeFloat64(ctx, i, out, t, opt)\n\t}\n\treturn nil\n}\n\nfunc writeJSONFloat64(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\ti := val.(float64)\n\tswitch t.Type {\n\tcase thrift.I08:\n\t\treturn writeInt8(ctx, int8(i), out, t, opt)\n\tcase thrift.I16:\n\t\treturn writeInt16(ctx, int16(i), out, t, opt)\n\tcase thrift.I32:\n\t\treturn writeInt32(ctx, int32(i), out, t, opt)\n\tcase thrift.I64:\n\t\treturn writeInt64(ctx, int64(i), out, t, opt)\n\tcase thrift.DOUBLE:\n\t\treturn writeFloat64(ctx, i, out, t, opt)\n\t}\n\treturn fmt.Errorf(\"need number type, but got: %s\", t.Type)\n}\n\nfunc writeFloat64(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\treturn out.WriteDouble(val.(float64))\n}\n\nfunc writeString(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\treturn out.WriteString(val.(string))\n}\n\nfunc writeBase64Binary(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tbytes, err := base64.StdEncoding.DecodeString(val.(string))\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn out.WriteBinary(bytes)\n}\n\nfunc writeBinary(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\treturn out.WriteBinary(val.([]byte))\n}\n\nfunc writeBinaryList(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tl := val.([]byte)\n\tlength := len(l)\n\tif err := out.WriteListBegin(t.Elem.Type.ToThriftTType(), length); err != nil {\n\t\treturn err\n\t}\n\tfor _, b := range l {\n\t\tif err := out.WriteByte(int8(b)); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn out.WriteListEnd()\n}\n\nfunc writeList(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tl := val.([]interface{})\n\tlength := len(l)\n\tif err := out.WriteListBegin(t.Elem.Type.ToThriftTType(), length); err != nil {\n\t\treturn err\n\t}\n\tif length == 0 {\n\t\treturn out.WriteListEnd()\n\t}\n\tvar (\n\t\twriter writer\n\t\terr    error\n\t)\n\tfor _, elem := range l {\n\t\tif elem == nil {\n\t\t\tif err = writeEmptyValue(out, t.Elem, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\tif writer == nil {\n\t\t\t\tif writer, err = nextWriter(elem, t.Elem, opt); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err := writer(ctx, elem, out, t.Elem, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn out.WriteListEnd()\n}\n\nfunc writeJSONList(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tl := val.([]gjson.Result)\n\tlength := len(l)\n\tif err := out.WriteListBegin(t.Elem.Type.ToThriftTType(), length); err != nil {\n\t\treturn err\n\t}\n\tif length == 0 {\n\t\treturn out.WriteListEnd()\n\t}\n\tfor _, elem := range l {\n\t\tv, writer, err := nextJSONWriter(&elem, t.Elem, opt)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := writer(ctx, v, out, t.Elem, opt); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn out.WriteListEnd()\n}\n\nfunc writeInterfaceMap(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tm := val.(map[interface{}]interface{})\n\tlength := len(m)\n\tif err := out.WriteMapBegin(t.Key.Type.ToThriftTType(), t.Elem.Type.ToThriftTType(), length); err != nil {\n\t\treturn err\n\t}\n\tif length == 0 {\n\t\treturn out.WriteMapEnd()\n\t}\n\tvar (\n\t\tkeyWriter  writer\n\t\telemWriter writer\n\t\terr        error\n\t)\n\tfor key, elem := range m {\n\t\tif keyWriter == nil {\n\t\t\tif keyWriter, err = nextWriter(key, t.Key, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif err := keyWriter(ctx, key, out, t.Key, opt); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif elem == nil {\n\t\t\tif err = writeEmptyValue(out, t.Elem, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\tif elemWriter == nil {\n\t\t\t\tif elemWriter, err = nextWriter(elem, t.Elem, opt); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err := elemWriter(ctx, elem, out, t.Elem, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn out.WriteMapEnd()\n}\n\nfunc writeStringMap(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tm := val.(map[string]interface{})\n\tlength := len(m)\n\tif err := out.WriteMapBegin(t.Key.Type.ToThriftTType(), t.Elem.Type.ToThriftTType(), length); err != nil {\n\t\treturn err\n\t}\n\tif length == 0 {\n\t\treturn out.WriteMapEnd()\n\t}\n\n\tvar (\n\t\tkeyWriter  writer\n\t\telemWriter writer\n\t)\n\tfor key, elem := range m {\n\t\t_key, err := buildinTypeFromString(key, t.Key)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif keyWriter == nil {\n\t\t\tif keyWriter, err = nextWriter(_key, t.Key, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif err := keyWriter(ctx, _key, out, t.Key, opt); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif elem == nil {\n\t\t\tif err = writeEmptyValue(out, t.Elem, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\tif elemWriter == nil {\n\t\t\t\tif elemWriter, err = nextWriter(elem, t.Elem, opt); err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\tif err := elemWriter(ctx, elem, out, t.Elem, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn out.WriteMapEnd()\n}\n\nfunc writeStringJSONMap(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *\n```\n# Function Signature: func writeStruct(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n# Natural Language Annotations: // writeStruct iter with Descriptor, can check the field's required and others\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc writeStruct(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tvar err error\n\tswitch v := val.(type) {\n\tcase map[string]interface{}:\n\t\terr = wrapStructWriter(ctx, v, out, t, opt)\n\tcase proto.Message:\n\t\terr = wrapStructWriter(ctx, v, out, t, opt)\n\tdefault:\n\t\terr = fmt.Errorf(\"unsupported type:%T, expected type:%s\", v, t)\n\t}\n\tif err != nil {\n\t\treturn err\n\t}\n\tfor _, field := range t.Struct.FieldsByName {\n\t\tif field.Required {\n\t\t\t_, err := mapGetter(val, field)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"field[%s] is required but not set\", field.Name)\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n```", "```go\nfunc writeStruct(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\tif err := out.WriteStructBegin(t.Struct.Name); err != nil {\n\t\treturn err\n\t}\n\tvar fieldGetter fieldGetter\n\tvar st interface{}\n\tswitch val := val.(type) {\n\tcase map[string]interface{}:\n\t\tfieldGetter = mapGetter\n\t\tst = val\n\tcase proto.Message:\n\t\tfieldGetter = pbGetter\n\t\tst = val\n\tdefault:\n\t\treturn fmt.Errorf(\"unsupported struct type:%T\", val)\n\t}\n\tfor name, field := range t.Struct.FieldsByName {\n\t\tif field.IsException {\n\t\t\t// generic server ignore the exception, because no description for exception\n\t\t\t// generic handler just return error\n\t\t\tcontinue\n\t\t}\n\t\tif err := out.WriteFieldBegin(field.Name, field.Type.Type.ToThriftTType(), int16(field.ID)); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tv, ok := fieldGetter(st, field)\n\t\tif !ok {\n\t\t\tif field.Required {\n\t\t\t\treturn fmt.Errorf(\"required field[%s] not exist\", name)\n\t\t\t}\n\t\t\tif err := writeEmptyValue(out, field.Type, opt); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\twriter, err := nextWriter(v, field.Type, opt)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"nextWriter of field[%s] error %w\", name, err)\n\t\t\t}\n\t\t\tif err := writer(ctx, v, out, field.Type, opt); err != nil {\n\t\t\t\treturn fmt.Errorf(\"writer of field[%s] error %w\", name, err)\n\t\t\t}\n\t\t}\n\t\tif err := out.WriteFieldEnd(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif err := out.WriteFieldStop(); err != nil {\n\t\treturn err\n\t}\n\treturn out.WriteStructEnd()\n}\n```\nThis code is a modified version of the `writeStruct` function. It iterates over the fields in the struct descriptor and checks for each field if it is optional or required. If it is required and does not exist in the input map, an error is returned. If it is optional and does not exist, an empty value is written to the output protocol. If it does exist, the appropriate writer function is called to handle the field's value.", "```go\nfunc writeStruct(ctx context.Context, val interface{}, out thrift.TProtocol, t *descriptor.TypeDescriptor, opt *writerOption) error {\n\t// First check if the value is nil\n\tif val == nil {\n\t\t// If the struct is required, return an error\n\t\tif t.Required {\n\t\t\treturn fmt.Errorf(\"required field is nil\")\n\t\t}\n\t\t// If the struct is not required, write an empty value\n\t\treturn writeEmptyValue(out, t, opt)\n\t}\n\n\t// Write the struct begin\n\tif err := out.WriteStructBegin(t.Struct.Name); err != nil {\n\t\treturn err\n\t}\n\n\t// Iterate over the fields in the struct\n\tfor name, field := range t.Struct.FieldsByName {\n\t\t// If the field is an exception, ignore it\n\t\tif field.IsException {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Get the field's value\n\t\tv, ok := mapGetter(val, field)\n\t\tif !ok {\n\t\t\tv = nil\n\t\t}\n\n\t\t// If the field is required and its value is nil, return an error\n\t\tif field.Required && v == nil {\n\t\t\treturn fmt.Errorf(\"required field [%s] is nil\", name)\n\t\t}\n\n\t\t// If the field's value is nil, skip it\n\t\tif v == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Write the field begin\n\t\tif err := out.WriteFieldBegin(field.Name, field.Type.Type.ToThriftTType(), int16(field.ID)); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Get the next writer for the field's type\n\t\twriter, err := nextWriter(v, field.Type, opt)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"nextWriter of field [%s] error: %w\", name, err)\n\t\t}\n\n\t\t// Use the writer to write the field's value\n\t\tif err := writer(ctx, v, out, field.Type, opt); err != nil {\n\t\t\treturn fmt.Errorf(\"writer of field [%s] error: %w\", name, err)\n\t\t}\n\n\t\t// Write the field end\n\t\tif err := out.WriteFieldEnd(); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Write the field stop\n\tif err := out.WriteFieldStop(); err != nil {\n\t\treturn err\n\t}\n\n\t// Write the struct end\n\treturn out.WriteStructEnd()\n}\n```"]}, "kitex-pkg/generic/proto/json.go-Write": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/proto/json.go:\n```\n/*\n * Copyright 2023 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage proto\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\n\t\"github.com/cloudwego/dynamicgo/conv\"\n\tdconvj2p \"github.com/cloudwego/dynamicgo/conv/j2p\"\n\tdconvp2j \"github.com/cloudwego/dynamicgo/conv/p2j\"\n\tdproto \"github.com/cloudwego/dynamicgo/proto\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/perrors\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// NewWriteJSON build WriteJSON according to ServiceDescriptor\nfunc NewWriteJSON(svc *dproto.ServiceDescriptor, method string, isClient bool, convOpts *conv.Options) (*WriteJSON, error) {\n\tfnDsc := svc.LookupMethodByName(method)\n\tif fnDsc == nil {\n\t\treturn nil, fmt.Errorf(\"missing method: %s in service: %s\", method, svc.Name())\n\t}\n\n\t// from the proto.ServiceDescriptor, get the TypeDescriptor\n\ttypeDescriptor := fnDsc.Input()\n\tif !isClient {\n\t\ttypeDescriptor = fnDsc.Output()\n\t}\n\n\tws := &WriteJSON{\n\t\tdynamicgoConvOpts: convOpts,\n\t\tdynamicgoTypeDsc:  typeDescriptor,\n\t\tisClient:          isClient,\n\t}\n\treturn ws, nil\n}\n\n// WriteJSON implement of MessageWriter\ntype WriteJSON struct {\n\tdynamicgoConvOpts *conv.Options\n\tdynamicgoTypeDsc  *dproto.TypeDescriptor\n\tisClient          bool\n}\n\nvar _ MessageWriter = (*WriteJSON)(nil)\n\n// Write converts msg to protobuf wire format and returns an output bytebuffer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NewReadJSON build ReadJSON according to ServiceDescriptor\nfunc NewReadJSON(svc *dproto.ServiceDescriptor, isClient bool, convOpts *conv.Options) (*ReadJSON, error) {\n\t// extract svc to be used to convert later\n\treturn &ReadJSON{\n\t\tdynamicgoConvOpts: convOpts,\n\t\tdynamicgoSvcDsc:   svc,\n\t\tisClient:          isClient,\n\t}, nil\n}\n\n// ReadJSON implement of MessageReaderWithMethod\ntype ReadJSON struct {\n\tdynamicgoConvOpts *conv.Options\n\tdynamicgoSvcDsc   *dproto.ServiceDescriptor\n\tisClient          bool\n}\n\nvar _ MessageReader = (*ReadJSON)(nil)\n\n// Read reads data from actualMsgBuf and convert to json string\nfunc (m *ReadJSON) Read(ctx context.Context, method string, actualMsgBuf []byte) (interface{}, error) {\n\t// create dynamic message here, once method string has been extracted\n\tfnDsc := m.dynamicgoSvcDsc.LookupMethodByName(method)\n\tif fnDsc == nil {\n\t\treturn nil, fmt.Errorf(\"missing method: %s in service: %s\", method, m.dynamicgoSvcDsc.Name())\n\t}\n\n\t// from the dproto.ServiceDescriptor, get the TypeDescriptor\n\ttypeDescriptor := fnDsc.Output()\n\tif !m.isClient {\n\t\ttypeDescriptor = fnDsc.Input()\n\t}\n\n\tcv := dconvp2j.NewBinaryConv(*m.dynamicgoConvOpts)\n\tout, err := cv.Do(context.Background(), typeDescriptor, actualMsgBuf)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn string(out), nil\n}\n\n```\n# Function Signature: func (m *WriteJSON) Write(ctx context.Context, msg interface{}) (interface{}, error) {\n# Natural Language Annotations: // Write converts msg to protobuf wire format and returns an output bytebuffer\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (m *WriteJSON) Write(ctx context.Context, msg interface{}) (interface{}, error) {\n\t// Check if the message type matches the expected type\n\ttypeDescriptor := m.dynamicgoTypeDsc\n\tif typeDescriptor == nil {\n\t\treturn nil, fmt.Errorf(\"missing type descriptor for method\")\n\t}\n\n\t// Convert the message to protobuf wire format\n\tcv := dconvj2p.NewBinaryConv(*m.dynamicgoConvOpts)\n\tout, err := cv.Do(context.Background(), typeDescriptor, msg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Return the output bytebuffer\n\treturn out, nil\n}\n```", "```go\nfunc (m *WriteJSON) Write(ctx context.Context, msg interface{}) (interface{}, error) {\n\t// convert msg to protobuf wire format\n\tcv := dconvj2p.NewJSONConv(*m.dynamicgoConvOpts)\n\tbuf, err := cv.Do(ctx, m.dynamicgoTypeDsc, msg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// return output bytebuffer\n\treturn buf, nil\n}\n```", "```go\nfunc (m *WriteJSON) Write(ctx context.Context, msg interface{}) (interface{}, error) {\n\ttypeDescriptor := m.dynamicgoTypeDsc\n\tif !m.isClient {\n\t\ttypeDescriptor = m.dynamicgoTypeDsc.Elem()\n\t}\n\n\tcv := dconvj2p.NewBinaryConv(*m.dynamicgoConvOpts)\n\tout, err := cv.Do(ctx, typeDescriptor, msg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn out, nil\n}\n```"]}, "kitex-pkg/generic/descriptor/tree.go-addRoute": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/generic/descriptor/tree.go:\n```\n/*\n * Copyright 2013 Julien Schmidt. All rights reserved.\n * Use of this source code is governed by a BSD-style license that can be found\n * in the LICENSE file.\n *\n * This file may have been modified by CloudWeGo authors. All CloudWeGo\n * Modifications are Copyright 2021 CloudWeGo Authors.\n */\n\npackage descriptor\n\nimport (\n\t\"fmt\"\n\t\"net/url\"\n\t\"strings\"\n)\n\nfunc countParams(path string) uint16 {\n\tvar n uint\n\tfor i := range []byte(path) {\n\t\tswitch path[i] {\n\t\tcase ':', '*':\n\t\t\tn++\n\t\t}\n\t}\n\treturn uint16(n)\n}\n\ntype nodeType uint8\n\nconst (\n\tstatic nodeType = iota // default\n\tparam\n\tcatchAll\n\tparamLabel = byte(':')\n\tanyLabel   = byte('*')\n\tslash      = \"/\"\n\tnilString  = \"\"\n)\n\ntype (\n\tnode struct {\n\t\tnType    nodeType\n\t\tlabel    byte\n\t\tprefix   string\n\t\tparent   *node\n\t\tchildren children\n\t\t// original path\n\t\tppath string\n\t\t// param names\n\t\tpnames     []string\n\t\tfunction   *FunctionDescriptor\n\t\tparamChild *node\n\t\tanyChild   *node\n\t\t// isLeaf indicates that node does not have child routes\n\t\tisLeaf bool\n\t}\n\tchildren []*node\n)\n\nfunc checkPathValid(path string) {\n\tif path == nilString {\n\t\tpanic(\"empty path\")\n\t}\n\tif path[0] != '/' {\n\t\tpanic(\"path must begin with '/'\")\n\t}\n\tfor i, c := range []byte(path) {\n\t\tswitch c {\n\t\tcase ':':\n\t\t\tif (i < len(path)-1 && path[i+1] == '/') || i == (len(path)-1) {\n\t\t\t\tpanic(\"wildcards must be named with a non-empty name in path '\" + path + \"'\")\n\t\t\t}\n\t\t\ti++\n\t\t\tfor ; i < len(path) && path[i] != '/'; i++ {\n\t\t\t\tif path[i] == ':' || path[i] == '*' {\n\t\t\t\t\tpanic(\"only one wildcard per path segment is allowed, find multi in path '\" + path + \"'\")\n\t\t\t\t}\n\t\t\t}\n\t\tcase '*':\n\t\t\tif i == len(path)-1 {\n\t\t\t\tpanic(\"wildcards must be named with a non-empty name in path '\" + path + \"'\")\n\t\t\t}\n\t\t\tif i > 0 && path[i-1] != '/' {\n\t\t\t\tpanic(\" no / before wildcards in path \" + path)\n\t\t\t}\n\t\t\tfor ; i < len(path); i++ {\n\t\t\t\tif path[i] == '/' {\n\t\t\t\t\tpanic(\"catch-all routes are only allowed at the end of the path in path '\" + path + \"'\")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// addRoute adds a node with the given function to the path.\n// Not concurrency-safe!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (n *node) insert(path string, function *FunctionDescriptor, t nodeType, ppath string, pnames []string) {\n\tcurrentNode := n\n\tsearch := path\n\n\tfor {\n\t\tsearchLen := len(search)\n\t\tprefixLen := len(currentNode.prefix)\n\t\tlcpLen := 0\n\n\t\tmax := prefixLen\n\t\tif searchLen < max {\n\t\t\tmax = searchLen\n\t\t}\n\t\tfor ; lcpLen < max && search[lcpLen] == currentNode.prefix[lcpLen]; lcpLen++ {\n\t\t}\n\n\t\tif lcpLen == 0 {\n\t\t\tcurrentNode.label = search[0]\n\t\t\tcurrentNode.prefix = search\n\t\t\tif function != nil {\n\t\t\t\tcurrentNode.nType = t\n\t\t\t\tcurrentNode.function = function\n\t\t\t\tcurrentNode.ppath = ppath\n\t\t\t\tcurrentNode.pnames = pnames\n\t\t\t}\n\t\t\tcurrentNode.isLeaf = currentNode.children == nil && currentNode.paramChild == nil && currentNode.anyChild == nil\n\t\t} else if lcpLen < prefixLen {\n\t\t\t// Split node\n\t\t\tn := newNode(\n\t\t\t\tcurrentNode.nType,\n\t\t\t\tcurrentNode.prefix[lcpLen:],\n\t\t\t\tcurrentNode,\n\t\t\t\tcurrentNode.children,\n\t\t\t\tcurrentNode.function,\n\t\t\t\tcurrentNode.ppath,\n\t\t\t\tcurrentNode.pnames,\n\t\t\t\tcurrentNode.paramChild,\n\t\t\t\tcurrentNode.anyChild,\n\t\t\t)\n\t\t\t// Update parent path for all children to new node\n\t\t\tfor _, child := range currentNode.children {\n\t\t\t\tchild.parent = n\n\t\t\t}\n\t\t\tif currentNode.paramChild != nil {\n\t\t\t\tcurrentNode.paramChild.parent = n\n\t\t\t}\n\t\t\tif currentNode.anyChild != nil {\n\t\t\t\tcurrentNode.anyChild.parent = n\n\t\t\t}\n\n\t\t\t// Reset parent node\n\t\t\tcurrentNode.nType = static\n\t\t\tcurrentNode.label = currentNode.prefix[0]\n\t\t\tcurrentNode.prefix = currentNode.prefix[:lcpLen]\n\t\t\tcurrentNode.children = nil\n\t\t\tcurrentNode.function = nil\n\t\t\tcurrentNode.ppath = nilString\n\t\t\tcurrentNode.pnames = nil\n\t\t\tcurrentNode.paramChild = nil\n\t\t\tcurrentNode.anyChild = nil\n\t\t\tcurrentNode.isLeaf = false\n\n\t\t\t// Only Static children could reach here\n\t\t\tcurrentNode.children = append(currentNode.children, n)\n\n\t\t\tif lcpLen == searchLen {\n\t\t\t\t// At parent node\n\t\t\t\tcurrentNode.nType = t\n\t\t\t\tcurrentNode.function = function\n\t\t\t\tcurrentNode.ppath = ppath\n\t\t\t\tcurrentNode.pnames = pnames\n\t\t\t} else {\n\t\t\t\t// Create child node\n\t\t\t\tn = newNode(t, search[lcpLen:], currentNode, nil, function, ppath, pnames, nil, nil)\n\t\t\t\t// Only Static children could reach here\n\t\t\t\tcurrentNode.children = append(currentNode.children, n)\n\t\t\t}\n\t\t\tcurrentNode.isLeaf = currentNode.children == nil && currentNode.paramChild == nil && currentNode.anyChild == nil\n\t\t} else if lcpLen < searchLen {\n\t\t\tsearch = search[lcpLen:]\n\t\t\tc := currentNode.findChildWithLabel(search[0])\n\t\t\tif c != nil {\n\t\t\t\t// Go deeper\n\t\t\t\tcurrentNode = c\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Create child node\n\t\t\tn := newNode(t, search, currentNode, nil, function, ppath, pnames, nil, nil)\n\t\t\tswitch t {\n\t\t\tcase static:\n\t\t\t\tcurrentNode.children = append(currentNode.children, n)\n\t\t\tcase param:\n\t\t\t\tcurrentNode.paramChild = n\n\t\t\tcase catchAll:\n\t\t\t\tcurrentNode.anyChild = n\n\t\t\t}\n\t\t\tcurrentNode.isLeaf = currentNode.children == nil && currentNode.paramChild == nil && currentNode.anyChild == nil\n\t\t} else {\n\t\t\t// Node already exists\n\t\t\tif currentNode.function != nil && function != nil {\n\t\t\t\tpanic(\"handlers are already registered for path '\" + ppath + \"'\")\n\t\t\t}\n\n\t\t\tif function != nil {\n\t\t\t\tcurrentNode.function = function\n\t\t\t\tcurrentNode.ppath = ppath\n\t\t\t\tif len(currentNode.pnames) == 0 {\n\t\t\t\t\tcurrentNode.pnames = pnames\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn\n\t}\n}\n\n// Returns the function registered with the given path (key). The values of\n// wildcards are saved to a map.\n// If no function can be found, a TSR (trailing slash redirect) recommendation is\n// made if a function exists with an extra (without the) trailing slash for the\n// given path.\nfunc (n *node) getValue(path string, params func() *Params, unescape bool) (function *FunctionDescriptor, ps *Params, tsr bool) {\n\tvar (\n\t\tcn          = n    // current node\n\t\tsearch      = path // current path\n\t\tsearchIndex = 0\n\t\tparamIndex  int\n\t)\n\n\tbacktrackToNextNodeType := func(fromNodeType nodeType) (nextNodeType nodeType, valid bool) {\n\t\tprevious := cn\n\t\tcn = previous.parent\n\t\tvalid = cn != nil\n\n\t\t// Next node type by priority\n\t\tif previous.nType == catchAll {\n\t\t\tnextNodeType = static\n\t\t} else {\n\t\t\tnextNodeType = previous.nType + 1\n\t\t}\n\n\t\tif fromNodeType == static {\n\t\t\t// when backtracking is done from static type block we did not change search so nothing to restore\n\t\t\treturn\n\t\t}\n\n\t\t// restore search to value it was before we move to current node we are backtracking from.\n\t\tif previous.nType == static {\n\t\t\tsearchIndex -= len(previous.prefix)\n\t\t} else {\n\t\t\tparamIndex--\n\t\t\t// for param/any node.prefix value is always `:`/`*` so we cannot deduce searchIndex from that and must use pValue\n\t\t\t// for that index as it would also contain part of path we cut off before moving into node we are backtracking from\n\t\t\tsearchIndex -= len(ps.params[paramIndex].Value)\n\t\t\tps.params = ps.params[:paramIndex]\n\t\t}\n\t\tsearch = path[searchIndex:]\n\t\treturn\n\t}\n\n\t// search order: static > param > any\n\tfor {\n\t\tif cn.nType == static {\n\t\t\tif len(search) >= len(cn.prefix) && cn.prefix == search[:len(cn.prefix)] {\n\t\t\t\t// Continue search\n\t\t\t\tsearch = search[len(cn.prefix):]\n\t\t\t\tsearchIndex = searchIndex + len(cn.prefix)\n\t\t\t} else {\n\t\t\t\t// not equal\n\t\t\t\tif (len(cn.prefix) == len(search)+1) &&\n\t\t\t\t\t(cn.prefix[len(search)]) == '/' && cn.prefix[:len(search)] == search && (cn.function != nil || cn.anyChild != nil) {\n\t\t\t\t\ttsr = true\n\t\t\t\t}\n\t\t\t\t// No matching prefix, let's backtrack to the first possible alternative node of the decision path\n\t\t\t\tnk, ok := backtrackToNextNodeType(static)\n\t\t\t\tif !ok {\n\t\t\t\t\treturn // No other possibilities on the decision path\n\t\t\t\t} else if nk == param {\n\t\t\t\t\tgoto Param\n\t\t\t\t} else {\n\t\t\t\t\t// Not found (this should never be possible for static node we are looking currently)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif search == nilString && cn.function != nil {\n\t\t\tfunction = cn.function\n\t\t\tbreak\n\t\t}\n\n\t\t// Static node\n\t\tif search != nilString {\n\t\t\t// If it can execute that logic, there is handler registered on the current node and search is `/`.\n\t\t\tif search == \"/\" && cn.function != nil {\n\t\t\t\ttsr = true\n\t\t\t}\n\t\t\tif child := cn.findChild(search[0]); child != nil {\n\t\t\t\tcn = child\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\tif search == nilString {\n\t\t\tif cd := cn.findChild('/'); cd != nil && (cd.function != nil || cd.anyChild != nil) {\n\t\t\t\ttsr = true\n\t\t\t}\n\t\t}\n\n\tParam:\n\t\t// Param node\n\t\tif child := cn.paramChild; search != nilString && child != nil {\n\t\t\tcn = child\n\t\t\ti := strings.Index(search, slash)\n\t\t\tif i == -1 {\n\t\t\t\ti = len(search)\n\t\t\t}\n\t\t\tif ps == nil {\n\t\t\t\tps = params()\n\t\t\t}\n\t\t\tval := search[:i]\n\t\t\tif unescape {\n\t\t\t\tif v, err := url.QueryUnescape(val); err == nil {\n\t\t\t\t\tval = v\n\t\t\t\t}\n\t\t\t}\n\t\t\tps.params = ps.params[:paramIndex+1]\n\t\t\tps.params[paramIndex].Value = val\n\t\t\tparamIndex++\n\t\t\tsearch = search[i:]\n\t\t\tsearchIndex = searchIndex + i\n\t\t\tif search == nilString {\n\t\t\t\tif cd := cn.findChild('/'); cd != nil && (cd.function != nil || cd.anyChild != nil) {\n\t\t\t\t\ttsr = true\n\t\t\t\t}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\tAny:\n\t\t// Any node\n\t\tif child := cn.anyChild; child != nil {\n\t\t\t// If any node is found, use remaining path for paramValues\n\t\t\tcn = child\n\t\t\tif ps == nil {\n\t\t\t\tps = params()\n\t\t\t}\n\t\t\tindex := len(cn.pnames) - 1\n\t\t\tval := search\n\t\t\tif unescape {\n\t\t\t\tif v, err := url.QueryUnescape(val); err == nil {\n\t\t\t\t\tval = v\n\t\t\t\t}\n\t\t\t}\n\t\t\tps.params = ps.params[:paramIndex+1]\n\t\t\tps.params[index].Value = val\n\t\t\t// update indexes/search in case we need to backtrack when no handler match is found\n\t\t\tparamIndex++\n\t\t\tsearchIndex += len(search)\n\t\t\tsearch = nilString\n\t\t\tfunction = cn.function\n\t\t\tbreak\n\t\t}\n\n\t\t// Let's backtrack to the first possible alternative node of the decision path\n\t\tnk, ok := backtrackToNextNodeType(catchAll)\n\t\tif !ok {\n\t\t\tbreak // No other possibilities on the decision path\n\t\t} else if nk == param {\n\t\t\tgoto Param\n\t\t} else if nk == catchAll {\n\t\t\tgoto Any\n\t\t} else {\n\t\t\t// Not found\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif cn != nil {\n\t\tfor i, name := range cn.pnames {\n\t\t\tps.params[i].Key = name\n\t\t}\n\t}\n\n\treturn\n}\n\nfunc (n *node) findChild(l byte) *node {\n\tfor _, c := range n.children {\n\t\tif c.label == l {\n\t\t\treturn c\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (n *node) findChildWithLabel(l byte) *node {\n\tfor _, c := range n.children {\n\t\tif c.label == l {\n\t\t\treturn c\n\t\t}\n\t}\n\tif l == paramLabel {\n\t\treturn n.paramChild\n\t}\n\tif l == anyLabel {\n\t\treturn n.anyChild\n\t}\n\treturn nil\n}\n\nfunc newNode(t nodeType, pre string, p *node, child children, f *FunctionDescriptor, ppath string, pnames []string, paramChildren, anyChildren *node) *node {\n\treturn &node{\n\t\tnType:      t,\n\t\tlabel:      pre[0],\n\t\tprefix:     pre,\n\t\tparent:     p,\n\t\tchildren:   child,\n\t\tppath:      ppath,\n\t\tpnames:     pnames,\n\t\tfunction:   f,\n\t\tparamChild: paramChildren,\n\t\tanyChild:   anyChildren,\n\t\tisLeaf:     child == nil && paramChildren == nil && anyChildren == nil,\n\t}\n}\n\n```\n# Function Signature: func (n *node) addRoute(path string, function *FunctionDescriptor) {\n# Natural Language Annotations: // addRoute adds a node with the given function to the path.\n// Not concurrency-safe!\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (n *node) addRoute(path string, function *FunctionDescriptor) {\n\tcheckPathValid(path)\n\n\tt := static\n\tppath := path\n\tpnames := make([]string, countParams(path))\n\n\tn.insert(path, function, t, ppath, pnames)\n}\n```", "```go\nfunc (n *node) addRoute(path string, function *FunctionDescriptor) {\n\tcheckPathValid(path)\n\n\tvar ppath string\n\tvar pnames []string\n\tif function != nil {\n\t\tppath = path\n\t\tpnames = make([]string, countParams(path))\n\t}\n\n\tn.insert(path, function, static, ppath, pnames)\n}\n```", "```go\nfunc (n *node) addRoute(path string, function *FunctionDescriptor) {\n\tcheckPathValid(path)\n\tn.insert(path, function, static, path, make([]string, countParams(path)))\n}\n```"]}, "kitex-pkg/mem/span.go-NewSpanCache": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/mem/span.go:\n```\n/*\n * Copyright 2024 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage mem\n\nimport (\n\t\"math/bits\"\n\t\"sync/atomic\"\n\n\t\"github.com/bytedance/gopkg/lang/dirtmake\"\n)\n\n/* Span Cache: A thread-safe linear allocator\n\nDesign:\n1. [GC Friendly]: Centralize a batch of small bytes slice into a big size bytes slice to avoid malloc too many objects\n2. [Thread Safe]: Multi thread may share a same span, but it should fall back to the native allocator if lock conflict\n*/\n\nconst (\n\tspanCacheSize = 10\n\tminSpanObject = 128                                  // 128 B\n\tmaxSpanObject = (minSpanObject << spanCacheSize) - 1 // 128 KB\n\tminSpanClass  = 8                                    // = spanClass(minSpanObject)\n)\n\ntype spanCache struct {\n\tspans [spanCacheSize]*span\n}\n\n// NewSpanCache returns a spanCache with the given spanSize,\n// each span is used to allocate a binary of a specific size level.\n\n\n\n\n\n\n\n\n// Make returns a [:n:n] bytes slice from a cached buffer\n// NOTE: Make will not clear the underlay bytes for performance concern. So caller MUST set every byte before read.\nfunc (c *spanCache) Make(n int) []byte {\n\tsclass := spanClass(n) - minSpanClass\n\tif sclass < 0 || sclass >= len(c.spans) {\n\t\treturn dirtmake.Bytes(n, n)\n\t}\n\treturn c.spans[sclass].Make(n)\n}\n\n// Copy is an alias function for make-and-copy pattern\nfunc (c *spanCache) Copy(buf []byte) (p []byte) {\n\tp = c.Make(len(buf))\n\tcopy(p, buf)\n\treturn p\n}\n\n// NewSpan returns a span with given size\nfunc NewSpan(size int) *span {\n\tsp := new(span)\n\tsp.size = uint32(size)\n\tsp.buffer = dirtmake.Bytes(0, size)\n\treturn sp\n}\n\ntype span struct {\n\tlock   uint32\n\tread   uint32 // read index of buffer\n\tsize   uint32 // size of buffer\n\tbuffer []byte\n}\n\n// Make returns a [:n:n] bytes slice from a cached buffer\n// NOTE: Make will not clear the underlay bytes for performance concern. So caller MUST set every byte before read.\nfunc (b *span) Make(_n int) []byte {\n\tn := uint32(_n)\n\tif n >= b.size || !atomic.CompareAndSwapUint32(&b.lock, 0, 1) {\n\t\t// fallback path: make a new byte slice if current goroutine cannot get the lock or n is out of size\n\t\treturn dirtmake.Bytes(int(n), int(n))\n\t}\nSTART:\n\tb.read += n\n\t// fast path\n\tif b.read <= b.size {\n\t\tbuf := b.buffer[b.read-n : b.read : b.read]\n\t\tatomic.StoreUint32(&b.lock, 0)\n\t\treturn buf\n\t}\n\t// slow path: create a new buffer\n\tb.buffer = dirtmake.Bytes(int(b.size), int(b.size))\n\tb.read = 0\n\tgoto START\n}\n\n// Copy is an alias function for make-and-copy pattern\nfunc (b *span) Copy(buf []byte) (p []byte) {\n\tp = b.Make(len(buf))\n\tcopy(p, buf)\n\treturn p\n}\n\n// spanClass calc the minimum number of bits required to represent x\n// [2^sclass,2^(sclass+1)) bytes in a same span class\nfunc spanClass(size int) int {\n\tif size == 0 {\n\t\treturn 0\n\t}\n\treturn bits.Len(uint(size))\n}\n\n```\n# Function Signature: func NewSpanCache(spanSize int) *spanCache {\n# Natural Language Annotations: // NewSpanCache returns a spanCache with the given spanSize,\n// each span is used to allocate a binary of a specific size level.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewSpanCache(spanSize int) *spanCache {\n    sc := &spanCache{}\n    sc.spans = [spanCacheSize]*span{}\n    for i := range sc.spans {\n        sc.spans[i] = NewSpan(spanSize)\n    }\n    return sc\n}\n```", "```go\nfunc NewSpanCache(spanSize int) *spanCache {\n\tsc := &spanCache{}\n\tfor i := range sc.spans {\n\t\tsc.spans[i] = NewSpan(spanSize * (1 << (minSpanClass + i)))\n\t}\n\treturn sc\n}\n```", "```go\nfunc NewSpanCache(spanSize int) *spanCache {\n    sc := new(spanCache)\n    for i := range sc.spans {\n        sc.spans[i] = NewSpan(spanSize)\n    }\n    return sc\n}\n```"]}, "kitex-pkg/profiler/profiler.go-Pause": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/profiler/profiler.go:\n```\n/*\n * Copyright 2022 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage profiler\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"runtime/pprof\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/google/pprof/profile\"\n\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n)\n\ntype profilerContextKey struct{}\n\ntype Profiler interface {\n\tRun(ctx context.Context) (err error)\n\tStop()\n\tPause()\n\tResume()\n\tPrepare(ctx context.Context) context.Context\n\tTag(ctx context.Context, tags ...string) context.Context\n\tUntag(ctx context.Context)\n\tLookup(ctx context.Context, key string) (string, bool)\n}\n\ntype Processor func(profiles []*TagsProfile) error\n\nfunc LogProcessor(profiles []*TagsProfile) error {\n\tif len(profiles) == 0 {\n\t\treturn nil\n\t}\n\tklog.Infof(\"KITEX: profiler collect %d records\", len(profiles))\n\tfor _, p := range profiles {\n\t\tif p.Key != \"\" {\n\t\t\tklog.Infof(\"KITEX: profiler - %s %.2f%%\", p.Key, p.Percent*100)\n\t\t} else {\n\t\t\tklog.Infof(\"KITEX: profiler - type=default %.2f%%\", p.Percent*100)\n\t\t}\n\t}\n\tklog.Info(\"---------------------------------\")\n\treturn nil\n}\n\nfunc NewProfiler(processor Processor, interval, window time.Duration) *profiler {\n\tif processor == nil {\n\t\tprocessor = LogProcessor\n\t}\n\treturn &profiler{\n\t\tstateCond: sync.NewCond(&sync.Mutex{}),\n\t\tprocessor: processor,\n\t\tinterval:  interval,\n\t\twindow:    window,\n\t}\n}\n\nvar _ Profiler = (*profiler)(nil)\n\nconst (\n\t// state changes:\n\t//   running => pausing => paused => resuming => running\n\t//           => stopped\n\tstateRunning  = 0\n\tstatePausing  = 1\n\tstatePaused   = 2\n\tstateResuming = 3\n\tstateStopped  = 4\n)\n\ntype profiler struct {\n\tdata      bytes.Buffer // protobuf\n\tstate     int\n\tstateCond *sync.Cond\n\t// settings\n\tprocessor Processor\n\tinterval  time.Duration // sleep time between every profiling window\n\twindow    time.Duration // profiling in the window, go pprof collect stack profile every 10ms\n}\n\n// Tag current goroutine with tagged ctx\n// it's used for reuse goroutine scenario\nfunc Tag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok {\n\t\tpc.profiler.Tag(ctx)\n\t}\n}\n\n// Untag current goroutine with tagged ctx\n// it's used for reuse goroutine scenario\nfunc Untag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok {\n\t\tpc.profiler.Untag(ctx)\n\t}\n}\n\ntype profilerContext struct {\n\tprofiler Profiler\n\tuntagCtx context.Context\n\ttags     []string\n}\n\nfunc newProfilerContext(profiler Profiler) *profilerContext {\n\treturn &profilerContext{\n\t\tprofiler: profiler,\n\t\ttags:     make([]string, 0, 12),\n\t}\n}\n\n// Prepare the profiler context\nfunc (p *profiler) Prepare(ctx context.Context) context.Context {\n\tif c := ctx.Value(profilerContextKey{}); c != nil {\n\t\treturn ctx\n\t}\n\treturn context.WithValue(ctx, profilerContextKey{}, newProfilerContext(p))\n}\n\n// State return current profiler state\nfunc (p *profiler) State() (state int) {\n\tp.stateCond.L.Lock()\n\tstate = p.state\n\tp.stateCond.L.Unlock()\n\treturn state\n}\n\n// Stop the profiler\nfunc (p *profiler) Stop() {\n\tif p.State() == stateStopped {\n\t\treturn\n\t}\n\t// stateRunning => stateStopped\n\tp.stateChange(stateRunning, stateStopped)\n}\n\n// Pause the profiler.\n// The profiler has been paused when Pause() return\n\n\n\n\n\n\n\n\n\n\n// Resume the profiler.\n// The profiler has been resumed when Resume() return\n\n\n\n\n\n\n\n\n\n\n// Run start analyse the pprof data with interval and window settings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Tag current goroutine with tags\n// If ctx already tagged, append the existed tags\nfunc (p *profiler) Tag(ctx context.Context, tags ...string) context.Context {\n\tpctx, ok := ctx.Value(profilerContextKey{}).(*profilerContext)\n\tif !ok {\n\t\tpctx = newProfilerContext(p)\n\t\tctx = context.WithValue(ctx, profilerContextKey{}, pctx)\n\t}\n\tif pctx.untagCtx == nil {\n\t\tpctx.untagCtx = ctx\n\t}\n\tpctx.tags = append(pctx.tags, tags...)\n\t// do not return pprof ctx\n\tpprof.SetGoroutineLabels(pprof.WithLabels(context.Background(), pprof.Labels(pctx.tags...)))\n\treturn ctx\n}\n\n// Untag current goroutine\n// Only untag if ctx already tagged, will not clear the goroutine labels if not tagged by profiler\nfunc (p *profiler) Untag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok && pc.untagCtx != nil {\n\t\t// if ctx have untagCtx, that means the current goroutine created by a tagged goroutine\n\t\t// we need to untag the goroutine when finished\n\t\t// else, do nothing\n\t\tpprof.SetGoroutineLabels(pc.untagCtx)\n\t}\n}\n\nfunc (p *profiler) Lookup(ctx context.Context, key string) (string, bool) {\n\treturn pprof.Label(ctx, key)\n}\n\nfunc (p *profiler) stateChange(from, to int) {\n\tp.stateCond.L.Lock()\n\tfor p.state != from { // wait state to from first\n\t\tp.stateCond.Wait()\n\t}\n\tp.state = to\n\tp.stateCond.L.Unlock()\n\tp.stateCond.Broadcast()\n}\n\nfunc (p *profiler) stateWait(to int) {\n\tp.stateCond.L.Lock()\n\tfor p.state != to {\n\t\tp.stateCond.Wait()\n\t}\n\tp.stateCond.L.Unlock()\n}\n\nfunc (p *profiler) startProfile() error {\n\tp.data.Reset()\n\treturn pprof.StartCPUProfile(&p.data)\n}\n\nfunc (p *profiler) stopProfile() {\n\tpprof.StopCPUProfile()\n}\n\nfunc (p *profiler) analyse() ([]*TagsProfile, error) {\n\t// parse protobuf data\n\tpf, err := profile.ParseData(p.data.Bytes())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// filter cpu value index\n\tsampleIdx := -1\n\tfor idx, st := range pf.SampleType {\n\t\tif st.Type == \"cpu\" {\n\t\t\tsampleIdx = idx\n\t\t\tbreak\n\t\t}\n\t}\n\tif sampleIdx < 0 {\n\t\treturn nil, errors.New(\"profiler: sample type not found\")\n\t}\n\n\t// calculate every sample expense\n\tcounter := map[string]*TagsProfile{} // map[tagsKey]funcProfile\n\tvar total int64\n\tfor _, sm := range pf.Sample {\n\t\tvalue := sm.Value[sampleIdx]\n\t\ttags := labelToTags(sm.Label)\n\t\ttagsKey := tagsToKey(tags)\n\t\ttp, ok := counter[tagsKey]\n\t\tif !ok {\n\t\t\ttp = &TagsProfile{}\n\t\t\tcounter[tagsKey] = tp\n\t\t\ttp.Key = tagsKey\n\t\t\ttp.Tags = tags\n\t\t}\n\t\ttp.Value += value\n\t\ttotal += value\n\t}\n\n\t// compensate value with duration date\n\tdurationRate := float64((p.interval + p.window) / p.window)\n\tprofiles := make([]*TagsProfile, 0, len(counter)) // flat to array\n\tfor _, l := range counter {\n\t\tl.Percent = float64(l.Value) / float64(total)\n\t\tl.Value = int64(durationRate * float64(l.Value))\n\t\tprofiles = append(profiles, l)\n\t}\n\treturn profiles, nil\n}\n\n// TagsProfile is the stats result group by tag key\ntype TagsProfile struct {\n\tKey     string   // eg: a=1,b=2\n\tTags    []string // eg: [\"a\", \"1\", \"b\", \"2\"]\n\tValue   int64    // pprof cpu times\n\tPercent float64  // <= 1.0\n}\n\nfunc labelToTags(label map[string][]string) []string {\n\ttags := make([]string, 0, len(label)*2)\n\tfor k, v := range label {\n\t\ttags = append(tags, k, strings.Join(v, \",\"))\n\t}\n\treturn tags\n}\n\nfunc tagsToKey(tags []string) string {\n\tif len(tags)%2 != 0 {\n\t\treturn \"\"\n\t}\n\ttagsPair := make([]string, 0, len(tags)/2)\n\tfor i := 0; i < len(tags); i += 2 {\n\t\ttagsPair = append(tagsPair, fmt.Sprintf(\"%s=%s\", tags[i], tags[i+1]))\n\t}\n\t// sort tags to make it a unique key\n\tsort.Strings(tagsPair)\n\treturn strings.Join(tagsPair, \"|\")\n}\n\n```\n# Function Signature: func (p *profiler) Pause() {\n# Natural Language Annotations: // Pause the profiler.\n// The profiler has been paused when Pause() return\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (p *profiler) Pause() {\n\tif p.State() == stateStopped {\n\t\treturn\n\t}\n\t// stateRunning => statePausing => statePaused => stateResuming => stateRunning\n\tp.stateChange(stateRunning, statePausing)\n\tp.stateChange(statePausing, statePaused)\n}\n```\nThis function is used to pause the profiler. It first checks the current state of the profiler. If the state is stopped, it returns without doing anything. If the state is running, it changes the state to pausing and then to paused. \n\nIn the next state, it will change the state to resuming and then to running, effectively pausing the profiler.", "```go\nfunc (p *profiler) Pause() {\n\t// stateRunning => statePausing\n\tp.stateChange(stateRunning, statePausing)\n\n\t// statePausing => statePaused\n\tp.stateChange(statePausing, statePaused)\n}\n```", "```go\nfunc (p *profiler) Pause() {\n\t// stateRunning => statePausing\n\tp.stateChange(stateRunning, statePausing)\n\t// statePausing => statePaused\n\tp.stateChange(statePausing, statePaused)\n}\n```"]}, "kitex-pkg/profiler/profiler.go-Resume": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/profiler/profiler.go:\n```\n/*\n * Copyright 2022 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage profiler\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"runtime/pprof\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/google/pprof/profile\"\n\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n)\n\ntype profilerContextKey struct{}\n\ntype Profiler interface {\n\tRun(ctx context.Context) (err error)\n\tStop()\n\tPause()\n\tResume()\n\tPrepare(ctx context.Context) context.Context\n\tTag(ctx context.Context, tags ...string) context.Context\n\tUntag(ctx context.Context)\n\tLookup(ctx context.Context, key string) (string, bool)\n}\n\ntype Processor func(profiles []*TagsProfile) error\n\nfunc LogProcessor(profiles []*TagsProfile) error {\n\tif len(profiles) == 0 {\n\t\treturn nil\n\t}\n\tklog.Infof(\"KITEX: profiler collect %d records\", len(profiles))\n\tfor _, p := range profiles {\n\t\tif p.Key != \"\" {\n\t\t\tklog.Infof(\"KITEX: profiler - %s %.2f%%\", p.Key, p.Percent*100)\n\t\t} else {\n\t\t\tklog.Infof(\"KITEX: profiler - type=default %.2f%%\", p.Percent*100)\n\t\t}\n\t}\n\tklog.Info(\"---------------------------------\")\n\treturn nil\n}\n\nfunc NewProfiler(processor Processor, interval, window time.Duration) *profiler {\n\tif processor == nil {\n\t\tprocessor = LogProcessor\n\t}\n\treturn &profiler{\n\t\tstateCond: sync.NewCond(&sync.Mutex{}),\n\t\tprocessor: processor,\n\t\tinterval:  interval,\n\t\twindow:    window,\n\t}\n}\n\nvar _ Profiler = (*profiler)(nil)\n\nconst (\n\t// state changes:\n\t//   running => pausing => paused => resuming => running\n\t//           => stopped\n\tstateRunning  = 0\n\tstatePausing  = 1\n\tstatePaused   = 2\n\tstateResuming = 3\n\tstateStopped  = 4\n)\n\ntype profiler struct {\n\tdata      bytes.Buffer // protobuf\n\tstate     int\n\tstateCond *sync.Cond\n\t// settings\n\tprocessor Processor\n\tinterval  time.Duration // sleep time between every profiling window\n\twindow    time.Duration // profiling in the window, go pprof collect stack profile every 10ms\n}\n\n// Tag current goroutine with tagged ctx\n// it's used for reuse goroutine scenario\nfunc Tag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok {\n\t\tpc.profiler.Tag(ctx)\n\t}\n}\n\n// Untag current goroutine with tagged ctx\n// it's used for reuse goroutine scenario\nfunc Untag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok {\n\t\tpc.profiler.Untag(ctx)\n\t}\n}\n\ntype profilerContext struct {\n\tprofiler Profiler\n\tuntagCtx context.Context\n\ttags     []string\n}\n\nfunc newProfilerContext(profiler Profiler) *profilerContext {\n\treturn &profilerContext{\n\t\tprofiler: profiler,\n\t\ttags:     make([]string, 0, 12),\n\t}\n}\n\n// Prepare the profiler context\nfunc (p *profiler) Prepare(ctx context.Context) context.Context {\n\tif c := ctx.Value(profilerContextKey{}); c != nil {\n\t\treturn ctx\n\t}\n\treturn context.WithValue(ctx, profilerContextKey{}, newProfilerContext(p))\n}\n\n// State return current profiler state\nfunc (p *profiler) State() (state int) {\n\tp.stateCond.L.Lock()\n\tstate = p.state\n\tp.stateCond.L.Unlock()\n\treturn state\n}\n\n// Stop the profiler\nfunc (p *profiler) Stop() {\n\tif p.State() == stateStopped {\n\t\treturn\n\t}\n\t// stateRunning => stateStopped\n\tp.stateChange(stateRunning, stateStopped)\n}\n\n// Pause the profiler.\n// The profiler has been paused when Pause() return\n\n\n\n\n\n\n\n\n\n\n// Resume the profiler.\n// The profiler has been resumed when Resume() return\n\n\n\n\n\n\n\n\n\n\n// Run start analyse the pprof data with interval and window settings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Tag current goroutine with tags\n// If ctx already tagged, append the existed tags\nfunc (p *profiler) Tag(ctx context.Context, tags ...string) context.Context {\n\tpctx, ok := ctx.Value(profilerContextKey{}).(*profilerContext)\n\tif !ok {\n\t\tpctx = newProfilerContext(p)\n\t\tctx = context.WithValue(ctx, profilerContextKey{}, pctx)\n\t}\n\tif pctx.untagCtx == nil {\n\t\tpctx.untagCtx = ctx\n\t}\n\tpctx.tags = append(pctx.tags, tags...)\n\t// do not return pprof ctx\n\tpprof.SetGoroutineLabels(pprof.WithLabels(context.Background(), pprof.Labels(pctx.tags...)))\n\treturn ctx\n}\n\n// Untag current goroutine\n// Only untag if ctx already tagged, will not clear the goroutine labels if not tagged by profiler\nfunc (p *profiler) Untag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok && pc.untagCtx != nil {\n\t\t// if ctx have untagCtx, that means the current goroutine created by a tagged goroutine\n\t\t// we need to untag the goroutine when finished\n\t\t// else, do nothing\n\t\tpprof.SetGoroutineLabels(pc.untagCtx)\n\t}\n}\n\nfunc (p *profiler) Lookup(ctx context.Context, key string) (string, bool) {\n\treturn pprof.Label(ctx, key)\n}\n\nfunc (p *profiler) stateChange(from, to int) {\n\tp.stateCond.L.Lock()\n\tfor p.state != from { // wait state to from first\n\t\tp.stateCond.Wait()\n\t}\n\tp.state = to\n\tp.stateCond.L.Unlock()\n\tp.stateCond.Broadcast()\n}\n\nfunc (p *profiler) stateWait(to int) {\n\tp.stateCond.L.Lock()\n\tfor p.state != to {\n\t\tp.stateCond.Wait()\n\t}\n\tp.stateCond.L.Unlock()\n}\n\nfunc (p *profiler) startProfile() error {\n\tp.data.Reset()\n\treturn pprof.StartCPUProfile(&p.data)\n}\n\nfunc (p *profiler) stopProfile() {\n\tpprof.StopCPUProfile()\n}\n\nfunc (p *profiler) analyse() ([]*TagsProfile, error) {\n\t// parse protobuf data\n\tpf, err := profile.ParseData(p.data.Bytes())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// filter cpu value index\n\tsampleIdx := -1\n\tfor idx, st := range pf.SampleType {\n\t\tif st.Type == \"cpu\" {\n\t\t\tsampleIdx = idx\n\t\t\tbreak\n\t\t}\n\t}\n\tif sampleIdx < 0 {\n\t\treturn nil, errors.New(\"profiler: sample type not found\")\n\t}\n\n\t// calculate every sample expense\n\tcounter := map[string]*TagsProfile{} // map[tagsKey]funcProfile\n\tvar total int64\n\tfor _, sm := range pf.Sample {\n\t\tvalue := sm.Value[sampleIdx]\n\t\ttags := labelToTags(sm.Label)\n\t\ttagsKey := tagsToKey(tags)\n\t\ttp, ok := counter[tagsKey]\n\t\tif !ok {\n\t\t\ttp = &TagsProfile{}\n\t\t\tcounter[tagsKey] = tp\n\t\t\ttp.Key = tagsKey\n\t\t\ttp.Tags = tags\n\t\t}\n\t\ttp.Value += value\n\t\ttotal += value\n\t}\n\n\t// compensate value with duration date\n\tdurationRate := float64((p.interval + p.window) / p.window)\n\tprofiles := make([]*TagsProfile, 0, len(counter)) // flat to array\n\tfor _, l := range counter {\n\t\tl.Percent = float64(l.Value) / float64(total)\n\t\tl.Value = int64(durationRate * float64(l.Value))\n\t\tprofiles = append(profiles, l)\n\t}\n\treturn profiles, nil\n}\n\n// TagsProfile is the stats result group by tag key\ntype TagsProfile struct {\n\tKey     string   // eg: a=1,b=2\n\tTags    []string // eg: [\"a\", \"1\", \"b\", \"2\"]\n\tValue   int64    // pprof cpu times\n\tPercent float64  // <= 1.0\n}\n\nfunc labelToTags(label map[string][]string) []string {\n\ttags := make([]string, 0, len(label)*2)\n\tfor k, v := range label {\n\t\ttags = append(tags, k, strings.Join(v, \",\"))\n\t}\n\treturn tags\n}\n\nfunc tagsToKey(tags []string) string {\n\tif len(tags)%2 != 0 {\n\t\treturn \"\"\n\t}\n\ttagsPair := make([]string, 0, len(tags)/2)\n\tfor i := 0; i < len(tags); i += 2 {\n\t\ttagsPair = append(tagsPair, fmt.Sprintf(\"%s=%s\", tags[i], tags[i+1]))\n\t}\n\t// sort tags to make it a unique key\n\tsort.Strings(tagsPair)\n\treturn strings.Join(tagsPair, \"|\")\n}\n\n```\n# Function Signature: func (p *profiler) Resume() {\n# Natural Language Annotations: // Resume the profiler.\n// The profiler has been resumed when Resume() return\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (p *profiler) Resume() {\n\t// Resume the profiler.\n\t// The profiler has been resumed when Resume() return\n\tif p.State() == stateStopped {\n\t\treturn\n\t}\n\t// stateRunning => stateStopped\n\tp.stateChange(stateStopped, stateRunning)\n}\n```", "```go\nfunc (p *profiler) Resume() {\n\t// Resume the profiler.\n\t// The profiler has been resumed when Resume() return\n\tif p.State() == stateStopped {\n\t\treturn\n\t}\n\t// statePausing => stateResuming => stateRunning\n\tp.stateChange(statePausing, stateResuming)\n\tp.stateChange(stateResuming, stateRunning)\n}\n```", "```go\nfunc (p *profiler) Resume() {\n\t// Resume the profiler.\n\t// The profiler has been resumed when Resume() return\n\tif p.State() == stateStopped {\n\t\treturn\n\t}\n\t// statePaused => stateResuming => stateRunning\n\tp.stateChange(statePaused, stateResuming)\n\tp.stateChange(stateResuming, stateRunning)\n}\n```"]}, "kitex-pkg/profiler/profiler.go-Run": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/profiler/profiler.go:\n```\n/*\n * Copyright 2022 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage profiler\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"runtime/pprof\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/google/pprof/profile\"\n\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n)\n\ntype profilerContextKey struct{}\n\ntype Profiler interface {\n\tRun(ctx context.Context) (err error)\n\tStop()\n\tPause()\n\tResume()\n\tPrepare(ctx context.Context) context.Context\n\tTag(ctx context.Context, tags ...string) context.Context\n\tUntag(ctx context.Context)\n\tLookup(ctx context.Context, key string) (string, bool)\n}\n\ntype Processor func(profiles []*TagsProfile) error\n\nfunc LogProcessor(profiles []*TagsProfile) error {\n\tif len(profiles) == 0 {\n\t\treturn nil\n\t}\n\tklog.Infof(\"KITEX: profiler collect %d records\", len(profiles))\n\tfor _, p := range profiles {\n\t\tif p.Key != \"\" {\n\t\t\tklog.Infof(\"KITEX: profiler - %s %.2f%%\", p.Key, p.Percent*100)\n\t\t} else {\n\t\t\tklog.Infof(\"KITEX: profiler - type=default %.2f%%\", p.Percent*100)\n\t\t}\n\t}\n\tklog.Info(\"---------------------------------\")\n\treturn nil\n}\n\nfunc NewProfiler(processor Processor, interval, window time.Duration) *profiler {\n\tif processor == nil {\n\t\tprocessor = LogProcessor\n\t}\n\treturn &profiler{\n\t\tstateCond: sync.NewCond(&sync.Mutex{}),\n\t\tprocessor: processor,\n\t\tinterval:  interval,\n\t\twindow:    window,\n\t}\n}\n\nvar _ Profiler = (*profiler)(nil)\n\nconst (\n\t// state changes:\n\t//   running => pausing => paused => resuming => running\n\t//           => stopped\n\tstateRunning  = 0\n\tstatePausing  = 1\n\tstatePaused   = 2\n\tstateResuming = 3\n\tstateStopped  = 4\n)\n\ntype profiler struct {\n\tdata      bytes.Buffer // protobuf\n\tstate     int\n\tstateCond *sync.Cond\n\t// settings\n\tprocessor Processor\n\tinterval  time.Duration // sleep time between every profiling window\n\twindow    time.Duration // profiling in the window, go pprof collect stack profile every 10ms\n}\n\n// Tag current goroutine with tagged ctx\n// it's used for reuse goroutine scenario\nfunc Tag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok {\n\t\tpc.profiler.Tag(ctx)\n\t}\n}\n\n// Untag current goroutine with tagged ctx\n// it's used for reuse goroutine scenario\nfunc Untag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok {\n\t\tpc.profiler.Untag(ctx)\n\t}\n}\n\ntype profilerContext struct {\n\tprofiler Profiler\n\tuntagCtx context.Context\n\ttags     []string\n}\n\nfunc newProfilerContext(profiler Profiler) *profilerContext {\n\treturn &profilerContext{\n\t\tprofiler: profiler,\n\t\ttags:     make([]string, 0, 12),\n\t}\n}\n\n// Prepare the profiler context\nfunc (p *profiler) Prepare(ctx context.Context) context.Context {\n\tif c := ctx.Value(profilerContextKey{}); c != nil {\n\t\treturn ctx\n\t}\n\treturn context.WithValue(ctx, profilerContextKey{}, newProfilerContext(p))\n}\n\n// State return current profiler state\nfunc (p *profiler) State() (state int) {\n\tp.stateCond.L.Lock()\n\tstate = p.state\n\tp.stateCond.L.Unlock()\n\treturn state\n}\n\n// Stop the profiler\nfunc (p *profiler) Stop() {\n\tif p.State() == stateStopped {\n\t\treturn\n\t}\n\t// stateRunning => stateStopped\n\tp.stateChange(stateRunning, stateStopped)\n}\n\n// Pause the profiler.\n// The profiler has been paused when Pause() return\n\n\n\n\n\n\n\n\n\n\n// Resume the profiler.\n// The profiler has been resumed when Resume() return\n\n\n\n\n\n\n\n\n\n\n// Run start analyse the pprof data with interval and window settings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Tag current goroutine with tags\n// If ctx already tagged, append the existed tags\nfunc (p *profiler) Tag(ctx context.Context, tags ...string) context.Context {\n\tpctx, ok := ctx.Value(profilerContextKey{}).(*profilerContext)\n\tif !ok {\n\t\tpctx = newProfilerContext(p)\n\t\tctx = context.WithValue(ctx, profilerContextKey{}, pctx)\n\t}\n\tif pctx.untagCtx == nil {\n\t\tpctx.untagCtx = ctx\n\t}\n\tpctx.tags = append(pctx.tags, tags...)\n\t// do not return pprof ctx\n\tpprof.SetGoroutineLabels(pprof.WithLabels(context.Background(), pprof.Labels(pctx.tags...)))\n\treturn ctx\n}\n\n// Untag current goroutine\n// Only untag if ctx already tagged, will not clear the goroutine labels if not tagged by profiler\nfunc (p *profiler) Untag(ctx context.Context) {\n\tif pc, ok := ctx.Value(profilerContextKey{}).(*profilerContext); ok && pc.untagCtx != nil {\n\t\t// if ctx have untagCtx, that means the current goroutine created by a tagged goroutine\n\t\t// we need to untag the goroutine when finished\n\t\t// else, do nothing\n\t\tpprof.SetGoroutineLabels(pc.untagCtx)\n\t}\n}\n\nfunc (p *profiler) Lookup(ctx context.Context, key string) (string, bool) {\n\treturn pprof.Label(ctx, key)\n}\n\nfunc (p *profiler) stateChange(from, to int) {\n\tp.stateCond.L.Lock()\n\tfor p.state != from { // wait state to from first\n\t\tp.stateCond.Wait()\n\t}\n\tp.state = to\n\tp.stateCond.L.Unlock()\n\tp.stateCond.Broadcast()\n}\n\nfunc (p *profiler) stateWait(to int) {\n\tp.stateCond.L.Lock()\n\tfor p.state != to {\n\t\tp.stateCond.Wait()\n\t}\n\tp.stateCond.L.Unlock()\n}\n\nfunc (p *profiler) startProfile() error {\n\tp.data.Reset()\n\treturn pprof.StartCPUProfile(&p.data)\n}\n\nfunc (p *profiler) stopProfile() {\n\tpprof.StopCPUProfile()\n}\n\nfunc (p *profiler) analyse() ([]*TagsProfile, error) {\n\t// parse protobuf data\n\tpf, err := profile.ParseData(p.data.Bytes())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// filter cpu value index\n\tsampleIdx := -1\n\tfor idx, st := range pf.SampleType {\n\t\tif st.Type == \"cpu\" {\n\t\t\tsampleIdx = idx\n\t\t\tbreak\n\t\t}\n\t}\n\tif sampleIdx < 0 {\n\t\treturn nil, errors.New(\"profiler: sample type not found\")\n\t}\n\n\t// calculate every sample expense\n\tcounter := map[string]*TagsProfile{} // map[tagsKey]funcProfile\n\tvar total int64\n\tfor _, sm := range pf.Sample {\n\t\tvalue := sm.Value[sampleIdx]\n\t\ttags := labelToTags(sm.Label)\n\t\ttagsKey := tagsToKey(tags)\n\t\ttp, ok := counter[tagsKey]\n\t\tif !ok {\n\t\t\ttp = &TagsProfile{}\n\t\t\tcounter[tagsKey] = tp\n\t\t\ttp.Key = tagsKey\n\t\t\ttp.Tags = tags\n\t\t}\n\t\ttp.Value += value\n\t\ttotal += value\n\t}\n\n\t// compensate value with duration date\n\tdurationRate := float64((p.interval + p.window) / p.window)\n\tprofiles := make([]*TagsProfile, 0, len(counter)) // flat to array\n\tfor _, l := range counter {\n\t\tl.Percent = float64(l.Value) / float64(total)\n\t\tl.Value = int64(durationRate * float64(l.Value))\n\t\tprofiles = append(profiles, l)\n\t}\n\treturn profiles, nil\n}\n\n// TagsProfile is the stats result group by tag key\ntype TagsProfile struct {\n\tKey     string   // eg: a=1,b=2\n\tTags    []string // eg: [\"a\", \"1\", \"b\", \"2\"]\n\tValue   int64    // pprof cpu times\n\tPercent float64  // <= 1.0\n}\n\nfunc labelToTags(label map[string][]string) []string {\n\ttags := make([]string, 0, len(label)*2)\n\tfor k, v := range label {\n\t\ttags = append(tags, k, strings.Join(v, \",\"))\n\t}\n\treturn tags\n}\n\nfunc tagsToKey(tags []string) string {\n\tif len(tags)%2 != 0 {\n\t\treturn \"\"\n\t}\n\ttagsPair := make([]string, 0, len(tags)/2)\n\tfor i := 0; i < len(tags); i += 2 {\n\t\ttagsPair = append(tagsPair, fmt.Sprintf(\"%s=%s\", tags[i], tags[i+1]))\n\t}\n\t// sort tags to make it a unique key\n\tsort.Strings(tagsPair)\n\treturn strings.Join(tagsPair, \"|\")\n}\n\n```\n# Function Signature: func (p *profiler) Run(ctx context.Context) (err error) {\n# Natural Language Annotations: // Run start analyse the pprof data with interval and window settings\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (p *profiler) Run(ctx context.Context) (err error) {\n\t// stateRunning => stateStopped\n\tp.stateChange(stateRunning, statePausing)\n\tdefer p.stateChange(statePausing, stateStopped)\n\n\ttimer := time.NewTimer(p.interval)\n\tdefer timer.Stop()\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\t// context cancel\n\t\t\treturn ctx.Err()\n\t\tcase <-timer.C:\n\t\t\t// start profiling\n\t\t\tif err := p.startProfile(); err != nil {\n\t\t\t\tklog.Errorf(\"KITEX: profiler start fail, err=%s\", err.Error())\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// sleep window\n\t\t\ttime.Sleep(p.window)\n\t\t\t// stop profiling\n\t\t\tp.stopProfile()\n\t\t\t// analyse pprof data\n\t\t\tprofiles, err := p.analyse()\n\t\t\tif err != nil {\n\t\t\t\tklog.Errorf(\"KITEX: profiler analyse fail, err=%s\", err.Error())\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// processor\n\t\t\tif err := p.processor(profiles); err != nil {\n\t\t\t\tklog.Errorf(\"KITEX: profiler processor fail, err=%s\", err.Error())\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// reset timer\n\t\t\tif !timer.Stop() {\n\t\t\t\tselect {\n\t\t\t\tcase <-timer.C:\n\t\t\t\tdefault:\n\t\t\t\t}\n\t\t\t}\n\t\t\ttimer.Reset(p.interval)\n\t\t}\n\t}\n}\n```", "```go\nfunc (p *profiler) Run(ctx context.Context) (err error) {\n\tp.stateChange(stateStopped, stateRunning)\n\tdefer p.stateChange(stateRunning, stateStopped)\n\n\tticker := time.NewTicker(p.interval + p.window)\n\tdefer ticker.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tcase <-ticker.C:\n\t\t\tif p.State() == stateRunning {\n\t\t\t\tif err := p.startProfile(); err != nil {\n\t\t\t\t\treturn fmt.Errorf(\"profiler: start profile error, %w\", err)\n\t\t\t\t}\n\t\t\t\ttime.Sleep(p.window)\n\t\t\t\tp.stopProfile()\n\t\t\t\tprofiles, err := p.analyse()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn fmt.Errorf(\"profiler: analyse profile error, %w\", err)\n\t\t\t\t}\n\t\t\t\tif err := p.processor(profiles); err != nil {\n\t\t\t\t\treturn fmt.Errorf(\"profiler: process profile error, %w\", err)\n\t\t\t\t}\n\t\t\t} else if p.State() == stateStopped {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t}\n}\n```", "```go\nfunc (p *profiler) Run(ctx context.Context) (err error) {\n\t// stateRunning => stateStopped\n\tp.stateChange(stateRunning, stateStopped)\n\n\t// stateRunning => statePausing\n\tp.stateChange(stateRunning, statePausing)\n\n\t// Run start analyse the pprof data with interval and window settings\n\tprofiles, err := p.analyse()\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// statePausing => statePaused\n\tp.stateChange(statePausing, statePaused)\n\n\t// statePaused => stateResuming\n\tp.stateChange(statePaused, stateResuming)\n\n\t// stateResuming => stateStopped\n\tp.stateChange(stateResuming, stateStopped)\n\n\t// stateStopped => stateRunning\n\tp.stateChange(stateStopped, stateRunning)\n\n\t// Run the processor with the profiles\n\treturn p.processor(profiles)\n}\n```"]}, "kitex-pkg/acl/acl.go-NewACLMiddleware": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/pkg/acl/acl.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package acl implements ACL functionality.\npackage acl\n\nimport (\n\t\"context\"\n\t\"errors\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n)\n\n// RejectFunc judges if to reject a request by the given context and request.\n// Returns a reason if rejected, otherwise returns nil.\ntype RejectFunc func(ctx context.Context, request interface{}) (reason error)\n\n// NewACLMiddleware creates a new ACL middleware using the provided reject funcs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func NewACLMiddleware(rules []RejectFunc) endpoint.Middleware {\n# Natural Language Annotations: // NewACLMiddleware creates a new ACL middleware using the provided reject funcs.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewACLMiddleware(rules []RejectFunc) endpoint.Middleware {\n    return func(next endpoint.Endpoint) endpoint.Endpoint {\n        return func(ctx context.Context, request, response interface{}) (err error) {\n            for _, rule := range rules {\n                reason := rule(ctx, request)\n                if reason != nil {\n                    return kerrors.NewServiceUnavailable(\"acl\", reason.Error())\n                }\n            }\n            return next(ctx, request, response)\n        }\n    }\n}\n```", "```go\nfunc NewACLMiddleware(rules []RejectFunc) endpoint.Middleware {\n    return func(next endpoint.Endpoint) endpoint.Endpoint {\n        return func(ctx context.Context, request, response interface{}) (err error) {\n            for _, rule := range rules {\n                reason := rule(ctx, request)\n                if reason != nil {\n                    return kerrors.ErrACL.WithCause(reason)\n                }\n            }\n            return next(ctx, request, response)\n        }\n    }\n}\n```", "```go\nfunc NewACLMiddleware(rules []RejectFunc) endpoint.Middleware {\n    return func(next endpoint.Endpoint) endpoint.Endpoint {\n        return func(ctx context.Context, request, response interface{}) (err error) {\n            for _, rule := range rules {\n                if reason := rule(ctx, request); reason != nil {\n                    return kerrors.NewBizStatusErrorWithExtraInfo(err, 403, \"Access Denied\", map[string]interface{}{\n                        \"reason\": reason.Error(),\n                    })\n                }\n            }\n            return next(ctx, request, response)\n        }\n    }\n}\n```"]}, "kitex-client/context_middleware.go-WithContextMiddlewares": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/context_middleware.go:\n```\n/*\n * Copyright 2022 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n)\n\ntype ctxMWChainKey struct{}\n\n// WithContextMiddlewares add middlewares into current ctx\n// Every client receive this ctx will execute the middlewares.\n// Note that ContextMiddleware is prior to the ClientMiddleware.\n\n\n\n\n\n\n\n\n\n\n\n\nfunc getContextMiddleware(ctx context.Context) endpoint.Middleware {\n\tmws, _ := ctx.Value(ctxMWChainKey{}).(endpoint.Middleware)\n\treturn mws\n}\n\n// contextMW execute the ContextMiddlewares in ctx\nfunc contextMW(next endpoint.Endpoint) endpoint.Endpoint {\n\treturn func(ctx context.Context, req, resp interface{}) (err error) {\n\t\tmw := getContextMiddleware(ctx)\n\t\tif mw != nil {\n\t\t\treturn mw(next)(ctx, req, resp)\n\t\t}\n\t\treturn next(ctx, req, resp)\n\t}\n}\n\n```\n# Function Signature: func WithContextMiddlewares(ctx context.Context, mws ...endpoint.Middleware) context.Context {\n# Natural Language Annotations: // WithContextMiddlewares add middlewares into current ctx\n// Every client receive this ctx will execute the middlewares.\n// Note that ContextMiddleware is prior to the ClientMiddleware.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithContextMiddlewares(ctx context.Context, mws ...endpoint.Middleware) context.Context {\n\tvar mw endpoint.Middleware\n\tif len(mws) > 0 {\n\t\tmw = mws[0]\n\t\tfor i := 1; i < len(mws); i++ {\n\t\t\tmw = endpoint.Chain(mw, mws[i])\n\t\t}\n\t}\n\n\treturn context.WithValue(ctx, ctxMWChainKey{}, mw)\n}\n```", "```go\nfunc WithContextMiddlewares(ctx context.Context, mws ...endpoint.Middleware) context.Context {\n\t// Create a new context with the middleware chain\n\tnewCtx := context.WithValue(ctx, ctxMWChainKey{}, middlewareChain(mws...))\n\treturn newCtx\n}\n\n// middlewareChain chains middlewares.\nfunc middlewareChain(mws ...endpoint.Middleware) endpoint.Middleware {\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\tfor i := len(mws) - 1; i >= 0; i-- {\n\t\t\tnext = mws[i](next)\n\t\t}\n\t\treturn next\n\t}\n}\n```", "```go\npackage client\n\nimport (\n\t\"context\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n)\n\nfunc WithContextMiddlewares(ctx context.Context, mws ...endpoint.Middleware) context.Context {\n\tvar newMw endpoint.Middleware\n\tfor _, mw := range mws {\n\t\tif newMw == nil {\n\t\t\tnewMw = mw\n\t\t} else {\n\t\t\tnewMw = newMw.Chain(mw)\n\t\t}\n\t}\n\treturn context.WithValue(ctx, ctxMWChainKey{}, newMw)\n}\n```"]}, "kitex-client/stream.go-Header": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/stream.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"sync/atomic\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/remotecli\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n)\n\n// Streaming client streaming interface for code generate\ntype Streaming interface {\n\tStream(ctx context.Context, method string, request, response interface{}) error\n}\n\n// Stream implements the Streaming interface\nfunc (kc *kClient) Stream(ctx context.Context, method string, request, response interface{}) error {\n\tif !kc.inited {\n\t\tpanic(\"client not initialized\")\n\t}\n\tif kc.closed {\n\t\tpanic(\"client is already closed\")\n\t}\n\tif ctx == nil {\n\t\tpanic(\"ctx is nil\")\n\t}\n\tvar ri rpcinfo.RPCInfo\n\tctx, ri, _ = kc.initRPCInfo(ctx, method, 0, nil)\n\n\trpcinfo.AsMutableRPCConfig(ri.Config()).SetInteractionMode(rpcinfo.Streaming)\n\tctx = rpcinfo.NewCtxWithRPCInfo(ctx, ri)\n\n\tctx = kc.opt.TracerCtl.DoStart(ctx, ri)\n\treturn kc.sEps(ctx, request, response)\n}\n\nfunc (kc *kClient) invokeSendEndpoint() endpoint.SendEndpoint {\n\treturn func(stream streaming.Stream, req interface{}) (err error) {\n\t\treturn stream.SendMsg(req)\n\t}\n}\n\nfunc (kc *kClient) invokeRecvEndpoint() endpoint.RecvEndpoint {\n\treturn func(stream streaming.Stream, resp interface{}) (err error) {\n\t\treturn stream.RecvMsg(resp)\n\t}\n}\n\nfunc (kc *kClient) invokeStreamingEndpoint() (endpoint.Endpoint, error) {\n\thandler, err := kc.opt.RemoteOpt.CliHandlerFactory.NewTransHandler(kc.opt.RemoteOpt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, h := range kc.opt.MetaHandlers {\n\t\tif shdlr, ok := h.(remote.StreamingMetaHandler); ok {\n\t\t\tkc.opt.RemoteOpt.StreamingMetaHandlers = append(kc.opt.RemoteOpt.StreamingMetaHandlers, shdlr)\n\t\t}\n\t}\n\n\trecvEndpoint := kc.opt.Streaming.BuildRecvInvokeChain(kc.invokeRecvEndpoint())\n\tsendEndpoint := kc.opt.Streaming.BuildSendInvokeChain(kc.invokeSendEndpoint())\n\n\treturn func(ctx context.Context, req, resp interface{}) (err error) {\n\t\t// req and resp as &streaming.Stream\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tst, scm, err := remotecli.NewStream(ctx, ri, handler, kc.opt.RemoteOpt)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tclientStream := newStream(st, scm, kc, ri, kc.getStreamingMode(ri), sendEndpoint, recvEndpoint)\n\t\tresp.(*streaming.Result).Stream = clientStream\n\t\treturn\n\t}, nil\n}\n\nfunc (kc *kClient) getStreamingMode(ri rpcinfo.RPCInfo) serviceinfo.StreamingMode {\n\tmethodInfo := kc.svcInfo.MethodInfo(ri.Invocation().MethodName())\n\tif methodInfo == nil {\n\t\treturn serviceinfo.StreamingNone\n\t}\n\treturn methodInfo.StreamingMode()\n}\n\ntype stream struct {\n\tstream streaming.Stream\n\tscm    *remotecli.StreamConnManager\n\tkc     *kClient\n\tri     rpcinfo.RPCInfo\n\n\tstreamingMode serviceinfo.StreamingMode\n\tsendEndpoint  endpoint.SendEndpoint\n\trecvEndpoint  endpoint.RecvEndpoint\n\tfinished      uint32\n}\n\nvar _ streaming.WithDoFinish = (*stream)(nil)\n\nfunc newStream(s streaming.Stream, scm *remotecli.StreamConnManager, kc *kClient, ri rpcinfo.RPCInfo,\n\tmode serviceinfo.StreamingMode, sendEP endpoint.SendEndpoint, recvEP endpoint.RecvEndpoint,\n) *stream {\n\treturn &stream{\n\t\tstream:        s,\n\t\tscm:           scm,\n\t\tkc:            kc,\n\t\tri:            ri,\n\t\tstreamingMode: mode,\n\t\tsendEndpoint:  sendEP,\n\t\trecvEndpoint:  recvEP,\n\t}\n}\n\nfunc (s *stream) SetTrailer(metadata.MD) {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SetHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SendHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\n// Header returns the header metadata sent by the server if any.\n// If a non-nil error is returned, stream.DoFinish() will be called to record the EndOfStream\n\n\n\n\n\n\n\nfunc (s *stream) Trailer() metadata.MD {\n\treturn s.stream.Trailer()\n}\n\nfunc (s *stream) Context() context.Context {\n\treturn s.stream.Context()\n}\n\n// RecvMsg receives a message from the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n\n\n\n\n\n\n// SendMsg sends a message to the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n// Close will send a frame with EndStream=true to the server.\n// It will always return a nil\nfunc (s *stream) Close() error {\n\treturn s.stream.Close()\n}\n\n// DoFinish implements the streaming.WithDoFinish interface, and it records the end of stream\n// It will release the connection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc isRPCError(err error) bool {\n\tif err == nil {\n\t\treturn false\n\t}\n\tif err == io.EOF {\n\t\treturn false\n\t}\n\t_, isBizStatusError := err.(kerrors.BizStatusErrorIface)\n\t// if a tracer needs to get the BizStatusError, it should read from rpcinfo.invocation.bizStatusErr\n\treturn !isBizStatusError\n}\n\n```\n# Function Signature: func (s *stream) Header() (md metadata.MD, err error) {\n# Natural Language Annotations: // Header returns the header metadata sent by the server if any.\n// If a non-nil error is returned, stream.DoFinish() will be called to record the EndOfStream\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (s *stream) Header() (md metadata.MD, err error) {\n\tif s.stream == nil {\n\t\terr = io.EOF\n\t\treturn\n\t}\n\tmd, err = s.stream.Header()\n\tif err != nil && !isRPCError(err) {\n\t\ts.DoFinish()\n\t}\n\treturn\n}\n```", "```go\nfunc (s *stream) Header() (md metadata.MD, err error) {\n    // Header returns the header metadata sent by the server if any.\n    // If a non-nil error is returned, stream.DoFinish() will be called to record the EndOfStream\n    md, err = s.stream.Header()\n    if err != nil && err != io.EOF {\n        s.DoFinish()\n    }\n    return\n}\n```", "```go\nfunc (s *stream) Header() (md metadata.MD, err error) {\n\tmd, err = s.stream.Header()\n\tif err != nil {\n\t\ts.DoFinish()\n\t}\n\treturn\n}\n```"]}, "kitex-client/stream.go-RecvMsg": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/stream.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"sync/atomic\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/remotecli\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n)\n\n// Streaming client streaming interface for code generate\ntype Streaming interface {\n\tStream(ctx context.Context, method string, request, response interface{}) error\n}\n\n// Stream implements the Streaming interface\nfunc (kc *kClient) Stream(ctx context.Context, method string, request, response interface{}) error {\n\tif !kc.inited {\n\t\tpanic(\"client not initialized\")\n\t}\n\tif kc.closed {\n\t\tpanic(\"client is already closed\")\n\t}\n\tif ctx == nil {\n\t\tpanic(\"ctx is nil\")\n\t}\n\tvar ri rpcinfo.RPCInfo\n\tctx, ri, _ = kc.initRPCInfo(ctx, method, 0, nil)\n\n\trpcinfo.AsMutableRPCConfig(ri.Config()).SetInteractionMode(rpcinfo.Streaming)\n\tctx = rpcinfo.NewCtxWithRPCInfo(ctx, ri)\n\n\tctx = kc.opt.TracerCtl.DoStart(ctx, ri)\n\treturn kc.sEps(ctx, request, response)\n}\n\nfunc (kc *kClient) invokeSendEndpoint() endpoint.SendEndpoint {\n\treturn func(stream streaming.Stream, req interface{}) (err error) {\n\t\treturn stream.SendMsg(req)\n\t}\n}\n\nfunc (kc *kClient) invokeRecvEndpoint() endpoint.RecvEndpoint {\n\treturn func(stream streaming.Stream, resp interface{}) (err error) {\n\t\treturn stream.RecvMsg(resp)\n\t}\n}\n\nfunc (kc *kClient) invokeStreamingEndpoint() (endpoint.Endpoint, error) {\n\thandler, err := kc.opt.RemoteOpt.CliHandlerFactory.NewTransHandler(kc.opt.RemoteOpt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, h := range kc.opt.MetaHandlers {\n\t\tif shdlr, ok := h.(remote.StreamingMetaHandler); ok {\n\t\t\tkc.opt.RemoteOpt.StreamingMetaHandlers = append(kc.opt.RemoteOpt.StreamingMetaHandlers, shdlr)\n\t\t}\n\t}\n\n\trecvEndpoint := kc.opt.Streaming.BuildRecvInvokeChain(kc.invokeRecvEndpoint())\n\tsendEndpoint := kc.opt.Streaming.BuildSendInvokeChain(kc.invokeSendEndpoint())\n\n\treturn func(ctx context.Context, req, resp interface{}) (err error) {\n\t\t// req and resp as &streaming.Stream\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tst, scm, err := remotecli.NewStream(ctx, ri, handler, kc.opt.RemoteOpt)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tclientStream := newStream(st, scm, kc, ri, kc.getStreamingMode(ri), sendEndpoint, recvEndpoint)\n\t\tresp.(*streaming.Result).Stream = clientStream\n\t\treturn\n\t}, nil\n}\n\nfunc (kc *kClient) getStreamingMode(ri rpcinfo.RPCInfo) serviceinfo.StreamingMode {\n\tmethodInfo := kc.svcInfo.MethodInfo(ri.Invocation().MethodName())\n\tif methodInfo == nil {\n\t\treturn serviceinfo.StreamingNone\n\t}\n\treturn methodInfo.StreamingMode()\n}\n\ntype stream struct {\n\tstream streaming.Stream\n\tscm    *remotecli.StreamConnManager\n\tkc     *kClient\n\tri     rpcinfo.RPCInfo\n\n\tstreamingMode serviceinfo.StreamingMode\n\tsendEndpoint  endpoint.SendEndpoint\n\trecvEndpoint  endpoint.RecvEndpoint\n\tfinished      uint32\n}\n\nvar _ streaming.WithDoFinish = (*stream)(nil)\n\nfunc newStream(s streaming.Stream, scm *remotecli.StreamConnManager, kc *kClient, ri rpcinfo.RPCInfo,\n\tmode serviceinfo.StreamingMode, sendEP endpoint.SendEndpoint, recvEP endpoint.RecvEndpoint,\n) *stream {\n\treturn &stream{\n\t\tstream:        s,\n\t\tscm:           scm,\n\t\tkc:            kc,\n\t\tri:            ri,\n\t\tstreamingMode: mode,\n\t\tsendEndpoint:  sendEP,\n\t\trecvEndpoint:  recvEP,\n\t}\n}\n\nfunc (s *stream) SetTrailer(metadata.MD) {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SetHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SendHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\n// Header returns the header metadata sent by the server if any.\n// If a non-nil error is returned, stream.DoFinish() will be called to record the EndOfStream\n\n\n\n\n\n\n\nfunc (s *stream) Trailer() metadata.MD {\n\treturn s.stream.Trailer()\n}\n\nfunc (s *stream) Context() context.Context {\n\treturn s.stream.Context()\n}\n\n// RecvMsg receives a message from the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n\n\n\n\n\n\n// SendMsg sends a message to the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n// Close will send a frame with EndStream=true to the server.\n// It will always return a nil\nfunc (s *stream) Close() error {\n\treturn s.stream.Close()\n}\n\n// DoFinish implements the streaming.WithDoFinish interface, and it records the end of stream\n// It will release the connection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc isRPCError(err error) bool {\n\tif err == nil {\n\t\treturn false\n\t}\n\tif err == io.EOF {\n\t\treturn false\n\t}\n\t_, isBizStatusError := err.(kerrors.BizStatusErrorIface)\n\t// if a tracer needs to get the BizStatusError, it should read from rpcinfo.invocation.bizStatusErr\n\treturn !isBizStatusError\n}\n\n```\n# Function Signature: func (s *stream) RecvMsg(m interface{}) (err error) {\n# Natural Language Annotations: // RecvMsg receives a message from the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (s *stream) RecvMsg(m interface{}) (err error) {\n\terr = s.stream.RecvMsg(m)\n\tif err != nil && isRPCError(err) {\n\t\ts.DoFinish(err)\n\t}\n\treturn\n}\n```", "```go\nfunc (s *stream) RecvMsg(m interface{}) (err error) {\n\terr = s.stream.RecvMsg(m)\n\tif err != nil && isRPCError(err) {\n\t\ts.DoFinish()\n\t}\n\treturn err\n}\n```", "```go\nfunc (s *stream) RecvMsg(m interface{}) (err error) {\n\terr = s.stream.RecvMsg(m)\n\tif err != nil {\n\t\ts.DoFinish()\n\t}\n\treturn\n}\n```"]}, "kitex-client/stream.go-SendMsg": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/stream.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"sync/atomic\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/remotecli\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n)\n\n// Streaming client streaming interface for code generate\ntype Streaming interface {\n\tStream(ctx context.Context, method string, request, response interface{}) error\n}\n\n// Stream implements the Streaming interface\nfunc (kc *kClient) Stream(ctx context.Context, method string, request, response interface{}) error {\n\tif !kc.inited {\n\t\tpanic(\"client not initialized\")\n\t}\n\tif kc.closed {\n\t\tpanic(\"client is already closed\")\n\t}\n\tif ctx == nil {\n\t\tpanic(\"ctx is nil\")\n\t}\n\tvar ri rpcinfo.RPCInfo\n\tctx, ri, _ = kc.initRPCInfo(ctx, method, 0, nil)\n\n\trpcinfo.AsMutableRPCConfig(ri.Config()).SetInteractionMode(rpcinfo.Streaming)\n\tctx = rpcinfo.NewCtxWithRPCInfo(ctx, ri)\n\n\tctx = kc.opt.TracerCtl.DoStart(ctx, ri)\n\treturn kc.sEps(ctx, request, response)\n}\n\nfunc (kc *kClient) invokeSendEndpoint() endpoint.SendEndpoint {\n\treturn func(stream streaming.Stream, req interface{}) (err error) {\n\t\treturn stream.SendMsg(req)\n\t}\n}\n\nfunc (kc *kClient) invokeRecvEndpoint() endpoint.RecvEndpoint {\n\treturn func(stream streaming.Stream, resp interface{}) (err error) {\n\t\treturn stream.RecvMsg(resp)\n\t}\n}\n\nfunc (kc *kClient) invokeStreamingEndpoint() (endpoint.Endpoint, error) {\n\thandler, err := kc.opt.RemoteOpt.CliHandlerFactory.NewTransHandler(kc.opt.RemoteOpt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, h := range kc.opt.MetaHandlers {\n\t\tif shdlr, ok := h.(remote.StreamingMetaHandler); ok {\n\t\t\tkc.opt.RemoteOpt.StreamingMetaHandlers = append(kc.opt.RemoteOpt.StreamingMetaHandlers, shdlr)\n\t\t}\n\t}\n\n\trecvEndpoint := kc.opt.Streaming.BuildRecvInvokeChain(kc.invokeRecvEndpoint())\n\tsendEndpoint := kc.opt.Streaming.BuildSendInvokeChain(kc.invokeSendEndpoint())\n\n\treturn func(ctx context.Context, req, resp interface{}) (err error) {\n\t\t// req and resp as &streaming.Stream\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tst, scm, err := remotecli.NewStream(ctx, ri, handler, kc.opt.RemoteOpt)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tclientStream := newStream(st, scm, kc, ri, kc.getStreamingMode(ri), sendEndpoint, recvEndpoint)\n\t\tresp.(*streaming.Result).Stream = clientStream\n\t\treturn\n\t}, nil\n}\n\nfunc (kc *kClient) getStreamingMode(ri rpcinfo.RPCInfo) serviceinfo.StreamingMode {\n\tmethodInfo := kc.svcInfo.MethodInfo(ri.Invocation().MethodName())\n\tif methodInfo == nil {\n\t\treturn serviceinfo.StreamingNone\n\t}\n\treturn methodInfo.StreamingMode()\n}\n\ntype stream struct {\n\tstream streaming.Stream\n\tscm    *remotecli.StreamConnManager\n\tkc     *kClient\n\tri     rpcinfo.RPCInfo\n\n\tstreamingMode serviceinfo.StreamingMode\n\tsendEndpoint  endpoint.SendEndpoint\n\trecvEndpoint  endpoint.RecvEndpoint\n\tfinished      uint32\n}\n\nvar _ streaming.WithDoFinish = (*stream)(nil)\n\nfunc newStream(s streaming.Stream, scm *remotecli.StreamConnManager, kc *kClient, ri rpcinfo.RPCInfo,\n\tmode serviceinfo.StreamingMode, sendEP endpoint.SendEndpoint, recvEP endpoint.RecvEndpoint,\n) *stream {\n\treturn &stream{\n\t\tstream:        s,\n\t\tscm:           scm,\n\t\tkc:            kc,\n\t\tri:            ri,\n\t\tstreamingMode: mode,\n\t\tsendEndpoint:  sendEP,\n\t\trecvEndpoint:  recvEP,\n\t}\n}\n\nfunc (s *stream) SetTrailer(metadata.MD) {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SetHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SendHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\n// Header returns the header metadata sent by the server if any.\n// If a non-nil error is returned, stream.DoFinish() will be called to record the EndOfStream\n\n\n\n\n\n\n\nfunc (s *stream) Trailer() metadata.MD {\n\treturn s.stream.Trailer()\n}\n\nfunc (s *stream) Context() context.Context {\n\treturn s.stream.Context()\n}\n\n// RecvMsg receives a message from the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n\n\n\n\n\n\n// SendMsg sends a message to the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n// Close will send a frame with EndStream=true to the server.\n// It will always return a nil\nfunc (s *stream) Close() error {\n\treturn s.stream.Close()\n}\n\n// DoFinish implements the streaming.WithDoFinish interface, and it records the end of stream\n// It will release the connection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc isRPCError(err error) bool {\n\tif err == nil {\n\t\treturn false\n\t}\n\tif err == io.EOF {\n\t\treturn false\n\t}\n\t_, isBizStatusError := err.(kerrors.BizStatusErrorIface)\n\t// if a tracer needs to get the BizStatusError, it should read from rpcinfo.invocation.bizStatusErr\n\treturn !isBizStatusError\n}\n\n```\n# Function Signature: func (s *stream) SendMsg(m interface{}) (err error) {\n# Natural Language Annotations: // SendMsg sends a message to the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (s *stream) SendMsg(m interface{}) (err error) {\n\terr = s.sendEndpoint(s.stream, m)\n\tif err != nil {\n\t\ts.DoFinish()\n\t}\n\treturn\n}\n```", "```go\nfunc (s *stream) SendMsg(m interface{}) (err error) {\n\terr = s.sendEndpoint(s.stream, m)\n\tif err != nil {\n\t\ts.DoFinish()\n\t}\n\treturn\n}\n```\nThis function `SendMsg` sends a message to the server. If an error occurs during the sending process, it calls the `DoFinish` function to record the end of the stream.", "```go\nfunc (s *stream) SendMsg(m interface{}) (err error) {\n\terr = s.sendEndpoint(s.stream, m)\n\tif err != nil {\n\t\ts.DoFinish()\n\t}\n\treturn\n}\n```"]}, "kitex-client/stream.go-DoFinish": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/stream.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"io\"\n\t\"sync/atomic\"\n\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/metadata\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/remotecli\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n)\n\n// Streaming client streaming interface for code generate\ntype Streaming interface {\n\tStream(ctx context.Context, method string, request, response interface{}) error\n}\n\n// Stream implements the Streaming interface\nfunc (kc *kClient) Stream(ctx context.Context, method string, request, response interface{}) error {\n\tif !kc.inited {\n\t\tpanic(\"client not initialized\")\n\t}\n\tif kc.closed {\n\t\tpanic(\"client is already closed\")\n\t}\n\tif ctx == nil {\n\t\tpanic(\"ctx is nil\")\n\t}\n\tvar ri rpcinfo.RPCInfo\n\tctx, ri, _ = kc.initRPCInfo(ctx, method, 0, nil)\n\n\trpcinfo.AsMutableRPCConfig(ri.Config()).SetInteractionMode(rpcinfo.Streaming)\n\tctx = rpcinfo.NewCtxWithRPCInfo(ctx, ri)\n\n\tctx = kc.opt.TracerCtl.DoStart(ctx, ri)\n\treturn kc.sEps(ctx, request, response)\n}\n\nfunc (kc *kClient) invokeSendEndpoint() endpoint.SendEndpoint {\n\treturn func(stream streaming.Stream, req interface{}) (err error) {\n\t\treturn stream.SendMsg(req)\n\t}\n}\n\nfunc (kc *kClient) invokeRecvEndpoint() endpoint.RecvEndpoint {\n\treturn func(stream streaming.Stream, resp interface{}) (err error) {\n\t\treturn stream.RecvMsg(resp)\n\t}\n}\n\nfunc (kc *kClient) invokeStreamingEndpoint() (endpoint.Endpoint, error) {\n\thandler, err := kc.opt.RemoteOpt.CliHandlerFactory.NewTransHandler(kc.opt.RemoteOpt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, h := range kc.opt.MetaHandlers {\n\t\tif shdlr, ok := h.(remote.StreamingMetaHandler); ok {\n\t\t\tkc.opt.RemoteOpt.StreamingMetaHandlers = append(kc.opt.RemoteOpt.StreamingMetaHandlers, shdlr)\n\t\t}\n\t}\n\n\trecvEndpoint := kc.opt.Streaming.BuildRecvInvokeChain(kc.invokeRecvEndpoint())\n\tsendEndpoint := kc.opt.Streaming.BuildSendInvokeChain(kc.invokeSendEndpoint())\n\n\treturn func(ctx context.Context, req, resp interface{}) (err error) {\n\t\t// req and resp as &streaming.Stream\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tst, scm, err := remotecli.NewStream(ctx, ri, handler, kc.opt.RemoteOpt)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t\tclientStream := newStream(st, scm, kc, ri, kc.getStreamingMode(ri), sendEndpoint, recvEndpoint)\n\t\tresp.(*streaming.Result).Stream = clientStream\n\t\treturn\n\t}, nil\n}\n\nfunc (kc *kClient) getStreamingMode(ri rpcinfo.RPCInfo) serviceinfo.StreamingMode {\n\tmethodInfo := kc.svcInfo.MethodInfo(ri.Invocation().MethodName())\n\tif methodInfo == nil {\n\t\treturn serviceinfo.StreamingNone\n\t}\n\treturn methodInfo.StreamingMode()\n}\n\ntype stream struct {\n\tstream streaming.Stream\n\tscm    *remotecli.StreamConnManager\n\tkc     *kClient\n\tri     rpcinfo.RPCInfo\n\n\tstreamingMode serviceinfo.StreamingMode\n\tsendEndpoint  endpoint.SendEndpoint\n\trecvEndpoint  endpoint.RecvEndpoint\n\tfinished      uint32\n}\n\nvar _ streaming.WithDoFinish = (*stream)(nil)\n\nfunc newStream(s streaming.Stream, scm *remotecli.StreamConnManager, kc *kClient, ri rpcinfo.RPCInfo,\n\tmode serviceinfo.StreamingMode, sendEP endpoint.SendEndpoint, recvEP endpoint.RecvEndpoint,\n) *stream {\n\treturn &stream{\n\t\tstream:        s,\n\t\tscm:           scm,\n\t\tkc:            kc,\n\t\tri:            ri,\n\t\tstreamingMode: mode,\n\t\tsendEndpoint:  sendEP,\n\t\trecvEndpoint:  recvEP,\n\t}\n}\n\nfunc (s *stream) SetTrailer(metadata.MD) {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SetHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\nfunc (s *stream) SendHeader(metadata.MD) error {\n\tpanic(\"this method should only be used in server side stream!\")\n}\n\n// Header returns the header metadata sent by the server if any.\n// If a non-nil error is returned, stream.DoFinish() will be called to record the EndOfStream\n\n\n\n\n\n\n\nfunc (s *stream) Trailer() metadata.MD {\n\treturn s.stream.Trailer()\n}\n\nfunc (s *stream) Context() context.Context {\n\treturn s.stream.Context()\n}\n\n// RecvMsg receives a message from the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n\n\n\n\n\n\n// SendMsg sends a message to the server.\n// If an error is returned, stream.DoFinish() will be called to record the end of stream\n\n\n\n\n\n\n\n// Close will send a frame with EndStream=true to the server.\n// It will always return a nil\nfunc (s *stream) Close() error {\n\treturn s.stream.Close()\n}\n\n// DoFinish implements the streaming.WithDoFinish interface, and it records the end of stream\n// It will release the connection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc isRPCError(err error) bool {\n\tif err == nil {\n\t\treturn false\n\t}\n\tif err == io.EOF {\n\t\treturn false\n\t}\n\t_, isBizStatusError := err.(kerrors.BizStatusErrorIface)\n\t// if a tracer needs to get the BizStatusError, it should read from rpcinfo.invocation.bizStatusErr\n\treturn !isBizStatusError\n}\n\n```\n# Function Signature: func (s *stream) DoFinish(err error) {\n# Natural Language Annotations: // DoFinish implements the streaming.WithDoFinish interface, and it records the end of stream\n// It will release the connection.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (s *stream) DoFinish(err error) {\n\tif atomic.SwapUint32(&s.finished, 1) == 0 {\n\t\tif isRPCError(err) && s.streamingMode == serviceinfo.StreamingBoth {\n\t\t\ts.stream.Close()\n\t\t}\n\t\tif s.ri.Config().StreamingRPCTimeout() > 0 {\n\t\t\ts.stream.SetTimeout(s.ri.Config().StreamingRPCTimeout())\n\t\t}\n\t\ts.scm.ReleaseConn()\n\t}\n}\n```\nThis function implements the `DoFinish` method of the `WithDoFinish` interface, which is used to release the connection. It sets the atomic variable `finished` to 1 to ensure that the function is only executed once. If the error is an RPC error and the streaming mode is both, it closes the stream. Afterwards, it sets the timeout of the stream based on the streaming RPC timeout configuration, if it is set. Finally, it releases the connection using the `ReleaseConn` method of `StreamConnManager`.", "```go\nfunc (s *stream) DoFinish(err error) {\n\t// DoFinish implements the streaming.WithDoFinish interface, and it records the end of stream\n\t// It will release the connection.\n\tif atomic.CompareAndSwapUint32(&s.finished, 0, 1) {\n\t\tif s.scm != nil {\n\t\t\ts.scm.ReleaseConn()\n\t\t}\n\t\tif err != nil && !isRPCError(err) {\n\t\t\t// TODO: log the error\n\t\t}\n\t}\n}\n```", "```go\nfunc (s *stream) DoFinish(err error) {\n\tif atomic.CompareAndSwapUint32(&s.finished, 0, 1) {\n\t\tif isRPCError(err) {\n\t\t\ts.scm.ReleaseConn(s.stream.Context())\n\t\t}\n\t\tif s.streamingMode == serviceinfo.StreamingNone {\n\t\t\ts.scm.ReleaseConn(s.stream.Context())\n\t\t}\n\t\tif s.streamingMode == serviceinfo.StreamingCall {\n\t\t\ts.scm.ReleaseConn(s.stream.Context())\n\t\t}\n\t\tif s.streamingMode == serviceinfo.StreamingReturn {\n\t\t\ts.scm.ReleaseConn(s.stream.Context())\n\t\t}\n\t\tif s.streamingMode == serviceinfo.StreamingDuplex {\n\t\t\t// no release for duplex mode, it will be released by close call\n\t\t}\n\t}\n}\n```"]}, "kitex-client/client.go-NewClient": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/client.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"runtime\"\n\t\"runtime/debug\"\n\t\"strconv\"\n\t\"sync/atomic\"\n\n\t\"github.com/bytedance/gopkg/cloud/metainfo\"\n\t\"github.com/cloudwego/localsession/backup\"\n\n\t\"github.com/cloudwego/kitex/client/callopt\"\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/acl\"\n\t\"github.com/cloudwego/kitex/pkg/consts\"\n\t\"github.com/cloudwego/kitex/pkg/diagnosis\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/proxy\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/bound\"\n\t\"github.com/cloudwego/kitex/pkg/remote/remotecli\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpctimeout\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Client is the core interface abstraction of kitex client.\n// It is designed for generated codes and should not be used directly.\n// Parameter method specifies the method of a RPC call.\n// Request is a packing of request parameters in the actual method defined in IDL, consist of zero, one\n// or multiple arguments. So is response to the actual result type.\n// Response may be nil to address oneway calls.\ntype Client interface {\n\tCall(ctx context.Context, method string, request, response interface{}) error\n}\n\ntype kClient struct {\n\tsvcInfo *serviceinfo.ServiceInfo\n\tmws     []endpoint.Middleware\n\teps     endpoint.Endpoint\n\tsEps    endpoint.Endpoint\n\topt     *client.Options\n\tlbf     *lbcache.BalancerFactory\n\n\tinited bool\n\tclosed bool\n}\n\n// Set finalizer on kClient does not take effect, because kClient has a circular reference problem\n// when construct the endpoint.Endpoint in the invokeHandleEndpoint,\n// so wrapping kClient as kcFinalizerClient, and set finalizer on kcFinalizerClient, it can solve this problem.\ntype kcFinalizerClient struct {\n\t*kClient\n}\n\nfunc (kf *kcFinalizerClient) Call(ctx context.Context, method string, request, response interface{}) error {\n\tdefer runtime.KeepAlive(kf)\n\treturn kf.kClient.Call(ctx, method, request, response)\n}\n\n// NewClient creates a kitex.Client with the given ServiceInfo, it is from generated code.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (kc *kClient) init() (err error) {\n\tinitTransportProtocol(kc.svcInfo, kc.opt.Configs)\n\tif err = kc.checkOptions(); err != nil {\n\t\treturn err\n\t}\n\tif err = kc.initCircuitBreaker(); err != nil {\n\t\treturn err\n\t}\n\tif err = kc.initRetryer(); err != nil {\n\t\treturn err\n\t}\n\tif err = kc.initProxy(); err != nil {\n\t\treturn err\n\t}\n\tif err = kc.initConnPool(); err != nil {\n\t\treturn err\n\t}\n\tif err = kc.initLBCache(); err != nil {\n\t\treturn err\n\t}\n\tctx := kc.initContext()\n\tkc.initMiddlewares(ctx)\n\tkc.initStreamMiddlewares(ctx)\n\tkc.initDebugService()\n\tkc.richRemoteOption()\n\tif err = kc.buildInvokeChain(); err != nil {\n\t\treturn err\n\t}\n\tif err = kc.warmingUp(); err != nil {\n\t\treturn err\n\t}\n\tkc.inited = true\n\treturn nil\n}\n\nfunc (kc *kClient) checkOptions() (err error) {\n\tif kc.opt.Svr.ServiceName == \"\" {\n\t\treturn errors.New(\"service name is required\")\n\t}\n\treturn nil\n}\n\nfunc (kc *kClient) initCircuitBreaker() error {\n\tif kc.opt.CBSuite != nil {\n\t\tkc.opt.CBSuite.SetEventBusAndQueue(kc.opt.Bus, kc.opt.Events)\n\t}\n\treturn nil\n}\n\nfunc (kc *kClient) initRetryer() error {\n\tif kc.opt.RetryContainer == nil {\n\t\tif kc.opt.RetryMethodPolicies == nil {\n\t\t\treturn nil\n\t\t}\n\t\tkc.opt.InitRetryContainer()\n\t}\n\treturn kc.opt.RetryContainer.Init(kc.opt.RetryMethodPolicies, kc.opt.RetryWithResult)\n}\n\nfunc (kc *kClient) initContext() context.Context {\n\tctx := context.Background()\n\tctx = context.WithValue(ctx, endpoint.CtxEventBusKey, kc.opt.Bus)\n\tctx = context.WithValue(ctx, endpoint.CtxEventQueueKey, kc.opt.Events)\n\tctx = context.WithValue(ctx, rpctimeout.TimeoutAdjustKey, &kc.opt.ExtraTimeout)\n\tif chr, ok := kc.opt.Proxy.(proxy.ContextHandler); ok {\n\t\tctx = chr.HandleContext(ctx)\n\t}\n\treturn ctx\n}\n\nfunc (kc *kClient) initProxy() error {\n\tif kc.opt.Proxy != nil {\n\t\tcfg := proxy.Config{\n\t\t\tServerInfo:   kc.opt.Svr,\n\t\t\tResolver:     kc.opt.Resolver,\n\t\t\tBalancer:     kc.opt.Balancer,\n\t\t\tPool:         kc.opt.RemoteOpt.ConnPool,\n\t\t\tFixedTargets: kc.opt.Targets,\n\t\t\tRPCConfig:    kc.opt.Configs,\n\t\t}\n\t\tif err := kc.opt.Proxy.Configure(&cfg); err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// update fields in the client option for further use.\n\t\tkc.opt.Resolver = cfg.Resolver\n\t\tkc.opt.Balancer = cfg.Balancer\n\t\t// close predefined pool when proxy init new pool.\n\t\tif cfg.Pool != kc.opt.RemoteOpt.ConnPool && kc.opt.RemoteOpt.ConnPool != nil {\n\t\t\tkc.opt.RemoteOpt.ConnPool.Close()\n\t\t}\n\t\tkc.opt.RemoteOpt.ConnPool = cfg.Pool\n\t\tkc.opt.Targets = cfg.FixedTargets\n\t}\n\treturn nil\n}\n\nfunc (kc *kClient) initConnPool() error {\n\tpool := kc.opt.RemoteOpt.ConnPool\n\tkc.opt.CloseCallbacks = append(kc.opt.CloseCallbacks, pool.Close)\n\n\tif df, ok := pool.(interface{ Dump() interface{} }); ok {\n\t\tkc.opt.DebugService.RegisterProbeFunc(diagnosis.ConnPoolKey, df.Dump)\n\t}\n\tif r, ok := pool.(remote.ConnPoolReporter); ok && kc.opt.RemoteOpt.EnableConnPoolReporter {\n\t\tr.EnableReporter()\n\t}\n\n\tif long, ok := pool.(remote.LongConnPool); ok {\n\t\tkc.opt.Bus.Watch(discovery.ChangeEventName, func(ev *event.Event) {\n\t\t\tch, ok := ev.Extra.(*discovery.Change)\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tfor _, inst := range ch.Removed {\n\t\t\t\tif addr := inst.Address(); addr != nil {\n\t\t\t\t\tlong.Clean(addr.Network(), addr.String())\n\t\t\t\t}\n\t\t\t}\n\t\t})\n\t}\n\treturn nil\n}\n\nfunc (kc *kClient) initLBCache() error {\n\tif kc.opt.Proxy != nil && kc.opt.Resolver == nil {\n\t\treturn nil\n\t}\n\tonChange := discoveryEventHandler(discovery.ChangeEventName, kc.opt.Bus, kc.opt.Events)\n\tonDelete := discoveryEventHandler(discovery.DeleteEventName, kc.opt.Bus, kc.opt.Events)\n\tresolver := kc.opt.Resolver\n\tif resolver == nil {\n\t\t// fake a resolver instead of returning an error directly because users may use\n\t\t// callopt.WithHostPort to specify target addresses after NewClient.\n\t\tresolver = &discovery.SynthesizedResolver{\n\t\t\tResolveFunc: func(ctx context.Context, target string) (discovery.Result, error) {\n\t\t\t\treturn discovery.Result{}, kerrors.ErrNoResolver\n\t\t\t},\n\t\t\tNameFunc: func() string { return \"no_resolver\" },\n\t\t}\n\t}\n\t// because we cannot ensure that user's custom loadbalancer is cacheable, we need to disable it here\n\tcacheOpts := lbcache.Options{DiagnosisService: kc.opt.DebugService, Cacheable: false}\n\tbalancer := kc.opt.Balancer\n\tif balancer == nil {\n\t\t// default internal lb balancer is cacheable\n\t\tcacheOpts.Cacheable = true\n\t\tbalancer = loadbalance.NewWeightedBalancer()\n\t}\n\tif kc.opt.BalancerCacheOpt != nil {\n\t\tcacheOpts = *kc.opt.BalancerCacheOpt\n\t}\n\tkc.lbf = lbcache.NewBalancerFactory(resolver, balancer, cacheOpts)\n\trbIdx := kc.lbf.RegisterRebalanceHook(onChange)\n\tkc.opt.CloseCallbacks = append(kc.opt.CloseCallbacks, func() error {\n\t\tkc.lbf.DeregisterRebalanceHook(rbIdx)\n\t\treturn nil\n\t})\n\tdIdx := kc.lbf.RegisterDeleteHook(onDelete)\n\tkc.opt.CloseCallbacks = append(kc.opt.CloseCallbacks, func() error {\n\t\tkc.lbf.DeregisterDeleteHook(dIdx)\n\t\treturn nil\n\t})\n\treturn nil\n}\n\nfunc (kc *kClient) initMiddlewares(ctx context.Context) {\n\tbuilderMWs := richMWsWithBuilder(ctx, kc.opt.MWBs)\n\t// integrate xds if enabled\n\tif kc.opt.XDSEnabled && kc.opt.XDSRouterMiddleware != nil && kc.opt.Proxy == nil {\n\t\tkc.mws = append(kc.mws, kc.opt.XDSRouterMiddleware)\n\t}\n\tkc.mws = append(kc.mws, kc.opt.CBSuite.ServiceCBMW(), rpcTimeoutMW(ctx), contextMW)\n\tkc.mws = append(kc.mws, builderMWs...)\n\tkc.mws = append(kc.mws, acl.NewACLMiddleware(kc.opt.ACLRules))\n\tif kc.opt.Proxy == nil {\n\t\tkc.mws = append(kc.mws, newResolveMWBuilder(kc.lbf)(ctx))\n\t\tkc.mws = append(kc.mws, kc.opt.CBSuite.InstanceCBMW())\n\t\tkc.mws = append(kc.mws, richMWsWithBuilder(ctx, kc.opt.IMWBs)...)\n\t} else {\n\t\tif kc.opt.Resolver != nil { // customized service discovery\n\t\t\tkc.mws = append(kc.mws, newResolveMWBuilder(kc.lbf)(ctx))\n\t\t}\n\t\tkc.mws = append(kc.mws, newProxyMW(kc.opt.Proxy))\n\t}\n\tkc.mws = append(kc.mws, newIOErrorHandleMW(kc.opt.ErrHandle))\n}\n\nfunc (kc *kClient) initStreamMiddlewares(ctx context.Context) {\n\tkc.opt.Streaming.EventHandler = kc.opt.TracerCtl.GetStreamEventHandler()\n\tkc.opt.Streaming.InitMiddlewares(ctx)\n}\n\nfunc richMWsWithBuilder(ctx context.Context, mwBs []endpoint.MiddlewareBuilder) (mws []endpoint.Middleware) {\n\tfor i := range mwBs {\n\t\tmws = append(mws, mwBs[i](ctx))\n\t}\n\treturn\n}\n\n// initRPCInfo initializes the RPCInfo structure and attaches it to context.\nfunc (kc *kClient) initRPCInfo(ctx context.Context, method string, retryTimes int, firstRI rpcinfo.RPCInfo) (context.Context, rpcinfo.RPCInfo, *callopt.CallOptions) {\n\treturn initRPCInfo(ctx, method, kc.opt, kc.svcInfo, retryTimes, firstRI)\n}\n\nfunc applyCallOptions(ctx context.Context, cfg rpcinfo.MutableRPCConfig, svr remoteinfo.RemoteInfo, opt *client.Options) (context.Context, *callopt.CallOptions) {\n\tcos := CallOptionsFromCtx(ctx)\n\tif len(cos) > 0 {\n\t\tinfo, callOpts := callopt.Apply(cos, cfg, svr, opt.Locks, opt.HTTPResolver)\n\t\tctx = context.WithValue(ctx, ctxCallOptionInfoKey, info)\n\t\treturn ctx, callOpts\n\t}\n\topt.Locks.ApplyLocks(cfg, svr)\n\treturn ctx, nil\n}\n\n// Call implements the Client interface .\nfunc (kc *kClient) Call(ctx context.Context, method string, request, response interface{}) (err error) {\n\t// merge backup context if no metainfo found in ctx\n\tctx = backup.RecoverCtxOnDemands(ctx, kc.opt.CtxBackupHandler)\n\n\tvalidateForCall(ctx, kc.inited, kc.closed)\n\tvar ri rpcinfo.RPCInfo\n\tvar callOpts *callopt.CallOptions\n\tctx, ri, callOpts = kc.initRPCInfo(ctx, method, 0, nil)\n\n\tctx = kc.opt.TracerCtl.DoStart(ctx, ri)\n\tvar reportErr error\n\tvar recycleRI bool\n\tdefer func() {\n\t\tif panicInfo := recover(); panicInfo != nil {\n\t\t\terr = rpcinfo.ClientPanicToErr(ctx, panicInfo, ri, false)\n\t\t\treportErr = err\n\t\t}\n\t\tkc.opt.TracerCtl.DoFinish(ctx, ri, reportErr)\n\t\tif recycleRI {\n\t\t\t// why need check recycleRI to decide if recycle RPCInfo?\n\t\t\t// 1. no retry, rpc timeout happen will cause panic when response return\n\t\t\t// 2. retry success, will cause panic when first call return\n\t\t\t// 3. backup request may cause panic, cannot recycle first RPCInfo\n\t\t\t// RPCInfo will be recycled after rpc is finished,\n\t\t\t// holding RPCInfo in a new goroutine is forbidden.\n\t\t\trpcinfo.PutRPCInfo(ri)\n\t\t}\n\t\tcallOpts.Recycle()\n\t}()\n\n\tcallOptRetry := getCalloptRetryPolicy(callOpts)\n\tif kc.opt.RetryContainer == nil && callOptRetry != nil && callOptRetry.Enable {\n\t\t// setup retry in callopt\n\t\tkc.opt.InitRetryContainer()\n\t}\n\n\t// Add necessary keys to context for isolation between kitex client method calls\n\tctx = retry.PrepareRetryContext(ctx)\n\n\tif kc.opt.RetryContainer == nil {\n\t\t// call without retry policy\n\t\terr = kc.eps(ctx, request, response)\n\t\tif err == nil {\n\t\t\trecycleRI = true\n\t\t}\n\t} else {\n\t\tvar lastRI rpcinfo.RPCInfo\n\t\tlastRI, recycleRI, err = kc.opt.RetryContainer.WithRetryIfNeeded(ctx, callOptRetry, kc.rpcCallWithRetry(ri, method, request, response), ri, request)\n\t\tif ri != lastRI {\n\t\t\t// reset ri of ctx to lastRI\n\t\t\tctx = rpcinfo.NewCtxWithRPCInfo(ctx, lastRI)\n\t\t}\n\t\tri = lastRI\n\t}\n\n\t// do fallback if with setup\n\terr, reportErr = doFallbackIfNeeded(ctx, ri, request, response, err, kc.opt.Fallback, callOpts)\n\treturn err\n}\n\nfunc (kc *kClient) rpcCallWithRetry(ri rpcinfo.RPCInfo, method string, request, response interface{}) retry.RPCCallFunc {\n\t// call with retry policy\n\tvar callTimes int32\n\t// prevRI represents a value of rpcinfo.RPCInfo type.\n\tvar prevRI atomic.Value\n\treturn func(ctx context.Context, r retry.Retryer) (rpcinfo.RPCInfo, interface{}, error) {\n\t\tcurrCallTimes := int(atomic.AddInt32(&callTimes, 1))\n\t\tcRI := ri\n\t\tif currCallTimes > 1 {\n\t\t\tctx, cRI, _ = kc.initRPCInfo(ctx, method, currCallTimes-1, ri)\n\t\t\tctx = metainfo.WithPersistentValue(ctx, retry.TransitKey, strconv.Itoa(currCallTimes-1))\n\t\t\tif prevRI.Load() == nil {\n\t\t\t\tprevRI.Store(ri)\n\t\t\t}\n\t\t\tr.Prepare(ctx, prevRI.Load().(rpcinfo.RPCInfo), cRI)\n\t\t\tprevRI.Store(cRI)\n\t\t}\n\t\tcallErr := kc.eps(ctx, request, response)\n\t\treturn cRI, response, callErr\n\t}\n}\n\nfunc (kc *kClient) initDebugService() {\n\tif ds := kc.opt.DebugService; ds != nil {\n\t\tds.RegisterProbeFunc(diagnosis.DestServiceKey, diagnosis.WrapAsProbeFunc(kc.opt.Svr.ServiceName))\n\t\tds.RegisterProbeFunc(diagnosis.OptionsKey, diagnosis.WrapAsProbeFunc(kc.opt.DebugInfo))\n\t\tds.RegisterProbeFunc(diagnosis.ChangeEventsKey, kc.opt.Events.Dump)\n\t\tds.RegisterProbeFunc(diagnosis.ServiceInfosKey, diagnosis.WrapAsProbeFunc(map[string]*serviceinfo.ServiceInfo{kc.svcInfo.ServiceName: kc.svcInfo}))\n\t}\n}\n\nfunc (kc *kClient) richRemoteOption() {\n\tkc.opt.RemoteOpt.SvcInfo = kc.svcInfo\n\t// for client trans info handler\n\tif len(kc.opt.MetaHandlers) > 0 {\n\t\t// TODO in stream situations, meta is only assembled when the stream creates\n\t\t// metaHandler needs to be called separately.\n\t\t// (newClientStreamer: call WriteMeta before remotecli.NewClient)\n\t\ttransInfoHdlr := bound.NewTransMetaHandler(kc.opt.MetaHandlers)\n\t\tkc.opt.RemoteOpt.PrependBoundHandler(transInfoHdlr)\n\t}\n}\n\nfunc (kc *kClient) buildInvokeChain() error {\n\tinnerHandlerEp, err := kc.invokeHandleEndpoint()\n\tif err != nil {\n\t\treturn err\n\t}\n\tkc.eps = endpoint.Chain(kc.mws...)(innerHandlerEp)\n\n\tinnerStreamingEp, err := kc.invokeStreamingEndpoint()\n\tif err != nil {\n\t\treturn err\n\t}\n\tkc.sEps = endpoint.Chain(kc.mws...)(innerStreamingEp)\n\treturn nil\n}\n\nfunc (kc *kClient) invokeHandleEndpoint() (endpoint.Endpoint, error) {\n\ttransPipl, err := newCliTransHandler(kc.opt.RemoteOpt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn func(ctx context.Context, req, resp interface{}) (err error) {\n\t\tvar sendMsg remote.Message\n\t\tvar recvMsg remote.Message\n\t\tdefer func() {\n\t\t\tremote.RecycleMessage(sendMsg)\n\t\t\t// Notice, recycle and decode may race if decode exec in another goroutine.\n\t\t\t// No race now, it is ok to recycle. Or recvMsg recycle depend on recv err\n\t\t\tremote.RecycleMessage(recvMsg)\n\t\t}()\n\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\tmethodName := ri.Invocation().MethodName()\n\n\t\tcli, err := remotecli.NewClient(ctx, ri, transPipl, kc.opt.RemoteOpt)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\n\t\tdefer cli.Recycle()\n\t\tconfig := ri.Config()\n\t\tm := kc.svcInfo.MethodInfo(methodName)\n\t\tif m == nil {\n\t\t\treturn fmt.Errorf(\"method info is nil, methodName=%s, serviceInfo=%+v\", methodName, kc.svcInfo)\n\t\t} else if m.OneWay() {\n\t\t\tsendMsg = remote.NewMessage(req, kc.svcInfo, ri, remote.Oneway, remote.Client)\n\t\t} else {\n\t\t\tsendMsg = remote.NewMessage(req, kc.svcInfo, ri, remote.Call, remote.Client)\n\t\t}\n\t\tprotocolInfo := remote.NewProtocolInfo(config.TransportProtocol(), kc.svcInfo.PayloadCodec)\n\t\tsendMsg.SetProtocolInfo(protocolInfo)\n\n\t\tif err = cli.Send(ctx, ri, sendMsg); err != nil {\n\t\t\treturn\n\t\t}\n\t\tif m.OneWay() {\n\t\t\tcli.Recv(ctx, ri, nil)\n\t\t\treturn nil\n\t\t}\n\n\t\trecvMsg = remote.NewMessage(resp, kc.opt.RemoteOpt.SvcInfo, ri, remote.Reply, remote.Client)\n\t\trecvMsg.SetProtocolInfo(protocolInfo)\n\t\terr = cli.Recv(ctx, ri, recvMsg)\n\t\treturn err\n\t}, nil\n}\n\n// Close is not concurrency safe.\nfunc (kc *kClient) Close() error {\n\tdefer func() {\n\t\tif err := recover(); err != nil {\n\t\t\tklog.Warnf(\"KITEX: panic when close client, error=%s, stack=%s\", err, string(debug.Stack()))\n\t\t}\n\t}()\n\tif kc.closed {\n\t\treturn nil\n\t}\n\tkc.closed = true\n\tvar errs utils.ErrChain\n\tfor _, cb := range kc.opt.CloseCallbacks {\n\t\tif err := cb(); err != nil {\n\t\t\terrs.Append(err)\n\t\t}\n\t}\n\tif kc.opt.CBSuite != nil {\n\t\tif err := kc.opt.CBSuite.Close(); err != nil {\n\t\t\terrs.Append(err)\n\t\t}\n\t}\n\tif errs.HasError() {\n\t\treturn errs\n\t}\n\treturn nil\n}\n\nfunc newCliTransHandler(opt *remote.ClientOption) (remote.ClientTransHandler, error) {\n\thandler, err := opt.CliHandlerFactory.NewTransHandler(opt)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ttransPl := remote.NewTransPipeline(handler)\n\tfor _, ib := range opt.Inbounds {\n\t\ttransPl.AddInboundHandler(ib)\n\t}\n\tfor _, ob := range opt.Outbounds {\n\t\ttransPl.AddOutboundHandler(ob)\n\t}\n\treturn transPl, nil\n}\n\nfunc initTransportProtocol(svcInfo *serviceinfo.ServiceInfo, cfg rpcinfo.RPCConfig) {\n\tmutableRPCConfig := rpcinfo.AsMutableRPCConfig(cfg)\n\tmutableRPCConfig.SetPayloadCodec(svcInfo.PayloadCodec)\n\tif svcInfo.PayloadCodec == serviceinfo.Protobuf && cfg.TransportProtocol()&transport.GRPC != transport.GRPC {\n\t\t// pb use ttheader framed by default\n\t\tmutableRPCConfig.SetTransportProtocol(transport.TTHeaderFramed)\n\t}\n}\n\nfunc (kc *kClient) warmingUp() error {\n\tif kc.opt.WarmUpOption == nil {\n\t\treturn nil\n\t}\n\twuo := kc.opt.WarmUpOption\n\tdoWarmupPool := wuo.PoolOption != nil && kc.opt.Proxy == nil\n\n\t// service discovery\n\tif kc.opt.Resolver == nil {\n\t\treturn nil\n\t}\n\tnas := make(map[string][]string)\n\tctx := context.Background()\n\n\tvar dests []rpcinfo.EndpointInfo\n\tif ro := kc.opt.WarmUpOption.ResolverOption; ro != nil {\n\t\tfor _, d := range ro.Dests {\n\t\t\tdests = append(dests, rpcinfo.FromBasicInfo(d))\n\t\t}\n\t}\n\tif len(dests) == 0 && doWarmupPool && len(wuo.PoolOption.Targets) == 0 {\n\t\t// build a default destination for the resolver\n\t\tcfg :=\n```\n# Function Signature: func NewClient(svcInfo *serviceinfo.ServiceInfo, opts ...Option) (Client, error) {\n# Natural Language Annotations: // NewClient creates a kitex.Client with the given ServiceInfo, it is from generated code.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewClient(svcInfo *serviceinfo.ServiceInfo, opts ...Option) (Client, error) {\n\topt := client.DefaultOption()\n\tfor _, o := range opts {\n\t\to.Apply(opt)\n\t}\n\tkc := &kClient{\n\t\tsvcInfo: svcInfo,\n\t\topt:     opt,\n\t}\n\tkf := &kcFinalizerClient{kClient: kc}\n\truntime.SetFinalizer(kf, func(kf *kcFinalizerClient) {\n\t\tkf.close()\n\t})\n\tif err := kc.init(); err != nil {\n\t\treturn nil, err\n\t}\n\treturn kf, nil\n}\n```", "```go\nfunc NewClient(svcInfo *serviceinfo.ServiceInfo, opts ...Option) (Client, error) {\n\topt := new(client.Options)\n\topt.Svr = rpcinfo.NewEndpointInfo(svcInfo.ServiceName, rpcinfo.TCP, svcInfo.Addr)\n\topt.TracerCtl = &trace.GlobalTracerController{}\n\topt.Streaming = new(remote.StreamingConfig)\n\topt.RemoteOpt = remote.NewDefaultClientOption(svcInfo, opt.Streaming)\n\topt.RemoteOpt.Locks = new(rpcinfo.Locks)\n\topt.CtxBackupHandler = backup.NewCtxBackupHandler()\n\n\tfor _, o := range opts {\n\t\to(opt)\n\t}\n\n\tkc := &kClient{\n\t\tsvcInfo: svcInfo,\n\t\topt:     opt,\n\t}\n\n\tif err := kc.init(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &kcFinalizerClient{kClient: kc}, nil\n}\n```", "```go\nfunc NewClient(svcInfo *serviceinfo.ServiceInfo, opts ...Option) (Client, error) {\n\topt := new(client.Options)\n\tfor _, v := range opts {\n\t\tv(opt)\n\t}\n\tkc := &kcFinalizerClient{\n\t\tkClient: &kClient{\n\t\t\tsvcInfo: svcInfo,\n\t\t\topt:     opt,\n\t\t},\n\t}\n\tif err := kc.init(); err != nil {\n\t\treturn nil, err\n\t}\n\truntime.SetFinalizer(kc, func(kc *kcFinalizerClient) {\n\t\tkc.Close()\n\t\tkc.Close() // double close for Close is idempotent\n\t})\n\treturn kc, nil\n}\n```"]}, "kitex-client/middlewares.go-newResolveMWBuilder": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/middlewares.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\n\t\"github.com/cloudwego/kitex/internal\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/proxy\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nconst maxRetry = 6\n\nfunc newProxyMW(prx proxy.ForwardProxy) endpoint.Middleware {\n\t// If you want to customize the processing logic of proxy middleware,\n\t// you can implement this interface to replace the default implementation.\n\tif p, ok := prx.(proxy.WithMiddleware); ok {\n\t\treturn p.ProxyMiddleware()\n\t}\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\terr := prx.ResolveProxyInstance(ctx)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\terr = next(ctx, request, response)\n\t\t\treturn err\n\t\t}\n\t}\n}\n\nfunc discoveryEventHandler(name string, bus event.Bus, queue event.Queue) func(d *discovery.Change) {\n\treturn func(d *discovery.Change) {\n\t\tnow := time.Now()\n\t\tbus.Dispatch(&event.Event{\n\t\t\tName:  name,\n\t\t\tTime:  now,\n\t\t\tExtra: d,\n\t\t})\n\t\tqueue.Push(&event.Event{\n\t\t\tName: name,\n\t\t\tTime: now,\n\t\t\tExtra: map[string]interface{}{\n\t\t\t\t\"Added\":   wrapInstances(d.Added),\n\t\t\t\t\"Updated\": wrapInstances(d.Updated),\n\t\t\t\t\"Removed\": wrapInstances(d.Removed),\n\t\t\t},\n\t\t})\n\t}\n}\n\n// newResolveMWBuilder creates a middleware for service discovery.\n// This middleware selects an appropriate instance based on the resolver and loadbalancer given.\n// If retryable error is encountered, it will retry until timeout or an unretryable error is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// newIOErrorHandleMW provides a hook point for io error handling.\nfunc newIOErrorHandleMW(errHandle func(context.Context, error) error) endpoint.Middleware {\n\tif errHandle == nil {\n\t\terrHandle = DefaultClientErrorHandler\n\t}\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) (err error) {\n\t\t\terr = next(ctx, request, response)\n\t\t\tif err == nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\treturn errHandle(ctx, err)\n\t\t}\n\t}\n}\n\n// DefaultClientErrorHandler is Default ErrorHandler for client\n// when no ErrorHandler is specified with Option `client.WithErrorHandler`, this ErrorHandler will be injected.\n// for thrift\u3001KitexProtobuf, >= v0.4.0 wrap protocol error to TransError, which will be more friendly.\n\n\n\n\n\n\n\n\n\n\n\n// ClientErrorHandlerWithAddr is ErrorHandler for client, which will add remote addr info into error\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype instInfo struct {\n\tAddress string\n\tWeight  int\n}\n\nfunc wrapInstances(insts []discovery.Instance) []*instInfo {\n\tif len(insts) == 0 {\n\t\treturn nil\n\t}\n\tinstInfos := make([]*instInfo, 0, len(insts))\n\tfor i := range insts {\n\t\tinst := insts[i]\n\t\taddr := fmt.Sprintf(\"%s://%s\", inst.Address().Network(), inst.Address().String())\n\t\tinstInfos = append(instInfos, &instInfo{Address: addr, Weight: inst.Weight()})\n\t}\n\treturn instInfos\n}\n\nfunc retryable(err error) bool {\n\treturn errors.Is(err, kerrors.ErrGetConnection) || errors.Is(err, kerrors.ErrCircuitBreak)\n}\n\nfunc getRemoteAddr(ctx context.Context) string {\n\tif ri := rpcinfo.GetRPCInfo(ctx); ri != nil && ri.To() != nil && ri.To().Address() != nil {\n\t\treturn ri.To().Address().String()\n\t}\n\treturn \"\"\n}\n\n```\n# Function Signature: func newResolveMWBuilder(lbf *lbcache.BalancerFactory) endpoint.MiddlewareBuilder {\n# Natural Language Annotations: // newResolveMWBuilder creates a middleware for service discovery.\n// This middleware selects an appropriate instance based on the resolver and loadbalancer given.\n// If retryable error is encountered, it will retry until timeout or an unretryable error is returned.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc newResolveMWBuilder(lbf *lbcache.BalancerFactory) endpoint.MiddlewareBuilder {\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\t\tif ri == nil {\n\t\t\t\treturn errors.New(\"rpcinfo is nil\")\n\t\t\t}\n\n\t\t\tresolver := ri.Resolver()\n\t\t\tif resolver == nil {\n\t\t\t\treturn errors.New(\"resolver is nil\")\n\t\t\t}\n\n\t\t\tins, err := resolver.Resolve(ri.ServiceName())\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\tif len(ins) == 0 {\n\t\t\t\treturn kerrors.ErrNoInstance\n\t\t\t}\n\n\t\t\tlb := lbf.Get(ri)\n\t\t\tif lb == nil {\n\t\t\t\treturn kerrors.ErrNoLoadbalance\n\t\t\t}\n\n\t\t\tvar instance discovery.Instance\n\t\t\tfor i := 1; i <= maxRetry; i++ {\n\t\t\t\tinstance = lb.Choose(ins)\n\t\t\t\terr = instance.Ping(ctx)\n\t\t\t\tif err == nil || !retryable(err) {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tklog.Warn(\"retry instance ping\", \"instance\", instance.Address(), \"error\", err, \"retry\", i)\n\t\t\t}\n\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\trctx := remoteinfo.NewRemoteInfo(ctx, instance.Address())\n\t\t\terr = next(rctx, request, response)\n\t\t\tif err == nil {\n\t\t\t\treturn nil\n\t\t\t}\n\n\t\t\tif errors.Is(err, kerrors.ErrRPCTimeout) {\n\t\t\t\treturn err\n\t\t\t}\n\n\t\t\treturn err\n\t\t}\n\t}\n}\n```", "Here's the generated code for the `newResolveMWBuilder` function:\n\n```go\nfunc newResolveMWBuilder(lbf *lbcache.BalancerFactory) endpoint.MiddlewareBuilder {\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\t\tresolver := ri.Resolver()\n\t\t\tif resolver == nil {\n\t\t\t\treturn next(ctx, request, response)\n\t\t\t}\n\n\t\t\tlb := lbf.GetLoadBalancer(ri)\n\n\t\t\t// Retry until timeout or unretryable error is returned\n\t\t\tvar err error\n\t\t\tfor i := 0; i < maxRetry; i++ {\n\t\t\t\tinstance, err := lb.Select(ctx)\n\t\t\t\tif err != nil {\n\t\t\t\t\tif !retryable(err) {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t\ttime.Sleep(time.Second)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tremoteInfo := ri.Clone()\n\t\t\t\tremoteInfo.SetInvocation(ri.Invocation().SetInstance(instance))\n\t\t\t\tctx = rpcinfo.WithRPCInfo(ctx, remoteInfo)\n\n\t\t\t\terr = next(ctx, request, response)\n\t\t\t\tif err == nil {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\t\t\t\tif !retryable(err) {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\ttime.Sleep(time.Second)\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\t}\n}\n```\nPlease note that this function depends on the `lbcache.BalancerFactory` and `retryable` function which are not included in the provided code. You may need to provide these functions or replace them with your own.", "```go\nfunc newResolveMWBuilder(lbf *lbcache.BalancerFactory) endpoint.MiddlewareBuilder {\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\tri := rpcinfo.GetRPCInfo(ctx)\n\t\t\tif ri == nil {\n\t\t\t\treturn next(ctx, request, response)\n\t\t\t}\n\t\t\tresolver := ri.Resolver()\n\t\t\tif resolver == nil {\n\t\t\t\treturn next(ctx, request, response)\n\t\t\t}\n\t\t\tlb := lbf.GetLoadbalancer(ri)\n\t\t\tif lb == nil {\n\t\t\t\treturn next(ctx, request, response)\n\t\t\t}\n\t\t\tvar ins discovery.Instance\n\t\t\tvar err error\n\t\t\tfor i := 0; i < maxRetry; i++ {\n\t\t\t\tins, err = lb.Next(ctx)\n\t\t\t\tif err != nil {\n\t\t\t\t\tif !retryable(err) {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t\tklog.Warnf(\"Retryable error occurred: %v\", err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\terr = resolver.Resolve(ins.Address().String())\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tremoteAddr := ins.Address().String()\n\t\t\tctx = rpcinfo.WithRemoteAddr(ctx, remoteAddr)\n\t\t\tctx = remoteinfo.NewRemoteInfo(ctx, ins)\n\t\t\treturn next(ctx, request, response)\n\t\t}\n\t}\n}\n```\nThis function creates a middleware that handles service discovery. It uses the provided load balancer factory to create a load balancer, and uses this load balancer to select an appropriate instance. If a retryable error is encountered, it will retry for a maximum of `maxRetry` times. The selected instance and its address are then stored in the context for use in subsequent middleware and the endpoint."]}, "kitex-client/middlewares.go-DefaultClientErrorHandler": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/middlewares.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\n\t\"github.com/cloudwego/kitex/internal\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/proxy\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nconst maxRetry = 6\n\nfunc newProxyMW(prx proxy.ForwardProxy) endpoint.Middleware {\n\t// If you want to customize the processing logic of proxy middleware,\n\t// you can implement this interface to replace the default implementation.\n\tif p, ok := prx.(proxy.WithMiddleware); ok {\n\t\treturn p.ProxyMiddleware()\n\t}\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\terr := prx.ResolveProxyInstance(ctx)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\terr = next(ctx, request, response)\n\t\t\treturn err\n\t\t}\n\t}\n}\n\nfunc discoveryEventHandler(name string, bus event.Bus, queue event.Queue) func(d *discovery.Change) {\n\treturn func(d *discovery.Change) {\n\t\tnow := time.Now()\n\t\tbus.Dispatch(&event.Event{\n\t\t\tName:  name,\n\t\t\tTime:  now,\n\t\t\tExtra: d,\n\t\t})\n\t\tqueue.Push(&event.Event{\n\t\t\tName: name,\n\t\t\tTime: now,\n\t\t\tExtra: map[string]interface{}{\n\t\t\t\t\"Added\":   wrapInstances(d.Added),\n\t\t\t\t\"Updated\": wrapInstances(d.Updated),\n\t\t\t\t\"Removed\": wrapInstances(d.Removed),\n\t\t\t},\n\t\t})\n\t}\n}\n\n// newResolveMWBuilder creates a middleware for service discovery.\n// This middleware selects an appropriate instance based on the resolver and loadbalancer given.\n// If retryable error is encountered, it will retry until timeout or an unretryable error is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// newIOErrorHandleMW provides a hook point for io error handling.\nfunc newIOErrorHandleMW(errHandle func(context.Context, error) error) endpoint.Middleware {\n\tif errHandle == nil {\n\t\terrHandle = DefaultClientErrorHandler\n\t}\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) (err error) {\n\t\t\terr = next(ctx, request, response)\n\t\t\tif err == nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\treturn errHandle(ctx, err)\n\t\t}\n\t}\n}\n\n// DefaultClientErrorHandler is Default ErrorHandler for client\n// when no ErrorHandler is specified with Option `client.WithErrorHandler`, this ErrorHandler will be injected.\n// for thrift\u3001KitexProtobuf, >= v0.4.0 wrap protocol error to TransError, which will be more friendly.\n\n\n\n\n\n\n\n\n\n\n\n// ClientErrorHandlerWithAddr is ErrorHandler for client, which will add remote addr info into error\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype instInfo struct {\n\tAddress string\n\tWeight  int\n}\n\nfunc wrapInstances(insts []discovery.Instance) []*instInfo {\n\tif len(insts) == 0 {\n\t\treturn nil\n\t}\n\tinstInfos := make([]*instInfo, 0, len(insts))\n\tfor i := range insts {\n\t\tinst := insts[i]\n\t\taddr := fmt.Sprintf(\"%s://%s\", inst.Address().Network(), inst.Address().String())\n\t\tinstInfos = append(instInfos, &instInfo{Address: addr, Weight: inst.Weight()})\n\t}\n\treturn instInfos\n}\n\nfunc retryable(err error) bool {\n\treturn errors.Is(err, kerrors.ErrGetConnection) || errors.Is(err, kerrors.ErrCircuitBreak)\n}\n\nfunc getRemoteAddr(ctx context.Context) string {\n\tif ri := rpcinfo.GetRPCInfo(ctx); ri != nil && ri.To() != nil && ri.To().Address() != nil {\n\t\treturn ri.To().Address().String()\n\t}\n\treturn \"\"\n}\n\n```\n# Function Signature: func DefaultClientErrorHandler(ctx context.Context, err error) error {\n# Natural Language Annotations: // DefaultClientErrorHandler is Default ErrorHandler for client\n// when no ErrorHandler is specified with Option `client.WithErrorHandler`, this ErrorHandler will be injected.\n// for thrift\u3001KitexProtobuf, >= v0.4.0 wrap protocol error to TransError, which will be more friendly.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc DefaultClientErrorHandler(ctx context.Context, err error) error {\n\t// DefaultClientErrorHandler is Default ErrorHandler for client\n\t// when no ErrorHandler is specified with Option `client.WithErrorHandler`, this ErrorHandler will be injected.\n\t// for thrift\u3001KitexProtobuf, >= v0.4.0 wrap protocol error to TransError, which will be more friendly.\n\n\t// Replace the following implementation with your own logic\n\tklog.Errorf(\"DefaultClientErrorHandler: %v\", err)\n\treturn err\n}\n```\nPlease replace the implementation with your own logic.", "```go\nfunc DefaultClientErrorHandler(ctx context.Context, err error) error {\n\t// DefaultClientErrorHandler is Default ErrorHandler for client\n\t// when no ErrorHandler is specified with Option `client.WithErrorHandler`, this ErrorHandler will be injected.\n\t// for thrift\u3001KitexProtobuf, >= v0.4.0 wrap protocol error to TransError, which will be more friendly.\n\n\t// Here is the generated code:\n\n\t// DefaultClientErrorHandler is the default error handler for client.\n\t// If no error handler is specified with Option `client.WithErrorHandler`, this error handler will be injected.\n\t// For thrift and KitexProtobuf versions >= v0.4.0, it wraps protocol errors into TransError, making the error more user-friendly.\n\t// It checks the error type and returns a new error with a more friendly error message.\n\n\t// Check if the error is a thrift protocol error\n\tif _, ok := err.(thrift.TProtocolException); ok {\n\t\t// Wrap the error into a TransError\n\t\terr = kerrors.NewTransError(err)\n\t}\n\n\t// Check if the error is a KitexProtobuf protocol error\n\tif _, ok := err.(protobuf.ProtobufCodecError); ok {\n\t\t// Wrap the error into a TransError\n\t\terr = kerrors.NewTransError(err)\n\t}\n\n\t// Return the error\n\treturn err\n}\n```", "```go\n// DefaultClientErrorHandler is Default ErrorHandler for client\n// when no ErrorHandler is specified with Option `client.WithErrorHandler`, this ErrorHandler will be injected.\n// for thrift\u3001KitexProtobuf, >= v0.4.0 wrap protocol error to TransError, which will be more friendly.\nfunc DefaultClientErrorHandler(ctx context.Context, err error) error {\n\t// Your implementation here\n\treturn err\n}\n```"]}, "kitex-client/middlewares.go-ClientErrorHandlerWithAddr": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/middlewares.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/apache/thrift/lib/go/thrift\"\n\n\t\"github.com/cloudwego/kitex/internal\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/event\"\n\t\"github.com/cloudwego/kitex/pkg/kerrors\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/proxy\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nconst maxRetry = 6\n\nfunc newProxyMW(prx proxy.ForwardProxy) endpoint.Middleware {\n\t// If you want to customize the processing logic of proxy middleware,\n\t// you can implement this interface to replace the default implementation.\n\tif p, ok := prx.(proxy.WithMiddleware); ok {\n\t\treturn p.ProxyMiddleware()\n\t}\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) error {\n\t\t\terr := prx.ResolveProxyInstance(ctx)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\terr = next(ctx, request, response)\n\t\t\treturn err\n\t\t}\n\t}\n}\n\nfunc discoveryEventHandler(name string, bus event.Bus, queue event.Queue) func(d *discovery.Change) {\n\treturn func(d *discovery.Change) {\n\t\tnow := time.Now()\n\t\tbus.Dispatch(&event.Event{\n\t\t\tName:  name,\n\t\t\tTime:  now,\n\t\t\tExtra: d,\n\t\t})\n\t\tqueue.Push(&event.Event{\n\t\t\tName: name,\n\t\t\tTime: now,\n\t\t\tExtra: map[string]interface{}{\n\t\t\t\t\"Added\":   wrapInstances(d.Added),\n\t\t\t\t\"Updated\": wrapInstances(d.Updated),\n\t\t\t\t\"Removed\": wrapInstances(d.Removed),\n\t\t\t},\n\t\t})\n\t}\n}\n\n// newResolveMWBuilder creates a middleware for service discovery.\n// This middleware selects an appropriate instance based on the resolver and loadbalancer given.\n// If retryable error is encountered, it will retry until timeout or an unretryable error is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// newIOErrorHandleMW provides a hook point for io error handling.\nfunc newIOErrorHandleMW(errHandle func(context.Context, error) error) endpoint.Middleware {\n\tif errHandle == nil {\n\t\terrHandle = DefaultClientErrorHandler\n\t}\n\treturn func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\treturn func(ctx context.Context, request, response interface{}) (err error) {\n\t\t\terr = next(ctx, request, response)\n\t\t\tif err == nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\treturn errHandle(ctx, err)\n\t\t}\n\t}\n}\n\n// DefaultClientErrorHandler is Default ErrorHandler for client\n// when no ErrorHandler is specified with Option `client.WithErrorHandler`, this ErrorHandler will be injected.\n// for thrift\u3001KitexProtobuf, >= v0.4.0 wrap protocol error to TransError, which will be more friendly.\n\n\n\n\n\n\n\n\n\n\n\n// ClientErrorHandlerWithAddr is ErrorHandler for client, which will add remote addr info into error\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype instInfo struct {\n\tAddress string\n\tWeight  int\n}\n\nfunc wrapInstances(insts []discovery.Instance) []*instInfo {\n\tif len(insts) == 0 {\n\t\treturn nil\n\t}\n\tinstInfos := make([]*instInfo, 0, len(insts))\n\tfor i := range insts {\n\t\tinst := insts[i]\n\t\taddr := fmt.Sprintf(\"%s://%s\", inst.Address().Network(), inst.Address().String())\n\t\tinstInfos = append(instInfos, &instInfo{Address: addr, Weight: inst.Weight()})\n\t}\n\treturn instInfos\n}\n\nfunc retryable(err error) bool {\n\treturn errors.Is(err, kerrors.ErrGetConnection) || errors.Is(err, kerrors.ErrCircuitBreak)\n}\n\nfunc getRemoteAddr(ctx context.Context) string {\n\tif ri := rpcinfo.GetRPCInfo(ctx); ri != nil && ri.To() != nil && ri.To().Address() != nil {\n\t\treturn ri.To().Address().String()\n\t}\n\treturn \"\"\n}\n\n```\n# Function Signature: func ClientErrorHandlerWithAddr(ctx context.Context, err error) error {\n# Natural Language Annotations: // ClientErrorHandlerWithAddr is ErrorHandler for client, which will add remote addr info into error\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// ClientErrorHandlerWithAddr is ErrorHandler for client, which will add remote addr info into error\nfunc ClientErrorHandlerWithAddr(ctx context.Context, err error) error {\n\tremoteAddr := getRemoteAddr(ctx)\n\treturn fmt.Errorf(\"ClientErrorHandlerWithAddr: %s, remoteAddr: %s\", err, remoteAddr)\n}\n```\nPlease note that the `getRemoteAddr` function is used to fetch the remote address from the context. Ensure that you have defined this function in your codebase before using it.", "```go\n// ClientErrorHandlerWithAddr is ErrorHandler for client, which will add remote addr info into error\nfunc ClientErrorHandlerWithAddr(ctx context.Context, err error) error {\n    remoteAddr := getRemoteAddr(ctx)\n    if remoteAddr == \"\" {\n        return fmt.Errorf(\"remote address not found in context: %w\", err)\n    }\n    return fmt.Errorf(\"remote address %s: %w\", remoteAddr, err)\n}\n```", "```go\nfunc ClientErrorHandlerWithAddr(ctx context.Context, err error) error {\n\tremoteAddr := getRemoteAddr(ctx)\n\tif remoteAddr != \"\" {\n\t\terr = fmt.Errorf(\"%w (remote address: %s)\", err, remoteAddr)\n\t}\n\treturn err\n}\n```"]}, "kitex-client/option.go-WithTransportProtocol": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithTransportProtocol(tp transport.Protocol) Option {\n# Natural Language Annotations: // WithTransportProtocol sets the transport protocol for client.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithTransportProtocol(tp transport.Protocol) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTransportProtocol(%+v)\", tp))\n\t\to.TransHdlr = tp\n\t}}\n}\n```", "```go\nfunc WithTransportProtocol(tp transport.Protocol) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTransportProtocol(%+v)\", tp))\n\t\to.Protocol = tp\n\t}}\n}\n```", "```go\nfunc WithTransportProtocol(tp transport.Protocol) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTransportProtocol(%+v)\", tp))\n\n\t\to.Svr.Protocol = tp\n\t}}\n}\n```"]}, "kitex-client/option.go-WithInstanceMW": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithInstanceMW(mw endpoint.Middleware) Option {\n# Natural Language Annotations: // WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\nfunc WithInstanceMW(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithInstanceMW(%+v)\", utils.GetFuncName(mw)))\n\t\to.InstanceMWBs = append(o.InstanceMWBs, mwb)\n\t}}\n}\n```", "```go\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\nfunc WithInstanceMW(mw endpoint.Middleware) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithInstanceMW(%+v)\", utils.GetFuncName(mw)))\n\t\to.InstanceMWs = append(o.InstanceMWs, mw)\n\t}}\n}\n```", "```go\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\nfunc WithInstanceMW(mw endpoint.Middleware) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithInstanceMW(%+v)\", utils.GetFuncName(mw)))\n\t\to.InstanceMWs = append(o.InstanceMWs, mw)\n\t}}\n}\n```"]}, "kitex-client/option.go-WithHostPorts": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithHostPorts(hostports ...string) Option {\n# Natural Language Annotations: // WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithHostPorts(hostports ...string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithHostPorts(%+v)\", hostports))\n\t\to.Svr.Target = rpcinfo.NewEndpointInfo(\"\", \"\", hostports, nil)\n\t}}\n}\n```", "```go\npackage client\n\nimport (\n\t\"fmt\"\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\nfunc WithHostPorts(hostports ...string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.HostPorts = hostports\n\t\tdi.Push(fmt.Sprintf(\"WithHostPorts(%v)\", hostports))\n\t}}\n}\n```", "```go\nfunc WithHostPorts(hostports ...string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHostPorts(%s)\", strings.Join(hostports, \",\")))\n\t\to.Svr.Resolver = func() (discovery.Resolver, error) {\n\t\t\tvar addrs []net.Addr\n\t\t\tfor _, hostport := range hostports {\n\t\t\t\taddrs = append(addrs, utils.NewNetAddr(\"tcp\", hostport))\n\t\t\t}\n\t\t\treturn discovery.NewFixedResolver(addrs)\n\t\t}\n\t}}\n}\n```"]}, "kitex-client/option.go-WithResolver": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithResolver(r discovery.Resolver) Option {\n# Natural Language Annotations: // WithResolver provides the Resolver for kitex client.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithResolver provides the Resolver for kitex client.\nfunc WithResolver(r discovery.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithResolver(%T)\", r))\n\n\t\to.Resolver = r\n\t}}\n}\n```", "```go\nfunc WithResolver(r discovery.Resolver) Option {\n\treturn Option{\n\t\tF: func(o *client.Options, di *utils.Slice) {\n\t\t\tdi.Push(fmt.Sprintf(\"WithResolver(%T)\", r))\n\t\t\to.Resolver = r\n\t\t},\n\t}\n}\n```", "```go\nfunc WithResolver(r discovery.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithResolver(%T)\", r))\n\n\t\to.Resolver = r\n\t}}\n}\n```"]}, "kitex-client/option.go-WithShortConnection": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithShortConnection() Option {\n# Natural Language Annotations: // WithShortConnection forces kitex to close connection after each call is finished.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithShortConnection() Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(\"WithShortConnection()\")\n\t\to.ShortConnection = true\n\t}}\n}\n```\n\nThis function is a type of `Option` which is used to modify the behavior of the client. In this case, it is setting the `ShortConnection` field of `client.Options` to `true`, which tells the client to close the connection after each call is finished.", "```go\n// WithShortConnection forces kitex to close connection after each call is finished.\nfunc WithShortConnection() Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(\"WithShortConnection()\")\n\t\to.TransportProtocol = \"short\"\n\t}}\n}\n```\n\nPlease note that this code assumes that the `client.Options` struct has a field called `TransportProtocol` which is used to configure the transport protocol for a client. This field is not explicitly mentioned in the provided code, but is likely to be used to set the transport protocol. If the actual field name is different, you will need to replace `TransportProtocol` with the correct field name.", "```go\nfunc WithShortConnection() Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(\"WithShortConnection()\")\n\t\to.ShortConn = true\n\t}}\n}\n```"]}, "kitex-client/option.go-WithLongConnection": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithLongConnection(cfg connpool.IdleConfig) Option {\n# Natural Language Annotations: // WithLongConnection enables long connection with kitex's built-in pooling implementation.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithLongConnection(cfg connpool.IdleConfig) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLongConnection(%+v)\", cfg))\n\n\t\to.LongConnPool = &cfg\n\t}}\n}\n```", "```go\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\nfunc WithLongConnection(cfg connpool.IdleConfig) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLongConnection(%+v)\", cfg))\n\n\t\to.Pool = connpool.NewLongPool(cfg)\n\t}}\n}\n```", "```go\nfunc WithLongConnection(cfg connpool.IdleConfig) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLongConnection(%+v)\", cfg))\n\n\t\to.LongConn = cfg\n\t}}\n}\n```"]}, "kitex-client/option.go-WithMuxConnection": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithMuxConnection(connNum int) Option {\n# Natural Language Annotations: // WithMuxConnection specifies the transport type to be mux.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithMuxConnection(connNum int) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMuxConnection(%d)\", connNum))\n\t\to.MuxConnection = connNum\n\t}}\n}\n```", "```go\nfunc WithMuxConnection(connNum int) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMuxConnection(%d)\", connNum))\n\t\to.MuxConnection = connNum\n\t}}\n}\n```", "```go\nfunc WithMuxConnection(connNum int) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMuxConnection(%d)\", connNum))\n\n\t\to.MuxConnection = connNum\n\t}}\n}\n```"]}, "kitex-client/option.go-WithTimeoutProvider": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithTimeoutProvider(p rpcinfo.TimeoutProvider) Option {\n# Natural Language Annotations: // WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithTimeoutProvider(p rpcinfo.TimeoutProvider) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTimeoutProvider(%T{%+v})\", p, p))\n\n\t\tif o.TimeoutProvider == nil {\n\t\t\to.TimeoutProvider = &p\n\t\t}\n\t}}\n}\n```", "```go\nfunc WithTimeoutProvider(p rpcinfo.TimeoutProvider) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTimeoutProvider(%T{%+v})\", p, p))\n\n\t\tif o.TimeoutProviderCtl == nil {\n\t\t\to.TimeoutProviderCtl = &rpcinfo.TimeoutController{}\n\t\t}\n\t\to.TimeoutProviderCtl.Append(p)\n\t}}\n}\n```", "```go\nfunc WithTimeoutProvider(p rpcinfo.TimeoutProvider) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTimeoutProvider(%T{%+v})\", p, p))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n```"]}, "kitex-client/option.go-WithCodec": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithCodec(c remote.Codec) Option {\n# Natural Language Annotations: // WithCodec to set a codec that handle other protocols which not support by kitex\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%T{%+v})\", c, c))\n\n\t\to.Codec = c\n\t}}\n}\n```", "```go\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%T{%+v})\", c, c))\n\t\to.Codec = c\n\t}}\n}\n```", "```go\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%T)\", c))\n\n\t\to.Codec = c\n\t}}\n}\n```"]}, "kitex-client/option.go-WithPayloadCodec": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithPayloadCodec(c remote.PayloadCodec) Option {\n# Natural Language Annotations: // WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithPayloadCodec(c remote.PayloadCodec) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithPayloadCodec(%T{%+v})\", c, c))\n\n\t\to.PayloadCodec = c\n\t}}\n}\n```", "```go\nfunc WithPayloadCodec(c remote.PayloadCodec) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithPayloadCodec(%T{%+v})\", c, c))\n\n\t\to.PayloadCodec = c\n\t}}\n}\n```", "```go\nfunc WithPayloadCodec(c remote.PayloadCodec) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithPayloadCodec(%T{%+v})\", c, c))\n\n\t\to.PayloadCodec = c\n\t}}\n}\n```"]}, "kitex-client/option.go-WithConnReporterEnabled": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithConnReporterEnabled() Option {\n# Natural Language Annotations: // WithConnReporterEnabled to enable reporting connection pool stats.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithConnReporterEnabled() Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnReporterEnabled()\"))\n\t\to.ConnReporterEnabled = true\n\t}}\n}\n```", "```go\nfunc WithConnReporterEnabled() Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(\"WithConnReporterEnabled\")\n\t\to.ConnReporterEnabled = true\n\t}}\n}\n```", "```go\n// WithConnReporterEnabled to enable reporting connection pool stats.\nfunc WithConnReporterEnabled() Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnReporterEnabled()\"))\n\t\to.ConnReporterEnabled = true\n\t}}\n}\n```"]}, "kitex-client/option.go-WithFailureRetry": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithFailureRetry(p *retry.FailurePolicy) Option {\n# Natural Language Annotations: // WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithFailureRetry(p *retry.FailurePolicy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithFailureRetry(%+v)\", p))\n\t\to.FailPolicy = p\n\t}}\n}\n```", "```go\nfunc WithFailureRetry(p *retry.FailurePolicy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithFailureRetry(%+v)\", p))\n\t\tif p == nil {\n\t\t\treturn\n\t\t}\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\to.RetryMethodPolicies[p.Method] = p\n\t}}\n}\n```", "```go\npackage client\n\nimport (\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"fmt\"\n)\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\nfunc WithFailureRetry(p *retry.FailurePolicy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithFailureRetry(%+v)\", p))\n\t\to.FailureRetry = p\n\t}}\n}\n```"]}, "kitex-client/option.go-WithBackupRequest": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithBackupRequest(p *retry.BackupPolicy) Option {\n# Natural Language Annotations: // WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage client\n\nimport (\n\t\"fmt\"\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\nfunc WithBackupRequest(p *retry.BackupPolicy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithBackupRequest(%+v)\", p))\n\t\to.BackupRequestPolicy = p\n\t}}\n}\n```", "```go\nfunc WithBackupRequest(p *retry.BackupPolicy) Option {\n    return Option{F: func(o *client.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithBackupRequest(%+v)\", p))\n\n        o.BackupRequest = p\n    }}\n}\n```", "```go\nfunc WithBackupRequest(p *retry.BackupPolicy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithBackupRequest(%+v)\", p))\n\t\to.BackupRequest = p\n\t}}\n}\n```"]}, "kitex-client/option.go-WithSpecifiedResultRetry": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithSpecifiedResultRetry(rr *retry.ShouldResultRetry) Option {\n# Natural Language Annotations: // WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithSpecifiedResultRetry(rr *retry.ShouldResultRetry) Option {\n    return Option{F: func(o *client.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithSpecifiedResultRetry(%+v)\", rr))\n        o.SpecifiedResultRetry = rr\n    }}\n}\n```", "```go\nfunc WithSpecifiedResultRetry(rr *retry.ShouldResultRetry) Option {\n    return Option{\n        F: func(o *client.Options, di *utils.Slice) {\n            di.Push(fmt.Sprintf(\"WithSpecifiedResultRetry(%+v)\", rr))\n            if o.RetryPolicy == nil {\n                o.RetryPolicy = &retry.FailurePolicy{}\n            }\n            o.RetryPolicy.ShouldResultRetry = rr\n        },\n    }\n}\n```", "```go\nfunc WithSpecifiedResultRetry(rr *retry.ShouldResultRetry) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithSpecifiedResultRetry(%+v)\", rr))\n\t\to.SpecifiedResultRetry = rr\n\t}}\n}\n```"]}, "kitex-client/option.go-WithCircuitBreaker": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithCircuitBreaker(s *circuitbreak.CBSuite) Option {\n# Natural Language Annotations: // WithCircuitBreaker adds a circuitbreaker suite for the client.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithCircuitBreaker(s *circuitbreak.CBSuite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCircuitBreaker(%+v)\", s))\n\t\to.CBSuite = s\n\t}}\n}\n```", "```go\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\nfunc WithCircuitBreaker(s *circuitbreak.CBSuite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCircuitBreaker(%+v)\", s))\n\t\to.CircuitBreaker = s\n\t}}\n}\n```", "```go\nfunc WithCircuitBreaker(s *circuitbreak.CBSuite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCircuitBreaker(%+v)\", s))\n\t\to.CBSuite = s\n\t}}\n}\n```"]}, "kitex-client/option.go-WithGRPCConnPoolSize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCConnPoolSize(s uint32) Option {\n# Natural Language Annotations: // WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithGRPCConnPoolSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCConnPoolSize(%d)\", s))\n\t\to.GRPCConnectOpts.ConnPoolSize = s\n\t}}\n}\n```\n\nIn this code, `WithGRPCConnPoolSize` is a function that takes an unsigned integer as an argument. It returns an `Option` struct. Inside the struct, there is a function (`F`) that takes a pointer to `client.Options` and a pointer to `utils.Slice` as arguments. This function pushes a formatted string to the `di` slice and sets the `ConnPoolSize` field of `GRPCConnectOpts` in the `o` struct to the provided value.", "```go\nfunc WithGRPCConnPoolSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCConnPoolSize(%d)\", s))\n\t\to.GRPCConnectOpts.ConnPoolSize = s\n\t}}\n}\n```", "```go\nfunc WithGRPCConnPoolSize(s uint32) Option {\n\treturn Option{\n\t\tF: func(o *client.Options, di *utils.Slice) {\n\t\t\tdi.Push(fmt.Sprintf(\"WithGRPCConnPoolSize(%d)\", s))\n\t\t\to.GRPCConnectOpts.ConnPoolSize = s\n\t\t},\n\t}\n}\n```"]}, "kitex-client/option.go-WithGRPCInitialWindowSize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCInitialWindowSize(s uint32) Option {\n# Natural Language Annotations: // WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithGRPCInitialWindowSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialWindowSize(%d)\", s))\n\t\to.GRPCConnectOpts.InitialWindowSize = s\n\t}}\n}\n```", "```go\nfunc WithGRPCInitialWindowSize(s uint32) Option {\n    return Option{F: func(o *client.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithGRPCInitialWindowSize(%d)\", s))\n        o.GRPCConnectOpts.InitialWindowSize = s\n    }}\n}\n```", "```go\nfunc WithGRPCInitialWindowSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialWindowSize(%d)\", s))\n\t\to.GRPCConnectOpts.InitialWindowSize = s\n\t}}\n}\n```"]}, "kitex-client/option.go-WithGRPCInitialConnWindowSize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCInitialConnWindowSize(s uint32) Option {\n# Natural Language Annotations: // WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithGRPCInitialConnWindowSize(s uint32) Option {\n    return Option{F: func(o *client.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithGRPCInitialConnWindowSize(%d)\", s))\n        o.GRPCConnectOpts.InitialConnWindowSize = s\n    }}\n}\n```", "```go\nfunc WithGRPCInitialConnWindowSize(s uint32) Option {\n    return Option{F: func(o *client.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithGRPCInitialConnWindowSize(%d)\", s))\n        o.GRPCConnectOpts.InitialConnWindowSize = s\n    }}\n}\n```", "```go\nfunc WithGRPCInitialConnWindowSize(s uint32) Option {\n\treturn Option{\n\t\tF: func(o *client.Options, di *utils.Slice) {\n\t\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialConnWindowSize(%d)\", s))\n\t\t\to.GRPCConnectOpts.InitialConnWindowSize = s\n\t\t},\n\t}\n}\n```"]}, "kitex-client/option.go-WithGRPCMaxHeaderListSize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCMaxHeaderListSize(s uint32) Option {\n# Natural Language Annotations: // WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithGRPCMaxHeaderListSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCMaxHeaderListSize(%d)\", s))\n\t\to.GRPCConnectOpts.MaxHeaderListSize = s\n\t}}\n}\n```", "```go\nfunc WithGRPCMaxHeaderListSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCMaxHeaderListSize(%d)\", s))\n\t\to.GRPCConnectOpts.MaxHeaderListSize = s\n\t}}\n}\n```", "```go\n// WithGRPCMaxHeaderListSize returns an Option that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\nfunc WithGRPCMaxHeaderListSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCMaxHeaderListSize(%d)\", s))\n\t\to.GRPCConnectOpts.MaxHeaderListSize = s\n\t}}\n}\n```"]}, "kitex-client/option.go-WithGRPCKeepaliveParams": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCKeepaliveParams(kp grpc.ClientKeepalive) Option {\n# Natural Language Annotations: // WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithGRPCKeepaliveParams(kp grpc.ClientKeepalive) Option {\n    return Option{F: func(o *client.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithGRPCKeepaliveParams(%+v)\", kp))\n        o.GRPCConnectOpts.KeepaliveParams = kp\n    }}\n}\n```", "```go\npackage client\n\nimport (\n\t\"fmt\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n)\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\nfunc WithGRPCKeepaliveParams(kp grpc.ClientKeepalive) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCKeepaliveParams(%+v)\", kp))\n\t\to.GRPCKeepAlive = kp\n\t}}\n}\n```", "```go\nfunc WithGRPCKeepaliveParams(kp grpc.ClientKeepalive) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCKeepaliveParams(%+v)\", kp))\n\t\to.GRPCConnectOpts.KeepaliveParams = kp\n\t}}\n}\n```"]}, "kitex-client/option.go-WithWarmingUp": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithWarmingUp(wuo *warmup.ClientOption) Option {\n# Natural Language Annotations: // WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithWarmingUp(wuo *warmup.ClientOption) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithWarmingUp(%+v)\", wuo))\n\t\to.Warmup = wuo\n\t}}\n}\n```", "```go\nfunc WithWarmingUp(wuo *warmup.ClientOption) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithWarmingUp(%+v)\", wuo))\n\t\to.Warmup = wuo\n\t}}\n}\n```", "```go\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\nfunc WithWarmingUp(wuo *warmup.ClientOption) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithWarmingUp(%+v)\", wuo))\n\t\to.Warmup = wuo\n\t}}\n}\n```"]}, "kitex-client/option.go-WithXDSSuite": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage client\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/circuitbreak\"\n\t\"github.com/cloudwego/kitex/pkg/connpool\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance\"\n\t\"github.com/cloudwego/kitex/pkg/loadbalance/lbcache\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n\t\"github.com/cloudwego/kitex/pkg/warmup\"\n\t\"github.com/cloudwego/kitex/pkg/xds\"\n\t\"github.com/cloudwego/kitex/transport\"\n)\n\n// Option is the only way to config client.\ntype Option = client.Option\n\n// Options is used to initialize a client.\ntype Options = client.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithTransportProtocol sets the transport protocol for client.\n\n\n\n\n\n\n\n\n\n\n\n// WithSuite adds an option suite for client.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMiddleware adds middleware for client to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\tmwb := func(ctx context.Context) endpoint.Middleware {\n\t\treturn mw\n\t}\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%+v)\", utils.GetFuncName(mw)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithMiddlewareBuilder adds middleware that depend on context for client to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v)\", utils.GetFuncName(mwb)))\n\t\to.MWBs = append(o.MWBs, mwb)\n\t}}\n}\n\n// WithInstanceMW adds middleware for client to handle request after service discovery and loadbalance process.\n\n\n\n\n\n\n\n\n\n\n// WithDestService specifies the name of target service.\nfunc WithDestService(svr string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDestService(%s)\", svr))\n\t\to.Svr.ServiceName = svr\n\t}}\n}\n\n// WithHostPorts specifies the target instance addresses when doing service discovery.\n// It overwrites the results from the Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithResolver provides the Resolver for kitex client.\n\n\n\n\n\n\n\n\n// WithHTTPResolver specifies resolver for url (which specified by WithURL).\nfunc WithHTTPResolver(r http.Resolver) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithHTTPResolver(%T)\", r))\n\n\t\to.HTTPResolver = r\n\t}}\n}\n\n// WithShortConnection forces kitex to close connection after each call is finished.\n\n\n\n\n\n\n\n\n// WithLongConnection enables long connection with kitex's built-in pooling implementation.\n\n\n\n\n\n\n\n\n// WithMuxConnection specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n\n// WithLogger sets the Logger for kitex client.\n// Deprecated: client uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"client.WithLogger is deprecated\")\n}\n\n// WithLoadBalancer sets the loadbalancer for client.\nfunc WithLoadBalancer(lb loadbalance.Loadbalancer, opts ...*lbcache.Options) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLoadBalancer(%+v, %+v)\", lb, opts))\n\t\to.Balancer = lb\n\t\tif len(opts) > 0 {\n\t\t\to.BalancerCacheOpt = opts[0]\n\t\t}\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout.\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRPCTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetRPCTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitRPCTimeout\n\t}}\n}\n\n// WithConnectTimeout specifies the connection timeout.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectTimeout(%dms)\", d.Milliseconds()))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetConnectTimeout(d)\n\t\to.Locks.Bits |= rpcinfo.BitConnectTimeout\n\t}}\n}\n\n// WithTimeoutProvider adds a TimeoutProvider to the client.\n// Note that the timeout settings provided by the TimeoutProvider\n// will be applied before the other timeout options in this package\n// and those in the callopt package. Thus it can not modify the\n// timeouts set by WithRPCTimeout or WithConnectTimeout.\n\n\n\n\n\n\n\n// WithTag sets the customize tag for service discovery, eg: idc, cluster.\nfunc WithTag(key, val string) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTag(%s=%s)\", key, val))\n\n\t\to.Svr.Tags[key] = val\n\t\to.Locks.Tags[key] = struct{}{}\n\t}}\n}\n\n// WithTracer adds a tracer to client.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for client.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\n\n\n\n\n\n\n\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n// WithConnReporterEnabled to enable reporting connection pool stats.\n\n\n\n\n\n\n\n\n// WithFailureRetry sets the failure retry policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithBackupRequest sets the backup request policy for client, it will take effect for all methods.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryMethodPolicies sets the retry policy for method.\n// The priority is higher than WithFailureRetry and WithBackupRequest. Only the methods which are not included by\n// this config will use the policy that is configured by WithFailureRetry or WithBackupRequest .\n// FailureRetry and BackupRequest can be set for different method at same time.\n// Notice: method name is case-sensitive, it should be same with the definition in IDL.\nfunc WithRetryMethodPolicies(mp map[string]retry.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif mp == nil {\n\t\t\treturn\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithRetryMethodPolicies(%+v)\", mp))\n\t\tif o.RetryMethodPolicies == nil {\n\t\t\to.RetryMethodPolicies = make(map[string]retry.Policy)\n\t\t}\n\t\twildcardCfg := o.RetryMethodPolicies[retry.Wildcard]\n\t\to.RetryMethodPolicies = mp\n\t\tif wildcardCfg.Enable && !mp[retry.Wildcard].Enable {\n\t\t\t// if there is enabled wildcard config before, keep it\n\t\t\to.RetryMethodPolicies[retry.Wildcard] = wildcardCfg\n\t\t}\n\t}}\n}\n\n// WithSpecifiedResultRetry is used with FailureRetry.\n// When you enable FailureRetry and want to retry with the specified error or response, you can configure this Option.\n// ShouldResultRetry is defined inside retry.FailurePolicy, so WithFailureRetry also can set ShouldResultRetry.\n// But if your retry policy is enabled by remote config, WithSpecifiedResultRetry is useful.\n\n\n\n\n\n\n\n\n\n\n// WithFallback is used to set the fallback policy for the client.\n// Demos are provided below:\n//\n//\tdemo1. fallback for error and resp\n//\t\t`client.WithFallback(fallback.NewFallbackPolicy(yourFBFunc))`\n//\tdemo2. fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`client.WithFallback(fallback.ErrorFallback(yourErrFBFunc).EnableReportAsFallback())`\n//\tdemo2. fallback for rpctime and circuit breaker\n//\t\t`client.WithFallback(fallback.TimeoutAndCBFallback(yourErrFBFunc))`\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tif !fallback.IsPolicyValid(fb) {\n\t\t\tpanic(fmt.Errorf(\"WithFallback: invalid '%+v'\", fb))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithFallback(%+v)\", fb))\n\t\to.Fallback = fb\n\t}}\n}\n\n// WithCircuitBreaker adds a circuitbreaker suite for the client.\n\n\n\n\n\n\n\n// WithGRPCConnPoolSize sets the value for the client connection pool size.\n// In general, you should not adjust the size of the connection pool, otherwise it may cause performance degradation.\n// You should adjust the size according to the actual situation.\n\n\n\n\n\n\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a\n// write on the wire. The corresponding memory allocation for this buffer will\n// be twice the size to keep syscalls low. The default value for this buffer is\n// 32KB.\n//\n// Zero will disable the write buffer such that each write will be on underlying\n// connection. Note: A Send call may not directly translate to a write.\n// It corresponds to the WithWriteBufferSize DialOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how\n// much data can be read at most for each read syscall.\n//\n// The default value for this buffer is 32KB. Zero will disable read buffer for\n// a connection so data framer can access the underlying conn directly.\n// It corresponds to the WithReadBufferSize DialOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%d)\", s))\n\t\to.GRPCConnectOpts.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize sets the value for initial window size on a grpc stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize sets the value for initial window size on a connection of grpc.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the WithInitialConnWindowSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a DialOption that specifies the maximum\n// (uncompressed) size of header list that the client is prepared to accept.\n// It corresponds to the WithMaxHeaderListSize DialOption of gRPC.\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns a DialOption that specifies keepalive parameters for the client transport.\n// It corresponds to the WithKeepaliveParams DialOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n// WithWarmingUp forces the client to do some warm-ups at the end of the initialization.\n\n\n\n\n\n\n\n// WithXDSSuite is used to set the xds suite for the client.\n\n\n\n\n\n\n\n\n\n\n\n// WithContextBackup enables local-session to retrieve context backuped by server,\n// in case of user don't correctly pass context into next RPC call.\n//   - backupHandler pass a handler to check and handler user-defined key-values according to current context, returning backup==false means no need further operations.\nfunc WithContextBackup(backupHandler func(prev, cur context.Context) (ctx context.Context, backup bool)) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithContextBackup({%v})\", reflect.TypeOf(backupHandler).String()))\n\t\to.CtxBackupHandler = backupHandler\n\t}}\n}\n\n```\n# Function Signature: func WithXDSSuite(suite xds.ClientSuite) Option {\n# Natural Language Annotations: // WithXDSSuite is used to set the xds suite for the client.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithXDSSuite(suite xds.ClientSuite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithXDSSuite(%T{%+v})\", suite, suite))\n\t\to.XDSSuite = suite\n\t}}\n}\n```", "```go\nfunc WithXDSSuite(suite xds.ClientSuite) Option {\n\treturn Option{F: func(o *client.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithXDSSuite(%v)\", reflect.TypeOf(suite).String()))\n\t\to.XDSSuite = suite\n\t}}\n}\n```", "```go\nfunc WithXDSSuite(suite xds.ClientSuite) Option {\n    return Option{F: func(o *client.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithXDSSuite(%+v)\", suite))\n        o.XDSSuite = suite\n    }}\n}\n```"]}, "kitex-client/callopt/options.go-WithHostPort": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/callopt/options.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package callopt contains options that control the behavior of client on request level.\npackage callopt\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nvar callOptionsPool = sync.Pool{\n\tNew: newOptions,\n}\n\ntype CallOptions struct {\n\tconfigs      rpcinfo.MutableRPCConfig\n\tsvr          remoteinfo.RemoteInfo\n\tlocks        *client.ConfigLocks\n\thttpResolver http.Resolver\n\n\t// export field for using in client\n\tRetryPolicy    retry.Policy\n\tFallback       *fallback.Policy\n\tCompressorName string\n}\n\nfunc newOptions() interface{} {\n\treturn &CallOptions{\n\t\tlocks: &client.ConfigLocks{\n\t\t\tTags: make(map[string]struct{}),\n\t\t},\n\t}\n}\n\n// Recycle zeros the call option and put it to the pool.\nfunc (co *CallOptions) Recycle() {\n\tif co == nil {\n\t\treturn\n\t}\n\tco.configs = nil\n\tco.svr = nil\n\tco.RetryPolicy = retry.Policy{}\n\tco.Fallback = nil\n\tco.locks.Zero()\n\tcallOptionsPool.Put(co)\n}\n\n// Option is a series of options used at the beginning of a RPC call.\ntype Option struct {\n\tf func(o *CallOptions, di *strings.Builder)\n}\n\n// F returns the function of the option.\n// It's useful for creating streamcall.Option from existing callopt.Option\n// Note: not all callopt.Option(s) are available for stream clients.\nfunc (o Option) F() func(o *CallOptions, di *strings.Builder) {\n\treturn o.f\n}\n\n// NewOption returns a new Option with the given function.\n// It's useful for converting streamcall.Option back to a callopt.Option\nfunc NewOption(f func(o *CallOptions, di *strings.Builder)) Option {\n\treturn Option{f: f}\n}\n\n// WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\nfunc setInstance(svr remoteinfo.RemoteInfo, hostport string) error {\n\tif _, err := net.ResolveTCPAddr(\"tcp\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"tcp\", hostport, discovery.DefaultWeight, nil))\n\t} else if _, err := net.ResolveUnixAddr(\"unix\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"unix\", hostport, discovery.DefaultWeight, nil))\n\t} else {\n\t\treturn fmt.Errorf(\"invalid '%s'\", hostport)\n\t}\n\treturn nil\n}\n\n// WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithHTTPHost specifies host in http header(work when RPC over http).\nfunc WithHTTPHost(host string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\to.svr.SetTag(rpcinfo.HTTPHost, host)\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithConnectTimeout specifies the connection timeout for a RPC call.\n\n\n\n\n\n\n\n\n\n// WithTag sets the tags for service discovery for a RPC call.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryPolicy sets the retry policy for a RPC call.\n// Build retry.Policy with retry.BuildFailurePolicy or retry.BuildBackupRequest instead of building retry.Policy directly.\n// Demos are provided below:\n//\n//\t  demo1. call with failure retry policy, default retry error is Timeout\n//\t  \t`resp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildFailurePolicy(retry.NewFailurePolicy())))`\n//\t  demo2. call with backup request policy\n//\t  \t`bp := retry.NewBackupPolicy(10)\n//\t\t \tbp.WithMaxRetryTimes(1)\n//\t  \tresp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildBackupRequest(bp)))`\nfunc WithRetryPolicy(p retry.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif !p.Enable {\n\t\t\treturn\n\t\t}\n\t\tif p.Type == retry.BackupType {\n\t\t\tdi.WriteString(\"WithBackupRequest\")\n\t\t} else {\n\t\t\tdi.WriteString(\"WithFailureRetry\")\n\t\t}\n\t\to.RetryPolicy = p\n\t}}\n}\n\n// WithFallback is used to set the fallback policy for a RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\n\n\n\n\n\n\n\n\n\n\n// WithGRPCCompressor specifies the compressor for the GRPC frame payload.\n// Supported compressor names: identity, gzip\n// Custom compressors can be registered via `encoding.RegisterCompressor`\nfunc WithGRPCCompressor(compressorName string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithGRPCCompressor\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(compressorName)\n\t\tdi.WriteByte(')')\n\t\to.CompressorName = compressorName\n\t}}\n}\n\n// Apply applies call options to the rpcinfo.RPCConfig and internal.RemoteInfo of kitex client.\n// The return value records the name and arguments of each option.\n// This function is for internal purpose only.\nfunc Apply(cos []Option, cfg rpcinfo.MutableRPCConfig, svr remoteinfo.RemoteInfo, locks *client.ConfigLocks, httpResolver http.Resolver) (string, *CallOptions) {\n\tvar buf strings.Builder\n\tbuf.Grow(64)\n\n\tco := callOptionsPool.Get().(*CallOptions)\n\tco.configs = cfg\n\tco.svr = svr\n\tco.locks.Merge(locks)\n\tco.httpResolver = httpResolver\n\n\tbuf.WriteByte('[')\n\tfor i := range cos {\n\t\tif i > 0 {\n\t\t\tbuf.WriteByte(',')\n\t\t}\n\t\tcos[i].f(co, &buf)\n\t}\n\tbuf.WriteByte(']')\n\n\tco.locks.ApplyLocks(cfg, svr)\n\treturn buf.String(), co\n}\n\n```\n# Function Signature: func WithHostPort(hostport string) Option {\n# Natural Language Annotations: // WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithHostPort(hostport string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithHostPort\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(hostport)\n\t\tdi.WriteByte(')')\n\t\tif err := setInstance(o.svr, hostport); err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}}\n}\n```", "```go\nfunc WithHostPort(hostport string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithHostPort\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(hostport)\n\t\tdi.WriteByte(')')\n\t\tif err := setInstance(o.svr, hostport); err != nil {\n\t\t\tpanic(fmt.Errorf(\"WithHostPort: %s\", err))\n\t\t}\n\t}}\n}\n```", "```go\nfunc WithHostPort(hostport string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithHostPort\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(hostport)\n\t\tdi.WriteByte(')')\n\t\tsetInstance(o.svr, hostport)\n\t}}\n}\n```"]}, "kitex-client/callopt/options.go-WithURL": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/callopt/options.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package callopt contains options that control the behavior of client on request level.\npackage callopt\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nvar callOptionsPool = sync.Pool{\n\tNew: newOptions,\n}\n\ntype CallOptions struct {\n\tconfigs      rpcinfo.MutableRPCConfig\n\tsvr          remoteinfo.RemoteInfo\n\tlocks        *client.ConfigLocks\n\thttpResolver http.Resolver\n\n\t// export field for using in client\n\tRetryPolicy    retry.Policy\n\tFallback       *fallback.Policy\n\tCompressorName string\n}\n\nfunc newOptions() interface{} {\n\treturn &CallOptions{\n\t\tlocks: &client.ConfigLocks{\n\t\t\tTags: make(map[string]struct{}),\n\t\t},\n\t}\n}\n\n// Recycle zeros the call option and put it to the pool.\nfunc (co *CallOptions) Recycle() {\n\tif co == nil {\n\t\treturn\n\t}\n\tco.configs = nil\n\tco.svr = nil\n\tco.RetryPolicy = retry.Policy{}\n\tco.Fallback = nil\n\tco.locks.Zero()\n\tcallOptionsPool.Put(co)\n}\n\n// Option is a series of options used at the beginning of a RPC call.\ntype Option struct {\n\tf func(o *CallOptions, di *strings.Builder)\n}\n\n// F returns the function of the option.\n// It's useful for creating streamcall.Option from existing callopt.Option\n// Note: not all callopt.Option(s) are available for stream clients.\nfunc (o Option) F() func(o *CallOptions, di *strings.Builder) {\n\treturn o.f\n}\n\n// NewOption returns a new Option with the given function.\n// It's useful for converting streamcall.Option back to a callopt.Option\nfunc NewOption(f func(o *CallOptions, di *strings.Builder)) Option {\n\treturn Option{f: f}\n}\n\n// WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\nfunc setInstance(svr remoteinfo.RemoteInfo, hostport string) error {\n\tif _, err := net.ResolveTCPAddr(\"tcp\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"tcp\", hostport, discovery.DefaultWeight, nil))\n\t} else if _, err := net.ResolveUnixAddr(\"unix\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"unix\", hostport, discovery.DefaultWeight, nil))\n\t} else {\n\t\treturn fmt.Errorf(\"invalid '%s'\", hostport)\n\t}\n\treturn nil\n}\n\n// WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithHTTPHost specifies host in http header(work when RPC over http).\nfunc WithHTTPHost(host string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\to.svr.SetTag(rpcinfo.HTTPHost, host)\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithConnectTimeout specifies the connection timeout for a RPC call.\n\n\n\n\n\n\n\n\n\n// WithTag sets the tags for service discovery for a RPC call.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryPolicy sets the retry policy for a RPC call.\n// Build retry.Policy with retry.BuildFailurePolicy or retry.BuildBackupRequest instead of building retry.Policy directly.\n// Demos are provided below:\n//\n//\t  demo1. call with failure retry policy, default retry error is Timeout\n//\t  \t`resp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildFailurePolicy(retry.NewFailurePolicy())))`\n//\t  demo2. call with backup request policy\n//\t  \t`bp := retry.NewBackupPolicy(10)\n//\t\t \tbp.WithMaxRetryTimes(1)\n//\t  \tresp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildBackupRequest(bp)))`\nfunc WithRetryPolicy(p retry.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif !p.Enable {\n\t\t\treturn\n\t\t}\n\t\tif p.Type == retry.BackupType {\n\t\t\tdi.WriteString(\"WithBackupRequest\")\n\t\t} else {\n\t\t\tdi.WriteString(\"WithFailureRetry\")\n\t\t}\n\t\to.RetryPolicy = p\n\t}}\n}\n\n// WithFallback is used to set the fallback policy for a RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\n\n\n\n\n\n\n\n\n\n\n// WithGRPCCompressor specifies the compressor for the GRPC frame payload.\n// Supported compressor names: identity, gzip\n// Custom compressors can be registered via `encoding.RegisterCompressor`\nfunc WithGRPCCompressor(compressorName string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithGRPCCompressor\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(compressorName)\n\t\tdi.WriteByte(')')\n\t\to.CompressorName = compressorName\n\t}}\n}\n\n// Apply applies call options to the rpcinfo.RPCConfig and internal.RemoteInfo of kitex client.\n// The return value records the name and arguments of each option.\n// This function is for internal purpose only.\nfunc Apply(cos []Option, cfg rpcinfo.MutableRPCConfig, svr remoteinfo.RemoteInfo, locks *client.ConfigLocks, httpResolver http.Resolver) (string, *CallOptions) {\n\tvar buf strings.Builder\n\tbuf.Grow(64)\n\n\tco := callOptionsPool.Get().(*CallOptions)\n\tco.configs = cfg\n\tco.svr = svr\n\tco.locks.Merge(locks)\n\tco.httpResolver = httpResolver\n\n\tbuf.WriteByte('[')\n\tfor i := range cos {\n\t\tif i > 0 {\n\t\t\tbuf.WriteByte(',')\n\t\t}\n\t\tcos[i].f(co, &buf)\n\t}\n\tbuf.WriteByte(']')\n\n\tco.locks.ApplyLocks(cfg, svr)\n\treturn buf.String(), co\n}\n\n```\n# Function Signature: func WithURL(url string) Option {\n# Natural Language Annotations: // WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithURL(url string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithURL\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(url)\n\t\tdi.WriteByte(')')\n\t\tsetInstance(o.svr, url)\n\t}}\n}\n```", "```go\nfunc WithURL(url string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithURL\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(url)\n\t\tdi.WriteByte(')')\n\t\to.svr.SetInstance(discovery.NewInstance(\"\", url, discovery.DefaultWeight, nil))\n\t}}\n}\n```\nThis function generates a new `Option` that sets the target for a RPC call with the given url. The url is resolved to a hostport and overwrites the result from the Resolver. The function also appends the name and argument of the option to a string builder `di` for later use.", "```go\nfunc WithURL(url string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithURL\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(url)\n\t\tdi.WriteByte(')')\n\n\t\tif err := setInstance(o.svr, url); err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t}}\n}\n```"]}, "kitex-client/callopt/options.go-WithRPCTimeout": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/callopt/options.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package callopt contains options that control the behavior of client on request level.\npackage callopt\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nvar callOptionsPool = sync.Pool{\n\tNew: newOptions,\n}\n\ntype CallOptions struct {\n\tconfigs      rpcinfo.MutableRPCConfig\n\tsvr          remoteinfo.RemoteInfo\n\tlocks        *client.ConfigLocks\n\thttpResolver http.Resolver\n\n\t// export field for using in client\n\tRetryPolicy    retry.Policy\n\tFallback       *fallback.Policy\n\tCompressorName string\n}\n\nfunc newOptions() interface{} {\n\treturn &CallOptions{\n\t\tlocks: &client.ConfigLocks{\n\t\t\tTags: make(map[string]struct{}),\n\t\t},\n\t}\n}\n\n// Recycle zeros the call option and put it to the pool.\nfunc (co *CallOptions) Recycle() {\n\tif co == nil {\n\t\treturn\n\t}\n\tco.configs = nil\n\tco.svr = nil\n\tco.RetryPolicy = retry.Policy{}\n\tco.Fallback = nil\n\tco.locks.Zero()\n\tcallOptionsPool.Put(co)\n}\n\n// Option is a series of options used at the beginning of a RPC call.\ntype Option struct {\n\tf func(o *CallOptions, di *strings.Builder)\n}\n\n// F returns the function of the option.\n// It's useful for creating streamcall.Option from existing callopt.Option\n// Note: not all callopt.Option(s) are available for stream clients.\nfunc (o Option) F() func(o *CallOptions, di *strings.Builder) {\n\treturn o.f\n}\n\n// NewOption returns a new Option with the given function.\n// It's useful for converting streamcall.Option back to a callopt.Option\nfunc NewOption(f func(o *CallOptions, di *strings.Builder)) Option {\n\treturn Option{f: f}\n}\n\n// WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\nfunc setInstance(svr remoteinfo.RemoteInfo, hostport string) error {\n\tif _, err := net.ResolveTCPAddr(\"tcp\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"tcp\", hostport, discovery.DefaultWeight, nil))\n\t} else if _, err := net.ResolveUnixAddr(\"unix\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"unix\", hostport, discovery.DefaultWeight, nil))\n\t} else {\n\t\treturn fmt.Errorf(\"invalid '%s'\", hostport)\n\t}\n\treturn nil\n}\n\n// WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithHTTPHost specifies host in http header(work when RPC over http).\nfunc WithHTTPHost(host string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\to.svr.SetTag(rpcinfo.HTTPHost, host)\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithConnectTimeout specifies the connection timeout for a RPC call.\n\n\n\n\n\n\n\n\n\n// WithTag sets the tags for service discovery for a RPC call.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryPolicy sets the retry policy for a RPC call.\n// Build retry.Policy with retry.BuildFailurePolicy or retry.BuildBackupRequest instead of building retry.Policy directly.\n// Demos are provided below:\n//\n//\t  demo1. call with failure retry policy, default retry error is Timeout\n//\t  \t`resp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildFailurePolicy(retry.NewFailurePolicy())))`\n//\t  demo2. call with backup request policy\n//\t  \t`bp := retry.NewBackupPolicy(10)\n//\t\t \tbp.WithMaxRetryTimes(1)\n//\t  \tresp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildBackupRequest(bp)))`\nfunc WithRetryPolicy(p retry.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif !p.Enable {\n\t\t\treturn\n\t\t}\n\t\tif p.Type == retry.BackupType {\n\t\t\tdi.WriteString(\"WithBackupRequest\")\n\t\t} else {\n\t\t\tdi.WriteString(\"WithFailureRetry\")\n\t\t}\n\t\to.RetryPolicy = p\n\t}}\n}\n\n// WithFallback is used to set the fallback policy for a RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\n\n\n\n\n\n\n\n\n\n\n// WithGRPCCompressor specifies the compressor for the GRPC frame payload.\n// Supported compressor names: identity, gzip\n// Custom compressors can be registered via `encoding.RegisterCompressor`\nfunc WithGRPCCompressor(compressorName string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithGRPCCompressor\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(compressorName)\n\t\tdi.WriteByte(')')\n\t\to.CompressorName = compressorName\n\t}}\n}\n\n// Apply applies call options to the rpcinfo.RPCConfig and internal.RemoteInfo of kitex client.\n// The return value records the name and arguments of each option.\n// This function is for internal purpose only.\nfunc Apply(cos []Option, cfg rpcinfo.MutableRPCConfig, svr remoteinfo.RemoteInfo, locks *client.ConfigLocks, httpResolver http.Resolver) (string, *CallOptions) {\n\tvar buf strings.Builder\n\tbuf.Grow(64)\n\n\tco := callOptionsPool.Get().(*CallOptions)\n\tco.configs = cfg\n\tco.svr = svr\n\tco.locks.Merge(locks)\n\tco.httpResolver = httpResolver\n\n\tbuf.WriteByte('[')\n\tfor i := range cos {\n\t\tif i > 0 {\n\t\t\tbuf.WriteByte(',')\n\t\t}\n\t\tcos[i].f(co, &buf)\n\t}\n\tbuf.WriteByte(']')\n\n\tco.locks.ApplyLocks(cfg, svr)\n\treturn buf.String(), co\n}\n\n```\n# Function Signature: func WithRPCTimeout(d time.Duration) Option {\n# Natural Language Annotations: // WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithRPCTimeout\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(d.String())\n\t\tdi.WriteByte(')')\n\t\to.configs.SetRPCTimeout(d)\n\t}}\n}\n```", "```go\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithRPCTimeout\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(d.String())\n\t\tdi.WriteByte(')')\n\t\to.configs.SetRPCTimeout(d)\n\t}}\n}\n```", "```go\nfunc WithRPCTimeout(d time.Duration) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\to.configs.SetRPCTimeout(d)\n\t\tdi.WriteString(\"WithRPCTimeout\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(d.String())\n\t\tdi.WriteByte(')')\n\t}}\n}\n```"]}, "kitex-client/callopt/options.go-WithConnectTimeout": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/callopt/options.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package callopt contains options that control the behavior of client on request level.\npackage callopt\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nvar callOptionsPool = sync.Pool{\n\tNew: newOptions,\n}\n\ntype CallOptions struct {\n\tconfigs      rpcinfo.MutableRPCConfig\n\tsvr          remoteinfo.RemoteInfo\n\tlocks        *client.ConfigLocks\n\thttpResolver http.Resolver\n\n\t// export field for using in client\n\tRetryPolicy    retry.Policy\n\tFallback       *fallback.Policy\n\tCompressorName string\n}\n\nfunc newOptions() interface{} {\n\treturn &CallOptions{\n\t\tlocks: &client.ConfigLocks{\n\t\t\tTags: make(map[string]struct{}),\n\t\t},\n\t}\n}\n\n// Recycle zeros the call option and put it to the pool.\nfunc (co *CallOptions) Recycle() {\n\tif co == nil {\n\t\treturn\n\t}\n\tco.configs = nil\n\tco.svr = nil\n\tco.RetryPolicy = retry.Policy{}\n\tco.Fallback = nil\n\tco.locks.Zero()\n\tcallOptionsPool.Put(co)\n}\n\n// Option is a series of options used at the beginning of a RPC call.\ntype Option struct {\n\tf func(o *CallOptions, di *strings.Builder)\n}\n\n// F returns the function of the option.\n// It's useful for creating streamcall.Option from existing callopt.Option\n// Note: not all callopt.Option(s) are available for stream clients.\nfunc (o Option) F() func(o *CallOptions, di *strings.Builder) {\n\treturn o.f\n}\n\n// NewOption returns a new Option with the given function.\n// It's useful for converting streamcall.Option back to a callopt.Option\nfunc NewOption(f func(o *CallOptions, di *strings.Builder)) Option {\n\treturn Option{f: f}\n}\n\n// WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\nfunc setInstance(svr remoteinfo.RemoteInfo, hostport string) error {\n\tif _, err := net.ResolveTCPAddr(\"tcp\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"tcp\", hostport, discovery.DefaultWeight, nil))\n\t} else if _, err := net.ResolveUnixAddr(\"unix\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"unix\", hostport, discovery.DefaultWeight, nil))\n\t} else {\n\t\treturn fmt.Errorf(\"invalid '%s'\", hostport)\n\t}\n\treturn nil\n}\n\n// WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithHTTPHost specifies host in http header(work when RPC over http).\nfunc WithHTTPHost(host string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\to.svr.SetTag(rpcinfo.HTTPHost, host)\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithConnectTimeout specifies the connection timeout for a RPC call.\n\n\n\n\n\n\n\n\n\n// WithTag sets the tags for service discovery for a RPC call.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryPolicy sets the retry policy for a RPC call.\n// Build retry.Policy with retry.BuildFailurePolicy or retry.BuildBackupRequest instead of building retry.Policy directly.\n// Demos are provided below:\n//\n//\t  demo1. call with failure retry policy, default retry error is Timeout\n//\t  \t`resp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildFailurePolicy(retry.NewFailurePolicy())))`\n//\t  demo2. call with backup request policy\n//\t  \t`bp := retry.NewBackupPolicy(10)\n//\t\t \tbp.WithMaxRetryTimes(1)\n//\t  \tresp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildBackupRequest(bp)))`\nfunc WithRetryPolicy(p retry.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif !p.Enable {\n\t\t\treturn\n\t\t}\n\t\tif p.Type == retry.BackupType {\n\t\t\tdi.WriteString(\"WithBackupRequest\")\n\t\t} else {\n\t\t\tdi.WriteString(\"WithFailureRetry\")\n\t\t}\n\t\to.RetryPolicy = p\n\t}}\n}\n\n// WithFallback is used to set the fallback policy for a RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\n\n\n\n\n\n\n\n\n\n\n// WithGRPCCompressor specifies the compressor for the GRPC frame payload.\n// Supported compressor names: identity, gzip\n// Custom compressors can be registered via `encoding.RegisterCompressor`\nfunc WithGRPCCompressor(compressorName string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithGRPCCompressor\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(compressorName)\n\t\tdi.WriteByte(')')\n\t\to.CompressorName = compressorName\n\t}}\n}\n\n// Apply applies call options to the rpcinfo.RPCConfig and internal.RemoteInfo of kitex client.\n// The return value records the name and arguments of each option.\n// This function is for internal purpose only.\nfunc Apply(cos []Option, cfg rpcinfo.MutableRPCConfig, svr remoteinfo.RemoteInfo, locks *client.ConfigLocks, httpResolver http.Resolver) (string, *CallOptions) {\n\tvar buf strings.Builder\n\tbuf.Grow(64)\n\n\tco := callOptionsPool.Get().(*CallOptions)\n\tco.configs = cfg\n\tco.svr = svr\n\tco.locks.Merge(locks)\n\tco.httpResolver = httpResolver\n\n\tbuf.WriteByte('[')\n\tfor i := range cos {\n\t\tif i > 0 {\n\t\t\tbuf.WriteByte(',')\n\t\t}\n\t\tcos[i].f(co, &buf)\n\t}\n\tbuf.WriteByte(']')\n\n\tco.locks.ApplyLocks(cfg, svr)\n\treturn buf.String(), co\n}\n\n```\n# Function Signature: func WithConnectTimeout(d time.Duration) Option {\n# Natural Language Annotations: // WithConnectTimeout specifies the connection timeout for a RPC call.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithConnectTimeout\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(d.String())\n\t\tdi.WriteByte(')')\n\t\to.svr.SetTag(rpcinfo.ConnectTimeout, d)\n\t}}\n}\n```", "```go\n// WithConnectTimeout specifies the connection timeout for a RPC call.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\to.locks.ConnectTimeout = d\n\t}}\n}\n```", "```go\n// WithConnectTimeout specifies the connection timeout for a RPC call.\nfunc WithConnectTimeout(d time.Duration) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithConnectTimeout\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(d.String())\n\t\tdi.WriteByte(')')\n\t\to.configs.SetConnectTimeout(d)\n\t}}\n}\n```"]}, "kitex-client/callopt/options.go-WithTag": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/callopt/options.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package callopt contains options that control the behavior of client on request level.\npackage callopt\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nvar callOptionsPool = sync.Pool{\n\tNew: newOptions,\n}\n\ntype CallOptions struct {\n\tconfigs      rpcinfo.MutableRPCConfig\n\tsvr          remoteinfo.RemoteInfo\n\tlocks        *client.ConfigLocks\n\thttpResolver http.Resolver\n\n\t// export field for using in client\n\tRetryPolicy    retry.Policy\n\tFallback       *fallback.Policy\n\tCompressorName string\n}\n\nfunc newOptions() interface{} {\n\treturn &CallOptions{\n\t\tlocks: &client.ConfigLocks{\n\t\t\tTags: make(map[string]struct{}),\n\t\t},\n\t}\n}\n\n// Recycle zeros the call option and put it to the pool.\nfunc (co *CallOptions) Recycle() {\n\tif co == nil {\n\t\treturn\n\t}\n\tco.configs = nil\n\tco.svr = nil\n\tco.RetryPolicy = retry.Policy{}\n\tco.Fallback = nil\n\tco.locks.Zero()\n\tcallOptionsPool.Put(co)\n}\n\n// Option is a series of options used at the beginning of a RPC call.\ntype Option struct {\n\tf func(o *CallOptions, di *strings.Builder)\n}\n\n// F returns the function of the option.\n// It's useful for creating streamcall.Option from existing callopt.Option\n// Note: not all callopt.Option(s) are available for stream clients.\nfunc (o Option) F() func(o *CallOptions, di *strings.Builder) {\n\treturn o.f\n}\n\n// NewOption returns a new Option with the given function.\n// It's useful for converting streamcall.Option back to a callopt.Option\nfunc NewOption(f func(o *CallOptions, di *strings.Builder)) Option {\n\treturn Option{f: f}\n}\n\n// WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\nfunc setInstance(svr remoteinfo.RemoteInfo, hostport string) error {\n\tif _, err := net.ResolveTCPAddr(\"tcp\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"tcp\", hostport, discovery.DefaultWeight, nil))\n\t} else if _, err := net.ResolveUnixAddr(\"unix\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"unix\", hostport, discovery.DefaultWeight, nil))\n\t} else {\n\t\treturn fmt.Errorf(\"invalid '%s'\", hostport)\n\t}\n\treturn nil\n}\n\n// WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithHTTPHost specifies host in http header(work when RPC over http).\nfunc WithHTTPHost(host string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\to.svr.SetTag(rpcinfo.HTTPHost, host)\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithConnectTimeout specifies the connection timeout for a RPC call.\n\n\n\n\n\n\n\n\n\n// WithTag sets the tags for service discovery for a RPC call.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryPolicy sets the retry policy for a RPC call.\n// Build retry.Policy with retry.BuildFailurePolicy or retry.BuildBackupRequest instead of building retry.Policy directly.\n// Demos are provided below:\n//\n//\t  demo1. call with failure retry policy, default retry error is Timeout\n//\t  \t`resp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildFailurePolicy(retry.NewFailurePolicy())))`\n//\t  demo2. call with backup request policy\n//\t  \t`bp := retry.NewBackupPolicy(10)\n//\t\t \tbp.WithMaxRetryTimes(1)\n//\t  \tresp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildBackupRequest(bp)))`\nfunc WithRetryPolicy(p retry.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif !p.Enable {\n\t\t\treturn\n\t\t}\n\t\tif p.Type == retry.BackupType {\n\t\t\tdi.WriteString(\"WithBackupRequest\")\n\t\t} else {\n\t\t\tdi.WriteString(\"WithFailureRetry\")\n\t\t}\n\t\to.RetryPolicy = p\n\t}}\n}\n\n// WithFallback is used to set the fallback policy for a RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\n\n\n\n\n\n\n\n\n\n\n// WithGRPCCompressor specifies the compressor for the GRPC frame payload.\n// Supported compressor names: identity, gzip\n// Custom compressors can be registered via `encoding.RegisterCompressor`\nfunc WithGRPCCompressor(compressorName string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithGRPCCompressor\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(compressorName)\n\t\tdi.WriteByte(')')\n\t\to.CompressorName = compressorName\n\t}}\n}\n\n// Apply applies call options to the rpcinfo.RPCConfig and internal.RemoteInfo of kitex client.\n// The return value records the name and arguments of each option.\n// This function is for internal purpose only.\nfunc Apply(cos []Option, cfg rpcinfo.MutableRPCConfig, svr remoteinfo.RemoteInfo, locks *client.ConfigLocks, httpResolver http.Resolver) (string, *CallOptions) {\n\tvar buf strings.Builder\n\tbuf.Grow(64)\n\n\tco := callOptionsPool.Get().(*CallOptions)\n\tco.configs = cfg\n\tco.svr = svr\n\tco.locks.Merge(locks)\n\tco.httpResolver = httpResolver\n\n\tbuf.WriteByte('[')\n\tfor i := range cos {\n\t\tif i > 0 {\n\t\t\tbuf.WriteByte(',')\n\t\t}\n\t\tcos[i].f(co, &buf)\n\t}\n\tbuf.WriteByte(']')\n\n\tco.locks.ApplyLocks(cfg, svr)\n\treturn buf.String(), co\n}\n\n```\n# Function Signature: func WithTag(key, val string) Option {\n# Natural Language Annotations: // WithTag sets the tags for service discovery for a RPC call.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithTag sets the tags for service discovery for a RPC call.\nfunc WithTag(key, val string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithTag\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(key)\n\t\tdi.WriteByte(',')\n\t\tdi.WriteString(val)\n\t\tdi.WriteByte(')')\n\t\to.locks.LockTags(key)\n\t\to.svr.Tag(rpcinfo.NewEndpointInfo(key, val))\n\t}}\n}\n```", "```go\nfunc WithTag(key, val string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithTag\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(key)\n\t\tdi.WriteString(\", \")\n\t\tdi.WriteString(val)\n\t\tdi.WriteByte(')')\n\t\to.locks.TagLock.Lock()\n\t\tdefer o.locks.TagLock.Unlock()\n\t\tif o.locks.Tags == nil {\n\t\t\to.locks.Tags = make(map[string]struct{})\n\t\t}\n\t\to.locks.Tags[key] = struct{}{}\n\t\to.configs.SetTag(key, val)\n\t}}\n}\n```", "```go\n// WithTag sets the tags for service discovery for a RPC call.\nfunc WithTag(key, val string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\to.svr.SetTag(key, val)\n\t}}\n}\n```"]}, "kitex-client/callopt/options.go-WithFallback": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/client/callopt/options.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package callopt contains options that control the behavior of client on request level.\npackage callopt\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/cloudwego/kitex/internal/client\"\n\t\"github.com/cloudwego/kitex/pkg/discovery\"\n\t\"github.com/cloudwego/kitex/pkg/fallback\"\n\t\"github.com/cloudwego/kitex/pkg/http\"\n\t\"github.com/cloudwego/kitex/pkg/retry\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo/remoteinfo\"\n)\n\nvar callOptionsPool = sync.Pool{\n\tNew: newOptions,\n}\n\ntype CallOptions struct {\n\tconfigs      rpcinfo.MutableRPCConfig\n\tsvr          remoteinfo.RemoteInfo\n\tlocks        *client.ConfigLocks\n\thttpResolver http.Resolver\n\n\t// export field for using in client\n\tRetryPolicy    retry.Policy\n\tFallback       *fallback.Policy\n\tCompressorName string\n}\n\nfunc newOptions() interface{} {\n\treturn &CallOptions{\n\t\tlocks: &client.ConfigLocks{\n\t\t\tTags: make(map[string]struct{}),\n\t\t},\n\t}\n}\n\n// Recycle zeros the call option and put it to the pool.\nfunc (co *CallOptions) Recycle() {\n\tif co == nil {\n\t\treturn\n\t}\n\tco.configs = nil\n\tco.svr = nil\n\tco.RetryPolicy = retry.Policy{}\n\tco.Fallback = nil\n\tco.locks.Zero()\n\tcallOptionsPool.Put(co)\n}\n\n// Option is a series of options used at the beginning of a RPC call.\ntype Option struct {\n\tf func(o *CallOptions, di *strings.Builder)\n}\n\n// F returns the function of the option.\n// It's useful for creating streamcall.Option from existing callopt.Option\n// Note: not all callopt.Option(s) are available for stream clients.\nfunc (o Option) F() func(o *CallOptions, di *strings.Builder) {\n\treturn o.f\n}\n\n// NewOption returns a new Option with the given function.\n// It's useful for converting streamcall.Option back to a callopt.Option\nfunc NewOption(f func(o *CallOptions, di *strings.Builder)) Option {\n\treturn Option{f: f}\n}\n\n// WithHostPort specifies the target address for a RPC call.\n// The given address will overwrite the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\nfunc setInstance(svr remoteinfo.RemoteInfo, hostport string) error {\n\tif _, err := net.ResolveTCPAddr(\"tcp\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"tcp\", hostport, discovery.DefaultWeight, nil))\n\t} else if _, err := net.ResolveUnixAddr(\"unix\", hostport); err == nil {\n\t\tsvr.SetInstance(discovery.NewInstance(\"unix\", hostport, discovery.DefaultWeight, nil))\n\t} else {\n\t\treturn fmt.Errorf(\"invalid '%s'\", hostport)\n\t}\n\treturn nil\n}\n\n// WithURL specifies the target for a RPC call with url.\n// The given url will be resolved to hostport and overwrites the result from Resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithHTTPHost specifies host in http header(work when RPC over http).\nfunc WithHTTPHost(host string) Option {\n\treturn Option{func(o *CallOptions, di *strings.Builder) {\n\t\to.svr.SetTag(rpcinfo.HTTPHost, host)\n\t}}\n}\n\n// WithRPCTimeout specifies the RPC timeout for a RPC call.\n// FIXME: callopt.WithRPCTimeout works only when client.WithRPCTimeout or\n// client.WithTimeoutProvider is specified.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithConnectTimeout specifies the connection timeout for a RPC call.\n\n\n\n\n\n\n\n\n\n// WithTag sets the tags for service discovery for a RPC call.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRetryPolicy sets the retry policy for a RPC call.\n// Build retry.Policy with retry.BuildFailurePolicy or retry.BuildBackupRequest instead of building retry.Policy directly.\n// Demos are provided below:\n//\n//\t  demo1. call with failure retry policy, default retry error is Timeout\n//\t  \t`resp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildFailurePolicy(retry.NewFailurePolicy())))`\n//\t  demo2. call with backup request policy\n//\t  \t`bp := retry.NewBackupPolicy(10)\n//\t\t \tbp.WithMaxRetryTimes(1)\n//\t  \tresp, err := cli.Mock(ctx, req, callopt.WithRetryPolicy(retry.BuildBackupRequest(bp)))`\nfunc WithRetryPolicy(p retry.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif !p.Enable {\n\t\t\treturn\n\t\t}\n\t\tif p.Type == retry.BackupType {\n\t\t\tdi.WriteString(\"WithBackupRequest\")\n\t\t} else {\n\t\t\tdi.WriteString(\"WithFailureRetry\")\n\t\t}\n\t\to.RetryPolicy = p\n\t}}\n}\n\n// WithFallback is used to set the fallback policy for a RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\n\n\n\n\n\n\n\n\n\n\n// WithGRPCCompressor specifies the compressor for the GRPC frame payload.\n// Supported compressor names: identity, gzip\n// Custom compressors can be registered via `encoding.RegisterCompressor`\nfunc WithGRPCCompressor(compressorName string) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithGRPCCompressor\")\n\t\tdi.WriteByte('(')\n\t\tdi.WriteString(compressorName)\n\t\tdi.WriteByte(')')\n\t\to.CompressorName = compressorName\n\t}}\n}\n\n// Apply applies call options to the rpcinfo.RPCConfig and internal.RemoteInfo of kitex client.\n// The return value records the name and arguments of each option.\n// This function is for internal purpose only.\nfunc Apply(cos []Option, cfg rpcinfo.MutableRPCConfig, svr remoteinfo.RemoteInfo, locks *client.ConfigLocks, httpResolver http.Resolver) (string, *CallOptions) {\n\tvar buf strings.Builder\n\tbuf.Grow(64)\n\n\tco := callOptionsPool.Get().(*CallOptions)\n\tco.configs = cfg\n\tco.svr = svr\n\tco.locks.Merge(locks)\n\tco.httpResolver = httpResolver\n\n\tbuf.WriteByte('[')\n\tfor i := range cos {\n\t\tif i > 0 {\n\t\t\tbuf.WriteByte(',')\n\t\t}\n\t\tcos[i].f(co, &buf)\n\t}\n\tbuf.WriteByte(']')\n\n\tco.locks.ApplyLocks(cfg, svr)\n\treturn buf.String(), co\n}\n\n```\n# Function Signature: func WithFallback(fb *fallback.Policy) Option {\n# Natural Language Annotations: // WithFallback is used to set the fallback policy for a RPC call.\n// Demos are provided below:\n//\n//\tdemo1. call with fallback for error\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc))`\n//\tdemo2. call with fallback for error and enable reportAsFallback, which sets reportAsFallback to be true and will do report(metric) as Fallback result\n//\t\t`resp, err := cli.Mock(ctx, req, callopt.WithFallback(fallback.ErrorFallback(yourFBFunc).EnableReportAsFallback())`\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithFallback\")\n\t\to.Fallback = fb\n\t}}\n}\n```", "```go\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tdi.WriteString(\"WithFallback\")\n\t\t// Assuming that fallback.Policy has a ToString() method\n\t\tdi.WriteString(fb.ToString())\n\t\to.Fallback = fb\n\t}}\n}\n```\n\nThis function receives a fallback policy and returns an Option which sets the fallback policy for the RPC call. The function is called with the fallback policy as an argument and returns an Option struct. The Option struct has a function f which sets the fallback policy for the CallOptions and writes the string representation of the fallback policy to the string builder.", "```go\nfunc WithFallback(fb *fallback.Policy) Option {\n\treturn Option{f: func(o *CallOptions, di *strings.Builder) {\n\t\tif !fb.Enable {\n\t\t\treturn\n\t\t}\n\t\tif fb.Type == fallback.ErrorType {\n\t\t\tdi.WriteString(\"WithFallback\")\n\t\t\tdi.WriteByte('(')\n\t\t\tdi.WriteString(fb.Name)\n\t\t\tdi.WriteByte(')')\n\t\t\to.Fallback = fb\n\t\t} else {\n\t\t\tdi.WriteString(\"WithFallbackReportAsFallback\")\n\t\t\tdi.WriteByte('(')\n\t\t\tdi.WriteString(fb.Name)\n\t\t\tdi.WriteByte(')')\n\t\t\to.Fallback = fb\n\t\t}\n\t}}\n}\n```"]}, "kitex-server/option_advanced.go-WithServerBasicInfo": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option_advanced.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package server defines the Options of server\npackage server\n\n// Notice!! This file defines the advanced Options of client, normal user should not use it.\n// It is used for customized extension.\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/acl\"\n\t\"github.com/cloudwego/kitex/pkg/diagnosis\"\n\t\"github.com/cloudwego/kitex/pkg/generic\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/profiler\"\n\t\"github.com/cloudwego/kitex/pkg/proxy\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// WithServerBasicInfo provides initial information for client endpoint in RPCInfo.\n\n\n\n\n\n\n\n\n\n\n// WithDiagnosisService sets the diagnosis service for gathering debug information.\nfunc WithDiagnosisService(ds diagnosis.Service) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDiagnosisService(%+v)\", ds))\n\n\t\to.DebugService = ds\n\t}}\n}\n\n// WithACLRules sets the ACL rules.\nfunc WithACLRules(rules ...acl.RejectFunc) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar names []string\n\t\tfor _, r := range rules {\n\t\t\tnames = append(names, utils.GetFuncName(r))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithACLRules(%+v)\", names))\n\n\t\to.ACLRules = append(o.ACLRules, rules...)\n\t}}\n}\n\n// WithMetaHandler adds a MetaHandler.\nfunc WithMetaHandler(h remote.MetaHandler) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMetaHandler(%T)\", h))\n\n\t\to.MetaHandlers = append(o.MetaHandlers, h)\n\t}}\n}\n\n// WithProxy sets the backward Proxy for server.\nfunc WithProxy(p proxy.ReverseProxy) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithProxy(%T)\", p))\n\n\t\tif o.Proxy != nil {\n\t\t\tpanic(fmt.Errorf(\"reassignment of Proxy is not allowed: %T -> %T\", o.Proxy, p))\n\t\t}\n\t\to.Proxy = p\n\t}}\n}\n\n// WithTransHandlerFactory sets the TransHandlerFactory for server.\nfunc WithTransHandlerFactory(f remote.ServerTransHandlerFactory) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithTransHandlerFactory(%T)\", f))\n\n\t\to.RemoteOpt.SvrHandlerFactory = f\n\t}}\n}\n\n// WithTransServerFactory sets the TransServerFactory for server.\nfunc WithTransServerFactory(f remote.TransServerFactory) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithTransServerFactory(%T)\", f))\n\n\t\to.RemoteOpt.TransServerFactory = f\n\t}}\n}\n\n// WithLimitReporter do report when server limit happen\nfunc WithLimitReporter(r limiter.LimitReporter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithLimitReporter(%T)\", r))\n\n\t\to.Limit.LimitReporter = r\n\t}}\n}\n\n// WithGeneric set Generic type for generic call\nfunc WithGeneric(g generic.Generic) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithGeneric(%T)\", g))\n\n\t\tif g == nil {\n\t\t\tpanic(\"invalid Generic: nil\")\n\t\t}\n\t\to.RemoteOpt.PayloadCodec = g.PayloadCodec()\n\t}}\n}\n\n// WithErrorHandler sets the error handler.\nfunc WithErrorHandler(f func(context.Context, error) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithErrorHandler(%+v)\", utils.GetFuncName(f)))\n\n\t\to.ErrHandle = f\n\t}}\n}\n\n// WithBoundHandler adds remote.BoundHandler for server.\nfunc WithBoundHandler(h remote.BoundHandler) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"AddBoundHandler(%T)\", h))\n\n\t\texist := false\n\t\tswitch handler := h.(type) {\n\t\tcase remote.InboundHandler:\n\t\t\tfor _, inboundHandler := range o.RemoteOpt.Inbounds {\n\t\t\t\tif reflect.DeepEqual(inboundHandler, handler) {\n\t\t\t\t\texist = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase remote.OutboundHandler:\n\t\t\tfor _, outboundHandler := range o.RemoteOpt.Outbounds {\n\t\t\t\tif reflect.DeepEqual(outboundHandler, handler) {\n\t\t\t\t\texist = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// prevent duplication\n\t\tif !exist {\n\t\t\tdoAddBoundHandler(h, o.RemoteOpt)\n\t\t} else {\n\t\t\tklog.Warnf(\"KITEX: BoundHandler already exists, BoundHandler=%v\", h)\n\t\t}\n\t}}\n}\n\n// WithExitSignal adds ExitSignal for server.\nfunc WithExitSignal(f func() <-chan error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"AddExitSignal(%+v)\", utils.GetFuncName(f)))\n\t\to.ExitSignal = f\n\t}}\n}\n\n// WithListener sets the listener for server, the priority is higher than WithServiceAddr\nfunc WithListener(ln net.Listener) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithListener(%+v)\", ln))\n\n\t\to.RemoteOpt.Listener = ln\n\t}}\n}\n\n// WithReusePort sets SO_REUSEPORT on listener, it is only used with Option `WithServiceAddr`.\n// It won't take effect when listener is specified by WithListener.\nfunc WithReusePort(reuse bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReusePort(%+v)\", reuse))\n\n\t\to.RemoteOpt.ReusePort = reuse\n\t}}\n}\n\n// WithSupportedTransportsFunc sets a function which converts supported transports from server option.\n\n\n\n\n\n\n\n\n\n// WithProfiler set a profiler to server.\nfunc WithProfiler(pc profiler.Profiler) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithProfiler(%T{%+v})\", pc, pc))\n\t\to.RemoteOpt.Profiler = pc\n\t}}\n}\n\n// WithProfilerTransInfoTagging set transinfo tagging function to profiler\n// TransInfoTagging extracting tags after TransInfo decoded but before message decoded.\n// At this stage, we can only get msg.TransInfo() and the real message payload is not decoded yet.\n// If upstream is not use TTHeader protocol, we can get nothing here.\n// So if you don't very care about the accuracy of statistics, we recommend to use WithProfilerMessageTagging to extract your custom tags.\nfunc WithProfilerTransInfoTagging(tagging remote.TransInfoTagging) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithProfilerTransInfoTagging(%+v)\", utils.GetFuncName(tagging)))\n\t\tinterTagging := o.RemoteOpt.ProfilerTransInfoTagging\n\t\tif interTagging == nil {\n\t\t\to.RemoteOpt.ProfilerTransInfoTagging = tagging\n\t\t\treturn\n\t\t}\n\t\to.RemoteOpt.ProfilerTransInfoTagging = func(ctx context.Context, msg remote.Message) (context.Context, []string) {\n\t\t\tc, t := tagging(ctx, msg)\n\t\t\tc2, t2 := interTagging(c, msg)\n\t\t\treturn c2, append(t, t2...)\n\t\t}\n\t}}\n}\n\n// WithProfilerMessageTagging set message tagging function to profiler\n// MessageTagging extracting tags after whole decode process finished.\n// At this stage, we can get the rpcInfo from ctx, and full complete message.\nfunc WithProfilerMessageTagging(tagging remote.MessageTagging) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithProfilerMessageTagging(%+v)\", utils.GetFuncName(tagging)))\n\t\tinterTagging := o.RemoteOpt.ProfilerMessageTagging\n\t\tif interTagging == nil {\n\t\t\to.RemoteOpt.ProfilerMessageTagging = tagging\n\t\t\treturn\n\t\t}\n\t\to.RemoteOpt.ProfilerMessageTagging = func(ctx context.Context, msg remote.Message) (context.Context, []string) {\n\t\t\tc, t := tagging(ctx, msg)\n\t\t\tc2, t2 := interTagging(c, msg)\n\t\t\treturn c2, append(t, t2...)\n\t\t}\n\t}}\n}\n\n```\n# Function Signature: func WithServerBasicInfo(ebi *rpcinfo.EndpointBasicInfo) Option {\n# Natural Language Annotations: // WithServerBasicInfo provides initial information for client endpoint in RPCInfo.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithServerBasicInfo provides initial information for client endpoint in RPCInfo.\nfunc WithServerBasicInfo(ebi *rpcinfo.EndpointBasicInfo) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithServerBasicInfo(%+v)\", ebi))\n\n\t\to.BasicInfo = ebi\n\t}}\n}\n```", "```go\nfunc WithServerBasicInfo(ebi *rpcinfo.EndpointBasicInfo) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithServerBasicInfo(%+v)\", ebi))\n\n\t\to.ServerBasicInfo = ebi\n\t}}\n}\n```", "```go\nfunc WithServerBasicInfo(ebi *rpcinfo.EndpointBasicInfo) Option {\n    return Option{F: func(o *internal_server.Options, di *utils.Slice) {\n        di.Push(fmt.Sprintf(\"WithServerBasicInfo(%+v)\", ebi))\n\n        o.ServerBasicInfo = ebi\n    }}\n}\n```"]}, "kitex-server/option_advanced.go-WithSupportedTransportsFunc": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option_advanced.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package server defines the Options of server\npackage server\n\n// Notice!! This file defines the advanced Options of client, normal user should not use it.\n// It is used for customized extension.\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"reflect\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/acl\"\n\t\"github.com/cloudwego/kitex/pkg/diagnosis\"\n\t\"github.com/cloudwego/kitex/pkg/generic\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/profiler\"\n\t\"github.com/cloudwego/kitex/pkg/proxy\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// WithServerBasicInfo provides initial information for client endpoint in RPCInfo.\n\n\n\n\n\n\n\n\n\n\n// WithDiagnosisService sets the diagnosis service for gathering debug information.\nfunc WithDiagnosisService(ds diagnosis.Service) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithDiagnosisService(%+v)\", ds))\n\n\t\to.DebugService = ds\n\t}}\n}\n\n// WithACLRules sets the ACL rules.\nfunc WithACLRules(rules ...acl.RejectFunc) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar names []string\n\t\tfor _, r := range rules {\n\t\t\tnames = append(names, utils.GetFuncName(r))\n\t\t}\n\t\tdi.Push(fmt.Sprintf(\"WithACLRules(%+v)\", names))\n\n\t\to.ACLRules = append(o.ACLRules, rules...)\n\t}}\n}\n\n// WithMetaHandler adds a MetaHandler.\nfunc WithMetaHandler(h remote.MetaHandler) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMetaHandler(%T)\", h))\n\n\t\to.MetaHandlers = append(o.MetaHandlers, h)\n\t}}\n}\n\n// WithProxy sets the backward Proxy for server.\nfunc WithProxy(p proxy.ReverseProxy) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithProxy(%T)\", p))\n\n\t\tif o.Proxy != nil {\n\t\t\tpanic(fmt.Errorf(\"reassignment of Proxy is not allowed: %T -> %T\", o.Proxy, p))\n\t\t}\n\t\to.Proxy = p\n\t}}\n}\n\n// WithTransHandlerFactory sets the TransHandlerFactory for server.\nfunc WithTransHandlerFactory(f remote.ServerTransHandlerFactory) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithTransHandlerFactory(%T)\", f))\n\n\t\to.RemoteOpt.SvrHandlerFactory = f\n\t}}\n}\n\n// WithTransServerFactory sets the TransServerFactory for server.\nfunc WithTransServerFactory(f remote.TransServerFactory) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithTransServerFactory(%T)\", f))\n\n\t\to.RemoteOpt.TransServerFactory = f\n\t}}\n}\n\n// WithLimitReporter do report when server limit happen\nfunc WithLimitReporter(r limiter.LimitReporter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithLimitReporter(%T)\", r))\n\n\t\to.Limit.LimitReporter = r\n\t}}\n}\n\n// WithGeneric set Generic type for generic call\nfunc WithGeneric(g generic.Generic) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithGeneric(%T)\", g))\n\n\t\tif g == nil {\n\t\t\tpanic(\"invalid Generic: nil\")\n\t\t}\n\t\to.RemoteOpt.PayloadCodec = g.PayloadCodec()\n\t}}\n}\n\n// WithErrorHandler sets the error handler.\nfunc WithErrorHandler(f func(context.Context, error) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.Once.OnceOrPanic()\n\t\tdi.Push(fmt.Sprintf(\"WithErrorHandler(%+v)\", utils.GetFuncName(f)))\n\n\t\to.ErrHandle = f\n\t}}\n}\n\n// WithBoundHandler adds remote.BoundHandler for server.\nfunc WithBoundHandler(h remote.BoundHandler) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"AddBoundHandler(%T)\", h))\n\n\t\texist := false\n\t\tswitch handler := h.(type) {\n\t\tcase remote.InboundHandler:\n\t\t\tfor _, inboundHandler := range o.RemoteOpt.Inbounds {\n\t\t\t\tif reflect.DeepEqual(inboundHandler, handler) {\n\t\t\t\t\texist = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\tcase remote.OutboundHandler:\n\t\t\tfor _, outboundHandler := range o.RemoteOpt.Outbounds {\n\t\t\t\tif reflect.DeepEqual(outboundHandler, handler) {\n\t\t\t\t\texist = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// prevent duplication\n\t\tif !exist {\n\t\t\tdoAddBoundHandler(h, o.RemoteOpt)\n\t\t} else {\n\t\t\tklog.Warnf(\"KITEX: BoundHandler already exists, BoundHandler=%v\", h)\n\t\t}\n\t}}\n}\n\n// WithExitSignal adds ExitSignal for server.\nfunc WithExitSignal(f func() <-chan error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"AddExitSignal(%+v)\", utils.GetFuncName(f)))\n\t\to.ExitSignal = f\n\t}}\n}\n\n// WithListener sets the listener for server, the priority is higher than WithServiceAddr\nfunc WithListener(ln net.Listener) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithListener(%+v)\", ln))\n\n\t\to.RemoteOpt.Listener = ln\n\t}}\n}\n\n// WithReusePort sets SO_REUSEPORT on listener, it is only used with Option `WithServiceAddr`.\n// It won't take effect when listener is specified by WithListener.\nfunc WithReusePort(reuse bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReusePort(%+v)\", reuse))\n\n\t\to.RemoteOpt.ReusePort = reuse\n\t}}\n}\n\n// WithSupportedTransportsFunc sets a function which converts supported transports from server option.\n\n\n\n\n\n\n\n\n\n// WithProfiler set a profiler to server.\nfunc WithProfiler(pc profiler.Profiler) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithProfiler(%T{%+v})\", pc, pc))\n\t\to.RemoteOpt.Profiler = pc\n\t}}\n}\n\n// WithProfilerTransInfoTagging set transinfo tagging function to profiler\n// TransInfoTagging extracting tags after TransInfo decoded but before message decoded.\n// At this stage, we can only get msg.TransInfo() and the real message payload is not decoded yet.\n// If upstream is not use TTHeader protocol, we can get nothing here.\n// So if you don't very care about the accuracy of statistics, we recommend to use WithProfilerMessageTagging to extract your custom tags.\nfunc WithProfilerTransInfoTagging(tagging remote.TransInfoTagging) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithProfilerTransInfoTagging(%+v)\", utils.GetFuncName(tagging)))\n\t\tinterTagging := o.RemoteOpt.ProfilerTransInfoTagging\n\t\tif interTagging == nil {\n\t\t\to.RemoteOpt.ProfilerTransInfoTagging = tagging\n\t\t\treturn\n\t\t}\n\t\to.RemoteOpt.ProfilerTransInfoTagging = func(ctx context.Context, msg remote.Message) (context.Context, []string) {\n\t\t\tc, t := tagging(ctx, msg)\n\t\t\tc2, t2 := interTagging(c, msg)\n\t\t\treturn c2, append(t, t2...)\n\t\t}\n\t}}\n}\n\n// WithProfilerMessageTagging set message tagging function to profiler\n// MessageTagging extracting tags after whole decode process finished.\n// At this stage, we can get the rpcInfo from ctx, and full complete message.\nfunc WithProfilerMessageTagging(tagging remote.MessageTagging) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithProfilerMessageTagging(%+v)\", utils.GetFuncName(tagging)))\n\t\tinterTagging := o.RemoteOpt.ProfilerMessageTagging\n\t\tif interTagging == nil {\n\t\t\to.RemoteOpt.ProfilerMessageTagging = tagging\n\t\t\treturn\n\t\t}\n\t\to.RemoteOpt.ProfilerMessageTagging = func(ctx context.Context, msg remote.Message) (context.Context, []string) {\n\t\t\tc, t := tagging(ctx, msg)\n\t\t\tc2, t2 := interTagging(c, msg)\n\t\t\treturn c2, append(t, t2...)\n\t\t}\n\t}}\n}\n\n```\n# Function Signature: func WithSupportedTransportsFunc(f func(option remote.ServerOption) []string) Option {\n# Natural Language Annotations: // WithSupportedTransportsFunc sets a function which converts supported transports from server option.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithSupportedTransportsFunc sets a function which converts supported transports from server option.\nfunc WithSupportedTransportsFunc(f func(option remote.ServerOption) []string) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithSupportedTransportsFunc(%+v)\", utils.GetFuncName(f)))\n\n\t\to.RemoteOpt.SupportedTransportsFunc = f\n\t}}\n}\n```", "```go\n// WithSupportedTransportsFunc sets a function which converts supported transports from server option.\nfunc WithSupportedTransportsFunc(f func(option remote.ServerOption) []string) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithSupportedTransportsFunc(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.SupportedTransportsFunc = f\n\t}}\n}\n```", "```go\n// WithSupportedTransportsFunc sets a function which converts supported transports from server option.\nfunc WithSupportedTransportsFunc(f func(option remote.ServerOption) []string) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithSupportedTransportsFunc(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.SupportedTransportsFunc = f\n\t}}\n}\n```"]}, "kitex-server/option.go-WithMuxTransport": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithMuxTransport() Option {\n# Natural Language Annotations: // WithMuxTransport specifies the transport type to be mux.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithMuxTransport specifies the transport type to be mux.\nfunc WithMuxTransport() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(\"WithMuxTransport()\")\n\n\t\to.RemoteOpt.TransConfig.MuxEnabled = true\n\t}}\n}\n```", "```go\nfunc WithMuxTransport() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(\"WithMuxTransport\")\n\n\t\to.RemoteOpt.TransConfig = trans.NewDefaultTransConfig()\n\t\to.RemoteOpt.TransConfig.TransHandlerFactory = netpollmux.NewTransHandlerFactory()\n\t}}\n}\n```", "```go\nfunc WithMuxTransport() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(\"WithMuxTransport()\")\n\n\t\to.RemoteOpt.TransCfg.TransType = netpollmux.NewTransporter\n\t}}\n}\n```"]}, "kitex-server/option.go-WithMiddleware": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithMiddleware(mw endpoint.Middleware) Option {\n# Natural Language Annotations: // WithMiddleware adds middleware for server to handle request.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%T{%+v})\", mw, mw))\n\t\to.MiddleWares = append(o.MiddleWares, mw)\n\t}}\n}\n```", "```go\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%T{%+v})\", mw, mw))\n\n\t\to.Middlewares = append(o.Middlewares, mw)\n\t}}\n}\n```", "```go\n// WithMiddleware adds middleware for server to handle request.\nfunc WithMiddleware(mw endpoint.Middleware) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddleware(%T{%+v})\", mw, mw))\n\n\t\tif o.Middlewares == nil {\n\t\t\to.Middlewares = []endpoint.Middleware{mw}\n\t\t} else {\n\t\t\to.Middlewares = append(o.Middlewares, mw)\n\t\t}\n\t}}\n}\n```"]}, "kitex-server/option.go-WithMiddlewareBuilder": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder, funcName ...string) Option {\n# Natural Language Annotations: // WithMiddlewareBuilder adds middleware that depend on context for server to handle request\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithMiddlewareBuilder adds middleware that depends on context for server to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder, funcName ...string) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%T{%+v}, %+v)\", mwb, mwb, funcName))\n\n\t\tif o.ServiceInfo == nil {\n\t\t\to.ServiceInfo = &serviceinfo.ServiceInfo{}\n\t\t}\n\t\to.ServiceInfo.MiddleWareBuilder = mwb\n\t\to.ServiceInfo.FuncName = funcName\n\t}}\n}\n```", "```go\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder, funcName ...string) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%+v, %+v)\", mwb, funcName))\n\n\t\tif len(funcName) == 0 {\n\t\t\to.MiddlewareBuilders = append(o.MiddlewareBuilders, endpoint.MiddlewareBuilderOne{\n\t\t\t\tMiddlewareBuilder: mwb,\n\t\t\t\tApplyOnAll:        true,\n\t\t\t})\n\t\t} else {\n\t\t\tfor _, name := range funcName {\n\t\t\t\to.MiddlewareBuilders = append(o.MiddlewareBuilders, endpoint.MiddlewareBuilderOne{\n\t\t\t\t\tMiddlewareBuilder: mwb,\n\t\t\t\t\tFuncName:          name,\n\t\t\t\t})\n\t\t\t}\n\t\t}\n\t}}\n}\n```", "```go\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\nfunc WithMiddlewareBuilder(mwb endpoint.MiddlewareBuilder, funcName ...string) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMiddlewareBuilder(%T{%+v}, %v)\", mwb, mwb, funcName))\n\n\t\tname := \"\"\n\t\tif len(funcName) > 0 {\n\t\t\tname = funcName[0]\n\t\t}\n\t\to.MiddlewareBuilders = append(o.MiddlewareBuilders, endpoint.MiddlewareBuilderFunc(func(next endpoint.Endpoint) endpoint.Endpoint {\n\t\t\treturn mwb(name, next)\n\t\t}))\n\t}}\n}\n```"]}, "kitex-server/option.go-WithExitWaitTime": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithExitWaitTime(timeout time.Duration) Option {\n# Natural Language Annotations: // WithExitWaitTime sets the wait duration for graceful shutdown.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithExitWaitTime(timeout time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithExitWaitTime(%v)\", timeout))\n\n\t\to.ExitWaitTime = timeout\n\t}}\n}\n```", "```go\nfunc WithExitWaitTime(timeout time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithExitWaitTime(%v)\", timeout))\n\n\t\to.ExitWaitTime = timeout\n\t}}\n}\n```", "```go\nfunc WithExitWaitTime(timeout time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithExitWaitTime(%v)\", timeout))\n\n\t\to.ExitWaitTimeout = &timeout\n\t}}\n}\n```"]}, "kitex-server/option.go-WithMaxConnIdleTime": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithMaxConnIdleTime(timeout time.Duration) Option {\n# Natural Language Annotations: // WithMaxConnIdleTime sets the max idle time on connection from clients.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithMaxConnIdleTime(timeout time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMaxConnIdleTime(%v)\", timeout))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetMaxConnIdleTime(timeout)\n\t\to.LockBits |= rpcinfo.BitMaxConnIdleTime\n\t}}\n}\n```", "```go\nfunc WithMaxConnIdleTime(timeout time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RemoteOpt.MaxConnIdleTime = timeout\n\t}}\n}\n```", "```go\nfunc WithMaxConnIdleTime(timeout time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithMaxConnIdleTime(%v)\", timeout))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetMaxConnIdleTime(timeout)\n\t\to.LockBits |= rpcinfo.BitMaxConnIdleTime\n\t}}\n}\n```"]}, "kitex-server/option.go-WithPayloadCodec": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithPayloadCodec(c remote.PayloadCodec) Option {\n# Natural Language Annotations: // WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\nfunc WithPayloadCodec(c remote.PayloadCodec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithPayloadCodec(%+v)\", c))\n\n\t\to.RemoteOpt.PayloadCodec = c\n\t}}\n}\n```", "```go\nfunc WithPayloadCodec(c remote.PayloadCodec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithPayloadCodec(%+v)\", c))\n\n\t\to.RemoteOpt.PayloadCodec = c\n\t}}\n}\n```", "```go\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\nfunc WithPayloadCodec(c remote.PayloadCodec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithPayloadCodec(%+v)\", c))\n\n\t\to.RemoteOpt.PayloadCodec = c\n\t}}\n}\n```"]}, "kitex-server/option.go-WithGRPCInitialWindowSize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCInitialWindowSize(s uint32) Option {\n# Natural Language Annotations: // WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithGRPCInitialWindowSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialWindowSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.InitialWindowSize = s\n\t}}\n}\n```\nPlease note that the actual implementation might differ slightly based on your specific needs and the current state of your codebase.", "```go\nfunc WithGRPCInitialWindowSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialWindowSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.InitialWindowSize = s\n\t}}\n}\n```", "```go\nfunc WithGRPCInitialWindowSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialWindowSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.InitialWindowSize = s\n\t}}\n}\n```\nThis function returns an Option that sets the initial window size for a gRPC stream. The lower bound for window size is 64K and any value smaller than that will be ignored. The value is stored in the RemoteOpt.GRPCCfg.InitialWindowSize field. This function corresponds to the InitialWindowSize ServerOption of gRPC."]}, "kitex-server/option.go-WithGRPCInitialConnWindowSize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCInitialConnWindowSize(s uint32) Option {\n# Natural Language Annotations: // WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithGRPCInitialConnWindowSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialConnWindowSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.InitialConnWindowSize = s\n\t}}\n}\n```", "```go\nfunc WithGRPCInitialConnWindowSize(s uint32) Option {\n\treturn Option{\n\t\tF: func(o *internal_server.Options, di *utils.Slice) {\n\t\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialConnWindowSize(%+v)\", s))\n\t\t\to.RemoteOpt.GRPCCfg.InitialConnWindowSize = s\n\t\t},\n\t}\n}\n```", "```go\nfunc WithGRPCInitialConnWindowSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCInitialConnWindowSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.InitialConnWindowSize = s\n\t}}\n}\n```"]}, "kitex-server/option.go-WithGRPCKeepaliveParams": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCKeepaliveParams(kp grpc.ServerKeepalive) Option {\n# Natural Language Annotations: // WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage server\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n)\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\nfunc WithGRPCKeepaliveParams(kp grpc.ServerKeepalive) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCKeepaliveParams(%+v)\", kp))\n\n\t\to.RemoteOpt.GRPCCfg.KeepaliveParams = kp\n\t}}\n}\n```", "```go\nfunc WithGRPCKeepaliveParams(kp grpc.ServerKeepalive) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCKeepaliveParams(%+v)\", kp))\n\n\t\to.RemoteOpt.GRPCCfg.KeepaliveParams = kp\n\t}}\n}\n```", "```go\nfunc WithGRPCKeepaliveParams(kp grpc.ServerKeepalive) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCKeepaliveParams(%+v)\", kp))\n\n\t\to.RemoteOpt.GRPCCfg.KeepaliveParams = kp\n\t}}\n}\n```"]}, "kitex-server/option.go-WithGRPCKeepaliveEnforcementPolicy": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCKeepaliveEnforcementPolicy(kep grpc.EnforcementPolicy) Option {\n# Natural Language Annotations: // WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithGRPCKeepaliveEnforcementPolicy(kep grpc.EnforcementPolicy) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCKeepaliveEnforcementPolicy(%+v)\", kep))\n\n\t\to.RemoteOpt.GRPCCfg.EnforcementPolicy = kep\n\t}}\n}\n```", "```go\nfunc WithGRPCKeepaliveEnforcementPolicy(kep grpc.EnforcementPolicy) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCKeepaliveEnforcementPolicy(%T{%+v})\", kep, kep))\n\n\t\to.RemoteOpt.GRPCCfg.KeepaliveEnforcementPolicy = kep\n\t}}\n}\n```", "```go\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\nfunc WithGRPCKeepaliveEnforcementPolicy(kep grpc.EnforcementPolicy) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCKeepaliveEnforcementPolicy(%+v)\", kep))\n\n\t\to.RemoteOpt.GRPCCfg.EnforcementPolicy = kep\n\t}}\n}\n```"]}, "kitex-server/option.go-WithGRPCMaxConcurrentStreams": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCMaxConcurrentStreams(n uint32) Option {\n# Natural Language Annotations: // WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithGRPCMaxConcurrentStreams(n uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCMaxConcurrentStreams(%+v)\", n))\n\n\t\to.RemoteOpt.GRPCCfg.MaxConcurrentStreams = n\n\t}}\n}\n```", "```go\nfunc WithGRPCMaxConcurrentStreams(n uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCMaxConcurrentStreams(%+v)\", n))\n\n\t\to.RemoteOpt.GRPCCfg.MaxConcurrentStreams = n\n\t}}\n}\n```", "```go\nfunc WithGRPCMaxConcurrentStreams(n uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCMaxConcurrentStreams(%+v)\", n))\n\n\t\to.RemoteOpt.GRPCCfg.MaxConcurrentStreams = n\n\t}}\n}\n```"]}, "kitex-server/option.go-WithGRPCMaxHeaderListSize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/option.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/cloudwego/localsession/backup\"\n\n\tinternal_server \"github.com/cloudwego/kitex/internal/server\"\n\t\"github.com/cloudwego/kitex/pkg/endpoint\"\n\t\"github.com/cloudwego/kitex/pkg/klog\"\n\t\"github.com/cloudwego/kitex/pkg/limit\"\n\t\"github.com/cloudwego/kitex/pkg/limiter\"\n\t\"github.com/cloudwego/kitex/pkg/registry\"\n\t\"github.com/cloudwego/kitex/pkg/remote\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/protobuf\"\n\t\"github.com/cloudwego/kitex/pkg/remote/codec/thrift\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/netpollmux\"\n\t\"github.com/cloudwego/kitex/pkg/remote/trans/nphttp2/grpc\"\n\t\"github.com/cloudwego/kitex/pkg/rpcinfo\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/pkg/stats\"\n\t\"github.com/cloudwego/kitex/pkg/streaming\"\n\t\"github.com/cloudwego/kitex/pkg/utils\"\n)\n\n// Option is the only way to config server.\ntype Option = internal_server.Option\n\n// Options is used to initialize the server.\ntype Options = internal_server.Options\n\n// A Suite is a collection of Options. It is useful to assemble multiple associated\n// Options as a single one to keep the order or presence in a desired manner.\ntype Suite interface {\n\tOptions() []Option\n}\n\n// WithSuite adds an option suite for server.\nfunc WithSuite(suite Suite) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tvar nested struct {\n\t\t\tSuite   string\n\t\t\tOptions utils.Slice\n\t\t}\n\t\tnested.Suite = fmt.Sprintf(\"%T(%+v)\", suite, suite)\n\n\t\tfor _, op := range suite.Options() {\n\t\t\top.F(o, &nested.Options)\n\t\t}\n\t\tdi.Push(nested)\n\t}}\n}\n\n// WithMuxTransport specifies the transport type to be mux.\n\n\n\n\n\n\n\n\n\n// WithMiddleware adds middleware for server to handle request.\n\n\n\n\n\n\n\n\n\n\n// WithMiddlewareBuilder adds middleware that depend on context for server to handle request\n\n\n\n\n\n\n\n// WithReadWriteTimeout sets the read/write timeout on network.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithReadWriteTimeout(d time.Duration) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithReadWriteTimeout(%v)\", d))\n\n\t\trpcinfo.AsMutableRPCConfig(o.Configs).SetReadWriteTimeout(d)\n\t\to.LockBits |= rpcinfo.BitReadWriteTimeout\n\t}}\n}\n\n// WithLogger sets the Logger for kitex server.\n// Deprecated: server uses the global klog.DefaultLogger.\nfunc WithLogger(logger klog.FormatLogger) Option {\n\tpanic(\"server.WithLogger is deprecated\")\n}\n\n// WithExitWaitTime sets the wait duration for graceful shutdown.\n\n\n\n\n\n\n\n\n\n// WithMaxConnIdleTime sets the max idle time on connection from clients.\n\n\n\n\n\n\n\n\n\n// WithLimit sets the limitation of concurrent connections or max QPS.\n// IMPORTANT: this option is not stable, and will be changed or removed in the future!!!\n// We don't promise compatibility for this option in future versions!!!\nfunc WithLimit(lim *limit.Option) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLimit(%+v)\", lim))\n\n\t\to.Limit.Limits = lim\n\t}}\n}\n\n// WithConnectionLimiter sets the limiter of connections.\n// If both WithLimit and WithConnectionLimiter are called, only the latter will take effect.\nfunc WithConnectionLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConnectionLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithQPSLimiter sets the limiter of max QPS.\n// If both WithLimit and WithQPSLimiter are called, only the latter will take effect.\nfunc WithQPSLimiter(qpsLimit limiter.RateLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithQPSLimiter(%T{%+v})\", qpsLimit, qpsLimit))\n\n\t\to.Limit.QPSLimit = qpsLimit\n\t}}\n}\n\n// WithTracer adds a tracer to server.\nfunc WithTracer(c stats.Tracer) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithTracer(%T{%+v})\", c, c))\n\n\t\tif o.TracerCtl == nil {\n\t\t\to.TracerCtl = &rpcinfo.TraceController{}\n\t\t}\n\t\to.TracerCtl.Append(c)\n\t}}\n}\n\n// WithStatsLevel sets the stats level for server.\nfunc WithStatsLevel(level stats.Level) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithStatsLevel(%+v)\", level))\n\t\tl := level\n\t\to.StatsLevel = &l\n\t}}\n}\n\n// WithServiceAddr sets the listen address for server.\nfunc WithServiceAddr(addr net.Addr) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithServiceAddr(%+v)\", addr))\n\n\t\to.RemoteOpt.Address = addr\n\t}}\n}\n\n// WithCodec to set a codec that handle other protocols which not support by kitex\nfunc WithCodec(c remote.Codec) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithCodec(%+v)\", c))\n\n\t\to.RemoteOpt.Codec = c\n\t}}\n}\n\n// WithPayloadCodec to set a payloadCodec that handle other payload which not support by kitex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WithRegistry to set a Registry to register service\nfunc WithRegistry(r registry.Registry) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistry(%T)\", r))\n\n\t\to.Registry = r\n\t}}\n}\n\n// WithRegistryInfo to set Registry Info if needed.\n// It is used to add customized info and is suggested to use with WithRegistry option.\nfunc WithRegistryInfo(info *registry.Info) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithRegistryInfo(%+v)\", info))\n\n\t\to.RegistryInfo = info\n\t}}\n}\n\n// WithGRPCWriteBufferSize determines how much data can be batched before doing a write on the wire.\n// The corresponding memory allocation for this buffer will be twice the size to keep syscalls low.\n// The default value for this buffer is 32KB.\n// Zero will disable the write buffer such that each write will be on underlying connection.\n// Note: A Send call may not directly translate to a write.\n// It corresponds to the WriteBufferSize ServerOption of gRPC.\nfunc WithGRPCWriteBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCWriteBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.WriteBufferSize = s\n\t}}\n}\n\n// WithGRPCReadBufferSize lets you set the size of read buffer, this determines how much data can be read at most\n// for one read syscall.\n// The default value for this buffer is 32KB.\n// Zero will disable read buffer for a connection so data framer can access the underlying\n// conn directly.\n// It corresponds to the ReadBufferSize ServerOption of gRPC.\nfunc WithGRPCReadBufferSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCReadBufferSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.ReadBufferSize = s\n\t}}\n}\n\n// WithGRPCInitialWindowSize returns a Option that sets window size for stream.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCInitialConnWindowSize returns an Option that sets window size for a connection.\n// The lower bound for window size is 64K and any value smaller than that will be ignored.\n// It corresponds to the InitialConnWindowSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveParams returns an Option that sets keepalive and max-age parameters for the server.\n// It corresponds to the KeepaliveParams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n\n\n\n\n// WithGRPCKeepaliveEnforcementPolicy returns an Option that sets keepalive enforcement policy for the server.\n// It corresponds to the KeepaliveEnforcementPolicy ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxConcurrentStreams returns an Option that will apply a limit on the number\n// of concurrent streams to each ServerTransport.\n// It corresponds to the MaxConcurrentStreams ServerOption of gRPC.\n\n\n\n\n\n\n\n\n// WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\n\n\n\n\n\n\n\n\nfunc WithGRPCUnknownServiceHandler(f func(ctx context.Context, methodName string, stream streaming.Stream) error) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCUnknownServiceHandler(%+v)\", utils.GetFuncName(f)))\n\t\to.RemoteOpt.GRPCUnknownServiceHandler = f\n\t}}\n}\n\n// Deprecated: Use WithConnectionLimiter instead.\nfunc WithConcurrencyLimiter(conLimit limiter.ConcurrencyLimiter) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithConcurrencyLimiter(%T{%+v})\", conLimit, conLimit))\n\n\t\to.Limit.ConLimit = conLimit\n\t}}\n}\n\n// WithContextBackup enables local-session to backup kitex server's context,\n// in case of user don't correctly pass context into next RPC call.\n//   - enable means enable context backup\n//   - async means backup session will propagate to children goroutines, otherwise it only works on the same goroutine handler\nfunc WithContextBackup(enable, async bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithLocalSession({%+v, %+v})\", enable, async))\n\t\to.BackupOpt = backup.DefaultOptions()\n\t\to.BackupOpt.Enable = enable\n\t\to.BackupOpt.EnableImplicitlyTransmitAsync = async\n\t}}\n}\n\n// WithRefuseTrafficWithoutServiceName returns an Option that only accepts traffics with service name.\n// This is used for a server with multi services and is one of the options to avoid a server startup error\n// when having conflicting method names between services without specifying a fallback service for the method.\nfunc WithRefuseTrafficWithoutServiceName() Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.RefuseTrafficWithoutServiceName = true\n\t}}\n}\n\n// WithEnableContextTimeout enables handler timeout.\n// Available since Kitex >= v0.9.0\n// If enabled, a timeout middleware will be added to the beginning of the middleware chain.\n// The timeout value will be read from RPCInfo.Config().RPCTimeout(), which can be set by a custom MetaHandler\n// NOTE:\n// If there's an error (excluding BizStatusError) returned by server handler or any middleware, cancel will be\n// called automatically.\n//\n// For an opensource Kitex user, TTHeader has builtin support of timeout-passing (not enabled by default):\n//   - Client side: add the following NewClient options for enabling TTHeader and setting the timeout to TTHeader\n//     client.WithTransportProtocol(transport.TTHeader),\n//     client.WithMetaHandler(transmeta.ClientTTHeaderHandler),\n//   - Server side: add the following NewServer options for reading from TTHeader and enable timeout control\n//     server.WithMetaHandler(transmeta.ServerTTHeaderHandler)\n//     server.WithEnableContextTimeout(true)\n//\n// For requests on GRPC transport, a deadline will be added to the context if the header 'grpc-timeout' is positive,\n// so there's no need to use this option.\nfunc WithEnableContextTimeout(enable bool) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\to.EnableContextTimeout = enable\n\t}}\n}\n\n```\n# Function Signature: func WithGRPCMaxHeaderListSize(s uint32) Option {\n# Natural Language Annotations: // WithGRPCMaxHeaderListSize returns a ServerOption that sets the max (uncompressed) size\n// of header list that the server is prepared to accept.\n// It corresponds to the MaxHeaderListSize ServerOption of gRPC.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc WithGRPCMaxHeaderListSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCMaxHeaderListSize(%+v)\", s))\n\t\to.RemoteOpt.GRPCCfg.MaxHeaderListSize = s\n\t}}\n}\n```", "```go\nfunc WithGRPCMaxHeaderListSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCMaxHeaderListSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.MaxHeaderListSize = s\n\t}}\n}\n```", "```go\nfunc WithGRPCMaxHeaderListSize(s uint32) Option {\n\treturn Option{F: func(o *internal_server.Options, di *utils.Slice) {\n\t\tdi.Push(fmt.Sprintf(\"WithGRPCMaxHeaderListSize(%+v)\", s))\n\n\t\to.RemoteOpt.GRPCCfg.MaxHeaderListSize = s\n\t}}\n}\n```"]}, "kitex-server/genericserver/server.go-NewServerWithServiceInfo": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/kitex/server/genericserver/server.go:\n```\n/*\n * Copyright 2021 CloudWeGo Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\n// Package genericserver ...\npackage genericserver\n\nimport (\n\t\"github.com/cloudwego/kitex/pkg/generic\"\n\t\"github.com/cloudwego/kitex/pkg/serviceinfo\"\n\t\"github.com/cloudwego/kitex/server\"\n)\n\n// NewServer creates a generic server with the given handler and options.\nfunc NewServer(handler generic.Service, g generic.Generic, opts ...server.Option) server.Server {\n\tsvcInfo := generic.ServiceInfo(g.PayloadCodecType())\n\treturn NewServerWithServiceInfo(handler, g, svcInfo, opts...)\n}\n\n// NewServerWithServiceInfo creates a generic server with the given handler, serviceInfo and options.\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func NewServerWithServiceInfo(handler generic.Service, g generic.Generic, svcInfo *serviceinfo.ServiceInfo, opts ...server.Option) server.Server {\n# Natural Language Annotations: // NewServerWithServiceInfo creates a generic server with the given handler, serviceInfo and options.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewServerWithServiceInfo(handler generic.Service, g generic.Generic, svcInfo *serviceinfo.ServiceInfo, opts ...server.Option) server.Server {\n\tsvc := g.NewService(handler, svcInfo)\n\tnetwork := g.Network()\n\taddress := g.Address()\n\ttransServer := server.NewServer(network, address, svc, opts...)\n\treturn transServer\n}\n```", "```go\nfunc NewServerWithServiceInfo(handler generic.Service, g generic.Generic, svcInfo *serviceinfo.ServiceInfo, opts ...server.Option) server.Server {\n\treturn server.NewServer(handler, svcInfo, opts...)\n}\n```", "```go\n// NewServerWithServiceInfo creates a generic server with the given handler, serviceInfo and options.\nfunc NewServerWithServiceInfo(handler generic.Service, g generic.Generic, svcInfo *serviceinfo.ServiceInfo, opts ...server.Option) server.Server {\n\tgenericOpts := []generic.Option{\n\t\tgeneric.WithServiceInfo(svcInfo),\n\t}\n\n\t// Apply server options to the generic options\n\tfor _, opt := range opts {\n\t\topt(&genericOpts)\n\t}\n\n\treturn generic.NewServer(handler, g, genericOpts...)\n}\n```"]}, "weaver-validate.go-validateRegistrations": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/validate.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage weaver\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"unicode\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/reflection\"\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"golang.org/x/exp/slices\"\n)\n\n// validateRegistrations validates the provided registrations, returning an\n// diagnostic error if they are invalid. Note that some validation is performed\n// by 'weaver generate', but because users can run a Service Weaver app after\n// forgetting to run 'weaver generate', some checks have to be done at runtime.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// isValidListenerName returns whether the provided name is a valid\n// weaver.Listener name.\nfunc isValidListenerName(name string) bool {\n\t// We allow valid Go identifiers [1]. This code is taken from [2].\n\t//\n\t// [1]: https://go.dev/ref/spec#Identifiers\n\t// [2]: https://cs.opensource.google/go/go/+/refs/tags/go1.20.6:src/go/token/token.go;l=331-341;drc=19309779ac5e2f5a2fd3cbb34421dafb2855ac21\n\tif name == \"\" {\n\t\treturn false\n\t}\n\tfor i, c := range name {\n\t\tif !unicode.IsLetter(c) && c != '_' && (i == 0 || !unicode.IsDigit(c)) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n```\n# Function Signature: func validateRegistrations(regs []*codegen.Registration) error {\n# Natural Language Annotations: // validateRegistrations validates the provided registrations, returning an\n// diagnostic error if they are invalid. Note that some validation is performed\n// by 'weaver generate', but because users can run a Service Weaver app after\n// forgetting to run 'weaver generate', some checks have to be done at runtime.\nPlease return the generated code surrounded by ```\n", "response": ["!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"]}, "weaver-sim/generators.go-Range": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/sim/generators.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage sim\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n)\n\n// TODO(mwhittaker): Add more generators as needed.\n\n// Booleans\n\n// Flip returns a [Generator] that returns true with probability p. Flip\n// panics if p is not in the range [0, 1].\nfunc Flip(p float64) Generator[bool] {\n\tif p < 0 || p > 1 {\n\t\tpanic(fmt.Errorf(\"Flip: probability p = %f not in range [0, 1]\", p))\n\t}\n\treturn generatorFunc[bool](func(r *rand.Rand) bool {\n\t\treturn r.Float64() <= p\n\t})\n}\n\n// Numerics\n\n// NonNegativeInt returns a [Generator] that returns non-negative integers.\n// Note that NonNegativeInt does not return all numbers. Instead, it biases\n// towards numbers closer to zero and other pathological numbers that are more\n// likely to induce bugs (e.g., math.MaxInt).\nfunc NonNegativeInt() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(0, 10)},\n\t\t{100, Range(10, 100)},\n\t\t{100, Range(100, 1000)},\n\t\t{100, Range(1000, 10_000)},\n\t\t{100, Range(10_000, 100_000)},\n\t\t{100, Range(100_000, 1_000_000)},\n\t\t{100, Range(1_000_000, 1_000_000_000)},\n\t\t{100, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{100, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Int returns a [Generator] that returns integers. Note that Int does not\n// return all integers equiprobably. Instead, it biases towards numbers closer\n// to zero and other pathological numbers that are more likely to induce bugs\n// (e.g., math.MaxInt, math.MinInt).\nfunc Int() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(-10, 10)},\n\t\t{50, Range(10, 100)},\n\t\t{50, Range(-100, -10)},\n\t\t{50, Range(100, 1000)},\n\t\t{50, Range(-1000, -100)},\n\t\t{50, Range(1000, 10_000)},\n\t\t{50, Range(-10_000, -1000)},\n\t\t{50, Range(10_000, 100_000)},\n\t\t{50, Range(-100_000, -10_000)},\n\t\t{50, Range(100_000, 1_000_000)},\n\t\t{50, Range(-1_000_000, -100_000)},\n\t\t{50, Range(1_000_000, 1_000_000_000)},\n\t\t{50, Range(-1_000_000_000, -1_000_000)},\n\t\t{50, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{50, Range(-1_000_000_000_000, -1_000_000_000)},\n\t\t{50, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{50, Range(math.MinInt, -1_000_000_000_000)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MinInt, math.MinInt+1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MinInt32-1, math.MinInt32, math.MinInt32+1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MinInt16-1, math.MinInt16, math.MinInt16+1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t\tmath.MinInt8-1, math.MinInt8, math.MinInt8+1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Float64 returns a [Generator] that returns 64-bit floats. Note that Float64\n// does not return all floats equiprobably. Instead, it biases towards numbers\n// closer to zero and other pathological numbers that are more likely to induce\n// bugs (e.g., NaN, infinity, -infinity, -0).\nfunc Float64() Generator[float64] {\n\tpathologies := OneOf(\n\t\tmath.NaN(),           // NaN\n\t\tmath.Inf(1),          // infinity\n\t\tmath.Inf(-1),         // -infinity\n\t\tmath.Copysign(0, -1), // -0\n\t)\n\treturn generatorFunc[float64](func(r *rand.Rand) float64 {\n\t\tif r.Intn(1000) == 0 {\n\t\t\t// 0.1% of the time, return a pathological float.\n\t\t\treturn pathologies.Generate(r)\n\t\t}\n\t\tconst stdev = 1e6\n\t\treturn r.NormFloat64() * stdev\n\t})\n}\n\n// Rune returns a [Generator] that returns runes equiprobably.\nfunc Rune() Generator[rune] {\n\treturn generatorFunc[rune](func(r *rand.Rand) rune {\n\t\t// Note that rune is an alias for int32.\n\t\treturn rune(r.Intn(math.MaxInt32 + 1))\n\t})\n}\n\n// Byte returns a [Generator] that returns bytes equiprobably.\nfunc Byte() Generator[byte] {\n\treturn generatorFunc[byte](func(r *rand.Rand) byte {\n\t\tvar buf [1]byte\n\t\tr.Read(buf[:])\n\t\treturn buf[0]\n\t})\n}\n\n// Range returns a [Generator] that returns integers equiprobably in the range\n// [low, high). Range panics if low >= high.\n\n\n\n\n\n\n\n\n\n\n// Strings\n\n// String returns a [Generator] that returns moderately sized readable strings,\n// with a bias towards smaller strings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Slices and Maps\n\n// Slice returns a [Generator] that returns slices of T. The size and contents\n// of the generated slices are determined by the provided generators.\nfunc Slice[T any](size Generator[int], values Generator[T]) Generator[[]T] {\n\treturn generatorFunc[[]T](func(r *rand.Rand) []T {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Slice: negative size %d\", n))\n\t\t}\n\t\txs := make([]T, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\txs[i] = values.Generate(r)\n\t\t}\n\t\treturn xs\n\t})\n}\n\n// Map returns a [Generator] that returns maps from K to V. The size and\n// contents of the the generated maps are determined by the provided\n// generators.\nfunc Map[K comparable, V any](size Generator[int], keys Generator[K], values Generator[V]) Generator[map[K]V] {\n\treturn generatorFunc[map[K]V](func(r *rand.Rand) map[K]V {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Map: negative size %d\", n))\n\t\t}\n\t\tkvs := make(map[K]V, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\tkvs[keys.Generate(r)] = values.Generate(r)\n\t\t}\n\t\treturn kvs\n\t})\n}\n\n// Combinators\n\n// OneOf returns a [Generator] that returns one of the provided values\n// equiprobably. OneOf panics if no values are provided.\n\n\n\n\n\n\n\n\n\n// Weight returns a [Generator] that generates values using the provided\n// generators. A generator is chosen with probability proportional to its\n// weight. For example, given the following choices:\n//\n//   - Weighted{1.0, OneOf(\"a\")}\n//   - Weighted{2.0, OneOf(\"b\")}\n//\n// Weight returns \"b\" twice as often as it returns \"a\". Note that the provided\n// weights do not have to sum to 1.\n//\n// Weight panics if no choices are provided, if any weight is negative, or if\n// the sum of all weight is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype Weighted[T any] struct {\n\tWeight float64\n\tGen    Generator[T]\n}\n\n// Filter returns a [Generator] that returns values from the provided generator\n// that satisfy the provided predicate.\nfunc Filter[T any](gen Generator[T], predicate func(T) bool) Generator[T] {\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\tfor {\n\t\t\tx := gen.Generate(r)\n\t\t\tif predicate(x) {\n\t\t\t\treturn x\n\t\t\t}\n\t\t}\n\t})\n}\n\n// generatorFunc[T] is an instance of Generator[T] that uses the provided\n// function to generate values.\ntype generatorFunc[T any] func(r *rand.Rand) T\n\n// Generate implements the Generator interface.\nfunc (g generatorFunc[T]) Generate(r *rand.Rand) T {\n\treturn g(r)\n}\n\n```\n# Function Signature: func Range(low, high int) Generator[int] {\n# Natural Language Annotations: // Range returns a [Generator] that returns integers equiprobably in the range\n// [low, high). Range panics if low >= high.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc Range(low, high int) Generator[int] {\n\tif low >= high {\n\t\tpanic(fmt.Errorf(\"Range: low >= high (%d >= %d)\", low, high))\n\t}\n\treturn generatorFunc[int](func(r *rand.Rand) int {\n\t\treturn r.Intn(high-low) + low\n\t})\n}\n```", "```go\nfunc Range(low, high int) Generator[int] {\n\tif low >= high {\n\t\tpanic(fmt.Errorf(\"Range: low %d is not less than high %d\", low, high))\n\t}\n\treturn generatorFunc[int](func(r *rand.Rand) int {\n\t\treturn r.Intn(high-low) + low\n\t})\n}\n```\n\nThis code defines a function called `Range` that takes two integers `low` and `high`, and returns a `Generator[int]`. The function first checks if `low` is greater than or equal to `high`, and if so, it panics with an error message. Then it creates a `generatorFunc[int]` that uses the provided function to generate integers in the range [low, high). The `Generate` method of the generator uses the `Intn` function from the `rand.Rand` package to generate a random integer in the range [0, high-low), then adds `low` to the result to shift it into the range [low, high).", "Here is the code for the `Range` function:\n\n```go\nfunc Range(low, high int) Generator[int] {\n\tif low >= high {\n\t\tpanic(fmt.Errorf(\"Range: low = %d >= high = %d\", low, high))\n\t}\n\treturn generatorFunc[int](func(r *rand.Rand) int {\n\t\treturn r.Intn(high-low) + low\n\t})\n}\n```\n\nThis function generates integers in the range `[low, high)` with equal probability. It uses the `rand.Rand.Intn` method which generates a random integer in the range `[0, n)`. This function is used to generate the random index, which is then added to `low` to get the actual integer in the desired range."]}, "weaver-sim/generators.go-String": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/sim/generators.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage sim\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n)\n\n// TODO(mwhittaker): Add more generators as needed.\n\n// Booleans\n\n// Flip returns a [Generator] that returns true with probability p. Flip\n// panics if p is not in the range [0, 1].\nfunc Flip(p float64) Generator[bool] {\n\tif p < 0 || p > 1 {\n\t\tpanic(fmt.Errorf(\"Flip: probability p = %f not in range [0, 1]\", p))\n\t}\n\treturn generatorFunc[bool](func(r *rand.Rand) bool {\n\t\treturn r.Float64() <= p\n\t})\n}\n\n// Numerics\n\n// NonNegativeInt returns a [Generator] that returns non-negative integers.\n// Note that NonNegativeInt does not return all numbers. Instead, it biases\n// towards numbers closer to zero and other pathological numbers that are more\n// likely to induce bugs (e.g., math.MaxInt).\nfunc NonNegativeInt() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(0, 10)},\n\t\t{100, Range(10, 100)},\n\t\t{100, Range(100, 1000)},\n\t\t{100, Range(1000, 10_000)},\n\t\t{100, Range(10_000, 100_000)},\n\t\t{100, Range(100_000, 1_000_000)},\n\t\t{100, Range(1_000_000, 1_000_000_000)},\n\t\t{100, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{100, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Int returns a [Generator] that returns integers. Note that Int does not\n// return all integers equiprobably. Instead, it biases towards numbers closer\n// to zero and other pathological numbers that are more likely to induce bugs\n// (e.g., math.MaxInt, math.MinInt).\nfunc Int() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(-10, 10)},\n\t\t{50, Range(10, 100)},\n\t\t{50, Range(-100, -10)},\n\t\t{50, Range(100, 1000)},\n\t\t{50, Range(-1000, -100)},\n\t\t{50, Range(1000, 10_000)},\n\t\t{50, Range(-10_000, -1000)},\n\t\t{50, Range(10_000, 100_000)},\n\t\t{50, Range(-100_000, -10_000)},\n\t\t{50, Range(100_000, 1_000_000)},\n\t\t{50, Range(-1_000_000, -100_000)},\n\t\t{50, Range(1_000_000, 1_000_000_000)},\n\t\t{50, Range(-1_000_000_000, -1_000_000)},\n\t\t{50, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{50, Range(-1_000_000_000_000, -1_000_000_000)},\n\t\t{50, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{50, Range(math.MinInt, -1_000_000_000_000)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MinInt, math.MinInt+1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MinInt32-1, math.MinInt32, math.MinInt32+1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MinInt16-1, math.MinInt16, math.MinInt16+1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t\tmath.MinInt8-1, math.MinInt8, math.MinInt8+1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Float64 returns a [Generator] that returns 64-bit floats. Note that Float64\n// does not return all floats equiprobably. Instead, it biases towards numbers\n// closer to zero and other pathological numbers that are more likely to induce\n// bugs (e.g., NaN, infinity, -infinity, -0).\nfunc Float64() Generator[float64] {\n\tpathologies := OneOf(\n\t\tmath.NaN(),           // NaN\n\t\tmath.Inf(1),          // infinity\n\t\tmath.Inf(-1),         // -infinity\n\t\tmath.Copysign(0, -1), // -0\n\t)\n\treturn generatorFunc[float64](func(r *rand.Rand) float64 {\n\t\tif r.Intn(1000) == 0 {\n\t\t\t// 0.1% of the time, return a pathological float.\n\t\t\treturn pathologies.Generate(r)\n\t\t}\n\t\tconst stdev = 1e6\n\t\treturn r.NormFloat64() * stdev\n\t})\n}\n\n// Rune returns a [Generator] that returns runes equiprobably.\nfunc Rune() Generator[rune] {\n\treturn generatorFunc[rune](func(r *rand.Rand) rune {\n\t\t// Note that rune is an alias for int32.\n\t\treturn rune(r.Intn(math.MaxInt32 + 1))\n\t})\n}\n\n// Byte returns a [Generator] that returns bytes equiprobably.\nfunc Byte() Generator[byte] {\n\treturn generatorFunc[byte](func(r *rand.Rand) byte {\n\t\tvar buf [1]byte\n\t\tr.Read(buf[:])\n\t\treturn buf[0]\n\t})\n}\n\n// Range returns a [Generator] that returns integers equiprobably in the range\n// [low, high). Range panics if low >= high.\n\n\n\n\n\n\n\n\n\n\n// Strings\n\n// String returns a [Generator] that returns moderately sized readable strings,\n// with a bias towards smaller strings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Slices and Maps\n\n// Slice returns a [Generator] that returns slices of T. The size and contents\n// of the generated slices are determined by the provided generators.\nfunc Slice[T any](size Generator[int], values Generator[T]) Generator[[]T] {\n\treturn generatorFunc[[]T](func(r *rand.Rand) []T {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Slice: negative size %d\", n))\n\t\t}\n\t\txs := make([]T, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\txs[i] = values.Generate(r)\n\t\t}\n\t\treturn xs\n\t})\n}\n\n// Map returns a [Generator] that returns maps from K to V. The size and\n// contents of the the generated maps are determined by the provided\n// generators.\nfunc Map[K comparable, V any](size Generator[int], keys Generator[K], values Generator[V]) Generator[map[K]V] {\n\treturn generatorFunc[map[K]V](func(r *rand.Rand) map[K]V {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Map: negative size %d\", n))\n\t\t}\n\t\tkvs := make(map[K]V, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\tkvs[keys.Generate(r)] = values.Generate(r)\n\t\t}\n\t\treturn kvs\n\t})\n}\n\n// Combinators\n\n// OneOf returns a [Generator] that returns one of the provided values\n// equiprobably. OneOf panics if no values are provided.\n\n\n\n\n\n\n\n\n\n// Weight returns a [Generator] that generates values using the provided\n// generators. A generator is chosen with probability proportional to its\n// weight. For example, given the following choices:\n//\n//   - Weighted{1.0, OneOf(\"a\")}\n//   - Weighted{2.0, OneOf(\"b\")}\n//\n// Weight returns \"b\" twice as often as it returns \"a\". Note that the provided\n// weights do not have to sum to 1.\n//\n// Weight panics if no choices are provided, if any weight is negative, or if\n// the sum of all weight is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype Weighted[T any] struct {\n\tWeight float64\n\tGen    Generator[T]\n}\n\n// Filter returns a [Generator] that returns values from the provided generator\n// that satisfy the provided predicate.\nfunc Filter[T any](gen Generator[T], predicate func(T) bool) Generator[T] {\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\tfor {\n\t\t\tx := gen.Generate(r)\n\t\t\tif predicate(x) {\n\t\t\t\treturn x\n\t\t\t}\n\t\t}\n\t})\n}\n\n// generatorFunc[T] is an instance of Generator[T] that uses the provided\n// function to generate values.\ntype generatorFunc[T any] func(r *rand.Rand) T\n\n// Generate implements the Generator interface.\nfunc (g generatorFunc[T]) Generate(r *rand.Rand) T {\n\treturn g(r)\n}\n\n```\n# Function Signature: func String() Generator[string] {\n# Natural Language Annotations: // Strings\n// String returns a [Generator] that returns moderately sized readable strings,\n// with a bias towards smaller strings.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc String() Generator[string] {\n\treturn generatorFunc[string](func(r *rand.Rand) string {\n\t\tlength := r.Intn(20) + 1\n\t\tbuf := make([]byte, length)\n\t\tfor i := range buf {\n\t\t\t// Generate a printable ASCII character.\n\t\t\tbuf[i] = byte(r.Intn(94) + 33)\n\t\t}\n\t\treturn string(buf)\n\t})\n}\n```", "```go\nfunc String() Generator[string] {\n\treturn generatorFunc[string](func(r *rand.Rand) string {\n\t\t// We use a normal distribution with a standard deviation of 20 to\n\t\t// determine the length of the string. The mean is set to 50 to create\n\t\t// a bias towards smaller strings.\n\t\tlength := int(math.Abs(r.NormFloat64()*20 + 50))\n\t\tvar buf bytes.Buffer\n\t\tfor i := 0; i < length; i++ {\n\t\t\tbuf.WriteRune(Rune().Generate(r))\n\t\t}\n\t\treturn buf.String()\n\t})\n}\n```\n\nThis code defines a function `String` that returns a `Generator[string]`. The generated strings are random sequences of runes, biased towards smaller strings. The length of the string is determined by a normal distribution with a mean length of 50 and a standard deviation of 20.", "```go\npackage sim\n\nimport (\n\t\"fmt\"\n\t\"math/rand\"\n\t\"strings\"\n)\n\nfunc String() Generator[string] {\n\treturn generatorFunc[string](func(r *rand.Rand) string {\n\t\tsize := Range(1, 100)\n\t\tchars := Slice(size,\n\t\t\tFilter(\n\t\t\t\tRune(),\n\t\t\t\tfunc(r rune) bool {\n\t\t\t\t\treturn r >= 32 && r <= 126 // printable ASCII\n\t\t\t\t},\n\t\t\t),\n\t\t)\n\t\tchoices := []Weighted[Generator[string]]{\n\t\t\t{10, chars},\n\t\t\t{10, Map(size, Slice(size, Rune()), Slice(size, Rune()))},\n\t\t\t{5, Map(size, Slice(size, Rune()), chars)},\n\t\t\t{5, Map(size, chars, Slice(size, Rune()))},\n\t\t}\n\t\treturn Weight(choices).Generate(r)\n\t})\n}\n\n// Range returns a [Generator] that returns integers in the range [low, high).\nfunc Range(low, high int) Generator[int] {\n\treturn generatorFunc[int](func(r *rand.Rand) int {\n\t\treturn r.Intn(high-low) + low\n\t})\n}\n\n// OneOf returns a [Generator] that returns one of the provided values\n// equiprobably. OneOf panics if no values are provided.\nfunc OneOf[T any](vs ...T) Generator[T] {\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\tif len(vs) == 0 {\n\t\t\tpanic(\"OneOf: no values provided\")\n\t\t}\n\t\treturn vs[r.Intn(len(vs))]\n\t})\n}\n\n// Weight returns a [Generator] that generates values using the provided\n// generators. A generator is chosen with probability proportional to its\n// weight. For example, given the following choices:\n//\n//   - Weighted{1.0, OneOf(\"a\")}\n//   - Weighted{2.0, OneOf(\"b\")}\n//\n// Weight returns \"b\" twice as often as it returns \"a\". Note that the provided\n// weights do not have to sum to 1.\nfunc Weight[T any](choices []Weighted[Generator[T]]) Generator[T] {\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\tvar totalWeight float64\n\t\tfor _, c := range choices {\n\t\t\tif c.Weight < 0 {\n\t\t\t\tpanic(fmt.Errorf(\"Weight: negative weight %f\", c.Weight))\n\t\t\t}\n\t\t\ttotalWeight += c.Weight\n\t\t}\n\t\tif totalWeight == 0 {\n\t\t\tpanic(\"Weight: total weight is zero\")\n\t\t}\n\t\tv, _ := WeightedChoice(choices, totalWeight, r.Float64())\n\t\treturn v.Gen.Generate(r)\n\t})\n}\n\n// WeightedChoice picks a choice with probability proportional to its weight.\nfunc WeightedChoice[T any](choices []Weighted[T], totalWeight float64, r float64) (choice Weighted[T], remainingWeight float64) {\n\tr *= totalWeight\n\tfor _, c := range choices {\n\t\tif r < c.Weight {\n\t\t\treturn c, totalWeight - c.Weight\n\t\t}\n\t\tr -= c.Weight\n\t}\n\t// This should be unreachable.\n\tpanic(\"WeightedChoice: internal error\")\n}\n\n// Slice returns a [Generator] that returns slices of T. The size and contents\n// of the generated slices are determined by the provided generators.\n\n// Map returns a [Generator] that returns maps from K to V. The size and\n// contents of the the generated maps are determined by the provided\n// generators.\n\n// Filter returns a [Generator] that returns values from the provided generator\n// that satisfy the provided predicate.\n\n// generatorFunc[T] is an instance of Generator[T] that uses the provided\n// function to generate values.\n\n// Generate implements the Generator interface.\n\n// Weighted[T] is a pair of a weight and a generator.\ntype Weighted[T any] struct {\n\tWeight float64\n\tGen    Generator[T]\n}\n\n// Generator generates values.\ntype Generator[T any] interface {\n\tGenerate(*rand.Rand) T\n}\n```"]}, "weaver-sim/generators.go-OneOf": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/sim/generators.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage sim\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n)\n\n// TODO(mwhittaker): Add more generators as needed.\n\n// Booleans\n\n// Flip returns a [Generator] that returns true with probability p. Flip\n// panics if p is not in the range [0, 1].\nfunc Flip(p float64) Generator[bool] {\n\tif p < 0 || p > 1 {\n\t\tpanic(fmt.Errorf(\"Flip: probability p = %f not in range [0, 1]\", p))\n\t}\n\treturn generatorFunc[bool](func(r *rand.Rand) bool {\n\t\treturn r.Float64() <= p\n\t})\n}\n\n// Numerics\n\n// NonNegativeInt returns a [Generator] that returns non-negative integers.\n// Note that NonNegativeInt does not return all numbers. Instead, it biases\n// towards numbers closer to zero and other pathological numbers that are more\n// likely to induce bugs (e.g., math.MaxInt).\nfunc NonNegativeInt() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(0, 10)},\n\t\t{100, Range(10, 100)},\n\t\t{100, Range(100, 1000)},\n\t\t{100, Range(1000, 10_000)},\n\t\t{100, Range(10_000, 100_000)},\n\t\t{100, Range(100_000, 1_000_000)},\n\t\t{100, Range(1_000_000, 1_000_000_000)},\n\t\t{100, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{100, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Int returns a [Generator] that returns integers. Note that Int does not\n// return all integers equiprobably. Instead, it biases towards numbers closer\n// to zero and other pathological numbers that are more likely to induce bugs\n// (e.g., math.MaxInt, math.MinInt).\nfunc Int() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(-10, 10)},\n\t\t{50, Range(10, 100)},\n\t\t{50, Range(-100, -10)},\n\t\t{50, Range(100, 1000)},\n\t\t{50, Range(-1000, -100)},\n\t\t{50, Range(1000, 10_000)},\n\t\t{50, Range(-10_000, -1000)},\n\t\t{50, Range(10_000, 100_000)},\n\t\t{50, Range(-100_000, -10_000)},\n\t\t{50, Range(100_000, 1_000_000)},\n\t\t{50, Range(-1_000_000, -100_000)},\n\t\t{50, Range(1_000_000, 1_000_000_000)},\n\t\t{50, Range(-1_000_000_000, -1_000_000)},\n\t\t{50, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{50, Range(-1_000_000_000_000, -1_000_000_000)},\n\t\t{50, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{50, Range(math.MinInt, -1_000_000_000_000)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MinInt, math.MinInt+1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MinInt32-1, math.MinInt32, math.MinInt32+1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MinInt16-1, math.MinInt16, math.MinInt16+1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t\tmath.MinInt8-1, math.MinInt8, math.MinInt8+1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Float64 returns a [Generator] that returns 64-bit floats. Note that Float64\n// does not return all floats equiprobably. Instead, it biases towards numbers\n// closer to zero and other pathological numbers that are more likely to induce\n// bugs (e.g., NaN, infinity, -infinity, -0).\nfunc Float64() Generator[float64] {\n\tpathologies := OneOf(\n\t\tmath.NaN(),           // NaN\n\t\tmath.Inf(1),          // infinity\n\t\tmath.Inf(-1),         // -infinity\n\t\tmath.Copysign(0, -1), // -0\n\t)\n\treturn generatorFunc[float64](func(r *rand.Rand) float64 {\n\t\tif r.Intn(1000) == 0 {\n\t\t\t// 0.1% of the time, return a pathological float.\n\t\t\treturn pathologies.Generate(r)\n\t\t}\n\t\tconst stdev = 1e6\n\t\treturn r.NormFloat64() * stdev\n\t})\n}\n\n// Rune returns a [Generator] that returns runes equiprobably.\nfunc Rune() Generator[rune] {\n\treturn generatorFunc[rune](func(r *rand.Rand) rune {\n\t\t// Note that rune is an alias for int32.\n\t\treturn rune(r.Intn(math.MaxInt32 + 1))\n\t})\n}\n\n// Byte returns a [Generator] that returns bytes equiprobably.\nfunc Byte() Generator[byte] {\n\treturn generatorFunc[byte](func(r *rand.Rand) byte {\n\t\tvar buf [1]byte\n\t\tr.Read(buf[:])\n\t\treturn buf[0]\n\t})\n}\n\n// Range returns a [Generator] that returns integers equiprobably in the range\n// [low, high). Range panics if low >= high.\n\n\n\n\n\n\n\n\n\n\n// Strings\n\n// String returns a [Generator] that returns moderately sized readable strings,\n// with a bias towards smaller strings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Slices and Maps\n\n// Slice returns a [Generator] that returns slices of T. The size and contents\n// of the generated slices are determined by the provided generators.\nfunc Slice[T any](size Generator[int], values Generator[T]) Generator[[]T] {\n\treturn generatorFunc[[]T](func(r *rand.Rand) []T {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Slice: negative size %d\", n))\n\t\t}\n\t\txs := make([]T, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\txs[i] = values.Generate(r)\n\t\t}\n\t\treturn xs\n\t})\n}\n\n// Map returns a [Generator] that returns maps from K to V. The size and\n// contents of the the generated maps are determined by the provided\n// generators.\nfunc Map[K comparable, V any](size Generator[int], keys Generator[K], values Generator[V]) Generator[map[K]V] {\n\treturn generatorFunc[map[K]V](func(r *rand.Rand) map[K]V {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Map: negative size %d\", n))\n\t\t}\n\t\tkvs := make(map[K]V, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\tkvs[keys.Generate(r)] = values.Generate(r)\n\t\t}\n\t\treturn kvs\n\t})\n}\n\n// Combinators\n\n// OneOf returns a [Generator] that returns one of the provided values\n// equiprobably. OneOf panics if no values are provided.\n\n\n\n\n\n\n\n\n\n// Weight returns a [Generator] that generates values using the provided\n// generators. A generator is chosen with probability proportional to its\n// weight. For example, given the following choices:\n//\n//   - Weighted{1.0, OneOf(\"a\")}\n//   - Weighted{2.0, OneOf(\"b\")}\n//\n// Weight returns \"b\" twice as often as it returns \"a\". Note that the provided\n// weights do not have to sum to 1.\n//\n// Weight panics if no choices are provided, if any weight is negative, or if\n// the sum of all weight is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype Weighted[T any] struct {\n\tWeight float64\n\tGen    Generator[T]\n}\n\n// Filter returns a [Generator] that returns values from the provided generator\n// that satisfy the provided predicate.\nfunc Filter[T any](gen Generator[T], predicate func(T) bool) Generator[T] {\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\tfor {\n\t\t\tx := gen.Generate(r)\n\t\t\tif predicate(x) {\n\t\t\t\treturn x\n\t\t\t}\n\t\t}\n\t})\n}\n\n// generatorFunc[T] is an instance of Generator[T] that uses the provided\n// function to generate values.\ntype generatorFunc[T any] func(r *rand.Rand) T\n\n// Generate implements the Generator interface.\nfunc (g generatorFunc[T]) Generate(r *rand.Rand) T {\n\treturn g(r)\n}\n\n```\n# Function Signature: func OneOf[T any](xs ...T) Generator[T] {\n# Natural Language Annotations: // Combinators\n// OneOf returns a [Generator] that returns one of the provided values\n// equiprobably. OneOf panics if no values are provided.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc OneOf[T any](xs ...T) Generator[T] {\n\tif len(xs) == 0 {\n\t\tpanic(\"OneOf: no values provided\")\n\t}\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\treturn xs[r.Intn(len(xs))]\n\t})\n}\n```\n\nThis function `OneOf` takes a variable number of arguments of type `T` and returns a `Generator[T]`. It generates one of the provided values, chosen randomly and with equal probability. If no values are provided, it panics with a descriptive message.", "```go\nfunc OneOf[T any](xs ...T) Generator[T] {\n    if len(xs) == 0 {\n        panic(\"OneOf: no values provided\")\n    }\n    return generatorFunc[T](func(r *rand.Rand) T {\n        return xs[r.Intn(len(xs))]\n    })\n}\n```", "```go\nfunc OneOf[T any](xs ...T) Generator[T] {\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\tif len(xs) == 0 {\n\t\t\tpanic(fmt.Errorf(\"OneOf: no values provided\"))\n\t\t}\n\t\treturn xs[r.Intn(len(xs))]\n\t})\n}\n```"]}, "weaver-sim/generators.go-Weight": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/sim/generators.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage sim\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n)\n\n// TODO(mwhittaker): Add more generators as needed.\n\n// Booleans\n\n// Flip returns a [Generator] that returns true with probability p. Flip\n// panics if p is not in the range [0, 1].\nfunc Flip(p float64) Generator[bool] {\n\tif p < 0 || p > 1 {\n\t\tpanic(fmt.Errorf(\"Flip: probability p = %f not in range [0, 1]\", p))\n\t}\n\treturn generatorFunc[bool](func(r *rand.Rand) bool {\n\t\treturn r.Float64() <= p\n\t})\n}\n\n// Numerics\n\n// NonNegativeInt returns a [Generator] that returns non-negative integers.\n// Note that NonNegativeInt does not return all numbers. Instead, it biases\n// towards numbers closer to zero and other pathological numbers that are more\n// likely to induce bugs (e.g., math.MaxInt).\nfunc NonNegativeInt() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(0, 10)},\n\t\t{100, Range(10, 100)},\n\t\t{100, Range(100, 1000)},\n\t\t{100, Range(1000, 10_000)},\n\t\t{100, Range(10_000, 100_000)},\n\t\t{100, Range(100_000, 1_000_000)},\n\t\t{100, Range(1_000_000, 1_000_000_000)},\n\t\t{100, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{100, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Int returns a [Generator] that returns integers. Note that Int does not\n// return all integers equiprobably. Instead, it biases towards numbers closer\n// to zero and other pathological numbers that are more likely to induce bugs\n// (e.g., math.MaxInt, math.MinInt).\nfunc Int() Generator[int] {\n\tchoices := []Weighted[int]{\n\t\t{100, Range(-10, 10)},\n\t\t{50, Range(10, 100)},\n\t\t{50, Range(-100, -10)},\n\t\t{50, Range(100, 1000)},\n\t\t{50, Range(-1000, -100)},\n\t\t{50, Range(1000, 10_000)},\n\t\t{50, Range(-10_000, -1000)},\n\t\t{50, Range(10_000, 100_000)},\n\t\t{50, Range(-100_000, -10_000)},\n\t\t{50, Range(100_000, 1_000_000)},\n\t\t{50, Range(-1_000_000, -100_000)},\n\t\t{50, Range(1_000_000, 1_000_000_000)},\n\t\t{50, Range(-1_000_000_000, -1_000_000)},\n\t\t{50, Range(1_000_000_000, 1_000_000_000_000)},\n\t\t{50, Range(-1_000_000_000_000, -1_000_000_000)},\n\t\t{50, Range(1_000_000_000_000, math.MaxInt)},\n\t\t{50, Range(math.MinInt, -1_000_000_000_000)},\n\t\t{1, OneOf(\n\t\t\tmath.MaxInt, math.MaxInt-1,\n\t\t\tmath.MinInt, math.MinInt+1,\n\t\t\tmath.MaxInt32+1, math.MaxInt32, math.MaxInt32-1,\n\t\t\tmath.MinInt32-1, math.MinInt32, math.MinInt32+1,\n\t\t\tmath.MaxInt16+1, math.MaxInt16, math.MaxInt16-1,\n\t\t\tmath.MinInt16-1, math.MinInt16, math.MinInt16+1,\n\t\t\tmath.MaxInt8+1, math.MaxInt8, math.MaxInt8-1,\n\t\t\tmath.MinInt8-1, math.MinInt8, math.MinInt8+1,\n\t\t)},\n\t}\n\treturn Weight(choices)\n}\n\n// Float64 returns a [Generator] that returns 64-bit floats. Note that Float64\n// does not return all floats equiprobably. Instead, it biases towards numbers\n// closer to zero and other pathological numbers that are more likely to induce\n// bugs (e.g., NaN, infinity, -infinity, -0).\nfunc Float64() Generator[float64] {\n\tpathologies := OneOf(\n\t\tmath.NaN(),           // NaN\n\t\tmath.Inf(1),          // infinity\n\t\tmath.Inf(-1),         // -infinity\n\t\tmath.Copysign(0, -1), // -0\n\t)\n\treturn generatorFunc[float64](func(r *rand.Rand) float64 {\n\t\tif r.Intn(1000) == 0 {\n\t\t\t// 0.1% of the time, return a pathological float.\n\t\t\treturn pathologies.Generate(r)\n\t\t}\n\t\tconst stdev = 1e6\n\t\treturn r.NormFloat64() * stdev\n\t})\n}\n\n// Rune returns a [Generator] that returns runes equiprobably.\nfunc Rune() Generator[rune] {\n\treturn generatorFunc[rune](func(r *rand.Rand) rune {\n\t\t// Note that rune is an alias for int32.\n\t\treturn rune(r.Intn(math.MaxInt32 + 1))\n\t})\n}\n\n// Byte returns a [Generator] that returns bytes equiprobably.\nfunc Byte() Generator[byte] {\n\treturn generatorFunc[byte](func(r *rand.Rand) byte {\n\t\tvar buf [1]byte\n\t\tr.Read(buf[:])\n\t\treturn buf[0]\n\t})\n}\n\n// Range returns a [Generator] that returns integers equiprobably in the range\n// [low, high). Range panics if low >= high.\n\n\n\n\n\n\n\n\n\n\n// Strings\n\n// String returns a [Generator] that returns moderately sized readable strings,\n// with a bias towards smaller strings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Slices and Maps\n\n// Slice returns a [Generator] that returns slices of T. The size and contents\n// of the generated slices are determined by the provided generators.\nfunc Slice[T any](size Generator[int], values Generator[T]) Generator[[]T] {\n\treturn generatorFunc[[]T](func(r *rand.Rand) []T {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Slice: negative size %d\", n))\n\t\t}\n\t\txs := make([]T, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\txs[i] = values.Generate(r)\n\t\t}\n\t\treturn xs\n\t})\n}\n\n// Map returns a [Generator] that returns maps from K to V. The size and\n// contents of the the generated maps are determined by the provided\n// generators.\nfunc Map[K comparable, V any](size Generator[int], keys Generator[K], values Generator[V]) Generator[map[K]V] {\n\treturn generatorFunc[map[K]V](func(r *rand.Rand) map[K]V {\n\t\tn := size.Generate(r)\n\t\tif n < 0 {\n\t\t\tpanic(fmt.Errorf(\"Map: negative size %d\", n))\n\t\t}\n\t\tkvs := make(map[K]V, n)\n\t\tfor i := 0; i < n; i++ {\n\t\t\tkvs[keys.Generate(r)] = values.Generate(r)\n\t\t}\n\t\treturn kvs\n\t})\n}\n\n// Combinators\n\n// OneOf returns a [Generator] that returns one of the provided values\n// equiprobably. OneOf panics if no values are provided.\n\n\n\n\n\n\n\n\n\n// Weight returns a [Generator] that generates values using the provided\n// generators. A generator is chosen with probability proportional to its\n// weight. For example, given the following choices:\n//\n//   - Weighted{1.0, OneOf(\"a\")}\n//   - Weighted{2.0, OneOf(\"b\")}\n//\n// Weight returns \"b\" twice as often as it returns \"a\". Note that the provided\n// weights do not have to sum to 1.\n//\n// Weight panics if no choices are provided, if any weight is negative, or if\n// the sum of all weight is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype Weighted[T any] struct {\n\tWeight float64\n\tGen    Generator[T]\n}\n\n// Filter returns a [Generator] that returns values from the provided generator\n// that satisfy the provided predicate.\nfunc Filter[T any](gen Generator[T], predicate func(T) bool) Generator[T] {\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\tfor {\n\t\t\tx := gen.Generate(r)\n\t\t\tif predicate(x) {\n\t\t\t\treturn x\n\t\t\t}\n\t\t}\n\t})\n}\n\n// generatorFunc[T] is an instance of Generator[T] that uses the provided\n// function to generate values.\ntype generatorFunc[T any] func(r *rand.Rand) T\n\n// Generate implements the Generator interface.\nfunc (g generatorFunc[T]) Generate(r *rand.Rand) T {\n\treturn g(r)\n}\n\n```\n# Function Signature: func Weight[T any](choices []Weighted[T]) Generator[T] {\n# Natural Language Annotations: // Weight returns a [Generator] that generates values using the provided\n// generators. A generator is chosen with probability proportional to its\n// weight. For example, given the following choices:\n//\n//   - Weighted{1.0, OneOf(\"a\")}\n//   - Weighted{2.0, OneOf(\"b\")}\n//\n// Weight returns \"b\" twice as often as it returns \"a\". Note that the provided\n// weights do not have to sum to 1.\n//\n// Weight panics if no choices are provided, if any weight is negative, or if\n// the sum of all weight is 0.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc Weight[T any](choices []Weighted[T]) Generator[T] {\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\tif len(choices) == 0 {\n\t\t\tpanic(\"Weight: no choices provided\")\n\t\t}\n\t\tvar sum float64\n\t\tfor _, choice := range choices {\n\t\t\tif choice.Weight < 0 {\n\t\t\t\tpanic(\"Weight: negative weight\")\n\t\t\t}\n\t\t\tsum += choice.Weight\n\t\t}\n\t\tif sum == 0 {\n\t\t\tpanic(\"Weight: sum of all weights is zero\")\n\t\t}\n\t\tvalue := r.Float64() * sum\n\t\tfor _, choice := range choices {\n\t\t\tif value < choice.Weight {\n\t\t\t\treturn choice.Gen.Generate(r)\n\t\t\t}\n\t\t\tvalue -= choice.Weight\n\t\t}\n\t\tpanic(\"Weight: internal error: code should be unreachable\")\n\t})\n}\n```\nThis function `Weight` returns a new `Generator[T]`. It uses a weighted random selection algorithm to choose one of the provided generators. The generator to choose is chosen with probability proportional to its weight. If no choices are provided, if any weight is negative, or if the sum of all weights is zero, the function panics.", "```go\nfunc Weight[T any](choices []Weighted[T]) Generator[T] {\n\t// Compute the total weight and check each weight and sum.\n\tvar totalWeight float64\n\tfor _, choice := range choices {\n\t\tif choice.Weight < 0 {\n\t\t\tpanic(fmt.Errorf(\"Weight: negative weight %f\", choice.Weight))\n\t\t}\n\t\ttotalWeight += choice.Weight\n\t}\n\tif totalWeight == 0 {\n\t\tpanic(fmt.Errorf(\"Weight: total weight is zero\"))\n\t}\n\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\t// Choose a random number in [0, totalWeight) and find the corresponding choice.\n\t\tchoice := choices[len(choices)-1]\n\t\tx := r.Float64() * totalWeight\n\t\tfor _, c := range choices {\n\t\t\tx -= c.Weight\n\t\t\tif x <= 0 {\n\t\t\t\tchoice = c\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\treturn choice.Gen.Generate(r)\n\t})\n}\n```", "```go\nfunc Weight[T any](choices []Weighted[T]) Generator[T] {\n\treturn generatorFunc[T](func(r *rand.Rand) T {\n\t\tif len(choices) == 0 {\n\t\t\tpanic(\"Weight: no choices provided\")\n\t\t}\n\n\t\tvar totalWeight float64\n\t\tfor _, choice := range choices {\n\t\t\tif choice.Weight < 0 {\n\t\t\t\tpanic(\"Weight: negative weight\")\n\t\t\t}\n\t\t\ttotalWeight += choice.Weight\n\t\t}\n\t\tif totalWeight == 0 {\n\t\t\tpanic(\"Weight: total weight is 0\")\n\t\t}\n\n\t\tval := r.Float64() * totalWeight\n\t\tfor _, choice := range choices {\n\t\t\tif val < choice.Weight {\n\t\t\t\treturn choice.Gen.Generate(r)\n\t\t\t}\n\t\t\tval -= choice.Weight\n\t\t}\n\n\t\tpanic(\"Weight: internal error\")\n\t})\n}\n```\n\nThis function generates a `Weight` generator that probabilistically chooses one of the generators provided in the `choices` slice. The probability of each generator being chosen is proportional to its weight, which is a float64 value in the range [0, 1]. If no choices are provided, or if the sum of all weights is 0, the function will panic."]}, "weaver-sim/simulator.go-New": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/sim/simulator.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package sim implements deterministic simulation.\n//\n// [Deterministic simulation][1] is a type of randomized testing in which\n// millions of random operations are run against a system (with randomly\n// injected failures) in an attempt to find bugs. See\n// serviceweaver.dev/blog/testing.html for an overview of determistic\n// simulation and its implementation in the sim package.\n//\n// # Generators\n//\n// A key component of deterministic simulation is the ability to\n// deterministically generate \"random\" values. We accomplish this with the\n// [Generator] interface:\n//\n//\ttype Generator[T any] interface {\n//\t    Generate(*rand.Rand) T\n//\t}\n//\n// A Generator[T] generates random values of type T. For example, the [Int]\n// function returns a Generator[int] that generates random integers.\n//\n// While random, a Generator is also deterministic. Given a random number\n// generator with a particular seed, a Generator will always produce the same\n// value:\n//\n//\t// x and y are always equal.\n//\tvar gen Gen[int] = ...\n//\tx := gen.Generate(rand.New(rand.NewSource(42)))\n//\ty := gen.Generate(rand.New(rand.NewSource(42)))\n//\n// The sim package includes generators that generate booleans, ints,\n// floats, runes, strings, slices, and maps (e.g., [Flip], [Int], [Float64],\n// [Rune], [String], [Range], [Map]). It also contains generator combinators\n// that combine existing generators into new generators (e.g., [OneOf],\n// [Weight], [Filter]). You can also implement your own custom generators by\n// implementing the Generator interface.\n//\n// # Workloads\n//\n// Deterministic simulation verifies a system by running random operations\n// against the system, checking for invariant violations along the way. A\n// workload defines the set of operations to run and the set of invariants to\n// check.\n//\n// Concretely, a workload is a struct that implements the [Workload] interface.\n// When a simulator executes a workload, it will randomly call the exported\n// methods of the struct with randomly generated values. We call these methods\n// *operations*. If an operation ever encounters an invariant violation, it\n// returns a non-nil error and the simulation is aborted.\n//\n// Consider the following workload as an example.\n//\n//\tfunc even(x int) bool {\n//\t\treturn x%2 == 0\n//\t}\n//\n//\ttype EvenWorkload struct {\n//\t\tx int\n//\t}\n//\n//\tfunc (e *EvenWorkload) Add(_ context.Context, y int) error {\n//\t\te.x = e.x + y\n//\t\tif !even(e.x) {\n//\t\t\treturn fmt.Errorf(\"%d is not even\", e.x)\n//\t\t}\n//\t\treturn nil\n//\t}\n//\n//\tfunc (e *EvenWorkload) Multiply(_ context.Context, y int) error {\n//\t\te.x = e.x * y\n//\t\tif !even(e.x) {\n//\t\t\treturn fmt.Errorf(\"%d is not even\", e.x)\n//\t\t}\n//\t\treturn nil\n//\t}\n//\n// An EvenWorkload has an integer x as state and defines two operations: Add\n// and Multiply. Add adds a value to x, and Multiply multiplies x. Both\n// operations check the invariant that x is even. Of course, this invariant\n// does not hold if we add arbitrary values to x.\n//\n// However, we control the arguments on which which operations are called.\n// Specifically, we add an Init method that registers a set of generators. The\n// simulator will call the workload's operations on values produced by these\n// generators.\n//\n//\tfunc (e *EvenWorkload) Init(r sim.Registrar) error {\n//\t\tr.RegisterGenerators(\"Add\", sim.Filter(sim.Int(), even))\n//\t\tr.RegisterGenerators(\"Multiply\", sim.Int())\n//\t\treturn nil\n//\t}\n//\n// Note that we only call the Add operation on even integers. Finally, we can\n// construct a simulator and simulate the EvenWorkload.\n//\n//\tfunc TestEvenWorkload(t *testing.T) {\n//\t\ts := sim.New(t, &EvenWorkload{}, sim.Options{})\n//\t\tr := s.Run(5 * time.Second)\n//\t\tif r.Err != nil {\n//\t\t\tt.Fatal(r.Err)\n//\t\t}\n//\t}\n//\n// In this trivial example, our workload did not use any Service Weaver\n// components, but most realistic workloads do. A workload can get a reference\n// to a component using weaver.Ref. See serviceweaver.dev/blog/testing.html for\n// a complete example.\n//\n// # Graveyard\n//\n// When the simulator runs a failed execution, it persists the failing inputs\n// to disk. The collection of saved failing inputs is called a *graveyard*, and\n// each individual entry is called a *graveyard entry*. When a simulator is\n// created, the first thing it does is load and re-simulate all graveyard\n// entries.\n//\n// We borrow the design of go's fuzzing library's corpus with only minor\n// changes [2]. When a simulator runs as part of a test named TestFoo, it\n// stores its graveyard entries in testdata/sim/TestFoo. Every graveyard entry\n// is a JSON file. Filenames are derived from the hash of the contents of the\n// graveyard entry. Here's an example testdata directory:\n//\n//\ttestdata/\n//\t\u2514\u2500\u2500 sim\n//\t    \u251c\u2500\u2500 TestCancelledSimulation\n//\t    \u2502\u00a0\u00a0 \u2514\u2500\u2500 a52f5ec5f94e674d.json\n//\t    \u251c\u2500\u2500 TestSimulateGraveyardEntries\n//\t    \u2502\u00a0\u00a0 \u251c\u2500\u2500 2bfe847328319dae.json\n//\t    \u2502\u00a0\u00a0 \u2514\u2500\u2500 a52f5ec5f94e674d.json\n//\t    \u2514\u2500\u2500 TestUnsuccessfulSimulation\n//\t        \u251c\u2500\u2500 2bfe847328319dae.json\n//\t        \u2514\u2500\u2500 a52f5ec5f94e674d.json\n//\n// As with go's fuzzing library, graveyard entries are never garbage collected.\n// Users are responsible for manually deleting graveyard entries when\n// appropriate.\n//\n// TODO(mwhittaker): Move things to the weavertest package.\n//\n// [1]: https://asatarin.github.io/testing-distributed-systems/#deterministic-simulation\n// [2]: https://go.dev/security/fuzz\npackage sim\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math/rand\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"runtime\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/reflection\"\n\t\"github.com/ServiceWeaver/weaver/internal/weaver\"\n\tswruntime \"github.com/ServiceWeaver/weaver/runtime\"\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/logging\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"golang.org/x/exp/maps\"\n\t\"golang.org/x/text/language\"\n\t\"golang.org/x/text/message\"\n)\n\n// FakeComponent is a copy of weavertest.FakeComponent. It's needed to access\n// the unexported fields.\n//\n// TODO(mwhittaker): Remove this once we merge with weavertest.\ntype FakeComponent struct {\n\tintf reflect.Type\n\timpl any\n}\n\n// Fake is a copy of weavertest.Fake.\n//\n// TODO(mwhittaker): Remove this once we merge with the weavertest package.\nfunc Fake[T any](impl any) FakeComponent {\n\tt := reflection.Type[T]()\n\tif _, ok := impl.(T); !ok {\n\t\tpanic(fmt.Sprintf(\"%T does not implement %v\", impl, t))\n\t}\n\treturn FakeComponent{intf: t, impl: impl}\n}\n\n// A Generator[T] generates random values of type T.\ntype Generator[T any] interface {\n\t// Generate returns a randomly generated value of type T. While Generate is\n\t// \"random\", it must be deterministic. That is, given the same instance of\n\t// *rand.Rand, Generate must always return the same value.\n\t//\n\t// TODO(mwhittaker): Generate should maybe take something other than a\n\t// *rand.Rand?\n\tGenerate(*rand.Rand) T\n}\n\n// A Registrar is used to register fakes and generators with a [Simulator].\ntype Registrar interface {\n\t// RegisterFake registers a fake implementation of a component.\n\tRegisterFake(FakeComponent)\n\n\t// RegisterGenerators registers generators for a workload method, one\n\t// generator per method argument. The number and type of the registered\n\t// generators must match the method. For example, given the method:\n\t//\n\t//     Foo(context.Context, int, bool) error\n\t//\n\t// we must register a Generator[int] and a Generator[bool]:\n\t//\n\t//     var r Registrar = ...\n\t//     var i Generator[int] = ...\n\t//     var b Generator[bool] = ...\n\t//     r.RegisterGenerators(\"Foo\", i, b)\n\t//\n\t// TODO(mwhittaker): Allow people to register a func(*rand.Rand) T instead\n\t// of a Generator[T] for convenience.\n\tRegisterGenerators(method string, generators ...any)\n}\n\n// A Workload defines the set of operations to run as part of a simulation.\n// Every workload is defined as a named struct. To execute a workload, a\n// simulator constructs an instance of the struct, calls the struct's Init\n// method, and then randomly calls the struct's exported methods. For example,\n// the following is a simple workload:\n//\n//\ttype myWorkload struct {}\n//\tfunc (w *myWorkload) Init(r sim.Registrar) {...}\n//\tfunc (w *myWorkload) Foo(context.Context, int) error {...}\n//\tfunc (w *myWorkload) Bar(context.Context, bool, string) error {...}\n//\tfunc (w *myWorkload) baz(context.Context) error {...}\n//\n// When this workload is executed, its Foo and Bar methods will be called with\n// random values generated by the generators registered in the Init method (see\n// [Registrar] for details). Note that unexported methods, like baz, are\n// ignored.\n//\n// Note that every exported workload method must receive a [context.Context] as\n// its first argument and must return a single error value. A simulation is\n// aborted when a method returns a non-nil error.\n//\n// TODO(mwhittaker): For now, the Init method is required. In the future, we\n// could make it optional and use default generators for methods.\ntype Workload interface {\n\t// Init initializes a workload. The Init method must also register\n\t// generators for every exported method.\n\tInit(Registrar) error\n}\n\n// Options configure a Simulator.\ntype Options struct {\n\t// TOML config file contents.\n\tConfig string\n\n\t// The number of executions to run in parallel. If Parallelism is 0, the\n\t// simulator picks the degree of parallelism.\n\tParallelism int\n}\n\n// A Simulator deterministically simulates a Service Weaver application. See\n// the package documentation for instructions on how to use a Simulator.\ntype Simulator struct {\n\topts       Options                                // options\n\tt          testing.TB                             // underlying test\n\tw          reflect.Type                           // workload type\n\tregsByIntf map[reflect.Type]*codegen.Registration // components, by interface\n\tinfo       componentInfo                          // component metadata\n\tconfig     *protos.AppConfig                      // application config\n}\n\n// Results are the results of simulating a workload.\ntype Results struct {\n\tErr           error         // first non-nil error returned by an op\n\tHistory       []Event       // a history of the error inducing run, if Err is not nil\n\tNumExecutions int           // number of executions ran\n\tNumOps        int           // number of ops ran\n\tDuration      time.Duration // duration of simulation\n}\n\n// New returns a new Simulator that simulates the provided workload.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// validateWorkload validates a workload struct of the provided type.\nfunc validateWorkload(w reflect.Type) error {\n\tvar errs []error\n\tnumOps := 0\n\tfor i := 0; i < w.NumMethod(); i++ {\n\t\tm := w.Method(i)\n\t\tif m.Name == \"Init\" {\n\t\t\tcontinue\n\t\t}\n\t\tnumOps++\n\n\t\t// Method should have type func(context.Context, ...) error.\n\t\terr := fmt.Errorf(\"method %s has type '%v' but should have type 'func(%v, context.Context, ...) error'\", m.Name, m.Type, w)\n\t\tswitch {\n\t\tcase m.Type.NumIn() < 2:\n\t\t\terrs = append(errs, fmt.Errorf(\"%w: no arguments\", err))\n\t\tcase m.Type.In(1) != reflection.Type[context.Context]():\n\t\t\terrs = append(errs, fmt.Errorf(\"%w: first argument is not context.Context\", err))\n\t\tcase m.Type.NumOut() == 0:\n\t\t\terrs = append(errs, fmt.Errorf(\"%w: no return value\", err))\n\t\tcase m.Type.NumOut() > 1:\n\t\t\terrs = append(errs, fmt.Errorf(\"%w: too many return values\", err))\n\t\tcase m.Type.Out(0) != reflection.Type[error]():\n\t\t\terrs = append(errs, fmt.Errorf(\"%w: return value is not error\", err))\n\t\t}\n\t}\n\tif numOps == 0 {\n\t\terrs = append(errs, fmt.Errorf(\"no exported methods\"))\n\t}\n\treturn errors.Join(errs...)\n}\n\n// newExecutor returns a new executor.\nfunc (s *Simulator) newExecutor() *executor {\n\treturn newExecutor(s.t, s.w, s.regsByIntf, s.info, s.config)\n}\n\n// graveyardDir returns the graveyard directory for this simulator.\nfunc (s *Simulator) graveyardDir() string {\n\t// Test names often contain slashes (\"/\"). We replace \"/\" with \"#\" to\n\t// safely use the test name as a directory name.\n\t//\n\t// TODO(mwhittaker): This mapping is sensitive to collisions. A test named\n\t// \"foo/bar\" collides with a test named \"foo#bar\". I think in practice,\n\t// this will likely not be a big issue. But, if people are running into\n\t// problems, we can use a more complex collision resistant sanitization.\n\tsanitized := strings.ReplaceAll(s.t.Name(), \"/\", \"#\")\n\treturn filepath.Join(\"testdata\", \"sim\", sanitized)\n}\n\n// Run runs a simulation for the provided duration.\nfunc (s *Simulator) Run(duration time.Duration) Results {\n\tctx, cancel := context.WithTimeout(context.Background(), duration)\n\tdefer cancel()\n\n\ts.t.Logf(\"Simulating workload %v for %v.\", s.w, duration)\n\tstats := &stats{start: time.Now()}\n\tswitch result, err := s.run(ctx, stats); {\n\tcase err != nil && err == ctx.Err():\n\t\t// The simulation was cancelled.\n\t\tresults := Results{\n\t\t\tNumExecutions: int(stats.numExecutions),\n\t\t\tNumOps:        int(stats.numOps),\n\t\t\tDuration:      time.Since(stats.start),\n\t\t}\n\t\ts.t.Log(results.summary())\n\t\treturn results\n\n\tcase err != nil:\n\t\t// The simulation failed to run properly.\n\t\ts.t.Fatalf(\"Simulator.Run: %v\", err)\n\t\treturn Results{}\n\n\tcase result.err != nil:\n\t\t// The simulation found a failing execution.\n\t\tresults := Results{\n\t\t\tErr:           result.err,\n\t\t\tHistory:       result.history,\n\t\t\tNumExecutions: int(stats.numExecutions),\n\t\t\tNumOps:        int(stats.numOps),\n\t\t\tDuration:      time.Since(stats.start),\n\t\t}\n\t\ts.t.Log(results.summary())\n\n\t\tentry := graveyardEntry{\n\t\t\tVersion:     version,\n\t\t\tSeed:        result.params.Seed,\n\t\t\tNumReplicas: result.params.NumReplicas,\n\t\t\tNumOps:      result.params.NumOps,\n\t\t\tFailureRate: result.params.FailureRate,\n\t\t\tYieldRate:   result.params.YieldRate,\n\t\t}\n\t\tif filename, err := writeGraveyardEntry(s.graveyardDir(), entry); err == nil {\n\t\t\ts.t.Logf(\"Failing input written to %s.\", filename)\n\t\t}\n\t\treturn results\n\n\tdefault:\n\t\t// All executions passed.\n\t\tresults := Results{\n\t\t\tNumExecutions: int(stats.numExecutions),\n\t\t\tNumOps:        int(stats.numOps),\n\t\t\tDuration:      time.Since(stats.start),\n\t\t}\n\t\ts.t.Log(results.summary())\n\t\treturn results\n\t}\n}\n\n// stats contains simulation statistics.\ntype stats struct {\n\tstart         time.Time // start of simulation\n\tnumExecutions int64     // number of fully executed executions\n\tnumOps        int64     // number of fully executed ops\n}\n\n// run runs a simulation until the provided context is cancelled. It returns\n// the hyperparameters and result of a failing execution if any are found.\nfunc (s *Simulator) run(ctx context.Context, stats *stats) (result, error) {\n\tctx, cancel := context.WithCancel(ctx)\n\tdefer cancel()\n\n\t// Spawn a goroutine to periodically print progress.\n\tdone := sync.WaitGroup{}\n\tdone.Add(1)\n\tgo func() {\n\t\tdefer done.Done()\n\t\ts.printProgress(ctx, stats)\n\t}()\n\n\t// Execute the graveyard entries.\n\tif r, err := s.executeGraveyard(ctx, stats); err != nil || r.err != nil {\n\t\treturn r, err\n\t}\n\n\t// Spawn n concurrent executors which read hyperparamters from the params\n\t// channel. Simulation ends when:\n\t//\n\t//     1. the context is cancelled;\n\t//     2. an execution fails to run properly (written to errs); or\n\t//     3. a failing execution is found (written to failing).\n\tn := s.opts.Parallelism\n\tif n == 0 {\n\t\tn = 10 * runtime.NumCPU()\n\t}\n\tparams := make(chan hyperparameters, n)\n\terrs := make(chan error, n)\n\tfailing := make(chan result, n)\n\n\ts.t.Logf(\"Executing with %d executors.\", n)\n\tdone.Add(n)\n\tfor i := 0; i < n; i++ {\n\t\tgo func() {\n\t\t\tdefer done.Done()\n\t\t\tswitch r, err := s.execute(ctx, stats, params); {\n\t\t\tcase err != nil && err == ctx.Err():\n\t\t\t\treturn\n\t\t\tcase err != nil:\n\t\t\t\terrs <- err\n\t\t\t\treturn\n\t\t\tcase r.err != nil:\n\t\t\t\tfailing <- r\n\t\t\t}\n\t\t}()\n\t}\n\n\t// Spawn a goroutine that writes to the params channel.\n\t//\n\t// TODO(mwhittaker): Use a smarter algorithm to sweep over hyperparameters.\n\tdone.Add(1)\n\tgo func() {\n\t\tdefer done.Done()\n\t\tseed := time.Now().UnixNano()\n\t\tfor numOps := 1; ; numOps++ {\n\t\t\tfor _, numReplicas := range []int{1, 2, 3} {\n\t\t\t\tfor _, failureRate := range []float64{0.0, 0.01, 0.05, 0.1} {\n\t\t\t\t\tfor _, yieldRate := range []float64{0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0} {\n\t\t\t\t\t\tfor i := 0; i < 1000; i++ {\n\t\t\t\t\t\t\tseed++\n\t\t\t\t\t\t\tp := hyperparameters{\n\t\t\t\t\t\t\t\tSeed:        seed,\n\t\t\t\t\t\t\t\tNumOps:      numOps,\n\t\t\t\t\t\t\t\tNumReplicas: numReplicas,\n\t\t\t\t\t\t\t\tFailureRate: failureRate,\n\t\t\t\t\t\t\t\tYieldRate:   yieldRate,\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tselect {\n\t\t\t\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t\tcase params <- p:\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Wait for the simulation to end.\n\tselect {\n\tcase <-ctx.Done():\n\t\tdone.Wait()\n\t\treturn result{}, ctx.Err()\n\tcase err := <-errs:\n\t\tcancel()\n\t\tdone.Wait()\n\t\treturn result{}, err\n\tcase r := <-failing:\n\t\tcancel()\n\t\tdone.Wait()\n\t\treturn r, nil\n\t}\n}\n\n// printProgress periodically prints the progress of the simulation.\nfunc (s *Simulator) printProgress(ctx context.Context, stats *stats) {\n\tprinter := message.NewPrinter(language.AmericanEnglish)\n\tticker := time.NewTicker(time.Second)\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\tticker.Stop()\n\t\t\treturn\n\t\tcase <-ticker.C:\n\t\t\telapsed := time.Since(stats.start)\n\t\t\ttruncated := elapsed.Truncate(time.Second)\n\t\t\texecs := atomic.LoadInt64(&stats.numExecutions)\n\t\t\tops := atomic.LoadInt64(&stats.numOps)\n\t\t\texecRate := printer.Sprintf(\"%0.0f\", float64(execs)/elapsed.Seconds())\n\t\t\topRate := printer.Sprintf(\"%0.0f\", float64(ops)/elapsed.Seconds())\n\t\t\ts.t.Logf(\"[%v] %s execs (%s execs/s), %s ops (%s ops/s)\", truncated, printer.Sprint(execs), execRate, printer.Sprint(ops), opRate)\n\t\t}\n\t}\n}\n\n// executeGraveyardEntries executes graveyard entries serially. Executing\n// graveyard entries serially is important to make errors repeatable. If we\n// execute graveyard entries in multiple goroutines, the user might see a\n// different error every time they run \"go test\", which is discombobulating.\nfunc (s *Simulator) executeGraveyard(ctx context.Context, stats *stats) (result, error) {\n\tgraveyard, err := readGraveyard(s.graveyardDir())\n\tif err != nil {\n\t\treturn result{}, err\n\t}\n\n\ts.t.Logf(\"Executing %d graveyard entries.\", len(graveyard))\n\texec := s.newExecutor()\n\tfor _, entry := range graveyard {\n\t\tp := hyperparameters{\n\t\t\tSeed:        entry.Seed,\n\t\t\tNumReplicas: entry.NumReplicas,\n\t\t\tNumOps:      entry.NumOps,\n\t\t\tFailureRate: entry.FailureRate,\n\t\t\tYieldRate:   entry.YieldRate,\n\t\t}\n\t\tr, err := exec.execute(ctx, p)\n\t\tif err != nil {\n\t\t\treturn result{}, err\n\t\t}\n\t\tatomic.AddInt64(&stats.numExecutions, 1)\n\t\tatomic.AddInt64(&stats.numOps, int64(p.NumOps))\n\t\tif r.err != nil {\n\t\t\treturn r, nil\n\t\t}\n\t}\n\ts.t.Log(\"Done executing graveyard entries.\")\n\treturn result{}, nil\n}\n\n// execute repeatedly performs executions until the provided context is\n// cancelled or until a failing result is found. Hyperparameters for the\n// executions are read from the provided params channel.\nfunc (s *Simulator) execute(ctx context.Context, stats *stats, params <-chan hyperparameters) (result, error) {\n\texec\n```\n# Function Signature: func New(t testing.TB, x Workload, opts Options) *Simulator {\n# Natural Language Annotations: // New returns a new Simulator that simulates the provided workload.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc New(t testing.TB, x Workload, opts Options) *Simulator {\n\t// Validate the workload\n\terr := validateWorkload(reflect.TypeOf(x))\n\tif err != nil {\n\t\tt.Fatalf(\"Invalid workload: %v\", err)\n\t}\n\n\t// Create a new Simulator\n\tsim := &Simulator{\n\t\topts:       opts,\n\t\tt:          t,\n\t\tw:          reflect.TypeOf(x),\n\t\tregsByIntf: make(map[reflect.Type]*codegen.Registration),\n\t\tinfo:       componentInfo{},\n\t\tconfig:     &protos.AppConfig{},\n\t}\n\n\t// Initialize the workload\n\terr = x.Init(sim)\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to initialize workload: %v\", err)\n\t}\n\n\treturn sim\n}\n```", "```go\nfunc New(t testing.TB, x Workload, opts Options) *Simulator {\n\t// Validate the workload.\n\tif err := validateWorkload(reflect.TypeOf(x)); err != nil {\n\t\tt.Fatalf(\"sim.New: %v\", err)\n\t}\n\n\t// Parse the config.\n\tvar config *protos.AppConfig\n\tif opts.Config != \"\" {\n\t\tvar err error\n\t\tconfig, err = parseConfig(opts.Config)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"sim.New: %v\", err)\n\t\t}\n\t}\n\n\t// Create and return the Simulator.\n\treturn &Simulator{\n\t\topts:       opts,\n\t\tt:          t,\n\t\tw:          reflect.TypeOf(x),\n\t\tregsByIntf: map[reflect.Type]*codegen.Registration{},\n\t\tinfo:       componentInfo{},\n\t\tconfig:     config,\n\t}\n}\n```", "```go\nfunc New(t testing.TB, w Workload, opts Options) *Simulator {\n\tif err := validateWorkload(reflect.TypeOf(w)); err != nil {\n\t\tt.Fatalf(\"invalid workload: %v\", err)\n\t}\n\n\tconfig, err := swruntime.ParseConfig(\"serviceweaver.toml\", opts.Config)\n\tif err != nil {\n\t\tt.Fatalf(\"failed to parse application config: %v\", err)\n\t}\n\n\tinfo := componentInfo{\n\t\tregistrations: make(map[string]*codegen.Registration),\n\t\tdeployments:   make(map[string]*protos.Deployment),\n\t}\n\tfor _, reg := range codegen.Registrations() {\n\t\tinfo.registrations[reg.Name] = reg\n\t\tinfo.deployments[reg.Name] = reg.Deployment\n\t}\n\n\treturn &Simulator{\n\t\topts:       opts,\n\t\tt:          t,\n\t\tw:          reflect.TypeOf(w),\n\t\tregsByIntf: info.registrations,\n\t\tinfo:       info,\n\t\tconfig:     config,\n\t}\n}\n```"]}, "weaver-sim/rand.go-pick": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/sim/rand.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage sim\n\nimport (\n\t\"fmt\"\n\t\"math/bits\"\n\t\"math/rand\"\n)\n\n// pop pops and returns a randomly selected element from the provided slice.\n// pop panics if the provided slice is empty.\nfunc pop[T any](r *rand.Rand, xs []T) (T, []T) {\n\tif len(xs) == 0 {\n\t\tpanic(fmt.Errorf(\"pop: empty slice\"))\n\t}\n\ti := r.Intn(len(xs))\n\tx := xs[i]\n\treturn x, append(xs[:i], xs[i+1:]...)\n}\n\n// pick returns a randomly selected element from the provided slice. pick\n// panics if the provided slice is empty.\n\n\n\n\n\n\n\n// flip returns true with probability p. For example, flip(0) always returns\n// false, flip(1) always returns true, and flip(0.5) returns true half the\n// time. flip panics if p is not in the range [0, 1].\nfunc flip(r *rand.Rand, p float64) bool {\n\tif p < 0 || p > 1 {\n\t\tpanic(fmt.Errorf(\"flip: probability %f not in range [0, 1.0]\", p))\n\t}\n\treturn r.Float64() <= p\n}\n\n// ints represents a remove-only set of integers in the range [low, high).\ntype ints struct {\n\tlow, high int\n\n\t// The integers in the set in no particular order.\n\telements []int\n\n\t// indices[x-low] is the index of element x in elements, or -1 if x is not\n\t// in the set.\n\tindices []int\n}\n\n// reset resets a set of integers to the range [low, high).\n// reset panics if low >= high.\nfunc (i *ints) reset(low, high int) {\n\tif low >= high {\n\t\tpanic(fmt.Errorf(\"newInts: low (%d) >= high (%d)\", low, high))\n\t}\n\n\ti.low = low\n\ti.high = high\n\tn := high - low\n\tif i.elements == nil {\n\t\ti.elements = make([]int, n)\n\t}\n\ti.elements = i.elements[:0]\n\tif i.indices == nil {\n\t\ti.indices = make([]int, n)\n\t}\n\ti.indices = i.indices[:0]\n\n\tfor j := 0; j < n; j++ {\n\t\ti.elements = append(i.elements, low+j)\n\t\ti.indices = append(i.indices, j)\n\t}\n}\n\n// has returns whether the provided integer is in the set.\nfunc (i *ints) has(x int) bool {\n\treturn i.low <= x && x < i.high && i.indices[x-i.low] != -1\n}\n\n// size returns the size of the set.\nfunc (i *ints) size() int {\n\treturn len(i.elements)\n}\n\n// pick returns a random element of the set.\nfunc (i *ints) pick(r *rand.Rand) int {\n\treturn i.elements[r.Intn(len(i.elements))]\n}\n\n// remove removes the provided element from the set. remove is a noop if the\n// provided element is not in the set.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// wyrand is an implementation of the wyrand pseudorandom number generator\n// algorithm from [1, 2]. This implementation also borrows from [3] and [4].\n//\n// 1: https://github.com/wangyi-fudan/wyhash\n// 2: https://github.com/wangyi-fudan/wyhash/blob/master/Modern%20Non-Cryptographic%20Hash%20Function%20and%20Pseudorandom%20Number%20Generator.pdf\n// 3: https://github.com/lemon-mint/exp-pkgs/blob/v0.0.25/hash/wyhash/wyhash.go\n// 4: https://github.com/dsincl12/wyrand/blob/5f074aba21f4f9022d8d73139357bf816fdf1c93/wyrand.go\ntype wyrand struct {\n\tseed uint64\n}\n\nvar _ rand.Source = &wyrand{}\nvar _ rand.Source64 = &wyrand{}\n\n// Seed implements the rand.Source interface.\nfunc (w *wyrand) Seed(seed int64) {\n\tw.seed = uint64(seed)\n}\n\n// Int63 implements the rand.Source interface.\nfunc (w *wyrand) Int63() int64 {\n\treturn int64(w.Uint64() >> 1)\n}\n\n// Uint64 implements the rand.Source64 interface.\nfunc (w *wyrand) Uint64() uint64 {\n\tw.seed += 0xa0761d6478bd642f\n\treturn wymix(w.seed, w.seed^0xe7037ed1a0b428db)\n\n}\n\n// See https://github.com/wangyi-fudan/wyhash for explanation.\nfunc wymix(x uint64, y uint64) uint64 {\n\thi, lo := bits.Mul64(x, y)\n\treturn hi ^ lo\n}\n\n```\n# Function Signature: func pick[T any](r *rand.Rand, xs []T) T {\n# Natural Language Annotations: // pick returns a randomly selected element from the provided slice. pick\n// panics if the provided slice is empty.\nPlease return the generated code surrounded by ```\n", "response": ["!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"]}, "weaver-sim/rand.go-remove": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/sim/rand.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage sim\n\nimport (\n\t\"fmt\"\n\t\"math/bits\"\n\t\"math/rand\"\n)\n\n// pop pops and returns a randomly selected element from the provided slice.\n// pop panics if the provided slice is empty.\nfunc pop[T any](r *rand.Rand, xs []T) (T, []T) {\n\tif len(xs) == 0 {\n\t\tpanic(fmt.Errorf(\"pop: empty slice\"))\n\t}\n\ti := r.Intn(len(xs))\n\tx := xs[i]\n\treturn x, append(xs[:i], xs[i+1:]...)\n}\n\n// pick returns a randomly selected element from the provided slice. pick\n// panics if the provided slice is empty.\n\n\n\n\n\n\n\n// flip returns true with probability p. For example, flip(0) always returns\n// false, flip(1) always returns true, and flip(0.5) returns true half the\n// time. flip panics if p is not in the range [0, 1].\nfunc flip(r *rand.Rand, p float64) bool {\n\tif p < 0 || p > 1 {\n\t\tpanic(fmt.Errorf(\"flip: probability %f not in range [0, 1.0]\", p))\n\t}\n\treturn r.Float64() <= p\n}\n\n// ints represents a remove-only set of integers in the range [low, high).\ntype ints struct {\n\tlow, high int\n\n\t// The integers in the set in no particular order.\n\telements []int\n\n\t// indices[x-low] is the index of element x in elements, or -1 if x is not\n\t// in the set.\n\tindices []int\n}\n\n// reset resets a set of integers to the range [low, high).\n// reset panics if low >= high.\nfunc (i *ints) reset(low, high int) {\n\tif low >= high {\n\t\tpanic(fmt.Errorf(\"newInts: low (%d) >= high (%d)\", low, high))\n\t}\n\n\ti.low = low\n\ti.high = high\n\tn := high - low\n\tif i.elements == nil {\n\t\ti.elements = make([]int, n)\n\t}\n\ti.elements = i.elements[:0]\n\tif i.indices == nil {\n\t\ti.indices = make([]int, n)\n\t}\n\ti.indices = i.indices[:0]\n\n\tfor j := 0; j < n; j++ {\n\t\ti.elements = append(i.elements, low+j)\n\t\ti.indices = append(i.indices, j)\n\t}\n}\n\n// has returns whether the provided integer is in the set.\nfunc (i *ints) has(x int) bool {\n\treturn i.low <= x && x < i.high && i.indices[x-i.low] != -1\n}\n\n// size returns the size of the set.\nfunc (i *ints) size() int {\n\treturn len(i.elements)\n}\n\n// pick returns a random element of the set.\nfunc (i *ints) pick(r *rand.Rand) int {\n\treturn i.elements[r.Intn(len(i.elements))]\n}\n\n// remove removes the provided element from the set. remove is a noop if the\n// provided element is not in the set.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// wyrand is an implementation of the wyrand pseudorandom number generator\n// algorithm from [1, 2]. This implementation also borrows from [3] and [4].\n//\n// 1: https://github.com/wangyi-fudan/wyhash\n// 2: https://github.com/wangyi-fudan/wyhash/blob/master/Modern%20Non-Cryptographic%20Hash%20Function%20and%20Pseudorandom%20Number%20Generator.pdf\n// 3: https://github.com/lemon-mint/exp-pkgs/blob/v0.0.25/hash/wyhash/wyhash.go\n// 4: https://github.com/dsincl12/wyrand/blob/5f074aba21f4f9022d8d73139357bf816fdf1c93/wyrand.go\ntype wyrand struct {\n\tseed uint64\n}\n\nvar _ rand.Source = &wyrand{}\nvar _ rand.Source64 = &wyrand{}\n\n// Seed implements the rand.Source interface.\nfunc (w *wyrand) Seed(seed int64) {\n\tw.seed = uint64(seed)\n}\n\n// Int63 implements the rand.Source interface.\nfunc (w *wyrand) Int63() int64 {\n\treturn int64(w.Uint64() >> 1)\n}\n\n// Uint64 implements the rand.Source64 interface.\nfunc (w *wyrand) Uint64() uint64 {\n\tw.seed += 0xa0761d6478bd642f\n\treturn wymix(w.seed, w.seed^0xe7037ed1a0b428db)\n\n}\n\n// See https://github.com/wangyi-fudan/wyhash for explanation.\nfunc wymix(x uint64, y uint64) uint64 {\n\thi, lo := bits.Mul64(x, y)\n\treturn hi ^ lo\n}\n\n```\n# Function Signature: func (i *ints) remove(x int) {\n# Natural Language Annotations: // remove removes the provided element from the set. remove is a noop if the\n// provided element is not in the set.\nPlease return the generated code surrounded by ```\n", "response": ["!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"]}, "weaver-runtime/config.go-ParseConfig": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/config.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage runtime\n\nimport (\n\t\"fmt\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/BurntSushi/toml\"\n\t\"github.com/ServiceWeaver/weaver/internal/env\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n)\n\n// ParseConfig parses the specified configuration input, which should\n// hold a set of sections in TOML format from the specified file.\n// The section corresponding to the common Service Weaver application\n// configuration is parsed and returned as a *AppConfig.\n//\n// sectionValidator(key, val) is used to validate every section config entry.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ParseConfigSection parses the config section for key into dst.\n// If shortKey is not empty, either key or shortKey is accepted.\n// If the named section is not found, returns nil without changing dst.\nfunc ParseConfigSection(key, shortKey string, sections map[string]string, dst any) error {\n\tsection, ok := sections[key]\n\tif shortKey != \"\" {\n\t\t// Fetch section listed for shortKey, if any.\n\t\tif shortKeySection, ok2 := sections[shortKey]; ok2 {\n\t\t\tif ok {\n\t\t\t\treturn fmt.Errorf(\"conflicting sections %q and %q\", shortKey, key)\n\t\t\t}\n\t\t\tkey, section, ok = shortKey, shortKeySection, ok2\n\t\t}\n\t}\n\tif !ok { // not found\n\t\treturn nil\n\t}\n\n\t// Parse and validate the section.\n\tmd, err := toml.Decode(section, dst)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif unknown := md.Undecoded(); len(unknown) != 0 {\n\t\treturn fmt.Errorf(\"section %q has unknown keys %v\", key, unknown)\n\t}\n\tif x, ok := dst.(interface{ Validate() error }); ok {\n\t\tif err := x.Validate(); err != nil {\n\t\t\treturn fmt.Errorf(\"section %q: %w\", key, err)\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc extractApp(file string, config *protos.AppConfig) error {\n\tconst appKey = \"github.com/ServiceWeaver/weaver\"\n\tconst shortAppKey = \"serviceweaver\"\n\n\t// appConfig holds the data from under appKey in the TOML config.\n\t// It matches the contents of the Config proto.\n\ttype appConfig struct {\n\t\tName     string\n\t\tBinary   string\n\t\tArgs     []string\n\t\tEnv      []string\n\t\tColocate [][]string\n\t\tRollout  time.Duration\n\t}\n\n\tparsed := &appConfig{}\n\tif err := ParseConfigSection(appKey, shortAppKey, config.Sections, parsed); err != nil {\n\t\treturn err\n\t}\n\n\t// Move struct fields into proto.\n\tconfig.Name = parsed.Name\n\tconfig.Binary = parsed.Binary\n\tconfig.Args = parsed.Args\n\tconfig.Env = parsed.Env\n\tconfig.RolloutNanos = int64(parsed.Rollout)\n\tfor _, colocate := range parsed.Colocate {\n\t\tgroup := &protos.ComponentGroup{Components: colocate}\n\t\tconfig.Colocate = append(config.Colocate, group)\n\t}\n\n\t// Canonicalize the config.\n\tif err := canonicalizeConfig(config, filepath.Dir(file)); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// canonicalizeConfig updates the provided config to canonical\n// form. All relative paths inside the configuration are resolved\n// relative to the provided directory.\nfunc canonicalizeConfig(c *protos.AppConfig, dir string) error {\n\t// Fill in the application name if necessary.\n\tbin := c.GetBinary()\n\tif c.Name == \"\" && bin != \"\" {\n\t\tc.Name = filepath.Base(bin)\n\t}\n\n\t// Convert relative paths inside the application config to absolute paths\n\t// interpreted starting at the directory containing the config file.\n\tif !filepath.IsAbs(bin) {\n\t\tbin, err := filepath.Abs(filepath.Join(dir, bin))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tc.Binary = bin\n\t}\n\n\t// Validate the environment variables.\n\tif _, err := env.Parse(c.Env); err != nil {\n\t\treturn fmt.Errorf(\"invalid Env: %v\", err)\n\t}\n\n\t// Validate the same_process entry.\n\tif err := checkSameProcess(c); err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\n// checkSameProcess checks that the same_process entry is valid.\nfunc checkSameProcess(c *protos.AppConfig) error {\n\tseen := map[string]struct{}{}\n\tfor _, components := range c.Colocate {\n\t\tfor _, component := range components.Components {\n\t\t\tif _, ok := seen[component]; ok {\n\t\t\t\treturn fmt.Errorf(\"component %q placed multiple times\", component)\n\t\t\t}\n\t\t\tseen[component] = struct{}{}\n\t\t}\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func ParseConfig(file string, input string, sectionValidator func(string, string) error) (*protos.AppConfig, error) {\n# Natural Language Annotations: // ParseConfig parses the specified configuration input, which should\n// hold a set of sections in TOML format from the specified file.\n// The section corresponding to the common Service Weaver application\n// configuration is parsed and returned as a *AppConfig.\n//\n// sectionValidator(key, val) is used to validate every section config entry.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage runtime\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"path/filepath\"\n\t\"strings\"\n\n\t\"github.com/BurntSushi/toml\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n)\n\n// ParseConfig parses the specified configuration input, which should\n// hold a set of sections in TOML format from the specified file.\n// The section corresponding to the common Service Weaver application\n// configuration is parsed and returned as a *AppConfig.\n//\n// sectionValidator(key, val) is used to validate every section config entry.\nfunc ParseConfig(file string, input string, sectionValidator func(string, string) error) (*protos.AppConfig, error) {\n\t// Parse the input as a TOML file.\n\tvar sections map[string]string\n\t_, err := toml.Decode(input, &sections)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to parse TOML: %w\", err)\n\t}\n\n\t// Validate each section.\n\tfor key, val := range sections {\n\t\tif err := sectionValidator(key, val); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to validate section %q: %w\", key, err)\n\t\t}\n\t}\n\n\t// Parse the application section.\n\tconfig := &protos.AppConfig{}\n\tif err := extractApp(file, config); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to extract application config: %w\", err)\n\t}\n\n\treturn config, nil\n}\n```\nThis code defines a `ParseConfig` function that takes a file path, a configuration input string, and a sectionValidator function as arguments. It first parses the input string as a TOML file into a map of sections. It then validates each section using the provided sectionValidator function. If any section fails validation, the function returns an error.\n\nAfter validating all sections, it extracts the application section using the `extractApp` function and returns the parsed `protos.AppConfig` object. If the extraction fails, the function returns an error.", "```go\nfunc ParseConfig(file string, input string, sectionValidator func(string, string) error) (*protos.AppConfig, error) {\n\tconfig := &protos.AppConfig{}\n\t_, err := toml.Decode(input, &config.Sections)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tfor key, val := range config.Sections {\n\t\tif err := sectionValidator(key, val); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tif err := extractApp(file, config); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn config, nil\n}\n```", "```go\nimport (\n\t\"fmt\"\n\t\"github.com/BurntSushi/toml\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"io/ioutil\"\n)\n\n// ParseConfig parses the specified configuration input, which should\n// hold a set of sections in TOML format from the specified file.\n// The section corresponding to the common Service Weaver application\n// configuration is parsed and returned as a *AppConfig.\n//\n// sectionValidator(key, val) is used to validate every section config entry.\nfunc ParseConfig(file string, input string, sectionValidator func(string, string) error) (*protos.AppConfig, error) {\n\t// Read the config file.\n\tdata, err := ioutil.ReadFile(file)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"could not read config file %q: %w\", file, err)\n\t}\n\n\t// Parse the config file.\n\tvar config protos.AppConfig\n\tif _, err := toml.Decode(string(data), &config); err != nil {\n\t\treturn nil, fmt.Errorf(\"could not parse config file %q: %w\", file, err)\n\t}\n\n\t// Validate each section.\n\tfor key, val := range config.Sections {\n\t\tif err := sectionValidator(key, val); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"invalid section %q: %w\", key, err)\n\t\t}\n\t}\n\n\t// Extract the application config.\n\tif err := extractApp(file, &config); err != nil {\n\t\treturn nil, fmt.Errorf(\"could not extract app config: %w\", err)\n\t}\n\n\treturn &config, nil\n}\n```"]}, "weaver-runtime/traces/db.go-OpenDB": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/traces/db.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package perfetto contains libraries for displaying trace information in the\n// Perfetto UI.\npackage traces\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/hex\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"net/http\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"time\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/ServiceWeaver/weaver/runtime/retry\"\n\t\"google.golang.org/protobuf/proto\"\n\t\"modernc.org/sqlite\"\n\tsqlite3 \"modernc.org/sqlite/lib\"\n)\n\n// DB is a trace database that stores traces on the local file system.\ntype DB struct {\n\t// Trace data is stored in a sqlite DB spread across two tables:\n\t// (1) traces:         serialized trace data, used for querying.\n\t// (2) encoded_spans:  full encoded span data, used for fetching all of the\n\t//                     spans that belong to a given trace.\n\tfname string\n\tdb    *sql.DB\n}\n\n// OpenDB opens the trace database persisted in the provided file. If the\n// file doesn't exist, this call creates it.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Close closes the trace database.\nfunc (d *DB) Close() error {\n\treturn d.db.Close()\n}\n\n// Store stores the given trace spans in the database.\nfunc (d *DB) Store(ctx context.Context, app, version string, spans *protos.TraceSpans) error {\n\t// NOTE: we insert all rows transactionally, as it is significantly faster\n\t// than inserting one row at a time [1].\n\t//\n\t// [1]: https://stackoverflow.com/questions/1711631/improve-insert-per-second-performance-of-sqlite\n\ttx, err := d.db.BeginTx(ctx, &sql.TxOptions{Isolation: sql.LevelLinearizable})\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer tx.Rollback()\n\tvar errs []error\n\tfor _, span := range spans.Span {\n\t\tif isRootSpan(span) {\n\t\t\tif err := d.storeTrace(ctx, tx, app, version, span); err != nil {\n\t\t\t\terrs = append(errs, err)\n\t\t\t}\n\t\t}\n\t\tif err := d.storeSpan(ctx, tx, span); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\tif errs != nil {\n\t\treturn errors.Join(errs...)\n\t}\n\treturn tx.Commit()\n}\n\nfunc (d *DB) storeTrace(ctx context.Context, tx *sql.Tx, app, version string, root *protos.Span) error {\n\tconst traceStmt = `INSERT INTO traces VALUES (?,?,?,?,?,?,?)`\n\t_, err := tx.ExecContext(ctx, traceStmt, hex.EncodeToString(root.TraceId), app, version, root.Name, root.StartMicros, root.EndMicros, spanStatus(root))\n\treturn err\n}\n\nfunc (d *DB) storeSpan(ctx context.Context, tx *sql.Tx, span *protos.Span) error {\n\tencoded, err := proto.Marshal(span)\n\tif err != nil {\n\t\treturn err\n\t}\n\tconst stmt = `INSERT INTO encoded_spans VALUES (?,?,?)`\n\t_, err = tx.ExecContext(ctx, stmt, hex.EncodeToString(span.TraceId), span.StartMicros, encoded)\n\treturn err\n}\n\n// TraceSummary stores summary information about a trace.\ntype TraceSummary struct {\n\tTraceID            string    // Unique trace identifier, in hex format.\n\tStartTime, EndTime time.Time // Start and end times for the trace.\n\tStatus             string    // Trace status string.\n}\n\n// QueryTraces returns the summaries of the traces that match the given\n// query arguments, namely:\n//   - That have been generated by the given application version.\n//   - That fit entirely in the given [startTime, endTime] time interval.\n//   - Whose duration is in the given [durationLower, durationUpper) range.\n//   - Who have an error status.\n//   - Who are in the most recent limit of trace spans.\n//\n// Any query argument that has a zero value (e.g., empty app or version,\n// zero endTime) is ignored, i.e., it matches all spans.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FetchSpans returns all of the spans that have a given trace id.\nfunc (d *DB) FetchSpans(ctx context.Context, traceID string) ([]*protos.Span, error) {\n\tconst query = `SELECT data FROM encoded_spans WHERE trace_id=?`\n\trows, err := d.queryDB(ctx, query, traceID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer rows.Close()\n\tvar spans []*protos.Span\n\tfor rows.Next() {\n\t\tvar encoded []byte\n\t\tif err := rows.Scan(&encoded); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tspan := &protos.Span{}\n\t\tif err := proto.Unmarshal(encoded, span); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tspans = append(spans, span)\n\t}\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\treturn spans, nil\n}\n\nfunc (d *DB) queryDB(ctx context.Context, query string, args ...any) (*sql.Rows, error) {\n\t// Keep retrying as long as we are getting the \"locked\" error.\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\trows, err := d.db.QueryContext(ctx, query, args...)\n\t\tif isLocked(err) {\n\t\t\tcontinue\n\t\t}\n\t\treturn rows, err\n\t}\n\treturn nil, ctx.Err()\n}\n\nfunc (d *DB) execDB(ctx context.Context, query string, args ...any) (sql.Result, error) {\n\t// Keep retrying as long as we are getting the \"locked\" error.\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\tres, err := d.db.ExecContext(ctx, query, args...)\n\t\tif isLocked(err) {\n\t\t\tcontinue\n\t\t}\n\t\treturn res, err\n\t}\n\treturn nil, ctx.Err()\n}\n\n// isLocked returns whether the error is a \"database is locked\" error.\nfunc isLocked(err error) bool {\n\tsqlError := &sqlite.Error{}\n\tok := errors.As(err, &sqlError)\n\treturn ok && (sqlError.Code() == sqlite3.SQLITE_BUSY || sqlError.Code() == sqlite3.SQLITE_LOCKED)\n}\n\n// isRootSpan returns true iff the given span is a root span.\nfunc isRootSpan(span *protos.Span) bool {\n\tvar nilSpanID [8]byte\n\treturn bytes.Equal(span.ParentSpanId, nilSpanID[:])\n}\n\n// spanStatus returns the span status string. It returns \"\" if the status is OK.\nfunc spanStatus(span *protos.Span) string {\n\t// Look for an error in the span status.\n\tif span.Status != nil && span.Status.Code == protos.Span_Status_ERROR {\n\t\tif span.Status.Error != \"\" {\n\t\t\treturn span.Status.Error\n\t\t} else {\n\t\t\treturn \"unknown error\"\n\t\t}\n\t}\n\n\t// Look for an HTTP error in the span attributes.\n\tfor _, attr := range span.Attributes {\n\t\tif attr.Key != \"http.status_code\" {\n\t\t\tcontinue\n\t\t}\n\t\tif attr.Value.Type != protos.Span_Attribute_Value_INT64 {\n\t\t\tcontinue\n\t\t}\n\t\tval, ok := attr.Value.Value.(*protos.Span_Attribute_Value_Num)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif val.Num >= 400 && val.Num < 600 {\n\t\t\treturn http.StatusText(int(val.Num))\n\t\t}\n\t}\n\n\t// No errors found.\n\treturn \"\"\n}\n\n```\n# Function Signature: func OpenDB(ctx context.Context, fname string) (*DB, error) {\n# Natural Language Annotations: // OpenDB opens the trace database persisted in the provided file. If the\n// file doesn't exist, this call creates it.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc OpenDB(ctx context.Context, fname string) (*DB, error) {\n\tif _, err := os.Stat(fname); os.IsNotExist(err) {\n\t\t// If the file doesn't exist, we create it.\n\t\tfile, err := os.Create(fname)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tfile.Close()\n\t}\n\n\tdb, err := sql.Open(\"sqlite\", fname)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &DB{\n\t\tfname: fname,\n\t\tdb:    db,\n\t}, nil\n}\n```", "```go\nfunc OpenDB(ctx context.Context, fname string) (*DB, error) {\n\tdb := &DB{fname: fname}\n\n\t// Check if the file does not exist.\n\tif _, err := os.Stat(fname); os.IsNotExist(err) {\n\t\t// Create the directory if it does not exist.\n\t\tdir := filepath.Dir(fname)\n\t\tif _, err := os.Stat(dir); os.IsNotExist(err) {\n\t\t\tif err = os.MkdirAll(dir, 0755); err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"failed to create directory %s: %w\", dir, err)\n\t\t\t}\n\t\t}\n\n\t\t// Create the file.\n\t\tfile, err := os.Create(fname)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"failed to create file %s: %w\", fname, err)\n\t\t}\n\t\tfile.Close()\n\t}\n\n\t// Open the database.\n\tvar err error\n\tif db.db, err = sql.Open(\"sqlite\", fname); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to open database %s: %w\", fname, err)\n\t}\n\n\t// Initialize the database if it is not initialized.\n\tif _, err = db.db.ExecContext(ctx, `\n\t\tCREATE TABLE IF NOT EXISTS traces (\n\t\t\ttrace_id TEXT PRIMARY KEY,\n\t\t\tapp TEXT,\n\t\t\tversion TEXT,\n\t\t\troot_span_name TEXT,\n\t\t\tstart_micros INTEGER,\n\t\t\tend_micros INTEGER,\n\t\t\tstatus TEXT\n\t\t);\n\t\tCREATE TABLE IF NOT EXISTS encoded_spans (\n\t\t\ttrace_id TEXT,\n\t\t\tstart_micros INTEGER,\n\t\t\tdata BLOB,\n\t\t\tFOREIGN KEY(trace_id) REFERENCES traces(trace_id)\n\t\t);\n\t`); err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to initialize database %s: %w\", fname, err)\n\t}\n\n\treturn db, nil\n}\n```", "```go\nfunc OpenDB(ctx context.Context, fname string) (*DB, error) {\n\t// Create the directory for the database file if it doesn't exist.\n\tif err := os.MkdirAll(filepath.Dir(fname), 0755); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Open the database.\n\tdb, err := sql.Open(\"sqlite\", fname)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Create the traces table if it doesn't exist.\n\tif _, err := db.ExecContext(ctx, `\n\t\tCREATE TABLE IF NOT EXISTS traces (\n\t\t\ttrace_id TEXT PRIMARY KEY,\n\t\t\tapp TEXT,\n\t\t\tversion TEXT,\n\t\t\tname TEXT,\n\t\t\tstart_micros INTEGER,\n\t\t\tend_micros INTEGER,\n\t\t\tstatus TEXT\n\t\t)\n\t`); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Create the encoded_spans table if it doesn't exist.\n\tif _, err := db.ExecContext(ctx, `\n\t\tCREATE TABLE IF NOT EXISTS encoded_spans (\n\t\t\ttrace_id TEXT,\n\t\t\tstart_micros INTEGER,\n\t\t\tdata BLOB,\n\t\t\tFOREIGN KEY(trace_id) REFERENCES traces(trace_id)\n\t\t)\n\t`); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &DB{fname: fname, db: db}, nil\n}\n```"]}, "weaver-runtime/traces/db.go-QueryTraces": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/traces/db.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package perfetto contains libraries for displaying trace information in the\n// Perfetto UI.\npackage traces\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/hex\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"net/http\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"time\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/ServiceWeaver/weaver/runtime/retry\"\n\t\"google.golang.org/protobuf/proto\"\n\t\"modernc.org/sqlite\"\n\tsqlite3 \"modernc.org/sqlite/lib\"\n)\n\n// DB is a trace database that stores traces on the local file system.\ntype DB struct {\n\t// Trace data is stored in a sqlite DB spread across two tables:\n\t// (1) traces:         serialized trace data, used for querying.\n\t// (2) encoded_spans:  full encoded span data, used for fetching all of the\n\t//                     spans that belong to a given trace.\n\tfname string\n\tdb    *sql.DB\n}\n\n// OpenDB opens the trace database persisted in the provided file. If the\n// file doesn't exist, this call creates it.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Close closes the trace database.\nfunc (d *DB) Close() error {\n\treturn d.db.Close()\n}\n\n// Store stores the given trace spans in the database.\nfunc (d *DB) Store(ctx context.Context, app, version string, spans *protos.TraceSpans) error {\n\t// NOTE: we insert all rows transactionally, as it is significantly faster\n\t// than inserting one row at a time [1].\n\t//\n\t// [1]: https://stackoverflow.com/questions/1711631/improve-insert-per-second-performance-of-sqlite\n\ttx, err := d.db.BeginTx(ctx, &sql.TxOptions{Isolation: sql.LevelLinearizable})\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer tx.Rollback()\n\tvar errs []error\n\tfor _, span := range spans.Span {\n\t\tif isRootSpan(span) {\n\t\t\tif err := d.storeTrace(ctx, tx, app, version, span); err != nil {\n\t\t\t\terrs = append(errs, err)\n\t\t\t}\n\t\t}\n\t\tif err := d.storeSpan(ctx, tx, span); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\tif errs != nil {\n\t\treturn errors.Join(errs...)\n\t}\n\treturn tx.Commit()\n}\n\nfunc (d *DB) storeTrace(ctx context.Context, tx *sql.Tx, app, version string, root *protos.Span) error {\n\tconst traceStmt = `INSERT INTO traces VALUES (?,?,?,?,?,?,?)`\n\t_, err := tx.ExecContext(ctx, traceStmt, hex.EncodeToString(root.TraceId), app, version, root.Name, root.StartMicros, root.EndMicros, spanStatus(root))\n\treturn err\n}\n\nfunc (d *DB) storeSpan(ctx context.Context, tx *sql.Tx, span *protos.Span) error {\n\tencoded, err := proto.Marshal(span)\n\tif err != nil {\n\t\treturn err\n\t}\n\tconst stmt = `INSERT INTO encoded_spans VALUES (?,?,?)`\n\t_, err = tx.ExecContext(ctx, stmt, hex.EncodeToString(span.TraceId), span.StartMicros, encoded)\n\treturn err\n}\n\n// TraceSummary stores summary information about a trace.\ntype TraceSummary struct {\n\tTraceID            string    // Unique trace identifier, in hex format.\n\tStartTime, EndTime time.Time // Start and end times for the trace.\n\tStatus             string    // Trace status string.\n}\n\n// QueryTraces returns the summaries of the traces that match the given\n// query arguments, namely:\n//   - That have been generated by the given application version.\n//   - That fit entirely in the given [startTime, endTime] time interval.\n//   - Whose duration is in the given [durationLower, durationUpper) range.\n//   - Who have an error status.\n//   - Who are in the most recent limit of trace spans.\n//\n// Any query argument that has a zero value (e.g., empty app or version,\n// zero endTime) is ignored, i.e., it matches all spans.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FetchSpans returns all of the spans that have a given trace id.\nfunc (d *DB) FetchSpans(ctx context.Context, traceID string) ([]*protos.Span, error) {\n\tconst query = `SELECT data FROM encoded_spans WHERE trace_id=?`\n\trows, err := d.queryDB(ctx, query, traceID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer rows.Close()\n\tvar spans []*protos.Span\n\tfor rows.Next() {\n\t\tvar encoded []byte\n\t\tif err := rows.Scan(&encoded); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tspan := &protos.Span{}\n\t\tif err := proto.Unmarshal(encoded, span); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tspans = append(spans, span)\n\t}\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\treturn spans, nil\n}\n\nfunc (d *DB) queryDB(ctx context.Context, query string, args ...any) (*sql.Rows, error) {\n\t// Keep retrying as long as we are getting the \"locked\" error.\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\trows, err := d.db.QueryContext(ctx, query, args...)\n\t\tif isLocked(err) {\n\t\t\tcontinue\n\t\t}\n\t\treturn rows, err\n\t}\n\treturn nil, ctx.Err()\n}\n\nfunc (d *DB) execDB(ctx context.Context, query string, args ...any) (sql.Result, error) {\n\t// Keep retrying as long as we are getting the \"locked\" error.\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\tres, err := d.db.ExecContext(ctx, query, args...)\n\t\tif isLocked(err) {\n\t\t\tcontinue\n\t\t}\n\t\treturn res, err\n\t}\n\treturn nil, ctx.Err()\n}\n\n// isLocked returns whether the error is a \"database is locked\" error.\nfunc isLocked(err error) bool {\n\tsqlError := &sqlite.Error{}\n\tok := errors.As(err, &sqlError)\n\treturn ok && (sqlError.Code() == sqlite3.SQLITE_BUSY || sqlError.Code() == sqlite3.SQLITE_LOCKED)\n}\n\n// isRootSpan returns true iff the given span is a root span.\nfunc isRootSpan(span *protos.Span) bool {\n\tvar nilSpanID [8]byte\n\treturn bytes.Equal(span.ParentSpanId, nilSpanID[:])\n}\n\n// spanStatus returns the span status string. It returns \"\" if the status is OK.\nfunc spanStatus(span *protos.Span) string {\n\t// Look for an error in the span status.\n\tif span.Status != nil && span.Status.Code == protos.Span_Status_ERROR {\n\t\tif span.Status.Error != \"\" {\n\t\t\treturn span.Status.Error\n\t\t} else {\n\t\t\treturn \"unknown error\"\n\t\t}\n\t}\n\n\t// Look for an HTTP error in the span attributes.\n\tfor _, attr := range span.Attributes {\n\t\tif attr.Key != \"http.status_code\" {\n\t\t\tcontinue\n\t\t}\n\t\tif attr.Value.Type != protos.Span_Attribute_Value_INT64 {\n\t\t\tcontinue\n\t\t}\n\t\tval, ok := attr.Value.Value.(*protos.Span_Attribute_Value_Num)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif val.Num >= 400 && val.Num < 600 {\n\t\t\treturn http.StatusText(int(val.Num))\n\t\t}\n\t}\n\n\t// No errors found.\n\treturn \"\"\n}\n\n```\n# Function Signature: func (d *DB) QueryTraces(ctx context.Context, app, version string, startTime, endTime time.Time, durationLower, durationUpper time.Duration, onlyErrors bool, limit int64) ([]TraceSummary, error) {\n# Natural Language Annotations: // QueryTraces returns the summaries of the traces that match the given\n// query arguments, namely:\n//   - That have been generated by the given application version.\n//   - That fit entirely in the given [startTime, endTime] time interval.\n//   - Whose duration is in the given [durationLower, durationUpper) range.\n//   - Who have an error status.\n//   - Who are in the most recent limit of trace spans.\n//\n// Any query argument that has a zero value (e.g., empty app or version,\n// zero endTime) is ignored, i.e., it matches all spans.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (d *DB) QueryTraces(ctx context.Context, app, version string, startTime, endTime time.Time, durationLower, durationUpper time.Duration, onlyErrors bool, limit int64) ([]TraceSummary, error) {\n\tvar filters []string\n\tvar args []interface{}\n\tif app != \"\" && version != \"\" {\n\t\tfilters = append(filters, \"app=? AND version=?\")\n\t\targs = append(args, app, version)\n\t}\n\tif !startTime.IsZero() && !endTime.IsZero() {\n\t\tfilters = append(filters, \"start_time>=? AND end_time<=?\")\n\t\targs = append(args, startTime.UnixNano()/1e3, endTime.UnixNano()/1e3)\n\t}\n\tif durationLower != 0 && durationUpper != 0 {\n\t\tfilters = append(filters, \"duration>=? AND duration<?\")\n\t\targs = append(args, durationLower.Microseconds(), durationUpper.Microseconds())\n\t}\n\tif onlyErrors {\n\t\tfilters = append(filters, \"status<>''\")\n\t}\n\tvar filter string\n\tif len(filters) > 0 {\n\t\tfilter = \" WHERE \" + strings.Join(filters, \" AND \")\n\t}\n\tquery := fmt.Sprintf(`\n\t\tSELECT trace_id, start_time, end_time, status\n\t\tFROM traces\n\t\t%s\n\t\tORDER BY start_time DESC\n\t\tLIMIT ?\n\t`, filter)\n\targs = append(args, limit)\n\trows, err := d.db.QueryContext(ctx, query, args...)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer rows.Close()\n\n\tvar summaries []TraceSummary\n\tfor rows.Next() {\n\t\tvar summary TraceSummary\n\t\tvar startTimeMicros, endTimeMicros int64\n\t\tif err := rows.Scan(&summary.TraceID, &startTimeMicros, &endTimeMicros, &summary.Status); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tsummary.StartTime = time.Unix(0, startTimeMicros*1e3)\n\t\tsummary.EndTime = time.Unix(0, endTimeMicros*1e3)\n\t\tsummaries = append(summaries, summary)\n\t}\n\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn summaries, nil\n}\n```", "```go\nfunc (d *DB) QueryTraces(ctx context.Context, app, version string, startTime, endTime time.Time, durationLower, durationUpper time.Duration, onlyErrors bool, limit int64) ([]TraceSummary, error) {\n\tconst query = `\n\t\tSELECT \n\t\t\ttrace_id, \n\t\t\tstart_time, \n\t\t\tend_time, \n\t\t\tstatus \n\t\tFROM traces \n\t\tWHERE \n\t\t\tapp = ? \n\t\t\tAND version = ? \n\t\t\tAND start_time >= ? \n\t\t\tAND end_time <= ? \n\t\t\tAND duration_micros >= ? \n\t\t\tAND duration_micros < ? \n\t\t\tAND (status != '' OR ? = 0) \n\t\tORDER BY end_time DESC \n\t\tLIMIT ?\n\t`\n\trows, err := d.queryDB(ctx, query, app, version, startTime, endTime, durationLower.Microseconds(), durationUpper.Microseconds(), onlyErrors, limit)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer rows.Close()\n\n\tvar summaries []TraceSummary\n\tfor rows.Next() {\n\t\tvar summary TraceSummary\n\t\tif err := rows.Scan(&summary.TraceID, &summary.StartTime, &summary.EndTime, &summary.Status); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tsummaries = append(summaries, summary)\n\t}\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn summaries, nil\n}\n```", "```go\nfunc (d *DB) QueryTraces(ctx context.Context, app, version string, startTime, endTime time.Time, durationLower, durationUpper time.Duration, onlyErrors bool, limit int64) ([]TraceSummary, error) {\n\tconst query = `\n\t\tSELECT\n\t\t\ttrace_id,\n\t\t\tstart_time,\n\t\t\tend_time,\n\t\t\tstatus\n\t\tFROM\n\t\t\ttraces\n\t\tWHERE\n\t\t\t(? = '' OR app = ?)\n\t\t\tAND (? = '0001-01-01' OR start_time >= ?)\n\t\t\tAND (? = '0001-01-01' OR end_time <= ?)\n\t\t\tAND (? = 0 OR end_time - start_time >= ?)\n\t\t\tAND (? = 0 OR end_time - start_time < ?)\n\t\t\tAND (? = '' OR status != '')\n\t\tORDER BY\n\t\t\tstart_time DESC\n\t\tLIMIT ?\n\t`\n\trows, err := d.db.QueryContext(ctx, query, app, app, startTime, startTime, endTime, endTime, durationLower.Nanoseconds(), durationUpper.Nanoseconds(), onlyErrors, limit)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer rows.Close()\n\n\tvar summaries []TraceSummary\n\tfor rows.Next() {\n\t\tvar summary TraceSummary\n\t\tif err := rows.Scan(&summary.TraceID, &summary.StartTime, &summary.EndTime, &summary.Status); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tsummaries = append(summaries, summary)\n\t}\n\tif err := rows.Err(); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn summaries, nil\n}\n```"]}, "weaver-runtime/retry/retry.go-Continue": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/retry/retry.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package retry contains code to perform retries with exponential backoff.\n//\n// Example: loop until doSomething() returns true or context hits deadline or is canceled.\n//\n//\tfor r := retry.Begin(); r.Continue(ctx); {\n//\t  if doSomething() {\n//\t    break\n//\t  }\n//\t}\npackage retry\n\nimport (\n\t\"context\"\n\t\"math\"\n\t\"math/rand\"\n\t\"sync\"\n\t\"time\"\n)\n\n// Retry holds state for managing retry loops with exponential backoff and jitter.\ntype Retry struct {\n\toptions Options\n\tattempt int\n}\n\n// Options are the options that configure a retry loop. Before the ith\n// iteration of a retry loop, retry.Continue() sleeps for a duration of\n// BackoffMinDuration * BackoffMultiplier^i, with added jitter.\ntype Options struct {\n\tBackoffMultiplier  float64 // If specified, must be at least 1.\n\tBackoffMinDuration time.Duration\n}\n\n// DefaultOptions is the default set of Options.\nvar DefaultOptions = Options{\n\tBackoffMultiplier:  1.3,\n\tBackoffMinDuration: 10 * time.Millisecond,\n}\n\nvar (\n\trngMu sync.Mutex\n\trng   *rand.Rand\n)\n\n// Begin initiates a new retry loop.\nfunc Begin() *Retry {\n\treturn BeginWithOptions(DefaultOptions)\n}\n\n// BeginWithOptions returns a new retry loop configured with the provided\n// options.\n//\n// Example: Sleep 1 second, then 2 seconds, then 4 seconds, and so on.\n//\n//\topts := retry.Options{\n//\t  BackoffMultiplier: 2.0,\n//\t  BackoffMinDuration: time.Second,\n//\t}\n//\tfor r := retry.BeginWithOptions(opts); r.Continue(ctx); {\n//\t  // Do nothing.\n//\t}\nfunc BeginWithOptions(options Options) *Retry {\n\treturn &Retry{options: options}\n}\n\n// Continue sleeps for an exponentially increasing interval (with jitter). It\n// stops its sleep early and returns false if context becomes done. If the\n// return value is false, ctx.Err() is guaranteed to be non-nil. The first\n// call does not sleep.\n\n\n\n\n\n\n\n\n// Reset resets a Retry to its initial state. Reset is useful if you want to\n// retry an operation with exponential backoff, but only if it is failing. For\n// example:\n//\n//\tfor r := retry.Begin(); r.Continue(ctx); {\n//\t    if err := doSomething(); err != nil {\n//\t        // Retry with backoff if we fail.\n//\t        continue\n//\t    }\n//\t    // Retry immediately if we succeed.\n//\t    r.Reset()\n//\t}\nfunc (r *Retry) Reset() {\n\tr.attempt = 0\n}\n\nfunc backoffDelay(i int, opts Options) time.Duration {\n\tmult := math.Pow(opts.BackoffMultiplier, float64(i))\n\treturn time.Duration(float64(opts.BackoffMinDuration) * mult)\n}\n\n// randomized sleeps for a random duration close to d, or until context is done,\n// whichever occurs first.\nfunc randomized(ctx context.Context, d time.Duration) {\n\tconst jitter = 0.4\n\tmult := 1 - jitter*randomFloat() // Subtract up to 40%\n\tsleep(ctx, time.Duration(float64(d)*mult))\n}\n\n// sleep sleeps for the specified duration d, or until context is done,\n// whichever occurs first.\nfunc sleep(ctx context.Context, d time.Duration) {\n\tt := time.NewTimer(d)\n\tdefer t.Stop()\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn\n\tcase <-t.C:\n\t}\n}\n\nfunc randomFloat() float64 {\n\t// Do not use the default rng since we do not want different processes\n\t// to pick the same deterministic random sequence.\n\trngMu.Lock()\n\tdefer rngMu.Unlock()\n\tif rng == nil {\n\t\trng = rand.New(rand.NewSource(time.Now().UnixNano()))\n\t}\n\treturn rng.Float64()\n}\n\n```\n# Function Signature: func (r *Retry) Continue(ctx context.Context) bool {\n# Natural Language Annotations: // Continue sleeps for an exponentially increasing interval (with jitter). It\n// stops its sleep early and returns false if context becomes done. If the\n// return value is false, ctx.Err() is guaranteed to be non-nil. The first\n// call does not sleep.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (r *Retry) Continue(ctx context.Context) bool {\n\tif r.attempt == 0 {\n\t\tr.attempt++\n\t\treturn true\n\t}\n\tbackoff := backoffDelay(r.attempt, r.options)\n\trandomized(ctx, backoff)\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn false\n\tdefault:\n\t\tr.attempt++\n\t\treturn true\n\t}\n}\n```", "```go\nfunc (r *Retry) Continue(ctx context.Context) bool {\n\tif r.attempt > 0 {\n\t\tdelay := backoffDelay(r.attempt, r.options)\n\t\trandomized(ctx, delay)\n\t}\n\tr.attempt++\n\tselect {\n\tcase <-ctx.Done():\n\t\treturn false\n\tdefault:\n\t\treturn true\n\t}\n}\n```", "```go\nfunc (r *Retry) Continue(ctx context.Context) bool {\n\tif r.attempt == 0 {\n\t\tr.attempt++\n\t\treturn true\n\t}\n\trandomized(ctx, backoffDelay(r.attempt, r.options))\n\tr.attempt++\n\tif ctx.Err() != nil {\n\t\treturn false\n\t}\n\treturn true\n}\n```"]}, "weaver-runtime/colors/tabularizer.go-Row": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/colors/tabularizer.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage colors\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"strings\"\n)\n\nvar dimColor = Color256(245) // a light gray\n\n// An Atom is a segment of text with a single formatting style.\ntype Atom struct {\n\tS         string // the text\n\tUnderline bool   // is it underlined?\n\tBold      bool   // is it bold?\n\tColor     Code   // text color\n}\n\n// String returns the atom with the appropriate ANSI escape sequences.\nfunc (a Atom) String() string {\n\tif !Enabled() {\n\t\treturn a.S\n\t}\n\n\tvar b strings.Builder\n\tb.WriteString(string(a.Color))\n\tif a.Underline {\n\t\tb.WriteString(string(Underline))\n\t}\n\tif a.Bold {\n\t\tb.WriteString(string(Bold))\n\t}\n\tb.WriteString(a.S)\n\tb.WriteString(string(Reset))\n\treturn b.String()\n}\n\n// dimmed returns a copy of the atom with a dim gray color.\nfunc (a Atom) dimmed() Atom {\n\ta.Color = dimColor\n\treturn a\n}\n\n// Text represents a contiguous sequence of atoms.\ntype Text []Atom\n\n// len returns the length of the printable characters in the text's constituent\n// atoms. ANSI escape sequences are not counted as part of this length.\nfunc (t Text) len() int {\n\treturn len(t.raw())\n}\n\n// raw returns the raw underlying text without any ANSI escape sequences.\nfunc (t Text) raw() string {\n\tvar b strings.Builder\n\tfor _, a := range t {\n\t\tb.WriteString(a.S)\n\t}\n\treturn b.String()\n}\n\n// String returns the text with the appropriate ANSI escape sequences.\nfunc (t Text) String() string {\n\tvar b strings.Builder\n\tfor _, a := range t {\n\t\tb.WriteString(a.String())\n\t}\n\treturn b.String()\n}\n\n// dimmed returns a copy of the text with a dim gray color.\nfunc (t Text) dimmed() Text {\n\tcloned := make(Text, len(t))\n\tfor i, a := range t {\n\t\tcloned[i] = a.dimmed()\n\t}\n\treturn Text(cloned)\n}\n\n// A Tabularizer produces pretty-printed tabularized text. Unlike\n// tabwriter.Writer [1], Tabularizer properly handles text with ANSI escape\n// codes. Here's what an example table looks like:\n//\n//\t\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n//\t\u2502 CATS                  \u2502\n//\t\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n//\t\u2502 NAME   \u2502 AGE \u2502 COLOR  \u2502\n//\t\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n//\t\u2502 belle  \u2502 1y  \u2502 tortie \u2502\n//\t\u2502 sidney \u2502 2y  \u2502 calico \u2502\n//\t\u2502 dakota \u2502 8m  \u2502 tuxedo \u2502\n//\t\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n//\n// The table format comes from duf [2].\n//\n// [1]: https://pkg.go.dev/text/tabwriter\n// [2]: https://github.com/muesli/duf\ntype Tabularizer struct {\n\tw      io.Writer // where to write\n\ttitle  []Text    // table title\n\trows   [][]Text  // buffered rows\n\twidths []int     // column widths\n\tdim    func(prev, row []string) []bool\n}\n\n// NewTabularizer returns a new tabularizer. The provided dim function\n// determines which columns in a row, if any, are dimmed.\nfunc NewTabularizer(w io.Writer, title []Text, dim func(prev, row []string) []bool) *Tabularizer {\n\treturn &Tabularizer{w: w, title: title, dim: dim}\n}\n\n// Row buffers a new Row to be tabularized. The Row isn't written until Flush\n// is called. Note that every Row reported to a tabularizer must be the same\n// length. A value can be a text, atom, string, or fmt.Stringer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Flush writes all buffered rows. Flush should only be called once, after all\n// rows have been written.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NoDim doesn't dim any columns.\nfunc NoDim(prev, row []string) []bool {\n\treturn make([]bool, len(row))\n}\n\n// PrefixDim dims the longest prefix of row that is identical to prev.\nfunc PrefixDim(prev, row []string) []bool {\n\tcols := make([]bool, len(row))\n\tfor i := 0; i < len(prev); i++ {\n\t\tif prev[i] == row[i] {\n\t\t\tcols[i] = true\n\t\t} else {\n\t\t\treturn cols\n\t\t}\n\t}\n\treturn cols\n}\n\n// FullDim dims any columns that are identical in prev.\nfunc FullDim(prev, row []string) []bool {\n\tcols := make([]bool, len(row))\n\tfor i := 0; i < len(prev); i++ {\n\t\tif prev[i] == row[i] {\n\t\t\tcols[i] = true\n\t\t}\n\t}\n\treturn cols\n}\n\n```\n# Function Signature: func (t *Tabularizer) Row(values ...any) {\n# Natural Language Annotations: // Row buffers a new Row to be tabularized. The Row isn't written until Flush\n// is called. Note that every Row reported to a tabularizer must be the same\n// length. A value can be a text, atom, string, or fmt.Stringer.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (t *Tabularizer) Row(values ...any) {\n\t// Convert the input values to Text\n\trow := make([]Text, len(values))\n\tfor i, v := range values {\n\t\tswitch v := v.(type) {\n\t\tcase string:\n\t\t\trow[i] = Text{Atom{S: v}}\n\t\tcase fmt.Stringer:\n\t\t\trow[i] = Text{Atom{S: v.String()}}\n\t\tcase Text:\n\t\t\trow[i] = v\n\t\tcase Atom:\n\t\t\trow[i] = Text{v}\n\t\t}\n\t}\n\n\t// Add the row to the Tabularizer\n\tt.rows = append(t.rows, row)\n\n\t// Update the widths of the columns if necessary\n\tif len(t.widths) == 0 {\n\t\tt.widths = make([]int, len(row))\n\t} else if len(row) != len(t.widths) {\n\t\tpanic(\"all rows must be the same length\")\n\t}\n\tfor i, atom := range row {\n\t\tif atom.len() > t.widths[i] {\n\t\t\tt.widths[i] = atom.len()\n\t\t}\n\t}\n}\n```", "```go\nfunc (t *Tabularizer) Row(values ...any) {\n\tt.rows = append(t.rows, make([]Text, 0))\n\trow := &t.rows[len(t.rows)-1]\n\tfor _, value := range values {\n\t\tswitch v := value.(type) {\n\t\tcase Text:\n\t\t\t*row = append(*row, v)\n\t\tcase Atom:\n\t\t\t*row = append(*row, Text{v})\n\t\tcase string:\n\t\t\t*row = append(*row, Text{{S: v}})\n\t\tcase fmt.Stringer:\n\t\t\t*row = append(*row, Text{{S: v.String()}})\n\t\t}\n\t}\n}\n```", "```go\nfunc (t *Tabularizer) Row(values ...any) {\n\tt.rows = append(t.rows, t.parseRow(values...))\n\tt.updateWidths(t.rows[len(t.rows)-1])\n}\n\nfunc (t *Tabularizer) parseRow(values ...any) []Text {\n\trow := make([]Text, len(values))\n\tfor i, value := range values {\n\t\tswitch v := value.(type) {\n\t\tcase string:\n\t\t\trow[i] = Text{Atom{S: v}}\n\t\tcase Atom:\n\t\t\trow[i] = Text{v}\n\t\tcase Text:\n\t\t\trow[i] = v\n\t\tcase fmt.Stringer:\n\t\t\trow[i] = Text{Atom{S: v.String()}}\n\t\tdefault:\n\t\t\trow[i] = Text{Atom{S: fmt.Sprintf(\"%v\", v)}}\n\t\t}\n\t}\n\treturn row\n}\n\nfunc (t *Tabularizer) updateWidths(row []Text) {\n\tif len(t.widths) == 0 {\n\t\tt.widths = make([]int, len(row))\n\t}\n\tfor i, text := range row {\n\t\tif text.len() > t.widths[i] {\n\t\t\tt.widths[i] = text.len()\n\t\t}\n\t}\n}\n```"]}, "weaver-runtime/colors/tabularizer.go-Flush": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/colors/tabularizer.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage colors\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"strings\"\n)\n\nvar dimColor = Color256(245) // a light gray\n\n// An Atom is a segment of text with a single formatting style.\ntype Atom struct {\n\tS         string // the text\n\tUnderline bool   // is it underlined?\n\tBold      bool   // is it bold?\n\tColor     Code   // text color\n}\n\n// String returns the atom with the appropriate ANSI escape sequences.\nfunc (a Atom) String() string {\n\tif !Enabled() {\n\t\treturn a.S\n\t}\n\n\tvar b strings.Builder\n\tb.WriteString(string(a.Color))\n\tif a.Underline {\n\t\tb.WriteString(string(Underline))\n\t}\n\tif a.Bold {\n\t\tb.WriteString(string(Bold))\n\t}\n\tb.WriteString(a.S)\n\tb.WriteString(string(Reset))\n\treturn b.String()\n}\n\n// dimmed returns a copy of the atom with a dim gray color.\nfunc (a Atom) dimmed() Atom {\n\ta.Color = dimColor\n\treturn a\n}\n\n// Text represents a contiguous sequence of atoms.\ntype Text []Atom\n\n// len returns the length of the printable characters in the text's constituent\n// atoms. ANSI escape sequences are not counted as part of this length.\nfunc (t Text) len() int {\n\treturn len(t.raw())\n}\n\n// raw returns the raw underlying text without any ANSI escape sequences.\nfunc (t Text) raw() string {\n\tvar b strings.Builder\n\tfor _, a := range t {\n\t\tb.WriteString(a.S)\n\t}\n\treturn b.String()\n}\n\n// String returns the text with the appropriate ANSI escape sequences.\nfunc (t Text) String() string {\n\tvar b strings.Builder\n\tfor _, a := range t {\n\t\tb.WriteString(a.String())\n\t}\n\treturn b.String()\n}\n\n// dimmed returns a copy of the text with a dim gray color.\nfunc (t Text) dimmed() Text {\n\tcloned := make(Text, len(t))\n\tfor i, a := range t {\n\t\tcloned[i] = a.dimmed()\n\t}\n\treturn Text(cloned)\n}\n\n// A Tabularizer produces pretty-printed tabularized text. Unlike\n// tabwriter.Writer [1], Tabularizer properly handles text with ANSI escape\n// codes. Here's what an example table looks like:\n//\n//\t\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n//\t\u2502 CATS                  \u2502\n//\t\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n//\t\u2502 NAME   \u2502 AGE \u2502 COLOR  \u2502\n//\t\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n//\t\u2502 belle  \u2502 1y  \u2502 tortie \u2502\n//\t\u2502 sidney \u2502 2y  \u2502 calico \u2502\n//\t\u2502 dakota \u2502 8m  \u2502 tuxedo \u2502\n//\t\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n//\n// The table format comes from duf [2].\n//\n// [1]: https://pkg.go.dev/text/tabwriter\n// [2]: https://github.com/muesli/duf\ntype Tabularizer struct {\n\tw      io.Writer // where to write\n\ttitle  []Text    // table title\n\trows   [][]Text  // buffered rows\n\twidths []int     // column widths\n\tdim    func(prev, row []string) []bool\n}\n\n// NewTabularizer returns a new tabularizer. The provided dim function\n// determines which columns in a row, if any, are dimmed.\nfunc NewTabularizer(w io.Writer, title []Text, dim func(prev, row []string) []bool) *Tabularizer {\n\treturn &Tabularizer{w: w, title: title, dim: dim}\n}\n\n// Row buffers a new Row to be tabularized. The Row isn't written until Flush\n// is called. Note that every Row reported to a tabularizer must be the same\n// length. A value can be a text, atom, string, or fmt.Stringer.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Flush writes all buffered rows. Flush should only be called once, after all\n// rows have been written.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// NoDim doesn't dim any columns.\nfunc NoDim(prev, row []string) []bool {\n\treturn make([]bool, len(row))\n}\n\n// PrefixDim dims the longest prefix of row that is identical to prev.\nfunc PrefixDim(prev, row []string) []bool {\n\tcols := make([]bool, len(row))\n\tfor i := 0; i < len(prev); i++ {\n\t\tif prev[i] == row[i] {\n\t\t\tcols[i] = true\n\t\t} else {\n\t\t\treturn cols\n\t\t}\n\t}\n\treturn cols\n}\n\n// FullDim dims any columns that are identical in prev.\nfunc FullDim(prev, row []string) []bool {\n\tcols := make([]bool, len(row))\n\tfor i := 0; i < len(prev); i++ {\n\t\tif prev[i] == row[i] {\n\t\t\tcols[i] = true\n\t\t}\n\t}\n\treturn cols\n}\n\n```\n# Function Signature: func (t *Tabularizer) Flush() {\n# Natural Language Annotations: // Flush writes all buffered rows. Flush should only be called once, after all\n// rows have been written.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (t *Tabularizer) Flush() error {\n\t// Flush writes all buffered rows. Flush should only be called once, after all\n\t// rows have been written.\n\n\t// Check if there are any rows to flush\n\tif len(t.rows) == 0 {\n\t\treturn nil\n\t}\n\n\t// Compute column widths\n\tt.widths = make([]int, len(t.rows[0]))\n\tfor _, row := range t.rows {\n\t\tfor j, atom := range row {\n\t\t\tif atom.len() > t.widths[j] {\n\t\t\t\tt.widths[j] = atom.len()\n\t\t\t}\n\t\t}\n\t}\n\n\t// Print title\n\tif _, err := fmt.Fprintln(t.w, t.formatRow(t.title, len(t.title), t.widths)); err != nil {\n\t\treturn err\n\t}\n\n\t// Print separator\n\tif _, err := fmt.Fprintln(t.w, t.formatSeparator(t.widths)); err != nil {\n\t\treturn err\n\t}\n\n\t// Print rows\n\tvar prev []string\n\tfor i, row := range t.rows {\n\t\t// Convert row to string slice\n\t\trowStr := make([]string, len(row))\n\t\tfor j, atom := range row {\n\t\t\trowStr[j] = atom.String()\n\t\t}\n\n\t\t// Determine which columns to dim\n\t\tdim := t.dim(prev, rowStr)\n\n\t\t// Dim the columns\n\t\tfor j, atom := range row {\n\t\t\tif dim[j] {\n\t\t\t\trow[j] = atom.dimmed()\n\t\t\t}\n\t\t}\n\n\t\t// Print the row\n\t\tif _, err := fmt.Fprintln(t.w, t.formatRow(row, i, t.widths)); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Update previous row\n\t\tprev = rowStr\n\t}\n\n\t// Print bottom edge\n\tif _, err := fmt.Fprintln(t.w, t.formatBottomEdge(t.widths)); err != nil {\n\t\treturn err\n\t}\n\n\t// Clear the buffered rows\n\tt.rows = nil\n\n\treturn nil\n}\n```", "```go\nfunc (t *Tabularizer) Flush() error {\n\tif len(t.rows) == 0 {\n\t\treturn fmt.Errorf(\"No rows to flush\")\n\t}\n\n\t// Determine column widths\n\tt.widths = make([]int, len(t.rows[0]))\n\tfor _, row := range t.rows {\n\t\tif len(row) != len(t.widths) {\n\t\t\treturn fmt.Errorf(\"All rows must be the same length\")\n\t\t}\n\t\tfor i, atom := range row {\n\t\t\tif atom.len() > t.widths[i] {\n\t\t\t\tt.widths[i] = atom.len()\n\t\t\t}\n\t\t}\n\t}\n\n\t// Write title\n\tt.writeRow(t.title, true, t.widths)\n\n\t// Write rows\n\tprev := make([]string, len(t.rows[0]))\n\tfor _, row := range t.rows {\n\t\tfor i, atom := range row {\n\t\t\tprev[i] = atom.raw()\n\t\t}\n\t\tt.writeRow(row, t.dim(prev, row), t.widths)\n\t}\n\n\t// Reset the buffer for the next use\n\tt.rows = nil\n\n\treturn nil\n}\n```", "Sure, here is the generated function named `Flush()` for the `Tabularizer` type:\n\n```go\nfunc (t *Tabularizer) Flush() {\n    // Flush writes all buffered rows. Flush should only be called once, after all\n    // rows have been written.\n\n    // First, we need to calculate the maximum width for each column.\n    widths := make([]int, len(t.title))\n    for _, row := range t.rows {\n        for i, cell := range row {\n            if cell.len() > widths[i] {\n                widths[i] = cell.len()\n            }\n        }\n    }\n\n    // Then, we can print each row.\n    for i, row := range t.rows {\n        // Apply dimming.\n        dimmed := t.dim(t.rows[i-1].raw(), row.raw())\n        for j, cell := range row {\n            if dimmed[j] {\n                row[j] = row[j].dimmed()\n            }\n        }\n\n        // Write the row.\n        fmt.Fprintln(t.w, row.String())\n    }\n}\n```\n\nPlease note that this function assumes that the `Dim` function has been defined elsewhere in your code and is of the correct type (`func(prev, row []string) []bool`). This function also assumes that the `Raw` and `String` methods for the `Text` type have been defined, as well as the `Dimmed` method for the `Atom` type. The `Dimmed` method is used to apply the dimming to the text of the cells in the table."]}, "weaver-runtime/protomsg/io.go-Write": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/protomsg/io.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package protomsg contains protobuf-related utilities.\npackage protomsg\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\t\"math\"\n\n\t\"google.golang.org/protobuf/proto\"\n)\n\n// maxMsgSize is the largest supported protobuf message.\nconst maxMsgSize = math.MaxInt32\n\n// Write writes a length prefixed protobuf to dst. Use Read to read it.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Read reads a length-prefixed protobuf from src. Messages above maxMsgSize\n// are not supported and cause an error to be returned.\nfunc Read(src io.Reader, msg proto.Message) error {\n\tvar hdr [4]byte\n\tif _, err := io.ReadFull(src, hdr[:]); err != nil {\n\t\treturn fmt.Errorf(\"read protobuf length: %w\", err)\n\t}\n\tn := binary.LittleEndian.Uint32(hdr[:])\n\tif n > maxMsgSize {\n\t\treturn fmt.Errorf(\"read protobuf: message size %d is too large\", n)\n\t}\n\tdata := make([]byte, int(n))\n\tif _, err := io.ReadFull(src, data); err != nil {\n\t\treturn fmt.Errorf(\"read protobuf data %d: %w\", n, err)\n\t}\n\treturn proto.Unmarshal(data, msg)\n}\n\n```\n# Function Signature: func Write(dst io.Writer, msg proto.Message) error {\n# Natural Language Annotations: // Write writes a length prefixed protobuf to dst. Use Read to read it.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage protomsg\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\n\t\"google.golang.org/protobuf/proto\"\n)\n\n// Write writes a length prefixed protobuf to dst. Use Read to read it.\nfunc Write(dst io.Writer, msg proto.Message) error {\n\tdata, err := proto.Marshal(msg)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"marshal protobuf: %w\", err)\n\t}\n\n\tvar hdr [4]byte\n\tbinary.LittleEndian.PutUint32(hdr[:], uint32(len(data)))\n\tif _, err := dst.Write(hdr[:]); err != nil {\n\t\treturn fmt.Errorf(\"write protobuf length: %w\", err)\n\t}\n\n\tif _, err := dst.Write(data); err != nil {\n\t\treturn fmt.Errorf(\"write protobuf data: %w\", err)\n\t}\n\n\treturn nil\n}\n```", "Here is the code for the Write function based on the function signature and natural language annotations:\n\n```go\npackage protomsg\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\n\t\"google.golang.org/protobuf/proto\"\n)\n\n// Write writes a length prefixed protobuf to dst. Use Read to read it.\nfunc Write(dst io.Writer, msg proto.Message) error {\n\tdata, err := proto.Marshal(msg)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"marshal protobuf: %w\", err)\n\t}\n\tvar hdr [4]byte\n\tn := uint32(len(data))\n\tbinary.LittleEndian.PutUint32(hdr[:], n)\n\tif _, err := dst.Write(hdr[:]); err != nil {\n\t\treturn fmt.Errorf(\"write protobuf length: %w\", err)\n\t}\n\tif _, err := dst.Write(data); err != nil {\n\t\treturn fmt.Errorf(\"write protobuf data: %w\", err)\n\t}\n\treturn nil\n}\n```\nPlease note that the Write function uses `proto.Marshal()` to convert the protobuf message into a byte slice. The length of the byte slice is then written to `dst` as a 4-byte little-endian integer. Finally, the byte slice itself is written to `dst`. If any of these operations fail, an error is returned.", "```go\npackage protomsg\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\n\t\"google.golang.org/protobuf/proto\"\n)\n\n// Write writes a length prefixed protobuf to dst. Use Read to read it.\nfunc Write(dst io.Writer, msg proto.Message) error {\n\tdata, err := proto.Marshal(msg)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"marshal protobuf: %w\", err)\n\t}\n\tvar hdr [4]byte\n\tbinary.LittleEndian.PutUint32(hdr[:], uint32(len(data)))\n\tif _, err := dst.Write(hdr[:]); err != nil {\n\t\treturn fmt.Errorf(\"write protobuf length: %w\", err)\n\t}\n\tif _, err := dst.Write(data); err != nil {\n\t\treturn fmt.Errorf(\"write protobuf data: %w\", err)\n\t}\n\treturn nil\n}\n```"]}, "weaver-runtime/bin/bin.go-ReadComponentGraph": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/bin/bin.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package bin contains code to extract data from a Service Weaver binary.\npackage bin\n\nimport (\n\t\"bytes\"\n\t\"debug/buildinfo\"\n\t\"debug/elf\"\n\t\"debug/macho\"\n\t\"debug/pe\"\n\t\"fmt\"\n\t\"os\"\n\t\"regexp\"\n\t\"strconv\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/graph\"\n\t\"github.com/ServiceWeaver/weaver/runtime/version\"\n\t\"golang.org/x/exp/maps\"\n\t\"golang.org/x/exp/slices\"\n)\n\n// versionData exists to embed the weaver module version and deployer API\n// version into a Service Weaver binary. We split declaring and assigning\n// versionData to prevent the compiler from erasing it.\n//\n//lint:ignore U1000 See comment above.\nvar versionData string\n\nfunc init() {\n\t// NOTE that versionData must be assigned a string constant that reflects\n\t// the value of version.DeployerVersion. If the string is not a\n\t// constant---if we try to use fmt.Sprintf, for example---it will not be\n\t// embedded in a Service Weaver binary.\n\tversionData = \"\u27e6wEaVeRvErSiOn:deployer=v0.24.0\u27e7\"\n}\n\n// rodata returns the read-only data section of the provided binary.\nfunc rodata(file string) ([]byte, error) {\n\tf, err := os.Open(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer f.Close()\n\n\t// Look at first few bytes to determine the file format.\n\tprefix := make([]byte, 4)\n\tif _, err := f.ReadAt(prefix, 0); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Handle the file formats we support.\n\tswitch {\n\tcase bytes.HasPrefix(prefix, []byte(\"\\x7FELF\")): // Linux\n\t\tf, err := elf.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\".rodata\").Data()\n\tcase bytes.HasPrefix(prefix, []byte(\"MZ\")): // Windows\n\t\tf, err := pe.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\".rdata\").Data()\n\tcase bytes.HasPrefix(prefix, []byte(\"\\xFE\\xED\\xFA\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\"__rodata\").Data()\n\tcase bytes.HasPrefix(prefix[1:], []byte(\"\\xFA\\xED\\xFE\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\"__rodata\").Data()\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown format\")\n\t}\n}\n\n// ReadComponentGraph reads component graph information from the specified\n// binary. It returns a slice of components and a component graph whose nodes\n// are indices into that slice.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ReadListeners reads the sets of listeners associated with each component\n// in the specified binary.\n\n\n\n\n\n\n\n\ntype Versions struct {\n\tModuleVersion   string         // Service Weaver library's module version\n\tDeployerVersion version.SemVer // see version.DeployerVersion\n}\n\n// ReadVersions reads the module version and deployer API version from the\n// specified binary.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// extractModuleVersion returns the version of the Service Weaver library\n// embedded in data.\nfunc extractModuleVersion(filename string) (string, error) {\n\tinfo, err := buildinfo.ReadFile(filename)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Find the Service Weaver module.\n\tconst weaverModule = \"github.com/ServiceWeaver/weaver\"\n\tfor _, m := range append(info.Deps, &info.Main) {\n\t\tif m.Path == weaverModule {\n\t\t\treturn m.Version, nil\n\t\t}\n\t}\n\treturn \"\", fmt.Errorf(\"Service Weaver module was not linked into the application binary: that's an error\")\n}\n\n// extractDeployerVersion returns the deployer API version embedded\n// in data.\nfunc extractDeployerVersion(data []byte) (version.SemVer, error) {\n\tre := regexp.MustCompile(`\u27e6wEaVeRvErSiOn:deployer=v([0-9]*?)\\.([0-9]*?)\\.([0-9]*?)\u27e7`)\n\tm := re.FindSubmatch(data)\n\tif m == nil {\n\t\treturn version.SemVer{}, fmt.Errorf(\"embedded versions not found\")\n\t}\n\tv := version.SemVer{}\n\tfor _, segment := range []struct {\n\t\tname string\n\t\tdata []byte\n\t\tdst  *int\n\t}{\n\t\t{\"deployer major version\", m[1], &v.Major},\n\t\t{\"deployer minor version\", m[2], &v.Minor},\n\t\t{\"deployer patch version\", m[3], &v.Patch},\n\t} {\n\t\ts := string(segment.data)\n\t\tx, err := strconv.Atoi(s)\n\t\tif err != nil {\n\t\t\treturn version.SemVer{}, fmt.Errorf(\"invalid embedded %s %q: %w\", segment.name, s, err)\n\t\t}\n\t\t*segment.dst = x\n\t}\n\treturn v, nil\n}\n\n```\n# Function Signature: func ReadComponentGraph(file string) ([]string, graph.Graph, error) {\n# Natural Language Annotations: // ReadComponentGraph reads component graph information from the specified\n// binary. It returns a slice of components and a component graph whose nodes\n// are indices into that slice.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ReadComponentGraph(file string) ([]string, graph.Graph, error) {\n\t// Get the read-only data section from the binary.\n\trodata, err := rodata(file)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\t// TODO: Implement parsing and extraction of component graph information.\n\t// This is a placeholder. You'll need to implement the actual logic to parse the binary and extract the component graph information.\n\t// Once you have the component graph information, you can create a slice of components and a graph.Graph.\n\t// For now, we're returning an empty slice of components and an empty graph.Graph.\n\t// Replace the empty slice and graph.Graph with your actual slice of components and graph.Graph.\n\treturn []string{}, graph.Graph{}, nil\n}\n```", "```go\nfunc ReadComponentGraph(file string) ([]string, graph.Graph, error) {\n\tdata, err := rodata(file)\n\tif err != nil {\n\t\treturn nil, graph.Graph{}, err\n\t}\n\n\t// TODO: Implement the logic to read the component graph information from the rodata section of the binary\n\t// This is a placeholder. You need to replace this with actual implementation.\n\tcomponents := []string{}\n\tgraph := graph.Graph{}\n\n\treturn components, graph, nil\n}\n```\nPlease note that the actual implementation to read the component graph information from the rodata section of the binary will depend on the specific format and structure used in the binary. You need to replace the placeholder code with actual implementation.", "```go\nfunc ReadComponentGraph(file string) ([]string, graph.Graph, error) {\n\t// Read the .rodata section of the binary\n\tdata, err := rodata(file)\n\tif err != nil {\n\t\treturn nil, graph.Graph{}, err\n\t}\n\n\t// Parse the component graph data\n\tcomponents, componentData, err := parseComponentGraphData(data)\n\tif err != nil {\n\t\treturn nil, graph.Graph{}, err\n\t}\n\n\t// Parse the component graph into a graph.Graph\n\tg, err := parseGraph(componentData)\n\tif err != nil {\n\t\treturn nil, graph.Graph{}, err\n\t}\n\n\treturn components, g, nil\n}\n\n// parseComponentGraphData parses the component graph data from the given byte slice\nfunc parseComponentGraphData(data []byte) ([]string, []string, error) {\n\t// TODO: Implement this function\n\t// This is a bit tricky because the component graph data is not directly visible in the binary.\n\t// It may be stored in string format or some other format that needs to be parsed.\n\t// This function should return the list of components and the data for each component.\n\treturn nil, nil, nil\n}\n\n// parseGraph parses the component graph data into a graph.Graph\nfunc parseGraph(componentData []string) (graph.Graph, error) {\n\t// TODO: Implement this function\n\t// This function should parse the componentData into a graph.Graph.\n\t// The exact implementation will depend on how the component data is stored in the binary.\n\treturn graph.Graph{}, nil\n}\n```\n\nPlease note that this is a placeholder implementation. The actual implementation of the `parseComponentGraphData` and `parseGraph` functions would require knowledge of the specific binary format and how the component graph data is stored within it. These functions are not included in the code you provided, so they would need to be implemented based on the specifics of your binary format."]}, "weaver-runtime/bin/bin.go-ReadListeners": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/bin/bin.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package bin contains code to extract data from a Service Weaver binary.\npackage bin\n\nimport (\n\t\"bytes\"\n\t\"debug/buildinfo\"\n\t\"debug/elf\"\n\t\"debug/macho\"\n\t\"debug/pe\"\n\t\"fmt\"\n\t\"os\"\n\t\"regexp\"\n\t\"strconv\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/graph\"\n\t\"github.com/ServiceWeaver/weaver/runtime/version\"\n\t\"golang.org/x/exp/maps\"\n\t\"golang.org/x/exp/slices\"\n)\n\n// versionData exists to embed the weaver module version and deployer API\n// version into a Service Weaver binary. We split declaring and assigning\n// versionData to prevent the compiler from erasing it.\n//\n//lint:ignore U1000 See comment above.\nvar versionData string\n\nfunc init() {\n\t// NOTE that versionData must be assigned a string constant that reflects\n\t// the value of version.DeployerVersion. If the string is not a\n\t// constant---if we try to use fmt.Sprintf, for example---it will not be\n\t// embedded in a Service Weaver binary.\n\tversionData = \"\u27e6wEaVeRvErSiOn:deployer=v0.24.0\u27e7\"\n}\n\n// rodata returns the read-only data section of the provided binary.\nfunc rodata(file string) ([]byte, error) {\n\tf, err := os.Open(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer f.Close()\n\n\t// Look at first few bytes to determine the file format.\n\tprefix := make([]byte, 4)\n\tif _, err := f.ReadAt(prefix, 0); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Handle the file formats we support.\n\tswitch {\n\tcase bytes.HasPrefix(prefix, []byte(\"\\x7FELF\")): // Linux\n\t\tf, err := elf.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\".rodata\").Data()\n\tcase bytes.HasPrefix(prefix, []byte(\"MZ\")): // Windows\n\t\tf, err := pe.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\".rdata\").Data()\n\tcase bytes.HasPrefix(prefix, []byte(\"\\xFE\\xED\\xFA\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\"__rodata\").Data()\n\tcase bytes.HasPrefix(prefix[1:], []byte(\"\\xFA\\xED\\xFE\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\"__rodata\").Data()\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown format\")\n\t}\n}\n\n// ReadComponentGraph reads component graph information from the specified\n// binary. It returns a slice of components and a component graph whose nodes\n// are indices into that slice.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ReadListeners reads the sets of listeners associated with each component\n// in the specified binary.\n\n\n\n\n\n\n\n\ntype Versions struct {\n\tModuleVersion   string         // Service Weaver library's module version\n\tDeployerVersion version.SemVer // see version.DeployerVersion\n}\n\n// ReadVersions reads the module version and deployer API version from the\n// specified binary.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// extractModuleVersion returns the version of the Service Weaver library\n// embedded in data.\nfunc extractModuleVersion(filename string) (string, error) {\n\tinfo, err := buildinfo.ReadFile(filename)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Find the Service Weaver module.\n\tconst weaverModule = \"github.com/ServiceWeaver/weaver\"\n\tfor _, m := range append(info.Deps, &info.Main) {\n\t\tif m.Path == weaverModule {\n\t\t\treturn m.Version, nil\n\t\t}\n\t}\n\treturn \"\", fmt.Errorf(\"Service Weaver module was not linked into the application binary: that's an error\")\n}\n\n// extractDeployerVersion returns the deployer API version embedded\n// in data.\nfunc extractDeployerVersion(data []byte) (version.SemVer, error) {\n\tre := regexp.MustCompile(`\u27e6wEaVeRvErSiOn:deployer=v([0-9]*?)\\.([0-9]*?)\\.([0-9]*?)\u27e7`)\n\tm := re.FindSubmatch(data)\n\tif m == nil {\n\t\treturn version.SemVer{}, fmt.Errorf(\"embedded versions not found\")\n\t}\n\tv := version.SemVer{}\n\tfor _, segment := range []struct {\n\t\tname string\n\t\tdata []byte\n\t\tdst  *int\n\t}{\n\t\t{\"deployer major version\", m[1], &v.Major},\n\t\t{\"deployer minor version\", m[2], &v.Minor},\n\t\t{\"deployer patch version\", m[3], &v.Patch},\n\t} {\n\t\ts := string(segment.data)\n\t\tx, err := strconv.Atoi(s)\n\t\tif err != nil {\n\t\t\treturn version.SemVer{}, fmt.Errorf(\"invalid embedded %s %q: %w\", segment.name, s, err)\n\t\t}\n\t\t*segment.dst = x\n\t}\n\treturn v, nil\n}\n\n```\n# Function Signature: func ReadListeners(file string) ([]codegen.ComponentListeners, error) {\n# Natural Language Annotations: // ReadListeners reads the sets of listeners associated with each component\n// in the specified binary.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ReadListeners(file string) ([]codegen.ComponentListeners, error) {\n\tdata, err := rodata(file)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to extract read-only data from binary: %w\", err)\n\t}\n\n\t// The listeners are embedded as a string in the binary.\n\tlistenersData, err := extractListenersData(data)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to extract listeners data from binary: %w\", err)\n\t}\n\n\t// Parse the listeners data.\n\tlisteners, err := parseListeners(listenersData)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to parse listeners data: %w\", err)\n\t}\n\n\treturn listeners, nil\n}\n```\n\nIn this code, we first extract the read-only data section from the binary file. Then, we extract the listeners data from this section. This data is then parsed into a list of `codegen.ComponentListeners`.\n\nPlease note that the `extractListenersData` and `parseListeners` functions are not provided in the code snippet you provided, so they need to be implemented. They will depend on the exact format of the data that is embedded in the binary.", "```go\npackage bin\n\nimport (\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/graph\"\n)\n\nfunc ReadListeners(file string) ([]codegen.ComponentListeners, error) {\n\t// Read the .rodata section from the file.\n\trodata, err := rodata(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// TODO: Implement the logic to read the listeners from the rodata.\n\t// This will depend on the specific format of the listener data in the binary.\n\t// For example, it might be a simple list of listeners, or it might be more\n\t// complex, with different types of listeners for different components.\n\t//\n\t// Once the listener data is read, it should be parsed and converted into a\n\t// slice of codegen.ComponentListeners. This will likely involve creating\n\t// codegen.ComponentListeners instances and setting their fields appropriately.\n\t//\n\t// If there are any errors during this process, they should be returned as\n\t// an error.\n\n\t// Dummy implementation\n\t// This is just a placeholder and will not work in the current state.\n\t// You need to implement the actual logic to read the listeners from the rodata.\n\tvar listeners []codegen.ComponentListeners\n\tfor i := 0; i < 3; i++ {\n\t\tlisteners = append(listeners, codegen.ComponentListeners{\n\t\t\tComponent: graph.ComponentID{\n\t\t\t\tName:    \"Component\" + strconv.Itoa(i+1),\n\t\t\t\tWeaveID: uint64(i + 1),\n\t\t\t},\n\t\t\tListener: \"Listener\" + strconv.Itoa(i+1),\n\t\t})\n\t}\n\n\treturn listeners, nil\n}\n```\nThis code defines a function `ReadListeners` that reads the sets of listeners associated with each component in the specified binary. It uses the `rodata` function to read the .rodata section from the file. \n\nHowever, the actual logic to read the listeners from the rodata is not implemented. This will depend on the specific format of the listener data in the binary. Once the listener data is read, it should be parsed and converted into a slice of `codegen.ComponentListeners`. This will likely involve creating `codegen.ComponentListeners` instances and setting their fields appropriately. If there are any errors during this process, they should be returned as an error.", "```go\nfunc ReadListeners(file string) ([]codegen.ComponentListeners, error) {\n\tdata, err := rodata(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Use regular expressions to find the listener definition sections.\n\tre := regexp.MustCompile(`\u27e6wEaVeRlIsTeNeRs:([0-9]*?)\u27e7([^\u27e6]*)`)\n\tmatches := re.FindAllSubmatch(data, -1)\n\tif matches == nil {\n\t\treturn nil, fmt.Errorf(\"no listener definitions found\")\n\t}\n\n\t// Parse each listener definition section.\n\tvar listeners []codegen.ComponentListeners\n\tfor _, m := range matches {\n\t\tid, err := strconv.Atoi(string(m[1]))\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"invalid listener id: %w\", err)\n\t\t}\n\n\t\tlistener, err := parseListener(string(m[2]))\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"parsing listener %d: %w\", id, err)\n\t\t}\n\n\t\t// Make sure we have enough space in listeners.\n\t\tfor len(listeners) <= id {\n\t\t\tlisteners = append(listeners, codegen.ComponentListeners{})\n\t\t}\n\t\tlisteners[id] = append(listeners[id], listener)\n\t}\n\n\treturn listeners, nil\n}\n\n// parseListener parses a single listener.\nfunc parseListener(s string) (codegen.Listener, error) {\n\tre := regexp.MustCompile(`\u27e6wEaVeRlIsTeNeRs:[0-9]*?\u27e7([^\u27e6]*?)`)\n\tm := re.FindStringSubmatch(s)\n\tif m == nil {\n\t\treturn codegen.Listener{}, fmt.Errorf(\"invalid listener definition\")\n\t}\n\n\t// Parse the listener.\n\tvar l codegen.Listener\n\tparts := strings.Split(m[1], \",\")\n\tfor _, part := range parts {\n\t\tkv := strings.SplitN(part, \"=\", 2)\n\t\tif len(kv) != 2 {\n\t\t\treturn codegen.Listener{}, fmt.Errorf(\"invalid listener definition: %q\", part)\n\t\t}\n\t\tk, v := strings.TrimSpace(kv[0]), strings.TrimSpace(kv[1])\n\t\tswitch k {\n\t\tcase \"name\":\n\t\t\tl.Name = v\n\t\tcase \"split\":\n\t\t\tx, err := strconv.Atoi(v)\n\t\t\tif err != nil {\n\t\t\t\treturn codegen.Listener{}, fmt.Errorf(\"invalid listener split: %w\", err)\n\t\t\t}\n\t\t\tl.Split = x\n\t\tdefault:\n\t\t\treturn codegen.Listener{}, fmt.Errorf(\"unknown listener key: %q\", k)\n\t\t}\n\t}\n\n\treturn l, nil\n}\n```"]}, "weaver-runtime/bin/bin.go-ReadVersions": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/bin/bin.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package bin contains code to extract data from a Service Weaver binary.\npackage bin\n\nimport (\n\t\"bytes\"\n\t\"debug/buildinfo\"\n\t\"debug/elf\"\n\t\"debug/macho\"\n\t\"debug/pe\"\n\t\"fmt\"\n\t\"os\"\n\t\"regexp\"\n\t\"strconv\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/graph\"\n\t\"github.com/ServiceWeaver/weaver/runtime/version\"\n\t\"golang.org/x/exp/maps\"\n\t\"golang.org/x/exp/slices\"\n)\n\n// versionData exists to embed the weaver module version and deployer API\n// version into a Service Weaver binary. We split declaring and assigning\n// versionData to prevent the compiler from erasing it.\n//\n//lint:ignore U1000 See comment above.\nvar versionData string\n\nfunc init() {\n\t// NOTE that versionData must be assigned a string constant that reflects\n\t// the value of version.DeployerVersion. If the string is not a\n\t// constant---if we try to use fmt.Sprintf, for example---it will not be\n\t// embedded in a Service Weaver binary.\n\tversionData = \"\u27e6wEaVeRvErSiOn:deployer=v0.24.0\u27e7\"\n}\n\n// rodata returns the read-only data section of the provided binary.\nfunc rodata(file string) ([]byte, error) {\n\tf, err := os.Open(file)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer f.Close()\n\n\t// Look at first few bytes to determine the file format.\n\tprefix := make([]byte, 4)\n\tif _, err := f.ReadAt(prefix, 0); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Handle the file formats we support.\n\tswitch {\n\tcase bytes.HasPrefix(prefix, []byte(\"\\x7FELF\")): // Linux\n\t\tf, err := elf.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\".rodata\").Data()\n\tcase bytes.HasPrefix(prefix, []byte(\"MZ\")): // Windows\n\t\tf, err := pe.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\".rdata\").Data()\n\tcase bytes.HasPrefix(prefix, []byte(\"\\xFE\\xED\\xFA\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\"__rodata\").Data()\n\tcase bytes.HasPrefix(prefix[1:], []byte(\"\\xFA\\xED\\xFE\")): // MacOS\n\t\tf, err := macho.NewFile(f)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn f.Section(\"__rodata\").Data()\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown format\")\n\t}\n}\n\n// ReadComponentGraph reads component graph information from the specified\n// binary. It returns a slice of components and a component graph whose nodes\n// are indices into that slice.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ReadListeners reads the sets of listeners associated with each component\n// in the specified binary.\n\n\n\n\n\n\n\n\ntype Versions struct {\n\tModuleVersion   string         // Service Weaver library's module version\n\tDeployerVersion version.SemVer // see version.DeployerVersion\n}\n\n// ReadVersions reads the module version and deployer API version from the\n// specified binary.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// extractModuleVersion returns the version of the Service Weaver library\n// embedded in data.\nfunc extractModuleVersion(filename string) (string, error) {\n\tinfo, err := buildinfo.ReadFile(filename)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Find the Service Weaver module.\n\tconst weaverModule = \"github.com/ServiceWeaver/weaver\"\n\tfor _, m := range append(info.Deps, &info.Main) {\n\t\tif m.Path == weaverModule {\n\t\t\treturn m.Version, nil\n\t\t}\n\t}\n\treturn \"\", fmt.Errorf(\"Service Weaver module was not linked into the application binary: that's an error\")\n}\n\n// extractDeployerVersion returns the deployer API version embedded\n// in data.\nfunc extractDeployerVersion(data []byte) (version.SemVer, error) {\n\tre := regexp.MustCompile(`\u27e6wEaVeRvErSiOn:deployer=v([0-9]*?)\\.([0-9]*?)\\.([0-9]*?)\u27e7`)\n\tm := re.FindSubmatch(data)\n\tif m == nil {\n\t\treturn version.SemVer{}, fmt.Errorf(\"embedded versions not found\")\n\t}\n\tv := version.SemVer{}\n\tfor _, segment := range []struct {\n\t\tname string\n\t\tdata []byte\n\t\tdst  *int\n\t}{\n\t\t{\"deployer major version\", m[1], &v.Major},\n\t\t{\"deployer minor version\", m[2], &v.Minor},\n\t\t{\"deployer patch version\", m[3], &v.Patch},\n\t} {\n\t\ts := string(segment.data)\n\t\tx, err := strconv.Atoi(s)\n\t\tif err != nil {\n\t\t\treturn version.SemVer{}, fmt.Errorf(\"invalid embedded %s %q: %w\", segment.name, s, err)\n\t\t}\n\t\t*segment.dst = x\n\t}\n\treturn v, nil\n}\n\n```\n# Function Signature: func ReadVersions(filename string) (Versions, error) {\n# Natural Language Annotations: // ReadVersions reads the module version and deployer API version from the\n// specified binary.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ReadVersions(filename string) (Versions, error) {\n\t// Read the module version.\n\tmoduleVersion, err := extractModuleVersion(filename)\n\tif err != nil {\n\t\treturn Versions{}, fmt.Errorf(\"unable to extract module version: %w\", err)\n\t}\n\n\t// Read the rodata section.\n\tdata, err := rodata(filename)\n\tif err != nil {\n\t\treturn Versions{}, fmt.Errorf(\"unable to read rodata section: %w\", err)\n\t}\n\n\t// Extract the deployer API version.\n\tdeployerVersion, err := extractDeployerVersion(data)\n\tif err != nil {\n\t\treturn Versions{}, fmt.Errorf(\"unable to extract deployer version: %w\", err)\n\t}\n\n\treturn Versions{\n\t\tModuleVersion:   moduleVersion,\n\t\tDeployerVersion: deployerVersion,\n\t}, nil\n}\n```", "Sure, here is the generated code:\n\n```go\nfunc ReadVersions(filename string) (Versions, error) {\n\trodata, err := rodata(filename)\n\tif err != nil {\n\t\treturn Versions{}, err\n\t}\n\n\tmoduleVersion, err := extractModuleVersion(filename)\n\tif err != nil {\n\t\treturn Versions{}, err\n\t}\n\n\tdeployerVersion, err := extractDeployerVersion(rodata)\n\tif err != nil {\n\t\treturn Versions{}, err\n\t}\n\n\treturn Versions{\n\t\tModuleVersion:   moduleVersion,\n\t\tDeployerVersion: deployerVersion,\n\t}, nil\n}\n```\n\nThis function reads the module version and deployer API version from the specified binary. It first reads the read-only data section of the binary using the `rodata` function. Then it extracts the module version and deployer API version from the read-only data using the `extractModuleVersion` and `extractDeployerVersion` functions respectively. If any error occurs during the process, it returns an error. Otherwise, it returns the module version and deployer API version.", "```go\nfunc ReadVersions(filename string) (Versions, error) {\n\trodata, err := rodata(filename)\n\tif err != nil {\n\t\treturn Versions{}, err\n\t}\n\n\tmoduleVersion, err := extractModuleVersion(filename)\n\tif err != nil {\n\t\treturn Versions{}, err\n\t}\n\n\tdeployerVersion, err := extractDeployerVersion(rodata)\n\tif err != nil {\n\t\treturn Versions{}, err\n\t}\n\n\treturn Versions{\n\t\tModuleVersion:   moduleVersion,\n\t\tDeployerVersion: deployerVersion,\n\t}, nil\n}\n```"]}, "weaver-runtime/prometheus/prometheus.go-TranslateMetricsToPrometheusTextFormat": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/prometheus/prometheus.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage prometheus\n\nimport (\n\t\"bytes\"\n\t\"fmt\"\n\t\"math\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/logging\"\n\t\"github.com/ServiceWeaver/weaver/runtime/metrics\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"golang.org/x/exp/maps\"\n)\n\n// escaper is used to format the labels according to [1]. Prometheus labels can\n// be any sequence of UTF-8 characters, but the backslash (\\), double-quote (\"),\n// and line feed (\\n) characters have to be escaped as \\\\, \\\", and \\n, respectively.\n//\n// [1] https://github.com/prometheus/docs/blob/main/content/docs/instrumenting/exposition_formats.md#text-format-details\nvar escaper = strings.NewReplacer(\"\\\\\", `\\\\`, \"\\n\", `\\n`, \"\\\"\", `\\\"`)\n\n// TranslateMetricsToPrometheusTextFormat translates Service Weaver\n// metrics (keyed by weavelet id) to a text format that can be\n// scraped by Prometheus [1].\n//\n// [1] https://prometheus.io/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// writeHelper generates a config.yaml file that can be used by prometheus to\n// scrape the exported metrics.\nfunc writeHelper(w *bytes.Buffer, lisAddr, path string) {\n\tconst help = `# Metrics in Prometheus text format [1].\n#\n# To visualize and query the metrics, make sure Prometheus is installed on\n# your local machine and then add the following stanza to your Prometheus yaml\n# config file:\n#\n# scrape_configs:\n# - job_name: 'prometheus-serviceweaver-scraper'\n#   scrape_interval: 5s\n#   metrics_path: %s\n#   static_configs:\n#     - targets: ['%s']\n#\n# [1]: https://prometheus.io\n\n`\n\tfmt.Fprintf(w, help, path, lisAddr)\n}\n\n// translateMetrics translates a slice of metrics from the Service Weaver format\n// to the Prometheus text format. For more details regarding the metric text\n// format for Prometheus, see [1].\n//\n// [1] https://github.com/prometheus/docs/blob/main/content/docs/instrumenting/exposition_formats.md#text-format-details\nfunc translateMetrics(w *bytes.Buffer, metrics []*metrics.MetricSnapshot) string {\n\tmetric := metrics[0]\n\n\t// Write the metric HELP. Note that all metrics have the same metric name,\n\t// so we should display the help and the type only once.\n\tif len(metric.Help) > 0 {\n\t\tw.WriteString(\"# HELP \" + metric.Name + \" \" + metric.Help + \"\\n\")\n\t}\n\n\t// Write the metric TYPE.\n\tw.WriteString(\"# TYPE \" + metric.Name)\n\n\tisHistogram := false\n\tswitch metric.Type {\n\tcase protos.MetricType_COUNTER:\n\t\tw.WriteString(\" counter\\n\")\n\tcase protos.MetricType_GAUGE:\n\t\tw.WriteString(\" gauge\\n\")\n\tcase protos.MetricType_HISTOGRAM:\n\t\tw.WriteString(\" histogram\\n\")\n\t\tisHistogram = true\n\t}\n\n\tfor idx, metric := range metrics {\n\t\t// Trim labels.\n\t\tlabels := maps.Clone(metric.Labels)\n\t\tdelete(labels, \"serviceweaver_app\")\n\t\tdelete(labels, \"serviceweaver_version\")\n\t\tif node, ok := labels[\"serviceweaver_node\"]; ok {\n\t\t\tlabels[\"serviceweaver_node\"] = logging.Shorten(node)\n\t\t}\n\n\t\t// Write the metric definitions.\n\t\t//\n\t\t// For counter and gauge metrics the definition looks like:\n\t\t// metric_name [\n\t\t//  \"{\" label_name \"=\" `\"` label_value `\"` { \",\" label_name \"=\" `\"` label_value `\"` } [ \",\" ] \"}\"\n\t\t// ] value [ timestamp ]\n\t\t//\n\t\t// For histograms:\n\t\t//  Each bucket count of a histogram named x is given as a separate sample\n\t\t//  line with the name x_bucket and a label {le=\"y\"} (where y is the upper bound of the bucket).\n\t\t//\n\t\t//  The bucket with label {le=\"+Inf\"} must exist. Its value must be identical to the value of x_count.\n\t\t//\n\t\t//  The buckets must appear in increasing numerical order of their label values (for the le).\n\t\t//\n\t\t//  The sample sum for a summary or histogram named x is given as a separate sample named x_sum.\n\t\t//\n\t\t//  The sample count for a summary or histogram named x is given as a separate sample named x_count.\n\t\tif isHistogram {\n\t\t\thasInf := false\n\n\t\t\tvar count uint64\n\t\t\tfor idx, bound := range metric.Bounds {\n\t\t\t\tcount += metric.Counts[idx]\n\t\t\t\twriteEntry(w, metric.Name, float64(count), \"_bucket\", labels, \"le\", bound)\n\t\t\t\tif math.IsInf(bound, +1) {\n\t\t\t\t\thasInf = true\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Account for the +Inf bucket.\n\t\t\tcount += metric.Counts[len(metric.Bounds)]\n\t\t\tif !hasInf {\n\t\t\t\twriteEntry(w, metric.Name, float64(count), \"_bucket\", labels, \"le\", math.Inf(+1))\n\t\t\t}\n\t\t\twriteEntry(w, metric.Name, metric.Value, \"_sum\", labels, \"\", 0)\n\t\t\twriteEntry(w, metric.Name, float64(count), \"_count\", labels, \"\", 0)\n\t\t} else { // counter or gauge\n\t\t\twriteEntry(w, metric.Name, metric.Value, \"\", labels, \"\", 0)\n\t\t}\n\t\tif isHistogram && idx != len(metrics)-1 {\n\t\t\tw.WriteByte('\\n')\n\t\t}\n\t}\n\tw.WriteByte('\\n')\n\treturn w.String()\n}\n\n// writeEntry generates a metric definition entry.\nfunc writeEntry(w *bytes.Buffer, metricName string, value float64, suffix string,\n\tlabels map[string]string, extraLabelName string, extraLabelValue float64) {\n\tw.WriteString(metricName)\n\tif len(suffix) > 0 {\n\t\tw.WriteString(suffix)\n\t}\n\twriteLabels(w, labels, extraLabelName, extraLabelValue)\n\tw.WriteString(\" \" + strconv.FormatFloat(value, 'f', -1, 64) + \"\\n\")\n}\n\n// writeEntry generates the metric labels.\nfunc writeLabels(w *bytes.Buffer, labels map[string]string,\n\textraLabelName string, extraLabelValue float64) {\n\tif len(labels) == 0 && extraLabelName == \"\" {\n\t\treturn\n\t}\n\n\tsortedLabels := maps.Keys(labels)\n\tsort.Strings(sortedLabels)\n\n\tseparator := \"{\"\n\tfor _, l := range sortedLabels {\n\t\tw.WriteString(separator + l + `=\"`)\n\t\tescaper.WriteString(w, labels[l])\n\t\tw.WriteByte('\"')\n\t\tseparator = \",\"\n\t}\n\tif len(extraLabelName) > 0 {\n\t\t// Set for a histogram metric only.\n\t\tw.WriteString(separator + extraLabelName + `=\"`)\n\t\tw.WriteString(strconv.FormatFloat(extraLabelValue, 'f', -1, 64) + \"\\\"\")\n\t}\n\tw.WriteString(\"}\")\n}\n\n```\n# Function Signature: func TranslateMetricsToPrometheusTextFormat(w *bytes.Buffer, ms []*metrics.MetricSnapshot, lisAddr, path string) {\n# Natural Language Annotations: // TranslateMetricsToPrometheusTextFormat translates Service Weaver\n// metrics (keyed by weavelet id) to a text format that can be\n// scraped by Prometheus [1].\n//\n// [1] https://prometheus.io/\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage prometheus\n\nimport (\n\t\"bytes\"\n\t\"github.com/ServiceWeaver/weaver/runtime/metrics\"\n)\n\n// TranslateMetricsToPrometheusTextFormat translates Service Weaver\n// metrics (keyed by weavelet id) to a text format that can be\n// scraped by Prometheus [1].\n//\n// [1] https://prometheus.io/\nfunc TranslateMetricsToPrometheusTextFormat(w *bytes.Buffer, ms []*metrics.MetricSnapshot, lisAddr, path string) {\n\t// Write the helper text.\n\twriteHelper(w, lisAddr, path)\n\t// Translate the metrics and write them to the buffer.\n\ttranslateMetrics(w, ms)\n}\n```", "```go\nfunc TranslateMetricsToPrometheusTextFormat(w *bytes.Buffer, ms []*metrics.MetricSnapshot, lisAddr, path string) {\n\t// Write the helper text.\n\twriteHelper(w, lisAddr, path)\n\n\t// Translate the metrics to the Prometheus text format.\n\ttranslateMetrics(w, ms)\n}\n```", "```go\nfunc TranslateMetricsToPrometheusTextFormat(w *bytes.Buffer, ms []*metrics.MetricSnapshot, lisAddr, path string) {\n\twriteHelper(w, lisAddr, path)\n\ttranslateMetrics(w, ms)\n}\n```"]}, "weaver-runtime/codegen/graph.go-ExtractEdges": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/codegen/graph.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage codegen\n\nimport (\n\t\"crypto/sha256\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"sort\"\n)\n\n// Component graph edges are embedded in the generated binary as\n// specially formatted strings. These strings can be extracted from\n// the binary to get the communication graph without having to execute\n// the binary.\n//\n// Each edge is represented by a string fragment that looks like:\n// \u27e6checksum:wEaVeReDgE:src\u2192dst\u27e7\n//\n// checksum is the first 8 bytes of the hex encoding of the SHA-256 of\n// the string \"wEaVeReDgE:src\u2192dst\" and src and dst are the fully qualified\n// component type names.\n\n// MakeEdgeString returns a string that should be emitted into generated\n// code to represent an edge from src to dst.\nfunc MakeEdgeString(src, dst string) string {\n\treturn fmt.Sprintf(\"\u27e6%s:wEaVeReDgE:%s\u2192%s\u27e7\\n\", checksumEdge(src, dst), src, dst)\n}\n\n// ExtractEdges returns the edges corresponding to MakeEdgeString() results\n// that occur in data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc checksumEdge(src, dst string) string {\n\tedge := fmt.Sprintf(\"wEaVeReDgE:%s\u2192%s\", src, dst)\n\tsum := sha256.Sum256([]byte(edge))\n\treturn fmt.Sprintf(\"%0x\", sum)[:8]\n}\n\n```\n# Function Signature: func ExtractEdges(data []byte) [][2]string {\n# Natural Language Annotations: // ExtractEdges returns the edges corresponding to MakeEdgeString() results\n// that occur in data.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ExtractEdges(data []byte) [][2]string {\n    edges := make([][2]string, 0)\n\n    // Regular expression to match the edge string\n    re := regexp.MustCompile(`\u27e6(\\w+):wEaVeReDgE:(\\w+)\u2192(\\w+)\u27e7`)\n\n    // Find all matches in the data\n    matches := re.FindAllStringSubmatch(string(data), -1)\n\n    for _, match := range matches {\n        // The match slice contains the full match as the first element,\n        // followed by the submatches.\n        sum := match[1]\n        src := match[2]\n        dst := match[3]\n\n        // Verify the checksum\n        if sum != checksumEdge(src, dst) {\n            continue\n        }\n\n        edges = append(edges, [2]string{src, dst})\n    }\n\n    return edges\n}\n```", "```go\npackage codegen\n\nimport (\n\t\"regexp\"\n\t\"strings\"\n\t\"crypto/sha256\"\n\t\"fmt\"\n)\n\n// ExtractEdges returns the edges corresponding to MakeEdgeString() results\n// that occur in data.\nfunc ExtractEdges(data []byte) [][2]string {\n\t// Initialize an empty slice to store the edges\n\tvar edges [][2]string\n\n\t// Compile the regular expression to match the edge string\n\tedgeRegex := regexp.MustCompile(`\u27e6(\\w+):wEaVeReDgE:(\\w+)\u2192(\\w+)\u27e7`)\n\n\t// Find all the matched substrings in the data\n\tmatches := edgeRegex.FindAllStringSubmatch(string(data), -1)\n\n\t// Iterate over the matches\n\tfor _, match := range matches {\n\t\t// Calculate the expected checksum\n\t\tedge := fmt.Sprintf(\"wEaVeReDgE:%s\u2192%s\", match[2], match[3])\n\t\tsum := sha256.Sum256([]byte(edge))\n\t\tchecksum := fmt.Sprintf(\"%0x\", sum)[:8]\n\n\t\t// If the checksum matches, append the edge to the slice\n\t\tif match[1] == checksum {\n\t\t\tedges = append(edges, [2]string{match[2], match[3]})\n\t\t}\n\t}\n\n\t// Return the slice of edges\n\treturn edges\n}\n```", "```go\npackage codegen\n\nimport (\n\t\"regexp\"\n\t\"strings\"\n)\n\n// ExtractEdges returns the edges corresponding to MakeEdgeString() results\n// that occur in data.\nfunc ExtractEdges(data []byte) [][2]string {\n\tedges := make([][2]string, 0)\n\tre := regexp.MustCompile(`\u27e6[0-9a-fA-F]{8}:wEaVeReDgE:([^\u2192]+)\u2192([^\u27e7]+)\u27e7`)\n\tmatches := re.FindAllStringSubmatch(string(data), -1)\n\tfor _, match := range matches {\n\t\tsrc := strings.TrimSpace(match[1])\n\t\tdst := strings.TrimSpace(match[2])\n\t\tedges = append(edges, [2]string{src, dst})\n\t}\n\treturn edges\n}\n```"]}, "weaver-runtime/codegen/registry.go-ComponentConfigValidator": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/codegen/registry.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage codegen\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/config\"\n\t\"github.com/ServiceWeaver/weaver/runtime\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// globalRegistry is the global registry used by Register and Registered.\nvar globalRegistry registry\n\n// Register registers a Service Weaver component.\nfunc Register(reg Registration) {\n\tif err := globalRegistry.register(reg); err != nil {\n\t\tpanic(err)\n\t}\n}\n\n// Registered returns the components registered with Register.\nfunc Registered() []*Registration {\n\treturn globalRegistry.allComponents()\n}\n\n// Find returns the registration of the named component.\nfunc Find(name string) (*Registration, bool) {\n\treturn globalRegistry.find(name)\n}\n\n// registry is a repository for registered Service Weaver components.\n// Entries are typically added to the default registry by calls\n// to Register in init functions in code generated by \"weaver generate\".\ntype registry struct {\n\tm          sync.Mutex\n\tcomponents map[reflect.Type]*Registration // the set of registered components, by their interface types\n\tbyName     map[string]*Registration       // map from full component name to registration\n}\n\n// Registration is the configuration needed to register a Service Weaver component.\ntype Registration struct {\n\tName      string       // full package-prefixed component name\n\tIface     reflect.Type // interface type for the component\n\tImpl      reflect.Type // implementation type (struct)\n\tRouted    bool         // True if calls to this component should be routed\n\tListeners []string     // the names of any weaver.Listeners\n\tNoRetry   []int        // indices of methods that should not be retried\n\n\t// Functions that return different types of stubs.\n\tLocalStubFn   func(impl any, caller string, tracer trace.Tracer) any\n\tClientStubFn  func(stub Stub, caller string) any\n\tServerStubFn  func(impl any, load func(key uint64, load float64)) Server\n\tReflectStubFn func(func(method string, ctx context.Context, args []any, returns []any) error) any\n\n\t// RefData holds a string containing the result of MakeEdgeString(Name, Dst)\n\t// for all components named Dst used by this component.\n\tRefData string\n}\n\n// register registers a Service Weaver component. If the registry's close method was\n// previously called, Register will fail and return a non-nil error.\nfunc (r *registry) register(reg Registration) error {\n\tif err := verifyRegistration(reg); err != nil {\n\t\treturn fmt.Errorf(\"Register(%q): %w\", reg.Name, err)\n\t}\n\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\tif old, ok := r.components[reg.Iface]; ok {\n\t\treturn fmt.Errorf(\"component %s already registered for type %v when registering %v\",\n\t\t\treg.Name, old.Impl, reg.Impl)\n\t}\n\tif r.components == nil {\n\t\tr.components = map[reflect.Type]*Registration{}\n\t}\n\tif r.byName == nil {\n\t\tr.byName = map[string]*Registration{}\n\t}\n\tptr := &reg\n\tr.components[reg.Iface] = ptr\n\tr.byName[reg.Name] = ptr\n\treturn nil\n}\n\nfunc verifyRegistration(reg Registration) error {\n\tif reg.Iface == nil {\n\t\treturn errors.New(\"missing component type\")\n\t}\n\tif reg.Iface.Kind() != reflect.Interface {\n\t\treturn errors.New(\"component type is not an interface\")\n\t}\n\tif reg.Impl == nil {\n\t\treturn errors.New(\"missing implementation type\")\n\t}\n\tif reg.Impl.Kind() != reflect.Struct {\n\t\treturn errors.New(\"implementation type is not a struct\")\n\t}\n\tif reg.LocalStubFn == nil {\n\t\treturn errors.New(\"nil LocalStubFn\")\n\t}\n\tif reg.ClientStubFn == nil {\n\t\treturn errors.New(\"nil ClientStubFn\")\n\t}\n\tif reg.ServerStubFn == nil {\n\t\treturn errors.New(\"nil ServerStubFn\")\n\t}\n\treturn nil\n}\n\n// allComponents returns all of the registered components, keyed by name.\nfunc (r *registry) allComponents() []*Registration {\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\n\tcomponents := make([]*Registration, 0, len(r.components))\n\tfor _, info := range r.components {\n\t\tcomponents = append(components, info)\n\t}\n\treturn components\n}\n\nfunc (r *registry) find(path string) (*Registration, bool) {\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\treg, ok := r.byName[path]\n\treturn reg, ok\n}\n\n// ComponentConfigValidator checks that cfg is a valid configuration\n// for the component type whose fully qualified name is given by path.\n//\n// TODO(mwhittaker): Move out of codegen package? It's not used by the\n// generated code.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// CallEdge records that fact that the Caller component uses the\n// Callee component. Both types are types of the corresponding\n// component interfaces.\ntype CallEdge struct {\n\tCaller reflect.Type\n\tCallee reflect.Type\n}\n\n// CallGraph returns the component call graph (as a list of CallEdge values).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func ComponentConfigValidator(path, cfg string) error {\n# Natural Language Annotations: // ComponentConfigValidator checks that cfg is a valid configuration\n// for the component type whose fully qualified name is given by path.\n//\n// TODO(mwhittaker): Move out of codegen package? It's not used by the\n// generated code.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ComponentConfigValidator(path, cfg string) error {\n\treg, ok := Find(path)\n\tif !ok {\n\t\treturn fmt.Errorf(\"component %q not found\", path)\n\t}\n\n\tcfgStructType := reg.Impl\n\tcfgStruct := reflect.New(cfgStructType).Interface()\n\tif err := config.Unmarshal([]byte(cfg), cfgStruct); err != nil {\n\t\treturn fmt.Errorf(\"invalid configuration for component %q: %w\", path, err)\n\t}\n\n\tif err := config.Validate(cfgStruct); err != nil {\n\t\treturn fmt.Errorf(\"invalid configuration for component %q: %w\", path, err)\n\t}\n\n\t// TODO: Add any additional validation logic here\n\n\treturn nil\n}\n```", "```go\nfunc ComponentConfigValidator(path, cfg string) error {\n\treg, ok := Find(path)\n\tif !ok {\n\t\treturn fmt.Errorf(\"component %s not found\", path)\n\t}\n\n\tif _, err := config.Parse(cfg, reg.Impl.Elem()); err != nil {\n\t\treturn fmt.Errorf(\"invalid config for component %s: %w\", path, err)\n\t}\n\n\treturn nil\n}\n```", "```go\nfunc ComponentConfigValidator(path, cfg string) error {\n\treg, ok := globalRegistry.find(path)\n\tif !ok {\n\t\treturn fmt.Errorf(\"component %s not found\", path)\n\t}\n\tcfgType := reflect.TypeOf(cfg)\n\tif cfgType != reg.Impl {\n\t\treturn fmt.Errorf(\"invalid configuration for component %s: expected %v, got %v\", path, reg.Impl, cfgType)\n\t}\n\treturn nil\n}\n```"]}, "weaver-runtime/codegen/registry.go-CallGraph": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/codegen/registry.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage codegen\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"strings\"\n\t\"sync\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/config\"\n\t\"github.com/ServiceWeaver/weaver/runtime\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// globalRegistry is the global registry used by Register and Registered.\nvar globalRegistry registry\n\n// Register registers a Service Weaver component.\nfunc Register(reg Registration) {\n\tif err := globalRegistry.register(reg); err != nil {\n\t\tpanic(err)\n\t}\n}\n\n// Registered returns the components registered with Register.\nfunc Registered() []*Registration {\n\treturn globalRegistry.allComponents()\n}\n\n// Find returns the registration of the named component.\nfunc Find(name string) (*Registration, bool) {\n\treturn globalRegistry.find(name)\n}\n\n// registry is a repository for registered Service Weaver components.\n// Entries are typically added to the default registry by calls\n// to Register in init functions in code generated by \"weaver generate\".\ntype registry struct {\n\tm          sync.Mutex\n\tcomponents map[reflect.Type]*Registration // the set of registered components, by their interface types\n\tbyName     map[string]*Registration       // map from full component name to registration\n}\n\n// Registration is the configuration needed to register a Service Weaver component.\ntype Registration struct {\n\tName      string       // full package-prefixed component name\n\tIface     reflect.Type // interface type for the component\n\tImpl      reflect.Type // implementation type (struct)\n\tRouted    bool         // True if calls to this component should be routed\n\tListeners []string     // the names of any weaver.Listeners\n\tNoRetry   []int        // indices of methods that should not be retried\n\n\t// Functions that return different types of stubs.\n\tLocalStubFn   func(impl any, caller string, tracer trace.Tracer) any\n\tClientStubFn  func(stub Stub, caller string) any\n\tServerStubFn  func(impl any, load func(key uint64, load float64)) Server\n\tReflectStubFn func(func(method string, ctx context.Context, args []any, returns []any) error) any\n\n\t// RefData holds a string containing the result of MakeEdgeString(Name, Dst)\n\t// for all components named Dst used by this component.\n\tRefData string\n}\n\n// register registers a Service Weaver component. If the registry's close method was\n// previously called, Register will fail and return a non-nil error.\nfunc (r *registry) register(reg Registration) error {\n\tif err := verifyRegistration(reg); err != nil {\n\t\treturn fmt.Errorf(\"Register(%q): %w\", reg.Name, err)\n\t}\n\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\tif old, ok := r.components[reg.Iface]; ok {\n\t\treturn fmt.Errorf(\"component %s already registered for type %v when registering %v\",\n\t\t\treg.Name, old.Impl, reg.Impl)\n\t}\n\tif r.components == nil {\n\t\tr.components = map[reflect.Type]*Registration{}\n\t}\n\tif r.byName == nil {\n\t\tr.byName = map[string]*Registration{}\n\t}\n\tptr := &reg\n\tr.components[reg.Iface] = ptr\n\tr.byName[reg.Name] = ptr\n\treturn nil\n}\n\nfunc verifyRegistration(reg Registration) error {\n\tif reg.Iface == nil {\n\t\treturn errors.New(\"missing component type\")\n\t}\n\tif reg.Iface.Kind() != reflect.Interface {\n\t\treturn errors.New(\"component type is not an interface\")\n\t}\n\tif reg.Impl == nil {\n\t\treturn errors.New(\"missing implementation type\")\n\t}\n\tif reg.Impl.Kind() != reflect.Struct {\n\t\treturn errors.New(\"implementation type is not a struct\")\n\t}\n\tif reg.LocalStubFn == nil {\n\t\treturn errors.New(\"nil LocalStubFn\")\n\t}\n\tif reg.ClientStubFn == nil {\n\t\treturn errors.New(\"nil ClientStubFn\")\n\t}\n\tif reg.ServerStubFn == nil {\n\t\treturn errors.New(\"nil ServerStubFn\")\n\t}\n\treturn nil\n}\n\n// allComponents returns all of the registered components, keyed by name.\nfunc (r *registry) allComponents() []*Registration {\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\n\tcomponents := make([]*Registration, 0, len(r.components))\n\tfor _, info := range r.components {\n\t\tcomponents = append(components, info)\n\t}\n\treturn components\n}\n\nfunc (r *registry) find(path string) (*Registration, bool) {\n\tr.m.Lock()\n\tdefer r.m.Unlock()\n\treg, ok := r.byName[path]\n\treturn reg, ok\n}\n\n// ComponentConfigValidator checks that cfg is a valid configuration\n// for the component type whose fully qualified name is given by path.\n//\n// TODO(mwhittaker): Move out of codegen package? It's not used by the\n// generated code.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// CallEdge records that fact that the Caller component uses the\n// Callee component. Both types are types of the corresponding\n// component interfaces.\ntype CallEdge struct {\n\tCaller reflect.Type\n\tCallee reflect.Type\n}\n\n// CallGraph returns the component call graph (as a list of CallEdge values).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func CallGraph() []CallEdge {\n# Natural Language Annotations: // CallGraph returns the component call graph (as a list of CallEdge values).\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc CallGraph() []CallEdge {\n\tglobalRegistry.m.Lock()\n\tdefer globalRegistry.m.Unlock()\n\t\n\tvar callGraph []CallEdge\n\tfor _, reg := range globalRegistry.components {\n\t\timplType := reg.Impl\n\t\tfor i := 0; i < implType.NumField(); i++ {\n\t\t\tfield := implType.Field(i)\n\t\t\tif field.Type.Kind() != reflect.Interface {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif reg, ok := globalRegistry.find(field.Type.String()); ok {\n\t\t\t\tcallGraph = append(callGraph, CallEdge{\n\t\t\t\t\tCaller: reg.Iface,\n\t\t\t\t\tCallee: field.Type,\n\t\t\t\t})\n\t\t\t}\n\t\t}\n\t}\n\treturn callGraph\n}\n```", "```go\nfunc CallGraph() []CallEdge {\n\tglobalRegistry.m.Lock()\n\tdefer globalRegistry.m.Unlock()\n\n\tvar edges []CallEdge\n\tfor _, info := range globalRegistry.components {\n\t\tfor _, method := range info.Methods {\n\t\t\tfor _, call := range method.Calls {\n\t\t\t\tedges = append(edges, CallEdge{\n\t\t\t\t\tCaller: info.Iface,\n\t\t\t\t\tCallee: call.Iface,\n\t\t\t\t})\n\t\t\t}\n\t\t}\n\t}\n\treturn edges\n}\n```", "```go\nfunc CallGraph() []CallEdge {\n\tglobalRegistry.m.Lock()\n\tdefer globalRegistry.m.Unlock()\n\n\tvar callGraph []CallEdge\n\tfor _, reg := range globalRegistry.components {\n\t\tcallGraph = append(callGraph, CallEdge{\n\t\t\tCaller: reg.Impl,\n\t\t\tCallee: reg.Iface,\n\t\t})\n\t}\n\n\treturn callGraph\n}\n```"]}, "weaver-runtime/codegen/listeners.go-MakeListenersString": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/codegen/listeners.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage codegen\n\nimport (\n\t\"crypto/sha256\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"sort\"\n\t\"strings\"\n)\n\n// All listeners used by a given component are embedded in the generated\n// binary as specially formatted strings. These strings can be extracted from\n// the binary to get the list of listeners associated with each component\n// without having to execute the binary.\n//\n// The set of listeners used by a given component is represented by a string\n// fragment that looks like:\n// \u27e6checksum:wEaVeRlIsTeNeRs:component\u2192listeners\u27e7\n//\n// checksum is the first 8 bytes of the hex encoding of the SHA-256 of\n// the string \"wEaVeRlIsTeNeRs:component\u2192listeners\"; component is the fully\n// qualified component type name; listeners is a comma-separated list of\n// all listener names associated with a given component.\n\n// MakeListenersString returns a string that should be emitted into generated\n// code to represent the set of listeners associated with a given component.\n\n\n\n\n\n\n\n// ComponentListeners represents a set of listeners for a given component.\ntype ComponentListeners struct {\n\t// Fully qualified component type name, e.g.,\n\t//   github.com/ServiceWeaver/weaver/Main.\n\tComponent string\n\n\t// The list of listener names associated with the component.\n\tListeners []string\n}\n\n// ExtractListeners returns the components and their listeners encoded using\n// MakeListenersString() in data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc checksumListeners(component, lisstr string) string {\n\tstr := fmt.Sprintf(\"wEaVeRlIsTeNeRs:%s\u2192%s\", component, lisstr)\n\tsum := sha256.Sum256([]byte(str))\n\treturn fmt.Sprintf(\"%0x\", sum)[:8]\n}\n\n```\n# Function Signature: func MakeListenersString(component string, listeners []string) string {\n# Natural Language Annotations: // All listeners used by a given component are embedded in the generated\n// binary as specially formatted strings. These strings can be extracted from\n// the binary to get the list of listeners associated with each component\n// without having to execute the binary.\n//\n// The set of listeners used by a given component is represented by a string\n// fragment that looks like:\n// \u27e6checksum:wEaVeRlIsTeNeRs:component\u2192listeners\u27e7\n//\n// checksum is the first 8 bytes of the hex encoding of the SHA-256 of\n// the string \"wEaVeRlIsTeNeRs:component\u2192listeners\"; component is the fully\n// qualified component type name; listeners is a comma-separated list of\n// all listener names associated with a given component.\n// MakeListenersString returns a string that should be emitted into generated\n// code to represent the set of listeners associated with a given component.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc MakeListenersString(component string, listeners []string) string {\n\tsort.Strings(listeners)\n\tlisstr := strings.Join(listeners, \",\")\n\tchecksum := checksumListeners(component, lisstr)\n\treturn fmt.Sprintf(\"\u27e6checksum:%s:wEaVeRlIsTeNeRs:%s\u2192%s\u27e7\", checksum, component, lisstr)\n}\n```", "```go\nfunc MakeListenersString(component string, listeners []string) string {\n\tsort.Strings(listeners)\n\tlisstr := strings.Join(listeners, \",\")\n\tchecksum := checksumListeners(component, lisstr)\n\treturn fmt.Sprintf(\"\u27e6checksum:%s:%s\u2192%s\u27e7\", checksum, component, lisstr)\n}\n```\n\nThis function takes a component name and a list of listener names as input, sorts the listeners alphabetically, joins them into a single string, computes the checksum for the given component and listeners, and finally returns a specially formatted string containing the checksum, component name, and listener names.", "```go\npackage codegen\n\nimport (\n\t\"crypto/sha256\"\n\t\"fmt\"\n\t\"strings\"\n)\n\nfunc MakeListenersString(component string, listeners []string) string {\n\t// Sort the listeners to ensure consistent output.\n\tsort.Strings(listeners)\n\n\t// Join the listeners into a comma-separated string.\n\tlisstr := strings.Join(listeners, \",\")\n\n\t// Compute the checksum.\n\tchecksum := checksumListeners(component, lisstr)\n\n\t// Return the formatted string.\n\treturn fmt.Sprintf(\"\u27e6checksum:%s:wEaVeRlIsTeNeRs:%s\u2192%s\u27e7\", checksum, component, lisstr)\n}\n```\n\nThis function takes a component name and a slice of listener names as input, sorts the listeners, joins them into a comma-separated string, computes the checksum of the string \"wEaVeRlIsTeNeRs:component\u2192listeners\", and then returns a formatted string that represents the set of listeners associated with the given component."]}, "weaver-runtime/codegen/listeners.go-ExtractListeners": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/codegen/listeners.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage codegen\n\nimport (\n\t\"crypto/sha256\"\n\t\"fmt\"\n\t\"regexp\"\n\t\"sort\"\n\t\"strings\"\n)\n\n// All listeners used by a given component are embedded in the generated\n// binary as specially formatted strings. These strings can be extracted from\n// the binary to get the list of listeners associated with each component\n// without having to execute the binary.\n//\n// The set of listeners used by a given component is represented by a string\n// fragment that looks like:\n// \u27e6checksum:wEaVeRlIsTeNeRs:component\u2192listeners\u27e7\n//\n// checksum is the first 8 bytes of the hex encoding of the SHA-256 of\n// the string \"wEaVeRlIsTeNeRs:component\u2192listeners\"; component is the fully\n// qualified component type name; listeners is a comma-separated list of\n// all listener names associated with a given component.\n\n// MakeListenersString returns a string that should be emitted into generated\n// code to represent the set of listeners associated with a given component.\n\n\n\n\n\n\n\n// ComponentListeners represents a set of listeners for a given component.\ntype ComponentListeners struct {\n\t// Fully qualified component type name, e.g.,\n\t//   github.com/ServiceWeaver/weaver/Main.\n\tComponent string\n\n\t// The list of listener names associated with the component.\n\tListeners []string\n}\n\n// ExtractListeners returns the components and their listeners encoded using\n// MakeListenersString() in data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc checksumListeners(component, lisstr string) string {\n\tstr := fmt.Sprintf(\"wEaVeRlIsTeNeRs:%s\u2192%s\", component, lisstr)\n\tsum := sha256.Sum256([]byte(str))\n\treturn fmt.Sprintf(\"%0x\", sum)[:8]\n}\n\n```\n# Function Signature: func ExtractListeners(data []byte) []ComponentListeners {\n# Natural Language Annotations: // ExtractListeners returns the components and their listeners encoded using\n// MakeListenersString() in data.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ExtractListeners(data []byte) []ComponentListeners {\n\t// This regular expression matches the string fragment representing a\n\t// component and its listeners.\n\tre := regexp.MustCompile(`\u27e6checksum:(?P<checksum>.*?):(?P<component>.*?)\u2192(?P<listeners>.*?)\u27e7`)\n\n\t// Find all matches in the data.\n\tmatches := re.FindAllStringSubmatch(string(data), -1)\n\n\t// For each match, check that the checksum is correct and, if it is,\n\t// add the component and its listeners to the result.\n\tresult := []ComponentListeners{}\n\tfor _, match := range matches {\n\t\tchecksum := match[1]\n\t\tcomponent := match[2]\n\t\tlisteners := strings.Split(match[3], \",\")\n\t\tif checksumListeners(component, strings.Join(listeners, \",\")) == checksum {\n\t\t\tresult = append(result, ComponentListeners{\n\t\t\t\tComponent: component,\n\t\t\t\tListeners: listeners,\n\t\t\t})\n\t\t}\n\t}\n\n\treturn result\n}\n```", "```go\nfunc ExtractListeners(data []byte) []ComponentListeners {\n\tpattern := regexp.MustCompile(`\u27e6checksum:([0-9A-Fa-f]{8}):([^\u2192]+)\u2192([^\u27e7]+)\u27e7`)\n\tmatches := pattern.FindAllStringSubmatch(string(data), -1)\n\tcomponents := make(map[string]ComponentListeners)\n\tfor _, match := range matches {\n\t\tchecksum := match[1]\n\t\tcomponent := match[2]\n\t\tlisteners := strings.Split(match[3], \",\")\n\t\tsort.Strings(listeners)\n\t\tif c, ok := components[component]; ok {\n\t\t\tc.Listeners = append(c.Listeners, listeners...)\n\t\t\tcomponents[component] = c\n\t\t} else {\n\t\t\tcomponents[component] = ComponentListeners{\n\t\t\t\tComponent: component,\n\t\t\t\tListeners: listeners,\n\t\t\t}\n\t\t}\n\t}\n\n\tresult := make([]ComponentListeners, 0, len(components))\n\tfor _, c := range components {\n\t\tsort.Strings(c.Listeners)\n\t\tresult = append(result, c)\n\t}\n\n\treturn result\n}\n```", "```go\nfunc ExtractListeners(data []byte) []ComponentListeners {\n\tvar listeners []ComponentListeners\n\tpattern := regexp.MustCompile(`\u27e6checksum:(\\w{8}):([^\u2192]+)\u2192([^\u27e7]+)\u27e7`)\n\tmatches := pattern.FindAllStringSubmatch(string(data), -1)\n\tfor _, match := range matches {\n\t\tif len(match) != 4 {\n\t\t\tcontinue\n\t\t}\n\t\tchecksum, component, lisstr := match[1], match[2], match[3]\n\t\tif checksum != checksumListeners(component, lisstr) {\n\t\t\tcontinue\n\t\t}\n\t\tlisteners = append(listeners, ComponentListeners{\n\t\t\tComponent: component,\n\t\t\tListeners: strings.Split(lisstr, \",\"),\n\t\t})\n\t}\n\treturn listeners\n}\n```\nPlease note that this function assumes a specific format for the data string that matches the pattern `\u27e6checksum:(\\w{8}):([^\u2192]+)\u2192([^\u27e7]+)\u27e7`. Each match consists of a checksum, a component string, and a listener string, separated by \"\u2192\" and the whole match is enclosed in `\u27e6` and `\u27e7`. The function also assumes that the checksum is the first 8 characters of the hex encoding of the SHA-256 of the string \"wEaVeRlIsTeNeRs:component\u2192listeners\". If any of these assumptions do not hold, the function may not work as expected."]}, "weaver-runtime/codegen/hash.go-Sum64": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/codegen/hash.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage codegen\n\nimport (\n\t\"crypto/sha256\"\n\t\"encoding/binary\"\n\t\"math\"\n)\n\n// Hasher computes a non-cryptographic hash of the sequence of values\n// added to it.\n//\n// If the same sequence of values is added to two differ Hashers, they\n// will produce the same result, even if they are in different processes.\ntype Hasher struct {\n\t// TODO: improve performance:\n\t// - do not accumulate everything; hash as we go\n\t// - use a non-cryptographically safe hasher\n\tenc Encoder\n}\n\n// Sum64 returns the 64-bit hash of the sequence of values added so far.\n// The resulting is in the range [1,2^64-2], i.e., it is never 0 or math.MaxUint64.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// WriteString adds a string to the hasher.\nfunc (h *Hasher) WriteString(v string) { h.enc.String(v) }\n\n// WriteFloat32 adds a float32 to the hasher.\nfunc (h *Hasher) WriteFloat32(v float32) { h.enc.Float32(v) }\n\n// WriteFloat64 adds a float64 to the hasher.\nfunc (h *Hasher) WriteFloat64(v float64) { h.enc.Float64(v) }\n\n// WriteInt adds a int to the hasher.\nfunc (h *Hasher) WriteInt(v int) { h.enc.Int(v) }\n\n// WriteInt8 adds a int8 to the hasher.\nfunc (h *Hasher) WriteInt8(v int8) { h.enc.Int8(v) }\n\n// WriteInt16 adds a int16 to the hasher.\nfunc (h *Hasher) WriteInt16(v int16) { h.enc.Int16(v) }\n\n// WriteInt32 adds a int32 to the hasher.\nfunc (h *Hasher) WriteInt32(v int32) { h.enc.Int32(v) }\n\n// WriteInt64 adds a int64 to the hasher.\nfunc (h *Hasher) WriteInt64(v int64) { h.enc.Int64(v) }\n\n// WriteUint adds a uint to the hasher.\nfunc (h *Hasher) WriteUint(v uint) { h.enc.Uint(v) }\n\n// WriteUint8 adds a uint8 to the hasher.\nfunc (h *Hasher) WriteUint8(v uint8) { h.enc.Uint8(v) }\n\n// WriteUint16 adds a uint16 to the hasher.\nfunc (h *Hasher) WriteUint16(v uint16) { h.enc.Uint16(v) }\n\n// WriteUint32 adds a uint32 to the hasher.\nfunc (h *Hasher) WriteUint32(v uint32) { h.enc.Uint32(v) }\n\n// WriteUint64 adds a uint64 to the hasher.\nfunc (h *Hasher) WriteUint64(v uint64) { h.enc.Uint64(v) }\n\n```\n# Function Signature: func (h *Hasher) Sum64() uint64 {\n# Natural Language Annotations: // Sum64 returns the 64-bit hash of the sequence of values added so far.\n// The resulting is in the range [1,2^64-2], i.e., it is never 0 or math.MaxUint64.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (h *Hasher) Sum64() uint64 {\n\tsum := sha256.Sum256(h.enc.Bytes())\n\treturn binary.BigEndian.Uint64(sum[:]) + 1\n}\n```", "```go\nfunc (h *Hasher) Sum64() uint64 {\n    sum := h.enc.Sum64()\n    if sum == math.MaxUint64 {\n        return 1\n    }\n    return sum + 1\n}\n```", "Sure, based on the given information, here's a simple implementation of the `Sum64` function. This function simply calls the `Sum` method of the `Encoder` and converts the result to a `uint64`. I'm assuming here that the `Encoder` interface has a `Sum` method that returns a `[]byte`.\n\n```go\npackage codegen\n\nimport (\n\t\"crypto/sha256\"\n\t\"encoding/binary\"\n\t\"math\"\n)\n\n// ...\n\n// Sum64 returns the 64-bit hash of the sequence of values added so far.\n// The resulting is in the range [1,2^64-2], i.e., it is never 0 or math.MaxUint64.\nfunc (h *Hasher) Sum64() uint64 {\n\t// Get the hash as bytes\n\thashBytes := h.enc.Sum()\n\n\t// Convert the bytes to a uint64\n\thashUint64 := binary.BigEndian.Uint64(hashBytes)\n\n\t// Ensure the hash is not 0 or math.MaxUint64\n\tif hashUint64 == 0 || hashUint64 == math.MaxUint64 {\n\t\t// Add 1 to the hash to ensure it's never 0 or math.MaxUint64\n\t\thashUint64++\n\t}\n\n\treturn hashUint64\n}\n```\n\nPlease note that this function assumes that the `Sum` method of the `Encoder` interface returns a byte slice that can be converted to a `uint64` without loss of information. This might not be the case if the `Encoder` interface is not compatible with the `sha256.Sum256` function or the `binary.BigEndian` package. If the `Encoder` interface is not compatible with these functions, you may need to adjust the implementation of the `Sum64` function."]}, "weaver-runtime/graph/adjacency.go-NewAdjacencyGraph": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/graph/adjacency.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage graph\n\nimport (\n\t\"fmt\"\n\n\t\"golang.org/x/exp/slices\"\n)\n\ntype adjacencyGraph struct {\n\t// out[n] stores a list of nodes that n has an outgoing edge to.\n\t// out[n] == nil means that n is not a node in the graph.\n\t// out[n] == []Node{} means that node n exists but has no outgoing edges.\n\tout [][]Node\n}\n\nvar _ Graph = &adjacencyGraph{}\n\n// NewAdjacencyGraph returns a Graph represented using adjacency lists.\n//\n// It panics if it specified edge nodes aren't in nodes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar _ Graph = &adjacencyGraph{}\n\n// PerNode implements the Graph interface.\nfunc (g *adjacencyGraph) PerNode(fn func(n Node)) {\n\tfor n, dsts := range g.out {\n\t\tif dsts == nil { // not a node\n\t\t\tcontinue\n\t\t}\n\t\tfn(Node(n))\n\t}\n}\n\n// PerOutEdge implements the Graph interface.\nfunc (g *adjacencyGraph) PerOutEdge(src Node, fn func(e Edge)) {\n\tif !isNode(src, g.out) {\n\t\tpanic(fmt.Sprintf(\"src %d is not a node\", src))\n\t}\n\tfor _, dst := range g.out[src] {\n\t\tfn(Edge{Src: src, Dst: dst})\n\t}\n}\n\n// NodeLimit implements the Graph interface.\nfunc (g *adjacencyGraph) NodeLimit() int {\n\treturn len(g.out)\n}\n\nfunc isNode(n Node, out [][]Node) bool {\n\treturn n >= 0 && int(n) < len(out) && out[n] != nil\n}\n\n```\n# Function Signature: func NewAdjacencyGraph(nodes []Node, edges []Edge) Graph {\n# Natural Language Annotations: // NewAdjacencyGraph returns a Graph represented using adjacency lists.\n//\n// It panics if it specified edge nodes aren't in nodes.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewAdjacencyGraph(nodes []Node, edges []Edge) Graph {\n\tadjacency := make([][]Node, len(nodes))\n\n\t// Initialize the adjacency list with an empty slice for each node\n\tfor i := range adjacency {\n\t\tadjacency[i] = make([]Node, 0)\n\t}\n\n\t// Populate the adjacency list with the edges\n\tfor _, edge := range edges {\n\t\tif !slices.Contains(nodes, edge.Src) || !slices.Contains(nodes, edge.Dst) {\n\t\t\tpanic(fmt.Sprintf(\"Node %v or %v not found in the nodes list\", edge.Src, edge.Dst))\n\t\t}\n\t\tadjacency[edge.Src] = append(adjacency[edge.Src], edge.Dst)\n\t}\n\n\treturn &adjacencyGraph{out: adjacency}\n}\n```", "```go\nfunc NewAdjacencyGraph(nodes []Node, edges []Edge) Graph {\n\tout := make([][]Node, len(nodes))\n\tfor _, e := range edges {\n\t\tif e.Src >= Node(len(nodes)) || e.Dst >= Node(len(nodes)) {\n\t\t\tpanic(fmt.Sprintf(\"edge nodes %d or %d aren't in nodes\", e.Src, e.Dst))\n\t\t}\n\t\tout[e.Src] = append(out[e.Src], e.Dst)\n\t}\n\treturn &adjacencyGraph{out: out}\n}\n```", "```go\nfunc NewAdjacencyGraph(nodes []Node, edges []Edge) Graph {\n\tout := make([][]Node, len(nodes))\n\tfor _, edge := range edges {\n\t\tsrc := edge.Src\n\t\tdst := edge.Dst\n\n\t\tif int(src) >= len(nodes) || int(dst) >= len(nodes) {\n\t\t\tpanic(fmt.Sprintf(\"edge (%d, %d) has node out of range\", src, dst))\n\t\t}\n\n\t\tout[src] = append(out[src], dst)\n\t}\n\n\treturn &adjacencyGraph{out: out}\n}\n```"]}, "weaver-runtime/graph/dfs.go-DFSAll": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/graph/dfs.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage graph\n\nimport \"slices\"\n\n// DFSAll performs a depth first search of all nodes in g.\n// If enter is non-nil, it is called on entry to a node.\n// If exit is non-nil, it is called on exit from a node.\n\n\n\n\n\n\n\n\n// PostOrder returns nodes in g in post-order.\nfunc PostOrder(g Graph) []Node {\n\tvar result []Node\n\tDFSAll(g, nil, func(n Node) {\n\t\tresult = append(result, n)\n\t})\n\treturn result\n}\n\n// ReversePostOrder returns nodes in g in reverse-post-order.\nfunc ReversePostOrder(g Graph) []Node {\n\tresult := PostOrder(g)\n\tslices.Reverse(result)\n\treturn result\n}\n\nfunc dfs(g Graph, roots []Node, enter, exit func(Node)) {\n\t// Stack holds nodes to traverse.  If we need to call exit, we\n\t// leave a negative marker at the appropriate place in the stack.\n\tvar stack []Node\n\tvisited := make([]bool, g.NodeLimit())\n\tfor _, r := range roots {\n\t\tstack = append(stack, r)\n\t\tfor len(stack) > 0 {\n\t\t\tn := stack[len(stack)-1]\n\t\t\tstack = stack[:len(stack)-1]\n\t\t\tif n < 0 {\n\t\t\t\texit(-n - 1)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif visited[n] {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tvisited[n] = true\n\t\t\tif exit != nil {\n\t\t\t\tstack = append(stack, -n-1) // Exit marker\n\t\t\t}\n\t\t\tif enter != nil {\n\t\t\t\tenter(n)\n\t\t\t}\n\t\t\tg.PerOutEdge(n, func(e Edge) {\n\t\t\t\tstack = append(stack, e.Dst)\n\t\t\t})\n\t\t}\n\t}\n}\n\n```\n# Function Signature: func DFSAll(g Graph, enter, exit func(Node)) {\n# Natural Language Annotations: // DFSAll performs a depth first search of all nodes in g.\n// If enter is non-nil, it is called on entry to a node.\n// If exit is non-nil, it is called on exit from a node.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// DFSAll performs a depth first search of all nodes in g.\n// If enter is non-nil, it is called on entry to a node.\n// If exit is non-nil, it is called on exit from a node.\nfunc DFSAll(g Graph, enter, exit func(Node)) {\n\t// Stack holds nodes to traverse.  If we need to call exit, we\n\t// leave a negative marker at the appropriate place in the stack.\n\tvar stack []Node\n\tvisited := make([]bool, g.NodeLimit())\n\tfor _, r := range roots {\n\t\tstack = append(stack, r)\n\t\tfor len(stack) > 0 {\n\t\t\tn := stack[len(stack)-1]\n\t\t\tstack = stack[:len(stack)-1]\n\t\t\tif n < 0 {\n\t\t\t\texit(-n - 1)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif visited[n] {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tvisited[n] = true\n\t\t\tif exit != nil {\n\t\t\t\tstack = append(stack, -n-1) // Exit marker\n\t\t\t}\n\t\t\tif enter != nil {\n\t\t\t\tenter(n)\n\t\t\t}\n\t\t\tg.PerOutEdge(n, func(e Edge) {\n\t\t\t\tstack = append(stack, e.Dst)\n\t\t\t})\n\t\t}\n\t}\n}\n```\nPlease note that the function `DFSAll` uses the `dfs` function which is not provided in the given code. If you have the definition of `dfs`, you can replace the body of `DFSAll` with the body of `dfs`.", "```go\npackage graph\n\nimport \"slices\"\n\n// DFSAll performs a depth first search of all nodes in g.\n// If enter is non-nil, it is called on entry to a node.\n// If exit is non-nil, it is called on exit from a node.\nfunc DFSAll(g Graph, enter, exit func(Node)) {\n\tvar stack []Node\n\tvisited := make([]bool, g.NodeLimit())\n\t\n\t// Iterate over each node in the graph\n\tfor i := 0; i < g.NodeLimit(); i++ {\n\t\tstack = append(stack, Node(i))\n\t\tfor len(stack) > 0 {\n\t\t\tn := stack[len(stack)-1]\n\t\t\tstack = stack[:len(stack)-1]\n\t\t\tif n < 0 {\n\t\t\t\texit(-n - 1)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif visited[n] {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tvisited[n] = true\n\t\t\tif exit != nil {\n\t\t\t\tstack = append(stack, -n-1) // Exit marker\n\t\t\t}\n\t\t\tif enter != nil {\n\t\t\t\tenter(n)\n\t\t\t}\n\t\t\tg.PerOutEdge(n, func(e Edge) {\n\t\t\t\tstack = append(stack, e.Dst)\n\t\t\t})\n\t\t}\n\t}\n}\n```", "```go\npackage graph\n\n// DFSAll performs a depth first search of all nodes in g.\n// If enter is non-nil, it is called on entry to a node.\n// If exit is non-nil, it is called on exit from a node.\nfunc DFSAll(g Graph, enter, exit func(Node)) {\n\tdfs(g, g.Nodes(), enter, exit)\n}\n```"]}, "weaver-runtime/metrics/labels.go-newLabelExtractor": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/metrics/labels.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage metrics\n\nimport (\n\t\"fmt\"\n\t\"reflect\"\n\t\"unicode\"\n\t\"unicode/utf8\"\n)\n\n// unexport returns a copy of s with the first letter lowercased.\nfunc unexport(s string) string {\n\t// NOTE(mwhittaker): Handling unicode complicates the implementation of\n\t// this function. I took this implementation from [1].\n\t//\n\t// [1]: https://groups.google.com/g/golang-nuts/c/WfpmVDQFecU/m/-1IBD5KI7GEJ.\n\tif s == \"\" {\n\t\treturn \"\"\n\t}\n\tr, n := utf8.DecodeRuneInString(s)\n\treturn string(unicode.ToLower(r)) + s[n:]\n}\n\n// typecheckLabels checks that L is a valid label struct type. See metricMap\n// for a description of valid label struct types.\nfunc typecheckLabels[L comparable]() error {\n\tvar x L\n\tt := reflect.TypeOf(x)\n\tif t.Kind() != reflect.Struct {\n\t\treturn fmt.Errorf(\"metric labels: type %T is not a struct\", x)\n\t}\n\n\tnames := map[string]struct{}{}\n\tfor i := 0; i < t.NumField(); i++ {\n\t\tfi := t.Field(i)\n\n\t\t// Check the type.\n\t\tif fi.Type.PkgPath() != \"\" {\n\t\t\t// Avoid named types like `type foo string`\n\t\t\treturn fmt.Errorf(\"metric labels: field %q of type %T has unsupported type %v\", fi.Name, x, fi.Type.Name())\n\t\t}\n\t\tswitch fi.Type.Kind() {\n\t\tcase reflect.String,\n\t\t\treflect.Bool,\n\t\t\treflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64,\n\t\t\treflect.Uint, reflect.Uint8, reflect.Uint16, reflect.Uint32, reflect.Uint64:\n\t\tdefault:\n\t\t\treturn fmt.Errorf(\"metric labels: field %q of type %T has unsupported type %v\", fi.Name, x, fi.Type.Name())\n\t\t}\n\n\t\t// Check the visibility.\n\t\tif !fi.IsExported() {\n\t\t\treturn fmt.Errorf(\"metric labels: field %q of type %T is unexported\", fi.Name, x)\n\t\t}\n\n\t\t// Check for duplicate fields.\n\t\tname := unexport(fi.Name)\n\t\tif alias, ok := fi.Tag.Lookup(\"weaver\"); ok {\n\t\t\tname = alias\n\t\t}\n\t\tif _, ok := names[name]; ok {\n\t\t\treturn fmt.Errorf(\"metric labels: type %T has duplicate field %q\", x, fi.Name)\n\t\t}\n\t\tnames[name] = struct{}{}\n\t}\n\n\treturn nil\n}\n\n// labelExtractor extracts labels from a label struct of type L.\ntype labelExtractor[L comparable] struct {\n\tfields []field\n}\n\ntype field struct {\n\tf    reflect.StructField // struct field\n\tname string              // field name, or alias if present\n}\n\n// newLabelExtractor returns a new labelExtractor that can extract the labels\n// from a label struct of type L. L must be a valid label struct type.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Extract extracts the labels from a label struct. The provided labels must be\n// the same type used to construct the labelExtractor.\nfunc (l *labelExtractor[L]) Extract(labels L) map[string]string {\n\tv := reflect.ValueOf(labels)\n\textracted := map[string]string{}\n\tfor _, field := range l.fields {\n\t\textracted[field.name] = fmt.Sprint(v.FieldByIndex(field.f.Index).Interface())\n\t}\n\treturn extracted\n}\n\n```\n# Function Signature: func newLabelExtractor[L comparable]() *labelExtractor[L] {\n# Natural Language Annotations: // newLabelExtractor returns a new labelExtractor that can extract the labels\n// from a label struct of type L. L must be a valid label struct type.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc newLabelExtractor[L comparable]() (*labelExtractor[L], error) {\n\terr := typecheckLabels[L]()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar x L\n\tt := reflect.TypeOf(x)\n\tfields := make([]field, t.NumField())\n\tfor i := 0; i < t.NumField(); i++ {\n\t\tfi := t.Field(i)\n\t\tname := unexport(fi.Name)\n\t\tif alias, ok := fi.Tag.Lookup(\"weaver\"); ok {\n\t\t\tname = alias\n\t\t}\n\t\tfields[i] = field{f: fi, name: name}\n\t}\n\n\treturn &labelExtractor[L]{fields: fields}, nil\n}\n```", "```go\nfunc newLabelExtractor[L comparable]() (*labelExtractor[L], error) {\n\tif err := typecheckLabels[L](); err != nil {\n\t\treturn nil, fmt.Errorf(\"newLabelExtractor: %v\", err)\n\t}\n\n\tvar x L\n\tt := reflect.TypeOf(x)\n\tfields := make([]field, t.NumField())\n\tfor i := 0; i < t.NumField(); i++ {\n\t\tf := t.Field(i)\n\t\tname := unexport(f.Name)\n\t\tif alias, ok := f.Tag.Lookup(\"weaver\"); ok {\n\t\t\tname = alias\n\t\t}\n\t\tfields[i] = field{f, name}\n\t}\n\n\treturn &labelExtractor[L]{\n\t\tfields: fields,\n\t}, nil\n}\n```", "```go\nfunc newLabelExtractor[L comparable]() (*labelExtractor[L], error) {\n\t// Check if L is a valid label struct type\n\tif err := typecheckLabels[L](); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Create the labelExtractor\n\tvar x L\n\tt := reflect.TypeOf(x)\n\tfields := make([]field, t.NumField())\n\tfor i := 0; i < t.NumField(); i++ {\n\t\tfi := t.Field(i)\n\t\tname := fi.Name\n\t\tif alias, ok := fi.Tag.Lookup(\"weaver\"); ok {\n\t\t\tname = alias\n\t\t}\n\t\tfields[i] = field{\n\t\t\tf:    fi,\n\t\t\tname: unexport(name),\n\t\t}\n\t}\n\n\treturn &labelExtractor[L]{\n\t\tfields: fields,\n\t}, nil\n}\n```"]}, "weaver-runtime/metrics/metrics.go-Snapshot": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/runtime/metrics/metrics.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package metrics implements Service Weaver metrics.\npackage metrics\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"math\"\n\t\"slices\"\n\t\"sort\"\n\t\"sync\"\n\t\"sync/atomic\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/google/uuid\"\n\t\"golang.org/x/exp/maps\"\n)\n\nvar (\n\t// metricNames stores the name of every metric (labeled or not).\n\tmetricNamesMu sync.RWMutex\n\tmetricNames   = map[string]bool{}\n\n\t// metrics stores every metric.\n\tmetricsMu sync.RWMutex\n\tmetrics   = []*Metric{}\n)\n\n// Metric is a thread-safe readable and writeable metric. It is the underlying\n// implementation of the user-facing metrics like Counter and Gauge.\n//\n// Every metric has a unique name assigned by the user. For example, the user\n// may create a histogram called \"http_request_duration\". Every metric also has\n// a fixed, possibly empty, set of labels. For example, the user may assign an\n// \"endpoint\" label to their \"http_request_duration\" to differentiate the\n// latency of different HTTP endpoints. A metric name and set of label values\n// uniquely identify a metric. For example, the following two metrics are\n// different:\n//\n//\thttp_request_duration{endpoint=\"/\"}\n//\thttp_request_duration{endpoint=\"/foo\"}\ntype Metric struct {\n\ttyp         protos.MetricType        // the type of the metric\n\tname        string                   // the globally unique metric name\n\thelp        string                   // a short description of the metric\n\tlabelsThunk func() map[string]string // the (deferred) metric labels\n\n\t// Users may call Get on the critical path of their application, so we want\n\t// a call of `Get(labels)` to be as fast as possible. Converting `labels`\n\t// into a map[string]string requires reflection and can be slow. Computing\n\t// the metric's id is similarly slow. We avoid doing either of these in the\n\t// call to Get and instead initialize them only when needed (i.e. before\n\t// exporting).\n\tonce   sync.Once         // used to initialize id and labels\n\tid     uint64            // globally unique metric id\n\tlabels map[string]string // materialized labels from calling labelsThunk\n\n\tfvalue atomicFloat64 // value for Counter and Gauge, sum for Histogram\n\tivalue atomic.Uint64 // integer increments for Counter (separated for speed)\n\n\t// For histograms only:\n\tputCount atomic.Uint64   // incremented on every Put, for change detection\n\tbounds   []float64       // histogram bounds\n\tcounts   []atomic.Uint64 // histogram counts\n}\n\n// A MetricSnapshot is a snapshot of a metric.\ntype MetricSnapshot struct {\n\tId     uint64\n\tType   protos.MetricType\n\tName   string\n\tLabels map[string]string\n\tHelp   string\n\n\tValue  float64\n\tBounds []float64\n\tCounts []uint64\n}\n\n// MetricDef returns a MetricDef derived from the metric.\nfunc (m *MetricSnapshot) MetricDef() *protos.MetricDef {\n\treturn &protos.MetricDef{\n\t\tId:     m.Id,\n\t\tName:   m.Name,\n\t\tTyp:    m.Type,\n\t\tHelp:   m.Help,\n\t\tLabels: m.Labels,\n\t\tBounds: m.Bounds,\n\t}\n}\n\n// MetricValue returns a MetricValue derived from the metric.\nfunc (m *MetricSnapshot) MetricValue() *protos.MetricValue {\n\treturn &protos.MetricValue{\n\t\tId:     m.Id,\n\t\tValue:  m.Value,\n\t\tCounts: m.Counts,\n\t}\n}\n\n// ToProto converts a MetricSnapshot to its proto equivalent.\nfunc (m *MetricSnapshot) ToProto() *protos.MetricSnapshot {\n\treturn &protos.MetricSnapshot{\n\t\tId:     m.Id,\n\t\tName:   m.Name,\n\t\tTyp:    m.Type,\n\t\tHelp:   m.Help,\n\t\tLabels: m.Labels,\n\t\tBounds: m.Bounds,\n\t\tValue:  m.Value,\n\t\tCounts: m.Counts,\n\t}\n}\n\n// UnProto converts a protos.MetricSnapshot into a metrics.MetricSnapshot.\nfunc UnProto(m *protos.MetricSnapshot) *MetricSnapshot {\n\treturn &MetricSnapshot{\n\t\tId:     m.Id,\n\t\tType:   m.Typ,\n\t\tName:   m.Name,\n\t\tLabels: m.Labels,\n\t\tHelp:   m.Help,\n\t\tValue:  m.Value,\n\t\tBounds: m.Bounds,\n\t\tCounts: m.Counts,\n\t}\n}\n\n// Clone returns a deep copy of m.\nfunc (m *MetricSnapshot) Clone() *MetricSnapshot {\n\tc := *m\n\tc.Labels = maps.Clone(m.Labels)\n\tc.Bounds = slices.Clone(m.Bounds)\n\tc.Counts = slices.Clone(m.Counts)\n\treturn &c\n}\n\n// config configures the creation of a metric.\ntype config struct {\n\tType   protos.MetricType\n\tName   string\n\tLabels func() map[string]string\n\tBounds []float64\n\tHelp   string\n}\n\n// Register registers and returns a new metric. Panics if a metric with the same name\n// has already been registered.\nfunc Register(typ protos.MetricType, name string, help string, bounds []float64) *Metric {\n\tm := RegisterMap[struct{}](typ, name, help, bounds)\n\treturn m.Get(struct{}{})\n}\n\n// newMetric registers and returns a new metric.\nfunc newMetric(config config) *Metric {\n\tmetricsMu.Lock()\n\tdefer metricsMu.Unlock()\n\tmetric := &Metric{\n\t\ttyp:         config.Type,\n\t\tname:        config.Name,\n\t\thelp:        config.Help,\n\t\tlabelsThunk: config.Labels,\n\t\tbounds:      config.Bounds,\n\t}\n\tif config.Type == protos.MetricType_HISTOGRAM {\n\t\tmetric.counts = make([]atomic.Uint64, len(config.Bounds)+1)\n\t}\n\tmetrics = append(metrics, metric)\n\treturn metric\n}\n\n// Name returns the name of the metric.\nfunc (m *Metric) Name() string {\n\treturn m.name\n}\n\n// Inc adds one to the metric value.\nfunc (m *Metric) Inc() {\n\tm.ivalue.Add(1)\n}\n\n// Add adds the provided delta to the metric's value.\nfunc (m *Metric) Add(delta float64) {\n\tm.fvalue.add(delta)\n}\n\n// Sub subtracts the provided delta from the metric's value.\nfunc (m *Metric) Sub(delta float64) {\n\tm.fvalue.add(-delta)\n}\n\n// Set sets the metric's value.\nfunc (m *Metric) Set(val float64) {\n\tm.fvalue.set(val)\n}\n\n// Put adds the provided value to the metric's histogram.\nfunc (m *Metric) Put(val float64) {\n\tvar idx int\n\tif len(m.bounds) == 0 || val < m.bounds[0] {\n\t\t// Skip binary search for values that fall in the first bucket\n\t\t// (often true for short latency operations).\n\t} else {\n\t\tidx = sort.SearchFloat64s(m.bounds, val)\n\t\tif idx < len(m.bounds) && val == m.bounds[idx] {\n\t\t\tidx++\n\t\t}\n\t}\n\tm.counts[idx].Add(1)\n\n\t// Microsecond latencies are often zero for very fast functions.\n\tif val != 0 {\n\t\tm.fvalue.add(val)\n\t}\n\tm.putCount.Add(1)\n}\n\n// initIdAndLabels initializes the id and labels of a metric.\n// We delay this initialization until the first time we export a\n// metric to avoid slowing down a Get() call.\nfunc (m *Metric) initIdAndLabels() {\n\tm.once.Do(func() {\n\t\tif labels := m.labelsThunk(); len(labels) > 0 {\n\t\t\tm.labels = labels\n\t\t}\n\t\tvar id [16]byte = uuid.New()\n\t\tm.id = binary.LittleEndian.Uint64(id[:8])\n\t})\n}\n\n// get returns the current value (sum of all added values for histograms).\nfunc (m *Metric) get() float64 {\n\treturn m.fvalue.get() + float64(m.ivalue.Load())\n}\n\n// Snapshot returns a snapshot of the metric. You must call Init at least once\n// before calling Snapshot.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// MetricDef returns a MetricDef derived from the metric. You must call Init at\n// least once before calling Snapshot.\nfunc (m *Metric) MetricDef() *protos.MetricDef {\n\treturn &protos.MetricDef{\n\t\tId:     m.id,\n\t\tName:   m.name,\n\t\tTyp:    m.typ,\n\t\tHelp:   m.help,\n\t\tLabels: maps.Clone(m.labels),\n\t\tBounds: slices.Clone(m.bounds),\n\t}\n}\n\n// MetricValue returns a MetricValue derived from the metric.\nfunc (m *Metric) MetricValue() *protos.MetricValue {\n\tvar counts []uint64\n\tif n := len(m.counts); n > 0 {\n\t\tcounts = make([]uint64, n)\n\t\tfor i := range m.counts {\n\t\t\tcounts[i] = m.counts[i].Load()\n\t\t}\n\t}\n\treturn &protos.MetricValue{\n\t\tId:     m.id,\n\t\tValue:  m.get(),\n\t\tCounts: counts,\n\t}\n}\n\n// MetricMap is a collection of metrics with the same name and label schema\n// but with different label values. See public metric documentation for\n// an explanation of labels.\n//\n// TODO(mwhittaker): Understand the behavior of prometheus and Google Cloud\n// Metrics when we add or remove metric labels over time.\ntype MetricMap[L comparable] struct {\n\tconfig    config             // configures the metrics returned by Get\n\textractor *labelExtractor[L] // extracts labels from a value of type L\n\tmu        sync.Mutex         // guards metrics\n\tmetrics   map[L]*Metric      // cache of metrics, by label\n}\n\nfunc RegisterMap[L comparable](typ protos.MetricType, name string, help string, bounds []float64) *MetricMap[L] {\n\tif err := typecheckLabels[L](); err != nil {\n\t\tpanic(err)\n\t}\n\tif name == \"\" {\n\t\tpanic(fmt.Errorf(\"empty metric name\"))\n\t}\n\tif typ == protos.MetricType_INVALID {\n\t\tpanic(fmt.Errorf(\"metric %q: invalid metric type %v\", name, typ))\n\t}\n\tfor _, x := range bounds {\n\t\tif math.IsNaN(x) {\n\t\t\tpanic(fmt.Errorf(\"metric %q: NaN histogram bound\", name))\n\t\t}\n\t}\n\tfor i := 0; i < len(bounds)-1; i++ {\n\t\tif bounds[i] >= bounds[i+1] {\n\t\t\tpanic(fmt.Errorf(\"metric %q: non-ascending histogram bounds %v\", name, bounds))\n\t\t}\n\t}\n\n\tmetricNamesMu.Lock()\n\tdefer metricNamesMu.Unlock()\n\tif metricNames[name] {\n\t\tpanic(fmt.Errorf(\"metric %q already exists\", name))\n\t}\n\tmetricNames[name] = true\n\treturn &MetricMap[L]{\n\t\tconfig:    config{Type: typ, Name: name, Help: help, Bounds: bounds},\n\t\textractor: newLabelExtractor[L](),\n\t\tmetrics:   map[L]*Metric{},\n\t}\n}\n\n// Name returns the name of the metricMap.\nfunc (mm *MetricMap[L]) Name() string {\n\treturn mm.config.Name\n}\n\n// Get returns the metric with the provided labels, constructing it if it\n// doesn't already exist. Multiple calls to Get with the same labels will\n// return the same metric.\nfunc (mm *MetricMap[L]) Get(labels L) *Metric {\n\tmm.mu.Lock()\n\tdefer mm.mu.Unlock()\n\tif metric, ok := mm.metrics[labels]; ok {\n\t\treturn metric\n\t}\n\tconfig := mm.config\n\tconfig.Labels = func() map[string]string {\n\t\treturn mm.extractor.Extract(labels)\n\t}\n\tmetric := newMetric(config)\n\tmm.metrics[labels] = metric\n\treturn metric\n}\n\n// Snapshot returns a snapshot of all currently registered metrics. The\n// snapshot is not guaranteed to be atomic.\nfunc Snapshot() []*MetricSnapshot {\n\tmetricsMu.RLock()\n\tdefer metricsMu.RUnlock()\n\tsnapshots := make([]*MetricSnapshot, 0, len(metrics))\n\tfor _, metric := range metrics {\n\t\tmetric.initIdAndLabels()\n\t\tsnapshots = append(snapshots, metric.Snapshot())\n\t}\n\treturn snapshots\n}\n\n```\n# Function Signature: func (m *Metric) Snapshot() *MetricSnapshot {\n# Natural Language Annotations: // Snapshot returns a snapshot of the metric. You must call Init at least once\n// before calling Snapshot.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (m *Metric) Snapshot() *MetricSnapshot {\n\tm.initIdAndLabels()\n\tvar counts []uint64\n\tif n := len(m.counts); n > 0 {\n\t\tcounts = make([]uint64, n)\n\t\tfor i := range m.counts {\n\t\t\tcounts[i] = m.counts[i].Load()\n\t\t}\n\t}\n\treturn &MetricSnapshot{\n\t\tId:     m.id,\n\t\tType:   m.typ,\n\t\tName:   m.name,\n\t\tLabels: maps.Clone(m.labels),\n\t\tHelp:   m.help,\n\t\tValue:  m.get(),\n\t\tBounds: slices.Clone(m.bounds),\n\t\tCounts: counts,\n\t}\n}\n```", "```go\nfunc (m *Metric) Snapshot() *MetricSnapshot {\n\tm.initIdAndLabels()\n\treturn &MetricSnapshot{\n\t\tId:     m.id,\n\t\tType:   m.typ,\n\t\tName:   m.name,\n\t\tLabels: maps.Clone(m.labels),\n\t\tHelp:   m.help,\n\t\tValue:  m.get(),\n\t\tBounds: slices.Clone(m.bounds),\n\t\tCounts: m.countsAsSlice(),\n\t}\n}\n\nfunc (m *Metric) countsAsSlice() []uint64 {\n\tif m.typ != protos.MetricType_HISTOGRAM {\n\t\treturn nil\n\t}\n\tcounts := make([]uint64, len(m.counts))\n\tfor i := range m.counts {\n\t\tcounts[i] = m.counts[i].Load()\n\t}\n\treturn counts\n}\n```", "```go\nfunc (m *Metric) Snapshot() *MetricSnapshot {\n\tm.initIdAndLabels()\n\treturn &MetricSnapshot{\n\t\tId:     m.id,\n\t\tType:   m.typ,\n\t\tName:   m.name,\n\t\tLabels: maps.Clone(m.labels),\n\t\tHelp:   m.help,\n\t\tValue:  m.get(),\n\t\tBounds: slices.Clone(m.bounds),\n\t\tCounts: m.countsClone(),\n\t}\n}\n\n// countsClone returns a copy of the counts slice.\nfunc (m *Metric) countsClone() []uint64 {\n\tif m.counts == nil {\n\t\treturn nil\n\t}\n\tcounts := make([]uint64, len(m.counts))\n\tfor i := range m.counts {\n\t\tcounts[i] = m.counts[i].Load()\n\t}\n\treturn counts\n}\n```"]}, "weaver-internal/tool/generate/types.go-sizeOfType": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/tool/generate/types.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generate\n\nimport (\n\t\"fmt\"\n\t\"go/types\"\n\t\"path\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"golang.org/x/tools/go/packages\"\n\t\"golang.org/x/tools/go/types/typeutil\"\n)\n\nconst weaverPackagePath = \"github.com/ServiceWeaver/weaver\"\n\n// typeSet holds type information needed by the code generator.\ntype typeSet struct {\n\tpkg            *packages.Package\n\timported       []importPkg          // imported packages\n\timportedByPath map[string]importPkg // imported, indexed by path\n\timportedByName map[string]importPkg // imported, indexed by name\n\n\tautomarshals          *typeutil.Map // types that implement AutoMarshal\n\tautomarshalCandidates *typeutil.Map // types that declare themselves AutoMarshal\n\n\t// If checked[t] != nil, then checked[t] is the cached result of calling\n\t// check(pkg, t, string[]{}). Otherwise, if checked[t] == nil, then t has\n\t// not yet been checked for serializability. Read typeutil.Map's\n\t// documentation for why checked shouldn't be a map[types.Type]bool.\n\tchecked typeutil.Map\n\n\t// If sizes[t] != nil, then sizes[t] == sizeOfType(t).\n\tsizes typeutil.Map\n\n\t// If measurable[t] != nil, then measurable[t] == isMeasurableType(t).\n\tmeasurable typeutil.Map\n}\n\n// importPkg is a package imported by the generated code.\ntype importPkg struct {\n\tpath  string // e.g., \"github.com/ServiceWeaver/weaver\"\n\tpkg   string // e.g., \"weaver\", \"context\", \"time\"\n\talias string // e.g., foo in `import foo \"context\"`\n\tlocal bool   // are we in this package?\n}\n\n// name returns the name by which the imported package should be referenced in\n// the generated code. If the package is imported without an alias, like this:\n//\n//\timport \"context\"\n//\n// then the name is the same as the package name (e.g., \"context\"). However, if\n// a package is imported with an alias, then the name is the alias:\n//\n//\timport thisIsAnAlias \"context\"\n//\n// If the package is local, an empty string is returned.\nfunc (i importPkg) name() string {\n\tif i.local {\n\t\treturn \"\"\n\t} else if i.alias != \"\" {\n\t\treturn i.alias\n\t}\n\treturn i.pkg\n}\n\n// qualify returns the provided member of the package, qualified with the\n// package name. For example, the \"Context\" type inside the \"context\" package\n// is qualified \"context.Context\". The \"Now\" function inside the \"time\" package\n// is qualified \"time.Now\". Note that the package name is not prefixed when\n// qualifying members of the local package.\nfunc (i importPkg) qualify(member string) string {\n\tif i.local {\n\t\treturn member\n\t}\n\treturn fmt.Sprintf(\"%s.%s\", i.name(), member)\n}\n\n// newTypeSet returns the container for types found in pkg.\nfunc newTypeSet(pkg *packages.Package, automarshals, automarshalCandidates *typeutil.Map) *typeSet {\n\treturn &typeSet{\n\t\tpkg:                   pkg,\n\t\timported:              []importPkg{},\n\t\timportedByPath:        map[string]importPkg{},\n\t\timportedByName:        map[string]importPkg{},\n\t\tautomarshals:          automarshals,\n\t\tautomarshalCandidates: automarshalCandidates,\n\t}\n}\n\n// importPackage imports a package with the provided path and package name. The\n// package is imported with an alias if there is a package name clash.\nfunc (tset *typeSet) importPackage(path, pkg string) importPkg {\n\tnewImportPkg := func(path, pkg, alias string, local bool) importPkg {\n\t\ti := importPkg{path: path, pkg: pkg, alias: alias, local: local}\n\t\ttset.imported = append(tset.imported, i)\n\t\ttset.importedByPath[i.path] = i\n\t\ttset.importedByName[i.name()] = i\n\t\treturn i\n\t}\n\n\tif imp, ok := tset.importedByPath[path]; ok {\n\t\t// This package has already been imported.\n\t\treturn imp\n\t}\n\n\tif _, ok := tset.importedByName[pkg]; !ok {\n\t\t// Import the package without an alias.\n\t\treturn newImportPkg(path, pkg, \"\", path == tset.pkg.PkgPath)\n\t}\n\n\t// Find an unused alias.\n\tvar alias string\n\tcounter := 1\n\tfor {\n\t\talias = fmt.Sprintf(\"%s%d\", pkg, counter)\n\t\tif _, ok := tset.importedByName[alias]; !ok {\n\t\t\tbreak\n\t\t}\n\t\tcounter++\n\t}\n\treturn newImportPkg(path, pkg, alias, path == tset.pkg.PkgPath)\n}\n\n// imports returns the list of packages to import in generated code.\nfunc (tset *typeSet) imports() []importPkg {\n\tsort.Slice(tset.imported, func(i, j int) bool {\n\t\treturn tset.imported[i].path < tset.imported[j].path\n\t})\n\treturn tset.imported\n}\n\n// checkSerializable checks that type t is serializable.\nfunc (tset *typeSet) checkSerializable(t types.Type) []error {\n\t// lineage can generate a human readable description of the lineage of a\n\t// checked type. As check recurses on type t, it encounters a number of\n\t// nested types. For example, if we have the following type A\n\t//\n\t//     type A struct{ x []chan int }\n\t//\n\t// then check(A) will encounter the types A, struct{ x []chan int }, []chan\n\t// int, chan int, and int. We associate each of these types with a\n\t// corresponding \"path\", a concise description of the relationship between\n\t// the root type and the nested types. For example, the type chan int has\n\t// path A.x[0].\n\t//\n\t// lineage is a stack that stores a history of these paths as check\n\t// traverses a type. For example, if we call check(A), then lineage will\n\t// look like this when the chan int is discovered:\n\t//\n\t//     []pathAndType{\n\t//         pathAndType{\"A\", A},\n\t//         pathAndType{\"A.x\", []chan int},\n\t//         pathAndType{\"A.x[0]\", chan int},\n\t//     }\n\t//\n\t// This lineage is printed in error messages as:\n\t//\n\t//     A (type A)\n\t//     A.x (type []chan int)\n\t//     A.x[0] (type chan int)\n\t//\n\t// Note that for brevity, not every encountered type is entered into the\n\t// lineage.\n\ttype pathAndType struct {\n\t\tpath string\n\t\tt    types.Type\n\t}\n\tvar lineage []pathAndType\n\n\tvar errors []error\n\taddError := func(err error) {\n\t\tvar builder strings.Builder\n\n\t\t// If the lineage is trivial, then don't show it.\n\t\tif len(lineage) > 1 {\n\t\t\tfmt.Fprintf(&builder, \"\\n    \")\n\t\t\tfor i, pn := range lineage {\n\t\t\t\tfmt.Fprintf(&builder, \"%v (type %v)\", pn.path, pn.t.String())\n\t\t\t\tif i < len(lineage)-1 {\n\t\t\t\t\tfmt.Fprintf(&builder, \"\\n    \")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tqualifier := func(pkg *types.Package) string { return pkg.Name() }\n\t\terr = fmt.Errorf(\"%s: %w%s\", types.TypeString(t, qualifier), err, builder.String())\n\t\terrors = append(errors, err)\n\t}\n\n\t// stack contains the set of types encountered in the call stack of check.\n\t// It's used to detect recursive types.\n\t//\n\t// More specifically, the check function below is performing an implicit\n\t// depth first search of the graph of types formed by t. We record the\n\t// stack of visited types in stack and know we have a recursive type if we\n\t// ever run into a type that is already in stack.\n\t//\n\t// For example, consider the following types:\n\t//\n\t//   type A struct { b: *B }\n\t//   type B struct { a: *A }\n\t//\n\t// Calling check on A will yield a call stack that looks something like:\n\t//\n\t//   check(A)\n\t//     check(struct { b: *B })\n\t//       check(*B)\n\t//         check(B)\n\t//           check(struct { a: *A })\n\t//             check(*A)\n\t//               check(A)\n\t//\n\t// When performing the second check(A) call, stack includes A, struct { b:\n\t// *B }, *B, B, struct { a: *A }, and *A. Because we called check on A and\n\t// A is already in stack, we detect a recursive type and mark A as not\n\t// serializable.\n\tvar stack typeutil.Map\n\n\t// check recursively checks whether a type t is serializable. See lineage\n\t// above for a description of path. record is true if the current type\n\t// should be recorded in lineage. There are a few things worth noting:\n\t//\n\t//   (1) The results of calling check are memoized in tset.checked, but not\n\t//       for some trivial arguments. Some arguments like chan int are not\n\t//       memoized because they are trivial to check and because not\n\t//       memoizing can lead to a clearer error message.\n\t//\n\t//   (2) Consider the type t = struct { x chan int; y chan int }. t is not\n\t//       serializable because neither x nor y is serializable. check\n\t//       reports errors for both x and y as not serializable.\n\t//       Alternatively, check could find that x is not serializable and\n\t//       then immediately report that t is not serializable, skipping y\n\t//       completely. check doesn't do this. check will inspect a type fully\n\t//       to report the full set of errors.\n\t//\n\t// Note that the function also takes the parent type pt. This is needed in cases\n\t// whether we need to know the type of the parent type t (e.g., a named type\n\t// that is a proto is serializable iff the parent type is a pointer).\n\tvar check func(t types.Type, path string, record bool) bool\n\n\tcheck = func(t types.Type, path string, record bool) bool {\n\t\tif record {\n\t\t\tlineage = append(lineage, pathAndType{path, t})\n\t\t\tdefer func() { lineage = lineage[:len(lineage)-1] }()\n\t\t}\n\n\t\t// Return early if we've already checked this type.\n\t\tif result := tset.checked.At(t); result != nil {\n\t\t\tb := result.(bool)\n\t\t\tif b {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\t// We've already encountered type t and determined that it is not\n\t\t\t// serializable. We won't recurse down type t to compute the full\n\t\t\t// lineage and explanation of why t isn't serializable because we\n\t\t\t// already did that when determining t wasn't serializable in the\n\t\t\t// first place. Instead, we instruct the user to read the\n\t\t\t// previously reported error.\n\t\t\taddError(fmt.Errorf(\"not a serializable type; see above for details\"))\n\t\t\treturn false\n\t\t}\n\n\t\t// Check for recursive types.\n\t\tif stack.At(t) != nil {\n\t\t\taddError(fmt.Errorf(\"serialization of recursive types not currently supported\"))\n\t\t\ttset.checked.Set(t, false)\n\t\t\treturn false\n\t\t}\n\t\tstack.Set(t, struct{}{})\n\t\tdefer func() { stack.Delete(t) }()\n\n\t\tswitch x := t.(type) {\n\t\tcase *types.Named:\n\t\t\t// No need to check if x is an unexported type from another package\n\t\t\t// since the Go compiler takes care of that.\n\n\t\t\t// Check if the type implements one of the marshaler interfaces.\n\t\t\tif tset.isProto(x) || tset.automarshals.At(t) != nil || tset.implementsAutoMarshal(x) || tset.hasMarshalBinary(x) {\n\t\t\t\ttset.checked.Set(t, true)\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is not a struct, then we simply recurse\n\t\t\t// on the underlying type.\n\t\t\ts, ok := x.Underlying().(*types.Struct)\n\t\t\tif !ok {\n\t\t\t\ttset.checked.Set(t, check(x.Underlying(), path, false))\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is a struct that has not been declared to\n\t\t\t// implement the AutoMarshal interface, then it is not\n\t\t\t// serializable.\n\t\t\tif tset.automarshalCandidates.At(t) == nil {\n\t\t\t\t// TODO(mwhittaker): Print out a link to documentation on\n\t\t\t\t// weaver.AutoMarshal.\n\t\t\t\taddError(fmt.Errorf(\"named structs are not serializable by default. Consider using weaver.AutoMarshal.\"))\n\t\t\t\ttset.checked.Set(t, false)\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is a struct that has been declared to\n\t\t\t// implement the AutoMarshal interface but hasn't yet been checked,\n\t\t\t// then we need to recurse to detect cycles.\n\t\t\tserializable := true\n\t\t\tfor i := 0; i < s.NumFields(); i++ {\n\t\t\t\tf := s.Field(i)\n\t\t\t\t// We store the result of calling check in b rather than\n\t\t\t\t// writing serializable = serializable && check(...) because we\n\t\t\t\t// don't want to short circuit and avoid calling check.\n\t\t\t\tb := check(f.Type(), path+\".\"+f.Name(), true)\n\t\t\t\tserializable = serializable && b\n\t\t\t}\n\t\t\ttset.checked.Set(t, serializable)\n\n\t\tcase *types.Interface:\n\t\t\t// TODO(sanjay): Support types.Interface only if we can figure out\n\t\t\t// a way to instantiate the type.\n\t\t\taddError(fmt.Errorf(\"serialization of interfaces not currently supported\"))\n\t\t\ttset.checked.Set(t, false)\n\n\t\tcase *types.Struct:\n\t\t\taddError(fmt.Errorf(\"struct literals are not serializable\"))\n\t\t\ttset.checked.Set(t, false)\n\n\t\tcase *types.Basic:\n\t\t\tswitch x.Kind() {\n\t\t\tcase types.Bool,\n\t\t\t\ttypes.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\t\ttypes.Float32, types.Float64,\n\t\t\t\ttypes.Complex64, types.Complex128,\n\t\t\t\ttypes.String:\n\t\t\t\t// Supported.\n\t\t\t\ttset.checked.Set(t, true)\n\t\t\tdefault:\n\t\t\t\tif isInvalid(t) {\n\t\t\t\t\taddError(fmt.Errorf(\"Maybe you forgot to run `go mod tidy`? Also try running `go build` to diagnose further.\"))\n\t\t\t\t} else {\n\t\t\t\t\taddError(fmt.Errorf(\"unsupported basic type\"))\n\t\t\t\t}\n\t\t\t\t// For a better error message, we don't memoize this.\n\t\t\t\treturn false\n\t\t\t}\n\n\t\tcase *types.Array:\n\t\t\ttset.checked.Set(t, check(x.Elem(), path+\"[0]\", true))\n\n\t\tcase *types.Slice:\n\t\t\ttset.checked.Set(t, check(x.Elem(), path+\"[0]\", true))\n\n\t\tcase *types.Pointer:\n\t\t\ttset.checked.Set(t, check(x.Elem(), \"(*\"+path+\")\", true))\n\n\t\tcase *types.Map:\n\t\t\tkeySerializable := check(x.Key(), path+\".key\", true)\n\t\t\tvalSerializable := check(x.Elem(), path+\".value\", true)\n\t\t\ttset.checked.Set(t, keySerializable && valSerializable)\n\n\t\tdefault:\n\t\t\taddError(fmt.Errorf(\"not a serializable type\"))\n\t\t\t// For a better error message, we don't memoize this.\n\t\t\treturn false\n\t\t}\n\n\t\treturn tset.checked.At(t).(bool)\n\t}\n\n\tcheck(t, t.String(), true)\n\treturn errors\n}\n\n// isFixedSizeType returns whether the provided type has a fixed serialization\n// size. Here is a summary of which types are fixed sized:\n//\n//   - Every basic type (e.g., bool, int) except string is fixed sized.\n//   - The array type [N]t is fixed sized if t is fixed sized.\n//   - A struct is fixed sized if the types of its fields are fixed sized.\n//   - A named type is fixed sized if its underlying type is fixed sized.\nfunc (tset *typeSet) isFixedSizeType(t types.Type) bool {\n\treturn tset.sizeOfType(t) >= 0\n}\n\n// sizeOfType returns the size of the serialization of t if t is fixed size, or\n// returns -1 otherwise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// isMeasurable returns whether the provided type is measurable.\n//\n// Informally, we say a type is measurable if we can cheaply compute the size\n// of its serialization at runtime. Some examples:\n//\n//   - Every fixed size type (e.g., int, bool, [3]int, struct{x, y int}) is\n//     measurable (with some restrictions on package locality; see below).\n//   - Strings are not fixed size, but they are measurable because we can\n//     cheaply compute the length of a string at runtime.\n//   - []string is not measurable because computing the size of the\n//     serialization of a []string would require us to compute the length of\n//     every string in the slice. This is a potentially expensive operation\n//     if the slice contains a large number of strings, so we consider\n//     []string to be not measurable.\n//   - For simplicity, we only consider a type measurable if the type and all\n//     its nested types are package local. For example, a struct { x\n//     otherpackage.T } is not measurable, even if otherpackage.T is\n//     measurable. We make an exception for weaver.AutoMarshal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// genTypeString returns the string representation of t as to be printed\n// in the generated code, updating import definitions to account for the\n// returned type string.\n//\n// Since this call has side-effects (i.e., updating import definitions), it\n// should only be called when the returned type string is written into\n// the generated file; otherwise, the generated code may end up with spurious\n// imports.\nfunc (tset *typeSet) genTypeString(t types.Type) string {\n\t// qualifier is passed to types.TypeString(Type, Qualifier) to determine\n\t// how packages are printed when pretty printing types. For this qualifier,\n\t// types in the root package are printed without their package name, while\n\t// types outside the root package are printed with their package name. For\n\t// example, if we're in root package foo, then the type foo.Bar is printed\n\t// as Bar, while the type io.Reader is printed as io.Reader. See [1] for\n\t// more information on qualifiers and pretty printing types.\n\t//\n\t// [1]: https://github.com/golang/example/tree/master/gotypes#formatting-support\n\tvar qualifier = func(pkg *types.Package) string {\n\t\tif pkg == tset.pkg.Types {\n\t\t\treturn \"\"\n\t\t}\n\t\treturn tset.importPackage(pkg.Path(), pkg.Name()).name()\n\t}\n\treturn types.TypeString(t, qualifier)\n}\n\n// isInvalid returns true iff the given type is invalid.\nfunc isInvalid(t types.Type) bool {\n\treturn t.String() == \"invalid type\"\n}\n\n// implementsError returns whether the provided type is a concrete type that\n// implements error.\nfunc (tset *typeSet) implementsError(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\treturn false\n\t}\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"Error\")\n\tmethod, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tsig, ok := method.Type().(*types.Signature)\n\tif !ok {\n\t\treturn false\n\t}\n\tif args := sig.Params(); args.Len() != 0 {\n\t\treturn false\n\t}\n\tif results := sig.Results(); results.Len() != 1 || !isString(results.At(0).Type()) {\n\t\treturn false\n\t}\n\treturn true\n}\n\n// isProto returns whether the provided type is a concrete type that implements\n// the proto.Message interface.\nfunc (tset *typeSet) isProto(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\t// A superinterface of proto.Message does \"implement\" proto.Message,\n\t\t// but we only accept concrete types that implement the interface.\n\t\treturn false\n\t}\n\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"ProtoReflect\")\n\tmethod, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tsig, ok := method.Type().(*types.Signature)\n\tif !ok {\n\t\treturn false\n\t}\n\trecv, args, results := sig.Recv(), sig.Params(), sig.Results()\n\tif args.Len() != 0 || results.Len() != 1 {\n\t\treturn false\n\t}\n\tif !isProtoMessage(results.At(0).Type()) {\n\t\treturn false\n\t}\n\t// Check the receiver. We avoid complicated embeddings by requiring that\n\t// the method is defined on the type itself.\n\t//\n\t// TODO(mwhittaker): Relax this requirement if it becomes annoying.\n\tif p, ok := recv.Type().(*types.Pointer); ok {\n\t\treturn types.Identical(p.Elem(), t)\n\t} else {\n\t\treturn types.Identical(recv.Type(), t)\n\t}\n}\n\nfunc isProtoMessage(t types.Type) bool {\n\tn, ok := t.(*types.Named)\n\tif !ok {\n\t\treturn false\n\t}\n\tconst protoreflect = \"google.golang.org/protobuf/reflect/protoreflect\"\n\treturn n.Obj().Pkg().Path() == protoreflect && n.Obj().Name() == \"Message\"\n}\n\n// implementsAutoMarshal returns whether the provided type is a concrete\n// type that implements the weaver.AutoMarshal interface.\nfunc (tset *typeSet) implementsAutoMarshal(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\t// A superinterface of AutoMarshal does \"implement\" the interface,\n\t\t// but we only accept concrete types that implement the interface.\n\t\treturn false\n\t}\n\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"WeaverMarshal\")\n\tmarshal, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tobj, _, _ = types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"WeaverUnmarshal\")\n\tunmarshal, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn is\n```\n# Function Signature: func (tset *typeSet) sizeOfType(t types.Type) int {\n# Natural Language Annotations: // sizeOfType returns the size of the serialization of t if t is fixed size, or\n// returns -1 otherwise.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (tset *typeSet) sizeOfType(t types.Type) int {\n\t// If the type is not a fixed size, return -1\n\tif !tset.isFixedSizeType(t) {\n\t\treturn -1\n\t}\n\n\t// Initialize the size as 0\n\tvar size int\n\n\t// Recursively calculate the size of the type\n\tswitch x := t.(type) {\n\tcase *types.Named:\n\t\t// If the underlying type is a struct, calculate the size of the struct\n\t\tif s, ok := x.Underlying().(*types.Struct); ok {\n\t\t\tfor i := 0; i < s.NumFields(); i++ {\n\t\t\t\tf := s.Field(i)\n\t\t\t\tsize += tset.sizeOfType(f.Type())\n\t\t\t}\n\t\t} else {\n\t\t\t// Otherwise, calculate the size of the underlying type\n\t\t\tsize += tset.sizeOfType(x.Underlying())\n\t\t}\n\n\tcase *types.Basic:\n\t\t// If the basic type is a string, return -1 as the size of a string is not fixed\n\t\tif x.Kind() == types.String {\n\t\t\treturn -1\n\t\t}\n\t\t// Otherwise, calculate the size of the basic type\n\t\tsize = x.Size()\n\n\tcase *types.Array:\n\t\t// Calculate the size of the array\n\t\tsize += tset.sizeOfType(x.Elem()) * int(x.Len())\n\n\tcase *types.Slice:\n\t\t// Slices do not have a fixed size, so return -1\n\t\treturn -1\n\n\tcase *types.Pointer:\n\t\t// Pointers do not have a fixed size, so return -1\n\t\treturn -1\n\n\tcase *types.Map:\n\t\t// Maps do not have a fixed size, so return -1\n\t\treturn -1\n\n\tdefault:\n\t\t// If the type is not a basic type, a struct, an array, a slice, a pointer, or a map, return -1\n\t\treturn -1\n\t}\n\n\treturn size\n}\n```", "```go\nfunc (tset *typeSet) sizeOfType(t types.Type) int {\n\t// If we've already computed the size of type t, then return it.\n\tif result := tset.sizes.At(t); result != nil {\n\t\treturn result.(int)\n\t}\n\n\tswitch x := t.(type) {\n\tcase *types.Named:\n\t\t// If the type implements the AutoMarshal interface, then its size is\n\t\t// determined by the size of the underlying type.\n\t\tif tset.implementsAutoMarshal(x) {\n\t\t\ttset.sizes.Set(t, tset.sizeOfType(x.Underlying()))\n\t\t\treturn tset.sizes.At(t).(int)\n\t\t}\n\t\t// If the type is a proto.Message, then its size is unknown.\n\t\tif tset.isProto(x) {\n\t\t\ttset.sizes.Set(t, -1)\n\t\t\treturn -1\n\t\t}\n\t\t// Otherwise, recurse on the underlying type.\n\t\ttset.sizes.Set(t, tset.sizeOfType(x.Underlying()))\n\t\treturn tset.sizes.At(t).(int)\n\n\tcase *types.Basic:\n\t\tswitch x.Kind() {\n\t\tcase types.Bool,\n\t\t\ttypes.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\ttypes.Float32, types.Float64,\n\t\t\ttypes.Complex64, types.Complex128:\n\t\t\t// Supported.\n\t\t\ttset.sizes.Set(t, x.Size())\n\t\t\treturn x.Size()\n\t\tdefault:\n\t\t\t// Unsupported.\n\t\t\ttset.sizes.Set(t, -1)\n\t\t\treturn -1\n\t\t}\n\n\tcase *types.Array:\n\t\telemSize := tset.sizeOfType(x.Elem())\n\t\tif elemSize < 0 {\n\t\t\ttset.sizes.Set(t, -1)\n\t\t\treturn -1\n\t\t}\n\t\ttset.sizes.Set(t, int(x.Len())*elemSize)\n\t\treturn tset.sizes.At(t).(int)\n\n\tcase *types.Slice:\n\t\t// Slice size is unknown.\n\t\ttset.sizes.Set(t, -1)\n\t\treturn -1\n\n\tcase *types.Pointer:\n\t\t// Pointer size is unknown.\n\t\ttset.sizes.Set(t, -1)\n\t\treturn -1\n\n\tcase *types.Map:\n\t\t// Map size is unknown.\n\t\ttset.sizes.Set(t, -1)\n\t\treturn -1\n\n\tcase *types.Struct:\n\t\ttset.sizes.Set(t, sizeOfStruct(x))\n\t\treturn tset.sizes.At(t).(int)\n\n\tdefault:\n\t\t// Unsupported.\n\t\ttset.sizes.Set(t, -1)\n\t\treturn -1\n\t}\n}\n\n// sizeOfStruct returns the size of the serialization of a struct type.\nfunc sizeOfStruct(s *types.Struct) int {\n\tvar size int\n\tfor i := 0; i < s.NumFields(); i++ {\n\t\tf := s.Field(i)\n\t\tsize += tset.sizeOfType(f.Type())\n\t}\n\treturn size\n}\n```", "```go\nfunc (tset *typeSet) sizeOfType(t types.Type) int {\n\t// sizeOfType is memoized in tset.sizes.\n\tif result := tset.sizes.At(t); result != nil {\n\t\tsize := result.(int)\n\t\tif size >= 0 {\n\t\t\treturn size\n\t\t}\n\t\t// We've already encountered type t and determined that it has\n\t\t// a variable size. We won't recurse down type t to compute the full\n\t\t// lineage and explanation of why t has a variable size because we\n\t\t// already did that when determining t had a variable size in the\n\t\t// first place. Instead, we instruct the user to read the\n\t\t// previously reported error.\n\t\treturn -1\n\t}\n\n\t// Check for recursive types.\n\tif tset.checked.At(t) != nil {\n\t\t// We've already encountered type t and determined that it has\n\t\t// a variable size. We won't recurse down type t to compute the full\n\t\t// lineage and explanation of why t has a variable size because we\n\t\t// already did that when determining t had a variable size in the\n\t\t// first place. Instead, we instruct the user to read the\n\t\t// previously reported error.\n\t\treturn -1\n\t}\n\n\tswitch x := t.(type) {\n\tcase *types.Named:\n\t\t// No need to check if x is an unexported type from another package\n\t\t// since the Go compiler takes care of that.\n\n\t\t// Check if the type implements one of the marshaler interfaces.\n\t\tif tset.isProto(x) || tset.automarshals.At(t) != nil || tset.implementsAutoMarshal(x) || tset.hasMarshalBinary(x) {\n\t\t\ttset.sizes.Set(t, -1)\n\t\t\tbreak\n\t\t}\n\n\t\t// If the underlying type is not a struct, then we simply recurse\n\t\t// on the underlying type.\n\t\ts, ok := x.Underlying().(*types.Struct)\n\t\tif !ok {\n\t\t\ttset.sizes.Set(t, tset.sizeOfType(x.Underlying()))\n\t\t\tbreak\n\t\t}\n\n\t\t// If the underlying type is a struct that has not been declared to\n\t\t// implement the AutoMarshal interface, then it is not\n\t\t// serializable.\n\t\tif tset.automarshalCandidates.At(t) == nil {\n\t\t\t// TODO(mwhittaker): Print out a link to documentation on\n\t\t\t// weaver.AutoMarshal.\n\t\t\ttset.sizes.Set(t, -1)\n\t\t\tbreak\n\t\t}\n\n\t\t// If the underlying type is a struct that has been declared to\n\t\t// implement the AutoMarshal interface but hasn't yet been checked,\n\t\t// then we need to recurse to detect cycles.\n\t\tsize := 0\n\t\tfor i := 0; i < s.NumFields(); i++ {\n\t\t\tf := s.Field(i)\n\t\t\t// We store the result of calling sizeOfType in size rather than\n\t\t\t// writing size = size + sizeOfType because we\n\t\t\t// don't want to short circuit and avoid calling sizeOfType.\n\t\t\tfieldSize := tset.sizeOfType(f.Type())\n\t\t\tif fieldSize < 0 {\n\t\t\t\tsize = -1\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tsize += fieldSize\n\t\t}\n\t\ttset.sizes.Set(t, size)\n\n\tcase *types.Interface:\n\t\t// TODO(sanjay): Support types.Interface only if we can figure out\n\t\t// a way to instantiate the type.\n\t\ttset.sizes.Set(t, -1)\n\t\tbreak\n\n\tcase *types.Struct:\n\t\ttset.sizes.Set(t, -1)\n\t\tbreak\n\n\tcase *types.Basic:\n\t\tswitch x.Kind() {\n\t\tcase types.Bool,\n\t\t\ttypes.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\ttypes.Float32, types.Float64,\n\t\t\ttypes.Complex64, types.Complex128,\n\t\t\ttypes.String:\n\t\t\t// Supported.\n\t\t\ttset.sizes.Set(t, -1)\n\t\t\tbreak\n\t\tdefault:\n\t\t\tif isInvalid(t) {\n\t\t\t\ttset.sizes.Set(t, -1)\n\t\t\t} else {\n\t\t\t\ttset.sizes.Set(t, 0)\n\t\t\t}\n\t\t\t// For a better error message, we don't memoize this.\n\t\t\treturn 0\n\t\t}\n\n\tcase *types.Array:\n\t\telemSize := tset.sizeOfType(x.Elem())\n\t\tif elemSize < 0 {\n\t\t\ttset.sizes.Set(t, -1)\n\t\t} else {\n\t\t\ttset.sizes.Set(t, int(x.Len())*elemSize)\n\t\t}\n\n\tcase *types.Slice:\n\t\ttset.sizes.Set(t, -1)\n\n\tcase *types.Pointer:\n\t\ttset.sizes.Set(t, tset.sizeOfType(x.Elem()))\n\n\tcase *types.Map:\n\t\tkeySize := tset.sizeOfType(x.Key())\n\t\tvalSize := tset.sizeOfType(x.Elem())\n\t\tif keySize < 0 || valSize < 0 {\n\t\t\ttset.sizes.Set(t, -1)\n\t\t} else {\n\t\t\ttset.sizes.Set(t, keySize+valSize)\n\t\t}\n\n\tdefault:\n\t\ttset.sizes.Set(t, -1)\n\t\tbreak\n\t}\n\n\treturn tset.sizes.At(t).(int)\n}\n```"]}, "weaver-internal/tool/generate/types.go-isMeasurable": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/tool/generate/types.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generate\n\nimport (\n\t\"fmt\"\n\t\"go/types\"\n\t\"path\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"golang.org/x/tools/go/packages\"\n\t\"golang.org/x/tools/go/types/typeutil\"\n)\n\nconst weaverPackagePath = \"github.com/ServiceWeaver/weaver\"\n\n// typeSet holds type information needed by the code generator.\ntype typeSet struct {\n\tpkg            *packages.Package\n\timported       []importPkg          // imported packages\n\timportedByPath map[string]importPkg // imported, indexed by path\n\timportedByName map[string]importPkg // imported, indexed by name\n\n\tautomarshals          *typeutil.Map // types that implement AutoMarshal\n\tautomarshalCandidates *typeutil.Map // types that declare themselves AutoMarshal\n\n\t// If checked[t] != nil, then checked[t] is the cached result of calling\n\t// check(pkg, t, string[]{}). Otherwise, if checked[t] == nil, then t has\n\t// not yet been checked for serializability. Read typeutil.Map's\n\t// documentation for why checked shouldn't be a map[types.Type]bool.\n\tchecked typeutil.Map\n\n\t// If sizes[t] != nil, then sizes[t] == sizeOfType(t).\n\tsizes typeutil.Map\n\n\t// If measurable[t] != nil, then measurable[t] == isMeasurableType(t).\n\tmeasurable typeutil.Map\n}\n\n// importPkg is a package imported by the generated code.\ntype importPkg struct {\n\tpath  string // e.g., \"github.com/ServiceWeaver/weaver\"\n\tpkg   string // e.g., \"weaver\", \"context\", \"time\"\n\talias string // e.g., foo in `import foo \"context\"`\n\tlocal bool   // are we in this package?\n}\n\n// name returns the name by which the imported package should be referenced in\n// the generated code. If the package is imported without an alias, like this:\n//\n//\timport \"context\"\n//\n// then the name is the same as the package name (e.g., \"context\"). However, if\n// a package is imported with an alias, then the name is the alias:\n//\n//\timport thisIsAnAlias \"context\"\n//\n// If the package is local, an empty string is returned.\nfunc (i importPkg) name() string {\n\tif i.local {\n\t\treturn \"\"\n\t} else if i.alias != \"\" {\n\t\treturn i.alias\n\t}\n\treturn i.pkg\n}\n\n// qualify returns the provided member of the package, qualified with the\n// package name. For example, the \"Context\" type inside the \"context\" package\n// is qualified \"context.Context\". The \"Now\" function inside the \"time\" package\n// is qualified \"time.Now\". Note that the package name is not prefixed when\n// qualifying members of the local package.\nfunc (i importPkg) qualify(member string) string {\n\tif i.local {\n\t\treturn member\n\t}\n\treturn fmt.Sprintf(\"%s.%s\", i.name(), member)\n}\n\n// newTypeSet returns the container for types found in pkg.\nfunc newTypeSet(pkg *packages.Package, automarshals, automarshalCandidates *typeutil.Map) *typeSet {\n\treturn &typeSet{\n\t\tpkg:                   pkg,\n\t\timported:              []importPkg{},\n\t\timportedByPath:        map[string]importPkg{},\n\t\timportedByName:        map[string]importPkg{},\n\t\tautomarshals:          automarshals,\n\t\tautomarshalCandidates: automarshalCandidates,\n\t}\n}\n\n// importPackage imports a package with the provided path and package name. The\n// package is imported with an alias if there is a package name clash.\nfunc (tset *typeSet) importPackage(path, pkg string) importPkg {\n\tnewImportPkg := func(path, pkg, alias string, local bool) importPkg {\n\t\ti := importPkg{path: path, pkg: pkg, alias: alias, local: local}\n\t\ttset.imported = append(tset.imported, i)\n\t\ttset.importedByPath[i.path] = i\n\t\ttset.importedByName[i.name()] = i\n\t\treturn i\n\t}\n\n\tif imp, ok := tset.importedByPath[path]; ok {\n\t\t// This package has already been imported.\n\t\treturn imp\n\t}\n\n\tif _, ok := tset.importedByName[pkg]; !ok {\n\t\t// Import the package without an alias.\n\t\treturn newImportPkg(path, pkg, \"\", path == tset.pkg.PkgPath)\n\t}\n\n\t// Find an unused alias.\n\tvar alias string\n\tcounter := 1\n\tfor {\n\t\talias = fmt.Sprintf(\"%s%d\", pkg, counter)\n\t\tif _, ok := tset.importedByName[alias]; !ok {\n\t\t\tbreak\n\t\t}\n\t\tcounter++\n\t}\n\treturn newImportPkg(path, pkg, alias, path == tset.pkg.PkgPath)\n}\n\n// imports returns the list of packages to import in generated code.\nfunc (tset *typeSet) imports() []importPkg {\n\tsort.Slice(tset.imported, func(i, j int) bool {\n\t\treturn tset.imported[i].path < tset.imported[j].path\n\t})\n\treturn tset.imported\n}\n\n// checkSerializable checks that type t is serializable.\nfunc (tset *typeSet) checkSerializable(t types.Type) []error {\n\t// lineage can generate a human readable description of the lineage of a\n\t// checked type. As check recurses on type t, it encounters a number of\n\t// nested types. For example, if we have the following type A\n\t//\n\t//     type A struct{ x []chan int }\n\t//\n\t// then check(A) will encounter the types A, struct{ x []chan int }, []chan\n\t// int, chan int, and int. We associate each of these types with a\n\t// corresponding \"path\", a concise description of the relationship between\n\t// the root type and the nested types. For example, the type chan int has\n\t// path A.x[0].\n\t//\n\t// lineage is a stack that stores a history of these paths as check\n\t// traverses a type. For example, if we call check(A), then lineage will\n\t// look like this when the chan int is discovered:\n\t//\n\t//     []pathAndType{\n\t//         pathAndType{\"A\", A},\n\t//         pathAndType{\"A.x\", []chan int},\n\t//         pathAndType{\"A.x[0]\", chan int},\n\t//     }\n\t//\n\t// This lineage is printed in error messages as:\n\t//\n\t//     A (type A)\n\t//     A.x (type []chan int)\n\t//     A.x[0] (type chan int)\n\t//\n\t// Note that for brevity, not every encountered type is entered into the\n\t// lineage.\n\ttype pathAndType struct {\n\t\tpath string\n\t\tt    types.Type\n\t}\n\tvar lineage []pathAndType\n\n\tvar errors []error\n\taddError := func(err error) {\n\t\tvar builder strings.Builder\n\n\t\t// If the lineage is trivial, then don't show it.\n\t\tif len(lineage) > 1 {\n\t\t\tfmt.Fprintf(&builder, \"\\n    \")\n\t\t\tfor i, pn := range lineage {\n\t\t\t\tfmt.Fprintf(&builder, \"%v (type %v)\", pn.path, pn.t.String())\n\t\t\t\tif i < len(lineage)-1 {\n\t\t\t\t\tfmt.Fprintf(&builder, \"\\n    \")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tqualifier := func(pkg *types.Package) string { return pkg.Name() }\n\t\terr = fmt.Errorf(\"%s: %w%s\", types.TypeString(t, qualifier), err, builder.String())\n\t\terrors = append(errors, err)\n\t}\n\n\t// stack contains the set of types encountered in the call stack of check.\n\t// It's used to detect recursive types.\n\t//\n\t// More specifically, the check function below is performing an implicit\n\t// depth first search of the graph of types formed by t. We record the\n\t// stack of visited types in stack and know we have a recursive type if we\n\t// ever run into a type that is already in stack.\n\t//\n\t// For example, consider the following types:\n\t//\n\t//   type A struct { b: *B }\n\t//   type B struct { a: *A }\n\t//\n\t// Calling check on A will yield a call stack that looks something like:\n\t//\n\t//   check(A)\n\t//     check(struct { b: *B })\n\t//       check(*B)\n\t//         check(B)\n\t//           check(struct { a: *A })\n\t//             check(*A)\n\t//               check(A)\n\t//\n\t// When performing the second check(A) call, stack includes A, struct { b:\n\t// *B }, *B, B, struct { a: *A }, and *A. Because we called check on A and\n\t// A is already in stack, we detect a recursive type and mark A as not\n\t// serializable.\n\tvar stack typeutil.Map\n\n\t// check recursively checks whether a type t is serializable. See lineage\n\t// above for a description of path. record is true if the current type\n\t// should be recorded in lineage. There are a few things worth noting:\n\t//\n\t//   (1) The results of calling check are memoized in tset.checked, but not\n\t//       for some trivial arguments. Some arguments like chan int are not\n\t//       memoized because they are trivial to check and because not\n\t//       memoizing can lead to a clearer error message.\n\t//\n\t//   (2) Consider the type t = struct { x chan int; y chan int }. t is not\n\t//       serializable because neither x nor y is serializable. check\n\t//       reports errors for both x and y as not serializable.\n\t//       Alternatively, check could find that x is not serializable and\n\t//       then immediately report that t is not serializable, skipping y\n\t//       completely. check doesn't do this. check will inspect a type fully\n\t//       to report the full set of errors.\n\t//\n\t// Note that the function also takes the parent type pt. This is needed in cases\n\t// whether we need to know the type of the parent type t (e.g., a named type\n\t// that is a proto is serializable iff the parent type is a pointer).\n\tvar check func(t types.Type, path string, record bool) bool\n\n\tcheck = func(t types.Type, path string, record bool) bool {\n\t\tif record {\n\t\t\tlineage = append(lineage, pathAndType{path, t})\n\t\t\tdefer func() { lineage = lineage[:len(lineage)-1] }()\n\t\t}\n\n\t\t// Return early if we've already checked this type.\n\t\tif result := tset.checked.At(t); result != nil {\n\t\t\tb := result.(bool)\n\t\t\tif b {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\t// We've already encountered type t and determined that it is not\n\t\t\t// serializable. We won't recurse down type t to compute the full\n\t\t\t// lineage and explanation of why t isn't serializable because we\n\t\t\t// already did that when determining t wasn't serializable in the\n\t\t\t// first place. Instead, we instruct the user to read the\n\t\t\t// previously reported error.\n\t\t\taddError(fmt.Errorf(\"not a serializable type; see above for details\"))\n\t\t\treturn false\n\t\t}\n\n\t\t// Check for recursive types.\n\t\tif stack.At(t) != nil {\n\t\t\taddError(fmt.Errorf(\"serialization of recursive types not currently supported\"))\n\t\t\ttset.checked.Set(t, false)\n\t\t\treturn false\n\t\t}\n\t\tstack.Set(t, struct{}{})\n\t\tdefer func() { stack.Delete(t) }()\n\n\t\tswitch x := t.(type) {\n\t\tcase *types.Named:\n\t\t\t// No need to check if x is an unexported type from another package\n\t\t\t// since the Go compiler takes care of that.\n\n\t\t\t// Check if the type implements one of the marshaler interfaces.\n\t\t\tif tset.isProto(x) || tset.automarshals.At(t) != nil || tset.implementsAutoMarshal(x) || tset.hasMarshalBinary(x) {\n\t\t\t\ttset.checked.Set(t, true)\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is not a struct, then we simply recurse\n\t\t\t// on the underlying type.\n\t\t\ts, ok := x.Underlying().(*types.Struct)\n\t\t\tif !ok {\n\t\t\t\ttset.checked.Set(t, check(x.Underlying(), path, false))\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is a struct that has not been declared to\n\t\t\t// implement the AutoMarshal interface, then it is not\n\t\t\t// serializable.\n\t\t\tif tset.automarshalCandidates.At(t) == nil {\n\t\t\t\t// TODO(mwhittaker): Print out a link to documentation on\n\t\t\t\t// weaver.AutoMarshal.\n\t\t\t\taddError(fmt.Errorf(\"named structs are not serializable by default. Consider using weaver.AutoMarshal.\"))\n\t\t\t\ttset.checked.Set(t, false)\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is a struct that has been declared to\n\t\t\t// implement the AutoMarshal interface but hasn't yet been checked,\n\t\t\t// then we need to recurse to detect cycles.\n\t\t\tserializable := true\n\t\t\tfor i := 0; i < s.NumFields(); i++ {\n\t\t\t\tf := s.Field(i)\n\t\t\t\t// We store the result of calling check in b rather than\n\t\t\t\t// writing serializable = serializable && check(...) because we\n\t\t\t\t// don't want to short circuit and avoid calling check.\n\t\t\t\tb := check(f.Type(), path+\".\"+f.Name(), true)\n\t\t\t\tserializable = serializable && b\n\t\t\t}\n\t\t\ttset.checked.Set(t, serializable)\n\n\t\tcase *types.Interface:\n\t\t\t// TODO(sanjay): Support types.Interface only if we can figure out\n\t\t\t// a way to instantiate the type.\n\t\t\taddError(fmt.Errorf(\"serialization of interfaces not currently supported\"))\n\t\t\ttset.checked.Set(t, false)\n\n\t\tcase *types.Struct:\n\t\t\taddError(fmt.Errorf(\"struct literals are not serializable\"))\n\t\t\ttset.checked.Set(t, false)\n\n\t\tcase *types.Basic:\n\t\t\tswitch x.Kind() {\n\t\t\tcase types.Bool,\n\t\t\t\ttypes.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\t\ttypes.Float32, types.Float64,\n\t\t\t\ttypes.Complex64, types.Complex128,\n\t\t\t\ttypes.String:\n\t\t\t\t// Supported.\n\t\t\t\ttset.checked.Set(t, true)\n\t\t\tdefault:\n\t\t\t\tif isInvalid(t) {\n\t\t\t\t\taddError(fmt.Errorf(\"Maybe you forgot to run `go mod tidy`? Also try running `go build` to diagnose further.\"))\n\t\t\t\t} else {\n\t\t\t\t\taddError(fmt.Errorf(\"unsupported basic type\"))\n\t\t\t\t}\n\t\t\t\t// For a better error message, we don't memoize this.\n\t\t\t\treturn false\n\t\t\t}\n\n\t\tcase *types.Array:\n\t\t\ttset.checked.Set(t, check(x.Elem(), path+\"[0]\", true))\n\n\t\tcase *types.Slice:\n\t\t\ttset.checked.Set(t, check(x.Elem(), path+\"[0]\", true))\n\n\t\tcase *types.Pointer:\n\t\t\ttset.checked.Set(t, check(x.Elem(), \"(*\"+path+\")\", true))\n\n\t\tcase *types.Map:\n\t\t\tkeySerializable := check(x.Key(), path+\".key\", true)\n\t\t\tvalSerializable := check(x.Elem(), path+\".value\", true)\n\t\t\ttset.checked.Set(t, keySerializable && valSerializable)\n\n\t\tdefault:\n\t\t\taddError(fmt.Errorf(\"not a serializable type\"))\n\t\t\t// For a better error message, we don't memoize this.\n\t\t\treturn false\n\t\t}\n\n\t\treturn tset.checked.At(t).(bool)\n\t}\n\n\tcheck(t, t.String(), true)\n\treturn errors\n}\n\n// isFixedSizeType returns whether the provided type has a fixed serialization\n// size. Here is a summary of which types are fixed sized:\n//\n//   - Every basic type (e.g., bool, int) except string is fixed sized.\n//   - The array type [N]t is fixed sized if t is fixed sized.\n//   - A struct is fixed sized if the types of its fields are fixed sized.\n//   - A named type is fixed sized if its underlying type is fixed sized.\nfunc (tset *typeSet) isFixedSizeType(t types.Type) bool {\n\treturn tset.sizeOfType(t) >= 0\n}\n\n// sizeOfType returns the size of the serialization of t if t is fixed size, or\n// returns -1 otherwise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// isMeasurable returns whether the provided type is measurable.\n//\n// Informally, we say a type is measurable if we can cheaply compute the size\n// of its serialization at runtime. Some examples:\n//\n//   - Every fixed size type (e.g., int, bool, [3]int, struct{x, y int}) is\n//     measurable (with some restrictions on package locality; see below).\n//   - Strings are not fixed size, but they are measurable because we can\n//     cheaply compute the length of a string at runtime.\n//   - []string is not measurable because computing the size of the\n//     serialization of a []string would require us to compute the length of\n//     every string in the slice. This is a potentially expensive operation\n//     if the slice contains a large number of strings, so we consider\n//     []string to be not measurable.\n//   - For simplicity, we only consider a type measurable if the type and all\n//     its nested types are package local. For example, a struct { x\n//     otherpackage.T } is not measurable, even if otherpackage.T is\n//     measurable. We make an exception for weaver.AutoMarshal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// genTypeString returns the string representation of t as to be printed\n// in the generated code, updating import definitions to account for the\n// returned type string.\n//\n// Since this call has side-effects (i.e., updating import definitions), it\n// should only be called when the returned type string is written into\n// the generated file; otherwise, the generated code may end up with spurious\n// imports.\nfunc (tset *typeSet) genTypeString(t types.Type) string {\n\t// qualifier is passed to types.TypeString(Type, Qualifier) to determine\n\t// how packages are printed when pretty printing types. For this qualifier,\n\t// types in the root package are printed without their package name, while\n\t// types outside the root package are printed with their package name. For\n\t// example, if we're in root package foo, then the type foo.Bar is printed\n\t// as Bar, while the type io.Reader is printed as io.Reader. See [1] for\n\t// more information on qualifiers and pretty printing types.\n\t//\n\t// [1]: https://github.com/golang/example/tree/master/gotypes#formatting-support\n\tvar qualifier = func(pkg *types.Package) string {\n\t\tif pkg == tset.pkg.Types {\n\t\t\treturn \"\"\n\t\t}\n\t\treturn tset.importPackage(pkg.Path(), pkg.Name()).name()\n\t}\n\treturn types.TypeString(t, qualifier)\n}\n\n// isInvalid returns true iff the given type is invalid.\nfunc isInvalid(t types.Type) bool {\n\treturn t.String() == \"invalid type\"\n}\n\n// implementsError returns whether the provided type is a concrete type that\n// implements error.\nfunc (tset *typeSet) implementsError(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\treturn false\n\t}\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"Error\")\n\tmethod, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tsig, ok := method.Type().(*types.Signature)\n\tif !ok {\n\t\treturn false\n\t}\n\tif args := sig.Params(); args.Len() != 0 {\n\t\treturn false\n\t}\n\tif results := sig.Results(); results.Len() != 1 || !isString(results.At(0).Type()) {\n\t\treturn false\n\t}\n\treturn true\n}\n\n// isProto returns whether the provided type is a concrete type that implements\n// the proto.Message interface.\nfunc (tset *typeSet) isProto(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\t// A superinterface of proto.Message does \"implement\" proto.Message,\n\t\t// but we only accept concrete types that implement the interface.\n\t\treturn false\n\t}\n\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"ProtoReflect\")\n\tmethod, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tsig, ok := method.Type().(*types.Signature)\n\tif !ok {\n\t\treturn false\n\t}\n\trecv, args, results := sig.Recv(), sig.Params(), sig.Results()\n\tif args.Len() != 0 || results.Len() != 1 {\n\t\treturn false\n\t}\n\tif !isProtoMessage(results.At(0).Type()) {\n\t\treturn false\n\t}\n\t// Check the receiver. We avoid complicated embeddings by requiring that\n\t// the method is defined on the type itself.\n\t//\n\t// TODO(mwhittaker): Relax this requirement if it becomes annoying.\n\tif p, ok := recv.Type().(*types.Pointer); ok {\n\t\treturn types.Identical(p.Elem(), t)\n\t} else {\n\t\treturn types.Identical(recv.Type(), t)\n\t}\n}\n\nfunc isProtoMessage(t types.Type) bool {\n\tn, ok := t.(*types.Named)\n\tif !ok {\n\t\treturn false\n\t}\n\tconst protoreflect = \"google.golang.org/protobuf/reflect/protoreflect\"\n\treturn n.Obj().Pkg().Path() == protoreflect && n.Obj().Name() == \"Message\"\n}\n\n// implementsAutoMarshal returns whether the provided type is a concrete\n// type that implements the weaver.AutoMarshal interface.\nfunc (tset *typeSet) implementsAutoMarshal(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\t// A superinterface of AutoMarshal does \"implement\" the interface,\n\t\t// but we only accept concrete types that implement the interface.\n\t\treturn false\n\t}\n\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"WeaverMarshal\")\n\tmarshal, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tobj, _, _ = types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"WeaverUnmarshal\")\n\tunmarshal, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn is\n```\n# Function Signature: func (tset *typeSet) isMeasurable(t types.Type) bool {\n# Natural Language Annotations: // isMeasurable returns whether the provided type is measurable.\n//\n// Informally, we say a type is measurable if we can cheaply compute the size\n// of its serialization at runtime. Some examples:\n//\n//   - Every fixed size type (e.g., int, bool, [3]int, struct{x, y int}) is\n//     measurable (with some restrictions on package locality; see below).\n//   - Strings are not fixed size, but they are measurable because we can\n//     cheaply compute the length of a string at runtime.\n//   - []string is not measurable because computing the size of the\n//     serialization of a []string would require us to compute the length of\n//     every string in the slice. This is a potentially expensive operation\n//     if the slice contains a large number of strings, so we consider\n//     []string to be not measurable.\n//   - For simplicity, we only consider a type measurable if the type and all\n//     its nested types are package local. For example, a struct { x\n//     otherpackage.T } is not measurable, even if otherpackage.T is\n//     measurable. We make an exception for weaver.AutoMarshal.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (tset *typeSet) isMeasurable(t types.Type) bool {\n\tswitch x := t.Underlying().(type) {\n\tcase *types.Basic:\n\t\tswitch x.Kind() {\n\t\tcase types.Bool,\n\t\t\ttypes.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\ttypes.Float32, types.Float64,\n\t\t\ttypes.Complex64, types.Complex128:\n\t\t\treturn true\n\t\tdefault:\n\t\t\treturn false\n\t\t}\n\tcase *types.Array:\n\t\treturn tset.isMeasurable(x.Elem())\n\tcase *types.Slice:\n\t\treturn false\n\tcase *types.Struct:\n\t\tfor i := 0; i < x.NumFields(); i++ {\n\t\t\tif !tset.isMeasurable(x.Field(i).Type()) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\treturn true\n\tcase *types.Pointer:\n\t\treturn tset.isMeasurable(x.Elem())\n\tcase *types.Map:\n\t\treturn tset.isMeasurable(x.Key()) && tset.isMeasurable(x.Elem())\n\tcase *types.Named:\n\t\t// Check if the type implements the weaver.AutoMarshal interface.\n\t\tif tset.implementsAutoMarshal(x) {\n\t\t\treturn true\n\t\t}\n\t\t// Check if type is package local.\n\t\tif tset.isPackageLocal(x) {\n\t\t\treturn tset.isMeasurable(x.Underlying())\n\t\t}\n\t\treturn false\n\tdefault:\n\t\treturn false\n\t}\n}\n```", "```go\nfunc (tset *typeSet) isMeasurable(t types.Type) bool {\n\t// Check if type t is a basic type\n\tif basic, ok := t.(*types.Basic); ok {\n\t\tswitch basic.Kind() {\n\t\tcase types.Bool,\n\t\t\ttypes.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\ttypes.Float32, types.Float64,\n\t\t\ttypes.Complex64, types.Complex128:\n\t\t\treturn true\n\t\tdefault:\n\t\t\treturn false\n\t\t}\n\t}\n\n\t// Check if type t is a named type\n\tif named, ok := t.(*types.Named); ok {\n\t\t// Check whether the named type is package local\n\t\tif named.Obj().Pkg().Path() != tset.pkg.PkgPath {\n\t\t\treturn false\n\t\t}\n\n\t\t// Check if the underlying type of the named type is measurable\n\t\treturn tset.isMeasurable(named.Underlying())\n\t}\n\n\t// Check if type t is an array type\n\tif array, ok := t.(*types.Array); ok {\n\t\t// Check if the element type of the array is measurable\n\t\treturn tset.isMeasurable(array.Elem())\n\t}\n\n\t// Check if type t is a slice type\n\tif _, ok := t.(*types.Slice); ok {\n\t\t// Slice types are not measurable\n\t\treturn false\n\t}\n\n\t// Check if type t is a pointer type\n\tif _, ok := t.(*types.Pointer); ok {\n\t\t// Pointer types are not measurable\n\t\treturn false\n\t}\n\n\t// Check if type t is a map type\n\tif _, ok := t.(*types.Map); ok {\n\t\t// Map types are not measurable\n\t\treturn false\n\t}\n\n\t// Check if type t is a struct type\n\tif s, ok := t.Underlying().(*types.Struct); ok {\n\t\tserializable := true\n\t\tfor i := 0; i < s.NumFields(); i++ {\n\t\t\tf := s.Field(i)\n\t\t\tif !tset.isMeasurable(f.Type()) {\n\t\t\t\tserializable = false\n\t\t\t}\n\t\t}\n\t\treturn serializable\n\t}\n\n\t// Check if type t is a string type\n\tif _, ok := t.(*types.Basic); ok && t.String() == \"string\" {\n\t\treturn true\n\t}\n\n\treturn false\n}\n```", "```go\nfunc (tset *typeSet) isMeasurable(t types.Type) bool {\n\t// If we've already computed whether a type is measurable or not, return the cached result.\n\tif result, ok := tset.measurable.At(t).(bool); ok {\n\t\treturn result\n\t}\n\n\tswitch x := t.Underlying().(type) {\n\tcase *types.Basic:\n\t\t// Every basic type (e.g., bool, int) except string is measurable.\n\t\tswitch x.Kind() {\n\t\tcase types.Bool,\n\t\t\ttypes.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\ttypes.Float32, types.Float64,\n\t\t\ttypes.Complex64, types.Complex128:\n\t\t\t// Supported.\n\t\t\ttset.measurable.Set(t, true)\n\t\tdefault:\n\t\t\tif x.Kind() == types.String {\n\t\t\t\t// Strings are measurable.\n\t\t\t\ttset.measurable.Set(t, true)\n\t\t\t} else {\n\t\t\t\ttset.measurable.Set(t, false)\n\t\t\t}\n\t\t}\n\n\tcase *types.Array:\n\t\t// An array type [N]t is measurable if t is measurable.\n\t\ttset.measurable.Set(t, tset.isMeasurable(x.Elem()))\n\n\tcase *types.Slice:\n\t\t// A slice type []t is not measurable.\n\t\ttset.measurable.Set(t, false)\n\n\tcase *types.Pointer:\n\t\t// A pointer type *t is measurable if t is measurable.\n\t\ttset.measurable.Set(t, tset.isMeasurable(x.Elem()))\n\n\tcase *types.Map:\n\t\t// A map type map[K]V is measurable if K and V are measurable.\n\t\ttset.measurable.Set(t, tset.isMeasurable(x.Key()) && tset.isMeasurable(x.Elem()))\n\n\tcase *types.Struct:\n\t\t// A struct type is measurable if all of its fields are measurable.\n\t\tmeasurable := true\n\t\tfor i := 0; i < x.NumFields(); i++ {\n\t\t\tf := x.Field(i)\n\t\t\tb := tset.isMeasurable(f.Type())\n\t\t\tmeasurable = measurable && b\n\t\t}\n\t\ttset.measurable.Set(t, measurable)\n\n\tcase *types.Named:\n\t\t// A named type is measurable if its underlying type is measurable.\n\t\ttset.measurable.Set(t, tset.isMeasurable(x.Underlying()))\n\n\tcase *types.Interface:\n\t\t// For simplicity, we consider a type measurable if the type and all\n\t\t// its nested types are package local. In this case, we check for the\n\t\t// weaver.AutoMarshal interface.\n\t\tautoMarshal := tset.automarshals.At(t) != nil\n\t\ttset.measurable.Set(t, autoMarshal)\n\n\tdefault:\n\t\ttset.measurable.Set(t, false)\n\t}\n\n\treturn tset.measurable.At(t).(bool)\n}\n```"]}, "weaver-internal/tool/generate/types.go-isValidRouterType": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/tool/generate/types.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generate\n\nimport (\n\t\"fmt\"\n\t\"go/types\"\n\t\"path\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"golang.org/x/tools/go/packages\"\n\t\"golang.org/x/tools/go/types/typeutil\"\n)\n\nconst weaverPackagePath = \"github.com/ServiceWeaver/weaver\"\n\n// typeSet holds type information needed by the code generator.\ntype typeSet struct {\n\tpkg            *packages.Package\n\timported       []importPkg          // imported packages\n\timportedByPath map[string]importPkg // imported, indexed by path\n\timportedByName map[string]importPkg // imported, indexed by name\n\n\tautomarshals          *typeutil.Map // types that implement AutoMarshal\n\tautomarshalCandidates *typeutil.Map // types that declare themselves AutoMarshal\n\n\t// If checked[t] != nil, then checked[t] is the cached result of calling\n\t// check(pkg, t, string[]{}). Otherwise, if checked[t] == nil, then t has\n\t// not yet been checked for serializability. Read typeutil.Map's\n\t// documentation for why checked shouldn't be a map[types.Type]bool.\n\tchecked typeutil.Map\n\n\t// If sizes[t] != nil, then sizes[t] == sizeOfType(t).\n\tsizes typeutil.Map\n\n\t// If measurable[t] != nil, then measurable[t] == isMeasurableType(t).\n\tmeasurable typeutil.Map\n}\n\n// importPkg is a package imported by the generated code.\ntype importPkg struct {\n\tpath  string // e.g., \"github.com/ServiceWeaver/weaver\"\n\tpkg   string // e.g., \"weaver\", \"context\", \"time\"\n\talias string // e.g., foo in `import foo \"context\"`\n\tlocal bool   // are we in this package?\n}\n\n// name returns the name by which the imported package should be referenced in\n// the generated code. If the package is imported without an alias, like this:\n//\n//\timport \"context\"\n//\n// then the name is the same as the package name (e.g., \"context\"). However, if\n// a package is imported with an alias, then the name is the alias:\n//\n//\timport thisIsAnAlias \"context\"\n//\n// If the package is local, an empty string is returned.\nfunc (i importPkg) name() string {\n\tif i.local {\n\t\treturn \"\"\n\t} else if i.alias != \"\" {\n\t\treturn i.alias\n\t}\n\treturn i.pkg\n}\n\n// qualify returns the provided member of the package, qualified with the\n// package name. For example, the \"Context\" type inside the \"context\" package\n// is qualified \"context.Context\". The \"Now\" function inside the \"time\" package\n// is qualified \"time.Now\". Note that the package name is not prefixed when\n// qualifying members of the local package.\nfunc (i importPkg) qualify(member string) string {\n\tif i.local {\n\t\treturn member\n\t}\n\treturn fmt.Sprintf(\"%s.%s\", i.name(), member)\n}\n\n// newTypeSet returns the container for types found in pkg.\nfunc newTypeSet(pkg *packages.Package, automarshals, automarshalCandidates *typeutil.Map) *typeSet {\n\treturn &typeSet{\n\t\tpkg:                   pkg,\n\t\timported:              []importPkg{},\n\t\timportedByPath:        map[string]importPkg{},\n\t\timportedByName:        map[string]importPkg{},\n\t\tautomarshals:          automarshals,\n\t\tautomarshalCandidates: automarshalCandidates,\n\t}\n}\n\n// importPackage imports a package with the provided path and package name. The\n// package is imported with an alias if there is a package name clash.\nfunc (tset *typeSet) importPackage(path, pkg string) importPkg {\n\tnewImportPkg := func(path, pkg, alias string, local bool) importPkg {\n\t\ti := importPkg{path: path, pkg: pkg, alias: alias, local: local}\n\t\ttset.imported = append(tset.imported, i)\n\t\ttset.importedByPath[i.path] = i\n\t\ttset.importedByName[i.name()] = i\n\t\treturn i\n\t}\n\n\tif imp, ok := tset.importedByPath[path]; ok {\n\t\t// This package has already been imported.\n\t\treturn imp\n\t}\n\n\tif _, ok := tset.importedByName[pkg]; !ok {\n\t\t// Import the package without an alias.\n\t\treturn newImportPkg(path, pkg, \"\", path == tset.pkg.PkgPath)\n\t}\n\n\t// Find an unused alias.\n\tvar alias string\n\tcounter := 1\n\tfor {\n\t\talias = fmt.Sprintf(\"%s%d\", pkg, counter)\n\t\tif _, ok := tset.importedByName[alias]; !ok {\n\t\t\tbreak\n\t\t}\n\t\tcounter++\n\t}\n\treturn newImportPkg(path, pkg, alias, path == tset.pkg.PkgPath)\n}\n\n// imports returns the list of packages to import in generated code.\nfunc (tset *typeSet) imports() []importPkg {\n\tsort.Slice(tset.imported, func(i, j int) bool {\n\t\treturn tset.imported[i].path < tset.imported[j].path\n\t})\n\treturn tset.imported\n}\n\n// checkSerializable checks that type t is serializable.\nfunc (tset *typeSet) checkSerializable(t types.Type) []error {\n\t// lineage can generate a human readable description of the lineage of a\n\t// checked type. As check recurses on type t, it encounters a number of\n\t// nested types. For example, if we have the following type A\n\t//\n\t//     type A struct{ x []chan int }\n\t//\n\t// then check(A) will encounter the types A, struct{ x []chan int }, []chan\n\t// int, chan int, and int. We associate each of these types with a\n\t// corresponding \"path\", a concise description of the relationship between\n\t// the root type and the nested types. For example, the type chan int has\n\t// path A.x[0].\n\t//\n\t// lineage is a stack that stores a history of these paths as check\n\t// traverses a type. For example, if we call check(A), then lineage will\n\t// look like this when the chan int is discovered:\n\t//\n\t//     []pathAndType{\n\t//         pathAndType{\"A\", A},\n\t//         pathAndType{\"A.x\", []chan int},\n\t//         pathAndType{\"A.x[0]\", chan int},\n\t//     }\n\t//\n\t// This lineage is printed in error messages as:\n\t//\n\t//     A (type A)\n\t//     A.x (type []chan int)\n\t//     A.x[0] (type chan int)\n\t//\n\t// Note that for brevity, not every encountered type is entered into the\n\t// lineage.\n\ttype pathAndType struct {\n\t\tpath string\n\t\tt    types.Type\n\t}\n\tvar lineage []pathAndType\n\n\tvar errors []error\n\taddError := func(err error) {\n\t\tvar builder strings.Builder\n\n\t\t// If the lineage is trivial, then don't show it.\n\t\tif len(lineage) > 1 {\n\t\t\tfmt.Fprintf(&builder, \"\\n    \")\n\t\t\tfor i, pn := range lineage {\n\t\t\t\tfmt.Fprintf(&builder, \"%v (type %v)\", pn.path, pn.t.String())\n\t\t\t\tif i < len(lineage)-1 {\n\t\t\t\t\tfmt.Fprintf(&builder, \"\\n    \")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tqualifier := func(pkg *types.Package) string { return pkg.Name() }\n\t\terr = fmt.Errorf(\"%s: %w%s\", types.TypeString(t, qualifier), err, builder.String())\n\t\terrors = append(errors, err)\n\t}\n\n\t// stack contains the set of types encountered in the call stack of check.\n\t// It's used to detect recursive types.\n\t//\n\t// More specifically, the check function below is performing an implicit\n\t// depth first search of the graph of types formed by t. We record the\n\t// stack of visited types in stack and know we have a recursive type if we\n\t// ever run into a type that is already in stack.\n\t//\n\t// For example, consider the following types:\n\t//\n\t//   type A struct { b: *B }\n\t//   type B struct { a: *A }\n\t//\n\t// Calling check on A will yield a call stack that looks something like:\n\t//\n\t//   check(A)\n\t//     check(struct { b: *B })\n\t//       check(*B)\n\t//         check(B)\n\t//           check(struct { a: *A })\n\t//             check(*A)\n\t//               check(A)\n\t//\n\t// When performing the second check(A) call, stack includes A, struct { b:\n\t// *B }, *B, B, struct { a: *A }, and *A. Because we called check on A and\n\t// A is already in stack, we detect a recursive type and mark A as not\n\t// serializable.\n\tvar stack typeutil.Map\n\n\t// check recursively checks whether a type t is serializable. See lineage\n\t// above for a description of path. record is true if the current type\n\t// should be recorded in lineage. There are a few things worth noting:\n\t//\n\t//   (1) The results of calling check are memoized in tset.checked, but not\n\t//       for some trivial arguments. Some arguments like chan int are not\n\t//       memoized because they are trivial to check and because not\n\t//       memoizing can lead to a clearer error message.\n\t//\n\t//   (2) Consider the type t = struct { x chan int; y chan int }. t is not\n\t//       serializable because neither x nor y is serializable. check\n\t//       reports errors for both x and y as not serializable.\n\t//       Alternatively, check could find that x is not serializable and\n\t//       then immediately report that t is not serializable, skipping y\n\t//       completely. check doesn't do this. check will inspect a type fully\n\t//       to report the full set of errors.\n\t//\n\t// Note that the function also takes the parent type pt. This is needed in cases\n\t// whether we need to know the type of the parent type t (e.g., a named type\n\t// that is a proto is serializable iff the parent type is a pointer).\n\tvar check func(t types.Type, path string, record bool) bool\n\n\tcheck = func(t types.Type, path string, record bool) bool {\n\t\tif record {\n\t\t\tlineage = append(lineage, pathAndType{path, t})\n\t\t\tdefer func() { lineage = lineage[:len(lineage)-1] }()\n\t\t}\n\n\t\t// Return early if we've already checked this type.\n\t\tif result := tset.checked.At(t); result != nil {\n\t\t\tb := result.(bool)\n\t\t\tif b {\n\t\t\t\treturn true\n\t\t\t}\n\t\t\t// We've already encountered type t and determined that it is not\n\t\t\t// serializable. We won't recurse down type t to compute the full\n\t\t\t// lineage and explanation of why t isn't serializable because we\n\t\t\t// already did that when determining t wasn't serializable in the\n\t\t\t// first place. Instead, we instruct the user to read the\n\t\t\t// previously reported error.\n\t\t\taddError(fmt.Errorf(\"not a serializable type; see above for details\"))\n\t\t\treturn false\n\t\t}\n\n\t\t// Check for recursive types.\n\t\tif stack.At(t) != nil {\n\t\t\taddError(fmt.Errorf(\"serialization of recursive types not currently supported\"))\n\t\t\ttset.checked.Set(t, false)\n\t\t\treturn false\n\t\t}\n\t\tstack.Set(t, struct{}{})\n\t\tdefer func() { stack.Delete(t) }()\n\n\t\tswitch x := t.(type) {\n\t\tcase *types.Named:\n\t\t\t// No need to check if x is an unexported type from another package\n\t\t\t// since the Go compiler takes care of that.\n\n\t\t\t// Check if the type implements one of the marshaler interfaces.\n\t\t\tif tset.isProto(x) || tset.automarshals.At(t) != nil || tset.implementsAutoMarshal(x) || tset.hasMarshalBinary(x) {\n\t\t\t\ttset.checked.Set(t, true)\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is not a struct, then we simply recurse\n\t\t\t// on the underlying type.\n\t\t\ts, ok := x.Underlying().(*types.Struct)\n\t\t\tif !ok {\n\t\t\t\ttset.checked.Set(t, check(x.Underlying(), path, false))\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is a struct that has not been declared to\n\t\t\t// implement the AutoMarshal interface, then it is not\n\t\t\t// serializable.\n\t\t\tif tset.automarshalCandidates.At(t) == nil {\n\t\t\t\t// TODO(mwhittaker): Print out a link to documentation on\n\t\t\t\t// weaver.AutoMarshal.\n\t\t\t\taddError(fmt.Errorf(\"named structs are not serializable by default. Consider using weaver.AutoMarshal.\"))\n\t\t\t\ttset.checked.Set(t, false)\n\t\t\t\tbreak\n\t\t\t}\n\n\t\t\t// If the underlying type is a struct that has been declared to\n\t\t\t// implement the AutoMarshal interface but hasn't yet been checked,\n\t\t\t// then we need to recurse to detect cycles.\n\t\t\tserializable := true\n\t\t\tfor i := 0; i < s.NumFields(); i++ {\n\t\t\t\tf := s.Field(i)\n\t\t\t\t// We store the result of calling check in b rather than\n\t\t\t\t// writing serializable = serializable && check(...) because we\n\t\t\t\t// don't want to short circuit and avoid calling check.\n\t\t\t\tb := check(f.Type(), path+\".\"+f.Name(), true)\n\t\t\t\tserializable = serializable && b\n\t\t\t}\n\t\t\ttset.checked.Set(t, serializable)\n\n\t\tcase *types.Interface:\n\t\t\t// TODO(sanjay): Support types.Interface only if we can figure out\n\t\t\t// a way to instantiate the type.\n\t\t\taddError(fmt.Errorf(\"serialization of interfaces not currently supported\"))\n\t\t\ttset.checked.Set(t, false)\n\n\t\tcase *types.Struct:\n\t\t\taddError(fmt.Errorf(\"struct literals are not serializable\"))\n\t\t\ttset.checked.Set(t, false)\n\n\t\tcase *types.Basic:\n\t\t\tswitch x.Kind() {\n\t\t\tcase types.Bool,\n\t\t\t\ttypes.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\t\ttypes.Float32, types.Float64,\n\t\t\t\ttypes.Complex64, types.Complex128,\n\t\t\t\ttypes.String:\n\t\t\t\t// Supported.\n\t\t\t\ttset.checked.Set(t, true)\n\t\t\tdefault:\n\t\t\t\tif isInvalid(t) {\n\t\t\t\t\taddError(fmt.Errorf(\"Maybe you forgot to run `go mod tidy`? Also try running `go build` to diagnose further.\"))\n\t\t\t\t} else {\n\t\t\t\t\taddError(fmt.Errorf(\"unsupported basic type\"))\n\t\t\t\t}\n\t\t\t\t// For a better error message, we don't memoize this.\n\t\t\t\treturn false\n\t\t\t}\n\n\t\tcase *types.Array:\n\t\t\ttset.checked.Set(t, check(x.Elem(), path+\"[0]\", true))\n\n\t\tcase *types.Slice:\n\t\t\ttset.checked.Set(t, check(x.Elem(), path+\"[0]\", true))\n\n\t\tcase *types.Pointer:\n\t\t\ttset.checked.Set(t, check(x.Elem(), \"(*\"+path+\")\", true))\n\n\t\tcase *types.Map:\n\t\t\tkeySerializable := check(x.Key(), path+\".key\", true)\n\t\t\tvalSerializable := check(x.Elem(), path+\".value\", true)\n\t\t\ttset.checked.Set(t, keySerializable && valSerializable)\n\n\t\tdefault:\n\t\t\taddError(fmt.Errorf(\"not a serializable type\"))\n\t\t\t// For a better error message, we don't memoize this.\n\t\t\treturn false\n\t\t}\n\n\t\treturn tset.checked.At(t).(bool)\n\t}\n\n\tcheck(t, t.String(), true)\n\treturn errors\n}\n\n// isFixedSizeType returns whether the provided type has a fixed serialization\n// size. Here is a summary of which types are fixed sized:\n//\n//   - Every basic type (e.g., bool, int) except string is fixed sized.\n//   - The array type [N]t is fixed sized if t is fixed sized.\n//   - A struct is fixed sized if the types of its fields are fixed sized.\n//   - A named type is fixed sized if its underlying type is fixed sized.\nfunc (tset *typeSet) isFixedSizeType(t types.Type) bool {\n\treturn tset.sizeOfType(t) >= 0\n}\n\n// sizeOfType returns the size of the serialization of t if t is fixed size, or\n// returns -1 otherwise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// isMeasurable returns whether the provided type is measurable.\n//\n// Informally, we say a type is measurable if we can cheaply compute the size\n// of its serialization at runtime. Some examples:\n//\n//   - Every fixed size type (e.g., int, bool, [3]int, struct{x, y int}) is\n//     measurable (with some restrictions on package locality; see below).\n//   - Strings are not fixed size, but they are measurable because we can\n//     cheaply compute the length of a string at runtime.\n//   - []string is not measurable because computing the size of the\n//     serialization of a []string would require us to compute the length of\n//     every string in the slice. This is a potentially expensive operation\n//     if the slice contains a large number of strings, so we consider\n//     []string to be not measurable.\n//   - For simplicity, we only consider a type measurable if the type and all\n//     its nested types are package local. For example, a struct { x\n//     otherpackage.T } is not measurable, even if otherpackage.T is\n//     measurable. We make an exception for weaver.AutoMarshal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// genTypeString returns the string representation of t as to be printed\n// in the generated code, updating import definitions to account for the\n// returned type string.\n//\n// Since this call has side-effects (i.e., updating import definitions), it\n// should only be called when the returned type string is written into\n// the generated file; otherwise, the generated code may end up with spurious\n// imports.\nfunc (tset *typeSet) genTypeString(t types.Type) string {\n\t// qualifier is passed to types.TypeString(Type, Qualifier) to determine\n\t// how packages are printed when pretty printing types. For this qualifier,\n\t// types in the root package are printed without their package name, while\n\t// types outside the root package are printed with their package name. For\n\t// example, if we're in root package foo, then the type foo.Bar is printed\n\t// as Bar, while the type io.Reader is printed as io.Reader. See [1] for\n\t// more information on qualifiers and pretty printing types.\n\t//\n\t// [1]: https://github.com/golang/example/tree/master/gotypes#formatting-support\n\tvar qualifier = func(pkg *types.Package) string {\n\t\tif pkg == tset.pkg.Types {\n\t\t\treturn \"\"\n\t\t}\n\t\treturn tset.importPackage(pkg.Path(), pkg.Name()).name()\n\t}\n\treturn types.TypeString(t, qualifier)\n}\n\n// isInvalid returns true iff the given type is invalid.\nfunc isInvalid(t types.Type) bool {\n\treturn t.String() == \"invalid type\"\n}\n\n// implementsError returns whether the provided type is a concrete type that\n// implements error.\nfunc (tset *typeSet) implementsError(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\treturn false\n\t}\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"Error\")\n\tmethod, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tsig, ok := method.Type().(*types.Signature)\n\tif !ok {\n\t\treturn false\n\t}\n\tif args := sig.Params(); args.Len() != 0 {\n\t\treturn false\n\t}\n\tif results := sig.Results(); results.Len() != 1 || !isString(results.At(0).Type()) {\n\t\treturn false\n\t}\n\treturn true\n}\n\n// isProto returns whether the provided type is a concrete type that implements\n// the proto.Message interface.\nfunc (tset *typeSet) isProto(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\t// A superinterface of proto.Message does \"implement\" proto.Message,\n\t\t// but we only accept concrete types that implement the interface.\n\t\treturn false\n\t}\n\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"ProtoReflect\")\n\tmethod, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tsig, ok := method.Type().(*types.Signature)\n\tif !ok {\n\t\treturn false\n\t}\n\trecv, args, results := sig.Recv(), sig.Params(), sig.Results()\n\tif args.Len() != 0 || results.Len() != 1 {\n\t\treturn false\n\t}\n\tif !isProtoMessage(results.At(0).Type()) {\n\t\treturn false\n\t}\n\t// Check the receiver. We avoid complicated embeddings by requiring that\n\t// the method is defined on the type itself.\n\t//\n\t// TODO(mwhittaker): Relax this requirement if it becomes annoying.\n\tif p, ok := recv.Type().(*types.Pointer); ok {\n\t\treturn types.Identical(p.Elem(), t)\n\t} else {\n\t\treturn types.Identical(recv.Type(), t)\n\t}\n}\n\nfunc isProtoMessage(t types.Type) bool {\n\tn, ok := t.(*types.Named)\n\tif !ok {\n\t\treturn false\n\t}\n\tconst protoreflect = \"google.golang.org/protobuf/reflect/protoreflect\"\n\treturn n.Obj().Pkg().Path() == protoreflect && n.Obj().Name() == \"Message\"\n}\n\n// implementsAutoMarshal returns whether the provided type is a concrete\n// type that implements the weaver.AutoMarshal interface.\nfunc (tset *typeSet) implementsAutoMarshal(t types.Type) bool {\n\tif _, ok := t.Underlying().(*types.Interface); ok {\n\t\t// A superinterface of AutoMarshal does \"implement\" the interface,\n\t\t// but we only accept concrete types that implement the interface.\n\t\treturn false\n\t}\n\n\tobj, _, _ := types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"WeaverMarshal\")\n\tmarshal, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\tobj, _, _ = types.LookupFieldOrMethod(t, true, tset.pkg.Types, \"WeaverUnmarshal\")\n\tunmarshal, ok := obj.(*types.Func)\n\tif !ok {\n\t\treturn false\n\t}\n\treturn is\n```\n# Function Signature: func isValidRouterType(t types.Type) bool {\n# Natural Language Annotations: // isValidRouterType returns whether the provided type is a valid router type.\n// A router type can be one of the following: an integer (signed or unsigned),\n// a float, or a string. Alternatively, it can be a struct that may optioanly\n// embed the weaver.AutoMarshal struct and rest of the fields must be either\n// integers, floats, or strings.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc isValidRouterType(t types.Type) bool {\n\tswitch x := t.Underlying().(type) {\n\tcase *types.Basic:\n\t\tswitch x.Kind() {\n\t\tcase types.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\ttypes.Float32, types.Float64,\n\t\t\ttypes.String:\n\t\t\treturn true\n\t\t}\n\tcase *types.Struct:\n\t\tfor i := 0; i < x.NumFields(); i++ {\n\t\t\tf := x.Field(i)\n\t\t\tswitch y := f.Type().Underlying().(type) {\n\t\t\tcase *types.Basic:\n\t\t\t\tswitch y.Kind() {\n\t\t\t\tcase types.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\t\t\ttypes.Float32, types.Float64,\n\t\t\t\t\ttypes.String:\n\t\t\t\tdefault:\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\tcase *types.Interface:\n\t\t\t\tif y.String() != \"github.com/ServiceWeaver/weaver.AutoMarshal\" {\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\tdefault:\n\t\treturn false\n\t}\n\n\treturn true\n}\n```", "Here's the generated code for the `isValidRouterType` function:\n\n```go\n// isValidRouterType returns whether the provided type is a valid router type.\n// A router type can be one of the following: an integer (signed or unsigned),\n// a float, or a string. Alternatively, it can be a struct that may optionally\n// embed the weaver.AutoMarshal struct and rest of the fields must be either\n// integers, floats, or strings.\nfunc isValidRouterType(t types.Type) bool {\n\tswitch x := t.(type) {\n\tcase *types.Basic:\n\t\tswitch x.Kind() {\n\t\tcase types.Int, types.Uint, types.Int8, types.Uint8, types.Int16, types.Uint16, types.Int32, types.Uint32, types.Int64, types.Uint64, types.Float32, types.Float64:\n\t\t\treturn true\n\t\tcase types.String:\n\t\t\treturn true\n\t\t}\n\tcase *types.Named:\n\t\tif _, ok := x.Underlying().(*types.Struct); ok {\n\t\t\ts := x.Underlying().(*types.Struct)\n\t\t\tfoundAutoMarshal := false\n\t\t\tfor i := 0; i < s.NumFields(); i++ {\n\t\t\t\tf := s.Field(i)\n\t\t\t\tswitch ft := f.Type().(type) {\n\t\t\t\tcase *types.Basic:\n\t\t\t\t\tswitch ft.Kind() {\n\t\t\t\t\tcase types.Int, types.Uint, types.Int8, types.Uint8, types.Int16, types.Uint16, types.Int32, types.Uint32, types.Int64, types.Uint64, types.Float32, types.Float64, types.String:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\treturn false\n\t\t\t\tcase *types.Pointer:\n\t\t\t\t\tif named, ok := ft.Elem().(*types.Named); ok {\n\t\t\t\t\t\tif named.Obj().Name() == \"AutoMarshal\" && named.Obj().Pkg().Path() == weaverPackagePath {\n\t\t\t\t\t\t\tif foundAutoMarshal {\n\t\t\t\t\t\t\t\treturn false\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tfoundAutoMarshal = true\n\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn false\n\t\t\t}\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n```\n\nThis code checks the provided type `t` and returns `true` if it's a valid router type as defined in the function's comments. If it's not a valid router type, it returns `false`.", "```go\nfunc isValidRouterType(t types.Type) bool {\n\tswitch x := t.(type) {\n\tcase *types.Basic:\n\t\tswitch x.Kind() {\n\t\tcase types.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\ttypes.Float32, types.Float64,\n\t\t\ttypes.String:\n\t\t\treturn true\n\t\tdefault:\n\t\t\treturn false\n\t\t}\n\tcase *types.Named:\n\t\tif _, ok := x.Underlying().(*types.Struct); ok {\n\t\t\ts, _ := x.Underlying().(*types.Struct)\n\t\t\tfor i := 0; i < s.NumFields(); i++ {\n\t\t\t\tf := s.Field(i)\n\t\t\t\tswitch ft := f.Type().(type) {\n\t\t\t\tcase *types.Basic:\n\t\t\t\t\tswitch ft.Kind() {\n\t\t\t\t\tcase types.Int, types.Int8, types.Int16, types.Int32, types.Int64,\n\t\t\t\t\t\ttypes.Uint, types.Uint8, types.Uint16, types.Uint32, types.Uint64,\n\t\t\t\t\t\ttypes.Float32, types.Float64,\n\t\t\t\t\t\ttypes.String:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tdefault:\n\t\t\t\t\t\treturn false\n\t\t\t\t\t}\n\t\t\t\tdefault:\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn true\n\t\t}\n\t\treturn false\n\tdefault:\n\t\treturn false\n\t}\n}\n```"]}, "weaver-internal/tool/generate/generator.go-sanitize": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/tool/generate/generator.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generate\n\nimport (\n\t\"bytes\"\n\t\"crypto/sha256\"\n\t\"errors\"\n\t\"fmt\"\n\t\"go/ast\"\n\t\"go/format\"\n\t\"go/parser\"\n\t\"go/token\"\n\t\"go/types\"\n\t\"io\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"unicode\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/files\"\n\t\"github.com/ServiceWeaver/weaver/internal/tool\"\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/colors\"\n\t\"github.com/ServiceWeaver/weaver/runtime/version\"\n\t\"golang.org/x/exp/maps\"\n\t\"golang.org/x/tools/go/packages\"\n\t\"golang.org/x/tools/go/types/typeutil\"\n)\n\n// TODO(rgrandl): Modify the generator code to use only the types package. Right\n// now we are doing code generation relying both on the types and ast packages,\n// which can be confusing and also we might do unnecessary work.\n\nconst (\n\tgeneratedCodeFile = \"weaver_gen.go\"\n\n\tUsage = `Generate code for a Service Weaver application.\n\nUsage:\n  weaver generate [packages]\n\nDescription:\n  \"weaver generate\" generates code for the Service Weaver applications in the\n  provided packages. For example, \"weaver generate . ./foo\" will generate code\n  for the Service Weaver applications in the current directory and in the ./foo\n  directory. For every package, the generated code is placed in a weaver_gen.go\n  file in the package's directory. For example, \"weaver generate . ./foo\" will\n  create ./weaver_gen.go and ./foo/weaver_gen.go.\n\n  You specify packages for \"weaver generate\" in the same way you specify\n  packages for go build, go test, go vet, etc. See \"go help packages\" for more\n  information.\n\n  Rather than invoking \"weaver generate\" directly, you can place a line of the\n  following form in one of the .go files in the package:\n\n      //go:generate weaver generate\n\n  and then use the normal \"go generate\" command.\n\nExamples:\n  # Generate code for the package in the current directory.\n  weaver generate\n\n  # Same as \"weaver generate\".\n  weaver generate .\n\n  # Generate code for the package in the ./foo directory.\n  weaver generate ./foo\n\n  # Generate code for all packages in all subdirectories of current directory.\n  weaver generate ./...`\n)\n\n// Options controls the operation of Generate.\ntype Options struct {\n\t// If non-nil, use the specified function to report warnings.\n\tWarn func(error)\n}\n\n// Generate generates Service Weaver code for the specified packages.\n// The list of supplied packages are treated similarly to the arguments\n// passed to \"go build\" (see \"go help packages\" for details).\nfunc Generate(dir string, pkgs []string, opt Options) error {\n\tif opt.Warn == nil {\n\t\topt.Warn = func(err error) { fmt.Fprintln(os.Stderr, err) }\n\t}\n\tfset := token.NewFileSet()\n\tcfg := &packages.Config{\n\t\tMode:       packages.NeedName | packages.NeedSyntax | packages.NeedImports | packages.NeedTypes | packages.NeedTypesInfo,\n\t\tDir:        dir,\n\t\tFset:       fset,\n\t\tParseFile:  parseNonWeaverGenFile,\n\t\tBuildFlags: []string{\"--tags=ignoreWeaverGen\"},\n\t}\n\tpkgList, err := packages.Load(cfg, pkgs...)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"packages.Load: %w\", err)\n\t}\n\n\tvar automarshals typeutil.Map\n\tvar errs []error\n\tfor _, pkg := range pkgList {\n\t\tg, err := newGenerator(opt, pkg, fset, &automarshals)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t\tcontinue\n\t\t}\n\t\tif err := g.generate(); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\treturn errors.Join(errs...)\n}\n\n// parseNonWeaverGenFile parses a Go file, except for weaver_gen.go files whose\n// contents are ignored since those contents may reference types that no longer\n// exist.\nfunc parseNonWeaverGenFile(fset *token.FileSet, filename string, src []byte) (*ast.File, error) {\n\tif filepath.Base(filename) == generatedCodeFile {\n\t\treturn parser.ParseFile(fset, filename, src, parser.PackageClauseOnly)\n\t}\n\treturn parser.ParseFile(fset, filename, src, parser.ParseComments|parser.DeclarationErrors)\n}\n\ntype generator struct {\n\tpkg            *packages.Package\n\ttset           *typeSet\n\tfileset        *token.FileSet\n\tcomponents     []*component\n\tsizeFuncNeeded typeutil.Map // types that need a serviceweaver_size_* function\n\tgenerated      typeutil.Map // memo cache for generateEncDecMethodsFor\n}\n\n// errorf is like fmt.Errorf but prefixes the error with the provided position.\nfunc errorf(fset *token.FileSet, pos token.Pos, format string, args ...interface{}) error {\n\t// Rewrite the position's filename relative to the current directory. This\n\t// replaces long filenames like \"/home/foo/ServiceWeaver/weaver/weaver.go\"\n\t// with much shorter filenames like \"./weaver.go\".\n\tposition := fset.Position(pos)\n\tif cwd, err := filepath.Abs(\".\"); err == nil {\n\t\tif filename, err := filepath.Rel(cwd, position.Filename); err == nil {\n\t\t\tposition.Filename = filename\n\t\t}\n\t}\n\n\tprefix := position.String()\n\tif colors.Enabled() {\n\t\t// Color the filename red when colors are enabled.\n\t\tprefix = fmt.Sprintf(\"%s%v%s\", colors.Color256(160), position, colors.Reset)\n\t}\n\treturn fmt.Errorf(\"%s: %w\", prefix, fmt.Errorf(format, args...))\n}\n\nfunc newGenerator(opt Options, pkg *packages.Package, fset *token.FileSet, automarshals *typeutil.Map) (*generator, error) {\n\t// Abort if there were any errors loading the package.\n\tvar errs []error\n\tfor _, err := range pkg.Errors {\n\t\terrs = append(errs, err)\n\t}\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Search every file in the package for types that embed the\n\t// weaver.AutoMarshal struct.\n\ttset := newTypeSet(pkg, automarshals, &typeutil.Map{})\n\tfor _, file := range pkg.Syntax {\n\t\tfilename := fset.Position(file.Package).Filename\n\t\tif filepath.Base(filename) == generatedCodeFile {\n\t\t\t// Ignore weaver_gen.go files.\n\t\t\tcontinue\n\t\t}\n\t\tts, err := findAutoMarshals(pkg, file)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t\tcontinue\n\t\t}\n\t\tfor _, t := range ts {\n\t\t\ttset.automarshalCandidates.Set(t, struct{}{})\n\t\t}\n\t}\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Just because a type embeds weaver.AutoMarshal doesn't mean we can\n\t// automatically marshal it. Some types, like `struct { x chan int }`, are\n\t// just not serializable. Here, we check that every type that embeds\n\t// weaver.AutoMarshal is actually serializable.\n\tfor _, t := range tset.automarshalCandidates.Keys() {\n\t\tn := t.(*types.Named)\n\t\tif err := errors.Join(tset.checkSerializable(n)...); err != nil {\n\t\t\terrs = append(errs, errorf(fset, n.Obj().Pos(), \"type %v is not serializable\\n%w\", t, err))\n\t\t\tcontinue\n\t\t}\n\t\ttset.automarshals.Set(t, struct{}{})\n\t}\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Find and process all components.\n\tcomponents := map[string]*component{}\n\tfor _, file := range pkg.Syntax {\n\t\tfilename := fset.Position(file.Package).Filename\n\t\tif filepath.Base(filename) == generatedCodeFile {\n\t\t\t// Ignore weaver_gen.go files.\n\t\t\tcontinue\n\t\t}\n\n\t\tfileComponents, err := findComponents(opt, pkg, file, tset)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tfor _, c := range fileComponents {\n\t\t\t// Check for component duplicates, two components that embed the\n\t\t\t// same weaver.Implements[T].\n\t\t\t//\n\t\t\t// TODO(mwhittaker): This code relies on the fact that a component\n\t\t\t// interface and component implementation have to be in the same\n\t\t\t// package. If we lift this requirement, then this code will break.\n\t\t\tif existing, ok := components[c.fullIntfName()]; ok {\n\t\t\t\terrs = append(errs, errorf(pkg.Fset, c.impl.Obj().Pos(),\n\t\t\t\t\t\"Duplicate implementation for component %s, other declaration: %v\",\n\t\t\t\t\tc.fullIntfName(), fset.Position(existing.impl.Obj().Pos())))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcomponents[c.fullIntfName()] = c\n\t\t}\n\t}\n\n\t// Find method attributes.\n\tfor _, file := range pkg.Syntax {\n\t\tfilename := fset.Position(file.Package).Filename\n\t\tif filepath.Base(filename) == generatedCodeFile {\n\t\t\t// Ignore weaver_gen.go files.\n\t\t\tcontinue\n\t\t}\n\t\tif err := findMethodAttributes(pkg, file, components); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &generator{\n\t\tpkg:        pkg,\n\t\ttset:       tset,\n\t\tfileset:    fset,\n\t\tcomponents: maps.Values(components),\n\t}, nil\n}\n\n// findComponents returns the components in the provided file. For example,\n// findComponents will find and return the following component.\n//\n//\ttype something struct {\n//\t    weaver.Implements[SomeComponentType]\n//\t    ...\n//\t}\nfunc findComponents(opt Options, pkg *packages.Package, f *ast.File, tset *typeSet) ([]*component, error) {\n\tvar components []*component\n\tvar errs []error\n\tfor _, d := range f.Decls {\n\t\tgendecl, ok := d.(*ast.GenDecl)\n\t\tif !ok || gendecl.Tok != token.TYPE {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, spec := range gendecl.Specs {\n\t\t\tts, ok := spec.(*ast.TypeSpec)\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcomponent, err := extractComponent(opt, pkg, f, tset, ts)\n\t\t\tif err != nil {\n\t\t\t\terrs = append(errs, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif component != nil {\n\t\t\t\tcomponents = append(components, component)\n\t\t\t}\n\t\t}\n\t}\n\treturn components, errors.Join(errs...)\n}\n\nfunc findMethodAttributes(pkg *packages.Package, f *ast.File, components map[string]*component) error {\n\t// Look for declarations of the form:\n\t//\tvar _ weaver.NotRetriable = Component.Method\n\tvar errs []error\n\tfor _, decl := range f.Decls {\n\t\tgendecl, ok := decl.(*ast.GenDecl)\n\t\tif !ok || gendecl.Tok != token.VAR {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, spec := range gendecl.Specs {\n\t\t\tvalspec, ok := spec.(*ast.ValueSpec)\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttypeAndValue, ok := pkg.TypesInfo.Types[valspec.Type]\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tt := typeAndValue.Type\n\t\t\tif !isWeaverNotRetriable(t) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfor _, val := range valspec.Values {\n\t\t\t\t// We allow non-blank vars for uniformity.\n\t\t\t\tcomp, method, ok := findComponentMethod(pkg, components, val)\n\t\t\t\tif !ok {\n\t\t\t\t\terrs = append(errs, errorf(pkg.Fset, valspec.Pos(), \"weaver.NonRetriable should only be assigned a value that identifies a method of a component implemented by this package\"))\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif comp.noretry == nil {\n\t\t\t\t\tcomp.noretry = map[string]struct{}{}\n\t\t\t\t}\n\t\t\t\tcomp.noretry[method] = struct{}{}\n\t\t\t}\n\t\t}\n\t}\n\treturn errors.Join(errs...)\n}\n\n// findComponentMethod returns the component and method if val is an expression of\n// the form C.M where C is a component listed in components and C has a method named M.\nfunc findComponentMethod(pkg *packages.Package, components map[string]*component, val ast.Expr) (*component, string, bool) {\n\tsel, ok := val.(*ast.SelectorExpr)\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tctypeval, ok := pkg.TypesInfo.Types[sel.X]\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tctype, ok := ctypeval.Type.(*types.Named)\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tcname := fullName(ctype)\n\tc, ok := components[cname]\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tmethod := sel.Sel.Name\n\tfor _, m := range c.methods() {\n\t\tif m.Name() == method {\n\t\t\treturn c, method, true\n\t\t}\n\t}\n\treturn nil, \"\", false\n}\n\n// findAutoMarshals returns the types in the provided file which embed the\n// weaver.AutoMarshal struct.\nfunc findAutoMarshals(pkg *packages.Package, f *ast.File) ([]*types.Named, error) {\n\tvar automarshals []*types.Named\n\tvar errs []error\n\tfor _, decl := range f.Decls {\n\t\tgendecl, ok := decl.(*ast.GenDecl)\n\t\tif !ok || gendecl.Tok != token.TYPE {\n\t\t\t// This is not a type declaration.\n\t\t\tcontinue\n\t\t}\n\n\t\t// Recall that a type declaration can have multiple type specs. We have\n\t\t// to iterate over all of them. For example:\n\t\t//\n\t\t//     type (\n\t\t//         a struct{} // Spec 1\n\t\t//         b struct{} // Spec 2\n\t\t//     )\n\t\tfor _, spec := range gendecl.Specs {\n\t\t\ttypespec, ok := spec.(*ast.TypeSpec)\n\t\t\tif !ok {\n\t\t\t\tpanic(errorf(pkg.Fset, spec.Pos(), \"type declaration has non-TypeSpec spec: %v\", spec))\n\t\t\t}\n\n\t\t\t// Extract the type's name.\n\t\t\tdef, ok := pkg.TypesInfo.Defs[typespec.Name]\n\t\t\tif !ok {\n\t\t\t\tpanic(errorf(pkg.Fset, spec.Pos(), \"name %v not found\", typespec.Name))\n\t\t\t}\n\t\t\tn, ok := def.Type().(*types.Named)\n\t\t\tif !ok {\n\t\t\t\t// For type aliases like `type Int = int`, Int has type int and\n\t\t\t\t// not type Named. We ignore these.\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Check if the type of the expression is struct.\n\t\t\tt, ok := pkg.TypesInfo.Types[typespec.Type].Type.(*types.Struct)\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Check for an embedded weaver.AutoMarshal field.\n\t\t\tautomarshal := false\n\t\t\tfor i := 0; i < t.NumFields(); i++ {\n\t\t\t\tf := t.Field(i)\n\t\t\t\tif f.Embedded() && isWeaverAutoMarshal(f.Type()) {\n\t\t\t\t\tautomarshal = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !automarshal {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Ignore generic types. Generic types don't play well with\n\t\t\t// embedded AutoMarshals. For example, consider the following type\n\t\t\t// declaration:\n\t\t\t//\n\t\t\t//     type Register[A any] struct {\n\t\t\t//         weaver.AutoMarshal\n\t\t\t//         a A\n\t\t\t//     }\n\t\t\t//\n\t\t\t// Is Register[A] serializable? It depends on A. Plus, we cannot\n\t\t\t// really generate WeaverMarshal and WeaverUnmarshal methods for\n\t\t\t// specific instantiations of Register[A]. Because of these\n\t\t\t// complications, we ignore generic types.\n\t\t\t//\n\t\t\t// TODO(mwhittaker): Handle generics somehow?\n\t\t\tif n.TypeParams() != nil { // generics have non-nil TypeParams()\n\t\t\t\terrs = append(errs, errorf(pkg.Fset, spec.Pos(),\n\t\t\t\t\t\"generic struct %v cannot embed weaver.AutoMarshal. See serviceweaver.dev/docs.html#serializable-types for more information.\",\n\t\t\t\t\tformatType(pkg, n)))\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tautomarshals = append(automarshals, n)\n\t\t}\n\t}\n\treturn automarshals, errors.Join(errs...)\n}\n\n// extractComponent attempts to extract a component from the provided TypeSpec.\n// It returns a nil component if the TypeSpec doesn't define a component.\nfunc extractComponent(opt Options, pkg *packages.Package, file *ast.File, tset *typeSet, spec *ast.TypeSpec) (*component, error) {\n\t// Check that the type spec is of the form `type t struct {...}`.\n\ts, ok := spec.Type.(*ast.StructType)\n\tif !ok {\n\t\t// This type declaration does not involve a struct. For example, it\n\t\t// might look like `type t int`. These non-struct type declarations\n\t\t// cannot be components.\n\t\treturn nil, nil\n\t}\n\tdef, ok := pkg.TypesInfo.Defs[spec.Name]\n\tif !ok {\n\t\tpanic(errorf(pkg.Fset, spec.Pos(), \"name %v not found\", spec.Name))\n\t}\n\timpl, ok := def.Type().(*types.Named)\n\tif !ok {\n\t\t// For type aliases like `type t = struct{}`, t has type *types.Struct\n\t\t// and not type *types.Named. We ignore these.\n\t\treturn nil, nil\n\t}\n\n\t// Find any weaver.Implements[T] or weaver.WithRouter[T] embedded fields.\n\tvar intf *types.Named   // The component interface type\n\tvar router *types.Named // Router type (if any)\n\tvar isMain bool         // Is intf weaver.Main?\n\tvar refs []*types.Named // T for which weaver.Ref[T] exists in struct\n\tvar listeners []string  // Names of all listener fields declared in struct\n\tfor _, f := range s.Fields.List {\n\t\ttypeAndValue, ok := pkg.TypesInfo.Types[f.Type]\n\t\tif !ok {\n\t\t\tpanic(errorf(pkg.Fset, f.Pos(), \"type %v not found\", f.Type))\n\t\t}\n\t\tt := typeAndValue.Type\n\n\t\tif isWeaverRef(t) {\n\t\t\t// The field f has type weaver.Ref[T].\n\t\t\targ := t.(*types.Named).TypeArgs().At(0)\n\t\t\tif isWeaverMain(arg) {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"components cannot contain a reference to weaver.Main\")\n\t\t\t}\n\t\t\tnamed, ok := arg.(*types.Named)\n\t\t\tif !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Ref argument %s is not a named type.\",\n\t\t\t\t\tformatType(pkg, arg))\n\t\t\t}\n\t\t\trefs = append(refs, named)\n\t\t} else if isWeaverListener(t) {\n\t\t\tlis, err := getListenerNamesFromStructField(pkg, f)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tlisteners = append(listeners, lis...)\n\t\t}\n\n\t\tif len(f.Names) != 0 {\n\t\t\t// Ignore unembedded fields.\n\t\t\t//\n\t\t\t// TODO(mwhittaker): Warn the user about unembedded\n\t\t\t// weaver.Implements, weaver.WithConfig, or weaver.WithRouter?\n\t\t\tcontinue\n\t\t}\n\n\t\tswitch {\n\t\t// The field f is an embedded weaver.Implements[T].\n\t\tcase isWeaverImplements(t):\n\t\t\t// Check that T is a named interface type inside the package.\n\t\t\targ := t.(*types.Named).TypeArgs().At(0)\n\t\t\tnamed, ok := arg.(*types.Named)\n\t\t\tif !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Implements argument %s is not a named type.\",\n\t\t\t\t\tformatType(pkg, arg))\n\t\t\t}\n\t\t\tisMain = isWeaverMain(arg)\n\t\t\tif !isMain && named.Obj().Pkg() != pkg.Types {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Implements argument %s is a type outside the current package. A component interface and implementation must be in the same package. If you can't move them into the same package, you can add `type %s %v` to the implementation's package and embed `weaver.Implements[%s]` instead of `weaver.Implements[%s]`.\",\n\t\t\t\t\tformatType(pkg, named), named.Obj().Name(), formatType(pkg, named), named.Obj().Name(), formatType(pkg, named))\n\t\t\t}\n\t\t\tif _, ok := named.Underlying().(*types.Interface); !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Implements argument %s is not an interface.\",\n\t\t\t\t\tformatType(pkg, named))\n\t\t\t}\n\t\t\tintf = named\n\n\t\t// The field f is an embedded weaver.WithRouter[T].\n\t\tcase isWeaverWithRouter(t):\n\t\t\t// Check that T is a named type inside the package.\n\t\t\targ := t.(*types.Named).TypeArgs().At(0)\n\t\t\tnamed, ok := arg.(*types.Named)\n\t\t\tif !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.WithRouter argument %s is not a named type.\",\n\t\t\t\t\tformatType(pkg, arg))\n\t\t\t}\n\t\t\tif named.Obj().Pkg() != pkg.Types {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.WithRouter argument %s is a type outside the current package.\",\n\t\t\t\t\tformatType(pkg, named))\n\t\t\t}\n\t\t\trouter = named\n\t\t}\n\t}\n\n\tif intf == nil {\n\t\t// TODO(mwhittaker): Warn the user if they embed weaver.WithRouter or\n\t\t// weaver.WithConfig but don't embed weaver.Implements.\n\t\treturn nil, nil\n\t}\n\n\t// Check that that the component implementation implements the component\n\t// interface.\n\tif !types.Implements(types.NewPointer(impl), intf.Underlying().(*types.Interface)) {\n\t\treturn nil, errorf(pkg.Fset, spec.Pos(),\n\t\t\t\"type %s embeds weaver\n```\n# Function Signature: func sanitize(t types.Type) string {\n# Natural Language Annotations: // sanitize generates a (somewhat pretty printed) name for the provided type\n// that is a valid go identifier [1]. sanitize also produces unique names. That\n// is, if u != t, then sanitize(u) != sanitize(t).\n//\n// Some examples:\n//\n//   - map[int]string -> map_int_string_589aebd1\n//   - map[int][]X    -> map_int_slice_X_ac498abc\n//   - []int          -> slice_int_1048ebf9\n//   - [][]string     -> slice_slice_string_13efa8aa\n//   - [20]int        -> array_20_int_00ae9a0a\n//   - *int           -> ptr_int_916711b2\n//\n// [1]: https://go.dev/ref/spec#Identifiers\nPlease return the generated code surrounded by ```\n", "response": ["!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"]}, "weaver-internal/tool/generate/generator.go-uniqueName": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/tool/generate/generator.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage generate\n\nimport (\n\t\"bytes\"\n\t\"crypto/sha256\"\n\t\"errors\"\n\t\"fmt\"\n\t\"go/ast\"\n\t\"go/format\"\n\t\"go/parser\"\n\t\"go/token\"\n\t\"go/types\"\n\t\"io\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"reflect\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"unicode\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/files\"\n\t\"github.com/ServiceWeaver/weaver/internal/tool\"\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/colors\"\n\t\"github.com/ServiceWeaver/weaver/runtime/version\"\n\t\"golang.org/x/exp/maps\"\n\t\"golang.org/x/tools/go/packages\"\n\t\"golang.org/x/tools/go/types/typeutil\"\n)\n\n// TODO(rgrandl): Modify the generator code to use only the types package. Right\n// now we are doing code generation relying both on the types and ast packages,\n// which can be confusing and also we might do unnecessary work.\n\nconst (\n\tgeneratedCodeFile = \"weaver_gen.go\"\n\n\tUsage = `Generate code for a Service Weaver application.\n\nUsage:\n  weaver generate [packages]\n\nDescription:\n  \"weaver generate\" generates code for the Service Weaver applications in the\n  provided packages. For example, \"weaver generate . ./foo\" will generate code\n  for the Service Weaver applications in the current directory and in the ./foo\n  directory. For every package, the generated code is placed in a weaver_gen.go\n  file in the package's directory. For example, \"weaver generate . ./foo\" will\n  create ./weaver_gen.go and ./foo/weaver_gen.go.\n\n  You specify packages for \"weaver generate\" in the same way you specify\n  packages for go build, go test, go vet, etc. See \"go help packages\" for more\n  information.\n\n  Rather than invoking \"weaver generate\" directly, you can place a line of the\n  following form in one of the .go files in the package:\n\n      //go:generate weaver generate\n\n  and then use the normal \"go generate\" command.\n\nExamples:\n  # Generate code for the package in the current directory.\n  weaver generate\n\n  # Same as \"weaver generate\".\n  weaver generate .\n\n  # Generate code for the package in the ./foo directory.\n  weaver generate ./foo\n\n  # Generate code for all packages in all subdirectories of current directory.\n  weaver generate ./...`\n)\n\n// Options controls the operation of Generate.\ntype Options struct {\n\t// If non-nil, use the specified function to report warnings.\n\tWarn func(error)\n}\n\n// Generate generates Service Weaver code for the specified packages.\n// The list of supplied packages are treated similarly to the arguments\n// passed to \"go build\" (see \"go help packages\" for details).\nfunc Generate(dir string, pkgs []string, opt Options) error {\n\tif opt.Warn == nil {\n\t\topt.Warn = func(err error) { fmt.Fprintln(os.Stderr, err) }\n\t}\n\tfset := token.NewFileSet()\n\tcfg := &packages.Config{\n\t\tMode:       packages.NeedName | packages.NeedSyntax | packages.NeedImports | packages.NeedTypes | packages.NeedTypesInfo,\n\t\tDir:        dir,\n\t\tFset:       fset,\n\t\tParseFile:  parseNonWeaverGenFile,\n\t\tBuildFlags: []string{\"--tags=ignoreWeaverGen\"},\n\t}\n\tpkgList, err := packages.Load(cfg, pkgs...)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"packages.Load: %w\", err)\n\t}\n\n\tvar automarshals typeutil.Map\n\tvar errs []error\n\tfor _, pkg := range pkgList {\n\t\tg, err := newGenerator(opt, pkg, fset, &automarshals)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t\tcontinue\n\t\t}\n\t\tif err := g.generate(); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\treturn errors.Join(errs...)\n}\n\n// parseNonWeaverGenFile parses a Go file, except for weaver_gen.go files whose\n// contents are ignored since those contents may reference types that no longer\n// exist.\nfunc parseNonWeaverGenFile(fset *token.FileSet, filename string, src []byte) (*ast.File, error) {\n\tif filepath.Base(filename) == generatedCodeFile {\n\t\treturn parser.ParseFile(fset, filename, src, parser.PackageClauseOnly)\n\t}\n\treturn parser.ParseFile(fset, filename, src, parser.ParseComments|parser.DeclarationErrors)\n}\n\ntype generator struct {\n\tpkg            *packages.Package\n\ttset           *typeSet\n\tfileset        *token.FileSet\n\tcomponents     []*component\n\tsizeFuncNeeded typeutil.Map // types that need a serviceweaver_size_* function\n\tgenerated      typeutil.Map // memo cache for generateEncDecMethodsFor\n}\n\n// errorf is like fmt.Errorf but prefixes the error with the provided position.\nfunc errorf(fset *token.FileSet, pos token.Pos, format string, args ...interface{}) error {\n\t// Rewrite the position's filename relative to the current directory. This\n\t// replaces long filenames like \"/home/foo/ServiceWeaver/weaver/weaver.go\"\n\t// with much shorter filenames like \"./weaver.go\".\n\tposition := fset.Position(pos)\n\tif cwd, err := filepath.Abs(\".\"); err == nil {\n\t\tif filename, err := filepath.Rel(cwd, position.Filename); err == nil {\n\t\t\tposition.Filename = filename\n\t\t}\n\t}\n\n\tprefix := position.String()\n\tif colors.Enabled() {\n\t\t// Color the filename red when colors are enabled.\n\t\tprefix = fmt.Sprintf(\"%s%v%s\", colors.Color256(160), position, colors.Reset)\n\t}\n\treturn fmt.Errorf(\"%s: %w\", prefix, fmt.Errorf(format, args...))\n}\n\nfunc newGenerator(opt Options, pkg *packages.Package, fset *token.FileSet, automarshals *typeutil.Map) (*generator, error) {\n\t// Abort if there were any errors loading the package.\n\tvar errs []error\n\tfor _, err := range pkg.Errors {\n\t\terrs = append(errs, err)\n\t}\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Search every file in the package for types that embed the\n\t// weaver.AutoMarshal struct.\n\ttset := newTypeSet(pkg, automarshals, &typeutil.Map{})\n\tfor _, file := range pkg.Syntax {\n\t\tfilename := fset.Position(file.Package).Filename\n\t\tif filepath.Base(filename) == generatedCodeFile {\n\t\t\t// Ignore weaver_gen.go files.\n\t\t\tcontinue\n\t\t}\n\t\tts, err := findAutoMarshals(pkg, file)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t\tcontinue\n\t\t}\n\t\tfor _, t := range ts {\n\t\t\ttset.automarshalCandidates.Set(t, struct{}{})\n\t\t}\n\t}\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Just because a type embeds weaver.AutoMarshal doesn't mean we can\n\t// automatically marshal it. Some types, like `struct { x chan int }`, are\n\t// just not serializable. Here, we check that every type that embeds\n\t// weaver.AutoMarshal is actually serializable.\n\tfor _, t := range tset.automarshalCandidates.Keys() {\n\t\tn := t.(*types.Named)\n\t\tif err := errors.Join(tset.checkSerializable(n)...); err != nil {\n\t\t\terrs = append(errs, errorf(fset, n.Obj().Pos(), \"type %v is not serializable\\n%w\", t, err))\n\t\t\tcontinue\n\t\t}\n\t\ttset.automarshals.Set(t, struct{}{})\n\t}\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Find and process all components.\n\tcomponents := map[string]*component{}\n\tfor _, file := range pkg.Syntax {\n\t\tfilename := fset.Position(file.Package).Filename\n\t\tif filepath.Base(filename) == generatedCodeFile {\n\t\t\t// Ignore weaver_gen.go files.\n\t\t\tcontinue\n\t\t}\n\n\t\tfileComponents, err := findComponents(opt, pkg, file, tset)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t\tcontinue\n\t\t}\n\n\t\tfor _, c := range fileComponents {\n\t\t\t// Check for component duplicates, two components that embed the\n\t\t\t// same weaver.Implements[T].\n\t\t\t//\n\t\t\t// TODO(mwhittaker): This code relies on the fact that a component\n\t\t\t// interface and component implementation have to be in the same\n\t\t\t// package. If we lift this requirement, then this code will break.\n\t\t\tif existing, ok := components[c.fullIntfName()]; ok {\n\t\t\t\terrs = append(errs, errorf(pkg.Fset, c.impl.Obj().Pos(),\n\t\t\t\t\t\"Duplicate implementation for component %s, other declaration: %v\",\n\t\t\t\t\tc.fullIntfName(), fset.Position(existing.impl.Obj().Pos())))\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcomponents[c.fullIntfName()] = c\n\t\t}\n\t}\n\n\t// Find method attributes.\n\tfor _, file := range pkg.Syntax {\n\t\tfilename := fset.Position(file.Package).Filename\n\t\tif filepath.Base(filename) == generatedCodeFile {\n\t\t\t// Ignore weaver_gen.go files.\n\t\t\tcontinue\n\t\t}\n\t\tif err := findMethodAttributes(pkg, file, components); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\n\tif err := errors.Join(errs...); err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &generator{\n\t\tpkg:        pkg,\n\t\ttset:       tset,\n\t\tfileset:    fset,\n\t\tcomponents: maps.Values(components),\n\t}, nil\n}\n\n// findComponents returns the components in the provided file. For example,\n// findComponents will find and return the following component.\n//\n//\ttype something struct {\n//\t    weaver.Implements[SomeComponentType]\n//\t    ...\n//\t}\nfunc findComponents(opt Options, pkg *packages.Package, f *ast.File, tset *typeSet) ([]*component, error) {\n\tvar components []*component\n\tvar errs []error\n\tfor _, d := range f.Decls {\n\t\tgendecl, ok := d.(*ast.GenDecl)\n\t\tif !ok || gendecl.Tok != token.TYPE {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, spec := range gendecl.Specs {\n\t\t\tts, ok := spec.(*ast.TypeSpec)\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tcomponent, err := extractComponent(opt, pkg, f, tset, ts)\n\t\t\tif err != nil {\n\t\t\t\terrs = append(errs, err)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif component != nil {\n\t\t\t\tcomponents = append(components, component)\n\t\t\t}\n\t\t}\n\t}\n\treturn components, errors.Join(errs...)\n}\n\nfunc findMethodAttributes(pkg *packages.Package, f *ast.File, components map[string]*component) error {\n\t// Look for declarations of the form:\n\t//\tvar _ weaver.NotRetriable = Component.Method\n\tvar errs []error\n\tfor _, decl := range f.Decls {\n\t\tgendecl, ok := decl.(*ast.GenDecl)\n\t\tif !ok || gendecl.Tok != token.VAR {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, spec := range gendecl.Specs {\n\t\t\tvalspec, ok := spec.(*ast.ValueSpec)\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttypeAndValue, ok := pkg.TypesInfo.Types[valspec.Type]\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tt := typeAndValue.Type\n\t\t\tif !isWeaverNotRetriable(t) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfor _, val := range valspec.Values {\n\t\t\t\t// We allow non-blank vars for uniformity.\n\t\t\t\tcomp, method, ok := findComponentMethod(pkg, components, val)\n\t\t\t\tif !ok {\n\t\t\t\t\terrs = append(errs, errorf(pkg.Fset, valspec.Pos(), \"weaver.NonRetriable should only be assigned a value that identifies a method of a component implemented by this package\"))\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif comp.noretry == nil {\n\t\t\t\t\tcomp.noretry = map[string]struct{}{}\n\t\t\t\t}\n\t\t\t\tcomp.noretry[method] = struct{}{}\n\t\t\t}\n\t\t}\n\t}\n\treturn errors.Join(errs...)\n}\n\n// findComponentMethod returns the component and method if val is an expression of\n// the form C.M where C is a component listed in components and C has a method named M.\nfunc findComponentMethod(pkg *packages.Package, components map[string]*component, val ast.Expr) (*component, string, bool) {\n\tsel, ok := val.(*ast.SelectorExpr)\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tctypeval, ok := pkg.TypesInfo.Types[sel.X]\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tctype, ok := ctypeval.Type.(*types.Named)\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tcname := fullName(ctype)\n\tc, ok := components[cname]\n\tif !ok {\n\t\treturn nil, \"\", false\n\t}\n\tmethod := sel.Sel.Name\n\tfor _, m := range c.methods() {\n\t\tif m.Name() == method {\n\t\t\treturn c, method, true\n\t\t}\n\t}\n\treturn nil, \"\", false\n}\n\n// findAutoMarshals returns the types in the provided file which embed the\n// weaver.AutoMarshal struct.\nfunc findAutoMarshals(pkg *packages.Package, f *ast.File) ([]*types.Named, error) {\n\tvar automarshals []*types.Named\n\tvar errs []error\n\tfor _, decl := range f.Decls {\n\t\tgendecl, ok := decl.(*ast.GenDecl)\n\t\tif !ok || gendecl.Tok != token.TYPE {\n\t\t\t// This is not a type declaration.\n\t\t\tcontinue\n\t\t}\n\n\t\t// Recall that a type declaration can have multiple type specs. We have\n\t\t// to iterate over all of them. For example:\n\t\t//\n\t\t//     type (\n\t\t//         a struct{} // Spec 1\n\t\t//         b struct{} // Spec 2\n\t\t//     )\n\t\tfor _, spec := range gendecl.Specs {\n\t\t\ttypespec, ok := spec.(*ast.TypeSpec)\n\t\t\tif !ok {\n\t\t\t\tpanic(errorf(pkg.Fset, spec.Pos(), \"type declaration has non-TypeSpec spec: %v\", spec))\n\t\t\t}\n\n\t\t\t// Extract the type's name.\n\t\t\tdef, ok := pkg.TypesInfo.Defs[typespec.Name]\n\t\t\tif !ok {\n\t\t\t\tpanic(errorf(pkg.Fset, spec.Pos(), \"name %v not found\", typespec.Name))\n\t\t\t}\n\t\t\tn, ok := def.Type().(*types.Named)\n\t\t\tif !ok {\n\t\t\t\t// For type aliases like `type Int = int`, Int has type int and\n\t\t\t\t// not type Named. We ignore these.\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Check if the type of the expression is struct.\n\t\t\tt, ok := pkg.TypesInfo.Types[typespec.Type].Type.(*types.Struct)\n\t\t\tif !ok {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Check for an embedded weaver.AutoMarshal field.\n\t\t\tautomarshal := false\n\t\t\tfor i := 0; i < t.NumFields(); i++ {\n\t\t\t\tf := t.Field(i)\n\t\t\t\tif f.Embedded() && isWeaverAutoMarshal(f.Type()) {\n\t\t\t\t\tautomarshal = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !automarshal {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Ignore generic types. Generic types don't play well with\n\t\t\t// embedded AutoMarshals. For example, consider the following type\n\t\t\t// declaration:\n\t\t\t//\n\t\t\t//     type Register[A any] struct {\n\t\t\t//         weaver.AutoMarshal\n\t\t\t//         a A\n\t\t\t//     }\n\t\t\t//\n\t\t\t// Is Register[A] serializable? It depends on A. Plus, we cannot\n\t\t\t// really generate WeaverMarshal and WeaverUnmarshal methods for\n\t\t\t// specific instantiations of Register[A]. Because of these\n\t\t\t// complications, we ignore generic types.\n\t\t\t//\n\t\t\t// TODO(mwhittaker): Handle generics somehow?\n\t\t\tif n.TypeParams() != nil { // generics have non-nil TypeParams()\n\t\t\t\terrs = append(errs, errorf(pkg.Fset, spec.Pos(),\n\t\t\t\t\t\"generic struct %v cannot embed weaver.AutoMarshal. See serviceweaver.dev/docs.html#serializable-types for more information.\",\n\t\t\t\t\tformatType(pkg, n)))\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tautomarshals = append(automarshals, n)\n\t\t}\n\t}\n\treturn automarshals, errors.Join(errs...)\n}\n\n// extractComponent attempts to extract a component from the provided TypeSpec.\n// It returns a nil component if the TypeSpec doesn't define a component.\nfunc extractComponent(opt Options, pkg *packages.Package, file *ast.File, tset *typeSet, spec *ast.TypeSpec) (*component, error) {\n\t// Check that the type spec is of the form `type t struct {...}`.\n\ts, ok := spec.Type.(*ast.StructType)\n\tif !ok {\n\t\t// This type declaration does not involve a struct. For example, it\n\t\t// might look like `type t int`. These non-struct type declarations\n\t\t// cannot be components.\n\t\treturn nil, nil\n\t}\n\tdef, ok := pkg.TypesInfo.Defs[spec.Name]\n\tif !ok {\n\t\tpanic(errorf(pkg.Fset, spec.Pos(), \"name %v not found\", spec.Name))\n\t}\n\timpl, ok := def.Type().(*types.Named)\n\tif !ok {\n\t\t// For type aliases like `type t = struct{}`, t has type *types.Struct\n\t\t// and not type *types.Named. We ignore these.\n\t\treturn nil, nil\n\t}\n\n\t// Find any weaver.Implements[T] or weaver.WithRouter[T] embedded fields.\n\tvar intf *types.Named   // The component interface type\n\tvar router *types.Named // Router type (if any)\n\tvar isMain bool         // Is intf weaver.Main?\n\tvar refs []*types.Named // T for which weaver.Ref[T] exists in struct\n\tvar listeners []string  // Names of all listener fields declared in struct\n\tfor _, f := range s.Fields.List {\n\t\ttypeAndValue, ok := pkg.TypesInfo.Types[f.Type]\n\t\tif !ok {\n\t\t\tpanic(errorf(pkg.Fset, f.Pos(), \"type %v not found\", f.Type))\n\t\t}\n\t\tt := typeAndValue.Type\n\n\t\tif isWeaverRef(t) {\n\t\t\t// The field f has type weaver.Ref[T].\n\t\t\targ := t.(*types.Named).TypeArgs().At(0)\n\t\t\tif isWeaverMain(arg) {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"components cannot contain a reference to weaver.Main\")\n\t\t\t}\n\t\t\tnamed, ok := arg.(*types.Named)\n\t\t\tif !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Ref argument %s is not a named type.\",\n\t\t\t\t\tformatType(pkg, arg))\n\t\t\t}\n\t\t\trefs = append(refs, named)\n\t\t} else if isWeaverListener(t) {\n\t\t\tlis, err := getListenerNamesFromStructField(pkg, f)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tlisteners = append(listeners, lis...)\n\t\t}\n\n\t\tif len(f.Names) != 0 {\n\t\t\t// Ignore unembedded fields.\n\t\t\t//\n\t\t\t// TODO(mwhittaker): Warn the user about unembedded\n\t\t\t// weaver.Implements, weaver.WithConfig, or weaver.WithRouter?\n\t\t\tcontinue\n\t\t}\n\n\t\tswitch {\n\t\t// The field f is an embedded weaver.Implements[T].\n\t\tcase isWeaverImplements(t):\n\t\t\t// Check that T is a named interface type inside the package.\n\t\t\targ := t.(*types.Named).TypeArgs().At(0)\n\t\t\tnamed, ok := arg.(*types.Named)\n\t\t\tif !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Implements argument %s is not a named type.\",\n\t\t\t\t\tformatType(pkg, arg))\n\t\t\t}\n\t\t\tisMain = isWeaverMain(arg)\n\t\t\tif !isMain && named.Obj().Pkg() != pkg.Types {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Implements argument %s is a type outside the current package. A component interface and implementation must be in the same package. If you can't move them into the same package, you can add `type %s %v` to the implementation's package and embed `weaver.Implements[%s]` instead of `weaver.Implements[%s]`.\",\n\t\t\t\t\tformatType(pkg, named), named.Obj().Name(), formatType(pkg, named), named.Obj().Name(), formatType(pkg, named))\n\t\t\t}\n\t\t\tif _, ok := named.Underlying().(*types.Interface); !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.Implements argument %s is not an interface.\",\n\t\t\t\t\tformatType(pkg, named))\n\t\t\t}\n\t\t\tintf = named\n\n\t\t// The field f is an embedded weaver.WithRouter[T].\n\t\tcase isWeaverWithRouter(t):\n\t\t\t// Check that T is a named type inside the package.\n\t\t\targ := t.(*types.Named).TypeArgs().At(0)\n\t\t\tnamed, ok := arg.(*types.Named)\n\t\t\tif !ok {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.WithRouter argument %s is not a named type.\",\n\t\t\t\t\tformatType(pkg, arg))\n\t\t\t}\n\t\t\tif named.Obj().Pkg() != pkg.Types {\n\t\t\t\treturn nil, errorf(pkg.Fset, f.Pos(),\n\t\t\t\t\t\"weaver.WithRouter argument %s is a type outside the current package.\",\n\t\t\t\t\tformatType(pkg, named))\n\t\t\t}\n\t\t\trouter = named\n\t\t}\n\t}\n\n\tif intf == nil {\n\t\t// TODO(mwhittaker): Warn the user if they embed weaver.WithRouter or\n\t\t// weaver.WithConfig but don't embed weaver.Implements.\n\t\treturn nil, nil\n\t}\n\n\t// Check that that the component implementation implements the component\n\t// interface.\n\tif !types.Implements(types.NewPointer(impl), intf.Underlying().(*types.Interface)) {\n\t\treturn nil, errorf(pkg.Fset, spec.Pos(),\n\t\t\t\"type %s embeds weaver\n```\n# Function Signature: func uniqueName(t types.Type) string {\n# Natural Language Annotations: // uniqueName returns a unique pretty printed representation of the provided\n// type (e.g., \"int\", \"map[int]bool\"). The key property is that if u != t, then\n// uniqueName(u) != uniqueName(t).\n//\n// Note that types.TypeString returns a pretty printed representation of a\n// string, but it is not guaranteed to be unique. For example, if have `type\n// int bool`, then TypeString returns \"int\" for both the named type int and the\n// primitive type int.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc uniqueName(t types.Type) string {\n\tvar buf bytes.Buffer\n\twriteUniqueType(&buf, t)\n\treturn buf.String()\n}\n\nfunc writeUniqueType(buf *bytes.Buffer, t types.Type) {\n\tswitch t := t.(type) {\n\tcase *types.Basic:\n\t\tbuf.WriteString(t.Name())\n\tcase *types.Array:\n\t\tif name, ok := arrayTypeNames[t]; ok {\n\t\t\tbuf.WriteString(name)\n\t\t} else {\n\t\t\tname := fmt.Sprintf(\"array%x\", sha256.Sum256([]byte(t.String())))\n\t\t\tarrayTypeNames[t] = name\n\t\t\tbuf.WriteString(name)\n\t\t}\n\tcase *types.Slice:\n\t\tif name, ok := sliceTypeNames[t]; ok {\n\t\t\tbuf.WriteString(name)\n\t\t} else {\n\t\t\tname := fmt.Sprintf(\"slice%x\", sha256.Sum256([]byte(t.String())))\n\t\t\tsliceTypeNames[t] = name\n\t\t\tbuf.WriteString(name)\n\t\t}\n\tcase *types.Struct:\n\t\tif name, ok := structTypeNames[t]; ok {\n\t\t\tbuf.WriteString(name)\n\t\t} else {\n\t\t\tname := fmt.Sprintf(\"struct%x\", sha256.Sum256([]byte(t.String())))\n\t\t\tstructTypeNames[t] = name\n\t\t\tbuf.WriteString(name)\n\t\t}\n\tcase *types.Pointer:\n\t\tif name, ok := pointerTypeNames[t]; ok {\n\t\t\tbuf.WriteString(name)\n\t\t} else {\n\t\t\tname := fmt.Sprintf(\"pointer%x\", sha256.Sum256([]byte(t.String())))\n\t\t\tpointerTypeNames[t] = name\n\t\t\tbuf.WriteString(name)\n\t\t}\n\tcase *types.Tuple:\n\t\tif name, ok := tupleTypeNames[t]; ok {\n\t\t\tbuf.WriteString(name)\n\t\t} else {\n\t\t\tname := fmt.Sprintf(\"tuple%x\", sha256.Sum256([]byte(t.String())))\n\t\t\ttupleTypeNames[t] = name\n\t\t\tbuf.WriteString(name)\n\t\t}\n\tcase *types.Signature:\n\t\tif name, ok := signatureTypeNames[t]; ok {\n\t\t\tbuf.WriteString(name)\n\t\t} else {\n\t\t\tname := fmt.Sprintf(\"signature%x\", sha256.Sum256([]byte(t.String())))\n\t\t\tsignatureTypeNames[t] = name\n\t\t\tbuf.WriteString(name)\n\t\t}\n\tcase *types.Interface:\n\t\tif name, ok := interfaceTypeNames[t]; ok {\n\t\t\tbuf.WriteString(name)\n\t\t} else {\n\t\t\tname := fmt.Sprintf(\"interface%x\", sha256.Sum256([]byte(t.String())))\n\t\t\tinterfaceTypeNames[t] = name\n\t\t\tbuf.WriteString(name)\n\t\t}\n\tcase *types.Map:\n\t\tif name, ok := mapTypeNames[t]; ok {\n\t\t\tbuf.WriteString(name)\n\t\t} else {\n\t\t\tname := fmt.Sprintf(\"map%x\", sha256.Sum256([]byte(t.String())))\n\t\t\tmapTypeNames[t] = name\n\t\t\tbuf.WriteString(name)\n\t\t}\n\tcase *types.Chan:\n\t\tif name, ok := chanTypeNames[t]; ok {\n\t\t\tbuf.WriteString(name)\n\t\t} else {\n\t\t\tname := fmt.Sprintf(\"chan%x\", sha256.Sum256([]byte(t.String())))\n\t\t\tchanTypeNames[t] = name\n\t\t\tbuf.WriteString(name)\n\t\t}\n\tcase *types.Named:\n\t\tif name, ok := namedTypeNames[t]; ok {\n\t\t\tbuf.WriteString(name)\n\t\t} else {\n\t\t\tname := fmt.Sprintf(\"named%x\", sha256.Sum256([]byte(t.String())))\n\t\t\tnamedTypeNames[t] = name\n\t\t\tbuf.WriteString(name)\n\t\t}\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"unknown type %T\", t))\n\t}\n}\n```\nThis function `uniqueName` uses a map to store unique names for types. If a name for a type already exists in the map, it uses the existing name. If not, it generates a new unique name and stores it in the map. The unique name is computed by hashing the string representation of the type.", "```go\nfunc uniqueName(t types.Type) string {\n\tbuf := new(bytes.Buffer)\n\ttypes.WriteType(buf, t, func(*types.Package) string { return \"\" })\n\treturn buf.String()\n}\n```", "```go\nfunc uniqueName(t types.Type) string {\n\tnamedType, isNamed := t.(*types.Named)\n\tif isNamed {\n\t\treturn namedType.Obj().Name()\n\t}\n\n\tpointerType, isPointer := t.(*types.Pointer)\n\tif isPointer {\n\t\treturn \"*\" + uniqueName(pointerType.Elem())\n\t}\n\n\tsliceType, isSlice := t.(*types.Slice)\n\tif isSlice {\n\t\treturn \"[]\" + uniqueName(sliceType.Elem())\n\t}\n\n\tarrayType, isArray := t.(*types.Array)\n\tif isArray {\n\t\treturn fmt.Sprintf(\"[%d]%s\", arrayType.Len(), uniqueName(arrayType.Elem()))\n\t}\n\n\tmapType, isMap := t.(*types.Map)\n\tif isMap {\n\t\treturn fmt.Sprintf(\"map[%s]%s\", uniqueName(mapType.Key()), uniqueName(mapType.Elem()))\n\t}\n\n\tsignature, isSignature := t.(*types.Signature)\n\tif isSignature {\n\t\tvar params []string\n\t\tfor i := 0; i < signature.Params().Len(); i++ {\n\t\t\tparam := signature.Params().At(i)\n\t\t\tparams = append(params, uniqueName(param.Type()))\n\t\t}\n\t\tvar results []string\n\t\tfor i := 0; i < signature.Results().Len(); i++ {\n\t\t\tresult := signature.Results().At(i)\n\t\t\tresults = append(results, uniqueName(result.Type()))\n\t\t}\n\t\treturn fmt.Sprintf(\"func(%s) (%s)\", strings.Join(params, \", \"), strings.Join(results, \", \"))\n\t}\n\n\tinterfaceType, isInterface := t.(*types.Interface)\n\tif isInterface {\n\t\tvar methods []string\n\t\tfor i := 0; i < interfaceType.NumMethods(); i++ {\n\t\t\tmethod := interfaceType.Method(i)\n\t\t\tmethods = append(methods, method.Name()+uniqueName(method.Type()))\n\t\t}\n\t\treturn fmt.Sprintf(\"interface{%s}\", strings.Join(methods, \", \"))\n\t}\n\n\tbasicType, isBasic := t.(*types.Basic)\n\tif isBasic {\n\t\treturn basicType.Name()\n\t}\n\n\tchanType, isChan := t.(*types.Chan)\n\tif isChan {\n\t\tdir := \"chan\"\n\t\tif chanType.Dir() == types.RecvOnly {\n\t\t\tdir = \"<-chan\"\n\t\t} else if chanType.Dir() == types.SendOnly {\n\t\t\tdir = \"chan<-\"\n\t\t}\n\t\treturn fmt.Sprintf(\"%s %s\", dir, uniqueName(chanType.Elem()))\n\t}\n\n\ttypeParamType, isTypeParam := t.(*types.TypeParam)\n\tif isTypeParam {\n\t\treturn typeParamType.Obj().Name()\n\t}\n\n\treturn t.String()\n}\n```"]}, "weaver-internal/tool/certs/certs.go-GenerateSignedCert": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/tool/certs/certs.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage certs\n\nimport (\n\t\"bytes\"\n\t\"crypto\"\n\t\"crypto/rand\"\n\t\"crypto/rsa\"\n\t\"crypto/x509\"\n\t\"crypto/x509/pkix\"\n\t\"encoding/pem\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math/big\"\n\t\"time\"\n)\n\n// GenerateCACert generates a self-signed CA certificate and a corresponding\n// private key.\n//\n// The returned certificate has a one-year validity and is attributed to a fake\n// authority. As such, it should only ever be used on a temporary basis and for\n// in-process certificate signing.\nfunc GenerateCACert() (*x509.Certificate, crypto.PrivateKey, error) {\n\treturn generateLeafCert(true /*isCA*/, \"ca\")\n}\n\n// GenerateSignedCert generates a certificate for the given DNS names, signed\n// by the given Certificate Authority, and a corresponding private key.\n//\n// The returned certificate has a one-year validity and should only ever\n// be used on a temporary basis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc generateLeafCert(isCA bool, names ...string) (*x509.Certificate, crypto.PrivateKey, error) {\n\tpriv, err := rsa.GenerateKey(rand.Reader, 4096)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tserialNumberLimit := new(big.Int).Lsh(big.NewInt(1), 128)\n\tserialNumber, err := rand.Int(rand.Reader, serialNumberLimit)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tkeyUsage := x509.KeyUsageDigitalSignature | x509.KeyUsageKeyEncipherment\n\tif isCA {\n\t\tkeyUsage |= x509.KeyUsageCertSign\n\t}\n\ttemplate := x509.Certificate{\n\t\tSerialNumber: serialNumber,\n\t\tSubject:      pkix.Name{Organization: []string{\"ACME Co.\"}},\n\t\tNotBefore:    time.Now(),\n\t\tNotAfter:     time.Now().Add(365 * 24 * time.Hour),\n\t\tKeyUsage:     keyUsage,\n\t\tExtKeyUsage: []x509.ExtKeyUsage{\n\t\t\tx509.ExtKeyUsageClientAuth,\n\t\t\tx509.ExtKeyUsageServerAuth,\n\t\t},\n\t\tBasicConstraintsValid: true,\n\t\tIsCA:                  isCA,\n\t\tDNSNames:              names,\n\t}\n\tcertDER, err := x509.CreateCertificate(rand.Reader, &template, &template, &priv.PublicKey, priv)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tcert, err := x509.ParseCertificate(certDER)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn cert, priv, nil\n}\n\n// PEMEncode returns the PEM-encoded blocks for the given certificate and\n// private key.\nfunc PEMEncode(cert *x509.Certificate, key crypto.PrivateKey) ([]byte, []byte, error) {\n\tkeyDER, err := x509.MarshalPKCS8PrivateKey(key)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tvar certOut bytes.Buffer\n\tif err := pem.Encode(&certOut, &pem.Block{\n\t\tType:  \"CERTIFICATE\",\n\t\tBytes: cert.Raw,\n\t}); err != nil {\n\t\treturn nil, nil, err\n\t}\n\tvar keyOut bytes.Buffer\n\tif err := pem.Encode(&keyOut, &pem.Block{\n\t\tType:  \"PRIVATE KEY\",\n\t\tBytes: keyDER,\n\t}); err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn certOut.Bytes(), keyOut.Bytes(), nil\n}\n\n// VerifySignedCert verifies the given signed certificate using the given\n// root CA, returning the DNS names stored in the leaf certificate.\nfunc VerifySignedCert(certDER []byte, ca *x509.Certificate) ([]string, error) {\n\tcaCertPool := x509.NewCertPool()\n\tcaCertPool.AddCert(ca)\n\topts := x509.VerifyOptions{\n\t\tRoots:       caCertPool,\n\t\tCurrentTime: time.Now(),\n\t}\n\n\tcert, err := x509.ParseCertificate(certDER)\n\tif err != nil {\n\t\treturn nil, errors.New(\"bad certificate\")\n\t}\n\tverifiedCerts, err := cert.Verify(opts)\n\tif err != nil {\n\t\treturn nil, errors.New(\"couldn't verify certificate chain\")\n\t}\n\tif len(verifiedCerts) != 1 {\n\t\treturn nil, fmt.Errorf(\"expected a single verified chain, got %d\", len(verifiedCerts))\n\t}\n\tverifiedCert := verifiedCerts[0]\n\tif len(verifiedCert) < 1 { // should never happen\n\t\treturn nil, fmt.Errorf(\"empty verified chain\")\n\t}\n\treturn verifiedCert[0].DNSNames, nil\n}\n\n```\n# Function Signature: func GenerateSignedCert(ca *x509.Certificate, caKey crypto.PrivateKey, names ...string) (*x509.Certificate, crypto.PrivateKey, error) {\n# Natural Language Annotations: // GenerateSignedCert generates a certificate for the given DNS names, signed\n// by the given Certificate Authority, and a corresponding private key.\n//\n// The returned certificate has a one-year validity and should only ever\n// be used on a temporary basis.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc GenerateSignedCert(ca *x509.Certificate, caKey crypto.PrivateKey, names ...string) (*x509.Certificate, crypto.PrivateKey, error) {\n\tpriv, err := rsa.GenerateKey(rand.Reader, 4096)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tserialNumberLimit := new(big.Int).Lsh(big.NewInt(1), 128)\n\tserialNumber, err := rand.Int(rand.Reader, serialNumberLimit)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tkeyUsage := x509.KeyUsageDigitalSignature | x509.KeyUsageKeyEncipherment\n\ttemplate := x509.Certificate{\n\t\tSerialNumber: serialNumber,\n\t\tSubject:      pkix.Name{Organization: []string{\"ACME Co.\"}},\n\t\tNotBefore:    time.Now(),\n\t\tNotAfter:     time.Now().Add(365 * 24 * time.Hour),\n\t\tKeyUsage:     keyUsage,\n\t\tExtKeyUsage: []x509.ExtKeyUsage{\n\t\t\tx509.ExtKeyUsageClientAuth,\n\t\t\tx509.ExtKeyUsageServerAuth,\n\t\t},\n\t\tBasicConstraintsValid: true,\n\t\tIsCA:                  false,\n\t\tDNSNames:              names,\n\t}\n\tcertDER, err := x509.CreateCertificate(rand.Reader, &template, ca, &priv.PublicKey, caKey)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tcert, err := x509.ParseCertificate(certDER)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn cert, priv, nil\n}\n```\n\nThe `GenerateSignedCert` function has been updated to include the signature of the parent certificate and private key. The `isCA` parameter in the `generateLeafCert` function call has been removed, and the `ca` and `caKey` parameters are passed directly to `x509.CreateCertificate`. The DNS names are passed as a variadic argument to the function.", "```go\nfunc GenerateSignedCert(ca *x509.Certificate, caKey crypto.PrivateKey, names ...string) (*x509.Certificate, crypto.PrivateKey, error) {\n\tpriv, err := rsa.GenerateKey(rand.Reader, 4096)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tserialNumberLimit := new(big.Int).Lsh(big.NewInt(1), 128)\n\tserialNumber, err := rand.Int(rand.Reader, serialNumberLimit)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tkeyUsage := x509.KeyUsageDigitalSignature | x509.KeyUsageKeyEncipherment\n\ttemplate := x509.Certificate{\n\t\tSerialNumber: serialNumber,\n\t\tSubject:      pkix.Name{Organization: []string{\"ACME Co.\"}},\n\t\tNotBefore:    time.Now(),\n\t\tNotAfter:     time.Now().Add(365 * 24 * time.Hour),\n\t\tKeyUsage:     keyUsage,\n\t\tExtKeyUsage: []x509.ExtKeyUsage{\n\t\t\tx509.ExtKeyUsageClientAuth,\n\t\t\tx509.ExtKeyUsageServerAuth,\n\t\t},\n\t\tBasicConstraintsValid: true,\n\t\tIsCA:                  false,\n\t\tDNSNames:              names,\n\t}\n\tcertDER, err := x509.CreateCertificate(rand.Reader, &template, ca, &priv.PublicKey, caKey)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tcert, err := x509.ParseCertificate(certDER)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn cert, priv, nil\n}\n```", "```go\nfunc GenerateSignedCert(ca *x509.Certificate, caKey crypto.PrivateKey, names ...string) (*x509.Certificate, crypto.PrivateKey, error) {\n\tpriv, err := rsa.GenerateKey(rand.Reader, 4096)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tserialNumberLimit := new(big.Int).Lsh(big.NewInt(1), 128)\n\tserialNumber, err := rand.Int(rand.Reader, serialNumberLimit)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tkeyUsage := x509.KeyUsageDigitalSignature | x509.KeyUsageKeyEncipherment\n\ttemplate := x509.Certificate{\n\t\tSerialNumber: serialNumber,\n\t\tSubject:      pkix.Name{Organization: []string{\"ACME Co.\"}},\n\t\tNotBefore:    time.Now(),\n\t\tNotAfter:     time.Now().Add(365 * 24 * time.Hour),\n\t\tKeyUsage:     keyUsage,\n\t\tExtKeyUsage: []x509.ExtKeyUsage{\n\t\t\tx509.ExtKeyUsageClientAuth,\n\t\t\tx509.ExtKeyUsageServerAuth,\n\t\t},\n\t\tBasicConstraintsValid: true,\n\t\tIsCA:                  false,\n\t\tDNSNames:              names,\n\t}\n\tcertDER, err := x509.CreateCertificate(rand.Reader, &template, ca, &priv.PublicKey, caKey)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tcert, err := x509.ParseCertificate(certDER)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\treturn cert, priv, nil\n}\n```"]}, "weaver-internal/weaver/logger.go-run": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/weaver/logger.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage weaver\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/logging\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n)\n\n// remoteLogger collects log entries into batches and sends these batches to a\n// specified function.\ntype remoteLogger struct {\n\tc        chan *protos.LogEntry\n\tfallback io.Writer              // Fallback destination when dst() returns an error\n\tpp       *logging.PrettyPrinter // Used when sending to dst fails\n}\n\nconst logBufferCount = 1000\n\nfunc newRemoteLogger(fallback io.Writer) *remoteLogger {\n\trl := &remoteLogger{\n\t\tc:        make(chan *protos.LogEntry, logBufferCount),\n\t\tfallback: fallback,\n\t\tpp:       logging.NewPrettyPrinter(false),\n\t}\n\treturn rl\n}\n\nfunc (rl *remoteLogger) log(entry *protos.LogEntry) {\n\t// TODO(sanjay): Drop if too many entries are buffered?\n\trl.c <- entry\n}\n\n// run collects log entries passed to log() and, and passes theme to dst. At\n// most one call to dst is outstanding at a time. Log entries that arrive while\n// a call is in progress are buffered and sent in the next call.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (rl *remoteLogger) run(ctx context.Context, dst func(context.Context, *protos.LogEntryBatch) error) {\n# Natural Language Annotations: // run collects log entries passed to log() and, and passes theme to dst. At\n// most one call to dst is outstanding at a time. Log entries that arrive while\n// a call is in progress are buffered and sent in the next call.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (rl *remoteLogger) run(ctx context.Context, dst func(context.Context, *protos.LogEntryBatch) error) {\n\tgo func() {\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\tcase entry := <-rl.c:\n\t\t\t\trl.send(ctx, dst, entry)\n\t\t\t}\n\t\t}\n\t}()\n}\n\nfunc (rl *remoteLogger) send(ctx context.Context, dst func(context.Context, *protos.LogEntryBatch) error, entry *protos.LogEntry) {\n\tbatch := &protos.LogEntryBatch{Entries: []*protos.LogEntry{entry}}\n\tfor i := 0; i < logBufferCount-1; i++ {\n\t\tselect {\n\t\tcase entry := <-rl.c:\n\t\t\tbatch.Entries = append(batch.Entries, entry)\n\t\tdefault:\n\t\t\tbreak\n\t\t}\n\t}\n\n\terr := dst(ctx, batch)\n\tif err != nil {\n\t\tfor _, entry := range batch.Entries {\n\t\t\trl.pp.Print(rl.fallback, entry)\n\t\t}\n\t}\n}\n```", "```go\nfunc (rl *remoteLogger) run(ctx context.Context, dst func(context.Context, *protos.LogEntryBatch) error) {\n\t// Create a goroutine to collect log entries and send them to dst.\n\tgo func() {\n\t\tvar batch []*protos.LogEntry\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\t// Context has been cancelled, so stop sending log entries.\n\t\t\t\treturn\n\t\t\tcase entry := <-rl.c:\n\t\t\t\t// Append the log entry to the batch.\n\t\t\t\tbatch = append(batch, entry)\n\t\t\t\tif len(batch) == logBufferCount {\n\t\t\t\t\t// The batch is full, so send it to dst.\n\t\t\t\t\tif err := rl.sendBatch(ctx, dst, batch); err != nil {\n\t\t\t\t\t\t// If sending the batch to dst fails, write each log entry to the fallback writer.\n\t\t\t\t\t\tfor _, entry := range batch {\n\t\t\t\t\t\t\trl.pp.Fprint(rl.fallback, entry)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\t// Clear the batch.\n\t\t\t\t\tbatch = nil\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\t// If there are no log entries in the channel, send the batch to dst.\n\t\t\t\tif len(batch) > 0 {\n\t\t\t\t\tif err := rl.sendBatch(ctx, dst, batch); err != nil {\n\t\t\t\t\t\t// If sending the batch to dst fails, write each log entry to the fallback writer.\n\t\t\t\t\t\tfor _, entry := range batch {\n\t\t\t\t\t\t\trl.pp.Fprint(rl.fallback, entry)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\t// Clear the batch.\n\t\t\t\t\tbatch = nil\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n}\n\n// sendBatch sends a batch of log entries to dst and writes any errors to the fallback writer.\nfunc (rl *remoteLogger) sendBatch(ctx context.Context, dst func(context.Context, *protos.LogEntryBatch) error, batch []*protos.LogEntry) error {\n\tentryBatch := &protos.LogEntryBatch{Entries: batch}\n\tif err := dst(ctx, entryBatch); err != nil {\n\t\tfor _, entry := range batch {\n\t\t\trl.pp.Fprint(rl.fallback, entry)\n\t\t}\n\t\treturn err\n\t}\n\treturn nil\n}\n```", "```go\nfunc (rl *remoteLogger) run(ctx context.Context, dst func(context.Context, *protos.LogEntryBatch) error) {\n\tgo func() {\n\t\t// Create a buffer to collect log entries.\n\t\tbuf := make([]*protos.LogEntry, 0, logBufferCount)\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\t// The context is done, so we stop the goroutine.\n\t\t\t\treturn\n\t\t\tcase entry := <-rl.c:\n\t\t\t\t// Append the log entry to the buffer.\n\t\t\t\tbuf = append(buf, entry)\n\n\t\t\t\t// If the buffer is full, send it.\n\t\t\t\tif len(buf) == cap(buf) {\n\t\t\t\t\tif err := rl.send(ctx, dst, buf); err != nil {\n\t\t\t\t\t\t// If sending fails, log the error to the fallback writer.\n\t\t\t\t\t\trl.fallback.Write([]byte(fmt.Sprintf(\"Failed to send log entries: %v\\n\", err)))\n\t\t\t\t\t}\n\t\t\t\t\t// Reset the buffer.\n\t\t\t\t\tbuf = buf[:0]\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\t// If there are no log entries to collect, and there are\n\t\t\t\t// entries in the buffer, send them.\n\t\t\t\tif len(buf) > 0 {\n\t\t\t\t\tif err := rl.send(ctx, dst, buf); err != nil {\n\t\t\t\t\t\t// If sending fails, log the error to the fallback writer.\n\t\t\t\t\t\trl.fallback.Write([]byte(fmt.Sprintf(\"Failed to send log entries: %v\\n\", err)))\n\t\t\t\t\t}\n\t\t\t\t\t// Reset the buffer.\n\t\t\t\t\tbuf = buf[:0]\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n}\n\nfunc (rl *remoteLogger) send(ctx context.Context, dst func(context.Context, *protos.LogEntryBatch) error, entries []*protos.LogEntry) error {\n\t// Create a log entry batch.\n\tbatch := &protos.LogEntryBatch{Entries: entries}\n\n\t// Send the batch to the destination.\n\tif err := dst(ctx, batch); err != nil {\n\t\t// If sending fails, log the error to the fallback writer.\n\t\tfor _, entry := range entries {\n\t\t\trl.fallback.Write([]byte(rl.pp.Format(entry)))\n\t\t}\n\t\treturn err\n\t}\n\treturn nil\n}\n```"]}, "weaver-internal/weaver/load.go-updateAssignment": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/weaver/load.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage weaver\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/DataDog/hyperloglog\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/lightstep/varopt\"\n)\n\nfunc approxEqual(a, b float64) bool {\n\tconst float64EqualityThreshold = 1e-9\n\treturn math.Abs(a-b) <= float64EqualityThreshold\n}\n\n// TODO(mwhittaker): Right now, load collection is slow. It grabs a mutex\n// every time the load needs to be updated. Make this faster.\n\n// loadCollector collects load for a Service Weaver component. As an example, imagine we\n// have a load collector lc for a Service Weaver component that owns slices [0, 10) and\n// [100, 200). We add the following load over the course of a second.\n//\n//   - lc.Add(0, 1)\n//   - lc.Add(1, 1)\n//   - lc.Add(2, 1)\n//   - lc.Add(3, 1)\n//   - lc.Add(100, 1)\n//   - lc.Add(101, 1)\n//\n// The load collector will report a load of 4 requests per second on the slice\n// [0, 10) and a load of 2 requests per second on the slice [100, 200).\ntype loadCollector struct {\n\tcomponent string           // Service Weaver component\n\taddr      string           // dialable address found in assignments\n\tnow       func() time.Time // time.Now usually, but injected fake in tests\n\n\tmu         sync.Mutex               // guards the following fields\n\tassignment *protos.Assignment       // latest assignment\n\tindex      index                    // index on assignment\n\tstart      time.Time                // start of load collection\n\tslices     map[uint64]*sliceSummary // keyed by start of slice\n}\n\n// sliceSummary contains a summary of the observed keys and load of a slice for\n// a replica.\ntype sliceSummary struct {\n\tslice  slice                    // the slice\n\tload   float64                  // total load\n\tcount  *hyperloglog.HyperLogLog // counts distinct elements\n\tsample *varopt.Varopt[uint64]   // reservoir sample of keys\n}\n\n// newLoadCollector returns a new load collector. Note that load is collected\n// with respect to an assignment, so load won't be collected until\n// UpdateAssignment is called.\nfunc newLoadCollector(component string, addr string) *loadCollector {\n\treturn &loadCollector{\n\t\tcomponent: component,\n\t\taddr:      addr,\n\t\tnow:       func() time.Time { return time.Now() },\n\t\tstart:     time.Now(),\n\t\tslices:    map[uint64]*sliceSummary{},\n\t}\n}\n\n// add adds load for the provided key.\nfunc (lc *loadCollector) add(key uint64, v float64) error {\n\tif v != 1.0 {\n\t\tpanic(\"load != 1.0 not yet implemented\")\n\t}\n\n\t// Find the corresponding slice.\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tif lc.assignment == nil {\n\t\t// Load is reported with respect to a given assignment. If we don't\n\t\t// have an assignment yet, then we don't record the load.\n\t\treturn nil\n\t}\n\tslice, found := lc.index.find(key)\n\tif !found {\n\t\t// TODO(mwhittaker): It is currently possible to receive a request for\n\t\t// a key that is not in our current assignment. For example, a\n\t\t// different weavelet may have an older or newer version of the current\n\t\t// assignment and send us keys not in our current assignment. In the\n\t\t// future, we may want to catch these requests and discard them. For\n\t\t// now, we execute them.\n\t\treturn nil\n\t}\n\tif !slice.replicaSet[lc.addr] {\n\t\treturn nil\n\t}\n\n\tsummary, found := lc.slices[slice.start]\n\tif !found {\n\t\tvar err error\n\t\tsummary, err = newSliceSummary(slice)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlc.slices[slice.start] = summary\n\t}\n\n\t// Update the load.\n\tsummary.load += v\n\n\t// Update the count. Note that we compute a hash of our key before passing\n\t// it to the hyperloglog, even though the key is itself a hash. The reason\n\t// is that this slice represents only a small sliver of the total hash\n\t// space. To operate correctly, a hyperloglog assumes values are drawn\n\t// uniformly from the space of all uint32s, so if we feed the hyperloglog\n\t// values only from this slice, the count will be inaccurate.\n\t//\n\t// TODO(mwhittaker): Compute the hash outside of the lock?\n\t// TODO(mwhittaker): Use a different sketch?\n\t// TODO(mwhittaker): If the slice is small (< 1024), we can count the\n\t// number of distinct elements exactly. Don't use a hyperloglog here.\n\t// TODO(mwhittaker): Start with an exact count and only switch to a\n\t// hyperloglog if the number of unique elements gets too big?\n\tsummary.count.Add(hyperloglog.Murmur64(key))\n\n\t// Update the sample. Note that Add takes in a key and a weight, but we are\n\t// recording unweighted samples, so we use a constant weight of 1.0 for\n\t// every key.\n\tif _, err := summary.sample.Add(key, 1.0); err != nil {\n\t\treturn fmt.Errorf(\"cannot sample %d: %v\", key, err)\n\t}\n\treturn nil\n}\n\n// updateAssignment updates a load collector with the latest assignment. The\n// load reported by a load collector is always scoped to a single assignment.\n// A load report never spans more than one assignment. Thus, UpdateAssignment\n// also clears the load collector's accumulated load.\n\n\n\n\n\n\n\n\n\n\n// report returns a report of the collected load. If the load collector\n// doesn't have any collected load---this is possible if the load collector\n// doesn't have an assignment yet---then Report returns nil.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// reset resets the load collector. If you want to collect load over 5\n// minute windows, for example, call Reset every five minutes.\nfunc (lc *loadCollector) reset() {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tlc.start = lc.now()\n\tlc.slices = map[uint64]*sliceSummary{}\n}\n\n// newSliceSummary returns a new sliceSummary for the provided slice with\n// initially 0 load.\nfunc newSliceSummary(slice slice) (*sliceSummary, error) {\n\t// Initialize the hyperloglog. A hyperloglog with n registers uses roughly\n\t// n bytes of memory. We choose n=1024 so that every hyperloglog takes\n\t// about a kilobyte of memory. Given that a weavelet should manage a\n\t// moderate number of slices and components, the total memory usage of all\n\t// hyperloglogs should be relatively small. New's documentation also\n\t// suggests that n be a power of 2.\n\tcount, err := hyperloglog.New(1024)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Initialize the reservoir sample. A reservoir sample of size n stores at\n\t// most n keys, or roughly 8n bytes. As with the hyperloglogs, this should\n\t// lead to a modest memory usage.\n\t//\n\t// TODO(mwhittaker): Compute the expected errors in our estimates based on\n\t// the size of the sample.\n\t// TODO(mwhittaker): When we switch to range sharding, keys might be large\n\t// and 1000 keys might be too big.\n\tr := rand.New(rand.NewSource(time.Now().UnixNano()))\n\tsample := varopt.New[uint64](1000, r)\n\n\treturn &sliceSummary{slice: slice, count: count, sample: sample}, nil\n}\n\n// splits splits the slice into subslices with roughly even load.\nfunc (s *sliceSummary) splits(delta time.Duration) []*protos.LoadReport_SubsliceLoad {\n\t// Splits divides the slice into subslices of roughly even load. In the\n\t// normal case, Splits splits a slice into 20 subslices, each representing\n\t// 5% of the total load. If the number of samples is small, however, fewer\n\t// splits are used. Moreover, if adjacent splits are formed from a single\n\t// hot key, they are combined.\n\n\t// Materialize and sort the sample.\n\tk := s.sample.Size()\n\txs := make([]uint64, k)\n\tfor i := 0; i < k; i++ {\n\t\tx, _ := s.sample.Get(i)\n\t\txs[i] = x\n\t}\n\tsort.Slice(xs, func(i, j int) bool { return xs[i] < xs[j] })\n\n\t// Determine the number of splits. More splits is better, but if we don't\n\t// have many points in our sample, then using a large number of splits will\n\t// lead to inaccurate estimates.\n\tvar n int\n\tswitch {\n\tcase k < 10:\n\t\tn = 1 // 100%\n\tcase k < 50:\n\t\tn = 2 // 50%\n\tcase k < 100:\n\t\tn = 4 // 25%\n\tcase k < 250:\n\t\tn = 5 // 20%\n\tcase k < 500:\n\t\tn = 10 // 10%\n\tdefault:\n\t\tn = 20 // 5%\n\t}\n\n\t// Adjust the first subslice so that it starts at our slice boundary.\n\ttotalLoad := s.load / delta.Seconds()\n\tsplits := subslices(totalLoad, xs, n)\n\tsplits[0].Start = s.slice.start\n\n\t// Double check that the split loads sum to the total load.\n\tvar sum float64\n\tfor _, split := range splits {\n\t\tsum += split.Load\n\t}\n\tif !approxEqual(sum, totalLoad) {\n\t\tpanic(fmt.Sprintf(\"bad sum of split loads: got %f, want %f\", sum, totalLoad))\n\t}\n\n\treturn splits\n}\n\n// subslices returns n splits of the provided points with roughly the same\n// load. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}, n =\n// 4, and a load of 10.0, subslices will return the following four splits:\n//\n//   - {Start: 10, Load: 2.5} // [10, 30)\n//   - {Start: 30, Load: 2.5} // [30, 50)\n//   - {Start: 50, Load: 2.5} // [50, 70)\n//   - {Start: 70, Load: 2.5} // [70, infinity)\n//\n// The returned splits are as even as possible on a best effort basis.\n// subslices only guarantees that the returned splits are contiguous and\n// sorted.\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// percentiles returns n equally spaced percentiles of the provided sorted set\n// of points. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}\n// and n = 4, percentiles will return []uint64{10, 30, 50, 70} where\n//\n//   - 10 is the 0th percentile,\n//   - 30 is the 25th percentile,\n//   - 50 is the 50th percentile,\n//   - 70 is the 75th percentile,\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\nfunc percentiles(xs []uint64, n int) []uint64 {\n\tps := make([]uint64, n)\n\tfor i := 0; i < n; i++ {\n\t\tps[i] = xs[int(float64(i)/float64(n)*float64(len(xs)))]\n\t}\n\treturn ps\n}\n\n// index is a read-only search index of a protos.Assignment, optimized to\n// find the slice that contains a key.\ntype index []slice\n\n// slice is the segment [start, end) of the key space, along with a set of\n// assigned replicas.\ntype slice struct {\n\tstart      uint64          // start of slice, inclusive\n\tend        uint64          // end of slice, exclusive\n\treplicas   []string        // replicas assigned to this slice\n\treplicaSet map[string]bool // replicas assigned to this slice\n}\n\n// newIndex returns a new index of the provided assignment.\nfunc newIndex(proto *protos.Assignment) index {\n\tn := len(proto.Slices)\n\tslices := make([]slice, n)\n\tfor i := 0; i < n; i++ {\n\t\t// Gather the set of replicas.\n\t\treplicas := proto.Slices[i].Replicas\n\t\treplicaSet := make(map[string]bool, len(replicas))\n\t\tfor _, replica := range replicas {\n\t\t\treplicaSet[replica] = true\n\t\t}\n\n\t\t// Compute the end of the slice.\n\t\tvar end uint64 = math.MaxUint64\n\t\tif i < n-1 {\n\t\t\tend = proto.Slices[i+1].Start\n\t\t}\n\n\t\t// Form the slice.\n\t\tslices[i] = slice{\n\t\t\tstart:      proto.Slices[i].Start,\n\t\t\tend:        end,\n\t\t\treplicas:   replicas,\n\t\t\treplicaSet: replicaSet,\n\t\t}\n\t}\n\treturn slices\n}\n\n// find finds the slice that contains the given key in O(log n) time where n is\n// the number of slices in the assignment.\nfunc (ind index) find(key uint64) (slice, bool) {\n\ti := sort.Search(len(ind), func(i int) bool {\n\t\treturn key < ind[i].end\n\t})\n\tif i == len(ind) {\n\t\treturn slice{}, false\n\t}\n\treturn ind[i], true\n}\n\n```\n# Function Signature: func (lc *loadCollector) updateAssignment(assignment *protos.Assignment) {\n# Natural Language Annotations: // updateAssignment updates a load collector with the latest assignment. The\n// load reported by a load collector is always scoped to a single assignment.\n// A load report never spans more than one assignment. Thus, UpdateAssignment\n// also clears the load collector's accumulated load.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (lc *loadCollector) updateAssignment(assignment *protos.Assignment) {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tlc.assignment = assignment\n\tlc.index = newIndex(assignment)\n\tlc.reset()\n}\n```\nThis function updates the load collector with the latest assignment. The assignment is given as an argument and the load collector updates its internal state with this assignment. The assignment is converted into an index, which is used to find the slice for a given key, and the load collector is reset. The lock is used to ensure that the update is thread-safe.", "```go\nfunc (lc *loadCollector) updateAssignment(assignment *protos.Assignment) {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\n\tlc.assignment = assignment\n\tlc.index = newIndex(lc.assignment)\n\tlc.reset()\n}\n```", "```go\nfunc (lc *loadCollector) updateAssignment(assignment *protos.Assignment) {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tlc.assignment = assignment\n\tlc.index = newIndex(assignment)\n\tlc.start = lc.now()\n\tlc.slices = map[uint64]*sliceSummary{}\n}\n```"]}, "weaver-internal/weaver/load.go-report": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/weaver/load.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage weaver\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/DataDog/hyperloglog\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/lightstep/varopt\"\n)\n\nfunc approxEqual(a, b float64) bool {\n\tconst float64EqualityThreshold = 1e-9\n\treturn math.Abs(a-b) <= float64EqualityThreshold\n}\n\n// TODO(mwhittaker): Right now, load collection is slow. It grabs a mutex\n// every time the load needs to be updated. Make this faster.\n\n// loadCollector collects load for a Service Weaver component. As an example, imagine we\n// have a load collector lc for a Service Weaver component that owns slices [0, 10) and\n// [100, 200). We add the following load over the course of a second.\n//\n//   - lc.Add(0, 1)\n//   - lc.Add(1, 1)\n//   - lc.Add(2, 1)\n//   - lc.Add(3, 1)\n//   - lc.Add(100, 1)\n//   - lc.Add(101, 1)\n//\n// The load collector will report a load of 4 requests per second on the slice\n// [0, 10) and a load of 2 requests per second on the slice [100, 200).\ntype loadCollector struct {\n\tcomponent string           // Service Weaver component\n\taddr      string           // dialable address found in assignments\n\tnow       func() time.Time // time.Now usually, but injected fake in tests\n\n\tmu         sync.Mutex               // guards the following fields\n\tassignment *protos.Assignment       // latest assignment\n\tindex      index                    // index on assignment\n\tstart      time.Time                // start of load collection\n\tslices     map[uint64]*sliceSummary // keyed by start of slice\n}\n\n// sliceSummary contains a summary of the observed keys and load of a slice for\n// a replica.\ntype sliceSummary struct {\n\tslice  slice                    // the slice\n\tload   float64                  // total load\n\tcount  *hyperloglog.HyperLogLog // counts distinct elements\n\tsample *varopt.Varopt[uint64]   // reservoir sample of keys\n}\n\n// newLoadCollector returns a new load collector. Note that load is collected\n// with respect to an assignment, so load won't be collected until\n// UpdateAssignment is called.\nfunc newLoadCollector(component string, addr string) *loadCollector {\n\treturn &loadCollector{\n\t\tcomponent: component,\n\t\taddr:      addr,\n\t\tnow:       func() time.Time { return time.Now() },\n\t\tstart:     time.Now(),\n\t\tslices:    map[uint64]*sliceSummary{},\n\t}\n}\n\n// add adds load for the provided key.\nfunc (lc *loadCollector) add(key uint64, v float64) error {\n\tif v != 1.0 {\n\t\tpanic(\"load != 1.0 not yet implemented\")\n\t}\n\n\t// Find the corresponding slice.\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tif lc.assignment == nil {\n\t\t// Load is reported with respect to a given assignment. If we don't\n\t\t// have an assignment yet, then we don't record the load.\n\t\treturn nil\n\t}\n\tslice, found := lc.index.find(key)\n\tif !found {\n\t\t// TODO(mwhittaker): It is currently possible to receive a request for\n\t\t// a key that is not in our current assignment. For example, a\n\t\t// different weavelet may have an older or newer version of the current\n\t\t// assignment and send us keys not in our current assignment. In the\n\t\t// future, we may want to catch these requests and discard them. For\n\t\t// now, we execute them.\n\t\treturn nil\n\t}\n\tif !slice.replicaSet[lc.addr] {\n\t\treturn nil\n\t}\n\n\tsummary, found := lc.slices[slice.start]\n\tif !found {\n\t\tvar err error\n\t\tsummary, err = newSliceSummary(slice)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlc.slices[slice.start] = summary\n\t}\n\n\t// Update the load.\n\tsummary.load += v\n\n\t// Update the count. Note that we compute a hash of our key before passing\n\t// it to the hyperloglog, even though the key is itself a hash. The reason\n\t// is that this slice represents only a small sliver of the total hash\n\t// space. To operate correctly, a hyperloglog assumes values are drawn\n\t// uniformly from the space of all uint32s, so if we feed the hyperloglog\n\t// values only from this slice, the count will be inaccurate.\n\t//\n\t// TODO(mwhittaker): Compute the hash outside of the lock?\n\t// TODO(mwhittaker): Use a different sketch?\n\t// TODO(mwhittaker): If the slice is small (< 1024), we can count the\n\t// number of distinct elements exactly. Don't use a hyperloglog here.\n\t// TODO(mwhittaker): Start with an exact count and only switch to a\n\t// hyperloglog if the number of unique elements gets too big?\n\tsummary.count.Add(hyperloglog.Murmur64(key))\n\n\t// Update the sample. Note that Add takes in a key and a weight, but we are\n\t// recording unweighted samples, so we use a constant weight of 1.0 for\n\t// every key.\n\tif _, err := summary.sample.Add(key, 1.0); err != nil {\n\t\treturn fmt.Errorf(\"cannot sample %d: %v\", key, err)\n\t}\n\treturn nil\n}\n\n// updateAssignment updates a load collector with the latest assignment. The\n// load reported by a load collector is always scoped to a single assignment.\n// A load report never spans more than one assignment. Thus, UpdateAssignment\n// also clears the load collector's accumulated load.\n\n\n\n\n\n\n\n\n\n\n// report returns a report of the collected load. If the load collector\n// doesn't have any collected load---this is possible if the load collector\n// doesn't have an assignment yet---then Report returns nil.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// reset resets the load collector. If you want to collect load over 5\n// minute windows, for example, call Reset every five minutes.\nfunc (lc *loadCollector) reset() {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tlc.start = lc.now()\n\tlc.slices = map[uint64]*sliceSummary{}\n}\n\n// newSliceSummary returns a new sliceSummary for the provided slice with\n// initially 0 load.\nfunc newSliceSummary(slice slice) (*sliceSummary, error) {\n\t// Initialize the hyperloglog. A hyperloglog with n registers uses roughly\n\t// n bytes of memory. We choose n=1024 so that every hyperloglog takes\n\t// about a kilobyte of memory. Given that a weavelet should manage a\n\t// moderate number of slices and components, the total memory usage of all\n\t// hyperloglogs should be relatively small. New's documentation also\n\t// suggests that n be a power of 2.\n\tcount, err := hyperloglog.New(1024)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Initialize the reservoir sample. A reservoir sample of size n stores at\n\t// most n keys, or roughly 8n bytes. As with the hyperloglogs, this should\n\t// lead to a modest memory usage.\n\t//\n\t// TODO(mwhittaker): Compute the expected errors in our estimates based on\n\t// the size of the sample.\n\t// TODO(mwhittaker): When we switch to range sharding, keys might be large\n\t// and 1000 keys might be too big.\n\tr := rand.New(rand.NewSource(time.Now().UnixNano()))\n\tsample := varopt.New[uint64](1000, r)\n\n\treturn &sliceSummary{slice: slice, count: count, sample: sample}, nil\n}\n\n// splits splits the slice into subslices with roughly even load.\nfunc (s *sliceSummary) splits(delta time.Duration) []*protos.LoadReport_SubsliceLoad {\n\t// Splits divides the slice into subslices of roughly even load. In the\n\t// normal case, Splits splits a slice into 20 subslices, each representing\n\t// 5% of the total load. If the number of samples is small, however, fewer\n\t// splits are used. Moreover, if adjacent splits are formed from a single\n\t// hot key, they are combined.\n\n\t// Materialize and sort the sample.\n\tk := s.sample.Size()\n\txs := make([]uint64, k)\n\tfor i := 0; i < k; i++ {\n\t\tx, _ := s.sample.Get(i)\n\t\txs[i] = x\n\t}\n\tsort.Slice(xs, func(i, j int) bool { return xs[i] < xs[j] })\n\n\t// Determine the number of splits. More splits is better, but if we don't\n\t// have many points in our sample, then using a large number of splits will\n\t// lead to inaccurate estimates.\n\tvar n int\n\tswitch {\n\tcase k < 10:\n\t\tn = 1 // 100%\n\tcase k < 50:\n\t\tn = 2 // 50%\n\tcase k < 100:\n\t\tn = 4 // 25%\n\tcase k < 250:\n\t\tn = 5 // 20%\n\tcase k < 500:\n\t\tn = 10 // 10%\n\tdefault:\n\t\tn = 20 // 5%\n\t}\n\n\t// Adjust the first subslice so that it starts at our slice boundary.\n\ttotalLoad := s.load / delta.Seconds()\n\tsplits := subslices(totalLoad, xs, n)\n\tsplits[0].Start = s.slice.start\n\n\t// Double check that the split loads sum to the total load.\n\tvar sum float64\n\tfor _, split := range splits {\n\t\tsum += split.Load\n\t}\n\tif !approxEqual(sum, totalLoad) {\n\t\tpanic(fmt.Sprintf(\"bad sum of split loads: got %f, want %f\", sum, totalLoad))\n\t}\n\n\treturn splits\n}\n\n// subslices returns n splits of the provided points with roughly the same\n// load. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}, n =\n// 4, and a load of 10.0, subslices will return the following four splits:\n//\n//   - {Start: 10, Load: 2.5} // [10, 30)\n//   - {Start: 30, Load: 2.5} // [30, 50)\n//   - {Start: 50, Load: 2.5} // [50, 70)\n//   - {Start: 70, Load: 2.5} // [70, infinity)\n//\n// The returned splits are as even as possible on a best effort basis.\n// subslices only guarantees that the returned splits are contiguous and\n// sorted.\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// percentiles returns n equally spaced percentiles of the provided sorted set\n// of points. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}\n// and n = 4, percentiles will return []uint64{10, 30, 50, 70} where\n//\n//   - 10 is the 0th percentile,\n//   - 30 is the 25th percentile,\n//   - 50 is the 50th percentile,\n//   - 70 is the 75th percentile,\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\nfunc percentiles(xs []uint64, n int) []uint64 {\n\tps := make([]uint64, n)\n\tfor i := 0; i < n; i++ {\n\t\tps[i] = xs[int(float64(i)/float64(n)*float64(len(xs)))]\n\t}\n\treturn ps\n}\n\n// index is a read-only search index of a protos.Assignment, optimized to\n// find the slice that contains a key.\ntype index []slice\n\n// slice is the segment [start, end) of the key space, along with a set of\n// assigned replicas.\ntype slice struct {\n\tstart      uint64          // start of slice, inclusive\n\tend        uint64          // end of slice, exclusive\n\treplicas   []string        // replicas assigned to this slice\n\treplicaSet map[string]bool // replicas assigned to this slice\n}\n\n// newIndex returns a new index of the provided assignment.\nfunc newIndex(proto *protos.Assignment) index {\n\tn := len(proto.Slices)\n\tslices := make([]slice, n)\n\tfor i := 0; i < n; i++ {\n\t\t// Gather the set of replicas.\n\t\treplicas := proto.Slices[i].Replicas\n\t\treplicaSet := make(map[string]bool, len(replicas))\n\t\tfor _, replica := range replicas {\n\t\t\treplicaSet[replica] = true\n\t\t}\n\n\t\t// Compute the end of the slice.\n\t\tvar end uint64 = math.MaxUint64\n\t\tif i < n-1 {\n\t\t\tend = proto.Slices[i+1].Start\n\t\t}\n\n\t\t// Form the slice.\n\t\tslices[i] = slice{\n\t\t\tstart:      proto.Slices[i].Start,\n\t\t\tend:        end,\n\t\t\treplicas:   replicas,\n\t\t\treplicaSet: replicaSet,\n\t\t}\n\t}\n\treturn slices\n}\n\n// find finds the slice that contains the given key in O(log n) time where n is\n// the number of slices in the assignment.\nfunc (ind index) find(key uint64) (slice, bool) {\n\ti := sort.Search(len(ind), func(i int) bool {\n\t\treturn key < ind[i].end\n\t})\n\tif i == len(ind) {\n\t\treturn slice{}, false\n\t}\n\treturn ind[i], true\n}\n\n```\n# Function Signature: func (lc *loadCollector) report() *protos.LoadReport_ComponentLoad {\n# Natural Language Annotations: // report returns a report of the collected load. If the load collector\n// doesn't have any collected load---this is possible if the load collector\n// doesn't have an assignment yet---then Report returns nil.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (lc *loadCollector) report() *protos.LoadReport_ComponentLoad {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tif lc.assignment == nil {\n\t\treturn nil\n\t}\n\n\treport := &protos.LoadReport_ComponentLoad{\n\t\tComponent: lc.component,\n\t\tAddress:   lc.addr,\n\t\tLoads:     make([]*protos.LoadReport_SubsliceLoad, 0),\n\t}\n\n\tfor _, summary := range lc.slices {\n\t\tdelta := lc.now().Sub(lc.start)\n\t\treport.Loads = append(report.Loads, summary.splits(delta)...)\n\t}\n\n\treturn report\n}\n```\nThis function generates a load report for the component and its address. It calculates the time delta from the start of the load collection and uses this to split the load for each slice. If there are no collected loads, it returns nil.", "```go\nfunc (lc *loadCollector) report() *protos.LoadReport_ComponentLoad {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tif lc.assignment == nil {\n\t\t// Load is reported with respect to a given assignment. If we don't\n\t\t// have an assignment yet, then we don't record the load.\n\t\treturn nil\n\t}\n\n\treport := &protos.LoadReport_ComponentLoad{\n\t\tComponent: lc.component,\n\t\tLoad:      make(map[uint64]*protos.LoadReport_SliceLoad),\n\t}\n\n\tfor start, summary := range lc.slices {\n\t\tslice := summary.slice\n\t\tload := summary.load / lc.now().Sub(lc.start).Seconds()\n\t\tcount := uint64(summary.count.Count())\n\t\tsplits := summary.splits(lc.now().Sub(lc.start))\n\n\t\treport.Load[start] = &protos.LoadReport_SliceLoad{\n\t\t\tStart:       slice.start,\n\t\t\tEnd:         slice.end,\n\t\t\tLoad:        load,\n\t\t\tDistinctKey: count,\n\t\t\tSubsliceLoad: splits,\n\t\t}\n\t}\n\n\treturn report\n}\n```", "```go\nfunc (lc *loadCollector) report() *protos.LoadReport_ComponentLoad {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tif lc.assignment == nil {\n\t\treturn nil\n\t}\n\n\treport := &protos.LoadReport_ComponentLoad{\n\t\tComponent: lc.component,\n\t\tLoad:      make(map[uint64]*protos.LoadReport_SliceLoad),\n\t}\n\n\t// Compute the delta.\n\tdelta := lc.now().Sub(lc.start).Seconds()\n\n\tfor start, summary := range lc.slices {\n\t\tsliceLoad := &protos.LoadReport_SliceLoad{\n\t\t\tStart:     start,\n\t\t\tEnd:       summary.slice.end,\n\t\t\tLoad:      summary.load / delta,\n\t\t\tNumKeys:   int32(summary.count.Estimate()),\n\t\t\tSubslices: summary.splits(time.Duration(delta) * time.Second),\n\t\t}\n\t\treport.Load[start] = sliceLoad\n\t}\n\n\treturn report\n}\n```"]}, "weaver-internal/weaver/load.go-subslices": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/weaver/load.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage weaver\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"math/rand\"\n\t\"sort\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/DataDog/hyperloglog\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/lightstep/varopt\"\n)\n\nfunc approxEqual(a, b float64) bool {\n\tconst float64EqualityThreshold = 1e-9\n\treturn math.Abs(a-b) <= float64EqualityThreshold\n}\n\n// TODO(mwhittaker): Right now, load collection is slow. It grabs a mutex\n// every time the load needs to be updated. Make this faster.\n\n// loadCollector collects load for a Service Weaver component. As an example, imagine we\n// have a load collector lc for a Service Weaver component that owns slices [0, 10) and\n// [100, 200). We add the following load over the course of a second.\n//\n//   - lc.Add(0, 1)\n//   - lc.Add(1, 1)\n//   - lc.Add(2, 1)\n//   - lc.Add(3, 1)\n//   - lc.Add(100, 1)\n//   - lc.Add(101, 1)\n//\n// The load collector will report a load of 4 requests per second on the slice\n// [0, 10) and a load of 2 requests per second on the slice [100, 200).\ntype loadCollector struct {\n\tcomponent string           // Service Weaver component\n\taddr      string           // dialable address found in assignments\n\tnow       func() time.Time // time.Now usually, but injected fake in tests\n\n\tmu         sync.Mutex               // guards the following fields\n\tassignment *protos.Assignment       // latest assignment\n\tindex      index                    // index on assignment\n\tstart      time.Time                // start of load collection\n\tslices     map[uint64]*sliceSummary // keyed by start of slice\n}\n\n// sliceSummary contains a summary of the observed keys and load of a slice for\n// a replica.\ntype sliceSummary struct {\n\tslice  slice                    // the slice\n\tload   float64                  // total load\n\tcount  *hyperloglog.HyperLogLog // counts distinct elements\n\tsample *varopt.Varopt[uint64]   // reservoir sample of keys\n}\n\n// newLoadCollector returns a new load collector. Note that load is collected\n// with respect to an assignment, so load won't be collected until\n// UpdateAssignment is called.\nfunc newLoadCollector(component string, addr string) *loadCollector {\n\treturn &loadCollector{\n\t\tcomponent: component,\n\t\taddr:      addr,\n\t\tnow:       func() time.Time { return time.Now() },\n\t\tstart:     time.Now(),\n\t\tslices:    map[uint64]*sliceSummary{},\n\t}\n}\n\n// add adds load for the provided key.\nfunc (lc *loadCollector) add(key uint64, v float64) error {\n\tif v != 1.0 {\n\t\tpanic(\"load != 1.0 not yet implemented\")\n\t}\n\n\t// Find the corresponding slice.\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tif lc.assignment == nil {\n\t\t// Load is reported with respect to a given assignment. If we don't\n\t\t// have an assignment yet, then we don't record the load.\n\t\treturn nil\n\t}\n\tslice, found := lc.index.find(key)\n\tif !found {\n\t\t// TODO(mwhittaker): It is currently possible to receive a request for\n\t\t// a key that is not in our current assignment. For example, a\n\t\t// different weavelet may have an older or newer version of the current\n\t\t// assignment and send us keys not in our current assignment. In the\n\t\t// future, we may want to catch these requests and discard them. For\n\t\t// now, we execute them.\n\t\treturn nil\n\t}\n\tif !slice.replicaSet[lc.addr] {\n\t\treturn nil\n\t}\n\n\tsummary, found := lc.slices[slice.start]\n\tif !found {\n\t\tvar err error\n\t\tsummary, err = newSliceSummary(slice)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlc.slices[slice.start] = summary\n\t}\n\n\t// Update the load.\n\tsummary.load += v\n\n\t// Update the count. Note that we compute a hash of our key before passing\n\t// it to the hyperloglog, even though the key is itself a hash. The reason\n\t// is that this slice represents only a small sliver of the total hash\n\t// space. To operate correctly, a hyperloglog assumes values are drawn\n\t// uniformly from the space of all uint32s, so if we feed the hyperloglog\n\t// values only from this slice, the count will be inaccurate.\n\t//\n\t// TODO(mwhittaker): Compute the hash outside of the lock?\n\t// TODO(mwhittaker): Use a different sketch?\n\t// TODO(mwhittaker): If the slice is small (< 1024), we can count the\n\t// number of distinct elements exactly. Don't use a hyperloglog here.\n\t// TODO(mwhittaker): Start with an exact count and only switch to a\n\t// hyperloglog if the number of unique elements gets too big?\n\tsummary.count.Add(hyperloglog.Murmur64(key))\n\n\t// Update the sample. Note that Add takes in a key and a weight, but we are\n\t// recording unweighted samples, so we use a constant weight of 1.0 for\n\t// every key.\n\tif _, err := summary.sample.Add(key, 1.0); err != nil {\n\t\treturn fmt.Errorf(\"cannot sample %d: %v\", key, err)\n\t}\n\treturn nil\n}\n\n// updateAssignment updates a load collector with the latest assignment. The\n// load reported by a load collector is always scoped to a single assignment.\n// A load report never spans more than one assignment. Thus, UpdateAssignment\n// also clears the load collector's accumulated load.\n\n\n\n\n\n\n\n\n\n\n// report returns a report of the collected load. If the load collector\n// doesn't have any collected load---this is possible if the load collector\n// doesn't have an assignment yet---then Report returns nil.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// reset resets the load collector. If you want to collect load over 5\n// minute windows, for example, call Reset every five minutes.\nfunc (lc *loadCollector) reset() {\n\tlc.mu.Lock()\n\tdefer lc.mu.Unlock()\n\tlc.start = lc.now()\n\tlc.slices = map[uint64]*sliceSummary{}\n}\n\n// newSliceSummary returns a new sliceSummary for the provided slice with\n// initially 0 load.\nfunc newSliceSummary(slice slice) (*sliceSummary, error) {\n\t// Initialize the hyperloglog. A hyperloglog with n registers uses roughly\n\t// n bytes of memory. We choose n=1024 so that every hyperloglog takes\n\t// about a kilobyte of memory. Given that a weavelet should manage a\n\t// moderate number of slices and components, the total memory usage of all\n\t// hyperloglogs should be relatively small. New's documentation also\n\t// suggests that n be a power of 2.\n\tcount, err := hyperloglog.New(1024)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Initialize the reservoir sample. A reservoir sample of size n stores at\n\t// most n keys, or roughly 8n bytes. As with the hyperloglogs, this should\n\t// lead to a modest memory usage.\n\t//\n\t// TODO(mwhittaker): Compute the expected errors in our estimates based on\n\t// the size of the sample.\n\t// TODO(mwhittaker): When we switch to range sharding, keys might be large\n\t// and 1000 keys might be too big.\n\tr := rand.New(rand.NewSource(time.Now().UnixNano()))\n\tsample := varopt.New[uint64](1000, r)\n\n\treturn &sliceSummary{slice: slice, count: count, sample: sample}, nil\n}\n\n// splits splits the slice into subslices with roughly even load.\nfunc (s *sliceSummary) splits(delta time.Duration) []*protos.LoadReport_SubsliceLoad {\n\t// Splits divides the slice into subslices of roughly even load. In the\n\t// normal case, Splits splits a slice into 20 subslices, each representing\n\t// 5% of the total load. If the number of samples is small, however, fewer\n\t// splits are used. Moreover, if adjacent splits are formed from a single\n\t// hot key, they are combined.\n\n\t// Materialize and sort the sample.\n\tk := s.sample.Size()\n\txs := make([]uint64, k)\n\tfor i := 0; i < k; i++ {\n\t\tx, _ := s.sample.Get(i)\n\t\txs[i] = x\n\t}\n\tsort.Slice(xs, func(i, j int) bool { return xs[i] < xs[j] })\n\n\t// Determine the number of splits. More splits is better, but if we don't\n\t// have many points in our sample, then using a large number of splits will\n\t// lead to inaccurate estimates.\n\tvar n int\n\tswitch {\n\tcase k < 10:\n\t\tn = 1 // 100%\n\tcase k < 50:\n\t\tn = 2 // 50%\n\tcase k < 100:\n\t\tn = 4 // 25%\n\tcase k < 250:\n\t\tn = 5 // 20%\n\tcase k < 500:\n\t\tn = 10 // 10%\n\tdefault:\n\t\tn = 20 // 5%\n\t}\n\n\t// Adjust the first subslice so that it starts at our slice boundary.\n\ttotalLoad := s.load / delta.Seconds()\n\tsplits := subslices(totalLoad, xs, n)\n\tsplits[0].Start = s.slice.start\n\n\t// Double check that the split loads sum to the total load.\n\tvar sum float64\n\tfor _, split := range splits {\n\t\tsum += split.Load\n\t}\n\tif !approxEqual(sum, totalLoad) {\n\t\tpanic(fmt.Sprintf(\"bad sum of split loads: got %f, want %f\", sum, totalLoad))\n\t}\n\n\treturn splits\n}\n\n// subslices returns n splits of the provided points with roughly the same\n// load. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}, n =\n// 4, and a load of 10.0, subslices will return the following four splits:\n//\n//   - {Start: 10, Load: 2.5} // [10, 30)\n//   - {Start: 30, Load: 2.5} // [30, 50)\n//   - {Start: 50, Load: 2.5} // [50, 70)\n//   - {Start: 70, Load: 2.5} // [70, infinity)\n//\n// The returned splits are as even as possible on a best effort basis.\n// subslices only guarantees that the returned splits are contiguous and\n// sorted.\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// percentiles returns n equally spaced percentiles of the provided sorted set\n// of points. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}\n// and n = 4, percentiles will return []uint64{10, 30, 50, 70} where\n//\n//   - 10 is the 0th percentile,\n//   - 30 is the 25th percentile,\n//   - 50 is the 50th percentile,\n//   - 70 is the 75th percentile,\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\nfunc percentiles(xs []uint64, n int) []uint64 {\n\tps := make([]uint64, n)\n\tfor i := 0; i < n; i++ {\n\t\tps[i] = xs[int(float64(i)/float64(n)*float64(len(xs)))]\n\t}\n\treturn ps\n}\n\n// index is a read-only search index of a protos.Assignment, optimized to\n// find the slice that contains a key.\ntype index []slice\n\n// slice is the segment [start, end) of the key space, along with a set of\n// assigned replicas.\ntype slice struct {\n\tstart      uint64          // start of slice, inclusive\n\tend        uint64          // end of slice, exclusive\n\treplicas   []string        // replicas assigned to this slice\n\treplicaSet map[string]bool // replicas assigned to this slice\n}\n\n// newIndex returns a new index of the provided assignment.\nfunc newIndex(proto *protos.Assignment) index {\n\tn := len(proto.Slices)\n\tslices := make([]slice, n)\n\tfor i := 0; i < n; i++ {\n\t\t// Gather the set of replicas.\n\t\treplicas := proto.Slices[i].Replicas\n\t\treplicaSet := make(map[string]bool, len(replicas))\n\t\tfor _, replica := range replicas {\n\t\t\treplicaSet[replica] = true\n\t\t}\n\n\t\t// Compute the end of the slice.\n\t\tvar end uint64 = math.MaxUint64\n\t\tif i < n-1 {\n\t\t\tend = proto.Slices[i+1].Start\n\t\t}\n\n\t\t// Form the slice.\n\t\tslices[i] = slice{\n\t\t\tstart:      proto.Slices[i].Start,\n\t\t\tend:        end,\n\t\t\treplicas:   replicas,\n\t\t\treplicaSet: replicaSet,\n\t\t}\n\t}\n\treturn slices\n}\n\n// find finds the slice that contains the given key in O(log n) time where n is\n// the number of slices in the assignment.\nfunc (ind index) find(key uint64) (slice, bool) {\n\ti := sort.Search(len(ind), func(i int) bool {\n\t\treturn key < ind[i].end\n\t})\n\tif i == len(ind) {\n\t\treturn slice{}, false\n\t}\n\treturn ind[i], true\n}\n\n```\n# Function Signature: func subslices(load float64, xs []uint64, n int) []*protos.LoadReport_SubsliceLoad {\n# Natural Language Annotations: // subslices returns n splits of the provided points with roughly the same\n// load. For example, given xs = []uint64{10, 20, 30, 40, 50, 60, 70, 80}, n =\n// 4, and a load of 10.0, subslices will return the following four splits:\n//\n//   - {Start: 10, Load: 2.5} // [10, 30)\n//   - {Start: 30, Load: 2.5} // [30, 50)\n//   - {Start: 50, Load: 2.5} // [50, 70)\n//   - {Start: 70, Load: 2.5} // [70, infinity)\n//\n// The returned splits are as even as possible on a best effort basis.\n// subslices only guarantees that the returned splits are contiguous and\n// sorted.\n//\n// REQUIRES xs is sorted in increasing order\n// REQUIRES n > 0\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc subslices(load float64, xs []uint64, n int) []*protos.LoadReport_SubsliceLoad {\n\t// Create an array of n empty subslices\n\tsubslices := make([]*protos.LoadReport_SubsliceLoad, n)\n\tfor i := 0; i < n; i++ {\n\t\tsubslices[i] = &protos.LoadReport_SubsliceLoad{}\n\t}\n\n\t// If there are no points, return subslices with no load\n\tif len(xs) == 0 {\n\t\treturn subslices\n\t}\n\n\t// Compute the target load per slice\n\ttargetLoad := load / float64(n)\n\n\t// Initialize start and end of each slice\n\tfor i := 0; i < n; i++ {\n\t\tsubslices[i].Start = xs[i*len(xs)/n]\n\t\tif i < n-1 {\n\t\t\tsubslices[i].End = xs[(i+1)*len(xs)/n]\n\t\t} else {\n\t\t\tsubslices[i].End = math.MaxUint64\n\t\t}\n\t}\n\n\t// Compute the load per slice by counting the points in each slice\n\tfor i := 0; i < len(xs); i++ {\n\t\tfor j := 0; j < n; j++ {\n\t\t\tif xs[i] >= subslices[j].Start && (j == n-1 || xs[i] < subslices[j+1].Start) {\n\t\t\t\tsubslices[j].Load += targetLoad\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\t// Adjust the load of the last slice to ensure total load is equal to the input load\n\tsubslices[n-1].Load = load - targetLoad*(float64(n)-1)\n\n\treturn subslices\n}\n```", "```go\nfunc subslices(load float64, xs []uint64, n int) []*protos.LoadReport_SubsliceLoad {\n\tsplits := make([]*protos.LoadReport_SubsliceLoad, n)\n\tsplitLoad := load / float64(n)\n\n\tif len(xs) == 0 {\n\t\t// If no points, distribute load evenly among splits\n\t\tfor i := 0; i < n; i++ {\n\t\t\tsplits[i] = &protos.LoadReport_SubsliceLoad{\n\t\t\t\tLoad: splitLoad,\n\t\t\t}\n\t\t}\n\t\treturn splits\n\t}\n\n\t// Calculate the load for each percentile\n\tpercentiles := percentileLoad(xs, n)\n\n\t// Distribute the load among the splits\n\tstart := xs[0]\n\tfor i := 0; i < n; i++ {\n\t\t// Determine the end of the split\n\t\tvar end uint64\n\t\tif i == n-1 {\n\t\t\tend = math.MaxUint64\n\t\t} else {\n\t\t\tend = percentiles[i]\n\t\t}\n\n\t\t// Calculate the load for the split\n\t\tvar splitLoad float64\n\t\tif i == 0 {\n\t\t\tsplitLoad = load * float64(end-start) / float64(xs[len(xs)-1]-start)\n\t\t} else {\n\t\t\tsplitLoad = load * float64(end-start) / float64(xs[len(xs)-1]-xs[0])\n\t\t}\n\n\t\t// Create the split\n\t\tsplits[i] = &protos.LoadReport_SubsliceLoad{\n\t\t\tStart: start,\n\t\t\tEnd:   end,\n\t\t\tLoad:  splitLoad,\n\t\t}\n\n\t\t// Update the start of the next split\n\t\tstart = end\n\t}\n\n\treturn splits\n}\n\nfunc percentileLoad(xs []uint64, n int) []uint64 {\n\tpercentiles := make([]uint64, n)\n\tfor i := 0; i < n; i++ {\n\t\tpercentiles[i] = xs[int(float64(i)/float64(n)*float64(len(xs)))]\n\t}\n\treturn percentiles\n}\n```", "```go\nfunc subslices(load float64, xs []uint64, n int) []*protos.LoadReport_SubsliceLoad {\n\tsplits := make([]*protos.LoadReport_SubsliceLoad, n)\n\tif len(xs) == 0 {\n\t\tfor i := 0; i < n; i++ {\n\t\t\tsplits[i] = &protos.LoadReport_SubsliceLoad{\n\t\t\t\tStart: 0,\n\t\t\t\tLoad:  load / float64(n),\n\t\t\t}\n\t\t}\n\t\treturn splits\n\t}\n\n\t// Calculate the total number of keys in the sample.\n\tk := len(xs)\n\n\t// Calculate the load per key in the sample.\n\tloadPerKey := load / float64(k)\n\n\t// Calculate the percentiles of the sample.\n\tpercentiles := percentiles(xs, n)\n\n\t// Form the subslices.\n\tfor i := 0; i < n; i++ {\n\t\tif i == 0 {\n\t\t\tsplits[i] = &protos.LoadReport_SubsliceLoad{\n\t\t\t\tStart: xs[0],\n\t\t\t\tLoad:  loadPerKey * float64(percentiles[i]),\n\t\t\t}\n\t\t} else {\n\t\t\tsplits[i] = &protos.LoadReport_SubsliceLoad{\n\t\t\t\tStart: percentiles[i-1],\n\t\t\t\tLoad:  loadPerKey * float64(percentiles[i]-percentiles[i-1]),\n\t\t\t}\n\t\t}\n\n\t\tif i == n-1 {\n\t\t\tsplits[i].Load += loadPerKey * float64(k-percentiles[i])\n\t\t}\n\t}\n\n\treturn splits\n}\n\nfunc percentiles(xs []uint64, n int) []uint64 {\n\tps := make([]uint64, n)\n\tfor i := 0; i < n; i++ {\n\t\tps[i] = xs[int(float64(i)/float64(n)*float64(len(xs)))]\n\t}\n\treturn ps\n}\n```"]}, "weaver-internal/weaver/routing.go-update": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/weaver/routing.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage weaver\n\nimport (\n\t\"context\"\n\t\"crypto/tls\"\n\t\"math/rand\"\n\t\"sync\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/cond\"\n\t\"github.com/ServiceWeaver/weaver/internal/net/call\"\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n\t\"github.com/google/uuid\"\n)\n\n// routingBalancer balances requests according to a routing assignment.\ntype routingBalancer struct {\n\tbalancer  call.Balancer // balancer to use for non-routed calls\n\ttlsConfig *tls.Config   // tls config to use; may be nil.\n\n\tmu         sync.RWMutex\n\tassignment *protos.Assignment\n\tindex      index\n\n\t// Map from address to connection. We currently allow just one\n\t// connection per address.\n\t// Guarded by mu.\n\tconns map[string]call.ReplicaConnection\n}\n\n// newRoutingBalancer returns a new routingBalancer.\nfunc newRoutingBalancer(tlsConfig *tls.Config) *routingBalancer {\n\treturn &routingBalancer{\n\t\tbalancer:  call.RoundRobin(),\n\t\ttlsConfig: tlsConfig,\n\t\tconns:     map[string]call.ReplicaConnection{},\n\t}\n}\n\n// Add adds c to the set of connections we are balancing across.\nfunc (rb *routingBalancer) Add(c call.ReplicaConnection) {\n\trb.balancer.Add(c)\n\n\trb.mu.Lock()\n\tdefer rb.mu.Unlock()\n\trb.conns[c.Address()] = c\n}\n\n// Remove removes c from the set of connections we are balancing across.\nfunc (rb *routingBalancer) Remove(c call.ReplicaConnection) {\n\trb.balancer.Remove(c)\n\n\trb.mu.Lock()\n\tdefer rb.mu.Unlock()\n\tdelete(rb.conns, c.Address())\n}\n\n// update updates the balancer with the provided assignment\n\n\n\n\n\n\n\n\n\n\n\n\n// Pick implements the call.Balancer interface.\nfunc (rb *routingBalancer) Pick(opts call.CallOptions) (call.ReplicaConnection, bool) {\n\tif opts.ShardKey == 0 {\n\t\t// If the method we're calling is not sharded (which is guaranteed to\n\t\t// be true for nonsharded components), then the shard key is 0.\n\t\treturn rb.balancer.Pick(opts)\n\t}\n\n\t// Grab the current assignment. It's possible that the current assignment\n\t// changes between when we release the lock and when we pick an endpoint,\n\t// but using a slightly stale assignment is okay.\n\trb.mu.RLock()\n\tassignment := rb.assignment\n\tindex := rb.index\n\trb.mu.RUnlock()\n\n\tif assignment == nil {\n\t\t// There is no assignment. This is possible if we haven't received an\n\t\t// assignment from the assigner yet.\n\t\treturn rb.balancer.Pick(opts)\n\t}\n\n\tslice, ok := index.find(opts.ShardKey)\n\tif !ok {\n\t\t// TODO(mwhittaker): Shouldn't this be impossible. Understand better\n\t\t// when this happens.\n\t\treturn rb.balancer.Pick(opts)\n\t}\n\n\t// Search for an available ReplicConnection starting at a random offset.\n\t// TODO(sanjay):Precompute the set of available ReplicaConnections per slice.\n\toffset := rand.Intn(len(slice.replicas))\n\trb.mu.RLock()\n\tdefer rb.mu.RUnlock()\n\tfor i, n := 0, len(slice.replicas); i < n; i++ {\n\t\toffset++\n\t\tif offset == n {\n\t\t\toffset = 0\n\t\t}\n\t\tif c, ok := rb.conns[slice.replicas[offset]]; ok {\n\t\t\treturn c, true\n\t\t}\n\t}\n\treturn nil, false\n}\n\n// routingResolver is a dummy resolver that returns whatever endpoints are\n// passed to the update method.\ntype routingResolver struct {\n\tm         sync.Mutex      // guards all of the following fields\n\tchanged   cond.Cond       // fires when endpoints changes\n\tversion   *call.Version   // the current version of endpoints\n\tendpoints []call.Endpoint // the endpoints returned by Resolve\n}\n\n// newRoutingResolver returns a new routingResolver.\nfunc newRoutingResolver() *routingResolver {\n\tr := &routingResolver{\n\t\tversion: &call.Version{Opaque: call.Missing.Opaque},\n\t}\n\tr.changed.L = &r.m\n\treturn r\n}\n\n// IsConstant implements the call.Resolver interface.\nfunc (rr *routingResolver) IsConstant() bool { return false }\n\n// update updates the resolver with the provided endpoints.\nfunc (rr *routingResolver) update(endpoints []call.Endpoint) {\n\trr.m.Lock()\n\tdefer rr.m.Unlock()\n\trr.version = &call.Version{Opaque: uuid.New().String()}\n\trr.endpoints = endpoints\n\trr.changed.Broadcast()\n}\n\n// Resolve implements the call.Resolver interface.\nfunc (rr *routingResolver) Resolve(ctx context.Context, version *call.Version) ([]call.Endpoint, *call.Version, error) {\n\trr.m.Lock()\n\tdefer rr.m.Unlock()\n\n\tif version == nil {\n\t\treturn rr.endpoints, rr.version, nil\n\t}\n\n\tfor *version == *rr.version {\n\t\tif err := rr.changed.Wait(ctx); err != nil {\n\t\t\treturn nil, nil, err\n\t\t}\n\t}\n\treturn rr.endpoints, rr.version, nil\n}\n\n```\n# Function Signature: func (rb *routingBalancer) update(assignment *protos.Assignment) {\n# Natural Language Annotations: // update updates the balancer with the provided assignment\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (rb *routingBalancer) update(assignment *protos.Assignment) {\n\trb.mu.Lock()\n\tdefer rb.mu.Unlock()\n\trb.assignment = assignment\n\trb.index = index{slices: make([]slice, len(assignment.Slices))}\n\tfor i, s := range assignment.Slices {\n\t\trb.index.slices[i] = slice{\n\t\t\tstart:    s.Start,\n\t\t\tlimit:    s.Limit,\n\t\t\treplicas: s.Replicas,\n\t\t}\n\t}\n}\n```", "```go\nfunc (rb *routingBalancer) update(assignment *protos.Assignment) {\n\trb.mu.Lock()\n\tdefer rb.mu.Unlock()\n\trb.assignment = assignment\n\trb.index = index{slices: assignment.Slices}\n}\n```", "```go\nfunc (rb *routingBalancer) update(assignment *protos.Assignment) {\n\trb.mu.Lock()\n\tdefer rb.mu.Unlock()\n\trb.assignment = assignment\n\trb.index = index{slices: make([]slice, len(assignment.Slices))}\n\tfor i, slice := range assignment.Slices {\n\t\trb.index.slices[i] = slice\n\t}\n}\n```"]}, "weaver-internal/status/registry.go-NewRegistry": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/status/registry.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package status contains code for implementing status related commands like\n// \"weaver multi status\" and \"weaver single dashboard\".\npackage status\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"syscall\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/files\"\n\t\"github.com/ServiceWeaver/weaver/runtime/colors\"\n)\n\n// A Registry is a persistent collection of Service Weaver application metadata.\n//\n// Tools like \"weaver multi status\" and \"weaver multi dashboard\" use the registry\n// to know which applications are running and to fetch the status of the\n// running applications.\ntype Registry struct {\n\t// A Registry stores registrations as files in a directory. Every\n\t// registration r is stored in a JSON file called\n\t// {r.DeploymentId}.registration.json.\n\t//\n\t// TODO(mwhittaker): Store as protos instead of JSON?\n\tdir string\n\n\t// newClient returns a new status client that curls the provided address.\n\t// It is a field of Registry to enable dependency injection in\n\t// registry_test.go.\n\tnewClient func(string) Server\n}\n\n// A Registration contains basic metadata about a Service Weaver application.\ntype Registration struct {\n\tDeploymentId string // deployment id (e.g, \"eba18295\")\n\tApp          string // app name (e.g., \"todo\")\n\tAddr         string // status server (e.g., \"localhost:12345\")\n}\n\n// Rolodex returns a pretty-printed rolodex displaying the registration.\n//\n//\t\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n//\t\u2502 app        : collatz                              \u2502\n//\t\u2502 deployment : fdeeb059-825b-4606-9e99-e22e63e10552 \u2502\n//\t\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nfunc (r Registration) Rolodex() string {\n\t// Declare the contents.\n\ttype kv struct {\n\t\tkey string\n\t\tval colors.Text\n\t}\n\tprefix, suffix := formatId(r.DeploymentId)\n\tkvs := []kv{\n\t\t{\"app        :\", colors.Text{colors.Atom{S: r.App}}},\n\t\t{\"deployment :\", colors.Text{prefix, suffix}},\n\t}\n\n\tlength := func(t colors.Text) int {\n\t\tvar n int\n\t\tfor _, a := range t {\n\t\t\tn += len(a.S)\n\t\t}\n\t\treturn n\n\t}\n\n\t// Calculate widths.\n\tvalWidth := 0\n\tfor _, kv := range kvs {\n\t\tif length(kv.val) > valWidth {\n\t\t\tvalWidth = length(kv.val)\n\t\t}\n\t}\n\twidth := valWidth + len(kvs[0].key) + 5\n\n\t// Pretty print.\n\tvar b strings.Builder\n\tfmt.Fprintf(&b, \"\u256d%s\u256e\\n\", strings.Repeat(\"\u2500\", width-2))\n\tfor _, kv := range kvs {\n\t\ts := kv.val.String()\n\t\tfmt.Fprintf(&b, \"\u2502 %s %-*s \u2502\\n\", kv.key, valWidth+len(s)-length(kv.val), s)\n\t}\n\tfmt.Fprintf(&b, \"\u2570%s\u256f\\n\", strings.Repeat(\"\u2500\", width-2))\n\treturn b.String()\n}\n\n// NewRegistry returns a registry that persists data to the provided directory.\n\n\n\n\n\n\n\n\n\n\n\n\n// Register adds a registration to the registry.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Unregister removes a registration from the registry.\nfunc (r *Registry) Unregister(_ context.Context, deploymentId string) error {\n\tfilename := fmt.Sprintf(\"%s.registration.json\", deploymentId)\n\tfilename = filepath.Join(r.dir, filename)\n\tif err := os.Remove(filename); err != nil {\n\t\treturn fmt.Errorf(\"registry: remove %q: %w\", filename, err)\n\t}\n\treturn nil\n}\n\n// Get returns the Registration for the provided deployment. If the deployment\n// doesn't exist or is not active, a non-nil error is returned.\nfunc (r *Registry) Get(ctx context.Context, deploymentId string) (Registration, error) {\n\t// TODO(mwhittaker): r.list() reads and parses every registration file.\n\t// This is inefficient, as we could instead stop reading and parsing as\n\t// soon as we find the corresponding registration file. Even more\n\t// efficient, we could match the deploymentId to the filenames instead of\n\t// reading and parsing the files. Since the number of registrations is\n\t// small, and the size of every registration file is small, I don't think\n\t// these optimizations are urgently needed.\n\tregs, err := r.list()\n\tif err != nil {\n\t\treturn Registration{}, err\n\t}\n\tfor _, reg := range regs {\n\t\tif reg.DeploymentId != deploymentId {\n\t\t\tcontinue\n\t\t}\n\t\tif r.dead(ctx, reg) {\n\t\t\treturn Registration{}, fmt.Errorf(\"registry: deployment %q not found\", deploymentId)\n\t\t}\n\t\treturn reg, nil\n\t}\n\treturn Registration{}, fmt.Errorf(\"registry: deployment %q not found\", deploymentId)\n}\n\n// List returns all active Registrations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// list returns all registrations, dead or alive.\nfunc (r *Registry) list() ([]Registration, error) {\n\tentries, err := os.ReadDir(r.dir)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"registry: read dir %q: %w\", r.dir, err)\n\t}\n\n\tvar regs []Registration\n\tfor _, entry := range entries {\n\t\tif !strings.HasSuffix(entry.Name(), \".registration.json\") {\n\t\t\t// Ignore non-registration files in the registry directory.\n\t\t\tcontinue\n\t\t}\n\t\tfilename := filepath.Join(r.dir, entry.Name())\n\t\tbytes, err := os.ReadFile(filename)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"registry: read file %q: %w\", filename, err)\n\t\t}\n\t\tvar reg Registration\n\t\tif err := json.Unmarshal(bytes, &reg); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"registry: decode file %q: %w\", filename, err)\n\t\t}\n\t\tregs = append(regs, reg)\n\t}\n\treturn regs, nil\n}\n\n// dead returns whether the provided registration is associated with a\n// deployment that is definitely dead.\nfunc (r *Registry) dead(ctx context.Context, reg Registration) bool {\n\tstatus, err := r.newClient(reg.Addr).Status(ctx)\n\tswitch {\n\tcase errors.Is(err, syscall.ECONNREFUSED):\n\t\t// There is no status server for this deployment, so we consider\n\t\t// the deployment dead.\n\t\treturn true\n\tcase errors.Is(err, syscall.Errno(10061)):\n\t\t// The syscall.ECONNREFUSED doesn't work on Windows. Windows will\n\t\t// return WSAECONNREFUSED(syscall.Errno = 10061) when the connection is\n\t\t// refused.\n\t\treturn true\n\tcase err != nil:\n\t\t// Something went wrong. The deployment may be dead, but we're not 100%\n\t\t// sure, so we return false.\n\t\treturn false\n\tcase status.DeploymentId != reg.DeploymentId:\n\t\t// The status server for this deployment is dead and has been\n\t\t// superseded by a newer status server.\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n```\n# Function Signature: func NewRegistry(_ context.Context, dir string) (*Registry, error) {\n# Natural Language Annotations: // NewRegistry returns a registry that persists data to the provided directory.\nPlease return the generated code surrounded by ```\n", "response": ["!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"]}, "weaver-internal/status/registry.go-Register": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/status/registry.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package status contains code for implementing status related commands like\n// \"weaver multi status\" and \"weaver single dashboard\".\npackage status\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"syscall\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/files\"\n\t\"github.com/ServiceWeaver/weaver/runtime/colors\"\n)\n\n// A Registry is a persistent collection of Service Weaver application metadata.\n//\n// Tools like \"weaver multi status\" and \"weaver multi dashboard\" use the registry\n// to know which applications are running and to fetch the status of the\n// running applications.\ntype Registry struct {\n\t// A Registry stores registrations as files in a directory. Every\n\t// registration r is stored in a JSON file called\n\t// {r.DeploymentId}.registration.json.\n\t//\n\t// TODO(mwhittaker): Store as protos instead of JSON?\n\tdir string\n\n\t// newClient returns a new status client that curls the provided address.\n\t// It is a field of Registry to enable dependency injection in\n\t// registry_test.go.\n\tnewClient func(string) Server\n}\n\n// A Registration contains basic metadata about a Service Weaver application.\ntype Registration struct {\n\tDeploymentId string // deployment id (e.g, \"eba18295\")\n\tApp          string // app name (e.g., \"todo\")\n\tAddr         string // status server (e.g., \"localhost:12345\")\n}\n\n// Rolodex returns a pretty-printed rolodex displaying the registration.\n//\n//\t\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n//\t\u2502 app        : collatz                              \u2502\n//\t\u2502 deployment : fdeeb059-825b-4606-9e99-e22e63e10552 \u2502\n//\t\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nfunc (r Registration) Rolodex() string {\n\t// Declare the contents.\n\ttype kv struct {\n\t\tkey string\n\t\tval colors.Text\n\t}\n\tprefix, suffix := formatId(r.DeploymentId)\n\tkvs := []kv{\n\t\t{\"app        :\", colors.Text{colors.Atom{S: r.App}}},\n\t\t{\"deployment :\", colors.Text{prefix, suffix}},\n\t}\n\n\tlength := func(t colors.Text) int {\n\t\tvar n int\n\t\tfor _, a := range t {\n\t\t\tn += len(a.S)\n\t\t}\n\t\treturn n\n\t}\n\n\t// Calculate widths.\n\tvalWidth := 0\n\tfor _, kv := range kvs {\n\t\tif length(kv.val) > valWidth {\n\t\t\tvalWidth = length(kv.val)\n\t\t}\n\t}\n\twidth := valWidth + len(kvs[0].key) + 5\n\n\t// Pretty print.\n\tvar b strings.Builder\n\tfmt.Fprintf(&b, \"\u256d%s\u256e\\n\", strings.Repeat(\"\u2500\", width-2))\n\tfor _, kv := range kvs {\n\t\ts := kv.val.String()\n\t\tfmt.Fprintf(&b, \"\u2502 %s %-*s \u2502\\n\", kv.key, valWidth+len(s)-length(kv.val), s)\n\t}\n\tfmt.Fprintf(&b, \"\u2570%s\u256f\\n\", strings.Repeat(\"\u2500\", width-2))\n\treturn b.String()\n}\n\n// NewRegistry returns a registry that persists data to the provided directory.\n\n\n\n\n\n\n\n\n\n\n\n\n// Register adds a registration to the registry.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Unregister removes a registration from the registry.\nfunc (r *Registry) Unregister(_ context.Context, deploymentId string) error {\n\tfilename := fmt.Sprintf(\"%s.registration.json\", deploymentId)\n\tfilename = filepath.Join(r.dir, filename)\n\tif err := os.Remove(filename); err != nil {\n\t\treturn fmt.Errorf(\"registry: remove %q: %w\", filename, err)\n\t}\n\treturn nil\n}\n\n// Get returns the Registration for the provided deployment. If the deployment\n// doesn't exist or is not active, a non-nil error is returned.\nfunc (r *Registry) Get(ctx context.Context, deploymentId string) (Registration, error) {\n\t// TODO(mwhittaker): r.list() reads and parses every registration file.\n\t// This is inefficient, as we could instead stop reading and parsing as\n\t// soon as we find the corresponding registration file. Even more\n\t// efficient, we could match the deploymentId to the filenames instead of\n\t// reading and parsing the files. Since the number of registrations is\n\t// small, and the size of every registration file is small, I don't think\n\t// these optimizations are urgently needed.\n\tregs, err := r.list()\n\tif err != nil {\n\t\treturn Registration{}, err\n\t}\n\tfor _, reg := range regs {\n\t\tif reg.DeploymentId != deploymentId {\n\t\t\tcontinue\n\t\t}\n\t\tif r.dead(ctx, reg) {\n\t\t\treturn Registration{}, fmt.Errorf(\"registry: deployment %q not found\", deploymentId)\n\t\t}\n\t\treturn reg, nil\n\t}\n\treturn Registration{}, fmt.Errorf(\"registry: deployment %q not found\", deploymentId)\n}\n\n// List returns all active Registrations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// list returns all registrations, dead or alive.\nfunc (r *Registry) list() ([]Registration, error) {\n\tentries, err := os.ReadDir(r.dir)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"registry: read dir %q: %w\", r.dir, err)\n\t}\n\n\tvar regs []Registration\n\tfor _, entry := range entries {\n\t\tif !strings.HasSuffix(entry.Name(), \".registration.json\") {\n\t\t\t// Ignore non-registration files in the registry directory.\n\t\t\tcontinue\n\t\t}\n\t\tfilename := filepath.Join(r.dir, entry.Name())\n\t\tbytes, err := os.ReadFile(filename)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"registry: read file %q: %w\", filename, err)\n\t\t}\n\t\tvar reg Registration\n\t\tif err := json.Unmarshal(bytes, &reg); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"registry: decode file %q: %w\", filename, err)\n\t\t}\n\t\tregs = append(regs, reg)\n\t}\n\treturn regs, nil\n}\n\n// dead returns whether the provided registration is associated with a\n// deployment that is definitely dead.\nfunc (r *Registry) dead(ctx context.Context, reg Registration) bool {\n\tstatus, err := r.newClient(reg.Addr).Status(ctx)\n\tswitch {\n\tcase errors.Is(err, syscall.ECONNREFUSED):\n\t\t// There is no status server for this deployment, so we consider\n\t\t// the deployment dead.\n\t\treturn true\n\tcase errors.Is(err, syscall.Errno(10061)):\n\t\t// The syscall.ECONNREFUSED doesn't work on Windows. Windows will\n\t\t// return WSAECONNREFUSED(syscall.Errno = 10061) when the connection is\n\t\t// refused.\n\t\treturn true\n\tcase err != nil:\n\t\t// Something went wrong. The deployment may be dead, but we're not 100%\n\t\t// sure, so we return false.\n\t\treturn false\n\tcase status.DeploymentId != reg.DeploymentId:\n\t\t// The status server for this deployment is dead and has been\n\t\t// superseded by a newer status server.\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n```\n# Function Signature: func (r *Registry) Register(ctx context.Context, reg Registration) error {\n# Natural Language Annotations: // Register adds a registration to the registry.\nPlease return the generated code surrounded by ```\n", "response": ["!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"]}, "weaver-internal/status/registry.go-List": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/status/registry.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package status contains code for implementing status related commands like\n// \"weaver multi status\" and \"weaver single dashboard\".\npackage status\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"syscall\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/files\"\n\t\"github.com/ServiceWeaver/weaver/runtime/colors\"\n)\n\n// A Registry is a persistent collection of Service Weaver application metadata.\n//\n// Tools like \"weaver multi status\" and \"weaver multi dashboard\" use the registry\n// to know which applications are running and to fetch the status of the\n// running applications.\ntype Registry struct {\n\t// A Registry stores registrations as files in a directory. Every\n\t// registration r is stored in a JSON file called\n\t// {r.DeploymentId}.registration.json.\n\t//\n\t// TODO(mwhittaker): Store as protos instead of JSON?\n\tdir string\n\n\t// newClient returns a new status client that curls the provided address.\n\t// It is a field of Registry to enable dependency injection in\n\t// registry_test.go.\n\tnewClient func(string) Server\n}\n\n// A Registration contains basic metadata about a Service Weaver application.\ntype Registration struct {\n\tDeploymentId string // deployment id (e.g, \"eba18295\")\n\tApp          string // app name (e.g., \"todo\")\n\tAddr         string // status server (e.g., \"localhost:12345\")\n}\n\n// Rolodex returns a pretty-printed rolodex displaying the registration.\n//\n//\t\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n//\t\u2502 app        : collatz                              \u2502\n//\t\u2502 deployment : fdeeb059-825b-4606-9e99-e22e63e10552 \u2502\n//\t\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nfunc (r Registration) Rolodex() string {\n\t// Declare the contents.\n\ttype kv struct {\n\t\tkey string\n\t\tval colors.Text\n\t}\n\tprefix, suffix := formatId(r.DeploymentId)\n\tkvs := []kv{\n\t\t{\"app        :\", colors.Text{colors.Atom{S: r.App}}},\n\t\t{\"deployment :\", colors.Text{prefix, suffix}},\n\t}\n\n\tlength := func(t colors.Text) int {\n\t\tvar n int\n\t\tfor _, a := range t {\n\t\t\tn += len(a.S)\n\t\t}\n\t\treturn n\n\t}\n\n\t// Calculate widths.\n\tvalWidth := 0\n\tfor _, kv := range kvs {\n\t\tif length(kv.val) > valWidth {\n\t\t\tvalWidth = length(kv.val)\n\t\t}\n\t}\n\twidth := valWidth + len(kvs[0].key) + 5\n\n\t// Pretty print.\n\tvar b strings.Builder\n\tfmt.Fprintf(&b, \"\u256d%s\u256e\\n\", strings.Repeat(\"\u2500\", width-2))\n\tfor _, kv := range kvs {\n\t\ts := kv.val.String()\n\t\tfmt.Fprintf(&b, \"\u2502 %s %-*s \u2502\\n\", kv.key, valWidth+len(s)-length(kv.val), s)\n\t}\n\tfmt.Fprintf(&b, \"\u2570%s\u256f\\n\", strings.Repeat(\"\u2500\", width-2))\n\treturn b.String()\n}\n\n// NewRegistry returns a registry that persists data to the provided directory.\n\n\n\n\n\n\n\n\n\n\n\n\n// Register adds a registration to the registry.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Unregister removes a registration from the registry.\nfunc (r *Registry) Unregister(_ context.Context, deploymentId string) error {\n\tfilename := fmt.Sprintf(\"%s.registration.json\", deploymentId)\n\tfilename = filepath.Join(r.dir, filename)\n\tif err := os.Remove(filename); err != nil {\n\t\treturn fmt.Errorf(\"registry: remove %q: %w\", filename, err)\n\t}\n\treturn nil\n}\n\n// Get returns the Registration for the provided deployment. If the deployment\n// doesn't exist or is not active, a non-nil error is returned.\nfunc (r *Registry) Get(ctx context.Context, deploymentId string) (Registration, error) {\n\t// TODO(mwhittaker): r.list() reads and parses every registration file.\n\t// This is inefficient, as we could instead stop reading and parsing as\n\t// soon as we find the corresponding registration file. Even more\n\t// efficient, we could match the deploymentId to the filenames instead of\n\t// reading and parsing the files. Since the number of registrations is\n\t// small, and the size of every registration file is small, I don't think\n\t// these optimizations are urgently needed.\n\tregs, err := r.list()\n\tif err != nil {\n\t\treturn Registration{}, err\n\t}\n\tfor _, reg := range regs {\n\t\tif reg.DeploymentId != deploymentId {\n\t\t\tcontinue\n\t\t}\n\t\tif r.dead(ctx, reg) {\n\t\t\treturn Registration{}, fmt.Errorf(\"registry: deployment %q not found\", deploymentId)\n\t\t}\n\t\treturn reg, nil\n\t}\n\treturn Registration{}, fmt.Errorf(\"registry: deployment %q not found\", deploymentId)\n}\n\n// List returns all active Registrations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// list returns all registrations, dead or alive.\nfunc (r *Registry) list() ([]Registration, error) {\n\tentries, err := os.ReadDir(r.dir)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"registry: read dir %q: %w\", r.dir, err)\n\t}\n\n\tvar regs []Registration\n\tfor _, entry := range entries {\n\t\tif !strings.HasSuffix(entry.Name(), \".registration.json\") {\n\t\t\t// Ignore non-registration files in the registry directory.\n\t\t\tcontinue\n\t\t}\n\t\tfilename := filepath.Join(r.dir, entry.Name())\n\t\tbytes, err := os.ReadFile(filename)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"registry: read file %q: %w\", filename, err)\n\t\t}\n\t\tvar reg Registration\n\t\tif err := json.Unmarshal(bytes, &reg); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"registry: decode file %q: %w\", filename, err)\n\t\t}\n\t\tregs = append(regs, reg)\n\t}\n\treturn regs, nil\n}\n\n// dead returns whether the provided registration is associated with a\n// deployment that is definitely dead.\nfunc (r *Registry) dead(ctx context.Context, reg Registration) bool {\n\tstatus, err := r.newClient(reg.Addr).Status(ctx)\n\tswitch {\n\tcase errors.Is(err, syscall.ECONNREFUSED):\n\t\t// There is no status server for this deployment, so we consider\n\t\t// the deployment dead.\n\t\treturn true\n\tcase errors.Is(err, syscall.Errno(10061)):\n\t\t// The syscall.ECONNREFUSED doesn't work on Windows. Windows will\n\t\t// return WSAECONNREFUSED(syscall.Errno = 10061) when the connection is\n\t\t// refused.\n\t\treturn true\n\tcase err != nil:\n\t\t// Something went wrong. The deployment may be dead, but we're not 100%\n\t\t// sure, so we return false.\n\t\treturn false\n\tcase status.DeploymentId != reg.DeploymentId:\n\t\t// The status server for this deployment is dead and has been\n\t\t// superseded by a newer status server.\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n```\n# Function Signature: func (r *Registry) List(ctx context.Context) ([]Registration, error) {\n# Natural Language Annotations: // List returns all active Registrations.\nPlease return the generated code surrounded by ```\n", "response": ["!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!", "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"]}, "weaver-internal/routing/routing.go-FormatAssignment": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/routing/routing.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package routing includes utilities for routing and assignments. See\n// https://serviceweaver.dev/docs.html#routing for more information on routing.\npackage routing\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n)\n\n// FormatAssignment pretty formats the provided assignment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// EqualSlices returns an assignment with slices of roughly equal size.\n// Replicas are assigned to slices in a round robin fashion. The returned\n// assignment has a version of 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// nextPowerOfTwo returns the least power of 2 that is greater or equal to x.\nfunc nextPowerOfTwo(x int) int {\n\tswitch {\n\tcase x == 0:\n\t\treturn 1\n\tcase x&(x-1) == 0:\n\t\t// x is already power of 2.\n\t\treturn x\n\tdefault:\n\t\treturn int(math.Pow(2, math.Ceil(math.Log2(float64(x)))))\n\t}\n}\n\n```\n# Function Signature: func FormatAssignment(a *protos.Assignment) string {\n# Natural Language Annotations: // FormatAssignment pretty formats the provided assignment.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc FormatAssignment(a *protos.Assignment) string {\n\tif a == nil {\n\t\treturn \"nil\"\n\t}\n\n\tb := strings.Builder{}\n\tfmt.Fprintf(&b, \"Assignment{\")\n\n\t// Format the assignment fields here.\n\t// This is a placeholder and should be replaced with actual assignment field formatting logic.\n\n\tfmt.Fprintf(&b, \"}\")\n\n\treturn b.String()\n}\n```", "```go\nimport (\n\t\"fmt\"\n\t\"strings\"\n)\n\n// FormatAssignment pretty formats the provided assignment.\nfunc FormatAssignment(a *protos.Assignment) string {\n\tvar b strings.Builder\n\n\tb.WriteString(fmt.Sprintf(\"Assignment Version: %d\\n\", a.Version))\n\n\tfor _, slice := range a.Slices {\n\t\tb.WriteString(fmt.Sprintf(\"Slice %s\\n\", slice.Id))\n\t\tb.WriteString(fmt.Sprintf(\"  Nonces: %v\\n\", slice.Nonces))\n\t\tb.WriteString(fmt.Sprintf(\"  HealthyReplicas: %v\\n\", slice.HealthyReplicas))\n\t\tb.WriteString(fmt.Sprintf(\"  UnhealthyReplicas: %v\\n\", slice.UnhealthyReplicas))\n\t\tb.WriteString(fmt.Sprintf(\"  MissingReplicas: %v\\n\", slice.MissingReplicas))\n\t}\n\n\treturn b.String()\n}\n```", "```go\n// FormatAssignment pretty formats the provided assignment.\nfunc FormatAssignment(a *protos.Assignment) string {\n\tif a == nil {\n\t\treturn \"nil\"\n\t}\n\n\tvar b strings.Builder\n\tfmt.Fprintf(&b, \"Assignment{Version:%d Keys:%v Replicas:%v}\", a.Version, a.Keys, a.Replicas)\n\treturn b.String()\n}\n```"]}, "weaver-internal/routing/routing.go-EqualSlices": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/routing/routing.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package routing includes utilities for routing and assignments. See\n// https://serviceweaver.dev/docs.html#routing for more information on routing.\npackage routing\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n)\n\n// FormatAssignment pretty formats the provided assignment.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// EqualSlices returns an assignment with slices of roughly equal size.\n// Replicas are assigned to slices in a round robin fashion. The returned\n// assignment has a version of 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// nextPowerOfTwo returns the least power of 2 that is greater or equal to x.\nfunc nextPowerOfTwo(x int) int {\n\tswitch {\n\tcase x == 0:\n\t\treturn 1\n\tcase x&(x-1) == 0:\n\t\t// x is already power of 2.\n\t\treturn x\n\tdefault:\n\t\treturn int(math.Pow(2, math.Ceil(math.Log2(float64(x)))))\n\t}\n}\n\n```\n# Function Signature: func EqualSlices(replicas []string) *protos.Assignment {\n# Natural Language Annotations: // EqualSlices returns an assignment with slices of roughly equal size.\n// Replicas are assigned to slices in a round robin fashion. The returned\n// assignment has a version of 0.\nPlease return the generated code surrounded by ```\n", "response": ["Sure, here is the generated function based on the provided information:\n\n```go\n// EqualSlices returns an assignment with slices of roughly equal size.\n// Replicas are assigned to slices in a round robin fashion. The returned\n// assignment has a version of 0.\nfunc EqualSlices(replicas []string) *protos.Assignment {\n\t// Sort replicas to ensure the order is consistent.\n\tsort.Strings(replicas)\n\n\t// Calculate the number of slices.\n\tnumSlices := nextPowerOfTwo(len(replicas))\n\n\t// Create the assignment.\n\tassignment := &protos.Assignment{\n\t\tVersion: 0,\n\t\tAssignment: make([]*protos.Assignment_Replicas, numSlices),\n\t}\n\n\t// Assign replicas to slices in a round robin fashion.\n\tfor i, replica := range replicas {\n\t\tslice := i % numSlices\n\t\tassignment.Assignment[slice] = &protos.Assignment_Replicas{\n\t\t\tReplicas: append(assignment.Assignment[slice].Replicas, replica),\n\t\t}\n\t}\n\n\treturn assignment\n}\n```\n\nThis function works by first sorting the replicas to ensure consistent ordering. Then, it calculates the number of slices as the next power of 2 that is greater or equal to the number of replicas. It then creates an assignment with the specified number of slices. The replicas are then assigned to slices in a round robin fashion.", "```go\npackage routing\n\nimport (\n\t\"github.com/ServiceWeaver/weaver/runtime/protos\"\n)\n\n// EqualSlices returns an assignment with slices of roughly equal size.\n// Replicas are assigned to slices in a round robin fashion. The returned\n// assignment has a version of 0.\nfunc EqualSlices(replicas []string) *protos.Assignment {\n\tnumSlices := nextPowerOfTwo(len(replicas))\n\tslices := make([]*protos.Assignment_Slice, numSlices)\n\tfor i := range slices {\n\t\tslices[i] = &protos.Assignment_Slice{\n\t\t\tGroup: &protos.Assignment_Group{},\n\t\t}\n\t}\n\n\tfor i, replica := range replicas {\n\t\tslice := slices[i%numSlices]\n\t\tslice.Group.Replicas = append(slice.Group.Replicas, replica)\n\t}\n\n\treturn &protos.Assignment{\n\t\tSlices: slices,\n\t\tVersion: 0,\n\t}\n}\n```", "```go\n// EqualSlices returns an assignment with slices of roughly equal size.\n// Replicas are assigned to slices in a round robin fashion. The returned\n// assignment has a version of 0.\nfunc EqualSlices(replicas []string) *protos.Assignment {\n    // Sort the replicas for a deterministic round-robin assignment.\n    sort.Strings(replicas)\n\n    // Calculate the number of slices.\n    numSlices := nextPowerOfTwo(len(replicas))\n\n    // Create the assignment.\n    assignment := &protos.Assignment{\n        Slices: make([]*protos.Slice, numSlices),\n    }\n\n    // Assign replicas to slices in a round-robin fashion.\n    for i, replica := range replicas {\n        sliceIdx := i % numSlices\n        if assignment.Slices[sliceIdx] == nil {\n            assignment.Slices[sliceIdx] = &protos.Slice{Replicas: []string{}}\n        }\n        assignment.Slices[sliceIdx].Replicas = append(assignment.Slices[sliceIdx].Replicas, replica)\n    }\n\n    return assignment\n}\n```"]}, "weaver-internal/queue/queue.go-Pop": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/queue/queue.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage queue\n\nimport (\n\t\"context\"\n\t\"sync\"\n\n\t\"github.com/ServiceWeaver/weaver/internal/cond\"\n)\n\n// Queue is a thread-safe queue.\n//\n// Unlike a Go channel, Queue doesn't have any constraints on how many\n// elements can be in the queue.\ntype Queue[T any] struct {\n\tmu    sync.Mutex\n\telems []T\n\twait  *cond.Cond\n}\n\n// Push places elem at the back of the queue.\nfunc (q *Queue[T]) Push(elem T) {\n\tq.mu.Lock()\n\tdefer q.mu.Unlock()\n\tq.init()\n\tq.elems = append(q.elems, elem)\n\tq.wait.Signal()\n}\n\n// Pop removes the element from the front of the queue and returns it.\n// It blocks if the queue is empty.\n// It returns an error if the passed-in context is canceled.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// init initializes the queue.\n//\n// REQUIRES: q.mu is held\nfunc (q *Queue[T]) init() {\n\tif q.wait == nil {\n\t\tq.wait = cond.NewCond(&q.mu)\n\t}\n}\n\n```\n# Function Signature: func (q *Queue[T]) Pop(ctx context.Context) (elem T, err error) {\n# Natural Language Annotations: // Pop removes the element from the front of the queue and returns it.\n// It blocks if the queue is empty.\n// It returns an error if the passed-in context is canceled.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (q *Queue[T]) Pop(ctx context.Context) (elem T, err error) {\n\tq.mu.Lock()\n\tdefer q.mu.Unlock()\n\tq.init()\n\n\tfor len(q.elems) == 0 && err == nil {\n\t\terr = ctx.Err()\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n\t\tq.wait.Wait(ctx)\n\t\terr = ctx.Err()\n\t}\n\n\tif err != nil {\n\t\treturn\n\t}\n\n\telem = q.elems[0]\n\tq.elems = q.elems[1:]\n\treturn elem, nil\n}\n```", "Sure, here is a function based on the provided function signature and natural language annotations:\n\n```go\n// Pop removes the element from the front of the queue and returns it.\n// It blocks if the queue is empty.\n// It returns an error if the passed-in context is canceled.\nfunc (q *Queue[T]) Pop(ctx context.Context) (elem T, err error) {\n\tq.mu.Lock()\n\tdefer q.mu.Unlock()\n\tq.init()\n\tfor len(q.elems) == 0 {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn elem, ctx.Err()\n\t\tdefault:\n\t\t\tq.wait.Wait()\n\t\t}\n\t}\n\telem = q.elems[0]\n\tq.elems = q.elems[1:]\n\treturn elem, nil\n}\n```", "```go\nfunc (q *Queue[T]) Pop(ctx context.Context) (elem T, err error) {\n\tq.mu.Lock()\n\tdefer q.mu.Unlock()\n\tq.init()\n\tfor len(q.elems) == 0 {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn elem, ctx.Err()\n\t\tdefault:\n\t\t\tq.wait.Wait()\n\t\t}\n\t}\n\telem = q.elems[0]\n\tq.elems = q.elems[1:]\n\treturn elem, nil\n}\n```"]}, "weaver-internal/pipe/pipe.go-MakePipePair": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/pipe/pipe.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package pipe extends os.exec, making it easier to create pipes to subcommands.\npackage pipe\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"os/exec\"\n)\n\n// Cmd is drop-in replacement for exec.Cmd, extended with the Pipe method.\ntype Cmd struct {\n\t*exec.Cmd\n\tcloseAfterStart []io.Closer // closed after Start()\n\tcloseAfterWait  []io.Closer // closed after Wait()\n}\n\n// CommandContext returns a new Cmd. See exec.CommandContext for details.\nfunc CommandContext(ctx context.Context, name string, arg ...string) *Cmd {\n\treturn &Cmd{Cmd: exec.CommandContext(ctx, name, arg...)}\n}\n\n// PipePair holds a pair of pipes that can be used for bi-directional\n// communication with a child process.\ntype PipePair struct {\n\tParentReader io.ReadCloser  // Reader from which parent can read\n\tParentWriter io.WriteCloser // Writer to which parent can write\n\tChildReader  uintptr        // Descriptor from which child can read\n\tChildWriter  uintptr        // Descriptor to which child can write\n}\n\n// MakePipePair makes a pair of pipes that can be used for bi-directional\n// communication with the child process.\n//\n// Cmd.ExtraFiles should not be modified directly if MakePipePair is called.\n//\n// Wait will close ParentWriter automatically after seeing the command exit. A\n// caller need only close ParentWriter to force the pipe to close sooner. For\n// example, if the command being run will not exit until standard input is\n// closed, the caller must close ParentWriter.\n//\n// Wait will close ParentReader automatically after seeing the command exit, so\n// most callers need not close ParentReader themselves. It is thus incorrect to\n// call Wait before all reads from ParentReader have completed. For the same\n// reason, it is incorrect to use Run when using MakePipePair. See the\n// exec.Cmd.StdoutPipe example [1] for idiomatic usage.\n//\n// [1]: https://pkg.go.dev/os/exec#example-Cmd.StdoutPipe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (c *Cmd) registerPipe(local, remote *os.File) uintptr {\n\tc.closeAfterStart = append(c.closeAfterStart, remote)\n\tc.closeAfterWait = append(c.closeAfterWait, local)\n\treturn addInheritedFile(c.Cmd, remote)\n}\n\n// Start is identical to exec.Command.Start.\nfunc (c *Cmd) Start() error {\n\tif err := c.Cmd.Start(); err != nil {\n\t\treturn err\n\t}\n\tcloseAll(&c.closeAfterStart)\n\treturn nil\n}\n\n// Wait is identical to exec.Command.Wait.\nfunc (c *Cmd) Wait() error {\n\tif err := c.Cmd.Wait(); err != nil {\n\t\treturn err\n\t}\n\tcloseAll(&c.closeAfterWait)\n\treturn nil\n}\n\n// Cleanup cleans up any unused resources.\nfunc (c *Cmd) Cleanup() {\n\tcloseAll(&c.closeAfterStart)\n\tcloseAll(&c.closeAfterWait)\n}\n\nfunc closeAll(files *[]io.Closer) {\n\tfor _, f := range *files {\n\t\tf.Close()\n\t}\n\t*files = nil\n}\n\n```\n# Function Signature: func (c *Cmd) MakePipePair() (PipePair, error) {\n# Natural Language Annotations: // MakePipePair makes a pair of pipes that can be used for bi-directional\n// communication with the child process.\n//\n// Cmd.ExtraFiles should not be modified directly if MakePipePair is called.\n//\n// Wait will close ParentWriter automatically after seeing the command exit. A\n// caller need only close ParentWriter to force the pipe to close sooner. For\n// example, if the command being run will not exit until standard input is\n// closed, the caller must close ParentWriter.\n//\n// Wait will close ParentReader automatically after seeing the command exit, so\n// most callers need not close ParentReader themselves. It is thus incorrect to\n// call Wait before all reads from ParentReader have completed. For the same\n// reason, it is incorrect to use Run when using MakePipePair. See the\n// exec.Cmd.StdoutPipe example [1] for idiomatic usage.\n//\n// [1]: https://pkg.go.dev/os/exec#example-Cmd.StdoutPipe\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (c *Cmd) MakePipePair() (PipePair, error) {\n\tparentReader, childWriter, err := os.Pipe()\n\tif err != nil {\n\t\treturn PipePair{}, err\n\t}\n\tparentWriter, childReader, err := os.Pipe()\n\tif err != nil {\n\t\tc.closeAfterStart = append(c.closeAfterStart, parentReader)\n\t\tcloseAll(&c.closeAfterStart)\n\t\treturn PipePair{}, err\n\t}\n\tc.Cmd.ExtraFiles = append(c.Cmd.ExtraFiles, parentReader, parentWriter)\n\treturn PipePair{\n\t\tParentReader: parentReader,\n\t\tParentWriter: parentWriter,\n\t\tChildReader:  c.registerPipe(childReader, os.NewFile(uintptr(3+len(c.Cmd.ExtraFiles)-2), \"childReader\")),\n\t\tChildWriter:  c.registerPipe(childWriter, os.NewFile(uintptr(3+len(c.Cmd.ExtraFiles)-1), \"childWriter\")),\n\t}, nil\n}\n```\nThe `MakePipePair` function creates a pair of pipes using `os.Pipe` for the parent and child ends. It then appends the parent ends to `Cmd.ExtraFiles` and registers the child ends using the `registerPipe` method. The `PipePair` struct is then returned with the file descriptors for the parent and child ends.\n\nPlease note that this function should be called before starting the command with `Start`. Also, the `Close` method of the `ParentReader` and `ParentWriter` in the `PipePair` struct should be called when you are done with the pipes to avoid leaking file descriptors.", "```go\npackage pipe\n\nimport (\n\t\"io\"\n\t\"os\"\n)\n\nfunc (c *Cmd) MakePipePair() (PipePair, error) {\n\tparentR, childW, err := os.Pipe()\n\tif err != nil {\n\t\treturn PipePair{}, err\n\t}\n\n\tparentW, childR, err := os.Pipe()\n\tif err != nil {\n\t\tparentR.Close()\n\t\tchildW.Close()\n\t\treturn PipePair{}, err\n\t}\n\n\tc.Cmd.ExtraFiles = append(c.Cmd.ExtraFiles, childR, childW)\n\n\treturn PipePair{\n\t\tParentReader: parentR,\n\t\tParentWriter: parentW,\n\t\tChildReader:  c.registerPipe(childR, os.NewFile(uintptr(len(c.Cmd.ExtraFiles)-2), \"pipe\")),\n\t\tChildWriter:  c.registerPipe(childW, os.NewFile(uintptr(len(c.Cmd.ExtraFiles)-1), \"pipe\")),\n\t}, nil\n}\n```\n\nIn the `MakePipePair` method, we are creating two pairs of pipes using the `os.Pipe` function. The first pipe pair is used for writing to the child process, and the second is used for reading from the child process. We add the child ends of the pipes to the `Cmd.ExtraFiles` list, which tells the `exec.Cmd` to close these file descriptors after starting the process, but before executing the command. We then register the parent ends of the pipes with the `Cmd`, so they are closed when the process exits. The `PipePair` struct is then returned, containing the parent ends of the pipes and the file descriptors for the child ends.", "```go\nfunc (c *Cmd) MakePipePair() (PipePair, error) {\n\tparentReader, childWriter, err := os.Pipe()\n\tif err != nil {\n\t\treturn PipePair{}, fmt.Errorf(\"failed to create parent reader: %w\", err)\n\t}\n\n\tparentWriter, childReader, err := os.Pipe()\n\tif err != nil {\n\t\tchildWriter.Close()\n\t\treturn PipePair{}, fmt.Errorf(\"failed to create parent writer: %w\", err)\n\t}\n\n\tc.Cmd.ExtraFiles = append(c.Cmd.ExtraFiles, childReader, childWriter)\n\n\treturn PipePair{\n\t\tParentReader: parentReader,\n\t\tParentWriter: parentWriter,\n\t\tChildReader:  c.registerPipe(childReader, parentReader),\n\t\tChildWriter:  c.registerPipe(childWriter, parentWriter),\n\t}, nil\n}\n```"]}, "weaver-internal/register/writeonce.go-Write": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/register/writeonce.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package register implements a write-once register.\npackage register\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n)\n\n// WriteOnce is a concurrent write-once register.\ntype WriteOnce[T any] struct {\n\tmu      sync.Mutex\n\tc       sync.Cond\n\twritten bool\n\tval     T\n}\n\n// Write writes to the register, or panics if the register was already written.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// TryWrite tries to write to the register and returns if the write succeeds.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Read returns the value of the register, blocking until it is written.\n\n\n\n\n\n\n\n\n\n\n\n// init initializes the register. We have an init method rather than a\n// WriteOnce constructor so that the zero value of WriteOnce is valid.\n//\n// REQUIRES: w.mu is held.\nfunc (w *WriteOnce[T]) init() {\n\tif w.c.L == nil {\n\t\tw.c.L = &w.mu\n\t}\n}\n\n```\n# Function Signature: func (w *WriteOnce[T]) Write(val T) {\n# Natural Language Annotations: // Write writes to the register, or panics if the register was already written.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// Write writes to the register, or panics if the register was already written.\nfunc (w *WriteOnce[T]) Write(val T) {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\tw.init()\n\tif w.written {\n\t\tpanic(fmt.Sprintf(\"writeonce: register already written with value %v\", w.val))\n\t}\n\tw.val = val\n\tw.written = true\n\tw.c.Broadcast()\n}\n```", "```go\nfunc (w *WriteOnce[T]) Write(val T) {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\n\tif w.written {\n\t\tpanic(\"write once register already written\")\n\t}\n\n\tw.val = val\n\tw.written = true\n\tw.c.Broadcast()\n}\n```", "```go\nfunc (w *WriteOnce[T]) Write(val T) {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\n\tif !w.written {\n\t\tw.val = val\n\t\tw.written = true\n\t\tw.c.Broadcast()\n\t} else {\n\t\tpanic(fmt.Errorf(\"write once register has already been written to\"))\n\t}\n}\n```"]}, "weaver-internal/register/writeonce.go-TryWrite": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/register/writeonce.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package register implements a write-once register.\npackage register\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n)\n\n// WriteOnce is a concurrent write-once register.\ntype WriteOnce[T any] struct {\n\tmu      sync.Mutex\n\tc       sync.Cond\n\twritten bool\n\tval     T\n}\n\n// Write writes to the register, or panics if the register was already written.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// TryWrite tries to write to the register and returns if the write succeeds.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Read returns the value of the register, blocking until it is written.\n\n\n\n\n\n\n\n\n\n\n\n// init initializes the register. We have an init method rather than a\n// WriteOnce constructor so that the zero value of WriteOnce is valid.\n//\n// REQUIRES: w.mu is held.\nfunc (w *WriteOnce[T]) init() {\n\tif w.c.L == nil {\n\t\tw.c.L = &w.mu\n\t}\n}\n\n```\n# Function Signature: func (w *WriteOnce[T]) TryWrite(val T) bool {\n# Natural Language Annotations: // TryWrite tries to write to the register and returns if the write succeeds.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (w *WriteOnce[T]) TryWrite(val T) bool {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\n\tif w.written {\n\t\treturn false\n\t}\n\n\tw.init()\n\tw.val = val\n\tw.written = true\n\tw.c.Broadcast()\n\n\treturn true\n}\n```\nHere, the TryWrite function is trying to write to the register. If the register was already written, it returns false. Otherwise, it sets the value of the register, changes the 'written' flag to true, broadcasts on the condition variable, and returns true. The function is locked to prevent concurrent access.", "```go\nfunc (w *WriteOnce[T]) TryWrite(val T) bool {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\tw.init()\n\tif w.written {\n\t\treturn false\n\t}\n\tw.val = val\n\tw.written = true\n\tw.c.Broadcast()\n\treturn true\n}\n```", "```Go\n// TryWrite tries to write to the register and returns if the write succeeds.\nfunc (w *WriteOnce[T]) TryWrite(val T) bool {\n\tw.mu.Lock()\n\tif !w.written {\n\t\tw.val = val\n\t\tw.written = true\n\t\tw.c.Broadcast()\n\t\tw.mu.Unlock()\n\t\treturn true\n\t}\n\tw.mu.Unlock()\n\treturn false\n}\n```"]}, "weaver-internal/register/writeonce.go-Read": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/register/writeonce.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package register implements a write-once register.\npackage register\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n)\n\n// WriteOnce is a concurrent write-once register.\ntype WriteOnce[T any] struct {\n\tmu      sync.Mutex\n\tc       sync.Cond\n\twritten bool\n\tval     T\n}\n\n// Write writes to the register, or panics if the register was already written.\n\n\n\n\n\n\n\n\n\n\n\n\n\n// TryWrite tries to write to the register and returns if the write succeeds.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Read returns the value of the register, blocking until it is written.\n\n\n\n\n\n\n\n\n\n\n\n// init initializes the register. We have an init method rather than a\n// WriteOnce constructor so that the zero value of WriteOnce is valid.\n//\n// REQUIRES: w.mu is held.\nfunc (w *WriteOnce[T]) init() {\n\tif w.c.L == nil {\n\t\tw.c.L = &w.mu\n\t}\n}\n\n```\n# Function Signature: func (w *WriteOnce[T]) Read() T {\n# Natural Language Annotations: // Read returns the value of the register, blocking until it is written.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (w *WriteOnce[T]) Read() T {\n\tw.mu.Lock()\n\tdefer w.mu.Unlock()\n\n\tw.init()\n\n\tfor !w.written {\n\t\tw.c.Wait()\n\t}\n\n\treturn w.val\n}\n```", "```go\nfunc (w *WriteOnce[T]) Read() T {\n    w.mu.Lock()\n    for !w.written {\n        w.c.Wait()\n    }\n    defer w.mu.Unlock()\n    return w.val\n}\n```", "```go\nfunc (w *WriteOnce[T]) Read() T {\n\t// Lock the mutex to ensure that only one goroutine can access the register at a time.\n\tw.mu.Lock()\n\n\t// Initialize the condition variable if it hasn't been done yet.\n\tw.init()\n\n\t// Wait until the register is written.\n\tfor !w.written {\n\t\tw.c.Wait()\n\t}\n\n\t// Read the value from the register.\n\tval := w.val\n\n\t// Unlock the mutex.\n\tw.mu.Unlock()\n\n\t// Return the value from the register.\n\treturn val\n}\n```"]}, "weaver-internal/versioned/versioned.go-RLock": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/versioned/versioned.go:\n```\n// Copyright 2023 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage versioned\n\nimport (\n\t\"sync\"\n\n\t\"github.com/google/uuid\"\n)\n\n// Versioned[T] is a linearizable register storing a value of type T.\n// Each update to the value changes its unique (but not necessarily ordered)\n// version. Spurious version changes are possible, i.e., the version may change\n// even if the value hasn't.\n//\n// Like a sync.Mutex, Versioned should not be copied.\ntype Versioned[T any] struct {\n\tmu      sync.RWMutex\n\tchanged sync.Cond\n\tVal     T\n\tversion string\n}\n\nfunc Version[T any](val T) *Versioned[T] {\n\tv := &Versioned[T]{Val: val, version: uuid.New().String()}\n\tv.changed.L = &v.mu\n\treturn v\n}\n\n// Lock acquires the write lock.\nfunc (v *Versioned[T]) Lock() {\n\tv.mu.Lock()\n}\n\n// Unlock releases the write lock.\nfunc (v *Versioned[T]) Unlock() {\n\tv.version = uuid.New().String()\n\tv.changed.Broadcast()\n\tv.mu.Unlock()\n}\n\n// RLock waits until the current version is different than the passed-in\n// version, and then acquires the read lock and returns the new\n// version.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// RUnlock releases the read lock.\nfunc (v *Versioned[T]) RUnlock() {\n\tv.mu.RUnlock()\n}\n\n```\n# Function Signature: func (v *Versioned[T]) RLock(version string) string {\n# Natural Language Annotations: // RLock waits until the current version is different than the passed-in\n// version, and then acquires the read lock and returns the new\n// version.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (v *Versioned[T]) RLock(version string) string {\n\tv.mu.RLock()\n\tfor version == v.version {\n\t\tv.changed.Wait()\n\t}\n\treturn v.version\n}\n```", "```go\nfunc (v *Versioned[T]) RLock(version string) string {\n    var newVersion string\n\n    v.mu.RLock()\n    for v.version == version {\n        v.changed.Wait()\n    }\n    newVersion = v.version\n    v.mu.RUnlock()\n\n    v.mu.RLock()\n    return newVersion\n}\n```", "```go\nfunc (v *Versioned[T]) RLock(version string) string {\n\tv.mu.RLock()\n\tfor v.version == version {\n\t\tv.changed.Wait()\n\t}\n\tv.mu.RUnlock()\n\tv.mu.RLock()\n\treturn v.version\n}\n```"]}, "weaver-internal/net/call/stub.go-makeStubMethods": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/net/call/stub.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage call\n\nimport (\n\t\"context\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// stub holds information about a client stub to the remote component.\ntype stub struct {\n\tconn          Connection   // connection to talk to the remote component\n\tmethods       []stubMethod // per method info\n\ttracer        trace.Tracer // component tracer\n\tinjectRetries int          // Number of artificial retries per retriable call\n}\n\ntype stubMethod struct {\n\tkey   MethodKey // key for remote component method\n\tretry bool      // Whether or not the method should be retred\n}\n\nvar _ codegen.Stub = &stub{}\n\n// NewStub creates a client-side stub of the type matching reg. Calls on the stub are sent on\n// conn to the component with the specified name.\nfunc NewStub(name string, reg *codegen.Registration, conn Connection, tracer trace.Tracer, injectRetries int) codegen.Stub {\n\treturn &stub{\n\t\tconn:          conn,\n\t\tmethods:       makeStubMethods(name, reg),\n\t\ttracer:        tracer,\n\t\tinjectRetries: injectRetries,\n\t}\n}\n\n// Tracer implements the codegen.Stub interface.\nfunc (s *stub) Tracer() trace.Tracer {\n\treturn s.tracer\n}\n\n// Run implements the codegen.Stub interface.\nfunc (s *stub) Run(ctx context.Context, method int, args []byte, shardKey uint64) (result []byte, err error) {\n\tm := s.methods[method]\n\topts := CallOptions{\n\t\tRetry:    m.retry,\n\t\tShardKey: shardKey,\n\t}\n\tn := 1\n\tif m.retry {\n\t\tn += s.injectRetries\n\t}\n\tfor i := 0; i < n; i++ {\n\t\tresult, err = s.conn.Call(ctx, m.key, args, opts)\n\t\t// No backoff since these retries are fake ones injected for testing.\n\t}\n\treturn\n}\n\n// makeStubMethods returns a slice of stub methods for the component methods of reg.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func makeStubMethods(fullName string, reg *codegen.Registration) []stubMethod {\n# Natural Language Annotations: // makeStubMethods returns a slice of stub methods for the component methods of reg.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc makeStubMethods(fullName string, reg *codegen.Registration) []stubMethod {\n\tn := len(reg.Methods)\n\tmethods := make([]stubMethod, n)\n\tfor i, m := range reg.Methods {\n\t\tmethods[i] = stubMethod{\n\t\t\tkey: MethodKey{\n\t\t\t\tComponent: fullName,\n\t\t\t\tName:     m.Name,\n\t\t\t},\n\t\t\tretry: m.Retry,\n\t\t}\n\t}\n\treturn methods\n}\n```\nPlease note that `MethodKey` is a struct that represents the key for a remote component method. It contains two fields: `Component` and `Name`. The `Component` field is the name of the component (in this case, `fullName`), and the `Name` field is the name of the method.", "```go\nfunc makeStubMethods(fullName string, reg *codegen.Registration) []stubMethod {\n\tmethods := make([]stubMethod, len(reg.Methods))\n\tfor i, m := range reg.Methods {\n\t\tmethods[i] = stubMethod{\n\t\t\tkey: MethodKey{\n\t\t\t\tName:      fullName,\n\t\t\t\tOperation: m.Name,\n\t\t\t},\n\t\t\tretry: m.Retry,\n\t\t}\n\t}\n\treturn methods\n}\n```", "```go\nfunc makeStubMethods(fullName string, reg *codegen.Registration) []stubMethod {\n\tmethods := make([]stubMethod, len(reg.Methods))\n\tfor i, m := range reg.Methods {\n\t\tmethods[i] = stubMethod{\n\t\t\tkey: MethodKey{\n\t\t\t\tComponent: fullName,\n\t\t\t\tMethod:    m.Name,\n\t\t\t},\n\t\t\tretry: m.Retriable,\n\t\t}\n\t}\n\treturn methods\n}\n```"]}, "weaver-internal/net/call/call.go-Serve": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/net/call/call.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package call implements an RPC mechanism.\npackage call\n\n// # Overview\n//\n// RPCs are conveyed across a bidirectional connection. A connection carries\n// a sequence of messages in each direction. A message has the following\n// information:\n//\trequest-id\t-- A number that identifies a particular RPC\n//\tmessage-type\t-- E.g., request or response\n//\tlength\t\t-- How many payload bytes follow\n//\tpayload\t\t-- length bytes of payload\n// The payload format varies depending on the message-type.\n// See msg.go for details.\n//\n// # Server operation\n//\n// The server listens for connections (typically on a TCP socket). For\n// each accepted connection, it starts a readRequests() goroutine that\n// reads messages from that connection. When readRequests() gets a\n// request message, it starts a runHandler() goroutine. runHandler()\n// looks up the registered handler for the message, runs it, and sends\n// the response back over the connection.\n//\n// # Client operation\n//\n// For each newly discovered server, the client starts a manage() goroutine\n// that connects to server, and then reads messages from the connection. If the\n// network connection breaks, manage() reconnects (after a retry delay).\n//\n// When the client wants to send an RPC, it selects one of its server\n// connections to use, creates a call object, assigns it a new request-id, and\n// registers the object in a map in the connection. It then sends a request\n// message over the connection and waits for the call object to be marked as\n// done.\n//\n// When the response arrives, it is picked up by readAndProcessMessage().\n// readAndProcessMessage() finds the call object corresponding to the\n// request-id in the response, and marks the call object as done which\n// wakes up goroutine that initiated the RPC.\n//\n// If a client is constructed with a non-constant resolver, the client also\n// spawns a watchResolver goroutine that repeatedly calls Resolve on the\n// resolver to get notified of updates to the set of endpoints. When the\n// endpoints are updated, existing connections are retained, and stale\n// connections are transitioned to a \"draining\" state.\n//\n// New RPCs are never issued over draining connections, but the pending\n// requests on a draining connection are allowed to finish. As soon as a\n// draining connection has no active calls, the connection closes itself. If\n// the resolver later returns a new set of endpoints that includes a draining\n// connection that hasn't closed itself, the draining connection is turned\n// back into a normal connection.\nimport (\n\t\"bufio\"\n\t\"context\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/logging\"\n\t\"github.com/ServiceWeaver/weaver/runtime/retry\"\n\t\"go.opentelemetry.io/otel/codes\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// Connection allows a client to send RPCs.\ntype Connection interface {\n\t// Call makes an RPC over a Connection.\n\tCall(context.Context, MethodKey, []byte, CallOptions) ([]byte, error)\n\n\t// Close closes a connection. Pending invocations of Call are cancelled and\n\t// return an error. All future invocations of Call fail and return an error\n\t// immediately. Close can be called more than once.\n\tClose()\n}\n\n// Listener allows the server to accept RPCs.\ntype Listener interface {\n\tAccept() (net.Conn, *HandlerMap, error)\n\tClose() error\n\tAddr() net.Addr\n}\n\n// reconnectingConnection is the concrete client-side Connection implementation.\n// It automatically reconnects to the servers on first call or the first call\n// after a shutdown.\ntype reconnectingConnection struct {\n\topts ClientOptions\n\n\t// mu guards the following fields and some of the fields in the\n\t// clientConnections inside connections and draining.\n\tmu     sync.Mutex\n\tconns  map[string]*clientConnection\n\tclosed bool\n\n\tresolver       Resolver\n\tcancelResolver func()         // cancels the watchResolver goroutine\n\tresolverDone   sync.WaitGroup // used to wait for watchResolver to finish\n}\n\n// connState is the state of a clientConnection (connection to a particular\n// server replica). missing is a special state used for unknown servers. A\n// typical sequence of transitions is:\n//\n//\tmissing -> disconnected -> checking -> idle <-> active -> draining -> missing\n//\n// The events that can cause state transition are:\n//\n// - register: server has shown up in resolver results\n// - unregister: server has dropped from resolver results\n// - connected: a connection has been successfully made\n// - checked: connection has been successfully checked\n// - callstart: call starts on connection\n// - lastdone: last active call on connection has ended\n// - fail: some protocol error is detected on the connection\n// - close: reconnectingConnection is being closed\n//\n// Each event has a corresponding clientConnection method below. See\n// those methods for the corresponding state transitions.\ntype connState int8\n\nconst (\n\tmissing      connState = iota\n\tdisconnected           // cannot be used for calls\n\tchecking               // checking new network connection\n\tidle                   // can be used for calls, no calls in-flight\n\tactive                 // can be used for calls, some calls in-flight\n\tdraining               // some calls in-flight, no new calls should be added\n)\n\nvar connStateNames = []string{\n\t\"missing\",\n\t\"disconnected\",\n\t\"checking\",\n\t\"idle\",\n\t\"active\",\n\t\"draining\",\n}\n\nfunc (s connState) String() string { return connStateNames[s] }\n\n// clientConnection manages one network connection on the client-side.\ntype clientConnection struct {\n\t// Immutable after construction.\n\trc       *reconnectingConnection // owner\n\tcanceler func()                  // Used to cancel goroutine handling connection\n\tlogger   *slog.Logger\n\tendpoint Endpoint\n\n\twlock sync.Mutex // Guards writes to c\n\n\t// Guarded by rc.mu\n\tstate          connState        // current connection state\n\tloggedShutdown bool             // Have we logged a shutdown error?\n\tinBalancer     bool             // Is c registered with the balancer?\n\tc              net.Conn         // Active network connection, or nil\n\tcbuf           *bufio.Reader    // Buffered reader wrapped around c\n\tversion        version          // Version number to use for connection\n\tcalls          map[uint64]*call // In-progress calls\n\tlastID         uint64           // Last assigned request ID for a call\n}\n\nvar _ ReplicaConnection = &clientConnection{}\n\n// call holds the state for an active call at the client.\ntype call struct {\n\tid         uint64\n\tdoneSignal chan struct{}\n\n\t// Fields below are accessed across goroutines, but their access is\n\t// synchronized via doneSignal, i.e., it is never concurrent.\n\terr      error\n\tresponse []byte\n\n\t// Is the call done?\n\t// This field is accessed across goroutines using atomics.\n\tdone uint32 // is the call done?\n\n}\n\n// serverConnection manages one network connection on the server-side.\ntype serverConnection struct {\n\topts        ServerOptions\n\tc           net.Conn\n\tcbuf        *bufio.Reader // Buffered reader wrapped around c\n\twlock       sync.Mutex    // Guards writes to c\n\tmu          sync.Mutex\n\tclosed      bool              // has c been closed?\n\tversion     version           // Version number to use for connection\n\tcancelFuncs map[uint64]func() // Cancellation functions for in-progress calls\n}\n\n// serverState tracks all live server-side connections so we can clean things up when canceled.\ntype serverState struct {\n\topts  ServerOptions\n\tmu    sync.Mutex\n\tconns map[*serverConnection]struct{} // Live connections\n}\n\n// Serve starts listening for connections and requests on l. It always returns a\n// non-nil error and closes l.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// onceCloseListener wraps a Listener, protecting it from multiple Close calls.\ntype onceCloseListener struct {\n\tListener\n\tcloser func() error // Must be result of sync.OnceValue\n}\n\nfunc (oc *onceCloseListener) Close() error {\n\treturn oc.closer()\n}\n\n// ServeOn serves client requests received over an already established\n// network connection with a client. This can be useful in tests or\n// when using custom networking transports.\nfunc ServeOn(ctx context.Context, conn net.Conn, hmap *HandlerMap, opts ServerOptions) {\n\tss := &serverState{opts: opts.withDefaults()}\n\tss.serveConnection(ctx, conn, hmap)\n}\n\nfunc (ss *serverState) serveConnection(ctx context.Context, conn net.Conn, hmap *HandlerMap) {\n\tc := &serverConnection{\n\t\topts:        ss.opts,\n\t\tc:           conn,\n\t\tcbuf:        bufio.NewReader(conn),\n\t\tversion:     initialVersion, // Updated when we hear from client\n\t\tcancelFuncs: map[uint64]func(){},\n\t}\n\tss.register(c)\n\n\tgo c.readRequests(ctx, hmap, func() { ss.unregister(c) })\n}\n\nfunc (ss *serverState) stop() {\n\tss.mu.Lock()\n\tdefer ss.mu.Unlock()\n\tfor c := range ss.conns {\n\t\tc.c.Close() // Should stop readRequests in its tracks\n\t}\n}\n\nfunc (ss *serverState) register(c *serverConnection) {\n\tss.mu.Lock()\n\tdefer ss.mu.Unlock()\n\tif ss.conns == nil {\n\t\tss.conns = map[*serverConnection]struct{}{}\n\t}\n\tss.conns[c] = struct{}{}\n}\n\nfunc (ss *serverState) unregister(c *serverConnection) {\n\tss.mu.Lock()\n\tdefer ss.mu.Unlock()\n\tdelete(ss.conns, c)\n}\n\n// Connect creates a connection to the servers at the endpoints returned by the\n// resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Close closes a connection.\nfunc (rc *reconnectingConnection) Close() {\n\tcloseWithLock := func() {\n\t\trc.mu.Lock()\n\t\tdefer rc.mu.Unlock()\n\t\tif rc.closed {\n\t\t\treturn\n\t\t}\n\t\trc.closed = true\n\t\tfor _, c := range rc.conns {\n\t\t\tc.close()\n\t\t}\n\t}\n\tcloseWithLock()\n\n\t// Cancel the watchResolver goroutine and wait for it to terminate. If the\n\t// watchResolver has already been terminated, then this code is a no-op.\n\t// Note that if we hold the lock while waiting for watchResolver to\n\t// terminate, we may deadlock.\n\trc.cancelResolver()\n\trc.resolverDone.Wait()\n}\n\n// Call makes an RPC over connection c, retrying it on network errors if retries are allowed.\nfunc (rc *reconnectingConnection) Call(ctx context.Context, h MethodKey, arg []byte, opts CallOptions) ([]byte, error) {\n\tif !opts.Retry {\n\t\treturn rc.callOnce(ctx, h, arg, opts)\n\t}\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\tresponse, err := rc.callOnce(ctx, h, arg, opts)\n\t\tif errors.Is(err, Unreachable) || errors.Is(err, CommunicationError) {\n\t\t\tcontinue\n\t\t}\n\t\treturn response, err\n\t}\n\treturn nil, ctx.Err()\n}\n\nfunc (rc *reconnectingConnection) callOnce(ctx context.Context, h MethodKey, arg []byte, opts CallOptions) ([]byte, error) {\n\tvar micros int64\n\tdeadline, haveDeadline := ctx.Deadline()\n\tif haveDeadline {\n\t\t// Send the deadline in the header. We use the relative time instead\n\t\t// of absolute in case there is significant clock skew. This does mean\n\t\t// that we will not count transmission delay against the deadline.\n\t\tmicros = time.Until(deadline).Microseconds()\n\t\tif micros <= 0 {\n\t\t\t// Fail immediately without attempting to send a zero or negative\n\t\t\t// deadline to the server which will be misinterpreted.\n\t\t\t<-ctx.Done()\n\t\t\treturn nil, ctx.Err()\n\t\t}\n\t}\n\n\t// Encode the header.\n\thdr := encodeHeader(ctx, h, micros)\n\n\t// Note that we send the header and the payload as follows:\n\t// [header_length][encoded_header][payload]\n\tvar hdrLen [hdrLenLen]byte\n\tbinary.LittleEndian.PutUint32(hdrLen[:], uint32(len(hdr)))\n\thdrSlice := append(hdrLen[:], hdr...)\n\n\trpc := &call{}\n\trpc.doneSignal = make(chan struct{})\n\n\t// TODO: Arrange to obey deadline in any reconnection done inside startCall.\n\tconn, nc, err := rc.startCall(ctx, rpc, opts)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := writeMessage(nc, &conn.wlock, requestMessage, rpc.id, hdrSlice, arg, rc.opts.WriteFlattenLimit); err != nil {\n\t\tconn.shutdown(\"client send request\", err)\n\t\tconn.endCall(rpc)\n\t\treturn nil, fmt.Errorf(\"%w: %s\", CommunicationError, err)\n\t}\n\n\tif rc.opts.OptimisticSpinDuration > 0 {\n\t\t// Optimistically spin, waiting for the results.\n\t\tfor start := time.Now(); time.Since(start) < rc.opts.OptimisticSpinDuration; {\n\t\t\tif atomic.LoadUint32(&rpc.done) > 0 {\n\t\t\t\treturn rpc.response, rpc.err\n\t\t\t}\n\t\t}\n\t}\n\n\tif cdone := ctx.Done(); cdone != nil {\n\t\tselect {\n\t\tcase <-rpc.doneSignal:\n\t\t\t// Regular return\n\t\tcase <-cdone:\n\t\t\t// Canceled or deadline expired.\n\t\t\tconn.endCall(rpc)\n\n\t\t\tif !haveDeadline || time.Now().Before(deadline) {\n\t\t\t\t// Early cancellation. Tell server about it.\n\t\t\t\tif err := writeMessage(nc, &conn.wlock, cancelMessage, rpc.id, nil, nil, rc.opts.WriteFlattenLimit); err != nil {\n\t\t\t\t\tconn.shutdown(\"client send cancel\", err)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn nil, ctx.Err()\n\t\t}\n\t} else {\n\t\t<-rpc.doneSignal\n\t}\n\treturn rpc.response, rpc.err\n}\n\n// watchResolver watches for updates to the set of endpoints. When a new set of\n// updates is available, watchResolver passes it to updateEndpoints.\n// REQUIRES: version != nil.\n// REQUIRES: rc.mu is not held.\nfunc (rc *reconnectingConnection) watchResolver(ctx context.Context, version *Version) {\n\tdefer rc.resolverDone.Done()\n\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\tendpoints, newVersion, err := rc.resolver.Resolve(ctx, version)\n\t\tif err != nil {\n\t\t\tlogError(rc.opts.Logger, \"watchResolver\", err)\n\t\t\tcontinue\n\t\t}\n\t\tif newVersion == nil {\n\t\t\tlogError(rc.opts.Logger, \"watchResolver\", errors.New(\"non-constant resolver returned a nil version\"))\n\t\t\tcontinue\n\t\t}\n\t\tif *version == *newVersion {\n\t\t\t// Resolver wishes to be called again after an appropriate delay.\n\t\t\tcontinue\n\t\t}\n\t\tif err := rc.updateEndpoints(ctx, endpoints); err != nil {\n\t\t\tlogError(rc.opts.Logger, \"watchResolver\", err)\n\t\t}\n\t\tversion = newVersion\n\t\tr.Reset()\n\t}\n}\n\n// updateEndpoints updates the set of endpoints. Existing connections are\n// retained, and stale connections are closed.\n// REQUIRES: rc.mu is not held.\nfunc (rc *reconnectingConnection) updateEndpoints(ctx context.Context, endpoints []Endpoint) error {\n\trc.mu.Lock()\n\tdefer rc.mu.Unlock()\n\n\tif rc.closed {\n\t\treturn fmt.Errorf(\"updateEndpoints on closed Connection\")\n\t}\n\n\t// Make new endpoints.\n\tkeep := make(map[string]struct{}, len(endpoints))\n\tfor _, endpoint := range endpoints {\n\t\taddr := endpoint.Address()\n\t\tkeep[addr] = struct{}{}\n\t\tif _, ok := rc.conns[addr]; !ok {\n\t\t\t// New endpoint, create connection and manage it.\n\t\t\tctx, cancel := context.WithCancel(ctx)\n\t\t\tc := &clientConnection{\n\t\t\t\trc:       rc,\n\t\t\t\tcanceler: cancel,\n\t\t\t\tlogger:   rc.opts.Logger,\n\t\t\t\tendpoint: endpoint,\n\t\t\t\tcalls:    map[uint64]*call{},\n\t\t\t\tlastID:   0,\n\t\t\t}\n\t\t\trc.conns[addr] = c\n\t\t\tc.register()\n\t\t\tgo c.manage(ctx)\n\t\t}\n\t}\n\n\t// Drop old endpoints.\n\tfor addr, c := range rc.conns {\n\t\tif _, ok := keep[addr]; ok {\n\t\t\t// Still live, so keep it.\n\t\t\tcontinue\n\t\t}\n\t\tc.unregister()\n\t}\n\n\treturn nil\n}\n\n// startCall registers a new in-progress call.\n// REQUIRES: rc.mu is not held.\nfunc (rc *reconnectingConnection) startCall(ctx context.Context, rpc *call, opts CallOptions) (*clientConnection, net.Conn, error) {\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\trc.mu.Lock()\n\t\tif rc.closed {\n\t\t\trc.mu.Unlock()\n\t\t\treturn nil, nil, fmt.Errorf(\"Call on closed Connection\")\n\t\t}\n\n\t\treplica, ok := rc.opts.Balancer.Pick(opts)\n\t\tif !ok {\n\t\t\trc.mu.Unlock()\n\t\t\tcontinue\n\t\t}\n\n\t\tc, ok := replica.(*clientConnection)\n\t\tif !ok {\n\t\t\trc.mu.Unlock()\n\t\t\treturn nil, nil, fmt.Errorf(\"internal error: wrong connection type %#v returned by load balancer\", replica)\n\t\t}\n\n\t\tc.lastID++\n\t\trpc.id = c.lastID\n\t\tc.calls[rpc.id] = rpc\n\t\tc.callstart()\n\t\tnc := c.c\n\t\trc.mu.Unlock()\n\n\t\treturn c, nc, nil\n\t}\n\n\treturn nil, nil, ctx.Err()\n}\n\nfunc (c *clientConnection) Address() string {\n\treturn c.endpoint.Address()\n}\n\n// State transition actions: all of these are called with rc.mu held.\n\nfunc (c *clientConnection) register() {\n\tswitch c.state {\n\tcase missing:\n\t\tc.setState(disconnected)\n\tcase draining:\n\t\t// We were attempting to get rid of the old connection, but it\n\t\t// seems like the server-side problem was transient, so we\n\t\t// resurrect the draining connection into a non-draining state.\n\t\t//\n\t\t// New state is active instead of idle since state==draining\n\t\t// implies there is at least one call in-flight.\n\t\tc.setState(active)\n\t}\n}\n\nfunc (c *clientConnection) unregister() {\n\tswitch c.state {\n\tcase disconnected, checking, idle:\n\t\tc.setState(missing)\n\tcase active:\n\t\tc.setState(draining)\n\t}\n}\n\nfunc (c *clientConnection) connected() {\n\tswitch c.state {\n\tcase disconnected:\n\t\tc.setState(checking)\n\t}\n}\n\nfunc (c *clientConnection) checked() {\n\tswitch c.state {\n\tcase checking:\n\t\tc.setState(idle)\n\t}\n}\n\nfunc (c *clientConnection) callstart() {\n\tswitch c.state {\n\tcase idle:\n\t\tc.setState(active)\n\t}\n}\n\nfunc (c *clientConnection) lastdone() {\n\tswitch c.state {\n\tcase active:\n\t\tc.setState(idle)\n\tcase draining:\n\t\tc.setState(missing)\n\t}\n}\n\nfunc (c *clientConnection) fail(details string, err error) {\n\tif !c.loggedShutdown {\n\t\tc.loggedShutdown = true\n\t\tlogError(c.logger, details, err)\n\t}\n\n\t// endCalls here so we can supply good errors.\n\tc.endCalls(fmt.Errorf(\"%w: %s: %s\", CommunicationError, details, err))\n\n\tswitch c.state {\n\tcase checking, idle, active:\n\t\tc.setState(disconnected)\n\tcase draining:\n\t\tc.setState(missing)\n\t}\n}\n\nfunc (c *clientConnection) close() {\n\t// endCalls here so we can supply good errors.\n\tc.endCalls(fmt.Errorf(\"%w: connection closed\", CommunicationError))\n\n\tc.setState(missing)\n}\n\n// checkInvariants verifies clientConnection invariants.\nfunc (c *clientConnection) checkInvariants() {\n\ts := c.state\n\n\t// connection in reconnectingConnection.conns iff state not in {missing}\n\tif _, ok := c.rc.conns[c.endpoint.Address()]; ok != (s != missing) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong connection table presence %v\", s, ok))\n\t}\n\n\t// has net.Conn iff state in {checking, idle, active, draining}\n\tif (c.c != nil) != (s == checking || s == idle || s == active || s == draining) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong net.Conn %v\", s, c.c))\n\t}\n\n\t// connection is in the balancer iff state in {idle, active}\n\tif c.inBalancer != (s == idle || s == active) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong balancer presence %v\", s, c.inBalancer))\n\t}\n\n\t// len(calls) > 0 iff state in {active, draining}\n\tif (len(c.calls) != 0) != (s == active || s == draining) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong number of calls %d\", s, len(c.calls)))\n\t}\n}\n\n// setState transitions to state s and updates any related state.\nfunc (c *clientConnection) setState(s connState) {\n\t// idle<-> active transitions may happen a lot, so short-circuit them\n\t// by avoiding logging and full invariant maintenance.\n\tif c.state == active && s == idle {\n\t\tc.state = idle\n\t\tif len(c.calls) != 0 {\n\t\t\tpanic(fmt.Sprintf(\"%v connection: wrong number of calls %d\", s, len(c.calls)))\n\t\t}\n\t\treturn\n\t} else if c.state == idle && s == active {\n\t\tc.state = active\n\t\tif len(c.calls) == 0 {\n\t\t\tpanic(fmt.Sprintf(\"%v connection: wrong number of calls %d\", s, len(c.calls)))\n\t\t}\n\t\treturn\n\t}\n\n\tc.logger.Info(\"connection\", \"addr\", c.endpoint.Address(), \"from\", c.state, \"to\", s)\n\tc.state = s\n\n\t// Fix membership in rc.conns.\n\tif s == missing {\n\t\tdelete(c.rc.conns, c.endpoint.Address())\n\t\tif c.canceler != nil {\n\t\t\tc.canceler() // Forces retry loop to end early\n\t\t\tc.canceler = nil\n\t\t}\n\t} // else: caller is responsible for adding c to rc.conns\n\n\t// Fix net.Conn presence.\n\tif s == missing || s == disconnected {\n\t\tif c.c != nil {\n\t\t\tc.c.Close()\n\t\t\tc.c = nil\n\t\t\tc.cbuf = nil\n\t\t}\n\t} // else: caller is responsible for setting c.c and c.cbuf\n\n\t// Fix balancer membership.\n\tif s == idle || s == active {\n\t\tif !c.inBalancer {\n\t\t\tc.rc.opts.Balancer.Add(c)\n\t\t\tc.inBalancer = true\n\t\t}\n\t} else {\n\t\tif c.inBalancer {\n\t\t\tc.rc.opts.Balancer.Remove(c)\n\t\t\tc.inBalancer = false\n\t\t}\n\t}\n\n\t// Fix in-flight calls.\n\tif s == active || s == draining {\n\t\t// Keep calls live\n\t} else {\n\t\tc.endCalls(fmt.Errorf(\"%w:\n```\n# Function Signature: func Serve(ctx context.Context, l Listener, opts ServerOptions) error {\n# Natural Language Annotations: // Serve starts listening for connections and requests on l. It always returns a\n// non-nil error and closes l.\nPlease return the generated code surrounded by ```\n", "response": ["Here is the generated code based on your function signature and natural language annotations:\n\n```go\npackage call\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"net\"\n)\n\nfunc Serve(ctx context.Context, l Listener, opts ServerOptions) error {\n\tss := &serverState{opts: opts.withDefaults()}\n\tgo func() {\n\t\t<-ctx.Done()\n\t\tss.stop()\n\t\tl.Close()\n\t}()\n\n\tfor {\n\t\tconn, err := l.Accept()\n\t\tif err != nil {\n\t\t\tif !errors.Is(err, net.ErrClosed) {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\treturn nil\n\t\t}\n\t\thmap := NewHandlerMap() // Assuming HandlerMap is a function that returns a new HandlerMap instance\n\t\tgo func() {\n\t\t\tServeOn(ctx, conn, hmap, opts)\n\t\t}()\n\t}\n}\n```\nPlease note that the `HandlerMap` instance is created here assuming that it is a function that returns a new `HandlerMap` instance. Please replace it with the actual creation process if it is not correct.", "```go\nfunc Serve(ctx context.Context, l Listener, opts ServerOptions) error {\n\tss := &serverState{opts: opts.withDefaults()}\n\tfor {\n\t\tconn, err := l.Accept()\n\t\tif err != nil {\n\t\t\tl.Close()\n\t\t\tss.stop()\n\t\t\treturn err\n\t\t}\n\t\tgo func() {\n\t\t\tdefer conn.Close()\n\t\t\tctx := logging.WithLogger(ctx, ss.opts.Logger)\n\t\t\thmap := &HandlerMap{} // Assuming HandlerMap is defined somewhere else\n\t\t\tServeOn(ctx, conn, hmap, ss.opts)\n\t\t}()\n\t}\n}\n```", "```go\npackage call\n\nimport (\n\t\"context\"\n\t\"fmt\"\n)\n\nfunc Serve(ctx context.Context, l Listener, opts ServerOptions) error {\n\tss := &serverState{opts: opts.withDefaults()}\n\n\tfor {\n\t\tconn, hmap, err := l.Accept()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to accept connection: %w\", err)\n\t\t}\n\n\t\tgo ss.serveConnection(ctx, conn, hmap)\n\t}\n}\n```"]}, "weaver-internal/net/call/call.go-Connect": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/net/call/call.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package call implements an RPC mechanism.\npackage call\n\n// # Overview\n//\n// RPCs are conveyed across a bidirectional connection. A connection carries\n// a sequence of messages in each direction. A message has the following\n// information:\n//\trequest-id\t-- A number that identifies a particular RPC\n//\tmessage-type\t-- E.g., request or response\n//\tlength\t\t-- How many payload bytes follow\n//\tpayload\t\t-- length bytes of payload\n// The payload format varies depending on the message-type.\n// See msg.go for details.\n//\n// # Server operation\n//\n// The server listens for connections (typically on a TCP socket). For\n// each accepted connection, it starts a readRequests() goroutine that\n// reads messages from that connection. When readRequests() gets a\n// request message, it starts a runHandler() goroutine. runHandler()\n// looks up the registered handler for the message, runs it, and sends\n// the response back over the connection.\n//\n// # Client operation\n//\n// For each newly discovered server, the client starts a manage() goroutine\n// that connects to server, and then reads messages from the connection. If the\n// network connection breaks, manage() reconnects (after a retry delay).\n//\n// When the client wants to send an RPC, it selects one of its server\n// connections to use, creates a call object, assigns it a new request-id, and\n// registers the object in a map in the connection. It then sends a request\n// message over the connection and waits for the call object to be marked as\n// done.\n//\n// When the response arrives, it is picked up by readAndProcessMessage().\n// readAndProcessMessage() finds the call object corresponding to the\n// request-id in the response, and marks the call object as done which\n// wakes up goroutine that initiated the RPC.\n//\n// If a client is constructed with a non-constant resolver, the client also\n// spawns a watchResolver goroutine that repeatedly calls Resolve on the\n// resolver to get notified of updates to the set of endpoints. When the\n// endpoints are updated, existing connections are retained, and stale\n// connections are transitioned to a \"draining\" state.\n//\n// New RPCs are never issued over draining connections, but the pending\n// requests on a draining connection are allowed to finish. As soon as a\n// draining connection has no active calls, the connection closes itself. If\n// the resolver later returns a new set of endpoints that includes a draining\n// connection that hasn't closed itself, the draining connection is turned\n// back into a normal connection.\nimport (\n\t\"bufio\"\n\t\"context\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"github.com/ServiceWeaver/weaver/runtime/logging\"\n\t\"github.com/ServiceWeaver/weaver/runtime/retry\"\n\t\"go.opentelemetry.io/otel/codes\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// Connection allows a client to send RPCs.\ntype Connection interface {\n\t// Call makes an RPC over a Connection.\n\tCall(context.Context, MethodKey, []byte, CallOptions) ([]byte, error)\n\n\t// Close closes a connection. Pending invocations of Call are cancelled and\n\t// return an error. All future invocations of Call fail and return an error\n\t// immediately. Close can be called more than once.\n\tClose()\n}\n\n// Listener allows the server to accept RPCs.\ntype Listener interface {\n\tAccept() (net.Conn, *HandlerMap, error)\n\tClose() error\n\tAddr() net.Addr\n}\n\n// reconnectingConnection is the concrete client-side Connection implementation.\n// It automatically reconnects to the servers on first call or the first call\n// after a shutdown.\ntype reconnectingConnection struct {\n\topts ClientOptions\n\n\t// mu guards the following fields and some of the fields in the\n\t// clientConnections inside connections and draining.\n\tmu     sync.Mutex\n\tconns  map[string]*clientConnection\n\tclosed bool\n\n\tresolver       Resolver\n\tcancelResolver func()         // cancels the watchResolver goroutine\n\tresolverDone   sync.WaitGroup // used to wait for watchResolver to finish\n}\n\n// connState is the state of a clientConnection (connection to a particular\n// server replica). missing is a special state used for unknown servers. A\n// typical sequence of transitions is:\n//\n//\tmissing -> disconnected -> checking -> idle <-> active -> draining -> missing\n//\n// The events that can cause state transition are:\n//\n// - register: server has shown up in resolver results\n// - unregister: server has dropped from resolver results\n// - connected: a connection has been successfully made\n// - checked: connection has been successfully checked\n// - callstart: call starts on connection\n// - lastdone: last active call on connection has ended\n// - fail: some protocol error is detected on the connection\n// - close: reconnectingConnection is being closed\n//\n// Each event has a corresponding clientConnection method below. See\n// those methods for the corresponding state transitions.\ntype connState int8\n\nconst (\n\tmissing      connState = iota\n\tdisconnected           // cannot be used for calls\n\tchecking               // checking new network connection\n\tidle                   // can be used for calls, no calls in-flight\n\tactive                 // can be used for calls, some calls in-flight\n\tdraining               // some calls in-flight, no new calls should be added\n)\n\nvar connStateNames = []string{\n\t\"missing\",\n\t\"disconnected\",\n\t\"checking\",\n\t\"idle\",\n\t\"active\",\n\t\"draining\",\n}\n\nfunc (s connState) String() string { return connStateNames[s] }\n\n// clientConnection manages one network connection on the client-side.\ntype clientConnection struct {\n\t// Immutable after construction.\n\trc       *reconnectingConnection // owner\n\tcanceler func()                  // Used to cancel goroutine handling connection\n\tlogger   *slog.Logger\n\tendpoint Endpoint\n\n\twlock sync.Mutex // Guards writes to c\n\n\t// Guarded by rc.mu\n\tstate          connState        // current connection state\n\tloggedShutdown bool             // Have we logged a shutdown error?\n\tinBalancer     bool             // Is c registered with the balancer?\n\tc              net.Conn         // Active network connection, or nil\n\tcbuf           *bufio.Reader    // Buffered reader wrapped around c\n\tversion        version          // Version number to use for connection\n\tcalls          map[uint64]*call // In-progress calls\n\tlastID         uint64           // Last assigned request ID for a call\n}\n\nvar _ ReplicaConnection = &clientConnection{}\n\n// call holds the state for an active call at the client.\ntype call struct {\n\tid         uint64\n\tdoneSignal chan struct{}\n\n\t// Fields below are accessed across goroutines, but their access is\n\t// synchronized via doneSignal, i.e., it is never concurrent.\n\terr      error\n\tresponse []byte\n\n\t// Is the call done?\n\t// This field is accessed across goroutines using atomics.\n\tdone uint32 // is the call done?\n\n}\n\n// serverConnection manages one network connection on the server-side.\ntype serverConnection struct {\n\topts        ServerOptions\n\tc           net.Conn\n\tcbuf        *bufio.Reader // Buffered reader wrapped around c\n\twlock       sync.Mutex    // Guards writes to c\n\tmu          sync.Mutex\n\tclosed      bool              // has c been closed?\n\tversion     version           // Version number to use for connection\n\tcancelFuncs map[uint64]func() // Cancellation functions for in-progress calls\n}\n\n// serverState tracks all live server-side connections so we can clean things up when canceled.\ntype serverState struct {\n\topts  ServerOptions\n\tmu    sync.Mutex\n\tconns map[*serverConnection]struct{} // Live connections\n}\n\n// Serve starts listening for connections and requests on l. It always returns a\n// non-nil error and closes l.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// onceCloseListener wraps a Listener, protecting it from multiple Close calls.\ntype onceCloseListener struct {\n\tListener\n\tcloser func() error // Must be result of sync.OnceValue\n}\n\nfunc (oc *onceCloseListener) Close() error {\n\treturn oc.closer()\n}\n\n// ServeOn serves client requests received over an already established\n// network connection with a client. This can be useful in tests or\n// when using custom networking transports.\nfunc ServeOn(ctx context.Context, conn net.Conn, hmap *HandlerMap, opts ServerOptions) {\n\tss := &serverState{opts: opts.withDefaults()}\n\tss.serveConnection(ctx, conn, hmap)\n}\n\nfunc (ss *serverState) serveConnection(ctx context.Context, conn net.Conn, hmap *HandlerMap) {\n\tc := &serverConnection{\n\t\topts:        ss.opts,\n\t\tc:           conn,\n\t\tcbuf:        bufio.NewReader(conn),\n\t\tversion:     initialVersion, // Updated when we hear from client\n\t\tcancelFuncs: map[uint64]func(){},\n\t}\n\tss.register(c)\n\n\tgo c.readRequests(ctx, hmap, func() { ss.unregister(c) })\n}\n\nfunc (ss *serverState) stop() {\n\tss.mu.Lock()\n\tdefer ss.mu.Unlock()\n\tfor c := range ss.conns {\n\t\tc.c.Close() // Should stop readRequests in its tracks\n\t}\n}\n\nfunc (ss *serverState) register(c *serverConnection) {\n\tss.mu.Lock()\n\tdefer ss.mu.Unlock()\n\tif ss.conns == nil {\n\t\tss.conns = map[*serverConnection]struct{}{}\n\t}\n\tss.conns[c] = struct{}{}\n}\n\nfunc (ss *serverState) unregister(c *serverConnection) {\n\tss.mu.Lock()\n\tdefer ss.mu.Unlock()\n\tdelete(ss.conns, c)\n}\n\n// Connect creates a connection to the servers at the endpoints returned by the\n// resolver.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Close closes a connection.\nfunc (rc *reconnectingConnection) Close() {\n\tcloseWithLock := func() {\n\t\trc.mu.Lock()\n\t\tdefer rc.mu.Unlock()\n\t\tif rc.closed {\n\t\t\treturn\n\t\t}\n\t\trc.closed = true\n\t\tfor _, c := range rc.conns {\n\t\t\tc.close()\n\t\t}\n\t}\n\tcloseWithLock()\n\n\t// Cancel the watchResolver goroutine and wait for it to terminate. If the\n\t// watchResolver has already been terminated, then this code is a no-op.\n\t// Note that if we hold the lock while waiting for watchResolver to\n\t// terminate, we may deadlock.\n\trc.cancelResolver()\n\trc.resolverDone.Wait()\n}\n\n// Call makes an RPC over connection c, retrying it on network errors if retries are allowed.\nfunc (rc *reconnectingConnection) Call(ctx context.Context, h MethodKey, arg []byte, opts CallOptions) ([]byte, error) {\n\tif !opts.Retry {\n\t\treturn rc.callOnce(ctx, h, arg, opts)\n\t}\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\tresponse, err := rc.callOnce(ctx, h, arg, opts)\n\t\tif errors.Is(err, Unreachable) || errors.Is(err, CommunicationError) {\n\t\t\tcontinue\n\t\t}\n\t\treturn response, err\n\t}\n\treturn nil, ctx.Err()\n}\n\nfunc (rc *reconnectingConnection) callOnce(ctx context.Context, h MethodKey, arg []byte, opts CallOptions) ([]byte, error) {\n\tvar micros int64\n\tdeadline, haveDeadline := ctx.Deadline()\n\tif haveDeadline {\n\t\t// Send the deadline in the header. We use the relative time instead\n\t\t// of absolute in case there is significant clock skew. This does mean\n\t\t// that we will not count transmission delay against the deadline.\n\t\tmicros = time.Until(deadline).Microseconds()\n\t\tif micros <= 0 {\n\t\t\t// Fail immediately without attempting to send a zero or negative\n\t\t\t// deadline to the server which will be misinterpreted.\n\t\t\t<-ctx.Done()\n\t\t\treturn nil, ctx.Err()\n\t\t}\n\t}\n\n\t// Encode the header.\n\thdr := encodeHeader(ctx, h, micros)\n\n\t// Note that we send the header and the payload as follows:\n\t// [header_length][encoded_header][payload]\n\tvar hdrLen [hdrLenLen]byte\n\tbinary.LittleEndian.PutUint32(hdrLen[:], uint32(len(hdr)))\n\thdrSlice := append(hdrLen[:], hdr...)\n\n\trpc := &call{}\n\trpc.doneSignal = make(chan struct{})\n\n\t// TODO: Arrange to obey deadline in any reconnection done inside startCall.\n\tconn, nc, err := rc.startCall(ctx, rpc, opts)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := writeMessage(nc, &conn.wlock, requestMessage, rpc.id, hdrSlice, arg, rc.opts.WriteFlattenLimit); err != nil {\n\t\tconn.shutdown(\"client send request\", err)\n\t\tconn.endCall(rpc)\n\t\treturn nil, fmt.Errorf(\"%w: %s\", CommunicationError, err)\n\t}\n\n\tif rc.opts.OptimisticSpinDuration > 0 {\n\t\t// Optimistically spin, waiting for the results.\n\t\tfor start := time.Now(); time.Since(start) < rc.opts.OptimisticSpinDuration; {\n\t\t\tif atomic.LoadUint32(&rpc.done) > 0 {\n\t\t\t\treturn rpc.response, rpc.err\n\t\t\t}\n\t\t}\n\t}\n\n\tif cdone := ctx.Done(); cdone != nil {\n\t\tselect {\n\t\tcase <-rpc.doneSignal:\n\t\t\t// Regular return\n\t\tcase <-cdone:\n\t\t\t// Canceled or deadline expired.\n\t\t\tconn.endCall(rpc)\n\n\t\t\tif !haveDeadline || time.Now().Before(deadline) {\n\t\t\t\t// Early cancellation. Tell server about it.\n\t\t\t\tif err := writeMessage(nc, &conn.wlock, cancelMessage, rpc.id, nil, nil, rc.opts.WriteFlattenLimit); err != nil {\n\t\t\t\t\tconn.shutdown(\"client send cancel\", err)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn nil, ctx.Err()\n\t\t}\n\t} else {\n\t\t<-rpc.doneSignal\n\t}\n\treturn rpc.response, rpc.err\n}\n\n// watchResolver watches for updates to the set of endpoints. When a new set of\n// updates is available, watchResolver passes it to updateEndpoints.\n// REQUIRES: version != nil.\n// REQUIRES: rc.mu is not held.\nfunc (rc *reconnectingConnection) watchResolver(ctx context.Context, version *Version) {\n\tdefer rc.resolverDone.Done()\n\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\tendpoints, newVersion, err := rc.resolver.Resolve(ctx, version)\n\t\tif err != nil {\n\t\t\tlogError(rc.opts.Logger, \"watchResolver\", err)\n\t\t\tcontinue\n\t\t}\n\t\tif newVersion == nil {\n\t\t\tlogError(rc.opts.Logger, \"watchResolver\", errors.New(\"non-constant resolver returned a nil version\"))\n\t\t\tcontinue\n\t\t}\n\t\tif *version == *newVersion {\n\t\t\t// Resolver wishes to be called again after an appropriate delay.\n\t\t\tcontinue\n\t\t}\n\t\tif err := rc.updateEndpoints(ctx, endpoints); err != nil {\n\t\t\tlogError(rc.opts.Logger, \"watchResolver\", err)\n\t\t}\n\t\tversion = newVersion\n\t\tr.Reset()\n\t}\n}\n\n// updateEndpoints updates the set of endpoints. Existing connections are\n// retained, and stale connections are closed.\n// REQUIRES: rc.mu is not held.\nfunc (rc *reconnectingConnection) updateEndpoints(ctx context.Context, endpoints []Endpoint) error {\n\trc.mu.Lock()\n\tdefer rc.mu.Unlock()\n\n\tif rc.closed {\n\t\treturn fmt.Errorf(\"updateEndpoints on closed Connection\")\n\t}\n\n\t// Make new endpoints.\n\tkeep := make(map[string]struct{}, len(endpoints))\n\tfor _, endpoint := range endpoints {\n\t\taddr := endpoint.Address()\n\t\tkeep[addr] = struct{}{}\n\t\tif _, ok := rc.conns[addr]; !ok {\n\t\t\t// New endpoint, create connection and manage it.\n\t\t\tctx, cancel := context.WithCancel(ctx)\n\t\t\tc := &clientConnection{\n\t\t\t\trc:       rc,\n\t\t\t\tcanceler: cancel,\n\t\t\t\tlogger:   rc.opts.Logger,\n\t\t\t\tendpoint: endpoint,\n\t\t\t\tcalls:    map[uint64]*call{},\n\t\t\t\tlastID:   0,\n\t\t\t}\n\t\t\trc.conns[addr] = c\n\t\t\tc.register()\n\t\t\tgo c.manage(ctx)\n\t\t}\n\t}\n\n\t// Drop old endpoints.\n\tfor addr, c := range rc.conns {\n\t\tif _, ok := keep[addr]; ok {\n\t\t\t// Still live, so keep it.\n\t\t\tcontinue\n\t\t}\n\t\tc.unregister()\n\t}\n\n\treturn nil\n}\n\n// startCall registers a new in-progress call.\n// REQUIRES: rc.mu is not held.\nfunc (rc *reconnectingConnection) startCall(ctx context.Context, rpc *call, opts CallOptions) (*clientConnection, net.Conn, error) {\n\tfor r := retry.Begin(); r.Continue(ctx); {\n\t\trc.mu.Lock()\n\t\tif rc.closed {\n\t\t\trc.mu.Unlock()\n\t\t\treturn nil, nil, fmt.Errorf(\"Call on closed Connection\")\n\t\t}\n\n\t\treplica, ok := rc.opts.Balancer.Pick(opts)\n\t\tif !ok {\n\t\t\trc.mu.Unlock()\n\t\t\tcontinue\n\t\t}\n\n\t\tc, ok := replica.(*clientConnection)\n\t\tif !ok {\n\t\t\trc.mu.Unlock()\n\t\t\treturn nil, nil, fmt.Errorf(\"internal error: wrong connection type %#v returned by load balancer\", replica)\n\t\t}\n\n\t\tc.lastID++\n\t\trpc.id = c.lastID\n\t\tc.calls[rpc.id] = rpc\n\t\tc.callstart()\n\t\tnc := c.c\n\t\trc.mu.Unlock()\n\n\t\treturn c, nc, nil\n\t}\n\n\treturn nil, nil, ctx.Err()\n}\n\nfunc (c *clientConnection) Address() string {\n\treturn c.endpoint.Address()\n}\n\n// State transition actions: all of these are called with rc.mu held.\n\nfunc (c *clientConnection) register() {\n\tswitch c.state {\n\tcase missing:\n\t\tc.setState(disconnected)\n\tcase draining:\n\t\t// We were attempting to get rid of the old connection, but it\n\t\t// seems like the server-side problem was transient, so we\n\t\t// resurrect the draining connection into a non-draining state.\n\t\t//\n\t\t// New state is active instead of idle since state==draining\n\t\t// implies there is at least one call in-flight.\n\t\tc.setState(active)\n\t}\n}\n\nfunc (c *clientConnection) unregister() {\n\tswitch c.state {\n\tcase disconnected, checking, idle:\n\t\tc.setState(missing)\n\tcase active:\n\t\tc.setState(draining)\n\t}\n}\n\nfunc (c *clientConnection) connected() {\n\tswitch c.state {\n\tcase disconnected:\n\t\tc.setState(checking)\n\t}\n}\n\nfunc (c *clientConnection) checked() {\n\tswitch c.state {\n\tcase checking:\n\t\tc.setState(idle)\n\t}\n}\n\nfunc (c *clientConnection) callstart() {\n\tswitch c.state {\n\tcase idle:\n\t\tc.setState(active)\n\t}\n}\n\nfunc (c *clientConnection) lastdone() {\n\tswitch c.state {\n\tcase active:\n\t\tc.setState(idle)\n\tcase draining:\n\t\tc.setState(missing)\n\t}\n}\n\nfunc (c *clientConnection) fail(details string, err error) {\n\tif !c.loggedShutdown {\n\t\tc.loggedShutdown = true\n\t\tlogError(c.logger, details, err)\n\t}\n\n\t// endCalls here so we can supply good errors.\n\tc.endCalls(fmt.Errorf(\"%w: %s: %s\", CommunicationError, details, err))\n\n\tswitch c.state {\n\tcase checking, idle, active:\n\t\tc.setState(disconnected)\n\tcase draining:\n\t\tc.setState(missing)\n\t}\n}\n\nfunc (c *clientConnection) close() {\n\t// endCalls here so we can supply good errors.\n\tc.endCalls(fmt.Errorf(\"%w: connection closed\", CommunicationError))\n\n\tc.setState(missing)\n}\n\n// checkInvariants verifies clientConnection invariants.\nfunc (c *clientConnection) checkInvariants() {\n\ts := c.state\n\n\t// connection in reconnectingConnection.conns iff state not in {missing}\n\tif _, ok := c.rc.conns[c.endpoint.Address()]; ok != (s != missing) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong connection table presence %v\", s, ok))\n\t}\n\n\t// has net.Conn iff state in {checking, idle, active, draining}\n\tif (c.c != nil) != (s == checking || s == idle || s == active || s == draining) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong net.Conn %v\", s, c.c))\n\t}\n\n\t// connection is in the balancer iff state in {idle, active}\n\tif c.inBalancer != (s == idle || s == active) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong balancer presence %v\", s, c.inBalancer))\n\t}\n\n\t// len(calls) > 0 iff state in {active, draining}\n\tif (len(c.calls) != 0) != (s == active || s == draining) {\n\t\tpanic(fmt.Sprintf(\"%v connection: wrong number of calls %d\", s, len(c.calls)))\n\t}\n}\n\n// setState transitions to state s and updates any related state.\nfunc (c *clientConnection) setState(s connState) {\n\t// idle<-> active transitions may happen a lot, so short-circuit them\n\t// by avoiding logging and full invariant maintenance.\n\tif c.state == active && s == idle {\n\t\tc.state = idle\n\t\tif len(c.calls) != 0 {\n\t\t\tpanic(fmt.Sprintf(\"%v connection: wrong number of calls %d\", s, len(c.calls)))\n\t\t}\n\t\treturn\n\t} else if c.state == idle && s == active {\n\t\tc.state = active\n\t\tif len(c.calls) == 0 {\n\t\t\tpanic(fmt.Sprintf(\"%v connection: wrong number of calls %d\", s, len(c.calls)))\n\t\t}\n\t\treturn\n\t}\n\n\tc.logger.Info(\"connection\", \"addr\", c.endpoint.Address(), \"from\", c.state, \"to\", s)\n\tc.state = s\n\n\t// Fix membership in rc.conns.\n\tif s == missing {\n\t\tdelete(c.rc.conns, c.endpoint.Address())\n\t\tif c.canceler != nil {\n\t\t\tc.canceler() // Forces retry loop to end early\n\t\t\tc.canceler = nil\n\t\t}\n\t} // else: caller is responsible for adding c to rc.conns\n\n\t// Fix net.Conn presence.\n\tif s == missing || s == disconnected {\n\t\tif c.c != nil {\n\t\t\tc.c.Close()\n\t\t\tc.c = nil\n\t\t\tc.cbuf = nil\n\t\t}\n\t} // else: caller is responsible for setting c.c and c.cbuf\n\n\t// Fix balancer membership.\n\tif s == idle || s == active {\n\t\tif !c.inBalancer {\n\t\t\tc.rc.opts.Balancer.Add(c)\n\t\t\tc.inBalancer = true\n\t\t}\n\t} else {\n\t\tif c.inBalancer {\n\t\t\tc.rc.opts.Balancer.Remove(c)\n\t\t\tc.inBalancer = false\n\t\t}\n\t}\n\n\t// Fix in-flight calls.\n\tif s == active || s == draining {\n\t\t// Keep calls live\n\t} else {\n\t\tc.endCalls(fmt.Errorf(\"%w:\n```\n# Function Signature: func Connect(ctx context.Context, resolver Resolver, opts ClientOptions) (Connection, error) {\n# Natural Language Annotations: // Connect creates a connection to the servers at the endpoints returned by the\n// resolver.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc Connect(ctx context.Context, resolver Resolver, opts ClientOptions) (Connection, error) {\n    rc := &reconnectingConnection{\n        opts:        opts,\n        conns:       make(map[string]*clientConnection),\n        resolver:    resolver,\n        resolverDone: sync.WaitGroup{},\n    }\n\n    var resolverErr error\n    ctxWithCancel, cancelResolver := context.WithCancel(ctx)\n    rc.cancelResolver = cancelResolver\n\n    rc.resolverDone.Add(1)\n    go func() {\n        defer rc.resolverDone.Done()\n        rc.watchResolver(ctxWithCancel, &Version{})\n    }()\n\n    endpoints, _, err := resolver.Resolve(ctxWithCancel, &Version{})\n    if err != nil {\n        resolverErr = err\n        cancelResolver()\n        return nil, fmt.Errorf(\"failed to resolve endpoints: %w\", err)\n    }\n\n    if err := rc.updateEndpoints(ctxWithCancel, endpoints); err != nil {\n        resolverErr = err\n        cancelResolver()\n        return nil, fmt.Errorf(\"failed to update endpoints: %w\", err)\n    }\n\n    return rc, nil\n}\n```", "```go\nfunc Connect(ctx context.Context, resolver Resolver, opts ClientOptions) (Connection, error) {\n    rc := &reconnectingConnection{\n        opts:        opts,\n        conns:       make(map[string]*clientConnection),\n        resolver:    resolver,\n        resolverDone: sync.WaitGroup{},\n    }\n\n    ctx, cancel := context.WithCancel(ctx)\n    rc.cancelResolver = func() {\n        cancel()\n    }\n\n    rc.resolverDone.Add(1)\n    go rc.watchResolver(ctx, &Version{})\n\n    endpoints, _, err := resolver.Resolve(ctx, nil)\n    if err != nil {\n        rc.Close()\n        return nil, err\n    }\n\n    if err := rc.updateEndpoints(ctx, endpoints); err != nil {\n        rc.Close()\n        return nil, err\n    }\n\n    return rc, nil\n}\n```", "```go\nfunc Connect(ctx context.Context, resolver Resolver, opts ClientOptions) (Connection, error) {\n\trc := &reconnectingConnection{\n\t\topts:        opts,\n\t\tconns:       make(map[string]*clientConnection),\n\t\tcancelResolver: func() {},\n\t}\n\n\t// Start the watchResolver goroutine.\n\trc.resolverDone.Add(1)\n\tgo rc.watchResolver(ctx, &Version{})\n\n\t// Update endpoints with the initial resolver result.\n\tif err := rc.updateEndpoints(ctx, resolver.InitialEndpoints()); err != nil {\n\t\trc.Close()\n\t\treturn nil, err\n\t}\n\n\treturn rc, nil\n}\n```"]}, "weaver-internal/net/call/msg.go-writeMessage": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/net/call/msg.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage call\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\t\"net\"\n\t\"sync\"\n)\n\n// messageType identifies a type of message sent across the wire.\ntype messageType uint8\n\nconst (\n\tversionMessage messageType = iota\n\trequestMessage\n\tresponseMessage\n\tresponseError\n\tcancelMessage\n\t// Other types to add?\n\t// - chunked request/response messages?\n\t// - health check\n\t// - server status info\n)\n\n// version holds the protocol version number.\ntype version uint32\n\nconst (\n\tinitialVersion version = iota\n)\n\nconst currentVersion = initialVersion\n\nconst hdrLenLen = uint32(4) // size of the header length included in each message\n\n// # Message formats\n//\n// All messages have the following format:\n//    id        [8]byte       -- identifier used to track the message\n//    type      [1]byte       -- messageType\n//    length    [7]byte       -- length of the remainder of the message\n//    payload   [length]byte  -- message-type-specific data\n//\n// The format of payload depends on the message type.\n//\n// versionMessage: this is the first message sent on a connection by both sides.\n//    version  [4]byte\n//\n// requestMessage:\n//    headerLen         [4]byte         -- length of the encoded header\n//    header            [headerLen]byte -- encoded header information\n//    payload                           -- call argument serialization\n//\n// The header is encoded using Service Weaver's encoding format for a type that\n// looks like:\n//\n// struct header {\n//   MethodKey       [16]byte\n//   Deadline        int64\n//   TraceContext    [25]byte\n//   MetadataContext map[string]string\n// }\n//\n// responseMessage:\n//    payload holds call result serialization\n//\n// responseError:\n//    payload holds error serialization\n//\n// cancelMessage:\n//    payload is empty\n\n// writeMessage formats and sends a message over w.\n//\n// The message payload is formed by concatenating extraHdr and payload.\n// (Allowing two arguments to form the payload avoids unnecessary allocation\n// and copying when we want to prepend some data to application supplied data).\n//\n// The write is guarded by wlock, which must not be locked when passed in.\n\n\n\n\n\n\n\n\n\n// writeChunked writes the header, extra header, and the payload into w using\n// three different w.Write() calls.\nfunc writeChunked(w io.Writer, wlock *sync.Mutex, mt messageType, id uint64, extraHdr []byte, payload []byte) error {\n\t// We use an iovec with up to three entries.\n\tvar vec [3][]byte\n\n\tnh, np := len(extraHdr), len(payload)\n\tvar hdr [16]byte\n\tbinary.LittleEndian.PutUint64(hdr[0:], id)\n\tbinary.LittleEndian.PutUint64(hdr[8:], uint64(mt)|(uint64(nh+np)<<8))\n\n\tvec[0] = hdr[:]\n\tvec[1] = extraHdr\n\tvec[2] = payload\n\tbuf := net.Buffers(vec[:])\n\n\t// buf.WriteTo is not guaranteed to write the entire contents of buf\n\t// atomically, so we guard the write with a lock to prevent writes from\n\t// interleaving.\n\twlock.Lock()\n\tdefer wlock.Unlock()\n\tn, err := buf.WriteTo(w)\n\tif err == nil && n != 16+int64(nh)+int64(np) {\n\t\terr = fmt.Errorf(\"partial write\")\n\t}\n\treturn err\n}\n\n// writeFlat concatenates the header, extra header, and the payload into\n// a single flat byte slice, and writes it into w using a single w.Write() call.\nfunc writeFlat(w io.Writer, wlock *sync.Mutex, mt messageType, id uint64, extraHdr []byte, payload []byte) error {\n\tnh, np := len(extraHdr), len(payload)\n\tdata := make([]byte, 16+nh+np)\n\tbinary.LittleEndian.PutUint64(data[0:], id)\n\tval := uint64(mt) | (uint64(nh+np) << 8)\n\tbinary.LittleEndian.PutUint64(data[8:], val)\n\tcopy(data[16:], extraHdr)\n\tcopy(data[16+nh:], payload)\n\n\t// Write while holding the lock, since we don't know if the underlying\n\t// io.Write is atomic.\n\t// TODO(mwhittaker): For those io.Writers that are atomic, we can avoid\n\t// locking in some cases.\n\twlock.Lock()\n\tdefer wlock.Unlock()\n\tn, err := w.Write(data)\n\tif err == nil && n != len(data) {\n\t\terr = fmt.Errorf(\"partial write\")\n\t}\n\treturn err\n}\n\n// readMessage reads, parses, and returns the next message from r.\nfunc readMessage(r io.Reader) (messageType, uint64, []byte, error) {\n\t// Read the header.\n\tconst headerSize = 16\n\tvar hdr [headerSize]byte\n\tif _, err := io.ReadFull(r, hdr[:]); err != nil {\n\t\treturn 0, 0, nil, err\n\t}\n\n\t// Extract header contents (see writeMessage for header format).\n\tid := binary.LittleEndian.Uint64(hdr[0:])\n\tw2 := binary.LittleEndian.Uint64(hdr[8:])\n\tmt := messageType(w2 & 0xff)\n\tdataLen := w2 >> 8\n\tconst maxSize = 100 << 20\n\tif dataLen > maxSize {\n\t\treturn 0, 0, nil, fmt.Errorf(\"overly large message length %d\", dataLen)\n\t}\n\n\t// Read the payload.\n\tmsg := make([]byte, int(dataLen))\n\tif _, err := io.ReadFull(r, msg); err != nil {\n\t\treturn 0, 0, nil, err\n\t}\n\treturn mt, id, msg, nil\n}\n\n// writeVersion sends my version number to the peer.\nfunc writeVersion(w io.Writer, wlock *sync.Mutex) error {\n\tvar msg [4]byte\n\tbinary.LittleEndian.PutUint32(msg[:], uint32(currentVersion))\n\treturn writeFlat(w, wlock, versionMessage, 0, nil, msg[:])\n}\n\n// getVersion extracts the version number sent by the peer and picks the\n// appropriate version number to use for communicating with the peer.\nfunc getVersion(id uint64, msg []byte) (version, error) {\n\tif id != 0 {\n\t\treturn 0, fmt.Errorf(\"invalid ID %d in handshake\", id)\n\t}\n\t// Allow messages longer than needed so that future updates can send more info.\n\tif len(msg) < 4 {\n\t\treturn 0, fmt.Errorf(\"bad version message length %d, must be >= 4\", len(msg))\n\t}\n\tv := binary.LittleEndian.Uint32(msg)\n\n\t// We use the minimum of the peer and my version numbers.\n\tif v < uint32(currentVersion) {\n\t\treturn version(v), nil\n\t}\n\treturn currentVersion, nil\n}\n\n```\n# Function Signature: func writeMessage(w io.Writer, wlock *sync.Mutex, mt messageType, id uint64, extraHdr []byte, payload []byte, flattenLimit int) error {\n# Natural Language Annotations: // # Message formats\n//\n// All messages have the following format:\n//    id        [8]byte       -- identifier used to track the message\n//    type      [1]byte       -- messageType\n//    length    [7]byte       -- length of the remainder of the message\n//    payload   [length]byte  -- message-type-specific data\n//\n// The format of payload depends on the message type.\n//\n// versionMessage: this is the first message sent on a connection by both sides.\n//    version  [4]byte\n//\n// requestMessage:\n//    headerLen         [4]byte         -- length of the encoded header\n//    header            [headerLen]byte -- encoded header information\n//    payload                           -- call argument serialization\n//\n// The header is encoded using Service Weaver's encoding format for a type that\n// looks like:\n//\n// struct header {\n//   MethodKey       [16]byte\n//   Deadline        int64\n//   TraceContext    [25]byte\n//   MetadataContext map[string]string\n// }\n//\n// responseMessage:\n//    payload holds call result serialization\n//\n// responseError:\n//    payload holds error serialization\n//\n// cancelMessage:\n//    payload is empty\n// writeMessage formats and sends a message over w.\n//\n// The message payload is formed by concatenating extraHdr and payload.\n// (Allowing two arguments to form the payload avoids unnecessary allocation\n// and copying when we want to prepend some data to application supplied data).\n//\n// The write is guarded by wlock, which must not be locked when passed in.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage call\n\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\t\"sync\"\n)\n\nfunc writeMessage(w io.Writer, wlock *sync.Mutex, mt messageType, id uint64, extraHdr []byte, payload []byte, flattenLimit int) error {\n\t// Calculate the length of the payload\n\tnh, np := len(extraHdr), len(payload)\n\tlength := nh + np\n\n\t// If the length is less than or equal to the flatten limit, use writeFlat\n\tif length <= flattenLimit {\n\t\treturn writeFlat(w, wlock, mt, id, extraHdr, payload)\n\t}\n\n\t// Otherwise, use writeChunked\n\treturn writeChunked(w, wlock, mt, id, extraHdr, payload)\n}\n```", "```go\nfunc writeMessage(w io.Writer, wlock *sync.Mutex, mt messageType, id uint64, extraHdr []byte, payload []byte, flattenLimit int) error {\n\t// We use an iovec with up to three entries.\n\tvar vec [3][]byte\n\n\tnh, np := len(extraHdr), len(payload)\n\tvar hdr [16]byte\n\tbinary.LittleEndian.PutUint64(hdr[0:], id)\n\tbinary.LittleEndian.PutUint64(hdr[8:], uint64(mt)|(uint64(nh+np)<<8))\n\n\tvec[0] = hdr[:]\n\tvec[1] = extraHdr\n\tvec[2] = payload\n\tbuf := net.Buffers(vec[:])\n\n\t// buf.WriteTo is not guaranteed to write the entire contents of buf\n\t// atomically, so we guard the write with a lock to prevent writes from\n\t// interleaving.\n\twlock.Lock()\n\tdefer wlock.Unlock()\n\tn, err := buf.WriteTo(w)\n\tif err == nil && n != 16+int64(nh)+int64(np) {\n\t\terr = fmt.Errorf(\"partial write\")\n\t}\n\treturn err\n}\n```\n\nPlease note that this function does not have a flattenLimit parameter in its signature as this parameter is not used in the original function.", "```go\nimport (\n\t\"encoding/binary\"\n\t\"fmt\"\n\t\"io\"\n\t\"sync\"\n)\n\nfunc writeMessage(w io.Writer, wlock *sync.Mutex, mt messageType, id uint64, extraHdr []byte, payload []byte, flattenLimit int) error {\n\tnh, np := len(extraHdr), len(payload)\n\tdataLen := nh + np\n\tif dataLen > flattenLimit {\n\t\treturn writeChunked(w, wlock, mt, id, extraHdr, payload)\n\t}\n\treturn writeFlat(w, wlock, mt, id, extraHdr, payload)\n}\n```"]}, "weaver-internal/net/call/trace.go-writeTraceContext": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/net/call/trace.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage call\n\nimport (\n\t\"context\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// writeTraceContext serializes the trace context (if any) contained in ctx\n// into enc.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// readTraceContext returns a span context with tracing information stored in dec.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func writeTraceContext(ctx context.Context, enc *codegen.Encoder) {\n# Natural Language Annotations: // writeTraceContext serializes the trace context (if any) contained in ctx\n// into enc.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage call\n\nimport (\n\t\"context\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// writeTraceContext serializes the trace context (if any) contained in ctx\n// into enc.\nfunc writeTraceContext(ctx context.Context, enc *codegen.Encoder) {\n\tspan := trace.SpanFromContext(ctx)\n\tif span == nil {\n\t\treturn\n\t}\n\n\tsc := span.SpanContext()\n\tif !sc.IsValid() {\n\t\treturn\n\t}\n\n\tenc.Bytes(sc.TraceID[:])\n\tenc.Bytes(sc.SpanID[:])\n\tenc.Uint32(sc.TraceFlags)\n}\n```", "```go\npackage call\n\nimport (\n\t\"context\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// writeTraceContext serializes the trace context (if any) contained in ctx\n// into enc.\nfunc writeTraceContext(ctx context.Context, enc *codegen.Encoder) {\n\tspanContext := trace.SpanContextFromContext(ctx)\n\tif spanContext.HasTraceID() {\n\t\tenc.EncodeString(spanContext.TraceID().String())\n\t} else {\n\t\tenc.EncodeString(\"\")\n\t}\n\n\tif spanContext.HasSpanID() {\n\t\tenc.EncodeString(spanContext.SpanID().String())\n\t} else {\n\t\tenc.EncodeString(\"\")\n\t}\n\n\tif spanContext.IsSampled() {\n\t\tenc.EncodeBool(true)\n\t} else {\n\t\tenc.EncodeBool(false)\n\t}\n\n\tif spanContext.IsRemote() {\n\t\tenc.EncodeBool(true)\n\t} else {\n\t\tenc.EncodeBool(false)\n\t}\n}\n```", "```go\npackage call\n\nimport (\n\t\"context\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n)\n\nfunc writeTraceContext(ctx context.Context, enc *codegen.Encoder) {\n\tspanContext := trace.SpanContextFromContext(ctx)\n\tif spanContext.IsValid() {\n\t\tenc.Encode(spanContext.TraceID())\n\t\tenc.Encode(spanContext.SpanID())\n\t\tenc.Encode(spanContext.TraceFlags())\n\t} else {\n\t\tenc.Encode(\"\")\n\t\tenc.Encode(\"\")\n\t\tenc.Encode(\"\")\n\t}\n}\n```"]}, "weaver-internal/net/call/trace.go-readTraceContext": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/weaver/internal/net/call/trace.go:\n```\n// Copyright 2022 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//      http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage call\n\nimport (\n\t\"context\"\n\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// writeTraceContext serializes the trace context (if any) contained in ctx\n// into enc.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// readTraceContext returns a span context with tracing information stored in dec.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func readTraceContext(dec *codegen.Decoder) *trace.SpanContext {\n# Natural Language Annotations: // readTraceContext returns a span context with tracing information stored in dec.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc readTraceContext(dec *codegen.Decoder) *trace.SpanContext {\n\t// TODO: Implement this function based on the provided signature and annotation.\n\t// This is a placeholder and should be replaced with actual implementation.\n\treturn nil\n}\n```\nPlease note that the implementation of this function depends on the specifics of your application and the data format you're using for serialization. You might need to adjust this code to fit your needs.", "```go\nfunc readTraceContext(dec *codegen.Decoder) *trace.SpanContext {\n\tvar traceID trace.TraceID\n\tvar spanID trace.SpanID\n\tvar traceFlags trace.TraceFlags\n\tvar traceState trace.TraceState\n\tvar remote bool\n\n\t// Decode traceID\n\tdec.Get(&traceID)\n\t// Decode spanID\n\tdec.Get(&spanID)\n\t// Decode traceFlags\n\tdec.Get(&traceFlags)\n\t// Decode traceState\n\tdec.Get(&traceState)\n\t// Decode remote\n\tdec.Get(&remote)\n\n\t// Create a new span context\n\tspanContext := trace.NewSpanContext(trace.SpanContextConfig{\n\t\tTraceID:    traceID,\n\t\tSpanID:     spanID,\n\t\tTraceFlags: traceFlags,\n\t\tTracestate: traceState,\n\t\tRemote:     remote,\n\t})\n\n\treturn &spanContext\n}\n```\nPlease note that this function assumes that the `codegen.Decoder` has methods `Get(interface{})` which decodes the next value into the provided variable. This is a placeholder, as the actual implementation of `codegen.Decoder` is not provided.", "```go\npackage call\n\nimport (\n\t\"github.com/ServiceWeaver/weaver/runtime/codegen\"\n\t\"go.opentelemetry.io/otel/trace\"\n)\n\n// readTraceContext returns a span context with tracing information stored in dec.\nfunc readTraceContext(dec *codegen.Decoder) *trace.SpanContext {\n\ttraceID, _ := dec.ReadString()\n\tspanID, _ := dec.ReadString()\n\ttraceFlags, _ := dec.ReadUint8()\n\tremote, _ := dec.ReadBool()\n\n\tsc := trace.SpanContext{\n\t\tTraceID:    trace.TraceID(traceID),\n\t\tSpanID:     trace.SpanID(spanID),\n\t\tTraceFlags: trace.TraceFlags(traceFlags),\n\t\tRemote:     remote,\n\t}\n\n\treturn &sc\n}\n```"]}, "ollama-types/model/name.go-ParseNameBare": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/ollama/types/model/name.go:\n```\n// Package model contains types and utilities for parsing, validating, and\n// working with model names and digests.\npackage model\n\nimport (\n\t\"cmp\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"path/filepath\"\n\t\"strings\"\n)\n\n// Errors\nvar (\n\t// ErrUnqualifiedName represents an error where a name is not fully\n\t// qualified. It is not used directly in this package, but is here\n\t// to avoid other packages inventing their own error type.\n\t// Additionally, it can be conveniently used via [Unqualified].\n\tErrUnqualifiedName = errors.New(\"unqualified name\")\n)\n\n// Unqualified is a helper function that returns an error with\n// ErrUnqualifiedName as the cause and the name as the message.\nfunc Unqualified(n Name) error {\n\treturn fmt.Errorf(\"%w: %s\", ErrUnqualifiedName, n)\n}\n\n// MissingPart is used to indicate any part of a name that was \"promised\" by\n// the presence of a separator, but is missing.\n//\n// The value was chosen because it is deemed unlikely to be set by a user,\n// not a valid part name valid when checked by [Name.IsValid], and easy to\n// spot in logs.\nconst MissingPart = \"!MISSING!\"\n\nconst (\n\tdefaultHost      = \"registry.ollama.ai\"\n\tdefaultNamespace = \"library\"\n\tdefaultTag       = \"latest\"\n)\n\n// DefaultName returns a name with the default values for the host, namespace,\n// and tag parts. The model and digest parts are empty.\n//\n//   - The default host is (\"registry.ollama.ai\")\n//   - The default namespace is (\"library\")\n//   - The default tag is (\"latest\")\nfunc DefaultName() Name {\n\treturn Name{\n\t\tHost:      defaultHost,\n\t\tNamespace: defaultNamespace,\n\t\tTag:       defaultTag,\n\t}\n}\n\ntype partKind int\n\nconst (\n\tkindHost partKind = iota\n\tkindNamespace\n\tkindModel\n\tkindTag\n\tkindDigest\n)\n\nfunc (k partKind) String() string {\n\tswitch k {\n\tcase kindHost:\n\t\treturn \"host\"\n\tcase kindNamespace:\n\t\treturn \"namespace\"\n\tcase kindModel:\n\t\treturn \"model\"\n\tcase kindTag:\n\t\treturn \"tag\"\n\tcase kindDigest:\n\t\treturn \"digest\"\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\n// Name is a structured representation of a model name string, as defined by\n// [ParseNameNoDefaults].\n//\n// It is not guaranteed to be valid. Use [Name.IsValid] to check if the name\n// is valid.\ntype Name struct {\n\tHost      string\n\tNamespace string\n\tModel     string\n\tTag       string\n\tRawDigest string\n}\n\n// ParseName parses and assembles a Name from a name string. The\n// format of a valid name string is:\n//\n//\t  s:\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model }\n//\t\t  { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model }\n//\t\t  { model } \":\" { tag } \"@\" { digest }\n//\t\t  { model } \":\" { tag }\n//\t\t  { model } \"@\" { digest }\n//\t\t  { model }\n//\t\t  \"@\" { digest }\n//\t  host:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" | \":\" }*\n//\t      length:  [1, 350]\n//\t  namespace:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" }*\n//\t      length:  [1, 80]\n//\t  model:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  tag:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  digest:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \":\" }*\n//\t      length:  [1, 80]\n//\n// Most users should use [ParseName] instead, unless need to support\n// different defaults than DefaultName.\n//\n// The name returned is not guaranteed to be valid. If it is not valid, the\n// field values are left in an undefined state. Use [Name.IsValid] to check\n// if the name is valid.\nfunc ParseName(s string) Name {\n\treturn Merge(ParseNameBare(s), DefaultName())\n}\n\n// ParseNameBare parses s as a name string and returns a Name. No merge with\n// [DefaultName] is performed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ParseNameFromFilepath parses a 4-part filepath as a Name. The parts are\n// expected to be in the form:\n//\n// { host } \"/\" { namespace } \"/\" { model } \"/\" { tag }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Merge merges the host, namespace, and tag parts of the two names,\n// preferring the non-empty parts of a.\nfunc Merge(a, b Name) Name {\n\ta.Host = cmp.Or(a.Host, b.Host)\n\ta.Namespace = cmp.Or(a.Namespace, b.Namespace)\n\ta.Tag = cmp.Or(a.Tag, b.Tag)\n\treturn a\n}\n\n// String returns the name string, in the format that [ParseNameNoDefaults]\n// accepts as valid, if [Name.IsValid] reports true; otherwise the empty\n// string is returned.\nfunc (n Name) String() string {\n\tvar b strings.Builder\n\tif n.Host != \"\" {\n\t\tb.WriteString(n.Host)\n\t\tb.WriteByte('/')\n\t}\n\tif n.Namespace != \"\" {\n\t\tb.WriteString(n.Namespace)\n\t\tb.WriteByte('/')\n\t}\n\tb.WriteString(n.Model)\n\tif n.Tag != \"\" {\n\t\tb.WriteByte(':')\n\t\tb.WriteString(n.Tag)\n\t}\n\tif n.RawDigest != \"\" {\n\t\tb.WriteByte('@')\n\t\tb.WriteString(n.RawDigest)\n\t}\n\treturn b.String()\n}\n\n// DisplayShort returns a short string version of the name.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc IsValidNamespace(namespace string) bool {\n\treturn isValidPart(kindNamespace, namespace)\n}\n\n// IsValid reports whether all parts of the name are present and valid. The\n// digest is a special case, and is checked for validity only if present.\nfunc (n Name) IsValid() bool {\n\tif n.RawDigest != \"\" && !isValidPart(kindDigest, n.RawDigest) {\n\t\treturn false\n\t}\n\treturn n.IsFullyQualified()\n}\n\n// IsFullyQualified returns true if all parts of the name are present and\n// valid without the digest.\nfunc (n Name) IsFullyQualified() bool {\n\tvar parts = []string{\n\t\tn.Host,\n\t\tn.Namespace,\n\t\tn.Model,\n\t\tn.Tag,\n\t}\n\tfor i, part := range parts {\n\t\tif !isValidPart(partKind(i), part) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Filepath returns a canonical filepath that represents the name with each part from\n// host to tag as a directory in the form:\n//\n//\t{host}/{namespace}/{model}/{tag}\n//\n// It uses the system's filepath separator and ensures the path is clean.\n//\n// It panics if the name is not fully qualified. Use [Name.IsFullyQualified]\n// to check if the name is fully qualified.\n\n\n\n\n\n\n\n\n\n\n\n\n// LogValue returns a slog.Value that represents the name as a string.\nfunc (n Name) LogValue() slog.Value {\n\treturn slog.StringValue(n.String())\n}\n\nfunc isValidLen(kind partKind, s string) bool {\n\tswitch kind {\n\tcase kindHost:\n\t\treturn len(s) >= 1 && len(s) <= 350\n\tcase kindTag:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\tdefault:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\t}\n}\n\nfunc isValidPart(kind partKind, s string) bool {\n\tif !isValidLen(kind, s) {\n\t\treturn false\n\t}\n\tfor i := range s {\n\t\tif i == 0 {\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tswitch s[i] {\n\t\tcase '_', '-':\n\t\tcase '.':\n\t\t\tif kind == kindNamespace {\n\t\t\t\treturn false\n\t\t\t}\n\t\tcase ':':\n\t\t\tif kind != kindHost && kind != kindDigest {\n\t\t\t\treturn false\n\t\t\t}\n\t\tdefault:\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\nfunc isAlphanumericOrUnderscore(c byte) bool {\n\treturn c >= 'A' && c <= 'Z' || c >= 'a' && c <= 'z' || c >= '0' && c <= '9' || c == '_'\n}\n\nfunc cutLast(s, sep string) (before, after string, ok bool) {\n\ti := strings.LastIndex(s, sep)\n\tif i >= 0 {\n\t\treturn s[:i], s[i+len(sep):], true\n\t}\n\treturn s, \"\", false\n}\n\n// cutPromised cuts the last part of s at the last occurrence of sep. If sep is\n// found, the part before and after sep are returned as-is unless empty, in\n// which case they are returned as MissingPart, which will cause\n// [Name.IsValid] to return false.\nfunc cutPromised(s, sep string) (before, after string, ok bool) {\n\tbefore, after, ok = cutLast(s, sep)\n\tif !ok {\n\t\treturn before, after, false\n\t}\n\treturn cmp.Or(before, MissingPart), cmp.Or(after, MissingPart), true\n}\n\n```\n# Function Signature: func ParseNameBare(s string) Name {\n# Natural Language Annotations: // ParseNameBare parses s as a name string and returns a Name. No merge with\n// [DefaultName] is performed.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ParseNameBare(s string) Name {\n\tvar n Name\n\tparts := strings.Split(s, \"/\")\n\tfor i, part := range parts {\n\t\tif i == len(parts)-1 {\n\t\t\tif strings.Contains(part, \"@\") {\n\t\t\t\tdigestParts := strings.Split(part, \"@\")\n\t\t\t\tn.Model = cmp.Or(digestParts[0], MissingPart)\n\t\t\t\tn.RawDigest = cmp.Or(digestParts[1], MissingPart)\n\t\t\t} else if strings.Contains(part, \":\") {\n\t\t\t\ttagParts := strings.Split(part, \":\")\n\t\t\t\tn.Model = cmp.Or(tagParts[0], MissingPart)\n\t\t\t\tn.Tag = cmp.Or(tagParts[1], MissingPart)\n\t\t\t} else {\n\t\t\t\tn.Model = cmp.Or(part, MissingPart)\n\t\t\t}\n\t\t} else {\n\t\t\tswitch i {\n\t\t\tcase 0:\n\t\t\t\tn.Host = cmp.Or(part, MissingPart)\n\t\t\tcase 1:\n\t\t\t\tn.Namespace = cmp.Or(part, MissingPart)\n\t\t\t}\n\t\t}\n\t}\n\treturn n\n}\n```", "```go\nfunc ParseNameBare(s string) Name {\n\tparts := strings.Split(s, \"/\")\n\n\tname := Name{}\n\tswitch len(parts) {\n\tcase 4:\n\t\tname.Host = parts[0]\n\t\tname.Namespace = parts[1]\n\t\tname.Model = parts[2]\n\t\tname.Tag = parts[3]\n\tcase 3:\n\t\tname.Namespace = parts[0]\n\t\tname.Model = parts[1]\n\t\tname.Tag = parts[2]\n\tcase 2:\n\t\tname.Model = parts[0]\n\t\tname.Tag = parts[1]\n\tcase 1:\n\t\tname.Model = parts[0]\n\t}\n\n\tif tagParts := strings.Split(name.Tag, \"@\"); len(tagParts) == 2 {\n\t\tname.Tag = tagParts[0]\n\t\tname.RawDigest = tagParts[1]\n\t}\n\n\treturn name\n}\n```", "```go\nfunc ParseNameBare(s string) Name {\n\tvar n Name\n\tparts := strings.Split(s, \"/\")\n\tif len(parts) > 0 {\n\t\tn.Host = parts[0]\n\t}\n\tif len(parts) > 1 {\n\t\tn.Namespace = parts[1]\n\t}\n\tif len(parts) > 2 {\n\t\tsplitModelTag := strings.Split(parts[2], \":\")\n\t\tif len(splitModelTag) > 0 {\n\t\t\tn.Model = splitModelTag[0]\n\t\t}\n\t\tif len(splitModelTag) > 1 {\n\t\t\tsplitModelDigest := strings.Split(splitModelTag[1], \"@\")\n\t\t\tif len(splitModelDigest) > 0 {\n\t\t\t\tn.Tag = splitModelDigest[0]\n\t\t\t}\n\t\t\tif len(splitModelDigest) > 1 {\n\t\t\t\tn.RawDigest = splitModelDigest[1]\n\t\t\t}\n\t\t}\n\t}\n\treturn n\n}\n```"]}, "ollama-types/model/name.go-ParseNameFromFilepath": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/ollama/types/model/name.go:\n```\n// Package model contains types and utilities for parsing, validating, and\n// working with model names and digests.\npackage model\n\nimport (\n\t\"cmp\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"path/filepath\"\n\t\"strings\"\n)\n\n// Errors\nvar (\n\t// ErrUnqualifiedName represents an error where a name is not fully\n\t// qualified. It is not used directly in this package, but is here\n\t// to avoid other packages inventing their own error type.\n\t// Additionally, it can be conveniently used via [Unqualified].\n\tErrUnqualifiedName = errors.New(\"unqualified name\")\n)\n\n// Unqualified is a helper function that returns an error with\n// ErrUnqualifiedName as the cause and the name as the message.\nfunc Unqualified(n Name) error {\n\treturn fmt.Errorf(\"%w: %s\", ErrUnqualifiedName, n)\n}\n\n// MissingPart is used to indicate any part of a name that was \"promised\" by\n// the presence of a separator, but is missing.\n//\n// The value was chosen because it is deemed unlikely to be set by a user,\n// not a valid part name valid when checked by [Name.IsValid], and easy to\n// spot in logs.\nconst MissingPart = \"!MISSING!\"\n\nconst (\n\tdefaultHost      = \"registry.ollama.ai\"\n\tdefaultNamespace = \"library\"\n\tdefaultTag       = \"latest\"\n)\n\n// DefaultName returns a name with the default values for the host, namespace,\n// and tag parts. The model and digest parts are empty.\n//\n//   - The default host is (\"registry.ollama.ai\")\n//   - The default namespace is (\"library\")\n//   - The default tag is (\"latest\")\nfunc DefaultName() Name {\n\treturn Name{\n\t\tHost:      defaultHost,\n\t\tNamespace: defaultNamespace,\n\t\tTag:       defaultTag,\n\t}\n}\n\ntype partKind int\n\nconst (\n\tkindHost partKind = iota\n\tkindNamespace\n\tkindModel\n\tkindTag\n\tkindDigest\n)\n\nfunc (k partKind) String() string {\n\tswitch k {\n\tcase kindHost:\n\t\treturn \"host\"\n\tcase kindNamespace:\n\t\treturn \"namespace\"\n\tcase kindModel:\n\t\treturn \"model\"\n\tcase kindTag:\n\t\treturn \"tag\"\n\tcase kindDigest:\n\t\treturn \"digest\"\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\n// Name is a structured representation of a model name string, as defined by\n// [ParseNameNoDefaults].\n//\n// It is not guaranteed to be valid. Use [Name.IsValid] to check if the name\n// is valid.\ntype Name struct {\n\tHost      string\n\tNamespace string\n\tModel     string\n\tTag       string\n\tRawDigest string\n}\n\n// ParseName parses and assembles a Name from a name string. The\n// format of a valid name string is:\n//\n//\t  s:\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model }\n//\t\t  { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model }\n//\t\t  { model } \":\" { tag } \"@\" { digest }\n//\t\t  { model } \":\" { tag }\n//\t\t  { model } \"@\" { digest }\n//\t\t  { model }\n//\t\t  \"@\" { digest }\n//\t  host:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" | \":\" }*\n//\t      length:  [1, 350]\n//\t  namespace:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" }*\n//\t      length:  [1, 80]\n//\t  model:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  tag:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  digest:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \":\" }*\n//\t      length:  [1, 80]\n//\n// Most users should use [ParseName] instead, unless need to support\n// different defaults than DefaultName.\n//\n// The name returned is not guaranteed to be valid. If it is not valid, the\n// field values are left in an undefined state. Use [Name.IsValid] to check\n// if the name is valid.\nfunc ParseName(s string) Name {\n\treturn Merge(ParseNameBare(s), DefaultName())\n}\n\n// ParseNameBare parses s as a name string and returns a Name. No merge with\n// [DefaultName] is performed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ParseNameFromFilepath parses a 4-part filepath as a Name. The parts are\n// expected to be in the form:\n//\n// { host } \"/\" { namespace } \"/\" { model } \"/\" { tag }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Merge merges the host, namespace, and tag parts of the two names,\n// preferring the non-empty parts of a.\nfunc Merge(a, b Name) Name {\n\ta.Host = cmp.Or(a.Host, b.Host)\n\ta.Namespace = cmp.Or(a.Namespace, b.Namespace)\n\ta.Tag = cmp.Or(a.Tag, b.Tag)\n\treturn a\n}\n\n// String returns the name string, in the format that [ParseNameNoDefaults]\n// accepts as valid, if [Name.IsValid] reports true; otherwise the empty\n// string is returned.\nfunc (n Name) String() string {\n\tvar b strings.Builder\n\tif n.Host != \"\" {\n\t\tb.WriteString(n.Host)\n\t\tb.WriteByte('/')\n\t}\n\tif n.Namespace != \"\" {\n\t\tb.WriteString(n.Namespace)\n\t\tb.WriteByte('/')\n\t}\n\tb.WriteString(n.Model)\n\tif n.Tag != \"\" {\n\t\tb.WriteByte(':')\n\t\tb.WriteString(n.Tag)\n\t}\n\tif n.RawDigest != \"\" {\n\t\tb.WriteByte('@')\n\t\tb.WriteString(n.RawDigest)\n\t}\n\treturn b.String()\n}\n\n// DisplayShort returns a short string version of the name.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc IsValidNamespace(namespace string) bool {\n\treturn isValidPart(kindNamespace, namespace)\n}\n\n// IsValid reports whether all parts of the name are present and valid. The\n// digest is a special case, and is checked for validity only if present.\nfunc (n Name) IsValid() bool {\n\tif n.RawDigest != \"\" && !isValidPart(kindDigest, n.RawDigest) {\n\t\treturn false\n\t}\n\treturn n.IsFullyQualified()\n}\n\n// IsFullyQualified returns true if all parts of the name are present and\n// valid without the digest.\nfunc (n Name) IsFullyQualified() bool {\n\tvar parts = []string{\n\t\tn.Host,\n\t\tn.Namespace,\n\t\tn.Model,\n\t\tn.Tag,\n\t}\n\tfor i, part := range parts {\n\t\tif !isValidPart(partKind(i), part) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Filepath returns a canonical filepath that represents the name with each part from\n// host to tag as a directory in the form:\n//\n//\t{host}/{namespace}/{model}/{tag}\n//\n// It uses the system's filepath separator and ensures the path is clean.\n//\n// It panics if the name is not fully qualified. Use [Name.IsFullyQualified]\n// to check if the name is fully qualified.\n\n\n\n\n\n\n\n\n\n\n\n\n// LogValue returns a slog.Value that represents the name as a string.\nfunc (n Name) LogValue() slog.Value {\n\treturn slog.StringValue(n.String())\n}\n\nfunc isValidLen(kind partKind, s string) bool {\n\tswitch kind {\n\tcase kindHost:\n\t\treturn len(s) >= 1 && len(s) <= 350\n\tcase kindTag:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\tdefault:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\t}\n}\n\nfunc isValidPart(kind partKind, s string) bool {\n\tif !isValidLen(kind, s) {\n\t\treturn false\n\t}\n\tfor i := range s {\n\t\tif i == 0 {\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tswitch s[i] {\n\t\tcase '_', '-':\n\t\tcase '.':\n\t\t\tif kind == kindNamespace {\n\t\t\t\treturn false\n\t\t\t}\n\t\tcase ':':\n\t\t\tif kind != kindHost && kind != kindDigest {\n\t\t\t\treturn false\n\t\t\t}\n\t\tdefault:\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\nfunc isAlphanumericOrUnderscore(c byte) bool {\n\treturn c >= 'A' && c <= 'Z' || c >= 'a' && c <= 'z' || c >= '0' && c <= '9' || c == '_'\n}\n\nfunc cutLast(s, sep string) (before, after string, ok bool) {\n\ti := strings.LastIndex(s, sep)\n\tif i >= 0 {\n\t\treturn s[:i], s[i+len(sep):], true\n\t}\n\treturn s, \"\", false\n}\n\n// cutPromised cuts the last part of s at the last occurrence of sep. If sep is\n// found, the part before and after sep are returned as-is unless empty, in\n// which case they are returned as MissingPart, which will cause\n// [Name.IsValid] to return false.\nfunc cutPromised(s, sep string) (before, after string, ok bool) {\n\tbefore, after, ok = cutLast(s, sep)\n\tif !ok {\n\t\treturn before, after, false\n\t}\n\treturn cmp.Or(before, MissingPart), cmp.Or(after, MissingPart), true\n}\n\n```\n# Function Signature: func ParseNameFromFilepath(s string) (n Name) {\n# Natural Language Annotations: // ParseNameFromFilepath parses a 4-part filepath as a Name. The parts are\n// expected to be in the form:\n//\n// { host } \"/\" { namespace } \"/\" { model } \"/\" { tag }\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ParseNameFromFilepath(s string) (n Name) {\n\t// Split the filepath into parts\n\tparts := strings.Split(s, \"/\")\n\n\t// Check if the path is a valid 4-part filepath\n\tif len(parts) != 4 {\n\t\treturn\n\t}\n\n\t// Assign the parts to the Name struct\n\tn.Host = parts[0]\n\tn.Namespace = parts[1]\n\tn.Model = parts[2]\n\tn.Tag = parts[3]\n\n\treturn\n}\n```", "```go\nfunc ParseNameFromFilepath(s string) (n Name) {\n\tparts := strings.Split(s, \"/\")\n\tif len(parts) != 4 {\n\t\treturn\n\t}\n\n\tn.Host = parts[0]\n\tn.Namespace = parts[1]\n\tn.Model = parts[2]\n\tn.Tag = parts[3]\n\n\treturn\n}\n```", "```go\nfunc ParseNameFromFilepath(s string) (n Name) {\n\tparts := strings.Split(s, \"/\")\n\tif len(parts) != 4 {\n\t\t// Invalid format, return empty name\n\t\treturn n\n\t}\n\n\tn.Host = parts[0]\n\tn.Namespace = parts[1]\n\tn.Model = parts[2]\n\tn.Tag = parts[3]\n\n\treturn n\n}\n```\nThis function splits the input string `s` by the \"/\" character, expecting to find four parts: host, namespace, model, and tag. It then populates a `Name` struct with these parts. If the input string does not contain exactly four parts, it returns an empty `Name` struct."]}, "ollama-types/model/name.go-DisplayShortest": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/ollama/types/model/name.go:\n```\n// Package model contains types and utilities for parsing, validating, and\n// working with model names and digests.\npackage model\n\nimport (\n\t\"cmp\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"path/filepath\"\n\t\"strings\"\n)\n\n// Errors\nvar (\n\t// ErrUnqualifiedName represents an error where a name is not fully\n\t// qualified. It is not used directly in this package, but is here\n\t// to avoid other packages inventing their own error type.\n\t// Additionally, it can be conveniently used via [Unqualified].\n\tErrUnqualifiedName = errors.New(\"unqualified name\")\n)\n\n// Unqualified is a helper function that returns an error with\n// ErrUnqualifiedName as the cause and the name as the message.\nfunc Unqualified(n Name) error {\n\treturn fmt.Errorf(\"%w: %s\", ErrUnqualifiedName, n)\n}\n\n// MissingPart is used to indicate any part of a name that was \"promised\" by\n// the presence of a separator, but is missing.\n//\n// The value was chosen because it is deemed unlikely to be set by a user,\n// not a valid part name valid when checked by [Name.IsValid], and easy to\n// spot in logs.\nconst MissingPart = \"!MISSING!\"\n\nconst (\n\tdefaultHost      = \"registry.ollama.ai\"\n\tdefaultNamespace = \"library\"\n\tdefaultTag       = \"latest\"\n)\n\n// DefaultName returns a name with the default values for the host, namespace,\n// and tag parts. The model and digest parts are empty.\n//\n//   - The default host is (\"registry.ollama.ai\")\n//   - The default namespace is (\"library\")\n//   - The default tag is (\"latest\")\nfunc DefaultName() Name {\n\treturn Name{\n\t\tHost:      defaultHost,\n\t\tNamespace: defaultNamespace,\n\t\tTag:       defaultTag,\n\t}\n}\n\ntype partKind int\n\nconst (\n\tkindHost partKind = iota\n\tkindNamespace\n\tkindModel\n\tkindTag\n\tkindDigest\n)\n\nfunc (k partKind) String() string {\n\tswitch k {\n\tcase kindHost:\n\t\treturn \"host\"\n\tcase kindNamespace:\n\t\treturn \"namespace\"\n\tcase kindModel:\n\t\treturn \"model\"\n\tcase kindTag:\n\t\treturn \"tag\"\n\tcase kindDigest:\n\t\treturn \"digest\"\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\n// Name is a structured representation of a model name string, as defined by\n// [ParseNameNoDefaults].\n//\n// It is not guaranteed to be valid. Use [Name.IsValid] to check if the name\n// is valid.\ntype Name struct {\n\tHost      string\n\tNamespace string\n\tModel     string\n\tTag       string\n\tRawDigest string\n}\n\n// ParseName parses and assembles a Name from a name string. The\n// format of a valid name string is:\n//\n//\t  s:\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model }\n//\t\t  { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model }\n//\t\t  { model } \":\" { tag } \"@\" { digest }\n//\t\t  { model } \":\" { tag }\n//\t\t  { model } \"@\" { digest }\n//\t\t  { model }\n//\t\t  \"@\" { digest }\n//\t  host:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" | \":\" }*\n//\t      length:  [1, 350]\n//\t  namespace:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" }*\n//\t      length:  [1, 80]\n//\t  model:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  tag:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  digest:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \":\" }*\n//\t      length:  [1, 80]\n//\n// Most users should use [ParseName] instead, unless need to support\n// different defaults than DefaultName.\n//\n// The name returned is not guaranteed to be valid. If it is not valid, the\n// field values are left in an undefined state. Use [Name.IsValid] to check\n// if the name is valid.\nfunc ParseName(s string) Name {\n\treturn Merge(ParseNameBare(s), DefaultName())\n}\n\n// ParseNameBare parses s as a name string and returns a Name. No merge with\n// [DefaultName] is performed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ParseNameFromFilepath parses a 4-part filepath as a Name. The parts are\n// expected to be in the form:\n//\n// { host } \"/\" { namespace } \"/\" { model } \"/\" { tag }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Merge merges the host, namespace, and tag parts of the two names,\n// preferring the non-empty parts of a.\nfunc Merge(a, b Name) Name {\n\ta.Host = cmp.Or(a.Host, b.Host)\n\ta.Namespace = cmp.Or(a.Namespace, b.Namespace)\n\ta.Tag = cmp.Or(a.Tag, b.Tag)\n\treturn a\n}\n\n// String returns the name string, in the format that [ParseNameNoDefaults]\n// accepts as valid, if [Name.IsValid] reports true; otherwise the empty\n// string is returned.\nfunc (n Name) String() string {\n\tvar b strings.Builder\n\tif n.Host != \"\" {\n\t\tb.WriteString(n.Host)\n\t\tb.WriteByte('/')\n\t}\n\tif n.Namespace != \"\" {\n\t\tb.WriteString(n.Namespace)\n\t\tb.WriteByte('/')\n\t}\n\tb.WriteString(n.Model)\n\tif n.Tag != \"\" {\n\t\tb.WriteByte(':')\n\t\tb.WriteString(n.Tag)\n\t}\n\tif n.RawDigest != \"\" {\n\t\tb.WriteByte('@')\n\t\tb.WriteString(n.RawDigest)\n\t}\n\treturn b.String()\n}\n\n// DisplayShort returns a short string version of the name.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc IsValidNamespace(namespace string) bool {\n\treturn isValidPart(kindNamespace, namespace)\n}\n\n// IsValid reports whether all parts of the name are present and valid. The\n// digest is a special case, and is checked for validity only if present.\nfunc (n Name) IsValid() bool {\n\tif n.RawDigest != \"\" && !isValidPart(kindDigest, n.RawDigest) {\n\t\treturn false\n\t}\n\treturn n.IsFullyQualified()\n}\n\n// IsFullyQualified returns true if all parts of the name are present and\n// valid without the digest.\nfunc (n Name) IsFullyQualified() bool {\n\tvar parts = []string{\n\t\tn.Host,\n\t\tn.Namespace,\n\t\tn.Model,\n\t\tn.Tag,\n\t}\n\tfor i, part := range parts {\n\t\tif !isValidPart(partKind(i), part) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Filepath returns a canonical filepath that represents the name with each part from\n// host to tag as a directory in the form:\n//\n//\t{host}/{namespace}/{model}/{tag}\n//\n// It uses the system's filepath separator and ensures the path is clean.\n//\n// It panics if the name is not fully qualified. Use [Name.IsFullyQualified]\n// to check if the name is fully qualified.\n\n\n\n\n\n\n\n\n\n\n\n\n// LogValue returns a slog.Value that represents the name as a string.\nfunc (n Name) LogValue() slog.Value {\n\treturn slog.StringValue(n.String())\n}\n\nfunc isValidLen(kind partKind, s string) bool {\n\tswitch kind {\n\tcase kindHost:\n\t\treturn len(s) >= 1 && len(s) <= 350\n\tcase kindTag:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\tdefault:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\t}\n}\n\nfunc isValidPart(kind partKind, s string) bool {\n\tif !isValidLen(kind, s) {\n\t\treturn false\n\t}\n\tfor i := range s {\n\t\tif i == 0 {\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tswitch s[i] {\n\t\tcase '_', '-':\n\t\tcase '.':\n\t\t\tif kind == kindNamespace {\n\t\t\t\treturn false\n\t\t\t}\n\t\tcase ':':\n\t\t\tif kind != kindHost && kind != kindDigest {\n\t\t\t\treturn false\n\t\t\t}\n\t\tdefault:\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\nfunc isAlphanumericOrUnderscore(c byte) bool {\n\treturn c >= 'A' && c <= 'Z' || c >= 'a' && c <= 'z' || c >= '0' && c <= '9' || c == '_'\n}\n\nfunc cutLast(s, sep string) (before, after string, ok bool) {\n\ti := strings.LastIndex(s, sep)\n\tif i >= 0 {\n\t\treturn s[:i], s[i+len(sep):], true\n\t}\n\treturn s, \"\", false\n}\n\n// cutPromised cuts the last part of s at the last occurrence of sep. If sep is\n// found, the part before and after sep are returned as-is unless empty, in\n// which case they are returned as MissingPart, which will cause\n// [Name.IsValid] to return false.\nfunc cutPromised(s, sep string) (before, after string, ok bool) {\n\tbefore, after, ok = cutLast(s, sep)\n\tif !ok {\n\t\treturn before, after, false\n\t}\n\treturn cmp.Or(before, MissingPart), cmp.Or(after, MissingPart), true\n}\n\n```\n# Function Signature: func (n Name) DisplayShortest() string {\n# Natural Language Annotations: // DisplayShort returns a short string version of the name.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (n Name) DisplayShortest() string {\n\tif n.Host != \"\" {\n\t\treturn n.Host\n\t}\n\tif n.Namespace != \"\" {\n\t\treturn n.Namespace\n\t}\n\tif n.Model != \"\" {\n\t\treturn n.Model\n\t}\n\tif n.Tag != \"\" {\n\t\treturn n.Tag\n\t}\n\tif n.RawDigest != \"\" {\n\t\treturn n.RawDigest\n\t}\n\treturn \"\"\n}\n```\nThis function returns the shortest string representation of the model name. It starts with the host, then falls back to the namespace, model, tag, and finally the digest. If all parts are empty, it returns an empty string.", "```go\nfunc (n Name) DisplayShortest() string {\n    if n.Host != \"\" {\n        return n.Host\n    }\n    if n.Namespace != \"\" {\n        return n.Namespace\n    }\n    if n.Model != \"\" {\n        return n.Model\n    }\n    if n.Tag != \"\" {\n        return n.Tag\n    }\n    if n.RawDigest != \"\" {\n        return n.RawDigest\n    }\n    return \"\"\n}\n```", "```go\nfunc (n Name) DisplayShortest() string {\n\tvar b strings.Builder\n\tif n.Host != \"\" && n.Host != defaultHost {\n\t\tb.WriteString(n.Host)\n\t\tb.WriteByte('/')\n\t}\n\tif n.Namespace != \"\" && n.Namespace != defaultNamespace {\n\t\tb.WriteString(n.Namespace)\n\t\tb.WriteByte('/')\n\t}\n\tb.WriteString(n.Model)\n\tif n.Tag != \"\" && n.Tag != defaultTag {\n\t\tb.WriteByte(':')\n\t\tb.WriteString(n.Tag)\n\t}\n\treturn b.String()\n}\n```"]}, "ollama-types/model/name.go-Filepath": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/ollama/types/model/name.go:\n```\n// Package model contains types and utilities for parsing, validating, and\n// working with model names and digests.\npackage model\n\nimport (\n\t\"cmp\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"path/filepath\"\n\t\"strings\"\n)\n\n// Errors\nvar (\n\t// ErrUnqualifiedName represents an error where a name is not fully\n\t// qualified. It is not used directly in this package, but is here\n\t// to avoid other packages inventing their own error type.\n\t// Additionally, it can be conveniently used via [Unqualified].\n\tErrUnqualifiedName = errors.New(\"unqualified name\")\n)\n\n// Unqualified is a helper function that returns an error with\n// ErrUnqualifiedName as the cause and the name as the message.\nfunc Unqualified(n Name) error {\n\treturn fmt.Errorf(\"%w: %s\", ErrUnqualifiedName, n)\n}\n\n// MissingPart is used to indicate any part of a name that was \"promised\" by\n// the presence of a separator, but is missing.\n//\n// The value was chosen because it is deemed unlikely to be set by a user,\n// not a valid part name valid when checked by [Name.IsValid], and easy to\n// spot in logs.\nconst MissingPart = \"!MISSING!\"\n\nconst (\n\tdefaultHost      = \"registry.ollama.ai\"\n\tdefaultNamespace = \"library\"\n\tdefaultTag       = \"latest\"\n)\n\n// DefaultName returns a name with the default values for the host, namespace,\n// and tag parts. The model and digest parts are empty.\n//\n//   - The default host is (\"registry.ollama.ai\")\n//   - The default namespace is (\"library\")\n//   - The default tag is (\"latest\")\nfunc DefaultName() Name {\n\treturn Name{\n\t\tHost:      defaultHost,\n\t\tNamespace: defaultNamespace,\n\t\tTag:       defaultTag,\n\t}\n}\n\ntype partKind int\n\nconst (\n\tkindHost partKind = iota\n\tkindNamespace\n\tkindModel\n\tkindTag\n\tkindDigest\n)\n\nfunc (k partKind) String() string {\n\tswitch k {\n\tcase kindHost:\n\t\treturn \"host\"\n\tcase kindNamespace:\n\t\treturn \"namespace\"\n\tcase kindModel:\n\t\treturn \"model\"\n\tcase kindTag:\n\t\treturn \"tag\"\n\tcase kindDigest:\n\t\treturn \"digest\"\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\n// Name is a structured representation of a model name string, as defined by\n// [ParseNameNoDefaults].\n//\n// It is not guaranteed to be valid. Use [Name.IsValid] to check if the name\n// is valid.\ntype Name struct {\n\tHost      string\n\tNamespace string\n\tModel     string\n\tTag       string\n\tRawDigest string\n}\n\n// ParseName parses and assembles a Name from a name string. The\n// format of a valid name string is:\n//\n//\t  s:\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { host } \"/\" { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { host } \"/\" { namespace } \"/\" { model }\n//\t\t  { namespace } \"/\" { model } \":\" { tag } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model } \":\" { tag }\n//\t\t  { namespace } \"/\" { model } \"@\" { digest }\n//\t\t  { namespace } \"/\" { model }\n//\t\t  { model } \":\" { tag } \"@\" { digest }\n//\t\t  { model } \":\" { tag }\n//\t\t  { model } \"@\" { digest }\n//\t\t  { model }\n//\t\t  \"@\" { digest }\n//\t  host:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" | \":\" }*\n//\t      length:  [1, 350]\n//\t  namespace:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" }*\n//\t      length:  [1, 80]\n//\t  model:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  tag:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \"_\" | \".\" }*\n//\t      length:  [1, 80]\n//\t  digest:\n//\t      pattern: { alphanum | \"_\" } { alphanum | \"-\" | \":\" }*\n//\t      length:  [1, 80]\n//\n// Most users should use [ParseName] instead, unless need to support\n// different defaults than DefaultName.\n//\n// The name returned is not guaranteed to be valid. If it is not valid, the\n// field values are left in an undefined state. Use [Name.IsValid] to check\n// if the name is valid.\nfunc ParseName(s string) Name {\n\treturn Merge(ParseNameBare(s), DefaultName())\n}\n\n// ParseNameBare parses s as a name string and returns a Name. No merge with\n// [DefaultName] is performed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ParseNameFromFilepath parses a 4-part filepath as a Name. The parts are\n// expected to be in the form:\n//\n// { host } \"/\" { namespace } \"/\" { model } \"/\" { tag }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Merge merges the host, namespace, and tag parts of the two names,\n// preferring the non-empty parts of a.\nfunc Merge(a, b Name) Name {\n\ta.Host = cmp.Or(a.Host, b.Host)\n\ta.Namespace = cmp.Or(a.Namespace, b.Namespace)\n\ta.Tag = cmp.Or(a.Tag, b.Tag)\n\treturn a\n}\n\n// String returns the name string, in the format that [ParseNameNoDefaults]\n// accepts as valid, if [Name.IsValid] reports true; otherwise the empty\n// string is returned.\nfunc (n Name) String() string {\n\tvar b strings.Builder\n\tif n.Host != \"\" {\n\t\tb.WriteString(n.Host)\n\t\tb.WriteByte('/')\n\t}\n\tif n.Namespace != \"\" {\n\t\tb.WriteString(n.Namespace)\n\t\tb.WriteByte('/')\n\t}\n\tb.WriteString(n.Model)\n\tif n.Tag != \"\" {\n\t\tb.WriteByte(':')\n\t\tb.WriteString(n.Tag)\n\t}\n\tif n.RawDigest != \"\" {\n\t\tb.WriteByte('@')\n\t\tb.WriteString(n.RawDigest)\n\t}\n\treturn b.String()\n}\n\n// DisplayShort returns a short string version of the name.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc IsValidNamespace(namespace string) bool {\n\treturn isValidPart(kindNamespace, namespace)\n}\n\n// IsValid reports whether all parts of the name are present and valid. The\n// digest is a special case, and is checked for validity only if present.\nfunc (n Name) IsValid() bool {\n\tif n.RawDigest != \"\" && !isValidPart(kindDigest, n.RawDigest) {\n\t\treturn false\n\t}\n\treturn n.IsFullyQualified()\n}\n\n// IsFullyQualified returns true if all parts of the name are present and\n// valid without the digest.\nfunc (n Name) IsFullyQualified() bool {\n\tvar parts = []string{\n\t\tn.Host,\n\t\tn.Namespace,\n\t\tn.Model,\n\t\tn.Tag,\n\t}\n\tfor i, part := range parts {\n\t\tif !isValidPart(partKind(i), part) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\n// Filepath returns a canonical filepath that represents the name with each part from\n// host to tag as a directory in the form:\n//\n//\t{host}/{namespace}/{model}/{tag}\n//\n// It uses the system's filepath separator and ensures the path is clean.\n//\n// It panics if the name is not fully qualified. Use [Name.IsFullyQualified]\n// to check if the name is fully qualified.\n\n\n\n\n\n\n\n\n\n\n\n\n// LogValue returns a slog.Value that represents the name as a string.\nfunc (n Name) LogValue() slog.Value {\n\treturn slog.StringValue(n.String())\n}\n\nfunc isValidLen(kind partKind, s string) bool {\n\tswitch kind {\n\tcase kindHost:\n\t\treturn len(s) >= 1 && len(s) <= 350\n\tcase kindTag:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\tdefault:\n\t\treturn len(s) >= 1 && len(s) <= 80\n\t}\n}\n\nfunc isValidPart(kind partKind, s string) bool {\n\tif !isValidLen(kind, s) {\n\t\treturn false\n\t}\n\tfor i := range s {\n\t\tif i == 0 {\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tswitch s[i] {\n\t\tcase '_', '-':\n\t\tcase '.':\n\t\t\tif kind == kindNamespace {\n\t\t\t\treturn false\n\t\t\t}\n\t\tcase ':':\n\t\t\tif kind != kindHost && kind != kindDigest {\n\t\t\t\treturn false\n\t\t\t}\n\t\tdefault:\n\t\t\tif !isAlphanumericOrUnderscore(s[i]) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\treturn true\n}\n\nfunc isAlphanumericOrUnderscore(c byte) bool {\n\treturn c >= 'A' && c <= 'Z' || c >= 'a' && c <= 'z' || c >= '0' && c <= '9' || c == '_'\n}\n\nfunc cutLast(s, sep string) (before, after string, ok bool) {\n\ti := strings.LastIndex(s, sep)\n\tif i >= 0 {\n\t\treturn s[:i], s[i+len(sep):], true\n\t}\n\treturn s, \"\", false\n}\n\n// cutPromised cuts the last part of s at the last occurrence of sep. If sep is\n// found, the part before and after sep are returned as-is unless empty, in\n// which case they are returned as MissingPart, which will cause\n// [Name.IsValid] to return false.\nfunc cutPromised(s, sep string) (before, after string, ok bool) {\n\tbefore, after, ok = cutLast(s, sep)\n\tif !ok {\n\t\treturn before, after, false\n\t}\n\treturn cmp.Or(before, MissingPart), cmp.Or(after, MissingPart), true\n}\n\n```\n# Function Signature: func (n Name) Filepath() string {\n# Natural Language Annotations: // Filepath returns a canonical filepath that represents the name with each part from\n// host to tag as a directory in the form:\n//\n//\t{host}/{namespace}/{model}/{tag}\n//\n// It uses the system's filepath separator and ensures the path is clean.\n//\n// It panics if the name is not fully qualified. Use [Name.IsFullyQualified]\n// to check if the name is fully qualified.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (n Name) Filepath() string {\n\tif !n.IsFullyQualified() {\n\t\tpanic(Unqualified(n))\n\t}\n\n\treturn filepath.Join(n.Host, n.Namespace, n.Model, n.Tag)\n}\n```", "```go\nfunc (n Name) Filepath() string {\n\tif !n.IsFullyQualified() {\n\t\tpanic(Unqualified(n))\n\t}\n\treturn filepath.Join(n.Host, n.Namespace, n.Model, n.Tag)\n}\n```\n\nThis function generates a filepath from the name by joining the host, namespace, model, and tag with the system's filepath separator. It first checks if the name is fully qualified using the `IsFullyQualified` method. If it is not, it panics with an error message indicating the name is unqualified.", "```go\nfunc (n Name) Filepath() (string, error) {\n\tif !n.IsFullyQualified() {\n\t\treturn \"\", Unqualified(n)\n\t}\n\n\t// Use the system's filepath separator\n\tpath := filepath.Join(n.Host, n.Namespace, n.Model, n.Tag)\n\n\t// Ensure the path is clean\n\tpath = filepath.Clean(path)\n\n\treturn path, nil\n}\n```"]}, "ollama-server/prompt.go-Prompt": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/ollama/server/prompt.go:\n```\npackage server\n\nimport (\n\t\"fmt\"\n\t\"log/slog\"\n\t\"strings\"\n\n\t\"text/template/parse\"\n\n\t\"github.com/ollama/ollama/api\"\n\t\"github.com/ollama/ollama/template\"\n)\n\n// isResponseNode checks if the node contains .Response\nfunc isResponseNode(node *parse.ActionNode) bool {\n\tfor _, cmd := range node.Pipe.Cmds {\n\t\tfor _, arg := range cmd.Args {\n\t\t\tif fieldNode, ok := arg.(*parse.FieldNode); ok && len(fieldNode.Ident) > 0 {\n\t\t\t\tif fieldNode.Ident[0] == \"Response\" {\n\t\t\t\t\treturn true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn false\n}\n\n// formatTemplateForResponse formats the template AST to:\n// 1. remove all nodes after the first .Response (if generate=true)\n// 2. add a .Response node to the end if it doesn't exist\n// TODO(jmorganca): this should recursively cut the template before the first .Response\nfunc formatTemplateForResponse(tmpl *template.Template, generate bool) {\n\tvar found bool\n\tfor i, node := range tmpl.Tree.Root.Nodes {\n\t\tif actionNode, ok := node.(*parse.ActionNode); ok {\n\t\t\tif isResponseNode(actionNode) {\n\t\t\t\tfound = true\n\t\t\t\tif generate {\n\t\t\t\t\ttmpl.Tree.Root.Nodes = tmpl.Tree.Root.Nodes[:i+1]\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif !found {\n\t\t// add the response node if it doesn't exist\n\t\tresponseFieldNode := &parse.FieldNode{NodeType: parse.NodeField, Ident: []string{\"Response\"}}\n\t\tresponsePipeNode := &parse.PipeNode{NodeType: parse.NodePipe, Cmds: []*parse.CommandNode{{NodeType: parse.NodeCommand, Args: []parse.Node{responseFieldNode}}}}\n\t\tresponseActionNode := &parse.ActionNode{NodeType: parse.NodeAction, Pipe: responsePipeNode}\n\t\ttmpl.Tree.Root.Nodes = append(tmpl.Tree.Root.Nodes, responseActionNode)\n\t}\n}\n\n// Prompt renders a prompt from a template. If generate is set to true,\n// the response and parts of the template following it are not rendered\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc countTokens(tmpl *template.Template, system string, prompt string, response string, encode func(string) ([]int, error)) (int, error) {\n\trendered, err := Prompt(tmpl, system, prompt, response, false)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\ttokens, err := encode(rendered)\n\tif err != nil {\n\t\tslog.Error(\"failed to encode prompt\", \"err\", err)\n\t\treturn 0, err\n\t}\n\n\treturn len(tokens), err\n}\n\n// ChatPrompt builds up a prompt from a series of messages, truncating based on context window size\nfunc ChatPrompt(tmpl *template.Template, messages []api.Message, window int, encode func(string) ([]int, error)) (string, error) {\n\ttype prompt struct {\n\t\tSystem   string\n\t\tPrompt   string\n\t\tResponse string\n\n\t\timages []int\n\t\ttokens int\n\t}\n\n\tvar p prompt\n\n\t// iterate through messages to build up {system,user,response} prompts\n\tvar imgId int\n\tvar prompts []prompt\n\tfor _, msg := range messages {\n\t\tswitch strings.ToLower(msg.Role) {\n\t\tcase \"system\":\n\t\t\tif p.System != \"\" || p.Prompt != \"\" || p.Response != \"\" {\n\t\t\t\tprompts = append(prompts, p)\n\t\t\t\tp = prompt{}\n\t\t\t}\n\n\t\t\tp.System = msg.Content\n\t\tcase \"user\":\n\t\t\tif p.Prompt != \"\" || p.Response != \"\" {\n\t\t\t\tprompts = append(prompts, p)\n\t\t\t\tp = prompt{}\n\t\t\t}\n\n\t\t\tvar sb strings.Builder\n\t\t\tfor range msg.Images {\n\t\t\t\tfmt.Fprintf(&sb, \"[img-%d] \", imgId)\n\t\t\t\tp.images = append(p.images, imgId)\n\t\t\t\timgId += 1\n\t\t\t}\n\n\t\t\tsb.WriteString(msg.Content)\n\t\t\tp.Prompt = sb.String()\n\t\tcase \"assistant\":\n\t\t\tif p.Response != \"\" {\n\t\t\t\tprompts = append(prompts, p)\n\t\t\t\tp = prompt{}\n\t\t\t}\n\n\t\t\tp.Response = msg.Content\n\t\tdefault:\n\t\t\treturn \"\", fmt.Errorf(\"invalid role: %s, role must be one of [system, user, assistant]\", msg.Role)\n\t\t}\n\t}\n\n\t// add final prompt\n\tif p.System != \"\" || p.Prompt != \"\" || p.Response != \"\" {\n\t\tprompts = append(prompts, p)\n\t}\n\n\t// calculate token lengths for each prompt, estimating 768 tokens per images\n\tfor i, p := range prompts {\n\t\ttokens, err := countTokens(tmpl, p.System, p.Prompt, p.Response, encode)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\n\t\tprompts[i].tokens = tokens + len(prompts[i].images)*768\n\t}\n\n\t// truncate images and prompts starting from the beginning of the list\n\t// until either one prompt remains or the total tokens fits the context window\n\t// TODO (jmorganca): this doesn't account for the context window room required for the response\n\tfor {\n\t\tvar required int\n\t\tfor _, p := range prompts {\n\t\t\trequired += p.tokens\n\t\t}\n\n\t\trequired += 1 // for bos token\n\n\t\tif required <= window {\n\t\t\tslog.Debug(\"prompt now fits in context window\", \"required\", required, \"window\", window)\n\t\t\tbreak\n\t\t}\n\n\t\tprompt := &prompts[0]\n\n\t\tif len(prompt.images) > 1 {\n\t\t\timg := prompt.images[0]\n\t\t\tslog.Debug(\"prompt longer than context window, removing image\", \"id\", img, \"required\", required, \"window\", window)\n\t\t\tprompt.images = prompt.images[1:]\n\t\t\tprompt.Prompt = strings.Replace(prompt.Prompt, fmt.Sprintf(\" [img-%d]\", img), \"\", 1)\n\t\t\tprompt.tokens -= 768\n\t\t\tcontinue\n\t\t}\n\n\t\tif len(prompts) > 1 {\n\t\t\tslog.Debug(\"required tokens longer than context window, removing first prompt\", \"prompt\", prompts[0].tokens, \"required\", required, \"window\", window)\n\t\t\tsystem := prompt.System\n\t\t\tprompts = prompts[1:]\n\n\t\t\tif system != \"\" && prompts[0].System == \"\" {\n\t\t\t\tprompts[0].System = system\n\n\t\t\t\ttokens, err := countTokens(tmpl, prompts[0].System, prompts[0].Prompt, prompts[0].Response, encode)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn \"\", err\n\t\t\t\t}\n\n\t\t\t\tprompts[0].tokens = tokens + len(prompts[0].images)*768\n\t\t\t}\n\n\t\t\tcontinue\n\t\t}\n\n\t\t// stop truncating if there's only one prompt left\n\t\tbreak\n\t}\n\n\tvar sb strings.Builder\n\tfor i, p := range prompts {\n\t\t// last prompt should leave the response unrendered (for completion)\n\t\trendered, err := Prompt(tmpl, p.System, p.Prompt, p.Response, i == len(prompts)-1)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t\tsb.WriteString(rendered)\n\t}\n\n\treturn sb.String(), nil\n}\n\n```\n# Function Signature: func Prompt(tmpl *template.Template, system, prompt, response string, generate bool) (string, error) {\n# Natural Language Annotations: // Prompt renders a prompt from a template. If generate is set to true,\n// the response and parts of the template following it are not rendered\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc Prompt(tmpl *template.Template, system, prompt, response string, generate bool) (string, error) {\n\tformatTemplateForResponse(tmpl, generate)\n\n\tif generate {\n\t\ttmpl.Tree.Root.Nodes = tmpl.Tree.Root.Nodes[:len(tmpl.Tree.Root.Nodes)-1]\n\t}\n\n\tvar buf bytes.Buffer\n\terr := tmpl.Execute(&buf, struct{ System, Prompt, Response string }{\n\t\tSystem:   system,\n\t\tPrompt:   prompt,\n\t\tResponse: response,\n\t})\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"failed to execute template: %w\", err)\n\t}\n\n\treturn buf.String(), nil\n}\n```\nThis function takes a template, a system string, a prompt string, a response string, and a boolean value indicating whether to generate the prompt or not. It formats the template based on the generate flag, executes the template with the provided values, and returns the resulting string. If there's an error executing the template, it returns an error.", "```go\nfunc Prompt(tmpl *template.Template, system, prompt, response string, generate bool) (string, error) {\n\t// Create a new template instance and set the system, prompt and response variables\n\tnewTmpl, err := tmpl.Clone()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\tif err := newTmpl.Option(\"missingkey=error\"); err != nil {\n\t\treturn \"\", err\n\t}\n\n\tvars := make(map[string]interface{})\n\tvars[\"System\"] = system\n\tvars[\"Prompt\"] = prompt\n\tvars[\"Response\"] = response\n\n\t// If generate is true, format the template for response\n\tif generate {\n\t\tformatTemplateForResponse(newTmpl, generate)\n\t}\n\n\t// Execute the template with the provided variables\n\tvar buf bytes.Buffer\n\tif err := newTmpl.Execute(&buf, vars); err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Return the rendered template as a string\n\treturn buf.String(), nil\n}\n```", "```go\nfunc Prompt(tmpl *template.Template, system, prompt, response string, generate bool) (string, error) {\n\t// format the template based on the generate flag\n\tformatTemplateForResponse(tmpl, generate)\n\n\ttype promptData struct {\n\t\tSystem   string\n\t\tPrompt   string\n\t\tResponse string\n\t}\n\n\tdata := promptData{\n\t\tSystem:   system,\n\t\tPrompt:   prompt,\n\t\tResponse: response,\n\t}\n\n\tvar buf bytes.Buffer\n\terr := tmpl.Execute(&buf, data)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\treturn buf.String(), nil\n}\n```"]}, "ollama-server/sched.go-unload": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/ollama/server/sched.go:\n```\npackage server\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"reflect\"\n\t\"runtime\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/ollama/ollama/api\"\n\t\"github.com/ollama/ollama/envconfig\"\n\t\"github.com/ollama/ollama/format\"\n\t\"github.com/ollama/ollama/gpu\"\n\t\"github.com/ollama/ollama/llm\"\n)\n\ntype LlmRequest struct {\n\tctx             context.Context //nolint:containedctx\n\tmodel           *Model\n\topts            api.Options\n\torigNumCtx      int // Track the initial ctx request\n\tsessionDuration *api.Duration\n\tsuccessCh       chan *runnerRef\n\terrCh           chan error\n\tschedAttempts   uint\n}\n\ntype Scheduler struct {\n\tpendingReqCh  chan *LlmRequest\n\tfinishedReqCh chan *LlmRequest\n\texpiredCh     chan *runnerRef\n\tunloadedCh    chan interface{}\n\n\tloaded   map[string]*runnerRef\n\tloadedMu sync.Mutex\n\n\tloadFn       func(req *LlmRequest, ggml *llm.GGML, gpus gpu.GpuInfoList, numParallel int)\n\tnewServerFn  func(gpus gpu.GpuInfoList, model string, ggml *llm.GGML, adapters []string, projectors []string, opts api.Options, numParallel int) (llm.LlamaServer, error)\n\tgetGpuFn     func() gpu.GpuInfoList\n\tgetCpuFn     func() gpu.GpuInfoList\n\treschedDelay time.Duration\n}\n\n// Default automatic value for number of models we allow per GPU\n// Model will still need to fit in VRAM, but loading many small models\n// on a large GPU can cause stalling\nvar defaultModelsPerGPU = 3\n\n// Default automatic value for parallel setting\n// Model will still need to fit in VRAM.  If this setting wont fit\n// we'll back off down to 1 to try to get it to fit\nvar defaultParallel = 4\n\nvar ErrMaxQueue = fmt.Errorf(\"server busy, please try again.  maximum pending requests exceeded\")\n\nfunc InitScheduler(ctx context.Context) *Scheduler {\n\tsched := &Scheduler{\n\t\tpendingReqCh:  make(chan *LlmRequest, envconfig.MaxQueuedRequests),\n\t\tfinishedReqCh: make(chan *LlmRequest, envconfig.MaxQueuedRequests),\n\t\texpiredCh:     make(chan *runnerRef, envconfig.MaxQueuedRequests),\n\t\tunloadedCh:    make(chan interface{}, envconfig.MaxQueuedRequests),\n\t\tloaded:        make(map[string]*runnerRef),\n\t\tnewServerFn:   llm.NewLlamaServer,\n\t\tgetGpuFn:      gpu.GetGPUInfo,\n\t\tgetCpuFn:      gpu.GetCPUInfo,\n\t\treschedDelay:  250 * time.Millisecond,\n\t}\n\tsched.loadFn = sched.load\n\treturn sched\n}\n\n// context must be canceled to decrement ref count and release the runner\nfunc (s *Scheduler) GetRunner(c context.Context, model *Model, opts api.Options, sessionDuration *api.Duration) (chan *runnerRef, chan error) {\n\tif opts.NumCtx < 4 {\n\t\topts.NumCtx = 4\n\t}\n\n\treq := &LlmRequest{\n\t\tctx:             c,\n\t\tmodel:           model,\n\t\topts:            opts,\n\t\tsessionDuration: sessionDuration,\n\t\tsuccessCh:       make(chan *runnerRef),\n\t\terrCh:           make(chan error, 1),\n\t}\n\n\tselect {\n\tcase s.pendingReqCh <- req:\n\tdefault:\n\t\treq.errCh <- ErrMaxQueue\n\t}\n\treturn req.successCh, req.errCh\n}\n\n// Returns immediately, spawns go routines for the scheduler which will shutdown when ctx is done\nfunc (s *Scheduler) Run(ctx context.Context) {\n\tslog.Debug(\"starting llm scheduler\")\n\tgo func() {\n\t\ts.processPending(ctx)\n\t}()\n\n\tgo func() {\n\t\ts.processCompleted(ctx)\n\t}()\n}\n\nfunc (s *Scheduler) processPending(ctx context.Context) {\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\tslog.Debug(\"shutting down scheduler pending loop\")\n\t\t\treturn\n\t\tcase pending := <-s.pendingReqCh:\n\t\t\t// Block other requests until we get this pending request running\n\t\t\tpending.schedAttempts++\n\t\t\tif pending.origNumCtx == 0 {\n\t\t\t\tpending.origNumCtx = pending.opts.NumCtx\n\t\t\t}\n\n\t\t\tif pending.ctx.Err() != nil {\n\t\t\t\tslog.Debug(\"pending request cancelled or timed out, skipping scheduling\")\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tnumParallel := envconfig.NumParallel\n\t\t\t// TODO (jmorganca): multimodal models don't support parallel yet\n\t\t\t// see https://github.com/ollama/ollama/issues/4165\n\t\t\tif len(pending.model.ProjectorPaths) > 0 && numParallel != 1 {\n\t\t\t\tnumParallel = 1\n\t\t\t\tslog.Warn(\"multimodal models don't support parallel requests yet\")\n\t\t\t}\n\t\t\t// Keep NumCtx and numParallel in sync\n\t\t\tif numParallel > 1 {\n\t\t\t\tpending.opts.NumCtx = pending.origNumCtx * numParallel\n\t\t\t}\n\n\t\t\tfor {\n\t\t\t\tvar runnerToExpire *runnerRef\n\t\t\t\ts.loadedMu.Lock()\n\t\t\t\trunner := s.loaded[pending.model.ModelPath]\n\t\t\t\tloadedCount := len(s.loaded)\n\t\t\t\ts.loadedMu.Unlock()\n\t\t\t\tif runner != nil {\n\t\t\t\t\tif runner.needsReload(ctx, pending) {\n\t\t\t\t\t\trunnerToExpire = runner\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// Runner is usable, return it\n\t\t\t\t\t\tpending.useLoadedRunner(runner, s.finishedReqCh)\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t} else if envconfig.MaxRunners > 0 && loadedCount >= envconfig.MaxRunners {\n\t\t\t\t\tslog.Debug(\"max runners achieved, unloading one to make room\", \"runner_count\", loadedCount)\n\t\t\t\t\trunnerToExpire = s.findRunnerToUnload()\n\t\t\t\t} else {\n\t\t\t\t\t// Either no models are loaded or below envconfig.MaxRunners\n\t\t\t\t\t// Get a refreshed GPU list\n\t\t\t\t\tvar gpus gpu.GpuInfoList\n\t\t\t\t\tif pending.opts.NumGPU == 0 {\n\t\t\t\t\t\tgpus = s.getCpuFn()\n\t\t\t\t\t} else {\n\t\t\t\t\t\tgpus = s.getGpuFn()\n\t\t\t\t\t}\n\n\t\t\t\t\tif envconfig.MaxRunners <= 0 {\n\t\t\t\t\t\t// No user specified MaxRunners, so figure out what automatic setting to use\n\t\t\t\t\t\t// If all GPUs have reliable free memory reporting, defaultModelsPerGPU * the number of GPUs\n\t\t\t\t\t\t// if any GPU has unreliable free memory reporting, 1x the number of GPUs\n\t\t\t\t\t\tallReliable := true\n\t\t\t\t\t\tfor _, gpu := range gpus {\n\t\t\t\t\t\t\tif gpu.UnreliableFreeMemory {\n\t\t\t\t\t\t\t\tallReliable = false\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif allReliable {\n\t\t\t\t\t\t\tenvconfig.MaxRunners = defaultModelsPerGPU * len(gpus)\n\t\t\t\t\t\t\tslog.Debug(\"updating default concurrency\", \"OLLAMA_MAX_LOADED_MODELS\", envconfig.MaxRunners, \"gpu_count\", len(gpus))\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tslog.Info(\"one or more GPUs detected that are unable to accurately report free memory - disabling default concurrency\")\n\t\t\t\t\t\t\tenvconfig.MaxRunners = len(gpus)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t// Load model for fitting\n\t\t\t\t\tggml, err := llm.LoadModel(pending.model.ModelPath, 0)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\tpending.errCh <- err\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\n\t\t\t\t\t// Evaluate if the model will fit in the available system memory, or if we should unload a model first\n\t\t\t\t\tif len(gpus) == 1 && gpus[0].Library == \"cpu\" {\n\t\t\t\t\t\t// simplifying assumption of defaultParallel when in CPU mode\n\t\t\t\t\t\tif numParallel <= 0 {\n\t\t\t\t\t\t\tnumParallel = defaultParallel\n\t\t\t\t\t\t\tpending.opts.NumCtx = pending.origNumCtx * numParallel\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif loadedCount == 0 {\n\t\t\t\t\t\t\tslog.Debug(\"cpu mode with first model, loading\")\n\t\t\t\t\t\t\ts.loadFn(pending, ggml, gpus, numParallel)\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t\trunnerToExpire = s.maybeFindCPURunnerToUnload(pending, ggml, gpus)\n\t\t\t\t\t\tif runnerToExpire == nil {\n\t\t\t\t\t\t\tslog.Debug(\"cpu mode with available system memory or first model, loading\")\n\t\t\t\t\t\t\ts.loadFn(pending, ggml, gpus, numParallel)\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t\t// else we need to expire a runner\n\t\t\t\t\t} else if loadedCount == 0 {\n\t\t\t\t\t\t// No models loaded. Load the model but prefer the best fit.\n\t\t\t\t\t\tslog.Debug(\"loading first model\", \"model\", pending.model.ModelPath)\n\t\t\t\t\t\tg := pickBestFitGPUs(pending, ggml, gpus, &numParallel)\n\t\t\t\t\t\tif g != nil {\n\t\t\t\t\t\t\tgpus = g\n\t\t\t\t\t\t}\n\t\t\t\t\t\ts.loadFn(pending, ggml, gpus, numParallel)\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\n\t\t\t\t\tif runnerToExpire == nil {\n\t\t\t\t\t\t// More than one loaded model, so we have to see if the\n\t\t\t\t\t\t// new one fits\n\t\t\t\t\t\t//\n\t\t\t\t\t\t// We want to avoid loading on any GPUs that have other\n\t\t\t\t\t\t// models still loading on them to avoid potential races\n\t\t\t\t\t\t// with VRAM consumption ramping up during load\n\t\t\t\t\t\tavailGpus := s.filterGPUsWithoutLoadingModels(gpus)\n\n\t\t\t\t\t\t// Update free memory from currently loaded models\n\t\t\t\t\t\ts.updateFreeSpace(availGpus)\n\t\t\t\t\t\tfitGpus := pickBestFitGPUs(pending, ggml, availGpus, &numParallel)\n\t\t\t\t\t\tif fitGpus != nil {\n\t\t\t\t\t\t\tslog.Debug(\"new model fits with existing models, loading\")\n\t\t\t\t\t\t\ts.loadFn(pending, ggml, fitGpus, numParallel)\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// We couldn't find a set of GPUs to fully load the new\n\t\t\t\t\t\t// model. If no other models are loading (both GPU lists\n\t\t\t\t\t\t// are the same) then we need to unload another model to\n\t\t\t\t\t\t// make room\n\t\t\t\t\t\tif len(availGpus) < len(gpus) {\n\t\t\t\t\t\t\t// There are other requests pending, and this one\n\t\t\t\t\t\t\t// needs more time, so put it on the back of the\n\t\t\t\t\t\t\t// queue so that we might satisfy other pending\n\t\t\t\t\t\t\t// requests that aren't blocked\n\t\t\t\t\t\t\tgo func() {\n\t\t\t\t\t\t\t\t// Process in a go routine to avoid deadlocking\n\t\t\t\t\t\t\t\t// the scheduler if our queue is full\n\t\t\t\t\t\t\t\tslog.Debug(\"delaying scheduling while other models finish loading\", \"attempts\", pending.schedAttempts, \"model\", pending.model.ModelPath)\n\t\t\t\t\t\t\t\ttime.Sleep(s.reschedDelay)\n\t\t\t\t\t\t\t\ts.pendingReqCh <- pending\n\t\t\t\t\t\t\t}()\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t\trunnerToExpire = s.findRunnerToUnload()\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif runnerToExpire == nil {\n\t\t\t\t\t// Shouildn't happen\n\t\t\t\t\tslog.Error(\"runner to expire was nil!\")\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\t// Trigger an expiration to unload once it's done\n\t\t\t\trunnerToExpire.refMu.Lock()\n\t\t\t\tslog.Debug(\"resetting model to expire immediately to make room\", \"modelPath\", runnerToExpire.modelPath, \"refCount\", runnerToExpire.refCount)\n\t\t\t\tif runnerToExpire.expireTimer != nil {\n\t\t\t\t\trunnerToExpire.expireTimer.Stop()\n\t\t\t\t\trunnerToExpire.expireTimer = nil\n\t\t\t\t}\n\t\t\t\trunnerToExpire.sessionDuration = 0\n\t\t\t\tif runnerToExpire.refCount <= 0 {\n\t\t\t\t\ts.expiredCh <- runnerToExpire\n\t\t\t\t}\n\t\t\t\trunnerToExpire.refMu.Unlock()\n\t\t\t\t// Wait for the unload to happen\n\t\t\t\t// Note: at this point we're queueing up all incoming requests, even if they were for\n\t\t\t\t// a different model that's loaded and not scheduled to be removed.\n\t\t\t\tslog.Debug(\"waiting for pending requests to complete and unload to occur\", \"modelPath\", runnerToExpire.modelPath)\n\t\t\t\tselect {\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\tslog.Debug(\"shutting down scheduler pending loop\")\n\t\t\t\t\treturn\n\t\t\t\tcase <-s.unloadedCh:\n\t\t\t\t\tslog.Debug(\"unload completed\", \"modelPath\", runnerToExpire.modelPath)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\tcase <-s.unloadedCh:\n\t\t\t// An unload request when there are no pending request can be ignored\n\t\t\tslog.Debug(\"ignoring unload event with no pending requests\")\n\t\t}\n\t}\n}\n\nfunc (s *Scheduler) processCompleted(ctx context.Context) {\n\t// Process completed requests, expired timers, and unloading models\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\tslog.Debug(\"shutting down scheduler completed loop\")\n\t\t\treturn\n\t\tcase finished := <-s.finishedReqCh:\n\t\t\ts.loadedMu.Lock()\n\t\t\trunner := s.loaded[finished.model.ModelPath]\n\t\t\ts.loadedMu.Unlock()\n\t\t\tif runner == nil {\n\t\t\t\tslog.Error(\"finished request signal received after model unloaded\", \"modelPath\", finished.model.ModelPath)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\trunner.refMu.Lock()\n\t\t\trunner.refCount--\n\t\t\tif runner.refCount <= 0 {\n\t\t\t\tif runner.sessionDuration <= 0 {\n\t\t\t\t\tslog.Debug(\"runner with zero duration has gone idle, expiring to unload\", \"modelPath\", runner.modelPath)\n\t\t\t\t\tif runner.expireTimer != nil {\n\t\t\t\t\t\trunner.expireTimer.Stop()\n\t\t\t\t\t\trunner.expireTimer = nil\n\t\t\t\t\t}\n\t\t\t\t\ts.expiredCh <- runner\n\t\t\t\t} else if runner.expireTimer == nil {\n\t\t\t\t\tslog.Debug(\"runner with non-zero duration has gone idle, adding timer\", \"modelPath\", runner.modelPath, \"duration\", runner.sessionDuration)\n\t\t\t\t\trunner.expireTimer = time.AfterFunc(runner.sessionDuration, func() {\n\t\t\t\t\t\tslog.Debug(\"timer expired, expiring to unload\", \"modelPath\", runner.modelPath)\n\t\t\t\t\t\trunner.refMu.Lock()\n\t\t\t\t\t\tdefer runner.refMu.Unlock()\n\t\t\t\t\t\tif runner.expireTimer != nil {\n\t\t\t\t\t\t\trunner.expireTimer.Stop()\n\t\t\t\t\t\t\trunner.expireTimer = nil\n\t\t\t\t\t\t}\n\t\t\t\t\t\ts.expiredCh <- runner\n\t\t\t\t\t})\n\t\t\t\t\trunner.expiresAt = time.Now().Add(runner.sessionDuration)\n\t\t\t\t} else {\n\t\t\t\t\tslog.Debug(\"runner with non-zero duration has gone idle, resetting timer\", \"modelPath\", runner.modelPath, \"duration\", runner.sessionDuration)\n\t\t\t\t\trunner.expireTimer.Reset(runner.sessionDuration)\n\t\t\t\t\trunner.expiresAt = time.Now().Add(runner.sessionDuration)\n\t\t\t\t}\n\t\t\t}\n\t\t\tslog.Debug(\"after processing request finished event\", \"modelPath\", runner.modelPath, \"refCount\", runner.refCount)\n\t\t\trunner.refMu.Unlock()\n\t\tcase runner := <-s.expiredCh:\n\t\t\tslog.Debug(\"runner expired event received\", \"modelPath\", runner.modelPath)\n\t\t\trunner.refMu.Lock()\n\t\t\tif runner.refCount > 0 {\n\t\t\t\t// Shouldn't happen, but safeguard to ensure no leaked runners\n\t\t\t\tslog.Debug(\"expired event with positive ref count, retrying\", \"modelPath\", runner.modelPath, \"refCount\", runner.refCount)\n\t\t\t\tgo func(runner *runnerRef) {\n\t\t\t\t\t// We can't unload yet, but want to as soon as the current request completes\n\t\t\t\t\t// So queue up another expired event\n\t\t\t\t\ttime.Sleep(10 * time.Millisecond)\n\t\t\t\t\ts.expiredCh <- runner\n\t\t\t\t}(runner)\n\t\t\t\trunner.refMu.Unlock()\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\ts.loadedMu.Lock()\n\t\t\tslog.Debug(\"got lock to unload\", \"modelPath\", runner.modelPath)\n\t\t\tfinished := runner.waitForVRAMRecovery()\n\t\t\trunner.unload()\n\t\t\tdelete(s.loaded, runner.modelPath)\n\t\t\ts.loadedMu.Unlock()\n\t\t\tslog.Debug(\"runner released\", \"modelPath\", runner.modelPath)\n\t\t\trunner.refMu.Unlock()\n\n\t\t\t<-finished\n\t\t\tslog.Debug(\"sending an unloaded event\", \"modelPath\", runner.modelPath)\n\t\t\ts.unloadedCh <- struct{}{}\n\t\t}\n\t}\n}\n\n// Complete the pending request and send the runner back to the requester\n// Wires up a finished event after the request context is completed\n// Updates session duration, and resets expiration timer\nfunc (pending *LlmRequest) useLoadedRunner(runner *runnerRef, finished chan *LlmRequest) {\n\trunner.refMu.Lock()\n\tdefer runner.refMu.Unlock()\n\trunner.refCount++\n\tif runner.expireTimer != nil {\n\t\trunner.expireTimer.Stop()\n\t\trunner.expireTimer = nil\n\t}\n\tif pending.sessionDuration != nil {\n\t\trunner.sessionDuration = pending.sessionDuration.Duration\n\t}\n\tpending.successCh <- runner\n\tgo func() {\n\t\t<-pending.ctx.Done()\n\t\tslog.Debug(\"context for request finished\")\n\t\tfinished <- pending\n\t}()\n}\n\nfunc (s *Scheduler) load(req *LlmRequest, ggml *llm.GGML, gpus gpu.GpuInfoList, numParallel int) {\n\tif numParallel < 1 {\n\t\tnumParallel = 1\n\t}\n\tsessionDuration := envconfig.KeepAlive\n\tif req.sessionDuration != nil {\n\t\tsessionDuration = req.sessionDuration.Duration\n\t}\n\tllama, err := s.newServerFn(gpus, req.model.ModelPath, ggml, req.model.AdapterPaths, req.model.ProjectorPaths, req.opts, numParallel)\n\tif err != nil {\n\t\t// some older models are not compatible with newer versions of llama.cpp\n\t\t// show a generalized compatibility error until there is a better way to\n\t\t// check for model compatibility\n\t\tif errors.Is(llm.ErrUnsupportedFormat, err) || strings.Contains(err.Error(), \"failed to load model\") {\n\t\t\terr = fmt.Errorf(\"%v: this model may be incompatible with your version of Ollama. If you previously pulled this model, try updating it by running `ollama pull %s`\", err, req.model.ShortName)\n\t\t}\n\t\tslog.Info(\"NewLlamaServer failed\", \"model\", req.model.ModelPath, \"error\", err)\n\t\treq.errCh <- err\n\t\treturn\n\t}\n\trunner := &runnerRef{\n\t\tmodel:           req.model,\n\t\tmodelPath:       req.model.ModelPath,\n\t\tllama:           llama,\n\t\tOptions:         &req.opts,\n\t\tsessionDuration: sessionDuration,\n\t\tgpus:            gpus,\n\t\testimatedVRAM:   llama.EstimatedVRAM(),\n\t\testimatedTotal:  llama.EstimatedTotal(),\n\t\tloading:         true,\n\t\trefCount:        1,\n\t}\n\trunner.numParallel = numParallel\n\trunner.refMu.Lock()\n\n\ts.loadedMu.Lock()\n\ts.loaded[req.model.ModelPath] = runner\n\tslog.Info(\"loaded runners\", \"count\", len(s.loaded))\n\ts.loadedMu.Unlock()\n\n\tgo func() {\n\t\tdefer runner.refMu.Unlock()\n\t\tif err = llama.WaitUntilRunning(req.ctx); err != nil {\n\t\t\tslog.Error(\"error loading llama server\", \"error\", err)\n\t\t\trunner.refCount--\n\t\t\treq.errCh <- err\n\t\t\tslog.Debug(\"triggering expiration for failed load\", \"model\", runner.modelPath)\n\t\t\ts.expiredCh <- runner\n\t\t\treturn\n\t\t}\n\t\tslog.Debug(\"finished setting up runner\", \"model\", req.model.ModelPath)\n\t\trunner.loading = false\n\t\tgo func() {\n\t\t\t<-req.ctx.Done()\n\t\t\tslog.Debug(\"context for request finished\")\n\t\t\ts.finishedReqCh <- req\n\t\t}()\n\t\treq.successCh <- runner\n\t}()\n}\n\nfunc (s *Scheduler) updateFreeSpace(allGpus gpu.GpuInfoList) {\n\ttype predKey struct {\n\t\tLibrary string\n\t\tID      string\n\t}\n\tpredMap := map[predKey]uint64{} // Sum up the total predicted usage per GPU for all runners\n\ts.loadedMu.Lock()\n\tfor _, r := range s.loaded {\n\t\tr.refMu.Lock()\n\t\tif r.llama != nil {\n\t\t\tfor _, gpu := range allGpus {\n\t\t\t\tpredMap[predKey{gpu.Library, gpu.ID}] += r.llama.EstimatedVRAMByGPU(gpu.ID)\n\t\t\t}\n\t\t} else {\n\t\t\tslog.Warn(\"unexpected nil runner reference, memory prediction may be incorrect\")\n\t\t}\n\t\tr.refMu.Unlock()\n\t}\n\ts.loadedMu.Unlock()\n\n\t// Now that we've summed up all the GPU usage predictions across all the loaded runners, update the gpu list\n\tfor i := range allGpus {\n\t\tif p, ok := predMap[predKey{allGpus[i].Library, allGpus[i].ID}]; ok {\n\t\t\tslog.Debug(\"gpu reported\", \"gpu\", allGpus[i].ID, \"library\", allGpus[i].Library, \"available\", format.HumanBytes2(allGpus[i].FreeMemory))\n\t\t\tif p > allGpus[i].TotalMemory {\n\t\t\t\t// Shouldn't happen\n\t\t\t\tslog.Warn(\"predicted usage exceeds VRAM\", \"gpu\", allGpus[i].ID, \"totalMemory\", allGpus[i].TotalMemory, \"predicted\", p)\n\t\t\t\tallGpus[i].FreeMemory = 0\n\t\t\t} else if (allGpus[i].TotalMemory - p) < allGpus[i].FreeMemory { // predicted free is smaller than reported free, use it\n\t\t\t\t// TODO maybe we should just always trust our numbers, since cuda's free memory reporting is laggy\n\t\t\t\t// and we might unload models we didn't actually need to.  The risk is if some other GPU intensive app is loaded\n\t\t\t\t// after we start our first runner, then we'll never acount for that, so picking the smallest free value seems prudent.\n\t\t\t\tallGpus[i].FreeMemory = allGpus[i].TotalMemory - p\n\t\t\t}\n\t\t\tslog.Info(\"updated VRAM based on existing loaded models\", \"gpu\", allGpus[i].ID, \"library\", allGpus[i].Library, \"total\", format.HumanBytes2(allGpus[i].TotalMemory), \"available\", format.HumanBytes2(allGpus[i].FreeMemory))\n\t\t}\n\t}\n}\n\n// While models are loading the VRAM consumption numbers will be indeterminate, so we have\n// to avoid scheduling another model on the same GPU(s) that haven't stabilized.\n// This routine returns the set of GPUs that do not have an active loading model.\n// If all GPUs have loading models, an empty list will be returned (not a single CPU entry)\nfunc (s *Scheduler) filterGPUsWithoutLoadingModels(allGpus gpu.GpuInfoList) gpu.GpuInfoList {\n\tret := append(gpu.GpuInfoList{}, allGpus...)\n\ts.loadedMu.Lock()\n\tdefer s.loadedMu.Unlock()\n\tfor _, runner := range s.loaded {\n\t\tif runner.loading {\n\t\t\tslog.Debug(\"overlapping loads detected\", \"gpus\", runner.gpus, \"model\", runner.modelPath)\n\t\t\tfor _, busyGPU := range runner.gpus {\n\t\t\t\tfor i := range ret {\n\t\t\t\t\tif ret[i].ID == busyGPU.ID {\n\t\t\t\t\t\tret = append(ret[:i], ret[i+1:]...)\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn ret\n}\n\n// TODO consolidate sched_types.go\ntype runnerRef struct {\n\trefMu sync.Mutex\n\t// refCond   sync.Cond // Signaled on transition from 1 -> 0\n```\n# Function Signature: func (runner *runnerRef) unload() {\n# Natural Language Annotations: // The refMu must already be held when calling unload\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (runner *runnerRef) unload() {\n\trunner.refMu.Lock()\n\tdefer runner.refMu.Unlock()\n\tif runner.llama != nil {\n\t\trunner.llama.Close()\n\t\trunner.llama = nil\n\t}\n}\n```", "```go\n// The refMu must already be held when calling unload\nfunc (runner *runnerRef) unload() {\n\tif runner.llama != nil {\n\t\trunner.llama.Stop()\n\t\trunner.llama = nil\n\t}\n}\n```", "```go\nfunc (runner *runnerRef) unload() {\n\trunner.refMu.Lock()\n\tdefer runner.refMu.Unlock()\n\tif runner.llama != nil {\n\t\tif err := runner.llama.Unload(); err != nil {\n\t\t\tslog.Error(\"error unloading llama server\", \"error\", err)\n\t\t}\n\t\trunner.llama = nil\n\t}\n}\n```"]}, "lo-retry.go-NewDebounce": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/retry.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n)\n\ntype debounce struct {\n\tafter     time.Duration\n\tmu        *sync.Mutex\n\ttimer     *time.Timer\n\tdone      bool\n\tcallbacks []func()\n}\n\nfunc (d *debounce) reset() {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tif d.done {\n\t\treturn\n\t}\n\n\tif d.timer != nil {\n\t\td.timer.Stop()\n\t}\n\n\td.timer = time.AfterFunc(d.after, func() {\n\t\tfor i := range d.callbacks {\n\t\t\td.callbacks[i]()\n\t\t}\n\t})\n}\n\nfunc (d *debounce) cancel() {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tif d.timer != nil {\n\t\td.timer.Stop()\n\t\td.timer = nil\n\t}\n\n\td.done = true\n}\n\n// NewDebounce creates a debounced instance that delays invoking functions given until after wait milliseconds have elapsed.\n// Play: https://go.dev/play/p/mz32VMK2nqe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype debounceByItem struct {\n\tmu    *sync.Mutex\n\ttimer *time.Timer\n\tcount int\n}\n\ntype debounceBy[T comparable] struct {\n\tafter     time.Duration\n\tmu        *sync.Mutex\n\titems     map[T]*debounceByItem\n\tcallbacks []func(key T, count int)\n}\n\nfunc (d *debounceBy[T]) reset(key T) {\n\td.mu.Lock()\n\tif _, ok := d.items[key]; !ok {\n\t\td.items[key] = &debounceByItem{\n\t\t\tmu:    new(sync.Mutex),\n\t\t\ttimer: nil,\n\t\t}\n\t}\n\n\titem := d.items[key]\n\n\td.mu.Unlock()\n\n\titem.mu.Lock()\n\tdefer item.mu.Unlock()\n\n\titem.count++\n\n\tif item.timer != nil {\n\t\titem.timer.Stop()\n\t}\n\n\titem.timer = time.AfterFunc(d.after, func() {\n\t\titem.mu.Lock()\n\t\tcount := item.count\n\t\titem.count = 0\n\t\titem.mu.Unlock()\n\n\t\tfor i := range d.callbacks {\n\t\t\td.callbacks[i](key, count)\n\t\t}\n\n\t})\n}\n\nfunc (d *debounceBy[T]) cancel(key T) {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tif item, ok := d.items[key]; ok {\n\t\titem.mu.Lock()\n\n\t\tif item.timer != nil {\n\t\t\titem.timer.Stop()\n\t\t\titem.timer = nil\n\t\t}\n\n\t\titem.mu.Unlock()\n\n\t\tdelete(d.items, key)\n\t}\n}\n\n// NewDebounceBy creates a debounced instance for each distinct key, that delays invoking functions given until after wait milliseconds have elapsed.\n// Play: https://go.dev/play/p/d3Vpt6pxhY8\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Attempt invokes a function N times until it returns valid output. Returning either the caught error or nil. When first argument is less than `1`, the function runs until a successful response is returned.\n// Play: https://go.dev/play/p/3ggJZ2ZKcMj\nfunc Attempt(maxIteration int, f func(index int) error) (int, error) {\n\tvar err error\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\t// for retries >= 0 {\n\t\terr = f(i)\n\t\tif err == nil {\n\t\t\treturn i + 1, nil\n\t\t}\n\t}\n\n\treturn maxIteration, err\n}\n\n\n\n// AttemptWithDelay invokes a function N times until it returns valid output,\n// with a pause between each call. Returning either the caught error or nil.\n// When first argument is less than `1`, the function runs until a successful\n// response is returned.\n// Play: https://go.dev/play/p/tVs6CygC7m1\nfunc AttemptWithDelay(maxIteration int, delay time.Duration, f func(index int, duration time.Duration) error) (int, time.Duration, error) {\n\tvar err error\n\n\tstart := time.Now()\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\terr = f(i, time.Since(start))\n\t\tif err == nil {\n\t\t\treturn i + 1, time.Since(start), nil\n\t\t}\n\n\t\tif maxIteration <= 0 || i+1 < maxIteration {\n\t\t\ttime.Sleep(delay)\n\t\t}\n\t}\n\n\treturn maxIteration, time.Since(start), err\n}\n\n// AttemptWhile invokes a function N times until it returns valid output.\n// Returning either the caught error or nil, and along with a bool value to identify\n// whether it needs invoke function continuously. It will terminate the invoke\n// immediately if second bool value is returned with falsy value. When first\n// argument is less than `1`, the function runs until a successful response is\n// returned.\nfunc AttemptWhile(maxIteration int, f func(int) (error, bool)) (int, error) {\n\tvar err error\n\tvar shouldContinueInvoke bool\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\t// for retries >= 0 {\n\t\terr, shouldContinueInvoke = f(i)\n\t\tif !shouldContinueInvoke { // if shouldContinueInvoke is false, then return immediately\n\t\t\treturn i + 1, err\n\t\t}\n\t\tif err == nil {\n\t\t\treturn i + 1, nil\n\t\t}\n\t}\n\n\treturn maxIteration, err\n}\n\n// AttemptWhileWithDelay invokes a function N times until it returns valid output,\n// with a pause between each call. Returning either the caught error or nil, and along\n// with a bool value to identify whether it needs to invoke function continuously.\n// It will terminate the invoke immediately if second bool value is returned with falsy\n// value. When first argument is less than `1`, the function runs until a successful\n// response is returned.\nfunc AttemptWhileWithDelay(maxIteration int, delay time.Duration, f func(int, time.Duration) (error, bool)) (int, time.Duration, error) {\n\tvar err error\n\tvar shouldContinueInvoke bool\n\n\tstart := time.Now()\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\terr, shouldContinueInvoke = f(i, time.Since(start))\n\t\tif !shouldContinueInvoke { // if shouldContinueInvoke is false, then return immediately\n\t\t\treturn i + 1, time.Since(start), err\n\t\t}\n\t\tif err == nil {\n\t\t\treturn i + 1, time.Since(start), nil\n\t\t}\n\n\t\tif maxIteration <= 0 || i+1 < maxIteration {\n\t\t\ttime.Sleep(delay)\n\t\t}\n\t}\n\n\treturn maxIteration, time.Since(start), err\n}\n\ntype transactionStep[T any] struct {\n\texec       func(T) (T, error)\n\tonRollback func(T) T\n}\n\n// NewTransaction instantiate a new transaction.\nfunc NewTransaction[T any]() *Transaction[T] {\n\treturn &Transaction[T]{\n\t\tsteps: []transactionStep[T]{},\n\t}\n}\n\n// Transaction implements a Saga pattern\ntype Transaction[T any] struct {\n\tsteps []transactionStep[T]\n}\n\n// Then adds a step to the chain of callbacks. It returns the same Transaction.\nfunc (t *Transaction[T]) Then(exec func(T) (T, error), onRollback func(T) T) *Transaction[T] {\n\tt.steps = append(t.steps, transactionStep[T]{\n\t\texec:       exec,\n\t\tonRollback: onRollback,\n\t})\n\n\treturn t\n}\n\n// Process runs the Transaction steps and rollbacks in case of errors.\nfunc (t *Transaction[T]) Process(state T) (T, error) {\n\tvar i int\n\tvar err error\n\n\tfor i < len(t.steps) {\n\t\tstate, err = t.steps[i].exec(state)\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n\n\t\ti++\n\t}\n\n\tif err == nil {\n\t\treturn state, nil\n\t}\n\n\tfor i > 0 {\n\t\ti--\n\t\tstate = t.steps[i].onRollback(state)\n\t}\n\n\treturn state, err\n}\n\n// throttle ?\n\n```\n# Function Signature: func NewDebounce(duration time.Duration, f ...func()) (func(), func()) {\n# Natural Language Annotations: // NewDebounce creates a debounced instance that delays invoking functions given until after wait milliseconds have elapsed.\n// Play: https://go.dev/play/p/mz32VMK2nqe\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewDebounce(duration time.Duration, f ...func()) (func(), func()) {\n    d := &debounce{\n        after:     duration,\n        mu:        new(sync.Mutex),\n        timer:     nil,\n        done:      false,\n        callbacks: f,\n    }\n\n    return d.reset, d.cancel\n}\n```\nThis function creates a new debounce instance with a given duration and a list of functions to be invoked after the duration has elapsed. It returns two functions: `reset` and `cancel`. The `reset` function resets the debounce timer and invokes all the functions. The `cancel` function stops the debounce timer and prevents any further invocations.", "```go\nfunc NewDebounce(duration time.Duration, f ...func()) (func(), func()) {\n\td := &debounce{\n\t\tafter:     duration,\n\t\tmu:        new(sync.Mutex),\n\t\ttimer:     nil,\n\t\tdone:      false,\n\t\tcallbacks: f,\n\t}\n\n\treturn d.reset, d.cancel\n}\n```\nThis code creates a new debounce instance with the given duration and functions. It returns two functions: `reset` and `cancel`. `reset` starts the timer for the debounce, and `cancel` stops the timer if it's already running.", "```go\nfunc NewDebounce(duration time.Duration, f ...func()) (func(), func()) {\n\td := &debounce{\n\t\tafter:     duration,\n\t\tmu:        new(sync.Mutex),\n\t\ttimer:     nil,\n\t\tdone:      false,\n\t\tcallbacks: f,\n\t}\n\n\treturn func() {\n\t\t\td.reset()\n\t\t}, func() {\n\t\t\td.cancel()\n\t\t}\n}\n```\nThis function creates a debounce and returns two functions: `reset()` and `cancel()`. `reset()` is used to reset the timer and invoke the callback functions if the time duration has elapsed. `cancel()` is used to stop the timer and stop any further invocation of the callback functions. The debounce instance is created with the given duration and functions."]}, "lo-retry.go-NewDebounceBy": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/retry.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n)\n\ntype debounce struct {\n\tafter     time.Duration\n\tmu        *sync.Mutex\n\ttimer     *time.Timer\n\tdone      bool\n\tcallbacks []func()\n}\n\nfunc (d *debounce) reset() {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tif d.done {\n\t\treturn\n\t}\n\n\tif d.timer != nil {\n\t\td.timer.Stop()\n\t}\n\n\td.timer = time.AfterFunc(d.after, func() {\n\t\tfor i := range d.callbacks {\n\t\t\td.callbacks[i]()\n\t\t}\n\t})\n}\n\nfunc (d *debounce) cancel() {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tif d.timer != nil {\n\t\td.timer.Stop()\n\t\td.timer = nil\n\t}\n\n\td.done = true\n}\n\n// NewDebounce creates a debounced instance that delays invoking functions given until after wait milliseconds have elapsed.\n// Play: https://go.dev/play/p/mz32VMK2nqe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype debounceByItem struct {\n\tmu    *sync.Mutex\n\ttimer *time.Timer\n\tcount int\n}\n\ntype debounceBy[T comparable] struct {\n\tafter     time.Duration\n\tmu        *sync.Mutex\n\titems     map[T]*debounceByItem\n\tcallbacks []func(key T, count int)\n}\n\nfunc (d *debounceBy[T]) reset(key T) {\n\td.mu.Lock()\n\tif _, ok := d.items[key]; !ok {\n\t\td.items[key] = &debounceByItem{\n\t\t\tmu:    new(sync.Mutex),\n\t\t\ttimer: nil,\n\t\t}\n\t}\n\n\titem := d.items[key]\n\n\td.mu.Unlock()\n\n\titem.mu.Lock()\n\tdefer item.mu.Unlock()\n\n\titem.count++\n\n\tif item.timer != nil {\n\t\titem.timer.Stop()\n\t}\n\n\titem.timer = time.AfterFunc(d.after, func() {\n\t\titem.mu.Lock()\n\t\tcount := item.count\n\t\titem.count = 0\n\t\titem.mu.Unlock()\n\n\t\tfor i := range d.callbacks {\n\t\t\td.callbacks[i](key, count)\n\t\t}\n\n\t})\n}\n\nfunc (d *debounceBy[T]) cancel(key T) {\n\td.mu.Lock()\n\tdefer d.mu.Unlock()\n\n\tif item, ok := d.items[key]; ok {\n\t\titem.mu.Lock()\n\n\t\tif item.timer != nil {\n\t\t\titem.timer.Stop()\n\t\t\titem.timer = nil\n\t\t}\n\n\t\titem.mu.Unlock()\n\n\t\tdelete(d.items, key)\n\t}\n}\n\n// NewDebounceBy creates a debounced instance for each distinct key, that delays invoking functions given until after wait milliseconds have elapsed.\n// Play: https://go.dev/play/p/d3Vpt6pxhY8\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Attempt invokes a function N times until it returns valid output. Returning either the caught error or nil. When first argument is less than `1`, the function runs until a successful response is returned.\n// Play: https://go.dev/play/p/3ggJZ2ZKcMj\nfunc Attempt(maxIteration int, f func(index int) error) (int, error) {\n\tvar err error\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\t// for retries >= 0 {\n\t\terr = f(i)\n\t\tif err == nil {\n\t\t\treturn i + 1, nil\n\t\t}\n\t}\n\n\treturn maxIteration, err\n}\n\n\n\n// AttemptWithDelay invokes a function N times until it returns valid output,\n// with a pause between each call. Returning either the caught error or nil.\n// When first argument is less than `1`, the function runs until a successful\n// response is returned.\n// Play: https://go.dev/play/p/tVs6CygC7m1\nfunc AttemptWithDelay(maxIteration int, delay time.Duration, f func(index int, duration time.Duration) error) (int, time.Duration, error) {\n\tvar err error\n\n\tstart := time.Now()\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\terr = f(i, time.Since(start))\n\t\tif err == nil {\n\t\t\treturn i + 1, time.Since(start), nil\n\t\t}\n\n\t\tif maxIteration <= 0 || i+1 < maxIteration {\n\t\t\ttime.Sleep(delay)\n\t\t}\n\t}\n\n\treturn maxIteration, time.Since(start), err\n}\n\n// AttemptWhile invokes a function N times until it returns valid output.\n// Returning either the caught error or nil, and along with a bool value to identify\n// whether it needs invoke function continuously. It will terminate the invoke\n// immediately if second bool value is returned with falsy value. When first\n// argument is less than `1`, the function runs until a successful response is\n// returned.\nfunc AttemptWhile(maxIteration int, f func(int) (error, bool)) (int, error) {\n\tvar err error\n\tvar shouldContinueInvoke bool\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\t// for retries >= 0 {\n\t\terr, shouldContinueInvoke = f(i)\n\t\tif !shouldContinueInvoke { // if shouldContinueInvoke is false, then return immediately\n\t\t\treturn i + 1, err\n\t\t}\n\t\tif err == nil {\n\t\t\treturn i + 1, nil\n\t\t}\n\t}\n\n\treturn maxIteration, err\n}\n\n// AttemptWhileWithDelay invokes a function N times until it returns valid output,\n// with a pause between each call. Returning either the caught error or nil, and along\n// with a bool value to identify whether it needs to invoke function continuously.\n// It will terminate the invoke immediately if second bool value is returned with falsy\n// value. When first argument is less than `1`, the function runs until a successful\n// response is returned.\nfunc AttemptWhileWithDelay(maxIteration int, delay time.Duration, f func(int, time.Duration) (error, bool)) (int, time.Duration, error) {\n\tvar err error\n\tvar shouldContinueInvoke bool\n\n\tstart := time.Now()\n\n\tfor i := 0; maxIteration <= 0 || i < maxIteration; i++ {\n\t\terr, shouldContinueInvoke = f(i, time.Since(start))\n\t\tif !shouldContinueInvoke { // if shouldContinueInvoke is false, then return immediately\n\t\t\treturn i + 1, time.Since(start), err\n\t\t}\n\t\tif err == nil {\n\t\t\treturn i + 1, time.Since(start), nil\n\t\t}\n\n\t\tif maxIteration <= 0 || i+1 < maxIteration {\n\t\t\ttime.Sleep(delay)\n\t\t}\n\t}\n\n\treturn maxIteration, time.Since(start), err\n}\n\ntype transactionStep[T any] struct {\n\texec       func(T) (T, error)\n\tonRollback func(T) T\n}\n\n// NewTransaction instantiate a new transaction.\nfunc NewTransaction[T any]() *Transaction[T] {\n\treturn &Transaction[T]{\n\t\tsteps: []transactionStep[T]{},\n\t}\n}\n\n// Transaction implements a Saga pattern\ntype Transaction[T any] struct {\n\tsteps []transactionStep[T]\n}\n\n// Then adds a step to the chain of callbacks. It returns the same Transaction.\nfunc (t *Transaction[T]) Then(exec func(T) (T, error), onRollback func(T) T) *Transaction[T] {\n\tt.steps = append(t.steps, transactionStep[T]{\n\t\texec:       exec,\n\t\tonRollback: onRollback,\n\t})\n\n\treturn t\n}\n\n// Process runs the Transaction steps and rollbacks in case of errors.\nfunc (t *Transaction[T]) Process(state T) (T, error) {\n\tvar i int\n\tvar err error\n\n\tfor i < len(t.steps) {\n\t\tstate, err = t.steps[i].exec(state)\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n\n\t\ti++\n\t}\n\n\tif err == nil {\n\t\treturn state, nil\n\t}\n\n\tfor i > 0 {\n\t\ti--\n\t\tstate = t.steps[i].onRollback(state)\n\t}\n\n\treturn state, err\n}\n\n// throttle ?\n\n```\n# Function Signature: func NewDebounceBy[T comparable](duration time.Duration, f ...func(key T, count int)) (func(key T), func(key T)) {\n# Natural Language Annotations: // NewDebounceBy creates a debounced instance for each distinct key, that delays invoking functions given until after wait milliseconds have elapsed.\n// Play: https://go.dev/play/p/d3Vpt6pxhY8\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewDebounceBy[T comparable](duration time.Duration, f ...func(key T, count int)) (func(key T), func(key T)) {\n\td := &debounceBy[T]{\n\t\tafter:     duration,\n\t\tmu:        new(sync.Mutex),\n\t\titems:     make(map[T]*debounceByItem),\n\t\tcallbacks: f,\n\t}\n\n\treturn func(key T) {\n\t\t\td.reset(key)\n\t\t}, func(key T) {\n\t\t\td.cancel(key)\n\t\t}\n}\n```", "```go\nfunc NewDebounceBy[T comparable](duration time.Duration, f ...func(key T, count int)) (func(key T), func(key T)) {\n\tdebounced := &debounceBy[T]{\n\t\tafter:     duration,\n\t\tmu:        new(sync.Mutex),\n\t\titems:     make(map[T]*debounceByItem),\n\t\tcallbacks: f,\n\t}\n\n\treturn func(key T) {\n\t\t\tdebounced.reset(key)\n\t\t}, func(key T) {\n\t\t\tdebounced.cancel(key)\n\t\t}\n}\n```", "```go\nfunc NewDebounceBy[T comparable](duration time.Duration, f ...func(key T, count int)) (func(key T), func(key T)) {\n    d := &debounceBy[T]{\n        after:     duration,\n        mu:        new(sync.Mutex),\n        items:     make(map[T]*debounceByItem),\n        callbacks: f,\n    }\n\n    return d.reset, d.cancel\n}\n```"]}, "lo-type_manipulation.go-FromPtr": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/type_manipulation.go:\n```\npackage lo\n\nimport \"reflect\"\n\n// IsNil checks if a value is nil or if it's a reference type with a nil underlying value.\nfunc IsNil(x any) bool {\n\tdefer func() { recover() }() // nolint:errcheck\n\treturn x == nil || reflect.ValueOf(x).IsNil()\n}\n\n// ToPtr returns a pointer copy of value.\nfunc ToPtr[T any](x T) *T {\n\treturn &x\n}\n\n// Nil returns a nil pointer of type.\nfunc Nil[T any]() *T {\n\treturn nil\n}\n\n// EmptyableToPtr returns a pointer copy of value if it's nonzero.\n// Otherwise, returns nil pointer.\nfunc EmptyableToPtr[T any](x T) *T {\n\t// \ud83e\udd2e\n\tisZero := reflect.ValueOf(&x).Elem().IsZero()\n\tif isZero {\n\t\treturn nil\n\t}\n\n\treturn &x\n}\n\n// FromPtr returns the pointer value or empty.\n\n\n\n\n\n\n\n\n// FromPtrOr returns the pointer value or the fallback value.\nfunc FromPtrOr[T any](x *T, fallback T) T {\n\tif x == nil {\n\t\treturn fallback\n\t}\n\n\treturn *x\n}\n\n// ToSlicePtr returns a slice of pointer copy of value.\nfunc ToSlicePtr[T any](collection []T) []*T {\n\tresult := make([]*T, len(collection))\n\n\tfor i := range collection {\n\t\tresult[i] = &collection[i]\n\t}\n\treturn result\n}\n\n// ToAnySlice returns a slice with all elements mapped to `any` type\nfunc ToAnySlice[T any](collection []T) []any {\n\tresult := make([]any, len(collection))\n\tfor i := range collection {\n\t\tresult[i] = collection[i]\n\t}\n\treturn result\n}\n\n// FromAnySlice returns an `any` slice with all elements mapped to a type.\n// Returns false in case of type conversion failure.\nfunc FromAnySlice[T any](in []any) (out []T, ok bool) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tout = []T{}\n\t\t\tok = false\n\t\t}\n\t}()\n\n\tresult := make([]T, len(in))\n\tfor i := range in {\n\t\tresult[i] = in[i].(T)\n\t}\n\treturn result, true\n}\n\n// Empty returns an empty value.\nfunc Empty[T any]() T {\n\tvar zero T\n\treturn zero\n}\n\n// IsEmpty returns true if argument is a zero value.\nfunc IsEmpty[T comparable](v T) bool {\n\tvar zero T\n\treturn zero == v\n}\n\n// IsNotEmpty returns true if argument is not a zero value.\nfunc IsNotEmpty[T comparable](v T) bool {\n\tvar zero T\n\treturn zero != v\n}\n\n// Coalesce returns the first non-empty arguments. Arguments must be comparable.\nfunc Coalesce[T comparable](values ...T) (result T, ok bool) {\n\tfor i := range values {\n\t\tif values[i] != result {\n\t\t\tresult = values[i]\n\t\t\tok = true\n\t\t\treturn\n\t\t}\n\t}\n\n\treturn\n}\n\n// CoalesceOrEmpty returns the first non-empty arguments. Arguments must be comparable.\nfunc CoalesceOrEmpty[T comparable](v ...T) T {\n\tresult, _ := Coalesce(v...)\n\treturn result\n}\n\n```\n# Function Signature: func FromPtr[T any](x *T) T {\n# Natural Language Annotations: // FromPtr returns the pointer value or empty.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// FromPtr returns the pointer value or empty.\nfunc FromPtr[T any](x *T) T {\n\tif x == nil {\n\t\tvar zero T\n\t\treturn zero\n\t}\n\n\treturn *x\n}\n```", "```go\n// FromPtr returns the pointer value or empty.\nfunc FromPtr[T any](x *T) T {\n\tvar zero T\n\tif x == nil {\n\t\treturn zero\n\t}\n\n\treturn *x\n}\n```", "```go\n// FromPtr returns the pointer value or empty.\nfunc FromPtr[T any](x *T) T {\n\tif x == nil {\n\t\tvar zero T\n\t\treturn zero\n\t}\n\treturn *x\n}\n```\n\nThis function takes a pointer of type `T` as an argument and returns the value that the pointer is pointing to. If the pointer is `nil`, it returns an empty value of type `T`."]}, "lo-errors.go-Try6": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/errors.go:\n```\npackage lo\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n)\n\n// Validate is a helper that creates an error when a condition is not met.\n// Play: https://go.dev/play/p/vPyh51XpCBt\nfunc Validate(ok bool, format string, args ...any) error {\n\tif !ok {\n\t\treturn fmt.Errorf(fmt.Sprintf(format, args...))\n\t}\n\treturn nil\n}\n\nfunc messageFromMsgAndArgs(msgAndArgs ...any) string {\n\tif len(msgAndArgs) == 1 {\n\t\tif msgAsStr, ok := msgAndArgs[0].(string); ok {\n\t\t\treturn msgAsStr\n\t\t}\n\t\treturn fmt.Sprintf(\"%+v\", msgAndArgs[0])\n\t}\n\tif len(msgAndArgs) > 1 {\n\t\treturn fmt.Sprintf(msgAndArgs[0].(string), msgAndArgs[1:]...)\n\t}\n\treturn \"\"\n}\n\n// must panics if err is error or false.\nfunc must(err any, messageArgs ...any) {\n\tif err == nil {\n\t\treturn\n\t}\n\n\tswitch e := err.(type) {\n\tcase bool:\n\t\tif !e {\n\t\t\tmessage := messageFromMsgAndArgs(messageArgs...)\n\t\t\tif message == \"\" {\n\t\t\t\tmessage = \"not ok\"\n\t\t\t}\n\n\t\t\tpanic(message)\n\t\t}\n\n\tcase error:\n\t\tmessage := messageFromMsgAndArgs(messageArgs...)\n\t\tif message != \"\" {\n\t\t\tpanic(message + \": \" + e.Error())\n\t\t} else {\n\t\t\tpanic(e.Error())\n\t\t}\n\n\tdefault:\n\t\tpanic(\"must: invalid err type '\" + reflect.TypeOf(err).Name() + \"', should either be a bool or an error\")\n\t}\n}\n\n// Must is a helper that wraps a call to a function returning a value and an error\n// and panics if err is error or false.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must[T any](val T, err any, messageArgs ...any) T {\n\tmust(err, messageArgs...)\n\treturn val\n}\n\n// Must0 has the same behavior as Must, but callback returns no variable.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must0(err any, messageArgs ...any) {\n\tmust(err, messageArgs...)\n}\n\n// Must1 is an alias to Must\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must1[T any](val T, err any, messageArgs ...any) T {\n\treturn Must(val, err, messageArgs...)\n}\n\n// Must2 has the same behavior as Must, but callback returns 2 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must2[T1, T2 any](val1 T1, val2 T2, err any, messageArgs ...any) (T1, T2) {\n\tmust(err, messageArgs...)\n\treturn val1, val2\n}\n\n// Must3 has the same behavior as Must, but callback returns 3 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must3[T1, T2, T3 any](val1 T1, val2 T2, val3 T3, err any, messageArgs ...any) (T1, T2, T3) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3\n}\n\n// Must4 has the same behavior as Must, but callback returns 4 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must4[T1, T2, T3, T4 any](val1 T1, val2 T2, val3 T3, val4 T4, err any, messageArgs ...any) (T1, T2, T3, T4) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3, val4\n}\n\n// Must5 has the same behavior as Must, but callback returns 5 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must5[T1, T2, T3, T4, T5 any](val1 T1, val2 T2, val3 T3, val4 T4, val5 T5, err any, messageArgs ...any) (T1, T2, T3, T4, T5) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3, val4, val5\n}\n\n// Must6 has the same behavior as Must, but callback returns 6 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must6[T1, T2, T3, T4, T5, T6 any](val1 T1, val2 T2, val3 T3, val4 T4, val5 T5, val6 T6, err any, messageArgs ...any) (T1, T2, T3, T4, T5, T6) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3, val4, val5, val6\n}\n\n// Try calls the function and return false in case of error.\nfunc Try(callback func() error) (ok bool) {\n\tok = true\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tok = false\n\t\t}\n\t}()\n\n\terr := callback()\n\tif err != nil {\n\t\tok = false\n\t}\n\n\treturn\n}\n\n// Try0 has the same behavior as Try, but callback returns no variable.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try0(callback func()) bool {\n\treturn Try(func() error {\n\t\tcallback()\n\t\treturn nil\n\t})\n}\n\n// Try1 is an alias to Try.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try1(callback func() error) bool {\n\treturn Try(callback)\n}\n\n// Try2 has the same behavior as Try, but callback returns 2 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try2[T any](callback func() (T, error)) bool {\n\treturn Try(func() error {\n\t\t_, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try3 has the same behavior as Try, but callback returns 3 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try3[T, R any](callback func() (T, R, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try4 has the same behavior as Try, but callback returns 4 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try4[T, R, S any](callback func() (T, R, S, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, _, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try5 has the same behavior as Try, but callback returns 5 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try5[T, R, S, Q any](callback func() (T, R, S, Q, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, _, _, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try6 has the same behavior as Try, but callback returns 6 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\n\n\n\n\n\n\n\n// TryOr has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr[A any](callback func() (A, error), fallbackA A) (A, bool) {\n\treturn TryOr1(callback, fallbackA)\n}\n\n// TryOr1 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr1[A any](callback func() (A, error), fallbackA A) (A, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, ok\n}\n\n// TryOr2 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr2[A, B any](callback func() (A, B, error), fallbackA A, fallbackB B) (A, B, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, ok\n}\n\n// TryOr3 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr3[A, B, C any](callback func() (A, B, C, error), fallbackA A, fallbackB B, fallbackC C) (A, B, C, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, ok\n}\n\n// TryOr4 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr4[A, B, C, D any](callback func() (A, B, C, D, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D) (A, B, C, D, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, d, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tfallbackD = d\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, fallbackD, ok\n}\n\n// TryOr5 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr5[A, B, C, D, E any](callback func() (A, B, C, D, E, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D, fallbackE E) (A, B, C, D, E, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, d, e, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tfallbackD = d\n\t\t\tfallbackE = e\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, fallbackD, fallbackE, ok\n}\n\n// TryOr6 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// TryWithErrorValue has the same behavior as Try, but also returns value passed to panic.\n// Play: https://go.dev/play/p/Kc7afQIT2Fs\nfunc TryWithErrorValue(callback func() error) (errorValue any, ok bool) {\n\tok = true\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tok = false\n\t\t\terrorValue = r\n\t\t}\n\t}()\n\n\terr := callback()\n\tif err != nil {\n\t\tok = false\n\t\terrorValue = err\n\t}\n\n\treturn\n}\n\n// TryCatch has the same behavior as Try, but calls the catch function in case of error.\n// Play: https://go.dev/play/p/PnOON-EqBiU\nfunc TryCatch(callback func() error, catch func()) {\n\tif !Try(callback) {\n\t\tcatch()\n\t}\n}\n\n// TryCatchWithErrorValue has the same behavior as TryWithErrorValue, but calls the catch function in case of error.\n// Play: https://go.dev/play/p/8Pc9gwX_GZO\nfunc TryCatchWithErrorValue(callback func() error, catch func(any)) {\n\tif err, ok := TryWithErrorValue(callback); !ok {\n\t\tcatch(err)\n\t}\n}\n\n// ErrorsAs is a shortcut for errors.As(err, &&T).\n// Play: https://go.dev/play/p/8wk5rH8UfrE\nfunc ErrorsAs[T error](err error) (T, bool) {\n\tvar t T\n\tok := errors.As(err, &t)\n\treturn t, ok\n}\n\n```\n# Function Signature: func Try6[T, R, S, Q, U any](callback func() (T, R, S, Q, U, error)) bool {\n# Natural Language Annotations: // Try6 has the same behavior as Try, but callback returns 6 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// Try6 has the same behavior as Try, but callback returns 6 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try6[T, R, S, Q, U any](callback func() (T, R, S, Q, U, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, _, _, _, err := callback()\n\t\treturn err\n\t})\n}\n```", "```go\n// Try6 has the same behavior as Try, but callback returns 6 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try6[T, R, S, Q, U any](callback func() (T, R, S, Q, U, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, _, _, _, err := callback()\n\t\treturn err\n\t})\n}\n```", "```go\nfunc Try6[T1, T2, T3, T4, T5, T6 any](callback func() (T1, T2, T3, T4, T5, T6, error)) bool {\n\treturn Try(callback)\n}\n```"]}, "lo-errors.go-TryOr6": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/errors.go:\n```\npackage lo\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"reflect\"\n)\n\n// Validate is a helper that creates an error when a condition is not met.\n// Play: https://go.dev/play/p/vPyh51XpCBt\nfunc Validate(ok bool, format string, args ...any) error {\n\tif !ok {\n\t\treturn fmt.Errorf(fmt.Sprintf(format, args...))\n\t}\n\treturn nil\n}\n\nfunc messageFromMsgAndArgs(msgAndArgs ...any) string {\n\tif len(msgAndArgs) == 1 {\n\t\tif msgAsStr, ok := msgAndArgs[0].(string); ok {\n\t\t\treturn msgAsStr\n\t\t}\n\t\treturn fmt.Sprintf(\"%+v\", msgAndArgs[0])\n\t}\n\tif len(msgAndArgs) > 1 {\n\t\treturn fmt.Sprintf(msgAndArgs[0].(string), msgAndArgs[1:]...)\n\t}\n\treturn \"\"\n}\n\n// must panics if err is error or false.\nfunc must(err any, messageArgs ...any) {\n\tif err == nil {\n\t\treturn\n\t}\n\n\tswitch e := err.(type) {\n\tcase bool:\n\t\tif !e {\n\t\t\tmessage := messageFromMsgAndArgs(messageArgs...)\n\t\t\tif message == \"\" {\n\t\t\t\tmessage = \"not ok\"\n\t\t\t}\n\n\t\t\tpanic(message)\n\t\t}\n\n\tcase error:\n\t\tmessage := messageFromMsgAndArgs(messageArgs...)\n\t\tif message != \"\" {\n\t\t\tpanic(message + \": \" + e.Error())\n\t\t} else {\n\t\t\tpanic(e.Error())\n\t\t}\n\n\tdefault:\n\t\tpanic(\"must: invalid err type '\" + reflect.TypeOf(err).Name() + \"', should either be a bool or an error\")\n\t}\n}\n\n// Must is a helper that wraps a call to a function returning a value and an error\n// and panics if err is error or false.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must[T any](val T, err any, messageArgs ...any) T {\n\tmust(err, messageArgs...)\n\treturn val\n}\n\n// Must0 has the same behavior as Must, but callback returns no variable.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must0(err any, messageArgs ...any) {\n\tmust(err, messageArgs...)\n}\n\n// Must1 is an alias to Must\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must1[T any](val T, err any, messageArgs ...any) T {\n\treturn Must(val, err, messageArgs...)\n}\n\n// Must2 has the same behavior as Must, but callback returns 2 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must2[T1, T2 any](val1 T1, val2 T2, err any, messageArgs ...any) (T1, T2) {\n\tmust(err, messageArgs...)\n\treturn val1, val2\n}\n\n// Must3 has the same behavior as Must, but callback returns 3 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must3[T1, T2, T3 any](val1 T1, val2 T2, val3 T3, err any, messageArgs ...any) (T1, T2, T3) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3\n}\n\n// Must4 has the same behavior as Must, but callback returns 4 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must4[T1, T2, T3, T4 any](val1 T1, val2 T2, val3 T3, val4 T4, err any, messageArgs ...any) (T1, T2, T3, T4) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3, val4\n}\n\n// Must5 has the same behavior as Must, but callback returns 5 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must5[T1, T2, T3, T4, T5 any](val1 T1, val2 T2, val3 T3, val4 T4, val5 T5, err any, messageArgs ...any) (T1, T2, T3, T4, T5) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3, val4, val5\n}\n\n// Must6 has the same behavior as Must, but callback returns 6 variables.\n// Play: https://go.dev/play/p/TMoWrRp3DyC\nfunc Must6[T1, T2, T3, T4, T5, T6 any](val1 T1, val2 T2, val3 T3, val4 T4, val5 T5, val6 T6, err any, messageArgs ...any) (T1, T2, T3, T4, T5, T6) {\n\tmust(err, messageArgs...)\n\treturn val1, val2, val3, val4, val5, val6\n}\n\n// Try calls the function and return false in case of error.\nfunc Try(callback func() error) (ok bool) {\n\tok = true\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tok = false\n\t\t}\n\t}()\n\n\terr := callback()\n\tif err != nil {\n\t\tok = false\n\t}\n\n\treturn\n}\n\n// Try0 has the same behavior as Try, but callback returns no variable.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try0(callback func()) bool {\n\treturn Try(func() error {\n\t\tcallback()\n\t\treturn nil\n\t})\n}\n\n// Try1 is an alias to Try.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try1(callback func() error) bool {\n\treturn Try(callback)\n}\n\n// Try2 has the same behavior as Try, but callback returns 2 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try2[T any](callback func() (T, error)) bool {\n\treturn Try(func() error {\n\t\t_, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try3 has the same behavior as Try, but callback returns 3 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try3[T, R any](callback func() (T, R, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try4 has the same behavior as Try, but callback returns 4 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try4[T, R, S any](callback func() (T, R, S, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, _, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try5 has the same behavior as Try, but callback returns 5 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\nfunc Try5[T, R, S, Q any](callback func() (T, R, S, Q, error)) bool {\n\treturn Try(func() error {\n\t\t_, _, _, _, err := callback()\n\t\treturn err\n\t})\n}\n\n// Try6 has the same behavior as Try, but callback returns 6 variables.\n// Play: https://go.dev/play/p/mTyyWUvn9u4\n\n\n\n\n\n\n\n// TryOr has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr[A any](callback func() (A, error), fallbackA A) (A, bool) {\n\treturn TryOr1(callback, fallbackA)\n}\n\n// TryOr1 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr1[A any](callback func() (A, error), fallbackA A) (A, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, ok\n}\n\n// TryOr2 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr2[A, B any](callback func() (A, B, error), fallbackA A, fallbackB B) (A, B, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, ok\n}\n\n// TryOr3 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr3[A, B, C any](callback func() (A, B, C, error), fallbackA A, fallbackB B, fallbackC C) (A, B, C, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, ok\n}\n\n// TryOr4 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr4[A, B, C, D any](callback func() (A, B, C, D, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D) (A, B, C, D, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, d, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tfallbackD = d\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, fallbackD, ok\n}\n\n// TryOr5 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nfunc TryOr5[A, B, C, D, E any](callback func() (A, B, C, D, E, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D, fallbackE E) (A, B, C, D, E, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, d, e, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tfallbackD = d\n\t\t\tfallbackE = e\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, fallbackD, fallbackE, ok\n}\n\n// TryOr6 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// TryWithErrorValue has the same behavior as Try, but also returns value passed to panic.\n// Play: https://go.dev/play/p/Kc7afQIT2Fs\nfunc TryWithErrorValue(callback func() error) (errorValue any, ok bool) {\n\tok = true\n\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\tok = false\n\t\t\terrorValue = r\n\t\t}\n\t}()\n\n\terr := callback()\n\tif err != nil {\n\t\tok = false\n\t\terrorValue = err\n\t}\n\n\treturn\n}\n\n// TryCatch has the same behavior as Try, but calls the catch function in case of error.\n// Play: https://go.dev/play/p/PnOON-EqBiU\nfunc TryCatch(callback func() error, catch func()) {\n\tif !Try(callback) {\n\t\tcatch()\n\t}\n}\n\n// TryCatchWithErrorValue has the same behavior as TryWithErrorValue, but calls the catch function in case of error.\n// Play: https://go.dev/play/p/8Pc9gwX_GZO\nfunc TryCatchWithErrorValue(callback func() error, catch func(any)) {\n\tif err, ok := TryWithErrorValue(callback); !ok {\n\t\tcatch(err)\n\t}\n}\n\n// ErrorsAs is a shortcut for errors.As(err, &&T).\n// Play: https://go.dev/play/p/8wk5rH8UfrE\nfunc ErrorsAs[T error](err error) (T, bool) {\n\tvar t T\n\tok := errors.As(err, &t)\n\treturn t, ok\n}\n\n```\n# Function Signature: func TryOr6[A, B, C, D, E, F any](callback func() (A, B, C, D, E, F, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D, fallbackE E, fallbackF F) (A, B, C, D, E, F, bool) {\n# Natural Language Annotations: // TryOr6 has the same behavior as Must, but returns a default value in case of error.\n// Play: https://go.dev/play/p/B4F7Wg2Zh9X\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc TryOr6[A, B, C, D, E, F any](callback func() (A, B, C, D, E, F, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D, fallbackE E, fallbackF F) (A, B, C, D, E, F, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, d, e, f, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tfallbackD = d\n\t\t\tfallbackE = e\n\t\t\tfallbackF = f\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, fallbackD, fallbackE, fallbackF, ok\n}\n```", "```go\nfunc TryOr6[A, B, C, D, E, F any](callback func() (A, B, C, D, E, F, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D, fallbackE E, fallbackF F) (A, B, C, D, E, F, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, d, e, f, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tfallbackD = d\n\t\t\tfallbackE = e\n\t\t\tfallbackF = f\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, fallbackD, fallbackE, fallbackF, ok\n}\n```", "```go\nfunc TryOr6[A, B, C, D, E, F any](callback func() (A, B, C, D, E, F, error), fallbackA A, fallbackB B, fallbackC C, fallbackD D, fallbackE E, fallbackF F) (A, B, C, D, E, F, bool) {\n\tok := false\n\n\tTry0(func() {\n\t\ta, b, c, d, e, f, err := callback()\n\t\tif err == nil {\n\t\t\tfallbackA = a\n\t\t\tfallbackB = b\n\t\t\tfallbackC = c\n\t\t\tfallbackD = d\n\t\t\tfallbackE = e\n\t\t\tfallbackF = f\n\t\t\tok = true\n\t\t}\n\t})\n\n\treturn fallbackA, fallbackB, fallbackC, fallbackD, fallbackE, fallbackF, ok\n}\n```"]}, "lo-string.go-Substring": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/string.go:\n```\npackage lo\n\nimport (\n\t\"regexp\"\n\t\"strings\"\n\t\"unicode\"\n\t\"unicode/utf8\"\n\n\t\"github.com/samber/lo/internal/rand\"\n\n\t\"golang.org/x/text/cases\"\n\t\"golang.org/x/text/language\"\n)\n\nvar (\n\tLowerCaseLettersCharset = []rune(\"abcdefghijklmnopqrstuvwxyz\")\n\tUpperCaseLettersCharset = []rune(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n\tLettersCharset          = append(LowerCaseLettersCharset, UpperCaseLettersCharset...)\n\tNumbersCharset          = []rune(\"0123456789\")\n\tAlphanumericCharset     = append(LettersCharset, NumbersCharset...)\n\tSpecialCharset          = []rune(\"!@#$%^&*()_+-=[]{}|;':\\\",./<>?\")\n\tAllCharset              = append(AlphanumericCharset, SpecialCharset...)\n\n\t// bearer:disable go_lang_permissive_regex_validation\n\tsplitWordReg = regexp.MustCompile(`([a-z])([A-Z0-9])|([a-zA-Z])([0-9])|([0-9])([a-zA-Z])|([A-Z])([A-Z])([a-z])`)\n\t// bearer:disable go_lang_permissive_regex_validation\n\tsplitNumberLetterReg = regexp.MustCompile(`([0-9])([a-zA-Z])`)\n)\n\n// RandomString return a random string.\n// Play: https://go.dev/play/p/rRseOQVVum4\nfunc RandomString(size int, charset []rune) string {\n\tif size <= 0 {\n\t\tpanic(\"lo.RandomString: Size parameter must be greater than 0\")\n\t}\n\tif len(charset) <= 0 {\n\t\tpanic(\"lo.RandomString: Charset parameter must not be empty\")\n\t}\n\n\tb := make([]rune, size)\n\tpossibleCharactersCount := len(charset)\n\tfor i := range b {\n\t\tb[i] = charset[rand.IntN(possibleCharactersCount)]\n\t}\n\treturn string(b)\n}\n\n// Substring return part of a string.\n// Play: https://go.dev/play/p/TQlxQi82Lu1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ChunkString returns an array of strings split into groups the length of size. If array can't be split evenly,\n// the final chunk will be the remaining elements.\n// Play: https://go.dev/play/p/__FLTuJVz54\nfunc ChunkString[T ~string](str T, size int) []T {\n\tif size <= 0 {\n\t\tpanic(\"lo.ChunkString: Size parameter must be greater than 0\")\n\t}\n\n\tif len(str) == 0 {\n\t\treturn []T{\"\"}\n\t}\n\n\tif size >= len(str) {\n\t\treturn []T{str}\n\t}\n\n\tvar chunks []T = make([]T, 0, ((len(str)-1)/size)+1)\n\tcurrentLen := 0\n\tcurrentStart := 0\n\tfor i := range str {\n\t\tif currentLen == size {\n\t\t\tchunks = append(chunks, str[currentStart:i])\n\t\t\tcurrentLen = 0\n\t\t\tcurrentStart = i\n\t\t}\n\t\tcurrentLen++\n\t}\n\tchunks = append(chunks, str[currentStart:])\n\treturn chunks\n}\n\n// RuneLength is an alias to utf8.RuneCountInString which returns the number of runes in string.\n// Play: https://go.dev/play/p/tuhgW_lWY8l\nfunc RuneLength(str string) int {\n\treturn utf8.RuneCountInString(str)\n}\n\n// PascalCase converts string to pascal case.\n\n\n\n\n\n\n\n\n// CamelCase converts string to camel case.\nfunc CamelCase(str string) string {\n\titems := Words(str)\n\tfor i, item := range items {\n\t\titem = strings.ToLower(item)\n\t\tif i > 0 {\n\t\t\titem = Capitalize(item)\n\t\t}\n\t\titems[i] = item\n\t}\n\treturn strings.Join(items, \"\")\n}\n\n// KebabCase converts string to kebab case.\nfunc KebabCase(str string) string {\n\titems := Words(str)\n\tfor i := range items {\n\t\titems[i] = strings.ToLower(items[i])\n\t}\n\treturn strings.Join(items, \"-\")\n}\n\n// SnakeCase converts string to snake case.\nfunc SnakeCase(str string) string {\n\titems := Words(str)\n\tfor i := range items {\n\t\titems[i] = strings.ToLower(items[i])\n\t}\n\treturn strings.Join(items, \"_\")\n}\n\n// Words splits string into an array of its words.\nfunc Words(str string) []string {\n\tstr = splitWordReg.ReplaceAllString(str, `$1$3$5$7 $2$4$6$8$9`)\n\t// example: Int8Value => Int 8Value => Int 8 Value\n\tstr = splitNumberLetterReg.ReplaceAllString(str, \"$1 $2\")\n\tvar result strings.Builder\n\tfor _, r := range str {\n\t\tif unicode.IsLetter(r) || unicode.IsDigit(r) {\n\t\t\tresult.WriteRune(r)\n\t\t} else {\n\t\t\tresult.WriteRune(' ')\n\t\t}\n\t}\n\treturn strings.Fields(result.String())\n}\n\n// Capitalize converts the first character of string to upper case and the remaining to lower case.\nfunc Capitalize(str string) string {\n\treturn cases.Title(language.English).String(str)\n}\n\n// Elipse truncates a string to a specified length and appends an ellipsis if truncated.\nfunc Elipse(str string, length int) string {\n\tif len(str) > length {\n\t\tif len(str) < 3 || length < 3 {\n\t\t\treturn \"...\"\n\t\t}\n\t\treturn str[0:length-3] + \"...\"\n\t}\n\n\treturn str\n}\n\n```\n# Function Signature: func Substring[T ~string](str T, offset int, length uint) T {\n# Natural Language Annotations: // Substring return part of a string.\n// Play: https://go.dev/play/p/TQlxQi82Lu1\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc Substring[T ~string](str T, offset int, length uint) T {\n\tif offset < 0 {\n\t\tpanic(\"lo.Substring: Offset parameter must be greater than or equal to 0\")\n\t}\n\n\truneStr := []rune(str)\n\tif offset > len(runeStr) {\n\t\tpanic(\"lo.Substring: Offset parameter must be less than or equal to the length of the string\")\n\t}\n\n\tend := min(int(length), len(runeStr))\n\n\treturn T(runeStr[offset:end])\n}\n\nfunc min(a, b int) int {\n\tif a < b {\n\t\treturn a\n\t}\n\treturn b\n}\n```", "```go\nfunc Substring[T ~string](str T, offset int, length uint) T {\n\tif offset < 0 {\n\t\tpanic(\"lo.Substring: Offset parameter must be greater than or equal to 0\")\n\t}\n\tif uint(len(str)) <= length {\n\t\treturn str\n\t}\n\tif uint(len(str)) < uint(offset)+length {\n\t\tpanic(\"lo.Substring: Offset plus length parameter must be less than or equal to the length of the string\")\n\t}\n\treturn str[offset : offset+int(length)]\n}\n```", "```go\n// Substring return part of a string.\n// Play: https://go.dev/play/p/TQlxQi82Lu1\nfunc Substring[T ~string](str T, offset int, length uint) T {\n\tif offset < 0 {\n\t\tpanic(\"lo.Substring: Offset parameter must be greater than or equal to 0\")\n\t}\n\n\tif uint(len(str)) <= length {\n\t\treturn str\n\t}\n\n\tif uint(len(str)) < uint(offset)+length {\n\t\tpanic(\"lo.Substring: Offset plus length parameter must be less than or equal to the length of the string\")\n\t}\n\n\treturn str[offset : offset+int(length)]\n}\n```"]}, "lo-string.go-PascalCase": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/string.go:\n```\npackage lo\n\nimport (\n\t\"regexp\"\n\t\"strings\"\n\t\"unicode\"\n\t\"unicode/utf8\"\n\n\t\"github.com/samber/lo/internal/rand\"\n\n\t\"golang.org/x/text/cases\"\n\t\"golang.org/x/text/language\"\n)\n\nvar (\n\tLowerCaseLettersCharset = []rune(\"abcdefghijklmnopqrstuvwxyz\")\n\tUpperCaseLettersCharset = []rune(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n\tLettersCharset          = append(LowerCaseLettersCharset, UpperCaseLettersCharset...)\n\tNumbersCharset          = []rune(\"0123456789\")\n\tAlphanumericCharset     = append(LettersCharset, NumbersCharset...)\n\tSpecialCharset          = []rune(\"!@#$%^&*()_+-=[]{}|;':\\\",./<>?\")\n\tAllCharset              = append(AlphanumericCharset, SpecialCharset...)\n\n\t// bearer:disable go_lang_permissive_regex_validation\n\tsplitWordReg = regexp.MustCompile(`([a-z])([A-Z0-9])|([a-zA-Z])([0-9])|([0-9])([a-zA-Z])|([A-Z])([A-Z])([a-z])`)\n\t// bearer:disable go_lang_permissive_regex_validation\n\tsplitNumberLetterReg = regexp.MustCompile(`([0-9])([a-zA-Z])`)\n)\n\n// RandomString return a random string.\n// Play: https://go.dev/play/p/rRseOQVVum4\nfunc RandomString(size int, charset []rune) string {\n\tif size <= 0 {\n\t\tpanic(\"lo.RandomString: Size parameter must be greater than 0\")\n\t}\n\tif len(charset) <= 0 {\n\t\tpanic(\"lo.RandomString: Charset parameter must not be empty\")\n\t}\n\n\tb := make([]rune, size)\n\tpossibleCharactersCount := len(charset)\n\tfor i := range b {\n\t\tb[i] = charset[rand.IntN(possibleCharactersCount)]\n\t}\n\treturn string(b)\n}\n\n// Substring return part of a string.\n// Play: https://go.dev/play/p/TQlxQi82Lu1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ChunkString returns an array of strings split into groups the length of size. If array can't be split evenly,\n// the final chunk will be the remaining elements.\n// Play: https://go.dev/play/p/__FLTuJVz54\nfunc ChunkString[T ~string](str T, size int) []T {\n\tif size <= 0 {\n\t\tpanic(\"lo.ChunkString: Size parameter must be greater than 0\")\n\t}\n\n\tif len(str) == 0 {\n\t\treturn []T{\"\"}\n\t}\n\n\tif size >= len(str) {\n\t\treturn []T{str}\n\t}\n\n\tvar chunks []T = make([]T, 0, ((len(str)-1)/size)+1)\n\tcurrentLen := 0\n\tcurrentStart := 0\n\tfor i := range str {\n\t\tif currentLen == size {\n\t\t\tchunks = append(chunks, str[currentStart:i])\n\t\t\tcurrentLen = 0\n\t\t\tcurrentStart = i\n\t\t}\n\t\tcurrentLen++\n\t}\n\tchunks = append(chunks, str[currentStart:])\n\treturn chunks\n}\n\n// RuneLength is an alias to utf8.RuneCountInString which returns the number of runes in string.\n// Play: https://go.dev/play/p/tuhgW_lWY8l\nfunc RuneLength(str string) int {\n\treturn utf8.RuneCountInString(str)\n}\n\n// PascalCase converts string to pascal case.\n\n\n\n\n\n\n\n\n// CamelCase converts string to camel case.\nfunc CamelCase(str string) string {\n\titems := Words(str)\n\tfor i, item := range items {\n\t\titem = strings.ToLower(item)\n\t\tif i > 0 {\n\t\t\titem = Capitalize(item)\n\t\t}\n\t\titems[i] = item\n\t}\n\treturn strings.Join(items, \"\")\n}\n\n// KebabCase converts string to kebab case.\nfunc KebabCase(str string) string {\n\titems := Words(str)\n\tfor i := range items {\n\t\titems[i] = strings.ToLower(items[i])\n\t}\n\treturn strings.Join(items, \"-\")\n}\n\n// SnakeCase converts string to snake case.\nfunc SnakeCase(str string) string {\n\titems := Words(str)\n\tfor i := range items {\n\t\titems[i] = strings.ToLower(items[i])\n\t}\n\treturn strings.Join(items, \"_\")\n}\n\n// Words splits string into an array of its words.\nfunc Words(str string) []string {\n\tstr = splitWordReg.ReplaceAllString(str, `$1$3$5$7 $2$4$6$8$9`)\n\t// example: Int8Value => Int 8Value => Int 8 Value\n\tstr = splitNumberLetterReg.ReplaceAllString(str, \"$1 $2\")\n\tvar result strings.Builder\n\tfor _, r := range str {\n\t\tif unicode.IsLetter(r) || unicode.IsDigit(r) {\n\t\t\tresult.WriteRune(r)\n\t\t} else {\n\t\t\tresult.WriteRune(' ')\n\t\t}\n\t}\n\treturn strings.Fields(result.String())\n}\n\n// Capitalize converts the first character of string to upper case and the remaining to lower case.\nfunc Capitalize(str string) string {\n\treturn cases.Title(language.English).String(str)\n}\n\n// Elipse truncates a string to a specified length and appends an ellipsis if truncated.\nfunc Elipse(str string, length int) string {\n\tif len(str) > length {\n\t\tif len(str) < 3 || length < 3 {\n\t\t\treturn \"...\"\n\t\t}\n\t\treturn str[0:length-3] + \"...\"\n\t}\n\n\treturn str\n}\n\n```\n# Function Signature: func PascalCase(str string) string {\n# Natural Language Annotations: // PascalCase converts string to pascal case.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc PascalCase(str string) string {\n    items := Words(str)\n    for i, item := range items {\n        item = strings.ToTitle(item)\n        items[i] = item\n    }\n    return strings.Join(items, \"\")\n}\n```", "```go\nfunc PascalCase(str string) string {\n\titems := Words(str)\n\tfor i, item := range items {\n\t\titem = strings.ToLower(item)\n\t\tif i > 0 {\n\t\t\titem = Capitalize(item)\n\t\t}\n\t\titems[i] = item\n\t}\n\treturn strings.Join(items, \"\")\n}\n```", "```go\nfunc PascalCase(str string) string {\n\titems := Words(str)\n\tfor i, item := range items {\n\t\titem = strings.ToUpper(item)\n\t\tif i > 0 {\n\t\t\titem = Capitalize(item)\n\t\t}\n\t\titems[i] = item\n\t}\n\treturn strings.Join(items, \"\")\n}\n```"]}, "lo-map.go-PickByValues": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/map.go:\n```\npackage lo\n\n// Keys creates an array of the map keys.\n// Play: https://go.dev/play/p/Uu11fHASqrU\nfunc Keys[K comparable, V any](in map[K]V) []K {\n\tresult := make([]K, 0, len(in))\n\n\tfor k := range in {\n\t\tresult = append(result, k)\n\t}\n\n\treturn result\n}\n\n// HasKey returns whether the given key exists.\n// Play: https://go.dev/play/p/aVwubIvECqS\nfunc HasKey[K comparable, V any](in map[K]V, key K) bool {\n\t_, ok := in[key]\n\treturn ok\n}\n\n// Values creates an array of the map values.\n// Play: https://go.dev/play/p/nnRTQkzQfF6\nfunc Values[K comparable, V any](in map[K]V) []V {\n\tresult := make([]V, 0, len(in))\n\n\tfor k := range in {\n\t\tresult = append(result, in[k])\n\t}\n\n\treturn result\n}\n\n// ValueOr returns the value of the given key or the fallback value if the key is not present.\n// Play: https://go.dev/play/p/bAq9mHErB4V\nfunc ValueOr[K comparable, V any](in map[K]V, key K, fallback V) V {\n\tif v, ok := in[key]; ok {\n\t\treturn v\n\t}\n\treturn fallback\n}\n\n// PickBy returns same map type filtered by given predicate.\n// Play: https://go.dev/play/p/kdg8GR_QMmf\nfunc PickBy[K comparable, V any, Map ~map[K]V](in Map, predicate func(key K, value V) bool) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tif predicate(k, in[k]) {\n\t\t\tr[k] = in[k]\n\t\t}\n\t}\n\treturn r\n}\n\n// PickByKeys returns same map type filtered by given keys.\n// Play: https://go.dev/play/p/R1imbuci9qU\nfunc PickByKeys[K comparable, V any, Map ~map[K]V](in Map, keys []K) Map {\n\tr := Map{}\n\tfor i := range keys {\n\t\tif v, ok := in[keys[i]]; ok {\n\t\t\tr[keys[i]] = v\n\t\t}\n\t}\n\treturn r\n}\n\n// PickByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/1zdzSvbfsJc\n\n\n\n\n\n\n\n\n\n\n// OmitBy returns same map type filtered by given predicate.\n// Play: https://go.dev/play/p/EtBsR43bdsd\nfunc OmitBy[K comparable, V any, Map ~map[K]V](in Map, predicate func(key K, value V) bool) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tif !predicate(k, in[k]) {\n\t\t\tr[k] = in[k]\n\t\t}\n\t}\n\treturn r\n}\n\n// OmitByKeys returns same map type filtered by given keys.\n// Play: https://go.dev/play/p/t1QjCrs-ysk\nfunc OmitByKeys[K comparable, V any, Map ~map[K]V](in Map, keys []K) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tr[k] = in[k]\n\t}\n\tfor i := range keys {\n\t\tdelete(r, keys[i])\n\t}\n\treturn r\n}\n\n// OmitByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/9UYZi-hrs8j\n\n\n\n\n\n\n\n\n\n\n// Entries transforms a map into array of key/value pairs.\n// Play:\nfunc Entries[K comparable, V any](in map[K]V) []Entry[K, V] {\n\tentries := make([]Entry[K, V], 0, len(in))\n\n\tfor k := range in {\n\t\tentries = append(entries, Entry[K, V]{\n\t\t\tKey:   k,\n\t\t\tValue: in[k],\n\t\t})\n\t}\n\n\treturn entries\n}\n\n// ToPairs transforms a map into array of key/value pairs.\n// Alias of Entries().\n// Play: https://go.dev/play/p/3Dhgx46gawJ\nfunc ToPairs[K comparable, V any](in map[K]V) []Entry[K, V] {\n\treturn Entries(in)\n}\n\n// FromEntries transforms an array of key/value pairs into a map.\n// Play: https://go.dev/play/p/oIr5KHFGCEN\nfunc FromEntries[K comparable, V any](entries []Entry[K, V]) map[K]V {\n\tout := make(map[K]V, len(entries))\n\n\tfor i := range entries {\n\t\tout[entries[i].Key] = entries[i].Value\n\t}\n\n\treturn out\n}\n\n// FromPairs transforms an array of key/value pairs into a map.\n// Alias of FromEntries().\n// Play: https://go.dev/play/p/oIr5KHFGCEN\nfunc FromPairs[K comparable, V any](entries []Entry[K, V]) map[K]V {\n\treturn FromEntries(entries)\n}\n\n// Invert creates a map composed of the inverted keys and values. If map\n// contains duplicate values, subsequent values overwrite property assignments\n// of previous values.\n// Play: https://go.dev/play/p/rFQ4rak6iA1\nfunc Invert[K comparable, V comparable](in map[K]V) map[V]K {\n\tout := make(map[V]K, len(in))\n\n\tfor k := range in {\n\t\tout[in[k]] = k\n\t}\n\n\treturn out\n}\n\n// Assign merges multiple maps from left to right.\n// Play: https://go.dev/play/p/VhwfJOyxf5o\nfunc Assign[K comparable, V any, Map ~map[K]V](maps ...Map) Map {\n\tcount := 0\n\tfor i := range maps {\n\t\tcount += len(maps[i])\n\t}\n\n\tout := make(Map, count)\n\tfor i := range maps {\n\t\tfor k := range maps[i] {\n\t\t\tout[k] = maps[i][k]\n\t\t}\n\t}\n\n\treturn out\n}\n\n// MapKeys manipulates a map keys and transforms it to a map of another type.\n// Play: https://go.dev/play/p/9_4WPIqOetJ\nfunc MapKeys[K comparable, V any, R comparable](in map[K]V, iteratee func(value V, key K) R) map[R]V {\n\tresult := make(map[R]V, len(in))\n\n\tfor k := range in {\n\t\tresult[iteratee(in[k], k)] = in[k]\n\t}\n\n\treturn result\n}\n\n// MapValues manipulates a map values and transforms it to a map of another type.\n// Play: https://go.dev/play/p/T_8xAfvcf0W\nfunc MapValues[K comparable, V any, R any](in map[K]V, iteratee func(value V, key K) R) map[K]R {\n\tresult := make(map[K]R, len(in))\n\n\tfor k := range in {\n\t\tresult[k] = iteratee(in[k], k)\n\t}\n\n\treturn result\n}\n\n// MapEntries manipulates a map entries and transforms it to a map of another type.\n// Play: https://go.dev/play/p/VuvNQzxKimT\nfunc MapEntries[K1 comparable, V1 any, K2 comparable, V2 any](in map[K1]V1, iteratee func(key K1, value V1) (K2, V2)) map[K2]V2 {\n\tresult := make(map[K2]V2, len(in))\n\n\tfor k1 := range in {\n\t\tk2, v2 := iteratee(k1, in[k1])\n\t\tresult[k2] = v2\n\t}\n\n\treturn result\n}\n\n// MapToSlice transforms a map into a slice based on specific iteratee\n// Play: https://go.dev/play/p/ZuiCZpDt6LD\nfunc MapToSlice[K comparable, V any, R any](in map[K]V, iteratee func(key K, value V) R) []R {\n\tresult := make([]R, 0, len(in))\n\n\tfor k := range in {\n\t\tresult = append(result, iteratee(k, in[k]))\n\t}\n\n\treturn result\n}\n\n```\n# Function Signature: func PickByValues[K comparable, V comparable, Map ~map[K]V](in Map, values []V) Map {\n# Natural Language Annotations: // PickByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/1zdzSvbfsJc\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// PickByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/1zdzSvbfsJc\nfunc PickByValues[K comparable, V comparable, Map ~map[K]V](in Map, values []V) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tfor v := range values {\n\t\t\tif in[k] == values[v] {\n\t\t\t\tr[k] = in[k]\n\t\t\t}\n\t\t}\n\t}\n\treturn r\n}\n```", "```go\nfunc PickByValues[K comparable, V comparable, Map ~map[K]V](in Map, values []V) Map {\n\tr := Map{}\n\tvalueMap := make(map[V]bool)\n\tfor _, v := range values {\n\t\tvalueMap[v] = true\n\t}\n\tfor k := range in {\n\t\tif _, ok := valueMap[in[k]]; ok {\n\t\t\tr[k] = in[k]\n\t\t}\n\t}\n\treturn r\n}\n```", "```go\nfunc PickByValues[K comparable, V comparable, Map ~map[K]V](in Map, values []V) Map {\n    r := Map{}\n    for k := range in {\n        for i := range values {\n            if in[k] == values[i] {\n                r[k] = in[k]\n            }\n        }\n    }\n    return r\n}\n```"]}, "lo-map.go-OmitByValues": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/map.go:\n```\npackage lo\n\n// Keys creates an array of the map keys.\n// Play: https://go.dev/play/p/Uu11fHASqrU\nfunc Keys[K comparable, V any](in map[K]V) []K {\n\tresult := make([]K, 0, len(in))\n\n\tfor k := range in {\n\t\tresult = append(result, k)\n\t}\n\n\treturn result\n}\n\n// HasKey returns whether the given key exists.\n// Play: https://go.dev/play/p/aVwubIvECqS\nfunc HasKey[K comparable, V any](in map[K]V, key K) bool {\n\t_, ok := in[key]\n\treturn ok\n}\n\n// Values creates an array of the map values.\n// Play: https://go.dev/play/p/nnRTQkzQfF6\nfunc Values[K comparable, V any](in map[K]V) []V {\n\tresult := make([]V, 0, len(in))\n\n\tfor k := range in {\n\t\tresult = append(result, in[k])\n\t}\n\n\treturn result\n}\n\n// ValueOr returns the value of the given key or the fallback value if the key is not present.\n// Play: https://go.dev/play/p/bAq9mHErB4V\nfunc ValueOr[K comparable, V any](in map[K]V, key K, fallback V) V {\n\tif v, ok := in[key]; ok {\n\t\treturn v\n\t}\n\treturn fallback\n}\n\n// PickBy returns same map type filtered by given predicate.\n// Play: https://go.dev/play/p/kdg8GR_QMmf\nfunc PickBy[K comparable, V any, Map ~map[K]V](in Map, predicate func(key K, value V) bool) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tif predicate(k, in[k]) {\n\t\t\tr[k] = in[k]\n\t\t}\n\t}\n\treturn r\n}\n\n// PickByKeys returns same map type filtered by given keys.\n// Play: https://go.dev/play/p/R1imbuci9qU\nfunc PickByKeys[K comparable, V any, Map ~map[K]V](in Map, keys []K) Map {\n\tr := Map{}\n\tfor i := range keys {\n\t\tif v, ok := in[keys[i]]; ok {\n\t\t\tr[keys[i]] = v\n\t\t}\n\t}\n\treturn r\n}\n\n// PickByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/1zdzSvbfsJc\n\n\n\n\n\n\n\n\n\n\n// OmitBy returns same map type filtered by given predicate.\n// Play: https://go.dev/play/p/EtBsR43bdsd\nfunc OmitBy[K comparable, V any, Map ~map[K]V](in Map, predicate func(key K, value V) bool) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tif !predicate(k, in[k]) {\n\t\t\tr[k] = in[k]\n\t\t}\n\t}\n\treturn r\n}\n\n// OmitByKeys returns same map type filtered by given keys.\n// Play: https://go.dev/play/p/t1QjCrs-ysk\nfunc OmitByKeys[K comparable, V any, Map ~map[K]V](in Map, keys []K) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tr[k] = in[k]\n\t}\n\tfor i := range keys {\n\t\tdelete(r, keys[i])\n\t}\n\treturn r\n}\n\n// OmitByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/9UYZi-hrs8j\n\n\n\n\n\n\n\n\n\n\n// Entries transforms a map into array of key/value pairs.\n// Play:\nfunc Entries[K comparable, V any](in map[K]V) []Entry[K, V] {\n\tentries := make([]Entry[K, V], 0, len(in))\n\n\tfor k := range in {\n\t\tentries = append(entries, Entry[K, V]{\n\t\t\tKey:   k,\n\t\t\tValue: in[k],\n\t\t})\n\t}\n\n\treturn entries\n}\n\n// ToPairs transforms a map into array of key/value pairs.\n// Alias of Entries().\n// Play: https://go.dev/play/p/3Dhgx46gawJ\nfunc ToPairs[K comparable, V any](in map[K]V) []Entry[K, V] {\n\treturn Entries(in)\n}\n\n// FromEntries transforms an array of key/value pairs into a map.\n// Play: https://go.dev/play/p/oIr5KHFGCEN\nfunc FromEntries[K comparable, V any](entries []Entry[K, V]) map[K]V {\n\tout := make(map[K]V, len(entries))\n\n\tfor i := range entries {\n\t\tout[entries[i].Key] = entries[i].Value\n\t}\n\n\treturn out\n}\n\n// FromPairs transforms an array of key/value pairs into a map.\n// Alias of FromEntries().\n// Play: https://go.dev/play/p/oIr5KHFGCEN\nfunc FromPairs[K comparable, V any](entries []Entry[K, V]) map[K]V {\n\treturn FromEntries(entries)\n}\n\n// Invert creates a map composed of the inverted keys and values. If map\n// contains duplicate values, subsequent values overwrite property assignments\n// of previous values.\n// Play: https://go.dev/play/p/rFQ4rak6iA1\nfunc Invert[K comparable, V comparable](in map[K]V) map[V]K {\n\tout := make(map[V]K, len(in))\n\n\tfor k := range in {\n\t\tout[in[k]] = k\n\t}\n\n\treturn out\n}\n\n// Assign merges multiple maps from left to right.\n// Play: https://go.dev/play/p/VhwfJOyxf5o\nfunc Assign[K comparable, V any, Map ~map[K]V](maps ...Map) Map {\n\tcount := 0\n\tfor i := range maps {\n\t\tcount += len(maps[i])\n\t}\n\n\tout := make(Map, count)\n\tfor i := range maps {\n\t\tfor k := range maps[i] {\n\t\t\tout[k] = maps[i][k]\n\t\t}\n\t}\n\n\treturn out\n}\n\n// MapKeys manipulates a map keys and transforms it to a map of another type.\n// Play: https://go.dev/play/p/9_4WPIqOetJ\nfunc MapKeys[K comparable, V any, R comparable](in map[K]V, iteratee func(value V, key K) R) map[R]V {\n\tresult := make(map[R]V, len(in))\n\n\tfor k := range in {\n\t\tresult[iteratee(in[k], k)] = in[k]\n\t}\n\n\treturn result\n}\n\n// MapValues manipulates a map values and transforms it to a map of another type.\n// Play: https://go.dev/play/p/T_8xAfvcf0W\nfunc MapValues[K comparable, V any, R any](in map[K]V, iteratee func(value V, key K) R) map[K]R {\n\tresult := make(map[K]R, len(in))\n\n\tfor k := range in {\n\t\tresult[k] = iteratee(in[k], k)\n\t}\n\n\treturn result\n}\n\n// MapEntries manipulates a map entries and transforms it to a map of another type.\n// Play: https://go.dev/play/p/VuvNQzxKimT\nfunc MapEntries[K1 comparable, V1 any, K2 comparable, V2 any](in map[K1]V1, iteratee func(key K1, value V1) (K2, V2)) map[K2]V2 {\n\tresult := make(map[K2]V2, len(in))\n\n\tfor k1 := range in {\n\t\tk2, v2 := iteratee(k1, in[k1])\n\t\tresult[k2] = v2\n\t}\n\n\treturn result\n}\n\n// MapToSlice transforms a map into a slice based on specific iteratee\n// Play: https://go.dev/play/p/ZuiCZpDt6LD\nfunc MapToSlice[K comparable, V any, R any](in map[K]V, iteratee func(key K, value V) R) []R {\n\tresult := make([]R, 0, len(in))\n\n\tfor k := range in {\n\t\tresult = append(result, iteratee(k, in[k]))\n\t}\n\n\treturn result\n}\n\n```\n# Function Signature: func OmitByValues[K comparable, V comparable, Map ~map[K]V](in Map, values []V) Map {\n# Natural Language Annotations: // OmitByValues returns same map type filtered by given values.\n// Play: https://go.dev/play/p/9UYZi-hrs8j\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc OmitByValues[K comparable, V comparable, Map ~map[K]V](in Map, values []V) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tr[k] = in[k]\n\t}\n\tfor i := range values {\n\t\tfor k := range r {\n\t\t\tif r[k] == values[i] {\n\t\t\t\tdelete(r, k)\n\t\t\t}\n\t\t}\n\t}\n\treturn r\n}\n```", "```go\nfunc OmitByValues[K comparable, V comparable, Map ~map[K]V](in Map, values []V) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tr[k] = in[k]\n\t}\n\tfor i := range values {\n\t\tfor k := range in {\n\t\t\tif in[k] == values[i] {\n\t\t\t\tdelete(r, k)\n\t\t\t}\n\t\t}\n\t}\n\treturn r\n}\n```", "```go\nfunc OmitByValues[K comparable, V comparable, Map ~map[K]V](in Map, values []V) Map {\n\tr := Map{}\n\tfor k := range in {\n\t\tr[k] = in[k]\n\t}\n\tfor i := range values {\n\t\tfor k := range in {\n\t\t\tif in[k] == values[i] {\n\t\t\t\tdelete(r, k)\n\t\t\t}\n\t\t}\n\t}\n\treturn r\n}\n```"]}, "lo-tuples.go-Zip9": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/tuples.go:\n```\npackage lo\n\n// T2 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T2[A, B any](a A, b B) Tuple2[A, B] {\n\treturn Tuple2[A, B]{A: a, B: b}\n}\n\n// T3 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T3[A, B, C any](a A, b B, c C) Tuple3[A, B, C] {\n\treturn Tuple3[A, B, C]{A: a, B: b, C: c}\n}\n\n// T4 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T4[A, B, C, D any](a A, b B, c C, d D) Tuple4[A, B, C, D] {\n\treturn Tuple4[A, B, C, D]{A: a, B: b, C: c, D: d}\n}\n\n// T5 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T5[A, B, C, D, E any](a A, b B, c C, d D, e E) Tuple5[A, B, C, D, E] {\n\treturn Tuple5[A, B, C, D, E]{A: a, B: b, C: c, D: d, E: e}\n}\n\n// T6 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T6[A, B, C, D, E, F any](a A, b B, c C, d D, e E, f F) Tuple6[A, B, C, D, E, F] {\n\treturn Tuple6[A, B, C, D, E, F]{A: a, B: b, C: c, D: d, E: e, F: f}\n}\n\n// T7 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T7[A, B, C, D, E, F, G any](a A, b B, c C, d D, e E, f F, g G) Tuple7[A, B, C, D, E, F, G] {\n\treturn Tuple7[A, B, C, D, E, F, G]{A: a, B: b, C: c, D: d, E: e, F: f, G: g}\n}\n\n// T8 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T8[A, B, C, D, E, F, G, H any](a A, b B, c C, d D, e E, f F, g G, h H) Tuple8[A, B, C, D, E, F, G, H] {\n\treturn Tuple8[A, B, C, D, E, F, G, H]{A: a, B: b, C: c, D: d, E: e, F: f, G: g, H: h}\n}\n\n// T9 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T9[A, B, C, D, E, F, G, H, I any](a A, b B, c C, d D, e E, f F, g G, h H, i I) Tuple9[A, B, C, D, E, F, G, H, I] {\n\treturn Tuple9[A, B, C, D, E, F, G, H, I]{A: a, B: b, C: c, D: d, E: e, F: f, G: g, H: h, I: i}\n}\n\n// Unpack2 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack2[A, B any](tuple Tuple2[A, B]) (A, B) {\n\treturn tuple.A, tuple.B\n}\n\n// Unpack3 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack3[A, B, C any](tuple Tuple3[A, B, C]) (A, B, C) {\n\treturn tuple.A, tuple.B, tuple.C\n}\n\n// Unpack4 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack4[A, B, C, D any](tuple Tuple4[A, B, C, D]) (A, B, C, D) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D\n}\n\n// Unpack5 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack5[A, B, C, D, E any](tuple Tuple5[A, B, C, D, E]) (A, B, C, D, E) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E\n}\n\n// Unpack6 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack6[A, B, C, D, E, F any](tuple Tuple6[A, B, C, D, E, F]) (A, B, C, D, E, F) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F\n}\n\n// Unpack7 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack7[A, B, C, D, E, F, G any](tuple Tuple7[A, B, C, D, E, F, G]) (A, B, C, D, E, F, G) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F, tuple.G\n}\n\n// Unpack8 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack8[A, B, C, D, E, F, G, H any](tuple Tuple8[A, B, C, D, E, F, G, H]) (A, B, C, D, E, F, G, H) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F, tuple.G, tuple.H\n}\n\n// Unpack9 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack9[A, B, C, D, E, F, G, H, I any](tuple Tuple9[A, B, C, D, E, F, G, H, I]) (A, B, C, D, E, F, G, H, I) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F, tuple.G, tuple.H, tuple.I\n}\n\n// Zip2 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip2[A, B any](a []A, b []B) []Tuple2[A, B] {\n\tsize := Max([]int{len(a), len(b)})\n\n\tresult := make([]Tuple2[A, B], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\n\t\tresult = append(result, Tuple2[A, B]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip3 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip3[A, B, C any](a []A, b []B, c []C) []Tuple3[A, B, C] {\n\tsize := Max([]int{len(a), len(b), len(c)})\n\n\tresult := make([]Tuple3[A, B, C], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\n\t\tresult = append(result, Tuple3[A, B, C]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip4 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip4[A, B, C, D any](a []A, b []B, c []C, d []D) []Tuple4[A, B, C, D] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d)})\n\n\tresult := make([]Tuple4[A, B, C, D], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\n\t\tresult = append(result, Tuple4[A, B, C, D]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip5 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip5[A, B, C, D, E any](a []A, b []B, c []C, d []D, e []E) []Tuple5[A, B, C, D, E] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e)})\n\n\tresult := make([]Tuple5[A, B, C, D, E], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\n\t\tresult = append(result, Tuple5[A, B, C, D, E]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip6 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip6[A, B, C, D, E, F any](a []A, b []B, c []C, d []D, e []E, f []F) []Tuple6[A, B, C, D, E, F] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f)})\n\n\tresult := make([]Tuple6[A, B, C, D, E, F], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\n\t\tresult = append(result, Tuple6[A, B, C, D, E, F]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip7 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip7[A, B, C, D, E, F, G any](a []A, b []B, c []C, d []D, e []E, f []F, g []G) []Tuple7[A, B, C, D, E, F, G] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g)})\n\n\tresult := make([]Tuple7[A, B, C, D, E, F, G], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\n\t\tresult = append(result, Tuple7[A, B, C, D, E, F, G]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t\tG: _g,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip8 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip8[A, B, C, D, E, F, G, H any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H) []Tuple8[A, B, C, D, E, F, G, H] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g), len(h)})\n\n\tresult := make([]Tuple8[A, B, C, D, E, F, G, H], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\t\t_h, _ := Nth(h, index)\n\n\t\tresult = append(result, Tuple8[A, B, C, D, E, F, G, H]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t\tG: _g,\n\t\t\tH: _h,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip9 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ZipBy2 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy2[A any, B any, Out any](a []A, b []B, iteratee func(a A, b B) Out) []Out {\n\tsize := Max([]int{len(a), len(b)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\n\t\tresult = append(result, iteratee(_a, _b))\n\t}\n\n\treturn result\n}\n\n// ZipBy3 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy3[A any, B any, C any, Out any](a []A, b []B, c []C, iteratee func(a A, b B, c C) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c))\n\t}\n\n\treturn result\n}\n\n// ZipBy4 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy4[A any, B any, C any, D any, Out any](a []A, b []B, c []C, d []D, iteratee func(a A, b B, c C, d D) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d))\n\t}\n\n\treturn result\n}\n\n// ZipBy5 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy5[A any, B any, C any, D any, E any, Out any](a []A, b []B, c []C, d []D, e []E, iteratee func(a A, b B, c C, d D, e E) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d, _e))\n\t}\n\n\treturn result\n}\n\n// ZipBy6 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy6[A any, B any, C any, D any, E any, F any, Out any](a []A, b []B, c []C, d []D, e []E, f []F, iteratee func(a A, b B, c C, d D, e E, f F) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d, _e, _f))\n\t}\n\n\treturn result\n}\n\n// ZipBy7 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy7[A any, B any, C any\n```\n# Function Signature: func Zip9[A, B, C, D, E, F, G, H, I any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H, i []I) []Tuple9[A, B, C, D, E, F, G, H, I] {\n# Natural Language Annotations: // Zip9 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage lo\n\n// Zip9 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip9[A, B, C, D, E, F, G, H, I any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H, i []I) []Tuple9[A, B, C, D, E, F, G, H, I] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g), len(h), len(i)})\n\n\tresult := make([]Tuple9[A, B, C, D, E, F, G, H, I], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\t\t_h, _ := Nth(h, index)\n\t\t_i, _ := Nth(i, index)\n\n\t\tresult = append(result, Tuple9[A, B, C, D, E, F, G, H, I]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t\tG: _g,\n\t\t\tH: _h,\n\t\t\tI: _i,\n\t\t})\n\t}\n\n\treturn result\n}\n```", "```go\npackage lo\n\n// Zip9 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip9[A, B, C, D, E, F, G, H, I any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H, i []I) []Tuple9[A, B, C, D, E, F, G, H, I] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g), len(h), len(i)})\n\n\tresult := make([]Tuple9[A, B, C, D, E, F, G, H, I], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\t\t_h, _ := Nth(h, index)\n\t\t_i, _ := Nth(i, index)\n\n\t\tresult = append(result, Tuple9[A, B, C, D, E, F, G, H, I]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t\tG: _g,\n\t\t\tH: _h,\n\t\t\tI: _i,\n\t\t})\n\t}\n\n\treturn result\n}\n```", "```go\npackage lo\n\n// Zip9 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip9[A, B, C, D, E, F, G, H, I any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H, i []I) []Tuple9[A, B, C, D, E, F, G, H, I] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g), len(h), len(i)})\n\n\tresult := make([]Tuple9[A, B, C, D, E, F, G, H, I], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\t\t_h, _ := Nth(h, index)\n\t\t_i, _ := Nth(i, index)\n\n\t\tresult = append(result, Tuple9[A, B, C, D, E, F, G, H, I]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t\tG: _g,\n\t\t\tH: _h,\n\t\t\tI: _i,\n\t\t})\n\t}\n\n\treturn result\n}\n```"]}, "lo-tuples.go-ZipBy9": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/tuples.go:\n```\npackage lo\n\n// T2 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T2[A, B any](a A, b B) Tuple2[A, B] {\n\treturn Tuple2[A, B]{A: a, B: b}\n}\n\n// T3 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T3[A, B, C any](a A, b B, c C) Tuple3[A, B, C] {\n\treturn Tuple3[A, B, C]{A: a, B: b, C: c}\n}\n\n// T4 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T4[A, B, C, D any](a A, b B, c C, d D) Tuple4[A, B, C, D] {\n\treturn Tuple4[A, B, C, D]{A: a, B: b, C: c, D: d}\n}\n\n// T5 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T5[A, B, C, D, E any](a A, b B, c C, d D, e E) Tuple5[A, B, C, D, E] {\n\treturn Tuple5[A, B, C, D, E]{A: a, B: b, C: c, D: d, E: e}\n}\n\n// T6 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T6[A, B, C, D, E, F any](a A, b B, c C, d D, e E, f F) Tuple6[A, B, C, D, E, F] {\n\treturn Tuple6[A, B, C, D, E, F]{A: a, B: b, C: c, D: d, E: e, F: f}\n}\n\n// T7 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T7[A, B, C, D, E, F, G any](a A, b B, c C, d D, e E, f F, g G) Tuple7[A, B, C, D, E, F, G] {\n\treturn Tuple7[A, B, C, D, E, F, G]{A: a, B: b, C: c, D: d, E: e, F: f, G: g}\n}\n\n// T8 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T8[A, B, C, D, E, F, G, H any](a A, b B, c C, d D, e E, f F, g G, h H) Tuple8[A, B, C, D, E, F, G, H] {\n\treturn Tuple8[A, B, C, D, E, F, G, H]{A: a, B: b, C: c, D: d, E: e, F: f, G: g, H: h}\n}\n\n// T9 creates a tuple from a list of values.\n// Play: https://go.dev/play/p/IllL3ZO4BQm\nfunc T9[A, B, C, D, E, F, G, H, I any](a A, b B, c C, d D, e E, f F, g G, h H, i I) Tuple9[A, B, C, D, E, F, G, H, I] {\n\treturn Tuple9[A, B, C, D, E, F, G, H, I]{A: a, B: b, C: c, D: d, E: e, F: f, G: g, H: h, I: i}\n}\n\n// Unpack2 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack2[A, B any](tuple Tuple2[A, B]) (A, B) {\n\treturn tuple.A, tuple.B\n}\n\n// Unpack3 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack3[A, B, C any](tuple Tuple3[A, B, C]) (A, B, C) {\n\treturn tuple.A, tuple.B, tuple.C\n}\n\n// Unpack4 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack4[A, B, C, D any](tuple Tuple4[A, B, C, D]) (A, B, C, D) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D\n}\n\n// Unpack5 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack5[A, B, C, D, E any](tuple Tuple5[A, B, C, D, E]) (A, B, C, D, E) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E\n}\n\n// Unpack6 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack6[A, B, C, D, E, F any](tuple Tuple6[A, B, C, D, E, F]) (A, B, C, D, E, F) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F\n}\n\n// Unpack7 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack7[A, B, C, D, E, F, G any](tuple Tuple7[A, B, C, D, E, F, G]) (A, B, C, D, E, F, G) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F, tuple.G\n}\n\n// Unpack8 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack8[A, B, C, D, E, F, G, H any](tuple Tuple8[A, B, C, D, E, F, G, H]) (A, B, C, D, E, F, G, H) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F, tuple.G, tuple.H\n}\n\n// Unpack9 returns values contained in tuple.\n// Play: https://go.dev/play/p/xVP_k0kJ96W\nfunc Unpack9[A, B, C, D, E, F, G, H, I any](tuple Tuple9[A, B, C, D, E, F, G, H, I]) (A, B, C, D, E, F, G, H, I) {\n\treturn tuple.A, tuple.B, tuple.C, tuple.D, tuple.E, tuple.F, tuple.G, tuple.H, tuple.I\n}\n\n// Zip2 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip2[A, B any](a []A, b []B) []Tuple2[A, B] {\n\tsize := Max([]int{len(a), len(b)})\n\n\tresult := make([]Tuple2[A, B], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\n\t\tresult = append(result, Tuple2[A, B]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip3 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip3[A, B, C any](a []A, b []B, c []C) []Tuple3[A, B, C] {\n\tsize := Max([]int{len(a), len(b), len(c)})\n\n\tresult := make([]Tuple3[A, B, C], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\n\t\tresult = append(result, Tuple3[A, B, C]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip4 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip4[A, B, C, D any](a []A, b []B, c []C, d []D) []Tuple4[A, B, C, D] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d)})\n\n\tresult := make([]Tuple4[A, B, C, D], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\n\t\tresult = append(result, Tuple4[A, B, C, D]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip5 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip5[A, B, C, D, E any](a []A, b []B, c []C, d []D, e []E) []Tuple5[A, B, C, D, E] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e)})\n\n\tresult := make([]Tuple5[A, B, C, D, E], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\n\t\tresult = append(result, Tuple5[A, B, C, D, E]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip6 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip6[A, B, C, D, E, F any](a []A, b []B, c []C, d []D, e []E, f []F) []Tuple6[A, B, C, D, E, F] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f)})\n\n\tresult := make([]Tuple6[A, B, C, D, E, F], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\n\t\tresult = append(result, Tuple6[A, B, C, D, E, F]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip7 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip7[A, B, C, D, E, F, G any](a []A, b []B, c []C, d []D, e []E, f []F, g []G) []Tuple7[A, B, C, D, E, F, G] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g)})\n\n\tresult := make([]Tuple7[A, B, C, D, E, F, G], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\n\t\tresult = append(result, Tuple7[A, B, C, D, E, F, G]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t\tG: _g,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip8 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\nfunc Zip8[A, B, C, D, E, F, G, H any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H) []Tuple8[A, B, C, D, E, F, G, H] {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g), len(h)})\n\n\tresult := make([]Tuple8[A, B, C, D, E, F, G, H], 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\t\t_h, _ := Nth(h, index)\n\n\t\tresult = append(result, Tuple8[A, B, C, D, E, F, G, H]{\n\t\t\tA: _a,\n\t\t\tB: _b,\n\t\t\tC: _c,\n\t\t\tD: _d,\n\t\t\tE: _e,\n\t\t\tF: _f,\n\t\t\tG: _g,\n\t\t\tH: _h,\n\t\t})\n\t}\n\n\treturn result\n}\n\n// Zip9 creates a slice of grouped elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\n// Play: https://go.dev/play/p/jujaA6GaJTp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ZipBy2 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy2[A any, B any, Out any](a []A, b []B, iteratee func(a A, b B) Out) []Out {\n\tsize := Max([]int{len(a), len(b)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\n\t\tresult = append(result, iteratee(_a, _b))\n\t}\n\n\treturn result\n}\n\n// ZipBy3 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy3[A any, B any, C any, Out any](a []A, b []B, c []C, iteratee func(a A, b B, c C) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c))\n\t}\n\n\treturn result\n}\n\n// ZipBy4 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy4[A any, B any, C any, D any, Out any](a []A, b []B, c []C, d []D, iteratee func(a A, b B, c C, d D) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d))\n\t}\n\n\treturn result\n}\n\n// ZipBy5 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy5[A any, B any, C any, D any, E any, Out any](a []A, b []B, c []C, d []D, e []E, iteratee func(a A, b B, c C, d D, e E) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d, _e))\n\t}\n\n\treturn result\n}\n\n// ZipBy6 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy6[A any, B any, C any, D any, E any, F any, Out any](a []A, b []B, c []C, d []D, e []E, f []F, iteratee func(a A, b B, c C, d D, e E, f F) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d, _e, _f))\n\t}\n\n\treturn result\n}\n\n// ZipBy7 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy7[A any, B any, C any\n```\n# Function Signature: func ZipBy9[A any, B any, C any, D any, E any, F any, G any, H any, I any, Out any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H, i []I, iteratee func(a A, b B, c C, d D, e E, f F, g G, h H, i I) Out) []Out {\n# Natural Language Annotations: // ZipBy9 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage lo\n\nimport \"github.com/samber/lo\"\n\n// ZipBy9 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy9[A any, B any, C any, D any, E any, F any, G any, H any, I any, Out any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H, i []I, iteratee func(a A, b B, c C, d D, e E, f F, g G, h H, i I) Out) []Out {\n\tsize := lo.Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g), len(h), len(i)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := lo.Nth(a, index)\n\t\t_b, _ := lo.Nth(b, index)\n\t\t_c, _ := lo.Nth(c, index)\n\t\t_d, _ := lo.Nth(d, index)\n\t\t_e, _ := lo.Nth(e, index)\n\t\t_f, _ := lo.Nth(f, index)\n\t\t_g, _ := lo.Nth(g, index)\n\t\t_h, _ := lo.Nth(h, index)\n\t\t_i, _ := lo.Nth(i, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d, _e, _f, _g, _h, _i))\n\t}\n\n\treturn result\n}\n```", "```go\n// ZipBy9 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy9[A any, B any, C any, D any, E any, F any, G any, H any, I any, Out any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H, i []I, iteratee func(a A, b B, c C, d D, e E, f F, g G, h H, i I) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g), len(h), len(i)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\t\t_h, _ := Nth(h, index)\n\t\t_i, _ := Nth(i, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d, _e, _f, _g, _h, _i))\n\t}\n\n\treturn result\n}\n```", "```go\npackage lo\n\n// ZipBy9 creates a slice of transformed elements, the first of which contains the first elements\n// of the given arrays, the second of which contains the second elements of the given arrays, and so on.\n// When collections have different size, the Tuple attributes are filled with zero value.\nfunc ZipBy9[A any, B any, C any, D any, E any, F any, G any, H any, I any, Out any](a []A, b []B, c []C, d []D, e []E, f []F, g []G, h []H, i []I, iteratee func(a A, b B, c C, d D, e E, f F, g G, h H, i I) Out) []Out {\n\tsize := Max([]int{len(a), len(b), len(c), len(d), len(e), len(f), len(g), len(h), len(i)})\n\n\tresult := make([]Out, 0, size)\n\n\tfor index := 0; index < size; index++ {\n\t\t_a, _ := Nth(a, index)\n\t\t_b, _ := Nth(b, index)\n\t\t_c, _ := Nth(c, index)\n\t\t_d, _ := Nth(d, index)\n\t\t_e, _ := Nth(e, index)\n\t\t_f, _ := Nth(f, index)\n\t\t_g, _ := Nth(g, index)\n\t\t_h, _ := Nth(h, index)\n\t\t_i, _ := Nth(i, index)\n\n\t\tresult = append(result, iteratee(_a, _b, _c, _d, _e, _f, _g, _h, _i))\n\t}\n\n\treturn result\n}\n```"]}, "lo-channel.go-ChannelDispatcher": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/channel.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/rand\"\n)\n\ntype DispatchingStrategy[T any] func(msg T, index uint64, channels []<-chan T) int\n\n// ChannelDispatcher distributes messages from input channels into N child channels.\n// Close events are propagated to children.\n// Underlying channels can have a fixed buffer capacity or be unbuffered when cap is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc createChannels[T any](count int, channelBufferCap int) []chan T {\n\tchildren := make([]chan T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tchildren = append(children, make(chan T, channelBufferCap))\n\t}\n\n\treturn children\n}\n\nfunc channelsToReadOnly[T any](children []chan T) []<-chan T {\n\troChildren := make([]<-chan T, 0, len(children))\n\n\tfor i := range children {\n\t\troChildren = append(roChildren, children[i])\n\t}\n\n\treturn roChildren\n}\n\nfunc closeChannels[T any](children []chan T) {\n\tfor i := 0; i < len(children); i++ {\n\t\tclose(children[i])\n\t}\n}\n\nfunc channelIsNotFull[T any](ch <-chan T) bool {\n\treturn cap(ch) == 0 || len(ch) < cap(ch)\n}\n\n// DispatchingStrategyRoundRobin distributes messages in a rotating sequential manner.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyRandom distributes messages in a random manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyWeightedRandom distributes messages in a weighted manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\nfunc DispatchingStrategyWeightedRandom[T any](weights []int) DispatchingStrategy[T] {\n\tseq := []int{}\n\n\tfor i := 0; i < len(weights); i++ {\n\t\tfor j := 0; j < weights[i]; j++ {\n\t\t\tseq = append(seq, i)\n\t\t}\n\t}\n\n\treturn func(msg T, index uint64, channels []<-chan T) int {\n\t\tfor {\n\t\t\ti := seq[rand.IntN(len(seq))]\n\t\t\tif channelIsNotFull(channels[i]) {\n\t\t\t\treturn i\n\t\t\t}\n\n\t\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t\t}\n\t}\n}\n\n// DispatchingStrategyFirst distributes messages in the first non-full channel.\n// If the capacity of the first channel is exceeded, the second channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyLeast distributes messages in the emptiest channel.\nfunc DispatchingStrategyLeast[T any](msg T, index uint64, channels []<-chan T) int {\n\tseq := Range(len(channels))\n\n\treturn MinBy(seq, func(item int, min int) bool {\n\t\treturn len(channels[item]) < len(channels[min])\n\t})\n}\n\n// DispatchingStrategyMost distributes messages in the fullest channel.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n// SliceToChannel returns a read-only channels of collection elements.\nfunc SliceToChannel[T any](bufferSize int, collection []T) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\tfor i := range collection {\n\t\t\tch <- collection[i]\n\t\t}\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// ChannelToSlice returns a slice built from channels items. Blocks until channel closes.\nfunc ChannelToSlice[T any](ch <-chan T) []T {\n\tcollection := []T{}\n\n\tfor item := range ch {\n\t\tcollection = append(collection, item)\n\t}\n\n\treturn collection\n}\n\n// Generator implements the generator design pattern.\nfunc Generator[T any](bufferSize int, generator func(yield func(T))) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\t// WARNING: infinite loop\n\t\tgenerator(func(t T) {\n\t\t\tch <- t\n\t\t})\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// Buffer creates a slice of n elements from a channel. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc Buffer[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\titem, ok := <-ch\n\t\tif !ok {\n\t\t\treturn buffer, index, time.Since(now), false\n\t\t}\n\n\t\tbuffer = append(buffer, item)\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// Batch creates a slice of n elements from a channel. Returns the slice and the slice length.\n//\n// Deprecated: Use [Buffer] instead.\nfunc Batch[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn Buffer(ch, size)\n}\n\n// BufferWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc BufferWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\texpire := time.NewTimer(timeout)\n\tdefer expire.Stop()\n\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\tselect {\n\t\tcase item, ok := <-ch:\n\t\t\tif !ok {\n\t\t\t\treturn buffer, index, time.Since(now), false\n\t\t\t}\n\n\t\t\tbuffer = append(buffer, item)\n\n\t\tcase <-expire.C:\n\t\t\treturn buffer, index, time.Since(now), true\n\t\t}\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// BatchWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n//\n// Deprecated: Use [BufferWithTimeout] instead.\nfunc BatchWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn BufferWithTimeout(ch, size, timeout)\n}\n\n// FanIn collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\nfunc FanIn[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\tout := make(chan T, channelBufferCap)\n\tvar wg sync.WaitGroup\n\n\t// Start an output goroutine for each input channel in upstreams.\n\twg.Add(len(upstreams))\n\tfor i := range upstreams {\n\t\tgo func(index int) {\n\t\t\tfor n := range upstreams[index] {\n\t\t\t\tout <- n\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(i)\n\t}\n\n\t// Start a goroutine to close out once all the output goroutines are done.\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(out)\n\t}()\n\treturn out\n}\n\n// ChannelMerge collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\n//\n// Deprecated: Use [FanIn] instead.\nfunc ChannelMerge[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\treturn FanIn(channelBufferCap, upstreams...)\n}\n\n// FanOut broadcasts all the upstream messages to multiple downstream channels.\n// When upstream channel reach EOF, downstream channels close. If any downstream\n// channels is full, broadcasting is paused.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func ChannelDispatcher[T any](stream <-chan T, count int, channelBufferCap int, strategy DispatchingStrategy[T]) []<-chan T {\n# Natural Language Annotations: // ChannelDispatcher distributes messages from input channels into N child channels.\n// Close events are propagated to children.\n// Underlying channels can have a fixed buffer capacity or be unbuffered when cap is 0.\nPlease return the generated code surrounded by ```\n", "response": ["Sure, here is the code for the `ChannelDispatcher` function:\n\n```go\nfunc ChannelDispatcher[T any](stream <-chan T, count int, channelBufferCap int, strategy DispatchingStrategy[T]) []<-chan T {\n\tchildren := createChannels[T](count, channelBufferCap)\n\troChildren := channelsToReadOnly(children)\n\n\tgo func() {\n\t\tdefer closeChannels(children)\n\n\t\tindex := uint64(0)\n\t\tfor msg := range stream {\n\t\t\ti := strategy(msg, index, roChildren)\n\t\t\tchildren[i] <- msg\n\t\t\tindex++\n\t\t}\n\t}()\n\n\treturn roChildren\n}\n```\n\nThis function works by creating a number of child channels equal to `count` and a read-only version of these channels. It then starts a goroutine that reads from the input stream and distributes the messages to the child channels using the provided strategy. When the input stream is closed, it also closes all the child channels. The function finally returns the read-only version of the child channels.", "```go\nfunc ChannelDispatcher[T any](stream <-chan T, count int, channelBufferCap int, strategy DispatchingStrategy[T]) []<-chan T {\n\tchildren := createChannels[T](count, channelBufferCap)\n\troChildren := channelsToReadOnly[T](children)\n\n\tgo func() {\n\t\tindex := uint64(0)\n\t\tfor msg := range stream {\n\t\t\ti := strategy(msg, index, roChildren)\n\t\t\tchildren[i] <- msg\n\n\t\t\tindex++\n\t\t}\n\n\t\tcloseChannels[T](children)\n\t}()\n\n\treturn roChildren\n}\n```", "```go\nfunc ChannelDispatcher[T any](stream <-chan T, count int, channelBufferCap int, strategy DispatchingStrategy[T]) []<-chan T {\n\tchildren := createChannels[T](count, channelBufferCap)\n\troChildren := channelsToReadOnly(children)\n\n\tgo func() {\n\t\tindex := uint64(0)\n\t\tfor msg := range stream {\n\t\t\tfor {\n\t\t\t\ti := strategy(msg, index, roChildren)\n\t\t\t\tif channelIsNotFull(children[i]) {\n\t\t\t\t\tchildren[i] <- msg\n\t\t\t\t\tindex++\n\t\t\t\t\tbreak\n\t\t\t\t}\n\n\t\t\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t\t\t}\n\t\t}\n\n\t\tcloseChannels(children)\n\t}()\n\n\treturn roChildren\n}\n```"]}, "lo-channel.go-DispatchingStrategyRoundRobin": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/channel.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/rand\"\n)\n\ntype DispatchingStrategy[T any] func(msg T, index uint64, channels []<-chan T) int\n\n// ChannelDispatcher distributes messages from input channels into N child channels.\n// Close events are propagated to children.\n// Underlying channels can have a fixed buffer capacity or be unbuffered when cap is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc createChannels[T any](count int, channelBufferCap int) []chan T {\n\tchildren := make([]chan T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tchildren = append(children, make(chan T, channelBufferCap))\n\t}\n\n\treturn children\n}\n\nfunc channelsToReadOnly[T any](children []chan T) []<-chan T {\n\troChildren := make([]<-chan T, 0, len(children))\n\n\tfor i := range children {\n\t\troChildren = append(roChildren, children[i])\n\t}\n\n\treturn roChildren\n}\n\nfunc closeChannels[T any](children []chan T) {\n\tfor i := 0; i < len(children); i++ {\n\t\tclose(children[i])\n\t}\n}\n\nfunc channelIsNotFull[T any](ch <-chan T) bool {\n\treturn cap(ch) == 0 || len(ch) < cap(ch)\n}\n\n// DispatchingStrategyRoundRobin distributes messages in a rotating sequential manner.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyRandom distributes messages in a random manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyWeightedRandom distributes messages in a weighted manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\nfunc DispatchingStrategyWeightedRandom[T any](weights []int) DispatchingStrategy[T] {\n\tseq := []int{}\n\n\tfor i := 0; i < len(weights); i++ {\n\t\tfor j := 0; j < weights[i]; j++ {\n\t\t\tseq = append(seq, i)\n\t\t}\n\t}\n\n\treturn func(msg T, index uint64, channels []<-chan T) int {\n\t\tfor {\n\t\t\ti := seq[rand.IntN(len(seq))]\n\t\t\tif channelIsNotFull(channels[i]) {\n\t\t\t\treturn i\n\t\t\t}\n\n\t\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t\t}\n\t}\n}\n\n// DispatchingStrategyFirst distributes messages in the first non-full channel.\n// If the capacity of the first channel is exceeded, the second channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyLeast distributes messages in the emptiest channel.\nfunc DispatchingStrategyLeast[T any](msg T, index uint64, channels []<-chan T) int {\n\tseq := Range(len(channels))\n\n\treturn MinBy(seq, func(item int, min int) bool {\n\t\treturn len(channels[item]) < len(channels[min])\n\t})\n}\n\n// DispatchingStrategyMost distributes messages in the fullest channel.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n// SliceToChannel returns a read-only channels of collection elements.\nfunc SliceToChannel[T any](bufferSize int, collection []T) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\tfor i := range collection {\n\t\t\tch <- collection[i]\n\t\t}\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// ChannelToSlice returns a slice built from channels items. Blocks until channel closes.\nfunc ChannelToSlice[T any](ch <-chan T) []T {\n\tcollection := []T{}\n\n\tfor item := range ch {\n\t\tcollection = append(collection, item)\n\t}\n\n\treturn collection\n}\n\n// Generator implements the generator design pattern.\nfunc Generator[T any](bufferSize int, generator func(yield func(T))) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\t// WARNING: infinite loop\n\t\tgenerator(func(t T) {\n\t\t\tch <- t\n\t\t})\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// Buffer creates a slice of n elements from a channel. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc Buffer[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\titem, ok := <-ch\n\t\tif !ok {\n\t\t\treturn buffer, index, time.Since(now), false\n\t\t}\n\n\t\tbuffer = append(buffer, item)\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// Batch creates a slice of n elements from a channel. Returns the slice and the slice length.\n//\n// Deprecated: Use [Buffer] instead.\nfunc Batch[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn Buffer(ch, size)\n}\n\n// BufferWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc BufferWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\texpire := time.NewTimer(timeout)\n\tdefer expire.Stop()\n\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\tselect {\n\t\tcase item, ok := <-ch:\n\t\t\tif !ok {\n\t\t\t\treturn buffer, index, time.Since(now), false\n\t\t\t}\n\n\t\t\tbuffer = append(buffer, item)\n\n\t\tcase <-expire.C:\n\t\t\treturn buffer, index, time.Since(now), true\n\t\t}\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// BatchWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n//\n// Deprecated: Use [BufferWithTimeout] instead.\nfunc BatchWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn BufferWithTimeout(ch, size, timeout)\n}\n\n// FanIn collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\nfunc FanIn[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\tout := make(chan T, channelBufferCap)\n\tvar wg sync.WaitGroup\n\n\t// Start an output goroutine for each input channel in upstreams.\n\twg.Add(len(upstreams))\n\tfor i := range upstreams {\n\t\tgo func(index int) {\n\t\t\tfor n := range upstreams[index] {\n\t\t\t\tout <- n\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(i)\n\t}\n\n\t// Start a goroutine to close out once all the output goroutines are done.\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(out)\n\t}()\n\treturn out\n}\n\n// ChannelMerge collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\n//\n// Deprecated: Use [FanIn] instead.\nfunc ChannelMerge[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\treturn FanIn(channelBufferCap, upstreams...)\n}\n\n// FanOut broadcasts all the upstream messages to multiple downstream channels.\n// When upstream channel reach EOF, downstream channels close. If any downstream\n// channels is full, broadcasting is paused.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func DispatchingStrategyRoundRobin[T any](msg T, index uint64, channels []<-chan T) int {\n# Natural Language Annotations: // DispatchingStrategyRoundRobin distributes messages in a rotating sequential manner.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc DispatchingStrategyRoundRobin[T any](msg T, index uint64, channels []<-chan T) int {\n\ti := int(index) % len(channels)\n\n\tfor !channelIsNotFull(channels[i]) {\n\t\ti = (i + 1) % len(channels)\n\t}\n\n\treturn i\n}\n```", "```go\nfunc DispatchingStrategyRoundRobin[T any](msg T, index uint64, channels []<-chan T) int {\n\tnextIndex := int(index) % len(channels)\n\n\tfor {\n\t\tif channelIsNotFull(channels[nextIndex]) {\n\t\t\treturn nextIndex\n\t\t}\n\n\t\tnextIndex = (nextIndex + 1) % len(channels)\n\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t}\n}\n```", "```go\nfunc DispatchingStrategyRoundRobin[T any](msg T, index uint64, channels []<-chan T) int {\n\tchannelCount := len(channels)\n\tnextChannel := int(index) % channelCount\n\n\tfor i := 0; i < channelCount; i++ {\n\t\tif channelIsNotFull(channels[nextChannel]) {\n\t\t\treturn nextChannel\n\t\t}\n\n\t\t// Move to the next channel in a circular manner\n\t\tnextChannel = (nextChannel + 1) % channelCount\n\t}\n\n\t// All channels are full, return -1 to indicate failure\n\treturn -1\n}\n```"]}, "lo-channel.go-DispatchingStrategyRandom": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/channel.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/rand\"\n)\n\ntype DispatchingStrategy[T any] func(msg T, index uint64, channels []<-chan T) int\n\n// ChannelDispatcher distributes messages from input channels into N child channels.\n// Close events are propagated to children.\n// Underlying channels can have a fixed buffer capacity or be unbuffered when cap is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc createChannels[T any](count int, channelBufferCap int) []chan T {\n\tchildren := make([]chan T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tchildren = append(children, make(chan T, channelBufferCap))\n\t}\n\n\treturn children\n}\n\nfunc channelsToReadOnly[T any](children []chan T) []<-chan T {\n\troChildren := make([]<-chan T, 0, len(children))\n\n\tfor i := range children {\n\t\troChildren = append(roChildren, children[i])\n\t}\n\n\treturn roChildren\n}\n\nfunc closeChannels[T any](children []chan T) {\n\tfor i := 0; i < len(children); i++ {\n\t\tclose(children[i])\n\t}\n}\n\nfunc channelIsNotFull[T any](ch <-chan T) bool {\n\treturn cap(ch) == 0 || len(ch) < cap(ch)\n}\n\n// DispatchingStrategyRoundRobin distributes messages in a rotating sequential manner.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyRandom distributes messages in a random manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyWeightedRandom distributes messages in a weighted manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\nfunc DispatchingStrategyWeightedRandom[T any](weights []int) DispatchingStrategy[T] {\n\tseq := []int{}\n\n\tfor i := 0; i < len(weights); i++ {\n\t\tfor j := 0; j < weights[i]; j++ {\n\t\t\tseq = append(seq, i)\n\t\t}\n\t}\n\n\treturn func(msg T, index uint64, channels []<-chan T) int {\n\t\tfor {\n\t\t\ti := seq[rand.IntN(len(seq))]\n\t\t\tif channelIsNotFull(channels[i]) {\n\t\t\t\treturn i\n\t\t\t}\n\n\t\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t\t}\n\t}\n}\n\n// DispatchingStrategyFirst distributes messages in the first non-full channel.\n// If the capacity of the first channel is exceeded, the second channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyLeast distributes messages in the emptiest channel.\nfunc DispatchingStrategyLeast[T any](msg T, index uint64, channels []<-chan T) int {\n\tseq := Range(len(channels))\n\n\treturn MinBy(seq, func(item int, min int) bool {\n\t\treturn len(channels[item]) < len(channels[min])\n\t})\n}\n\n// DispatchingStrategyMost distributes messages in the fullest channel.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n// SliceToChannel returns a read-only channels of collection elements.\nfunc SliceToChannel[T any](bufferSize int, collection []T) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\tfor i := range collection {\n\t\t\tch <- collection[i]\n\t\t}\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// ChannelToSlice returns a slice built from channels items. Blocks until channel closes.\nfunc ChannelToSlice[T any](ch <-chan T) []T {\n\tcollection := []T{}\n\n\tfor item := range ch {\n\t\tcollection = append(collection, item)\n\t}\n\n\treturn collection\n}\n\n// Generator implements the generator design pattern.\nfunc Generator[T any](bufferSize int, generator func(yield func(T))) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\t// WARNING: infinite loop\n\t\tgenerator(func(t T) {\n\t\t\tch <- t\n\t\t})\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// Buffer creates a slice of n elements from a channel. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc Buffer[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\titem, ok := <-ch\n\t\tif !ok {\n\t\t\treturn buffer, index, time.Since(now), false\n\t\t}\n\n\t\tbuffer = append(buffer, item)\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// Batch creates a slice of n elements from a channel. Returns the slice and the slice length.\n//\n// Deprecated: Use [Buffer] instead.\nfunc Batch[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn Buffer(ch, size)\n}\n\n// BufferWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc BufferWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\texpire := time.NewTimer(timeout)\n\tdefer expire.Stop()\n\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\tselect {\n\t\tcase item, ok := <-ch:\n\t\t\tif !ok {\n\t\t\t\treturn buffer, index, time.Since(now), false\n\t\t\t}\n\n\t\t\tbuffer = append(buffer, item)\n\n\t\tcase <-expire.C:\n\t\t\treturn buffer, index, time.Since(now), true\n\t\t}\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// BatchWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n//\n// Deprecated: Use [BufferWithTimeout] instead.\nfunc BatchWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn BufferWithTimeout(ch, size, timeout)\n}\n\n// FanIn collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\nfunc FanIn[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\tout := make(chan T, channelBufferCap)\n\tvar wg sync.WaitGroup\n\n\t// Start an output goroutine for each input channel in upstreams.\n\twg.Add(len(upstreams))\n\tfor i := range upstreams {\n\t\tgo func(index int) {\n\t\t\tfor n := range upstreams[index] {\n\t\t\t\tout <- n\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(i)\n\t}\n\n\t// Start a goroutine to close out once all the output goroutines are done.\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(out)\n\t}()\n\treturn out\n}\n\n// ChannelMerge collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\n//\n// Deprecated: Use [FanIn] instead.\nfunc ChannelMerge[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\treturn FanIn(channelBufferCap, upstreams...)\n}\n\n// FanOut broadcasts all the upstream messages to multiple downstream channels.\n// When upstream channel reach EOF, downstream channels close. If any downstream\n// channels is full, broadcasting is paused.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func DispatchingStrategyRandom[T any](msg T, index uint64, channels []<-chan T) int {\n# Natural Language Annotations: // DispatchingStrategyRandom distributes messages in a random manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc DispatchingStrategyRandom[T any](msg T, index uint64, channels []<-chan T) int {\n\tfor {\n\t\ti := rand.Intn(len(channels))\n\t\tif channelIsNotFull(channels[i]) {\n\t\t\treturn i\n\t\t}\n\n\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t}\n}\n```\n\nThe `DispatchingStrategyRandom` function generates a random index `i` within the range of the `channels` array's length. If the channel at index `i` is not full, that index is returned. If the channel is full, the function sleeps for a short duration before trying again. This is repeated indefinitely until a suitable index is found.", "```go\nfunc DispatchingStrategyRandom[T any](msg T, index uint64, channels []<-chan T) int {\n\tfor {\n\t\ti := rand.IntN(len(channels))\n\t\tif channelIsNotFull(channels[i]) {\n\t\t\treturn i\n\t\t}\n\n\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t}\n}\n```", "```go\nfunc DispatchingStrategyRandom[T any](msg T, index uint64, channels []<-chan T) int {\n\tfor {\n\t\ti := rand.IntN(len(channels)) // generate a random index\n\t\tif channelIsNotFull(channels[i]) {\n\t\t\treturn i // return the index of the channel if it's not full\n\t\t}\n\n\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t}\n}\n```"]}, "lo-channel.go-DispatchingStrategyFirst": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/channel.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/rand\"\n)\n\ntype DispatchingStrategy[T any] func(msg T, index uint64, channels []<-chan T) int\n\n// ChannelDispatcher distributes messages from input channels into N child channels.\n// Close events are propagated to children.\n// Underlying channels can have a fixed buffer capacity or be unbuffered when cap is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc createChannels[T any](count int, channelBufferCap int) []chan T {\n\tchildren := make([]chan T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tchildren = append(children, make(chan T, channelBufferCap))\n\t}\n\n\treturn children\n}\n\nfunc channelsToReadOnly[T any](children []chan T) []<-chan T {\n\troChildren := make([]<-chan T, 0, len(children))\n\n\tfor i := range children {\n\t\troChildren = append(roChildren, children[i])\n\t}\n\n\treturn roChildren\n}\n\nfunc closeChannels[T any](children []chan T) {\n\tfor i := 0; i < len(children); i++ {\n\t\tclose(children[i])\n\t}\n}\n\nfunc channelIsNotFull[T any](ch <-chan T) bool {\n\treturn cap(ch) == 0 || len(ch) < cap(ch)\n}\n\n// DispatchingStrategyRoundRobin distributes messages in a rotating sequential manner.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyRandom distributes messages in a random manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyWeightedRandom distributes messages in a weighted manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\nfunc DispatchingStrategyWeightedRandom[T any](weights []int) DispatchingStrategy[T] {\n\tseq := []int{}\n\n\tfor i := 0; i < len(weights); i++ {\n\t\tfor j := 0; j < weights[i]; j++ {\n\t\t\tseq = append(seq, i)\n\t\t}\n\t}\n\n\treturn func(msg T, index uint64, channels []<-chan T) int {\n\t\tfor {\n\t\t\ti := seq[rand.IntN(len(seq))]\n\t\t\tif channelIsNotFull(channels[i]) {\n\t\t\t\treturn i\n\t\t\t}\n\n\t\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t\t}\n\t}\n}\n\n// DispatchingStrategyFirst distributes messages in the first non-full channel.\n// If the capacity of the first channel is exceeded, the second channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyLeast distributes messages in the emptiest channel.\nfunc DispatchingStrategyLeast[T any](msg T, index uint64, channels []<-chan T) int {\n\tseq := Range(len(channels))\n\n\treturn MinBy(seq, func(item int, min int) bool {\n\t\treturn len(channels[item]) < len(channels[min])\n\t})\n}\n\n// DispatchingStrategyMost distributes messages in the fullest channel.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n// SliceToChannel returns a read-only channels of collection elements.\nfunc SliceToChannel[T any](bufferSize int, collection []T) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\tfor i := range collection {\n\t\t\tch <- collection[i]\n\t\t}\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// ChannelToSlice returns a slice built from channels items. Blocks until channel closes.\nfunc ChannelToSlice[T any](ch <-chan T) []T {\n\tcollection := []T{}\n\n\tfor item := range ch {\n\t\tcollection = append(collection, item)\n\t}\n\n\treturn collection\n}\n\n// Generator implements the generator design pattern.\nfunc Generator[T any](bufferSize int, generator func(yield func(T))) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\t// WARNING: infinite loop\n\t\tgenerator(func(t T) {\n\t\t\tch <- t\n\t\t})\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// Buffer creates a slice of n elements from a channel. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc Buffer[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\titem, ok := <-ch\n\t\tif !ok {\n\t\t\treturn buffer, index, time.Since(now), false\n\t\t}\n\n\t\tbuffer = append(buffer, item)\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// Batch creates a slice of n elements from a channel. Returns the slice and the slice length.\n//\n// Deprecated: Use [Buffer] instead.\nfunc Batch[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn Buffer(ch, size)\n}\n\n// BufferWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc BufferWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\texpire := time.NewTimer(timeout)\n\tdefer expire.Stop()\n\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\tselect {\n\t\tcase item, ok := <-ch:\n\t\t\tif !ok {\n\t\t\t\treturn buffer, index, time.Since(now), false\n\t\t\t}\n\n\t\t\tbuffer = append(buffer, item)\n\n\t\tcase <-expire.C:\n\t\t\treturn buffer, index, time.Since(now), true\n\t\t}\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// BatchWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n//\n// Deprecated: Use [BufferWithTimeout] instead.\nfunc BatchWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn BufferWithTimeout(ch, size, timeout)\n}\n\n// FanIn collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\nfunc FanIn[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\tout := make(chan T, channelBufferCap)\n\tvar wg sync.WaitGroup\n\n\t// Start an output goroutine for each input channel in upstreams.\n\twg.Add(len(upstreams))\n\tfor i := range upstreams {\n\t\tgo func(index int) {\n\t\t\tfor n := range upstreams[index] {\n\t\t\t\tout <- n\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(i)\n\t}\n\n\t// Start a goroutine to close out once all the output goroutines are done.\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(out)\n\t}()\n\treturn out\n}\n\n// ChannelMerge collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\n//\n// Deprecated: Use [FanIn] instead.\nfunc ChannelMerge[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\treturn FanIn(channelBufferCap, upstreams...)\n}\n\n// FanOut broadcasts all the upstream messages to multiple downstream channels.\n// When upstream channel reach EOF, downstream channels close. If any downstream\n// channels is full, broadcasting is paused.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func DispatchingStrategyFirst[T any](msg T, index uint64, channels []<-chan T) int {\n# Natural Language Annotations: // DispatchingStrategyFirst distributes messages in the first non-full channel.\n// If the capacity of the first channel is exceeded, the second channel will be selected and so on.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc DispatchingStrategyFirst[T any](msg T, index uint64, channels []<-chan T) int {\n\tfor i := range channels {\n\t\tif channelIsNotFull(channels[i]) {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n```", "```go\nfunc DispatchingStrategyFirst[T any](msg T, index uint64, channels []<-chan T) int {\n\tfor i := range channels {\n\t\tif channelIsNotFull(channels[i]) {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n```", "```go\nfunc DispatchingStrategyFirst[T any](msg T, index uint64, channels []<-chan T) int {\n\tfor i := range channels {\n\t\tif channelIsNotFull(channels[i]) {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n```"]}, "lo-channel.go-DispatchingStrategyMost": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/channel.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/rand\"\n)\n\ntype DispatchingStrategy[T any] func(msg T, index uint64, channels []<-chan T) int\n\n// ChannelDispatcher distributes messages from input channels into N child channels.\n// Close events are propagated to children.\n// Underlying channels can have a fixed buffer capacity or be unbuffered when cap is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc createChannels[T any](count int, channelBufferCap int) []chan T {\n\tchildren := make([]chan T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tchildren = append(children, make(chan T, channelBufferCap))\n\t}\n\n\treturn children\n}\n\nfunc channelsToReadOnly[T any](children []chan T) []<-chan T {\n\troChildren := make([]<-chan T, 0, len(children))\n\n\tfor i := range children {\n\t\troChildren = append(roChildren, children[i])\n\t}\n\n\treturn roChildren\n}\n\nfunc closeChannels[T any](children []chan T) {\n\tfor i := 0; i < len(children); i++ {\n\t\tclose(children[i])\n\t}\n}\n\nfunc channelIsNotFull[T any](ch <-chan T) bool {\n\treturn cap(ch) == 0 || len(ch) < cap(ch)\n}\n\n// DispatchingStrategyRoundRobin distributes messages in a rotating sequential manner.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyRandom distributes messages in a random manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyWeightedRandom distributes messages in a weighted manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\nfunc DispatchingStrategyWeightedRandom[T any](weights []int) DispatchingStrategy[T] {\n\tseq := []int{}\n\n\tfor i := 0; i < len(weights); i++ {\n\t\tfor j := 0; j < weights[i]; j++ {\n\t\t\tseq = append(seq, i)\n\t\t}\n\t}\n\n\treturn func(msg T, index uint64, channels []<-chan T) int {\n\t\tfor {\n\t\t\ti := seq[rand.IntN(len(seq))]\n\t\t\tif channelIsNotFull(channels[i]) {\n\t\t\t\treturn i\n\t\t\t}\n\n\t\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t\t}\n\t}\n}\n\n// DispatchingStrategyFirst distributes messages in the first non-full channel.\n// If the capacity of the first channel is exceeded, the second channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyLeast distributes messages in the emptiest channel.\nfunc DispatchingStrategyLeast[T any](msg T, index uint64, channels []<-chan T) int {\n\tseq := Range(len(channels))\n\n\treturn MinBy(seq, func(item int, min int) bool {\n\t\treturn len(channels[item]) < len(channels[min])\n\t})\n}\n\n// DispatchingStrategyMost distributes messages in the fullest channel.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n// SliceToChannel returns a read-only channels of collection elements.\nfunc SliceToChannel[T any](bufferSize int, collection []T) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\tfor i := range collection {\n\t\t\tch <- collection[i]\n\t\t}\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// ChannelToSlice returns a slice built from channels items. Blocks until channel closes.\nfunc ChannelToSlice[T any](ch <-chan T) []T {\n\tcollection := []T{}\n\n\tfor item := range ch {\n\t\tcollection = append(collection, item)\n\t}\n\n\treturn collection\n}\n\n// Generator implements the generator design pattern.\nfunc Generator[T any](bufferSize int, generator func(yield func(T))) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\t// WARNING: infinite loop\n\t\tgenerator(func(t T) {\n\t\t\tch <- t\n\t\t})\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// Buffer creates a slice of n elements from a channel. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc Buffer[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\titem, ok := <-ch\n\t\tif !ok {\n\t\t\treturn buffer, index, time.Since(now), false\n\t\t}\n\n\t\tbuffer = append(buffer, item)\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// Batch creates a slice of n elements from a channel. Returns the slice and the slice length.\n//\n// Deprecated: Use [Buffer] instead.\nfunc Batch[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn Buffer(ch, size)\n}\n\n// BufferWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc BufferWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\texpire := time.NewTimer(timeout)\n\tdefer expire.Stop()\n\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\tselect {\n\t\tcase item, ok := <-ch:\n\t\t\tif !ok {\n\t\t\t\treturn buffer, index, time.Since(now), false\n\t\t\t}\n\n\t\t\tbuffer = append(buffer, item)\n\n\t\tcase <-expire.C:\n\t\t\treturn buffer, index, time.Since(now), true\n\t\t}\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// BatchWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n//\n// Deprecated: Use [BufferWithTimeout] instead.\nfunc BatchWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn BufferWithTimeout(ch, size, timeout)\n}\n\n// FanIn collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\nfunc FanIn[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\tout := make(chan T, channelBufferCap)\n\tvar wg sync.WaitGroup\n\n\t// Start an output goroutine for each input channel in upstreams.\n\twg.Add(len(upstreams))\n\tfor i := range upstreams {\n\t\tgo func(index int) {\n\t\t\tfor n := range upstreams[index] {\n\t\t\t\tout <- n\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(i)\n\t}\n\n\t// Start a goroutine to close out once all the output goroutines are done.\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(out)\n\t}()\n\treturn out\n}\n\n// ChannelMerge collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\n//\n// Deprecated: Use [FanIn] instead.\nfunc ChannelMerge[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\treturn FanIn(channelBufferCap, upstreams...)\n}\n\n// FanOut broadcasts all the upstream messages to multiple downstream channels.\n// When upstream channel reach EOF, downstream channels close. If any downstream\n// channels is full, broadcasting is paused.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func DispatchingStrategyMost[T any](msg T, index uint64, channels []<-chan T) int {\n# Natural Language Annotations: // DispatchingStrategyMost distributes messages in the fullest channel.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\nPlease return the generated code surrounded by ```\n", "response": ["The function `DispatchingStrategyMost` is already defined in the file you provided. It is used to distribute messages in the fullest channel. If the channel capacity is exceeded, the next channel will be selected and so on.\n\nHere is the function:\n\n```go\n// DispatchingStrategyMost distributes messages in the fullest channel.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\nfunc DispatchingStrategyMost[T any](msg T, index uint64, channels []<-chan T) int {\n\tseq := Range(len(channels))\n\n\treturn MaxBy(seq, func(item int, max int) bool {\n\t\treturn len(channels[item]) > len(channels[max])\n\t})\n}\n```\n\nThis function uses the `MaxBy` function to determine the index of the channel with the most elements (i.e., the fullest channel). The `Range` function is used to generate a sequence of integers from 0 to the length of the channels slice. The `MaxBy` function is a helper function that returns the maximum value in a sequence based on a comparison function. \n\nIf you want to generate the function from scratch, you can use the following code:\n\n```go\nfunc DispatchingStrategyMost[T any](msg T, index uint64, channels []<-chan T) int {\n\tmaxLen := 0\n\tmaxIndex := 0\n\tfor i, ch := range channels {\n\t\tif len(ch) > maxLen {\n\t\t\tmaxLen = len(ch)\n\t\t\tmaxIndex = i\n\t\t}\n\t}\n\treturn maxIndex\n}\n```\n\nThis code manually iterates over the channels and keeps track of the index of the channel with the most elements. It then returns this index.", "```go\nfunc DispatchingStrategyMost[T any](msg T, index uint64, channels []<-chan T) int {\n\tseq := Range(len(channels))\n\n\treturn MaxBy(seq, func(item int, max int) bool {\n\t\treturn len(channels[item]) > len(channels[max])\n\t})\n}\n```", "The function you requested seems to be already present in the provided code. Here it is:\n\n```go\n// DispatchingStrategyMost distributes messages in the fullest channel.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\nfunc DispatchingStrategyMost[T any](msg T, index uint64, channels []<-chan T) int {\n\tseq := Range(len(channels))\n\n\treturn MinBy(seq, func(item int, min int) bool {\n\t\treturn len(channels[item]) < len(channels[min])\n\t})\n}\n```\n\nThis function distributes messages in the fullest channel. If the channel capacity is exceeded, the next channel will be selected and so on. The function uses the `Range` and `MinBy` functions from the `lo` package to find the index of the channel with the maximum length (i.e., the fullest channel). The `DispatchingStrategyMost` function returns this index."]}, "lo-channel.go-FanOut": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/channel.go:\n```\npackage lo\n\nimport (\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/rand\"\n)\n\ntype DispatchingStrategy[T any] func(msg T, index uint64, channels []<-chan T) int\n\n// ChannelDispatcher distributes messages from input channels into N child channels.\n// Close events are propagated to children.\n// Underlying channels can have a fixed buffer capacity or be unbuffered when cap is 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc createChannels[T any](count int, channelBufferCap int) []chan T {\n\tchildren := make([]chan T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tchildren = append(children, make(chan T, channelBufferCap))\n\t}\n\n\treturn children\n}\n\nfunc channelsToReadOnly[T any](children []chan T) []<-chan T {\n\troChildren := make([]<-chan T, 0, len(children))\n\n\tfor i := range children {\n\t\troChildren = append(roChildren, children[i])\n\t}\n\n\treturn roChildren\n}\n\nfunc closeChannels[T any](children []chan T) {\n\tfor i := 0; i < len(children); i++ {\n\t\tclose(children[i])\n\t}\n}\n\nfunc channelIsNotFull[T any](ch <-chan T) bool {\n\treturn cap(ch) == 0 || len(ch) < cap(ch)\n}\n\n// DispatchingStrategyRoundRobin distributes messages in a rotating sequential manner.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyRandom distributes messages in a random manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyWeightedRandom distributes messages in a weighted manner.\n// If the channel capacity is exceeded, another random channel will be selected and so on.\nfunc DispatchingStrategyWeightedRandom[T any](weights []int) DispatchingStrategy[T] {\n\tseq := []int{}\n\n\tfor i := 0; i < len(weights); i++ {\n\t\tfor j := 0; j < weights[i]; j++ {\n\t\t\tseq = append(seq, i)\n\t\t}\n\t}\n\n\treturn func(msg T, index uint64, channels []<-chan T) int {\n\t\tfor {\n\t\t\ti := seq[rand.IntN(len(seq))]\n\t\t\tif channelIsNotFull(channels[i]) {\n\t\t\t\treturn i\n\t\t\t}\n\n\t\t\ttime.Sleep(10 * time.Microsecond) // prevent CPU from burning \ud83d\udd25\n\t\t}\n\t}\n}\n\n// DispatchingStrategyFirst distributes messages in the first non-full channel.\n// If the capacity of the first channel is exceeded, the second channel will be selected and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n// DispatchingStrategyLeast distributes messages in the emptiest channel.\nfunc DispatchingStrategyLeast[T any](msg T, index uint64, channels []<-chan T) int {\n\tseq := Range(len(channels))\n\n\treturn MinBy(seq, func(item int, min int) bool {\n\t\treturn len(channels[item]) < len(channels[min])\n\t})\n}\n\n// DispatchingStrategyMost distributes messages in the fullest channel.\n// If the channel capacity is exceeded, the next channel will be selected and so on.\n\n\n\n\n\n\n\n\n// SliceToChannel returns a read-only channels of collection elements.\nfunc SliceToChannel[T any](bufferSize int, collection []T) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\tfor i := range collection {\n\t\t\tch <- collection[i]\n\t\t}\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// ChannelToSlice returns a slice built from channels items. Blocks until channel closes.\nfunc ChannelToSlice[T any](ch <-chan T) []T {\n\tcollection := []T{}\n\n\tfor item := range ch {\n\t\tcollection = append(collection, item)\n\t}\n\n\treturn collection\n}\n\n// Generator implements the generator design pattern.\nfunc Generator[T any](bufferSize int, generator func(yield func(T))) <-chan T {\n\tch := make(chan T, bufferSize)\n\n\tgo func() {\n\t\t// WARNING: infinite loop\n\t\tgenerator(func(t T) {\n\t\t\tch <- t\n\t\t})\n\n\t\tclose(ch)\n\t}()\n\n\treturn ch\n}\n\n// Buffer creates a slice of n elements from a channel. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc Buffer[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\titem, ok := <-ch\n\t\tif !ok {\n\t\t\treturn buffer, index, time.Since(now), false\n\t\t}\n\n\t\tbuffer = append(buffer, item)\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// Batch creates a slice of n elements from a channel. Returns the slice and the slice length.\n//\n// Deprecated: Use [Buffer] instead.\nfunc Batch[T any](ch <-chan T, size int) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn Buffer(ch, size)\n}\n\n// BufferWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n// @TODO: we should probably provide an helper that reuse the same buffer.\nfunc BufferWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\texpire := time.NewTimer(timeout)\n\tdefer expire.Stop()\n\n\tbuffer := make([]T, 0, size)\n\tindex := 0\n\tnow := time.Now()\n\n\tfor ; index < size; index++ {\n\t\tselect {\n\t\tcase item, ok := <-ch:\n\t\t\tif !ok {\n\t\t\t\treturn buffer, index, time.Since(now), false\n\t\t\t}\n\n\t\t\tbuffer = append(buffer, item)\n\n\t\tcase <-expire.C:\n\t\t\treturn buffer, index, time.Since(now), true\n\t\t}\n\t}\n\n\treturn buffer, index, time.Since(now), true\n}\n\n// BatchWithTimeout creates a slice of n elements from a channel, with timeout. Returns the slice and the slice length.\n//\n// Deprecated: Use [BufferWithTimeout] instead.\nfunc BatchWithTimeout[T any](ch <-chan T, size int, timeout time.Duration) (collection []T, length int, readTime time.Duration, ok bool) {\n\treturn BufferWithTimeout(ch, size, timeout)\n}\n\n// FanIn collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\nfunc FanIn[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\tout := make(chan T, channelBufferCap)\n\tvar wg sync.WaitGroup\n\n\t// Start an output goroutine for each input channel in upstreams.\n\twg.Add(len(upstreams))\n\tfor i := range upstreams {\n\t\tgo func(index int) {\n\t\t\tfor n := range upstreams[index] {\n\t\t\t\tout <- n\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(i)\n\t}\n\n\t// Start a goroutine to close out once all the output goroutines are done.\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(out)\n\t}()\n\treturn out\n}\n\n// ChannelMerge collects messages from multiple input channels into a single buffered channel.\n// Output messages has no priority. When all upstream channels reach EOF, downstream channel closes.\n//\n// Deprecated: Use [FanIn] instead.\nfunc ChannelMerge[T any](channelBufferCap int, upstreams ...<-chan T) <-chan T {\n\treturn FanIn(channelBufferCap, upstreams...)\n}\n\n// FanOut broadcasts all the upstream messages to multiple downstream channels.\n// When upstream channel reach EOF, downstream channels close. If any downstream\n// channels is full, broadcasting is paused.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func FanOut[T any](count int, channelsBufferCap int, upstream <-chan T) []<-chan T {\n# Natural Language Annotations: // FanOut broadcasts all the upstream messages to multiple downstream channels.\n// When upstream channel reach EOF, downstream channels close. If any downstream\n// channels is full, broadcasting is paused.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc FanOut[T any](count int, channelsBufferCap int, upstream <-chan T) []<-chan T {\n\tchildren := createChannels[T](count, channelsBufferCap)\n\n\tgo func() {\n\t\tfor msg := range upstream {\n\t\t\tfor _, ch := range children {\n\t\t\t\tch <- msg\n\t\t\t}\n\t\t}\n\n\t\tcloseChannels[T](children)\n\t}()\n\n\treturn channelsToReadOnly(children)\n}\n```\nThis function generates a FanOut pattern where each message from the upstream channel is broadcasted to all downstream channels. If any of the downstream channels becomes full, the function will pause the broadcasting until the channel is ready to receive more messages. Once the upstream channel reaches EOF, all downstream channels will be closed.", "```go\nfunc FanOut[T any](count int, channelsBufferCap int, upstream <-chan T) []<-chan T {\n\tdownstreams := createChannels[T](count, channelsBufferCap)\n\tvar wg sync.WaitGroup\n\n\t// Start an output goroutine for each downstream channel.\n\twg.Add(len(downstreams))\n\tfor i := range downstreams {\n\t\tgo func(index int) {\n\t\t\tfor msg := range upstream {\n\t\t\t\tdownstreams[index] <- msg\n\t\t\t}\n\t\t\twg.Done()\n\t\t}(i)\n\t}\n\n\t// Start a goroutine to close all downstream channels once all the output goroutines are done.\n\tgo func() {\n\t\twg.Wait()\n\t\tcloseChannels(downstreams)\n\t}()\n\treturn channelsToReadOnly(downstreams)\n}\n```", "```go\nfunc FanOut[T any](count int, channelsBufferCap int, upstream <-chan T) []<-chan T {\n\tchildren := createChannels[T](count, channelsBufferCap)\n\n\tgo func() {\n\t\tfor msg := range upstream {\n\t\t\tfor _, ch := range children {\n\t\t\t\tif cap(ch) == 0 || len(ch) < cap(ch) {\n\t\t\t\t\tch <- msg\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tcloseChannels(children)\n\t}()\n\n\treturn channelsToReadOnly(children)\n}\n```"]}, "lo-slice.go-Shuffle": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/slice.go:\n```\npackage lo\n\nimport (\n\t\"sort\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// Filter iterates over elements of collection, returning an array of all elements predicate returns truthy for.\n// Play: https://go.dev/play/p/Apjg3WeSi7K\nfunc Filter[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Map manipulates a slice and transforms it to a slice of another type.\n// Play: https://go.dev/play/p/OkPcYAhBo0D\nfunc Map[T any, R any](collection []T, iteratee func(item T, index int) R) []R {\n\tresult := make([]R, len(collection))\n\n\tfor i := range collection {\n\t\tresult[i] = iteratee(collection[i], i)\n\t}\n\n\treturn result\n}\n\n// FilterMap returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\n//\n// Play: https://go.dev/play/p/-AuYXfy7opz\nfunc FilterMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FlatMap manipulates a slice and transforms and flattens it to a slice of another type.\n// The transform function can either return a slice or a `nil`, and in the `nil` case\n// no value is added to the final slice.\n// Play: https://go.dev/play/p/YSoYmQTA8-U\nfunc FlatMap[T any, R any](collection []T, iteratee func(item T, index int) []R) []R {\n\tresult := make([]R, 0, len(collection))\n\n\tfor i := range collection {\n\t\tresult = append(result, iteratee(collection[i], i)...)\n\t}\n\n\treturn result\n}\n\n// Reduce reduces collection to a value which is the accumulated result of running each element in collection\n// through accumulator, where each successive invocation is supplied the return value of the previous.\n// Play: https://go.dev/play/p/R4UHXZNaaUG\nfunc Reduce[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := range collection {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ReduceRight helper is like Reduce except that it iterates over elements of collection from right to left.\n// Play: https://go.dev/play/p/Fq3W70l7wXF\nfunc ReduceRight[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := len(collection) - 1; i >= 0; i-- {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ForEach iterates over elements of collection and invokes iteratee for each element.\n// Play: https://go.dev/play/p/oofyiUPRf8t\nfunc ForEach[T any](collection []T, iteratee func(item T, index int)) {\n\tfor i := range collection {\n\t\titeratee(collection[i], i)\n\t}\n}\n\n// Times invokes the iteratee n times, returning an array of the results of each invocation.\n// The iteratee is invoked with index as argument.\n// Play: https://go.dev/play/p/vgQj3Glr6lT\nfunc Times[T any](count int, iteratee func(index int) T) []T {\n\tresult := make([]T, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult[i] = iteratee(i)\n\t}\n\n\treturn result\n}\n\n// Uniq returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array.\n// Play: https://go.dev/play/p/DTzbeXZ6iEN\nfunc Uniq[T comparable, Slice ~[]T](collection Slice) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[T]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tif _, ok := seen[collection[i]]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[collection[i]] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// UniqBy returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\n// Play: https://go.dev/play/p/g42Z3QSb53u\nfunc UniqBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[U]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif _, ok := seen[key]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[key] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// GroupBy returns an object composed of keys generated from the results of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/XnQBd_v6brd\nfunc GroupBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) map[U]Slice {\n\tresult := map[U]Slice{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresult[key] = append(result[key], collection[i])\n\t}\n\n\treturn result\n}\n\n// Chunk returns an array of elements split into groups the length of size. If array can't be split evenly,\n// the final chunk will be the remaining elements.\n// Play: https://go.dev/play/p/EeKl0AuTehH\nfunc Chunk[T any, Slice ~[]T](collection Slice, size int) []Slice {\n\tif size <= 0 {\n\t\tpanic(\"Second parameter must be greater than 0\")\n\t}\n\n\tchunksNum := len(collection) / size\n\tif len(collection)%size != 0 {\n\t\tchunksNum += 1\n\t}\n\n\tresult := make([]Slice, 0, chunksNum)\n\n\tfor i := 0; i < chunksNum; i++ {\n\t\tlast := (i + 1) * size\n\t\tif last > len(collection) {\n\t\t\tlast = len(collection)\n\t\t}\n\t\tresult = append(result, collection[i*size:last])\n\t}\n\n\treturn result\n}\n\n// PartitionBy returns an array of elements split into groups. The order of grouped values is\n// determined by the order they occur in collection. The grouping is generated from the results\n// of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/NfQ_nGjkgXW\nfunc PartitionBy[T any, K comparable, Slice ~[]T](collection Slice, iteratee func(item T) K) []Slice {\n\tresult := []Slice{}\n\tseen := map[K]int{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresultIndex, ok := seen[key]\n\t\tif !ok {\n\t\t\tresultIndex = len(result)\n\t\t\tseen[key] = resultIndex\n\t\t\tresult = append(result, Slice{})\n\t\t}\n\n\t\tresult[resultIndex] = append(result[resultIndex], collection[i])\n\t}\n\n\treturn result\n\n\t// unordered:\n\t// groups := GroupBy[T, K](collection, iteratee)\n\t// return Values[K, []T](groups)\n}\n\n// Flatten returns an array a single level deep.\n// Play: https://go.dev/play/p/rbp9ORaMpjw\nfunc Flatten[T any, Slice ~[]T](collection []Slice) Slice {\n\ttotalLen := 0\n\tfor i := range collection {\n\t\ttotalLen += len(collection[i])\n\t}\n\n\tresult := make(Slice, 0, totalLen)\n\tfor i := range collection {\n\t\tresult = append(result, collection[i]...)\n\t}\n\n\treturn result\n}\n\n// Interleave round-robin alternating input slices and sequentially appending value at index into result\n// Play: https://go.dev/play/p/-RJkTLQEDVt\nfunc Interleave[T any, Slice ~[]T](collections ...Slice) Slice {\n\tif len(collections) == 0 {\n\t\treturn Slice{}\n\t}\n\n\tmaxSize := 0\n\ttotalSize := 0\n\tfor i := range collections {\n\t\tsize := len(collections[i])\n\t\ttotalSize += size\n\t\tif size > maxSize {\n\t\t\tmaxSize = size\n\t\t}\n\t}\n\n\tif maxSize == 0 {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, totalSize)\n\n\tresultIdx := 0\n\tfor i := 0; i < maxSize; i++ {\n\t\tfor j := range collections {\n\t\t\tif len(collections[j])-1 < i {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tresult[resultIdx] = collections[j][i]\n\t\t\tresultIdx++\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Shuffle returns an array of shuffled values. Uses the Fisher-Yates shuffle algorithm.\n// Play: https://go.dev/play/p/Qp73bnTDnc7\n\n\n\n\n\n\n\n\n// Reverse reverses array so that the first element becomes the last, the second element becomes the second to last, and so on.\n// Play: https://go.dev/play/p/fhUMLvZ7vS6\nfunc Reverse[T any, Slice ~[]T](collection Slice) Slice {\n\tlength := len(collection)\n\thalf := length / 2\n\n\tfor i := 0; i < half; i = i + 1 {\n\t\tj := length - 1 - i\n\t\tcollection[i], collection[j] = collection[j], collection[i]\n\t}\n\n\treturn collection\n}\n\n// Fill fills elements of array with `initial` value.\n// Play: https://go.dev/play/p/VwR34GzqEub\n\n\n\n\n\n\n\n\n\n\n// Repeat builds a slice with N copies of initial value.\n// Play: https://go.dev/play/p/g3uHXbmc3b6\n\n\n\n\n\n\n\n\n\n\n// RepeatBy builds a slice with values returned by N calls of callback.\n// Play: https://go.dev/play/p/ozZLCtX_hNU\nfunc RepeatBy[T any](count int, predicate func(index int) T) []T {\n\tresult := make([]T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult = append(result, predicate(i))\n\t}\n\n\treturn result\n}\n\n// KeyBy transforms a slice or an array of structs to a map based on a pivot callback.\n// Play: https://go.dev/play/p/mdaClUAT-zZ\nfunc KeyBy[K comparable, V any](collection []V, iteratee func(item V) K) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk := iteratee(collection[i])\n\t\tresult[k] = collection[i]\n\t}\n\n\treturn result\n}\n\n// Associate returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc Associate[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk, v := transform(collection[i])\n\t\tresult[k] = v\n\t}\n\n\treturn result\n}\n\n// SliceToMap returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Alias of Associate().\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc SliceToMap[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\treturn Associate(collection, transform)\n}\n\n// Drop drops n elements from the beginning of a slice or array.\n// Play: https://go.dev/play/p/JswS7vXRJP2\nfunc Drop[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn make(Slice, 0)\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\n\treturn append(result, collection[n:]...)\n}\n\n// DropRight drops n elements from the end of a slice or array.\n// Play: https://go.dev/play/p/GG0nXkSJJa3\nfunc DropRight[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\treturn append(result, collection[:len(collection)-n]...)\n}\n\n// DropWhile drops elements from the beginning of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/7gBPYw2IK16\nfunc DropWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := 0\n\tfor ; i < len(collection); i++ {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-i)\n\treturn append(result, collection[i:]...)\n}\n\n// DropRightWhile drops elements from the end of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/3-n71oEC0Hz\nfunc DropRightWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := len(collection) - 1\n\tfor ; i >= 0; i-- {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, i+1)\n\treturn append(result, collection[:i+1]...)\n}\n\n// DropByIndex drops elements from a slice or array by the index.\n// A negative index will drop elements from the end of the slice.\n// Play: https://go.dev/play/p/bPIH4npZRxS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Reject is the opposite of Filter, this method returns the elements of collection that predicate does not return truthy for.\n// Play: https://go.dev/play/p/YkLMODy1WEL\nfunc Reject[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := Slice{}\n\n\tfor i := range collection {\n\t\tif !predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// RejectMap is the opposite of FilterMap, this method returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\nfunc RejectMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); !ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FilterReject mixes Filter and Reject, this method returns two slices, one for the elements of collection that\n// predicate returns truthy for and one for the elements that predicate does not return truthy for.\nfunc FilterReject[T any, Slice ~[]T](collection Slice, predicate func(T, int) bool) (kept Slice, rejected Slice) {\n\tkept = make(Slice, 0, len(collection))\n\trejected = make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tkept = append(kept, collection[i])\n\t\t} else {\n\t\t\trejected = append(rejected, collection[i])\n\t\t}\n\t}\n\n\treturn kept, rejected\n}\n\n// Count counts the number of elements in the collection that compare equal to value.\n// Play: https://go.dev/play/p/Y3FlK54yveC\nfunc Count[T comparable](collection []T, value T) (count int) {\n\tfor i := range collection {\n\t\tif collection[i] == value {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountBy counts the number of elements in the collection for which predicate is true.\n// Play: https://go.dev/play/p/ByQbNYQQi4X\nfunc CountBy[T any](collection []T, predicate func(item T) bool) (count int) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountValues counts the number of each element in the collection.\n// Play: https://go.dev/play/p/-p-PyLT4dfy\nfunc CountValues[T comparable](collection []T) map[T]int {\n\tresult := make(map[T]int)\n\n\tfor i := range collection {\n\t\tresult[collection[i]]++\n\t}\n\n\treturn result\n}\n\n// CountValuesBy counts the number of each element return from mapper function.\n// Is equivalent to chaining lo.Map and lo.CountValues.\n// Play: https://go.dev/play/p/2U0dG1SnOmS\nfunc CountValuesBy[T any, U comparable](collection []T, mapper func(item T) U) map[U]int {\n\tresult := make(map[U]int)\n\n\tfor i := range collection {\n\t\tresult[mapper(collection[i])]++\n\t}\n\n\treturn result\n}\n\n// Subset returns a copy of a slice from `offset` up to `length` elements. Like `slice[start:start+length]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/tOQu1GhFcog\nfunc Subset[T any, Slice ~[]T](collection Slice, offset int, length uint) Slice {\n\tsize := len(collection)\n\n\tif offset < 0 {\n\t\toffset = size + offset\n\t\tif offset < 0 {\n\t\t\toffset = 0\n\t\t}\n\t}\n\n\tif offset > size {\n\t\treturn Slice{}\n\t}\n\n\tif length > uint(size)-uint(offset) {\n\t\tlength = uint(size - offset)\n\t}\n\n\treturn collection[offset : offset+int(length)]\n}\n\n// Slice returns a copy of a slice from `start` up to, but not including `end`. Like `slice[start:end]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/8XWYhfMMA1h\nfunc Slice[T any, Slice ~[]T](collection Slice, start int, end int) Slice {\n\tsize := len(collection)\n\n\tif start >= end {\n\t\treturn Slice{}\n\t}\n\n\tif start > size {\n\t\tstart = size\n\t}\n\tif start < 0 {\n\t\tstart = 0\n\t}\n\n\tif end > size {\n\t\tend = size\n\t}\n\tif end < 0 {\n\t\tend = 0\n\t}\n\n\treturn collection[start:end]\n}\n\n// Replace returns a copy of the slice with the first n non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/XfPzmf9gql6\nfunc Replace[T comparable, Slice ~[]T](collection Slice, old T, new T, n int) Slice {\n\tresult := make(Slice, len(collection))\n\tcopy(result, collection)\n\n\tfor i := range result {\n\t\tif result[i] == old && n != 0 {\n\t\t\tresult[i] = new\n\t\t\tn--\n\t\t}\n\t}\n\n\treturn result\n}\n\n// ReplaceAll returns a copy of the slice with all non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/a9xZFUHfYcV\nfunc ReplaceAll[T comparable, Slice ~[]T](collection Slice, old T, new T) Slice {\n\treturn Replace(collection, old, new, -1)\n}\n\n// Compact returns a slice of all non-zero elements.\n// Play: https://go.dev/play/p/tXiy-iK6PAc\nfunc Compact[T comparable, Slice ~[]T](collection Slice) Slice {\n\tvar zero T\n\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif collection[i] != zero {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// IsSorted checks if a slice is sorted.\n// Play: https://go.dev/play/p/mc3qR-t4mcx\nfunc IsSorted[T constraints.Ordered](collection []T) bool {\n\tfor i := 1; i < len(collection); i++ {\n\t\tif collection[i-1] > collection[i] {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// IsSortedByKey checks if a slice is sorted by iteratee.\n// Play: https://go.dev/play/p/wiG6XyBBu49\nfunc IsSortedByKey[T any, K constraints.Ordered](collection []T, iteratee func(item T) K) bool {\n\tsize := len(collection)\n\n\tfor i := 0; i < size-1; i++ {\n\t\tif iteratee(collection[i]) > iteratee(collection[i+1]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Splice inserts multiple elements at index i. A negative index counts back\n// from the end of the slice. The helper is protected against overflow errors.\n// Play: https://go.dev/play/p/G5_GhkeSUBA\nfunc Splice[T any, Slice ~[]T](collection Slice, i int, elements ...T) Slice {\n\tsizeCollection := len(collection)\n\tsizeElements := len(elements)\n\toutput := make(Slice, 0, sizeCollection+sizeElements) // preallocate memory for the output slice\n\n\tif sizeElements == 0 {\n\t\treturn append(output, collection...) // simple copy\n\t} else if i > sizeCollection {\n\t\t// positive overflow\n\t\treturn append(append(output, collection...), elements...)\n\t} else if i < -sizeCollection {\n\t\t// negative overflow\n\t\treturn append(append(output, elements...), collection...)\n\t} else if i < 0 {\n\t\t// backward\n\t\ti = sizeCollection + i\n\t}\n\n\treturn append(append(append(output, collection[:i]...), elements...), collection[i:]...)\n}\n\n```\n# Function Signature: func Shuffle[T any, Slice ~[]T](collection Slice) Slice {\n# Natural Language Annotations: // Shuffle returns an array of shuffled values. Uses the Fisher-Yates shuffle algorithm.\n// Play: https://go.dev/play/p/Qp73bnTDnc7\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc Shuffle[T any, Slice ~[]T](collection Slice) Slice {\n\trand.Seed(time.Now().UnixNano())\n\trand.Shuffle(len(collection), func(i, j int) {\n\t\tcollection[i], collection[j] = collection[j], collection[i]\n\t})\n\treturn collection\n}\n```", "```go\nfunc Shuffle[T any, Slice ~[]T](collection Slice) Slice {\n\trand.Shuffle(len(collection), func(i, j int) {\n\t\tcollection[i], collection[j] = collection[j], collection[i]\n\t})\n\t\n\treturn collection\n}\n```", "```go\n// Shuffle returns an array of shuffled values. Uses the Fisher-Yates shuffle algorithm.\n// Play: https://go.dev/play/p/Qp73bnTDnc7\nfunc Shuffle[T any, Slice ~[]T](collection Slice) Slice {\n\tlength := len(collection)\n\tresult := make(Slice, length)\n\tcopy(result, collection)\n\n\tfor i := range result {\n\t\tj := rand.Intn(i + 1)\n\t\tresult[i], result[j] = result[j], result[i]\n\t}\n\n\treturn result\n}\n```\nThis function shuffles the elements of a slice. It does this using the Fisher-Yates shuffle algorithm, which works by iterating through the slice from the first to the last element, and for each element, randomly choosing another element and swapping them. This process is repeated until every possible pair of elements has been considered, effectively shuffling the elements of the slice."]}, "lo-slice.go-Fill": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/slice.go:\n```\npackage lo\n\nimport (\n\t\"sort\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// Filter iterates over elements of collection, returning an array of all elements predicate returns truthy for.\n// Play: https://go.dev/play/p/Apjg3WeSi7K\nfunc Filter[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Map manipulates a slice and transforms it to a slice of another type.\n// Play: https://go.dev/play/p/OkPcYAhBo0D\nfunc Map[T any, R any](collection []T, iteratee func(item T, index int) R) []R {\n\tresult := make([]R, len(collection))\n\n\tfor i := range collection {\n\t\tresult[i] = iteratee(collection[i], i)\n\t}\n\n\treturn result\n}\n\n// FilterMap returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\n//\n// Play: https://go.dev/play/p/-AuYXfy7opz\nfunc FilterMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FlatMap manipulates a slice and transforms and flattens it to a slice of another type.\n// The transform function can either return a slice or a `nil`, and in the `nil` case\n// no value is added to the final slice.\n// Play: https://go.dev/play/p/YSoYmQTA8-U\nfunc FlatMap[T any, R any](collection []T, iteratee func(item T, index int) []R) []R {\n\tresult := make([]R, 0, len(collection))\n\n\tfor i := range collection {\n\t\tresult = append(result, iteratee(collection[i], i)...)\n\t}\n\n\treturn result\n}\n\n// Reduce reduces collection to a value which is the accumulated result of running each element in collection\n// through accumulator, where each successive invocation is supplied the return value of the previous.\n// Play: https://go.dev/play/p/R4UHXZNaaUG\nfunc Reduce[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := range collection {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ReduceRight helper is like Reduce except that it iterates over elements of collection from right to left.\n// Play: https://go.dev/play/p/Fq3W70l7wXF\nfunc ReduceRight[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := len(collection) - 1; i >= 0; i-- {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ForEach iterates over elements of collection and invokes iteratee for each element.\n// Play: https://go.dev/play/p/oofyiUPRf8t\nfunc ForEach[T any](collection []T, iteratee func(item T, index int)) {\n\tfor i := range collection {\n\t\titeratee(collection[i], i)\n\t}\n}\n\n// Times invokes the iteratee n times, returning an array of the results of each invocation.\n// The iteratee is invoked with index as argument.\n// Play: https://go.dev/play/p/vgQj3Glr6lT\nfunc Times[T any](count int, iteratee func(index int) T) []T {\n\tresult := make([]T, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult[i] = iteratee(i)\n\t}\n\n\treturn result\n}\n\n// Uniq returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array.\n// Play: https://go.dev/play/p/DTzbeXZ6iEN\nfunc Uniq[T comparable, Slice ~[]T](collection Slice) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[T]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tif _, ok := seen[collection[i]]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[collection[i]] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// UniqBy returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\n// Play: https://go.dev/play/p/g42Z3QSb53u\nfunc UniqBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[U]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif _, ok := seen[key]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[key] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// GroupBy returns an object composed of keys generated from the results of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/XnQBd_v6brd\nfunc GroupBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) map[U]Slice {\n\tresult := map[U]Slice{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresult[key] = append(result[key], collection[i])\n\t}\n\n\treturn result\n}\n\n// Chunk returns an array of elements split into groups the length of size. If array can't be split evenly,\n// the final chunk will be the remaining elements.\n// Play: https://go.dev/play/p/EeKl0AuTehH\nfunc Chunk[T any, Slice ~[]T](collection Slice, size int) []Slice {\n\tif size <= 0 {\n\t\tpanic(\"Second parameter must be greater than 0\")\n\t}\n\n\tchunksNum := len(collection) / size\n\tif len(collection)%size != 0 {\n\t\tchunksNum += 1\n\t}\n\n\tresult := make([]Slice, 0, chunksNum)\n\n\tfor i := 0; i < chunksNum; i++ {\n\t\tlast := (i + 1) * size\n\t\tif last > len(collection) {\n\t\t\tlast = len(collection)\n\t\t}\n\t\tresult = append(result, collection[i*size:last])\n\t}\n\n\treturn result\n}\n\n// PartitionBy returns an array of elements split into groups. The order of grouped values is\n// determined by the order they occur in collection. The grouping is generated from the results\n// of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/NfQ_nGjkgXW\nfunc PartitionBy[T any, K comparable, Slice ~[]T](collection Slice, iteratee func(item T) K) []Slice {\n\tresult := []Slice{}\n\tseen := map[K]int{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresultIndex, ok := seen[key]\n\t\tif !ok {\n\t\t\tresultIndex = len(result)\n\t\t\tseen[key] = resultIndex\n\t\t\tresult = append(result, Slice{})\n\t\t}\n\n\t\tresult[resultIndex] = append(result[resultIndex], collection[i])\n\t}\n\n\treturn result\n\n\t// unordered:\n\t// groups := GroupBy[T, K](collection, iteratee)\n\t// return Values[K, []T](groups)\n}\n\n// Flatten returns an array a single level deep.\n// Play: https://go.dev/play/p/rbp9ORaMpjw\nfunc Flatten[T any, Slice ~[]T](collection []Slice) Slice {\n\ttotalLen := 0\n\tfor i := range collection {\n\t\ttotalLen += len(collection[i])\n\t}\n\n\tresult := make(Slice, 0, totalLen)\n\tfor i := range collection {\n\t\tresult = append(result, collection[i]...)\n\t}\n\n\treturn result\n}\n\n// Interleave round-robin alternating input slices and sequentially appending value at index into result\n// Play: https://go.dev/play/p/-RJkTLQEDVt\nfunc Interleave[T any, Slice ~[]T](collections ...Slice) Slice {\n\tif len(collections) == 0 {\n\t\treturn Slice{}\n\t}\n\n\tmaxSize := 0\n\ttotalSize := 0\n\tfor i := range collections {\n\t\tsize := len(collections[i])\n\t\ttotalSize += size\n\t\tif size > maxSize {\n\t\t\tmaxSize = size\n\t\t}\n\t}\n\n\tif maxSize == 0 {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, totalSize)\n\n\tresultIdx := 0\n\tfor i := 0; i < maxSize; i++ {\n\t\tfor j := range collections {\n\t\t\tif len(collections[j])-1 < i {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tresult[resultIdx] = collections[j][i]\n\t\t\tresultIdx++\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Shuffle returns an array of shuffled values. Uses the Fisher-Yates shuffle algorithm.\n// Play: https://go.dev/play/p/Qp73bnTDnc7\n\n\n\n\n\n\n\n\n// Reverse reverses array so that the first element becomes the last, the second element becomes the second to last, and so on.\n// Play: https://go.dev/play/p/fhUMLvZ7vS6\nfunc Reverse[T any, Slice ~[]T](collection Slice) Slice {\n\tlength := len(collection)\n\thalf := length / 2\n\n\tfor i := 0; i < half; i = i + 1 {\n\t\tj := length - 1 - i\n\t\tcollection[i], collection[j] = collection[j], collection[i]\n\t}\n\n\treturn collection\n}\n\n// Fill fills elements of array with `initial` value.\n// Play: https://go.dev/play/p/VwR34GzqEub\n\n\n\n\n\n\n\n\n\n\n// Repeat builds a slice with N copies of initial value.\n// Play: https://go.dev/play/p/g3uHXbmc3b6\n\n\n\n\n\n\n\n\n\n\n// RepeatBy builds a slice with values returned by N calls of callback.\n// Play: https://go.dev/play/p/ozZLCtX_hNU\nfunc RepeatBy[T any](count int, predicate func(index int) T) []T {\n\tresult := make([]T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult = append(result, predicate(i))\n\t}\n\n\treturn result\n}\n\n// KeyBy transforms a slice or an array of structs to a map based on a pivot callback.\n// Play: https://go.dev/play/p/mdaClUAT-zZ\nfunc KeyBy[K comparable, V any](collection []V, iteratee func(item V) K) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk := iteratee(collection[i])\n\t\tresult[k] = collection[i]\n\t}\n\n\treturn result\n}\n\n// Associate returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc Associate[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk, v := transform(collection[i])\n\t\tresult[k] = v\n\t}\n\n\treturn result\n}\n\n// SliceToMap returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Alias of Associate().\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc SliceToMap[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\treturn Associate(collection, transform)\n}\n\n// Drop drops n elements from the beginning of a slice or array.\n// Play: https://go.dev/play/p/JswS7vXRJP2\nfunc Drop[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn make(Slice, 0)\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\n\treturn append(result, collection[n:]...)\n}\n\n// DropRight drops n elements from the end of a slice or array.\n// Play: https://go.dev/play/p/GG0nXkSJJa3\nfunc DropRight[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\treturn append(result, collection[:len(collection)-n]...)\n}\n\n// DropWhile drops elements from the beginning of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/7gBPYw2IK16\nfunc DropWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := 0\n\tfor ; i < len(collection); i++ {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-i)\n\treturn append(result, collection[i:]...)\n}\n\n// DropRightWhile drops elements from the end of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/3-n71oEC0Hz\nfunc DropRightWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := len(collection) - 1\n\tfor ; i >= 0; i-- {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, i+1)\n\treturn append(result, collection[:i+1]...)\n}\n\n// DropByIndex drops elements from a slice or array by the index.\n// A negative index will drop elements from the end of the slice.\n// Play: https://go.dev/play/p/bPIH4npZRxS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Reject is the opposite of Filter, this method returns the elements of collection that predicate does not return truthy for.\n// Play: https://go.dev/play/p/YkLMODy1WEL\nfunc Reject[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := Slice{}\n\n\tfor i := range collection {\n\t\tif !predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// RejectMap is the opposite of FilterMap, this method returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\nfunc RejectMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); !ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FilterReject mixes Filter and Reject, this method returns two slices, one for the elements of collection that\n// predicate returns truthy for and one for the elements that predicate does not return truthy for.\nfunc FilterReject[T any, Slice ~[]T](collection Slice, predicate func(T, int) bool) (kept Slice, rejected Slice) {\n\tkept = make(Slice, 0, len(collection))\n\trejected = make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tkept = append(kept, collection[i])\n\t\t} else {\n\t\t\trejected = append(rejected, collection[i])\n\t\t}\n\t}\n\n\treturn kept, rejected\n}\n\n// Count counts the number of elements in the collection that compare equal to value.\n// Play: https://go.dev/play/p/Y3FlK54yveC\nfunc Count[T comparable](collection []T, value T) (count int) {\n\tfor i := range collection {\n\t\tif collection[i] == value {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountBy counts the number of elements in the collection for which predicate is true.\n// Play: https://go.dev/play/p/ByQbNYQQi4X\nfunc CountBy[T any](collection []T, predicate func(item T) bool) (count int) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountValues counts the number of each element in the collection.\n// Play: https://go.dev/play/p/-p-PyLT4dfy\nfunc CountValues[T comparable](collection []T) map[T]int {\n\tresult := make(map[T]int)\n\n\tfor i := range collection {\n\t\tresult[collection[i]]++\n\t}\n\n\treturn result\n}\n\n// CountValuesBy counts the number of each element return from mapper function.\n// Is equivalent to chaining lo.Map and lo.CountValues.\n// Play: https://go.dev/play/p/2U0dG1SnOmS\nfunc CountValuesBy[T any, U comparable](collection []T, mapper func(item T) U) map[U]int {\n\tresult := make(map[U]int)\n\n\tfor i := range collection {\n\t\tresult[mapper(collection[i])]++\n\t}\n\n\treturn result\n}\n\n// Subset returns a copy of a slice from `offset` up to `length` elements. Like `slice[start:start+length]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/tOQu1GhFcog\nfunc Subset[T any, Slice ~[]T](collection Slice, offset int, length uint) Slice {\n\tsize := len(collection)\n\n\tif offset < 0 {\n\t\toffset = size + offset\n\t\tif offset < 0 {\n\t\t\toffset = 0\n\t\t}\n\t}\n\n\tif offset > size {\n\t\treturn Slice{}\n\t}\n\n\tif length > uint(size)-uint(offset) {\n\t\tlength = uint(size - offset)\n\t}\n\n\treturn collection[offset : offset+int(length)]\n}\n\n// Slice returns a copy of a slice from `start` up to, but not including `end`. Like `slice[start:end]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/8XWYhfMMA1h\nfunc Slice[T any, Slice ~[]T](collection Slice, start int, end int) Slice {\n\tsize := len(collection)\n\n\tif start >= end {\n\t\treturn Slice{}\n\t}\n\n\tif start > size {\n\t\tstart = size\n\t}\n\tif start < 0 {\n\t\tstart = 0\n\t}\n\n\tif end > size {\n\t\tend = size\n\t}\n\tif end < 0 {\n\t\tend = 0\n\t}\n\n\treturn collection[start:end]\n}\n\n// Replace returns a copy of the slice with the first n non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/XfPzmf9gql6\nfunc Replace[T comparable, Slice ~[]T](collection Slice, old T, new T, n int) Slice {\n\tresult := make(Slice, len(collection))\n\tcopy(result, collection)\n\n\tfor i := range result {\n\t\tif result[i] == old && n != 0 {\n\t\t\tresult[i] = new\n\t\t\tn--\n\t\t}\n\t}\n\n\treturn result\n}\n\n// ReplaceAll returns a copy of the slice with all non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/a9xZFUHfYcV\nfunc ReplaceAll[T comparable, Slice ~[]T](collection Slice, old T, new T) Slice {\n\treturn Replace(collection, old, new, -1)\n}\n\n// Compact returns a slice of all non-zero elements.\n// Play: https://go.dev/play/p/tXiy-iK6PAc\nfunc Compact[T comparable, Slice ~[]T](collection Slice) Slice {\n\tvar zero T\n\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif collection[i] != zero {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// IsSorted checks if a slice is sorted.\n// Play: https://go.dev/play/p/mc3qR-t4mcx\nfunc IsSorted[T constraints.Ordered](collection []T) bool {\n\tfor i := 1; i < len(collection); i++ {\n\t\tif collection[i-1] > collection[i] {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// IsSortedByKey checks if a slice is sorted by iteratee.\n// Play: https://go.dev/play/p/wiG6XyBBu49\nfunc IsSortedByKey[T any, K constraints.Ordered](collection []T, iteratee func(item T) K) bool {\n\tsize := len(collection)\n\n\tfor i := 0; i < size-1; i++ {\n\t\tif iteratee(collection[i]) > iteratee(collection[i+1]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Splice inserts multiple elements at index i. A negative index counts back\n// from the end of the slice. The helper is protected against overflow errors.\n// Play: https://go.dev/play/p/G5_GhkeSUBA\nfunc Splice[T any, Slice ~[]T](collection Slice, i int, elements ...T) Slice {\n\tsizeCollection := len(collection)\n\tsizeElements := len(elements)\n\toutput := make(Slice, 0, sizeCollection+sizeElements) // preallocate memory for the output slice\n\n\tif sizeElements == 0 {\n\t\treturn append(output, collection...) // simple copy\n\t} else if i > sizeCollection {\n\t\t// positive overflow\n\t\treturn append(append(output, collection...), elements...)\n\t} else if i < -sizeCollection {\n\t\t// negative overflow\n\t\treturn append(append(output, elements...), collection...)\n\t} else if i < 0 {\n\t\t// backward\n\t\ti = sizeCollection + i\n\t}\n\n\treturn append(append(append(output, collection[:i]...), elements...), collection[i:]...)\n}\n\n```\n# Function Signature: func Fill[T Clonable[T]](collection []T, initial T) []T {\n# Natural Language Annotations: // Fill fills elements of array with `initial` value.\n// Play: https://go.dev/play/p/VwR34GzqEub\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage main\n\nimport (\n\t\"github.com/samber/lo\"\n)\n\n// Clonable is a constraint that matches any type that has a Clone method.\ntype Clonable[T any] interface {\n\tClone() T\n}\n\n// Fill fills elements of array with `initial` value.\n// Play: https://go.dev/play/p/VwR34GzqEub\nfunc Fill[T Clonable[T]](collection []T, initial T) []T {\n\tresult := make([]T, len(collection))\n\tfor i := range collection {\n\t\tresult[i] = initial.Clone()\n\t}\n\treturn result\n}\n```\nNote: This is a sample implementation based on the provided information. The actual implementation might vary based on the design and requirements of your project.", "```go\nfunc Fill[T Clonable[T]](collection []T, initial T) []T {\n\tfor i := range collection {\n\t\tcollection[i] = initial.Clone()\n\t}\n\n\treturn collection\n}\n```\nPlease note that this function assumes that the type `T` has a method `Clone() T`. If this method is not available, this function will not work.", "```go\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Fill fills elements of array with `initial` value.\n// Play: https://go.dev/play/p/VwR34GzqEub\nfunc Fill[T Clonable[T]](collection []T, initial T) []T {\n\tresult := make([]T, len(collection))\n\n\tfor i := range collection {\n\t\tresult[i] = initial.Clone()\n\t}\n\n\treturn result\n}\n```"]}, "lo-slice.go-Repeat": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/slice.go:\n```\npackage lo\n\nimport (\n\t\"sort\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// Filter iterates over elements of collection, returning an array of all elements predicate returns truthy for.\n// Play: https://go.dev/play/p/Apjg3WeSi7K\nfunc Filter[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Map manipulates a slice and transforms it to a slice of another type.\n// Play: https://go.dev/play/p/OkPcYAhBo0D\nfunc Map[T any, R any](collection []T, iteratee func(item T, index int) R) []R {\n\tresult := make([]R, len(collection))\n\n\tfor i := range collection {\n\t\tresult[i] = iteratee(collection[i], i)\n\t}\n\n\treturn result\n}\n\n// FilterMap returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\n//\n// Play: https://go.dev/play/p/-AuYXfy7opz\nfunc FilterMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FlatMap manipulates a slice and transforms and flattens it to a slice of another type.\n// The transform function can either return a slice or a `nil`, and in the `nil` case\n// no value is added to the final slice.\n// Play: https://go.dev/play/p/YSoYmQTA8-U\nfunc FlatMap[T any, R any](collection []T, iteratee func(item T, index int) []R) []R {\n\tresult := make([]R, 0, len(collection))\n\n\tfor i := range collection {\n\t\tresult = append(result, iteratee(collection[i], i)...)\n\t}\n\n\treturn result\n}\n\n// Reduce reduces collection to a value which is the accumulated result of running each element in collection\n// through accumulator, where each successive invocation is supplied the return value of the previous.\n// Play: https://go.dev/play/p/R4UHXZNaaUG\nfunc Reduce[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := range collection {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ReduceRight helper is like Reduce except that it iterates over elements of collection from right to left.\n// Play: https://go.dev/play/p/Fq3W70l7wXF\nfunc ReduceRight[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := len(collection) - 1; i >= 0; i-- {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ForEach iterates over elements of collection and invokes iteratee for each element.\n// Play: https://go.dev/play/p/oofyiUPRf8t\nfunc ForEach[T any](collection []T, iteratee func(item T, index int)) {\n\tfor i := range collection {\n\t\titeratee(collection[i], i)\n\t}\n}\n\n// Times invokes the iteratee n times, returning an array of the results of each invocation.\n// The iteratee is invoked with index as argument.\n// Play: https://go.dev/play/p/vgQj3Glr6lT\nfunc Times[T any](count int, iteratee func(index int) T) []T {\n\tresult := make([]T, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult[i] = iteratee(i)\n\t}\n\n\treturn result\n}\n\n// Uniq returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array.\n// Play: https://go.dev/play/p/DTzbeXZ6iEN\nfunc Uniq[T comparable, Slice ~[]T](collection Slice) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[T]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tif _, ok := seen[collection[i]]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[collection[i]] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// UniqBy returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\n// Play: https://go.dev/play/p/g42Z3QSb53u\nfunc UniqBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[U]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif _, ok := seen[key]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[key] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// GroupBy returns an object composed of keys generated from the results of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/XnQBd_v6brd\nfunc GroupBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) map[U]Slice {\n\tresult := map[U]Slice{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresult[key] = append(result[key], collection[i])\n\t}\n\n\treturn result\n}\n\n// Chunk returns an array of elements split into groups the length of size. If array can't be split evenly,\n// the final chunk will be the remaining elements.\n// Play: https://go.dev/play/p/EeKl0AuTehH\nfunc Chunk[T any, Slice ~[]T](collection Slice, size int) []Slice {\n\tif size <= 0 {\n\t\tpanic(\"Second parameter must be greater than 0\")\n\t}\n\n\tchunksNum := len(collection) / size\n\tif len(collection)%size != 0 {\n\t\tchunksNum += 1\n\t}\n\n\tresult := make([]Slice, 0, chunksNum)\n\n\tfor i := 0; i < chunksNum; i++ {\n\t\tlast := (i + 1) * size\n\t\tif last > len(collection) {\n\t\t\tlast = len(collection)\n\t\t}\n\t\tresult = append(result, collection[i*size:last])\n\t}\n\n\treturn result\n}\n\n// PartitionBy returns an array of elements split into groups. The order of grouped values is\n// determined by the order they occur in collection. The grouping is generated from the results\n// of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/NfQ_nGjkgXW\nfunc PartitionBy[T any, K comparable, Slice ~[]T](collection Slice, iteratee func(item T) K) []Slice {\n\tresult := []Slice{}\n\tseen := map[K]int{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresultIndex, ok := seen[key]\n\t\tif !ok {\n\t\t\tresultIndex = len(result)\n\t\t\tseen[key] = resultIndex\n\t\t\tresult = append(result, Slice{})\n\t\t}\n\n\t\tresult[resultIndex] = append(result[resultIndex], collection[i])\n\t}\n\n\treturn result\n\n\t// unordered:\n\t// groups := GroupBy[T, K](collection, iteratee)\n\t// return Values[K, []T](groups)\n}\n\n// Flatten returns an array a single level deep.\n// Play: https://go.dev/play/p/rbp9ORaMpjw\nfunc Flatten[T any, Slice ~[]T](collection []Slice) Slice {\n\ttotalLen := 0\n\tfor i := range collection {\n\t\ttotalLen += len(collection[i])\n\t}\n\n\tresult := make(Slice, 0, totalLen)\n\tfor i := range collection {\n\t\tresult = append(result, collection[i]...)\n\t}\n\n\treturn result\n}\n\n// Interleave round-robin alternating input slices and sequentially appending value at index into result\n// Play: https://go.dev/play/p/-RJkTLQEDVt\nfunc Interleave[T any, Slice ~[]T](collections ...Slice) Slice {\n\tif len(collections) == 0 {\n\t\treturn Slice{}\n\t}\n\n\tmaxSize := 0\n\ttotalSize := 0\n\tfor i := range collections {\n\t\tsize := len(collections[i])\n\t\ttotalSize += size\n\t\tif size > maxSize {\n\t\t\tmaxSize = size\n\t\t}\n\t}\n\n\tif maxSize == 0 {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, totalSize)\n\n\tresultIdx := 0\n\tfor i := 0; i < maxSize; i++ {\n\t\tfor j := range collections {\n\t\t\tif len(collections[j])-1 < i {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tresult[resultIdx] = collections[j][i]\n\t\t\tresultIdx++\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Shuffle returns an array of shuffled values. Uses the Fisher-Yates shuffle algorithm.\n// Play: https://go.dev/play/p/Qp73bnTDnc7\n\n\n\n\n\n\n\n\n// Reverse reverses array so that the first element becomes the last, the second element becomes the second to last, and so on.\n// Play: https://go.dev/play/p/fhUMLvZ7vS6\nfunc Reverse[T any, Slice ~[]T](collection Slice) Slice {\n\tlength := len(collection)\n\thalf := length / 2\n\n\tfor i := 0; i < half; i = i + 1 {\n\t\tj := length - 1 - i\n\t\tcollection[i], collection[j] = collection[j], collection[i]\n\t}\n\n\treturn collection\n}\n\n// Fill fills elements of array with `initial` value.\n// Play: https://go.dev/play/p/VwR34GzqEub\n\n\n\n\n\n\n\n\n\n\n// Repeat builds a slice with N copies of initial value.\n// Play: https://go.dev/play/p/g3uHXbmc3b6\n\n\n\n\n\n\n\n\n\n\n// RepeatBy builds a slice with values returned by N calls of callback.\n// Play: https://go.dev/play/p/ozZLCtX_hNU\nfunc RepeatBy[T any](count int, predicate func(index int) T) []T {\n\tresult := make([]T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult = append(result, predicate(i))\n\t}\n\n\treturn result\n}\n\n// KeyBy transforms a slice or an array of structs to a map based on a pivot callback.\n// Play: https://go.dev/play/p/mdaClUAT-zZ\nfunc KeyBy[K comparable, V any](collection []V, iteratee func(item V) K) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk := iteratee(collection[i])\n\t\tresult[k] = collection[i]\n\t}\n\n\treturn result\n}\n\n// Associate returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc Associate[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk, v := transform(collection[i])\n\t\tresult[k] = v\n\t}\n\n\treturn result\n}\n\n// SliceToMap returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Alias of Associate().\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc SliceToMap[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\treturn Associate(collection, transform)\n}\n\n// Drop drops n elements from the beginning of a slice or array.\n// Play: https://go.dev/play/p/JswS7vXRJP2\nfunc Drop[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn make(Slice, 0)\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\n\treturn append(result, collection[n:]...)\n}\n\n// DropRight drops n elements from the end of a slice or array.\n// Play: https://go.dev/play/p/GG0nXkSJJa3\nfunc DropRight[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\treturn append(result, collection[:len(collection)-n]...)\n}\n\n// DropWhile drops elements from the beginning of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/7gBPYw2IK16\nfunc DropWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := 0\n\tfor ; i < len(collection); i++ {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-i)\n\treturn append(result, collection[i:]...)\n}\n\n// DropRightWhile drops elements from the end of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/3-n71oEC0Hz\nfunc DropRightWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := len(collection) - 1\n\tfor ; i >= 0; i-- {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, i+1)\n\treturn append(result, collection[:i+1]...)\n}\n\n// DropByIndex drops elements from a slice or array by the index.\n// A negative index will drop elements from the end of the slice.\n// Play: https://go.dev/play/p/bPIH4npZRxS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Reject is the opposite of Filter, this method returns the elements of collection that predicate does not return truthy for.\n// Play: https://go.dev/play/p/YkLMODy1WEL\nfunc Reject[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := Slice{}\n\n\tfor i := range collection {\n\t\tif !predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// RejectMap is the opposite of FilterMap, this method returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\nfunc RejectMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); !ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FilterReject mixes Filter and Reject, this method returns two slices, one for the elements of collection that\n// predicate returns truthy for and one for the elements that predicate does not return truthy for.\nfunc FilterReject[T any, Slice ~[]T](collection Slice, predicate func(T, int) bool) (kept Slice, rejected Slice) {\n\tkept = make(Slice, 0, len(collection))\n\trejected = make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tkept = append(kept, collection[i])\n\t\t} else {\n\t\t\trejected = append(rejected, collection[i])\n\t\t}\n\t}\n\n\treturn kept, rejected\n}\n\n// Count counts the number of elements in the collection that compare equal to value.\n// Play: https://go.dev/play/p/Y3FlK54yveC\nfunc Count[T comparable](collection []T, value T) (count int) {\n\tfor i := range collection {\n\t\tif collection[i] == value {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountBy counts the number of elements in the collection for which predicate is true.\n// Play: https://go.dev/play/p/ByQbNYQQi4X\nfunc CountBy[T any](collection []T, predicate func(item T) bool) (count int) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountValues counts the number of each element in the collection.\n// Play: https://go.dev/play/p/-p-PyLT4dfy\nfunc CountValues[T comparable](collection []T) map[T]int {\n\tresult := make(map[T]int)\n\n\tfor i := range collection {\n\t\tresult[collection[i]]++\n\t}\n\n\treturn result\n}\n\n// CountValuesBy counts the number of each element return from mapper function.\n// Is equivalent to chaining lo.Map and lo.CountValues.\n// Play: https://go.dev/play/p/2U0dG1SnOmS\nfunc CountValuesBy[T any, U comparable](collection []T, mapper func(item T) U) map[U]int {\n\tresult := make(map[U]int)\n\n\tfor i := range collection {\n\t\tresult[mapper(collection[i])]++\n\t}\n\n\treturn result\n}\n\n// Subset returns a copy of a slice from `offset` up to `length` elements. Like `slice[start:start+length]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/tOQu1GhFcog\nfunc Subset[T any, Slice ~[]T](collection Slice, offset int, length uint) Slice {\n\tsize := len(collection)\n\n\tif offset < 0 {\n\t\toffset = size + offset\n\t\tif offset < 0 {\n\t\t\toffset = 0\n\t\t}\n\t}\n\n\tif offset > size {\n\t\treturn Slice{}\n\t}\n\n\tif length > uint(size)-uint(offset) {\n\t\tlength = uint(size - offset)\n\t}\n\n\treturn collection[offset : offset+int(length)]\n}\n\n// Slice returns a copy of a slice from `start` up to, but not including `end`. Like `slice[start:end]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/8XWYhfMMA1h\nfunc Slice[T any, Slice ~[]T](collection Slice, start int, end int) Slice {\n\tsize := len(collection)\n\n\tif start >= end {\n\t\treturn Slice{}\n\t}\n\n\tif start > size {\n\t\tstart = size\n\t}\n\tif start < 0 {\n\t\tstart = 0\n\t}\n\n\tif end > size {\n\t\tend = size\n\t}\n\tif end < 0 {\n\t\tend = 0\n\t}\n\n\treturn collection[start:end]\n}\n\n// Replace returns a copy of the slice with the first n non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/XfPzmf9gql6\nfunc Replace[T comparable, Slice ~[]T](collection Slice, old T, new T, n int) Slice {\n\tresult := make(Slice, len(collection))\n\tcopy(result, collection)\n\n\tfor i := range result {\n\t\tif result[i] == old && n != 0 {\n\t\t\tresult[i] = new\n\t\t\tn--\n\t\t}\n\t}\n\n\treturn result\n}\n\n// ReplaceAll returns a copy of the slice with all non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/a9xZFUHfYcV\nfunc ReplaceAll[T comparable, Slice ~[]T](collection Slice, old T, new T) Slice {\n\treturn Replace(collection, old, new, -1)\n}\n\n// Compact returns a slice of all non-zero elements.\n// Play: https://go.dev/play/p/tXiy-iK6PAc\nfunc Compact[T comparable, Slice ~[]T](collection Slice) Slice {\n\tvar zero T\n\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif collection[i] != zero {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// IsSorted checks if a slice is sorted.\n// Play: https://go.dev/play/p/mc3qR-t4mcx\nfunc IsSorted[T constraints.Ordered](collection []T) bool {\n\tfor i := 1; i < len(collection); i++ {\n\t\tif collection[i-1] > collection[i] {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// IsSortedByKey checks if a slice is sorted by iteratee.\n// Play: https://go.dev/play/p/wiG6XyBBu49\nfunc IsSortedByKey[T any, K constraints.Ordered](collection []T, iteratee func(item T) K) bool {\n\tsize := len(collection)\n\n\tfor i := 0; i < size-1; i++ {\n\t\tif iteratee(collection[i]) > iteratee(collection[i+1]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Splice inserts multiple elements at index i. A negative index counts back\n// from the end of the slice. The helper is protected against overflow errors.\n// Play: https://go.dev/play/p/G5_GhkeSUBA\nfunc Splice[T any, Slice ~[]T](collection Slice, i int, elements ...T) Slice {\n\tsizeCollection := len(collection)\n\tsizeElements := len(elements)\n\toutput := make(Slice, 0, sizeCollection+sizeElements) // preallocate memory for the output slice\n\n\tif sizeElements == 0 {\n\t\treturn append(output, collection...) // simple copy\n\t} else if i > sizeCollection {\n\t\t// positive overflow\n\t\treturn append(append(output, collection...), elements...)\n\t} else if i < -sizeCollection {\n\t\t// negative overflow\n\t\treturn append(append(output, elements...), collection...)\n\t} else if i < 0 {\n\t\t// backward\n\t\ti = sizeCollection + i\n\t}\n\n\treturn append(append(append(output, collection[:i]...), elements...), collection[i:]...)\n}\n\n```\n# Function Signature: func Repeat[T Clonable[T]](count int, initial T) []T {\n# Natural Language Annotations: // Repeat builds a slice with N copies of initial value.\n// Play: https://go.dev/play/p/g3uHXbmc3b6\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Repeat builds a slice with N copies of initial value.\n// Play: https://go.dev/play/p/g3uHXbmc3b6\nfunc Repeat[T constraints.Clonable[T]](count int, initial T) []T {\n\tresult := make([]T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult = append(result, initial.Clone())\n\t}\n\n\treturn result\n}\n```", "```go\n// Repeat builds a slice with N copies of initial value.\n// Play: https://go.dev/play/p/g3uHXbmc3b6\nfunc Repeat[T Clonable[T]](count int, initial T) []T {\n    result := make([]T, count)\n\n    for i := range result {\n        result[i] = initial.Clone()\n    }\n\n    return result\n}\n```\n\nPlease note that this code assumes that there is a `Clone` method in the `Clonable` interface, which is used to clone the initial value. This is to ensure that the function works correctly for types that are not natively cloneable in Go (e.g., slices, maps). If your type is natively cloneable, you don't need to implement this interface.", "```go\nfunc Repeat[T Clonable[T]](count int, initial T) []T {\n\tresult := make([]T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult = append(result, initial.Clone())\n\t}\n\n\treturn result\n}\n```"]}, "lo-slice.go-DropByIndex": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/slice.go:\n```\npackage lo\n\nimport (\n\t\"sort\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// Filter iterates over elements of collection, returning an array of all elements predicate returns truthy for.\n// Play: https://go.dev/play/p/Apjg3WeSi7K\nfunc Filter[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Map manipulates a slice and transforms it to a slice of another type.\n// Play: https://go.dev/play/p/OkPcYAhBo0D\nfunc Map[T any, R any](collection []T, iteratee func(item T, index int) R) []R {\n\tresult := make([]R, len(collection))\n\n\tfor i := range collection {\n\t\tresult[i] = iteratee(collection[i], i)\n\t}\n\n\treturn result\n}\n\n// FilterMap returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\n//\n// Play: https://go.dev/play/p/-AuYXfy7opz\nfunc FilterMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FlatMap manipulates a slice and transforms and flattens it to a slice of another type.\n// The transform function can either return a slice or a `nil`, and in the `nil` case\n// no value is added to the final slice.\n// Play: https://go.dev/play/p/YSoYmQTA8-U\nfunc FlatMap[T any, R any](collection []T, iteratee func(item T, index int) []R) []R {\n\tresult := make([]R, 0, len(collection))\n\n\tfor i := range collection {\n\t\tresult = append(result, iteratee(collection[i], i)...)\n\t}\n\n\treturn result\n}\n\n// Reduce reduces collection to a value which is the accumulated result of running each element in collection\n// through accumulator, where each successive invocation is supplied the return value of the previous.\n// Play: https://go.dev/play/p/R4UHXZNaaUG\nfunc Reduce[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := range collection {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ReduceRight helper is like Reduce except that it iterates over elements of collection from right to left.\n// Play: https://go.dev/play/p/Fq3W70l7wXF\nfunc ReduceRight[T any, R any](collection []T, accumulator func(agg R, item T, index int) R, initial R) R {\n\tfor i := len(collection) - 1; i >= 0; i-- {\n\t\tinitial = accumulator(initial, collection[i], i)\n\t}\n\n\treturn initial\n}\n\n// ForEach iterates over elements of collection and invokes iteratee for each element.\n// Play: https://go.dev/play/p/oofyiUPRf8t\nfunc ForEach[T any](collection []T, iteratee func(item T, index int)) {\n\tfor i := range collection {\n\t\titeratee(collection[i], i)\n\t}\n}\n\n// Times invokes the iteratee n times, returning an array of the results of each invocation.\n// The iteratee is invoked with index as argument.\n// Play: https://go.dev/play/p/vgQj3Glr6lT\nfunc Times[T any](count int, iteratee func(index int) T) []T {\n\tresult := make([]T, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult[i] = iteratee(i)\n\t}\n\n\treturn result\n}\n\n// Uniq returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array.\n// Play: https://go.dev/play/p/DTzbeXZ6iEN\nfunc Uniq[T comparable, Slice ~[]T](collection Slice) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[T]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tif _, ok := seen[collection[i]]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[collection[i]] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// UniqBy returns a duplicate-free version of an array, in which only the first occurrence of each element is kept.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\n// Play: https://go.dev/play/p/g42Z3QSb53u\nfunc UniqBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tseen := make(map[U]struct{}, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif _, ok := seen[key]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\tseen[key] = struct{}{}\n\t\tresult = append(result, collection[i])\n\t}\n\n\treturn result\n}\n\n// GroupBy returns an object composed of keys generated from the results of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/XnQBd_v6brd\nfunc GroupBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) map[U]Slice {\n\tresult := map[U]Slice{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresult[key] = append(result[key], collection[i])\n\t}\n\n\treturn result\n}\n\n// Chunk returns an array of elements split into groups the length of size. If array can't be split evenly,\n// the final chunk will be the remaining elements.\n// Play: https://go.dev/play/p/EeKl0AuTehH\nfunc Chunk[T any, Slice ~[]T](collection Slice, size int) []Slice {\n\tif size <= 0 {\n\t\tpanic(\"Second parameter must be greater than 0\")\n\t}\n\n\tchunksNum := len(collection) / size\n\tif len(collection)%size != 0 {\n\t\tchunksNum += 1\n\t}\n\n\tresult := make([]Slice, 0, chunksNum)\n\n\tfor i := 0; i < chunksNum; i++ {\n\t\tlast := (i + 1) * size\n\t\tif last > len(collection) {\n\t\t\tlast = len(collection)\n\t\t}\n\t\tresult = append(result, collection[i*size:last])\n\t}\n\n\treturn result\n}\n\n// PartitionBy returns an array of elements split into groups. The order of grouped values is\n// determined by the order they occur in collection. The grouping is generated from the results\n// of running each element of collection through iteratee.\n// Play: https://go.dev/play/p/NfQ_nGjkgXW\nfunc PartitionBy[T any, K comparable, Slice ~[]T](collection Slice, iteratee func(item T) K) []Slice {\n\tresult := []Slice{}\n\tseen := map[K]int{}\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tresultIndex, ok := seen[key]\n\t\tif !ok {\n\t\t\tresultIndex = len(result)\n\t\t\tseen[key] = resultIndex\n\t\t\tresult = append(result, Slice{})\n\t\t}\n\n\t\tresult[resultIndex] = append(result[resultIndex], collection[i])\n\t}\n\n\treturn result\n\n\t// unordered:\n\t// groups := GroupBy[T, K](collection, iteratee)\n\t// return Values[K, []T](groups)\n}\n\n// Flatten returns an array a single level deep.\n// Play: https://go.dev/play/p/rbp9ORaMpjw\nfunc Flatten[T any, Slice ~[]T](collection []Slice) Slice {\n\ttotalLen := 0\n\tfor i := range collection {\n\t\ttotalLen += len(collection[i])\n\t}\n\n\tresult := make(Slice, 0, totalLen)\n\tfor i := range collection {\n\t\tresult = append(result, collection[i]...)\n\t}\n\n\treturn result\n}\n\n// Interleave round-robin alternating input slices and sequentially appending value at index into result\n// Play: https://go.dev/play/p/-RJkTLQEDVt\nfunc Interleave[T any, Slice ~[]T](collections ...Slice) Slice {\n\tif len(collections) == 0 {\n\t\treturn Slice{}\n\t}\n\n\tmaxSize := 0\n\ttotalSize := 0\n\tfor i := range collections {\n\t\tsize := len(collections[i])\n\t\ttotalSize += size\n\t\tif size > maxSize {\n\t\t\tmaxSize = size\n\t\t}\n\t}\n\n\tif maxSize == 0 {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, totalSize)\n\n\tresultIdx := 0\n\tfor i := 0; i < maxSize; i++ {\n\t\tfor j := range collections {\n\t\t\tif len(collections[j])-1 < i {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tresult[resultIdx] = collections[j][i]\n\t\t\tresultIdx++\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Shuffle returns an array of shuffled values. Uses the Fisher-Yates shuffle algorithm.\n// Play: https://go.dev/play/p/Qp73bnTDnc7\n\n\n\n\n\n\n\n\n// Reverse reverses array so that the first element becomes the last, the second element becomes the second to last, and so on.\n// Play: https://go.dev/play/p/fhUMLvZ7vS6\nfunc Reverse[T any, Slice ~[]T](collection Slice) Slice {\n\tlength := len(collection)\n\thalf := length / 2\n\n\tfor i := 0; i < half; i = i + 1 {\n\t\tj := length - 1 - i\n\t\tcollection[i], collection[j] = collection[j], collection[i]\n\t}\n\n\treturn collection\n}\n\n// Fill fills elements of array with `initial` value.\n// Play: https://go.dev/play/p/VwR34GzqEub\n\n\n\n\n\n\n\n\n\n\n// Repeat builds a slice with N copies of initial value.\n// Play: https://go.dev/play/p/g3uHXbmc3b6\n\n\n\n\n\n\n\n\n\n\n// RepeatBy builds a slice with values returned by N calls of callback.\n// Play: https://go.dev/play/p/ozZLCtX_hNU\nfunc RepeatBy[T any](count int, predicate func(index int) T) []T {\n\tresult := make([]T, 0, count)\n\n\tfor i := 0; i < count; i++ {\n\t\tresult = append(result, predicate(i))\n\t}\n\n\treturn result\n}\n\n// KeyBy transforms a slice or an array of structs to a map based on a pivot callback.\n// Play: https://go.dev/play/p/mdaClUAT-zZ\nfunc KeyBy[K comparable, V any](collection []V, iteratee func(item V) K) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk := iteratee(collection[i])\n\t\tresult[k] = collection[i]\n\t}\n\n\treturn result\n}\n\n// Associate returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc Associate[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\tresult := make(map[K]V, len(collection))\n\n\tfor i := range collection {\n\t\tk, v := transform(collection[i])\n\t\tresult[k] = v\n\t}\n\n\treturn result\n}\n\n// SliceToMap returns a map containing key-value pairs provided by transform function applied to elements of the given slice.\n// If any of two pairs would have the same key the last one gets added to the map.\n// The order of keys in returned map is not specified and is not guaranteed to be the same from the original array.\n// Alias of Associate().\n// Play: https://go.dev/play/p/WHa2CfMO3Lr\nfunc SliceToMap[T any, K comparable, V any](collection []T, transform func(item T) (K, V)) map[K]V {\n\treturn Associate(collection, transform)\n}\n\n// Drop drops n elements from the beginning of a slice or array.\n// Play: https://go.dev/play/p/JswS7vXRJP2\nfunc Drop[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn make(Slice, 0)\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\n\treturn append(result, collection[n:]...)\n}\n\n// DropRight drops n elements from the end of a slice or array.\n// Play: https://go.dev/play/p/GG0nXkSJJa3\nfunc DropRight[T any, Slice ~[]T](collection Slice, n int) Slice {\n\tif len(collection) <= n {\n\t\treturn Slice{}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-n)\n\treturn append(result, collection[:len(collection)-n]...)\n}\n\n// DropWhile drops elements from the beginning of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/7gBPYw2IK16\nfunc DropWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := 0\n\tfor ; i < len(collection); i++ {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-i)\n\treturn append(result, collection[i:]...)\n}\n\n// DropRightWhile drops elements from the end of a slice or array while the predicate returns true.\n// Play: https://go.dev/play/p/3-n71oEC0Hz\nfunc DropRightWhile[T any, Slice ~[]T](collection Slice, predicate func(item T) bool) Slice {\n\ti := len(collection) - 1\n\tfor ; i >= 0; i-- {\n\t\tif !predicate(collection[i]) {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, i+1)\n\treturn append(result, collection[:i+1]...)\n}\n\n// DropByIndex drops elements from a slice or array by the index.\n// A negative index will drop elements from the end of the slice.\n// Play: https://go.dev/play/p/bPIH4npZRxS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Reject is the opposite of Filter, this method returns the elements of collection that predicate does not return truthy for.\n// Play: https://go.dev/play/p/YkLMODy1WEL\nfunc Reject[T any, Slice ~[]T](collection Slice, predicate func(item T, index int) bool) Slice {\n\tresult := Slice{}\n\n\tfor i := range collection {\n\t\tif !predicate(collection[i], i) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// RejectMap is the opposite of FilterMap, this method returns a slice which obtained after both filtering and mapping using the given callback function.\n// The callback function should return two values:\n//   - the result of the mapping operation and\n//   - whether the result element should be included or not.\nfunc RejectMap[T any, R any](collection []T, callback func(item T, index int) (R, bool)) []R {\n\tresult := []R{}\n\n\tfor i := range collection {\n\t\tif r, ok := callback(collection[i], i); !ok {\n\t\t\tresult = append(result, r)\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FilterReject mixes Filter and Reject, this method returns two slices, one for the elements of collection that\n// predicate returns truthy for and one for the elements that predicate does not return truthy for.\nfunc FilterReject[T any, Slice ~[]T](collection Slice, predicate func(T, int) bool) (kept Slice, rejected Slice) {\n\tkept = make(Slice, 0, len(collection))\n\trejected = make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif predicate(collection[i], i) {\n\t\t\tkept = append(kept, collection[i])\n\t\t} else {\n\t\t\trejected = append(rejected, collection[i])\n\t\t}\n\t}\n\n\treturn kept, rejected\n}\n\n// Count counts the number of elements in the collection that compare equal to value.\n// Play: https://go.dev/play/p/Y3FlK54yveC\nfunc Count[T comparable](collection []T, value T) (count int) {\n\tfor i := range collection {\n\t\tif collection[i] == value {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountBy counts the number of elements in the collection for which predicate is true.\n// Play: https://go.dev/play/p/ByQbNYQQi4X\nfunc CountBy[T any](collection []T, predicate func(item T) bool) (count int) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\tcount++\n\t\t}\n\t}\n\n\treturn count\n}\n\n// CountValues counts the number of each element in the collection.\n// Play: https://go.dev/play/p/-p-PyLT4dfy\nfunc CountValues[T comparable](collection []T) map[T]int {\n\tresult := make(map[T]int)\n\n\tfor i := range collection {\n\t\tresult[collection[i]]++\n\t}\n\n\treturn result\n}\n\n// CountValuesBy counts the number of each element return from mapper function.\n// Is equivalent to chaining lo.Map and lo.CountValues.\n// Play: https://go.dev/play/p/2U0dG1SnOmS\nfunc CountValuesBy[T any, U comparable](collection []T, mapper func(item T) U) map[U]int {\n\tresult := make(map[U]int)\n\n\tfor i := range collection {\n\t\tresult[mapper(collection[i])]++\n\t}\n\n\treturn result\n}\n\n// Subset returns a copy of a slice from `offset` up to `length` elements. Like `slice[start:start+length]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/tOQu1GhFcog\nfunc Subset[T any, Slice ~[]T](collection Slice, offset int, length uint) Slice {\n\tsize := len(collection)\n\n\tif offset < 0 {\n\t\toffset = size + offset\n\t\tif offset < 0 {\n\t\t\toffset = 0\n\t\t}\n\t}\n\n\tif offset > size {\n\t\treturn Slice{}\n\t}\n\n\tif length > uint(size)-uint(offset) {\n\t\tlength = uint(size - offset)\n\t}\n\n\treturn collection[offset : offset+int(length)]\n}\n\n// Slice returns a copy of a slice from `start` up to, but not including `end`. Like `slice[start:end]`, but does not panic on overflow.\n// Play: https://go.dev/play/p/8XWYhfMMA1h\nfunc Slice[T any, Slice ~[]T](collection Slice, start int, end int) Slice {\n\tsize := len(collection)\n\n\tif start >= end {\n\t\treturn Slice{}\n\t}\n\n\tif start > size {\n\t\tstart = size\n\t}\n\tif start < 0 {\n\t\tstart = 0\n\t}\n\n\tif end > size {\n\t\tend = size\n\t}\n\tif end < 0 {\n\t\tend = 0\n\t}\n\n\treturn collection[start:end]\n}\n\n// Replace returns a copy of the slice with the first n non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/XfPzmf9gql6\nfunc Replace[T comparable, Slice ~[]T](collection Slice, old T, new T, n int) Slice {\n\tresult := make(Slice, len(collection))\n\tcopy(result, collection)\n\n\tfor i := range result {\n\t\tif result[i] == old && n != 0 {\n\t\t\tresult[i] = new\n\t\t\tn--\n\t\t}\n\t}\n\n\treturn result\n}\n\n// ReplaceAll returns a copy of the slice with all non-overlapping instances of old replaced by new.\n// Play: https://go.dev/play/p/a9xZFUHfYcV\nfunc ReplaceAll[T comparable, Slice ~[]T](collection Slice, old T, new T) Slice {\n\treturn Replace(collection, old, new, -1)\n}\n\n// Compact returns a slice of all non-zero elements.\n// Play: https://go.dev/play/p/tXiy-iK6PAc\nfunc Compact[T comparable, Slice ~[]T](collection Slice) Slice {\n\tvar zero T\n\n\tresult := make(Slice, 0, len(collection))\n\n\tfor i := range collection {\n\t\tif collection[i] != zero {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// IsSorted checks if a slice is sorted.\n// Play: https://go.dev/play/p/mc3qR-t4mcx\nfunc IsSorted[T constraints.Ordered](collection []T) bool {\n\tfor i := 1; i < len(collection); i++ {\n\t\tif collection[i-1] > collection[i] {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// IsSortedByKey checks if a slice is sorted by iteratee.\n// Play: https://go.dev/play/p/wiG6XyBBu49\nfunc IsSortedByKey[T any, K constraints.Ordered](collection []T, iteratee func(item T) K) bool {\n\tsize := len(collection)\n\n\tfor i := 0; i < size-1; i++ {\n\t\tif iteratee(collection[i]) > iteratee(collection[i+1]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Splice inserts multiple elements at index i. A negative index counts back\n// from the end of the slice. The helper is protected against overflow errors.\n// Play: https://go.dev/play/p/G5_GhkeSUBA\nfunc Splice[T any, Slice ~[]T](collection Slice, i int, elements ...T) Slice {\n\tsizeCollection := len(collection)\n\tsizeElements := len(elements)\n\toutput := make(Slice, 0, sizeCollection+sizeElements) // preallocate memory for the output slice\n\n\tif sizeElements == 0 {\n\t\treturn append(output, collection...) // simple copy\n\t} else if i > sizeCollection {\n\t\t// positive overflow\n\t\treturn append(append(output, collection...), elements...)\n\t} else if i < -sizeCollection {\n\t\t// negative overflow\n\t\treturn append(append(output, elements...), collection...)\n\t} else if i < 0 {\n\t\t// backward\n\t\ti = sizeCollection + i\n\t}\n\n\treturn append(append(append(output, collection[:i]...), elements...), collection[i:]...)\n}\n\n```\n# Function Signature: func DropByIndex[T any](collection []T, indexes ...int) []T {\n# Natural Language Annotations: // DropByIndex drops elements from a slice or array by the index.\n// A negative index will drop elements from the end of the slice.\n// Play: https://go.dev/play/p/bPIH4npZRxS\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc DropByIndex[T any](collection []T, indexes ...int) []T {\n    result := make([]T, 0, len(collection))\n\n    indexMap := make(map[int]struct{})\n    for _, index := range indexes {\n        if index < 0 {\n            index = len(collection) + index\n        }\n        indexMap[index] = struct{}{}\n    }\n\n    for i, value := range collection {\n        if _, ok := indexMap[i]; !ok {\n            result = append(result, value)\n        }\n    }\n\n    return result\n}\n```", "```go\nfunc DropByIndex[T any](collection []T, indexes ...int) []T {\n\tfor _, index := range indexes {\n\t\tif index >= 0 && index < len(collection) {\n\t\t\tcollection = append(collection[:index], collection[index+1:]...)\n\t\t} else if index < 0 && -index <= len(collection) {\n\t\t\tcollection = append(collection[:len(collection)+index], collection[len(collection)+index+1:]...)\n\t\t}\n\t}\n\treturn collection\n}\n```", "```go\nfunc DropByIndex[T any](collection []T, indexes ...int) []T {\n\t// sort indexes in descending order to safely drop elements\n\tsort.Sort(sort.Reverse(sort.IntSlice(indexes)))\n\n\t// iterate over indexes and drop elements\n\tfor _, index := range indexes {\n\t\tif index < 0 {\n\t\t\t// negative index, count from the end\n\t\t\tindex = len(collection) + index\n\t\t}\n\t\tif index >= 0 && index < len(collection) {\n\t\t\t// drop element\n\t\t\tcollection = append(collection[:index], collection[index+1:]...)\n\t\t}\n\t}\n\n\treturn collection\n}\n```"]}, "lo-intersect.go-Some": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/intersect.go:\n```\npackage lo\n\n// Contains returns true if an element is present in a collection.\nfunc Contains[T comparable](collection []T, element T) bool {\n\tfor i := range collection {\n\t\tif collection[i] == element {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\n// ContainsBy returns true if predicate function return true.\nfunc ContainsBy[T any](collection []T, predicate func(item T) bool) bool {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\n// Every returns true if all elements of a subset are contained into a collection or if the subset is empty.\nfunc Every[T comparable](collection []T, subset []T) bool {\n\tfor i := range subset {\n\t\tif !Contains(collection, subset[i]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// EveryBy returns true if the predicate returns true for all of the elements in the collection or if the collection is empty.\nfunc EveryBy[T any](collection []T, predicate func(item T) bool) bool {\n\tfor i := range collection {\n\t\tif !predicate(collection[i]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Some returns true if at least 1 element of a subset is contained into a collection.\n// If the subset is empty Some returns false.\n\n\n\n\n\n\n\n\n\n\n// SomeBy returns true if the predicate returns true for any of the elements in the collection.\n// If the collection is empty SomeBy returns false.\nfunc SomeBy[T any](collection []T, predicate func(item T) bool) bool {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n\n// None returns true if no element of a subset are contained into a collection or if the subset is empty.\nfunc None[T comparable](collection []T, subset []T) bool {\n\tfor i := range subset {\n\t\tif Contains(collection, subset[i]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// NoneBy returns true if the predicate returns true for none of the elements in the collection or if the collection is empty.\nfunc NoneBy[T any](collection []T, predicate func(item T) bool) bool {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn true\n}\n\n// Intersect returns the intersection between two collections.\nfunc Intersect[T comparable, Slice ~[]T](list1 Slice, list2 Slice) Slice {\n\tresult := Slice{}\n\tseen := map[T]struct{}{}\n\n\tfor i := range list1 {\n\t\tseen[list1[i]] = struct{}{}\n\t}\n\n\tfor i := range list2 {\n\t\tif _, ok := seen[list2[i]]; ok {\n\t\t\tresult = append(result, list2[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Difference returns the difference between two collections.\n// The first value is the collection of element absent of list2.\n// The second value is the collection of element absent of list1.\nfunc Difference[T comparable, Slice ~[]T](list1 Slice, list2 Slice) (Slice, Slice) {\n\tleft := Slice{}\n\tright := Slice{}\n\n\tseenLeft := map[T]struct{}{}\n\tseenRight := map[T]struct{}{}\n\n\tfor i := range list1 {\n\t\tseenLeft[list1[i]] = struct{}{}\n\t}\n\n\tfor i := range list2 {\n\t\tseenRight[list2[i]] = struct{}{}\n\t}\n\n\tfor i := range list1 {\n\t\tif _, ok := seenRight[list1[i]]; !ok {\n\t\t\tleft = append(left, list1[i])\n\t\t}\n\t}\n\n\tfor i := range list2 {\n\t\tif _, ok := seenLeft[list2[i]]; !ok {\n\t\t\tright = append(right, list2[i])\n\t\t}\n\t}\n\n\treturn left, right\n}\n\n// Union returns all distinct elements from given collections.\n// result returns will not change the order of elements relatively.\nfunc Union[T comparable, Slice ~[]T](lists ...Slice) Slice {\n\tvar capLen int\n\n\tfor _, list := range lists {\n\t\tcapLen += len(list)\n\t}\n\n\tresult := make(Slice, 0, capLen)\n\tseen := make(map[T]struct{}, capLen)\n\n\tfor i := range lists {\n\t\tfor j := range lists[i] {\n\t\t\tif _, ok := seen[lists[i][j]]; !ok {\n\t\t\t\tseen[lists[i][j]] = struct{}{}\n\t\t\t\tresult = append(result, lists[i][j])\n\t\t\t}\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Without returns slice excluding all given values.\nfunc Without[T comparable, Slice ~[]T](collection Slice, exclude ...T) Slice {\n\tresult := make(Slice, 0, len(collection))\n\tfor i := range collection {\n\t\tif !Contains(exclude, collection[i]) {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\treturn result\n}\n\n// WithoutEmpty returns slice excluding empty values.\n//\n// Deprecated: Use lo.Compact instead.\nfunc WithoutEmpty[T comparable, Slice ~[]T](collection Slice) Slice {\n\treturn Compact(collection)\n}\n\n```\n# Function Signature: func Some[T comparable](collection []T, subset []T) bool {\n# Natural Language Annotations: // Some returns true if at least 1 element of a subset is contained into a collection.\n// If the subset is empty Some returns false.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// Some returns true if at least 1 element of a subset is contained into a collection.\n// If the subset is empty Some returns false.\nfunc Some[T comparable](collection []T, subset []T) bool {\n\tif len(subset) == 0 {\n\t\treturn false\n\t}\n\n\tfor _, subsetElement := range subset {\n\t\tfor _, collectionElement := range collection {\n\t\t\tif subsetElement == collectionElement {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t}\n\n\treturn false\n}\n```", "```go\nfunc Some[T comparable](collection []T, subset []T) bool {\n\tfor i := range subset {\n\t\tif Contains(collection, subset[i]) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n```", "```go\nfunc Some[T comparable](collection []T, subset []T) bool {\n\tfor _, subsetElement := range subset {\n\t\tif Contains(collection, subsetElement) {\n\t\t\treturn true\n\t\t}\n\t}\n\n\treturn false\n}\n```"]}, "lo-find.go-FindKey": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/find.go:\n```\npackage lo\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// IndexOf returns the index at which the first occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc IndexOf[T comparable](collection []T, element T) int {\n\tfor i := range collection {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// LastIndexOf returns the index at which the last occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc LastIndexOf[T comparable](collection []T, element T) int {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// Find search an element in a slice based on a predicate. It returns element and true if element was found.\nfunc Find[T any](collection []T, predicate func(item T) bool) (T, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, false\n}\n\n// FindIndexOf searches an element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindLastIndexOf searches last element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindLastIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindOrElse search an element in a slice based on a predicate. It returns the element if found or a given fallback value otherwise.\nfunc FindOrElse[T any](collection []T, fallback T, predicate func(item T) bool) T {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i]\n\t\t}\n\t}\n\n\treturn fallback\n}\n\n// FindKey returns the key of the first value matching.\n\n\n\n\n\n\n\n\n\n\n// FindKeyBy returns the key of the first element predicate returns truthy for.\n\n\n\n\n\n\n\n\n\n\n// FindUniques returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindUniques[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindUniquesBy returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindUniquesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicates returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindDuplicates[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[collection[i]] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicatesBy returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindDuplicatesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[key] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Min search the minimum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Min[T constraints.Ordered](collection []T) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item < min {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// MinBy search the minimum value of a collection using the given comparison function.\n// If several values of the collection are equal to the smallest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MinBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Earliest search the minimum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Earliest(times ...time.Time) time.Time {\n\tvar min time.Time\n\n\tif len(times) == 0 {\n\t\treturn min\n\t}\n\n\tmin = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.Before(min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Max searches the maximum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Max[T constraints.Ordered](collection []T) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item > max {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// MaxBy search the maximum value of a collection using the given comparison function.\n// If several values of the collection are equal to the greatest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MaxBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// Latest search the maximum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Latest(times ...time.Time) time.Time {\n\tvar max time.Time\n\n\tif len(times) == 0 {\n\t\treturn max\n\t}\n\n\tmax = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.After(max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// First returns the first element of a collection and check for availability of the first element.\nfunc First[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[0], true\n}\n\n// FirstOrEmpty returns the first element of a collection or zero value if empty.\nfunc FirstOrEmpty[T any](collection []T) T {\n\ti, _ := First(collection)\n\treturn i\n}\n\n// FirstOr returns the first element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Last returns the last element of a collection or error if empty.\nfunc Last[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[length-1], true\n}\n\n// Returns the last element of a collection or zero value if empty.\nfunc LastOrEmpty[T any](collection []T) T {\n\ti, _ := Last(collection)\n\treturn i\n}\n\n// LastOr returns the last element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Nth returns the element at index `nth` of collection. If `nth` is negative, the nth element\n// from the end is returned. An error is returned when nth is out of slice bounds.\nfunc Nth[T any, N constraints.Integer](collection []T, nth N) (T, error) {\n\tn := int(nth)\n\tl := len(collection)\n\tif n >= l || -n > l {\n\t\tvar t T\n\t\treturn t, fmt.Errorf(\"nth: %d out of slice bounds\", n)\n\t}\n\n\tif n >= 0 {\n\t\treturn collection[n], nil\n\t}\n\treturn collection[l+n], nil\n}\n\n// Sample returns a random item from collection.\n\n\n\n\n\n\n\n\n\n// Samples returns N random unique items from collection.\nfunc Samples[T any, Slice ~[]T](collection Slice, count int) Slice {\n\tsize := len(collection)\n\n\tcopy := append(Slice{}, collection...)\n\n\tresults := Slice{}\n\n\tfor i := 0; i < size && i < count; i++ {\n\t\tcopyLength := size - i\n\n\t\tindex := rand.IntN(size - i)\n\t\tresults = append(results, copy[index])\n\n\t\t// Removes element.\n\t\t// It is faster to swap with last element and remove it.\n\t\tcopy[index] = copy[copyLength-1]\n\t\tcopy = copy[:copyLength-1]\n\t}\n\n\treturn results\n}\n\n```\n# Function Signature: func FindKey[K comparable, V comparable](object map[K]V, value V) (K, bool) {\n# Natural Language Annotations: // FindKey returns the key of the first value matching.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc FindKey[K comparable, V comparable](object map[K]V, value V) (K, bool) {\n\tfor key, mapValue := range object {\n\t\tif mapValue == value {\n\t\t\treturn key, true\n\t\t}\n\t}\n\tvar result K\n\treturn result, false\n}\n```", "```go\nfunc FindKey[K comparable, V comparable](object map[K]V, value V) (K, bool) {\n\tfor key, val := range object {\n\t\tif val == value {\n\t\t\treturn key, true\n\t\t}\n\t}\n\n\tvar key K\n\treturn key, false\n}\n```", "```go\nfunc FindKey[K comparable, V comparable](object map[K]V, value V) (K, bool) {\n    for key, val := range object {\n        if val == value {\n            return key, true\n        }\n    }\n\n    var result K\n    return result, false\n}\n```"]}, "lo-find.go-FindKeyBy": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/find.go:\n```\npackage lo\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// IndexOf returns the index at which the first occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc IndexOf[T comparable](collection []T, element T) int {\n\tfor i := range collection {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// LastIndexOf returns the index at which the last occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc LastIndexOf[T comparable](collection []T, element T) int {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// Find search an element in a slice based on a predicate. It returns element and true if element was found.\nfunc Find[T any](collection []T, predicate func(item T) bool) (T, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, false\n}\n\n// FindIndexOf searches an element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindLastIndexOf searches last element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindLastIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindOrElse search an element in a slice based on a predicate. It returns the element if found or a given fallback value otherwise.\nfunc FindOrElse[T any](collection []T, fallback T, predicate func(item T) bool) T {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i]\n\t\t}\n\t}\n\n\treturn fallback\n}\n\n// FindKey returns the key of the first value matching.\n\n\n\n\n\n\n\n\n\n\n// FindKeyBy returns the key of the first element predicate returns truthy for.\n\n\n\n\n\n\n\n\n\n\n// FindUniques returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindUniques[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindUniquesBy returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindUniquesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicates returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindDuplicates[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[collection[i]] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicatesBy returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindDuplicatesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[key] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Min search the minimum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Min[T constraints.Ordered](collection []T) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item < min {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// MinBy search the minimum value of a collection using the given comparison function.\n// If several values of the collection are equal to the smallest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MinBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Earliest search the minimum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Earliest(times ...time.Time) time.Time {\n\tvar min time.Time\n\n\tif len(times) == 0 {\n\t\treturn min\n\t}\n\n\tmin = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.Before(min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Max searches the maximum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Max[T constraints.Ordered](collection []T) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item > max {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// MaxBy search the maximum value of a collection using the given comparison function.\n// If several values of the collection are equal to the greatest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MaxBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// Latest search the maximum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Latest(times ...time.Time) time.Time {\n\tvar max time.Time\n\n\tif len(times) == 0 {\n\t\treturn max\n\t}\n\n\tmax = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.After(max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// First returns the first element of a collection and check for availability of the first element.\nfunc First[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[0], true\n}\n\n// FirstOrEmpty returns the first element of a collection or zero value if empty.\nfunc FirstOrEmpty[T any](collection []T) T {\n\ti, _ := First(collection)\n\treturn i\n}\n\n// FirstOr returns the first element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Last returns the last element of a collection or error if empty.\nfunc Last[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[length-1], true\n}\n\n// Returns the last element of a collection or zero value if empty.\nfunc LastOrEmpty[T any](collection []T) T {\n\ti, _ := Last(collection)\n\treturn i\n}\n\n// LastOr returns the last element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Nth returns the element at index `nth` of collection. If `nth` is negative, the nth element\n// from the end is returned. An error is returned when nth is out of slice bounds.\nfunc Nth[T any, N constraints.Integer](collection []T, nth N) (T, error) {\n\tn := int(nth)\n\tl := len(collection)\n\tif n >= l || -n > l {\n\t\tvar t T\n\t\treturn t, fmt.Errorf(\"nth: %d out of slice bounds\", n)\n\t}\n\n\tif n >= 0 {\n\t\treturn collection[n], nil\n\t}\n\treturn collection[l+n], nil\n}\n\n// Sample returns a random item from collection.\n\n\n\n\n\n\n\n\n\n// Samples returns N random unique items from collection.\nfunc Samples[T any, Slice ~[]T](collection Slice, count int) Slice {\n\tsize := len(collection)\n\n\tcopy := append(Slice{}, collection...)\n\n\tresults := Slice{}\n\n\tfor i := 0; i < size && i < count; i++ {\n\t\tcopyLength := size - i\n\n\t\tindex := rand.IntN(size - i)\n\t\tresults = append(results, copy[index])\n\n\t\t// Removes element.\n\t\t// It is faster to swap with last element and remove it.\n\t\tcopy[index] = copy[copyLength-1]\n\t\tcopy = copy[:copyLength-1]\n\t}\n\n\treturn results\n}\n\n```\n# Function Signature: func FindKeyBy[K comparable, V any](object map[K]V, predicate func(key K, value V) bool) (K, bool) {\n# Natural Language Annotations: // FindKeyBy returns the key of the first element predicate returns truthy for.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc FindKeyBy[K comparable, V any](object map[K]V, predicate func(key K, value V) bool) (K, bool) {\n\tfor k, v := range object {\n\t\tif predicate(k, v) {\n\t\t\treturn k, true\n\t\t}\n\t}\n\tvar result K\n\treturn result, false\n}\n```", "```go\nfunc FindKeyBy[K comparable, V any](object map[K]V, predicate func(key K, value V) bool) (K, bool) {\n    for key, value := range object {\n        if predicate(key, value) {\n            return key, true\n        }\n    }\n\n    var result K\n    return result, false\n}\n```", "```go\nfunc FindKeyBy[K comparable, V any](object map[K]V, predicate func(key K, value V) bool) (K, bool) {\n\tfor key, value := range object {\n\t\tif predicate(key, value) {\n\t\t\treturn key, true\n\t\t}\n\t}\n\n\tvar result K\n\treturn result, false\n}\n```"]}, "lo-find.go-FirstOr": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/find.go:\n```\npackage lo\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// IndexOf returns the index at which the first occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc IndexOf[T comparable](collection []T, element T) int {\n\tfor i := range collection {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// LastIndexOf returns the index at which the last occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc LastIndexOf[T comparable](collection []T, element T) int {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// Find search an element in a slice based on a predicate. It returns element and true if element was found.\nfunc Find[T any](collection []T, predicate func(item T) bool) (T, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, false\n}\n\n// FindIndexOf searches an element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindLastIndexOf searches last element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindLastIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindOrElse search an element in a slice based on a predicate. It returns the element if found or a given fallback value otherwise.\nfunc FindOrElse[T any](collection []T, fallback T, predicate func(item T) bool) T {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i]\n\t\t}\n\t}\n\n\treturn fallback\n}\n\n// FindKey returns the key of the first value matching.\n\n\n\n\n\n\n\n\n\n\n// FindKeyBy returns the key of the first element predicate returns truthy for.\n\n\n\n\n\n\n\n\n\n\n// FindUniques returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindUniques[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindUniquesBy returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindUniquesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicates returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindDuplicates[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[collection[i]] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicatesBy returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindDuplicatesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[key] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Min search the minimum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Min[T constraints.Ordered](collection []T) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item < min {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// MinBy search the minimum value of a collection using the given comparison function.\n// If several values of the collection are equal to the smallest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MinBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Earliest search the minimum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Earliest(times ...time.Time) time.Time {\n\tvar min time.Time\n\n\tif len(times) == 0 {\n\t\treturn min\n\t}\n\n\tmin = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.Before(min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Max searches the maximum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Max[T constraints.Ordered](collection []T) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item > max {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// MaxBy search the maximum value of a collection using the given comparison function.\n// If several values of the collection are equal to the greatest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MaxBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// Latest search the maximum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Latest(times ...time.Time) time.Time {\n\tvar max time.Time\n\n\tif len(times) == 0 {\n\t\treturn max\n\t}\n\n\tmax = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.After(max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// First returns the first element of a collection and check for availability of the first element.\nfunc First[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[0], true\n}\n\n// FirstOrEmpty returns the first element of a collection or zero value if empty.\nfunc FirstOrEmpty[T any](collection []T) T {\n\ti, _ := First(collection)\n\treturn i\n}\n\n// FirstOr returns the first element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Last returns the last element of a collection or error if empty.\nfunc Last[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[length-1], true\n}\n\n// Returns the last element of a collection or zero value if empty.\nfunc LastOrEmpty[T any](collection []T) T {\n\ti, _ := Last(collection)\n\treturn i\n}\n\n// LastOr returns the last element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Nth returns the element at index `nth` of collection. If `nth` is negative, the nth element\n// from the end is returned. An error is returned when nth is out of slice bounds.\nfunc Nth[T any, N constraints.Integer](collection []T, nth N) (T, error) {\n\tn := int(nth)\n\tl := len(collection)\n\tif n >= l || -n > l {\n\t\tvar t T\n\t\treturn t, fmt.Errorf(\"nth: %d out of slice bounds\", n)\n\t}\n\n\tif n >= 0 {\n\t\treturn collection[n], nil\n\t}\n\treturn collection[l+n], nil\n}\n\n// Sample returns a random item from collection.\n\n\n\n\n\n\n\n\n\n// Samples returns N random unique items from collection.\nfunc Samples[T any, Slice ~[]T](collection Slice, count int) Slice {\n\tsize := len(collection)\n\n\tcopy := append(Slice{}, collection...)\n\n\tresults := Slice{}\n\n\tfor i := 0; i < size && i < count; i++ {\n\t\tcopyLength := size - i\n\n\t\tindex := rand.IntN(size - i)\n\t\tresults = append(results, copy[index])\n\n\t\t// Removes element.\n\t\t// It is faster to swap with last element and remove it.\n\t\tcopy[index] = copy[copyLength-1]\n\t\tcopy = copy[:copyLength-1]\n\t}\n\n\treturn results\n}\n\n```\n# Function Signature: func FirstOr[T any](collection []T, fallback T) T {\n# Natural Language Annotations: // FirstOr returns the first element of a collection or the fallback value if empty.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc FirstOr[T any](collection []T, fallback T) T {\n\tif len(collection) == 0 {\n\t\treturn fallback\n\t}\n\treturn collection[0]\n}\n```", "```go\nfunc FirstOr[T any](collection []T, fallback T) T {\n\ti, _ := First(collection)\n\tif i == fallback {\n\t\treturn i\n\t}\n\treturn fallback\n}\n```", "```go\n// FirstOr returns the first element of a collection or the fallback value if empty.\nfunc FirstOr[T any](collection []T, fallback T) T {\n\tif len(collection) == 0 {\n\t\treturn fallback\n\t}\n\n\treturn collection[0]\n}\n```"]}, "lo-find.go-LastOr": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/find.go:\n```\npackage lo\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// IndexOf returns the index at which the first occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc IndexOf[T comparable](collection []T, element T) int {\n\tfor i := range collection {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// LastIndexOf returns the index at which the last occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc LastIndexOf[T comparable](collection []T, element T) int {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// Find search an element in a slice based on a predicate. It returns element and true if element was found.\nfunc Find[T any](collection []T, predicate func(item T) bool) (T, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, false\n}\n\n// FindIndexOf searches an element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindLastIndexOf searches last element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindLastIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindOrElse search an element in a slice based on a predicate. It returns the element if found or a given fallback value otherwise.\nfunc FindOrElse[T any](collection []T, fallback T, predicate func(item T) bool) T {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i]\n\t\t}\n\t}\n\n\treturn fallback\n}\n\n// FindKey returns the key of the first value matching.\n\n\n\n\n\n\n\n\n\n\n// FindKeyBy returns the key of the first element predicate returns truthy for.\n\n\n\n\n\n\n\n\n\n\n// FindUniques returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindUniques[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindUniquesBy returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindUniquesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicates returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindDuplicates[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[collection[i]] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicatesBy returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindDuplicatesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[key] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Min search the minimum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Min[T constraints.Ordered](collection []T) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item < min {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// MinBy search the minimum value of a collection using the given comparison function.\n// If several values of the collection are equal to the smallest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MinBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Earliest search the minimum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Earliest(times ...time.Time) time.Time {\n\tvar min time.Time\n\n\tif len(times) == 0 {\n\t\treturn min\n\t}\n\n\tmin = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.Before(min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Max searches the maximum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Max[T constraints.Ordered](collection []T) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item > max {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// MaxBy search the maximum value of a collection using the given comparison function.\n// If several values of the collection are equal to the greatest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MaxBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// Latest search the maximum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Latest(times ...time.Time) time.Time {\n\tvar max time.Time\n\n\tif len(times) == 0 {\n\t\treturn max\n\t}\n\n\tmax = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.After(max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// First returns the first element of a collection and check for availability of the first element.\nfunc First[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[0], true\n}\n\n// FirstOrEmpty returns the first element of a collection or zero value if empty.\nfunc FirstOrEmpty[T any](collection []T) T {\n\ti, _ := First(collection)\n\treturn i\n}\n\n// FirstOr returns the first element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Last returns the last element of a collection or error if empty.\nfunc Last[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[length-1], true\n}\n\n// Returns the last element of a collection or zero value if empty.\nfunc LastOrEmpty[T any](collection []T) T {\n\ti, _ := Last(collection)\n\treturn i\n}\n\n// LastOr returns the last element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Nth returns the element at index `nth` of collection. If `nth` is negative, the nth element\n// from the end is returned. An error is returned when nth is out of slice bounds.\nfunc Nth[T any, N constraints.Integer](collection []T, nth N) (T, error) {\n\tn := int(nth)\n\tl := len(collection)\n\tif n >= l || -n > l {\n\t\tvar t T\n\t\treturn t, fmt.Errorf(\"nth: %d out of slice bounds\", n)\n\t}\n\n\tif n >= 0 {\n\t\treturn collection[n], nil\n\t}\n\treturn collection[l+n], nil\n}\n\n// Sample returns a random item from collection.\n\n\n\n\n\n\n\n\n\n// Samples returns N random unique items from collection.\nfunc Samples[T any, Slice ~[]T](collection Slice, count int) Slice {\n\tsize := len(collection)\n\n\tcopy := append(Slice{}, collection...)\n\n\tresults := Slice{}\n\n\tfor i := 0; i < size && i < count; i++ {\n\t\tcopyLength := size - i\n\n\t\tindex := rand.IntN(size - i)\n\t\tresults = append(results, copy[index])\n\n\t\t// Removes element.\n\t\t// It is faster to swap with last element and remove it.\n\t\tcopy[index] = copy[copyLength-1]\n\t\tcopy = copy[:copyLength-1]\n\t}\n\n\treturn results\n}\n\n```\n# Function Signature: func LastOr[T any](collection []T, fallback T) T {\n# Natural Language Annotations: // LastOr returns the last element of a collection or the fallback value if empty.\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// LastOr returns the last element of a collection or the fallback value if empty.\nfunc LastOr[T any](collection []T, fallback T) T {\n\ti, _ := Last(collection)\n\tif i == fallback {\n\t\treturn i\n\t}\n\treturn fallback\n}\n```", "```go\nfunc LastOr[T any](collection []T, fallback T) T {\n\ti, ok := Last(collection)\n\tif !ok {\n\t\treturn fallback\n\t}\n\treturn i\n}\n```", "```go\nfunc LastOr[T any](collection []T, fallback T) T {\n\ti, ok := Last(collection)\n\tif ok {\n\t\treturn i\n\t}\n\treturn fallback\n}\n```"]}, "lo-find.go-Sample": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/find.go:\n```\npackage lo\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/samber/lo/internal/constraints\"\n\t\"github.com/samber/lo/internal/rand\"\n)\n\n// IndexOf returns the index at which the first occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc IndexOf[T comparable](collection []T, element T) int {\n\tfor i := range collection {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// LastIndexOf returns the index at which the last occurrence of a value is found in an array or return -1\n// if the value cannot be found.\nfunc LastIndexOf[T comparable](collection []T, element T) int {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif collection[i] == element {\n\t\t\treturn i\n\t\t}\n\t}\n\n\treturn -1\n}\n\n// Find search an element in a slice based on a predicate. It returns element and true if element was found.\nfunc Find[T any](collection []T, predicate func(item T) bool) (T, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, false\n}\n\n// FindIndexOf searches an element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindLastIndexOf searches last element in a slice based on a predicate and returns the index and true.\n// It returns -1 and false if the element is not found.\nfunc FindLastIndexOf[T any](collection []T, predicate func(item T) bool) (T, int, bool) {\n\tlength := len(collection)\n\n\tfor i := length - 1; i >= 0; i-- {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i], i, true\n\t\t}\n\t}\n\n\tvar result T\n\treturn result, -1, false\n}\n\n// FindOrElse search an element in a slice based on a predicate. It returns the element if found or a given fallback value otherwise.\nfunc FindOrElse[T any](collection []T, fallback T, predicate func(item T) bool) T {\n\tfor i := range collection {\n\t\tif predicate(collection[i]) {\n\t\t\treturn collection[i]\n\t\t}\n\t}\n\n\treturn fallback\n}\n\n// FindKey returns the key of the first value matching.\n\n\n\n\n\n\n\n\n\n\n// FindKeyBy returns the key of the first element predicate returns truthy for.\n\n\n\n\n\n\n\n\n\n\n// FindUniques returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindUniques[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindUniquesBy returns a slice with all the unique elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindUniquesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; !duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicates returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the collection.\nfunc FindDuplicates[T comparable, Slice ~[]T](collection Slice) Slice {\n\tisDupl := make(map[T]bool, len(collection))\n\n\tfor i := range collection {\n\t\tduplicated, ok := isDupl[collection[i]]\n\t\tif !ok {\n\t\t\tisDupl[collection[i]] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[collection[i]] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tif duplicated := isDupl[collection[i]]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[collection[i]] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// FindDuplicatesBy returns a slice with the first occurrence of each duplicated elements of the collection.\n// The order of result values is determined by the order they occur in the array. It accepts `iteratee` which is\n// invoked for each element in array to generate the criterion by which uniqueness is computed.\nfunc FindDuplicatesBy[T any, U comparable, Slice ~[]T](collection Slice, iteratee func(item T) U) Slice {\n\tisDupl := make(map[U]bool, len(collection))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tduplicated, ok := isDupl[key]\n\t\tif !ok {\n\t\t\tisDupl[key] = false\n\t\t} else if !duplicated {\n\t\t\tisDupl[key] = true\n\t\t}\n\t}\n\n\tresult := make(Slice, 0, len(collection)-len(isDupl))\n\n\tfor i := range collection {\n\t\tkey := iteratee(collection[i])\n\n\t\tif duplicated := isDupl[key]; duplicated {\n\t\t\tresult = append(result, collection[i])\n\t\t\tisDupl[key] = false\n\t\t}\n\t}\n\n\treturn result\n}\n\n// Min search the minimum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Min[T constraints.Ordered](collection []T) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item < min {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// MinBy search the minimum value of a collection using the given comparison function.\n// If several values of the collection are equal to the smallest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MinBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar min T\n\n\tif len(collection) == 0 {\n\t\treturn min\n\t}\n\n\tmin = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Earliest search the minimum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Earliest(times ...time.Time) time.Time {\n\tvar min time.Time\n\n\tif len(times) == 0 {\n\t\treturn min\n\t}\n\n\tmin = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.Before(min) {\n\t\t\tmin = item\n\t\t}\n\t}\n\n\treturn min\n}\n\n// Max searches the maximum value of a collection.\n// Returns zero value when the collection is empty.\nfunc Max[T constraints.Ordered](collection []T) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif item > max {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// MaxBy search the maximum value of a collection using the given comparison function.\n// If several values of the collection are equal to the greatest value, returns the first such value.\n// Returns zero value when the collection is empty.\nfunc MaxBy[T any](collection []T, comparison func(a T, b T) bool) T {\n\tvar max T\n\n\tif len(collection) == 0 {\n\t\treturn max\n\t}\n\n\tmax = collection[0]\n\n\tfor i := 1; i < len(collection); i++ {\n\t\titem := collection[i]\n\n\t\tif comparison(item, max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// Latest search the maximum time.Time of a collection.\n// Returns zero value when the collection is empty.\nfunc Latest(times ...time.Time) time.Time {\n\tvar max time.Time\n\n\tif len(times) == 0 {\n\t\treturn max\n\t}\n\n\tmax = times[0]\n\n\tfor i := 1; i < len(times); i++ {\n\t\titem := times[i]\n\n\t\tif item.After(max) {\n\t\t\tmax = item\n\t\t}\n\t}\n\n\treturn max\n}\n\n// First returns the first element of a collection and check for availability of the first element.\nfunc First[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[0], true\n}\n\n// FirstOrEmpty returns the first element of a collection or zero value if empty.\nfunc FirstOrEmpty[T any](collection []T) T {\n\ti, _ := First(collection)\n\treturn i\n}\n\n// FirstOr returns the first element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Last returns the last element of a collection or error if empty.\nfunc Last[T any](collection []T) (T, bool) {\n\tlength := len(collection)\n\n\tif length == 0 {\n\t\tvar t T\n\t\treturn t, false\n\t}\n\n\treturn collection[length-1], true\n}\n\n// Returns the last element of a collection or zero value if empty.\nfunc LastOrEmpty[T any](collection []T) T {\n\ti, _ := Last(collection)\n\treturn i\n}\n\n// LastOr returns the last element of a collection or the fallback value if empty.\n\n\n\n\n\n\n\n\n\n// Nth returns the element at index `nth` of collection. If `nth` is negative, the nth element\n// from the end is returned. An error is returned when nth is out of slice bounds.\nfunc Nth[T any, N constraints.Integer](collection []T, nth N) (T, error) {\n\tn := int(nth)\n\tl := len(collection)\n\tif n >= l || -n > l {\n\t\tvar t T\n\t\treturn t, fmt.Errorf(\"nth: %d out of slice bounds\", n)\n\t}\n\n\tif n >= 0 {\n\t\treturn collection[n], nil\n\t}\n\treturn collection[l+n], nil\n}\n\n// Sample returns a random item from collection.\n\n\n\n\n\n\n\n\n\n// Samples returns N random unique items from collection.\nfunc Samples[T any, Slice ~[]T](collection Slice, count int) Slice {\n\tsize := len(collection)\n\n\tcopy := append(Slice{}, collection...)\n\n\tresults := Slice{}\n\n\tfor i := 0; i < size && i < count; i++ {\n\t\tcopyLength := size - i\n\n\t\tindex := rand.IntN(size - i)\n\t\tresults = append(results, copy[index])\n\n\t\t// Removes element.\n\t\t// It is faster to swap with last element and remove it.\n\t\tcopy[index] = copy[copyLength-1]\n\t\tcopy = copy[:copyLength-1]\n\t}\n\n\treturn results\n}\n\n```\n# Function Signature: func Sample[T any](collection []T) T {\n# Natural Language Annotations: // Sample returns a random item from collection.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc Sample[T any](collection []T) T {\n\trandIndex := rand.Intn(len(collection))\n\treturn collection[randIndex]\n}\n```", "```go\nfunc Sample[T any](collection []T) T {\n\tsize := len(collection)\n\tindex := rand.IntN(size)\n\treturn collection[index]\n}\n```", "Here is the generated function:\n\n```go\n// Sample returns a random item from collection.\nfunc Sample[T any](collection []T) T {\n\tvar result T\n\n\tlength := len(collection)\n\tif length == 0 {\n\t\treturn result\n\t}\n\n\tindex := rand.Intn(length)\n\treturn collection[index]\n}\n```"]}, "lo-math.go-Range": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/math.go:\n```\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Range creates an array of numbers (positive and/or negative) with given length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeFrom creates an array of numbers from start with specified length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeWithSteps creates an array of numbers (positive and/or negative) progressing from start up to, but not including end.\n// step set to zero will return empty array.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nfunc RangeWithSteps[T constraints.Integer | constraints.Float](start, end, step T) []T {\n\tresult := []T{}\n\tif start == end || step == 0 {\n\t\treturn result\n\t}\n\tif start < end {\n\t\tif step < 0 {\n\t\t\treturn result\n\t\t}\n\t\tfor i := start; i < end; i += step {\n\t\t\tresult = append(result, i)\n\t\t}\n\t\treturn result\n\t}\n\tif step > 0 {\n\t\treturn result\n\t}\n\tfor i := start; i > end; i += step {\n\t\tresult = append(result, i)\n\t}\n\treturn result\n}\n\n// Clamp clamps number within the inclusive lower and upper bounds.\n// Play: https://go.dev/play/p/RU4lJNC2hlI\nfunc Clamp[T constraints.Ordered](value T, min T, max T) T {\n\tif value < min {\n\t\treturn min\n\t} else if value > max {\n\t\treturn max\n\t}\n\treturn value\n}\n\n// Sum sums the values in a collection. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/upfeJVqs4Bt\nfunc Sum[T constraints.Float | constraints.Integer | constraints.Complex](collection []T) T {\n\tvar sum T = 0\n\tfor i := range collection {\n\t\tsum += collection[i]\n\t}\n\treturn sum\n}\n\n// SumBy summarizes the values in a collection using the given return value from the iteration function. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/Dz_a_7jN_ca\nfunc SumBy[T any, R constraints.Float | constraints.Integer | constraints.Complex](collection []T, iteratee func(item T) R) R {\n\tvar sum R = 0\n\tfor i := range collection {\n\t\tsum = sum + iteratee(collection[i])\n\t}\n\treturn sum\n}\n\n// Mean calculates the mean of a collection of numbers.\n\n\n\n\n\n\n\n\n\n// MeanBy calculates the mean of a collection of numbers using the given return value from the iteration function.\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func Range(elementNum int) []int {\n# Natural Language Annotations: // Range creates an array of numbers (positive and/or negative) with given length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nPlease return the generated code surrounded by ```\n", "response": ["```go\n// Range creates an array of numbers (positive and/or negative) with given length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nfunc Range(elementNum int) []int {\n\tresult := make([]int, elementNum)\n\tfor i := 0; i < elementNum; i++ {\n\t\tresult[i] = i\n\t}\n\treturn result\n}\n```", "```go\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Range creates an array of numbers (positive and/or negative) with given length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nfunc Range(elementNum int) []int {\n\tresult := make([]int, elementNum)\n\tfor i := 0; i < elementNum; i++ {\n\t\tresult[i] = i\n\t}\n\treturn result\n}\n```", "Here is the generated code:\n\n```go\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Range creates an array of numbers (positive and/or negative) with given length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nfunc Range(elementNum int) []int {\n\tresult := []int{}\n\tfor i := 0; i < elementNum; i++ {\n\t\tresult = append(result, i)\n\t}\n\treturn result\n}\n```\n\nThe `Range` function generates an array of integers from 0 to the length specified by `elementNum`. The generated array is then returned."]}, "lo-math.go-RangeFrom": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/math.go:\n```\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Range creates an array of numbers (positive and/or negative) with given length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeFrom creates an array of numbers from start with specified length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeWithSteps creates an array of numbers (positive and/or negative) progressing from start up to, but not including end.\n// step set to zero will return empty array.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nfunc RangeWithSteps[T constraints.Integer | constraints.Float](start, end, step T) []T {\n\tresult := []T{}\n\tif start == end || step == 0 {\n\t\treturn result\n\t}\n\tif start < end {\n\t\tif step < 0 {\n\t\t\treturn result\n\t\t}\n\t\tfor i := start; i < end; i += step {\n\t\t\tresult = append(result, i)\n\t\t}\n\t\treturn result\n\t}\n\tif step > 0 {\n\t\treturn result\n\t}\n\tfor i := start; i > end; i += step {\n\t\tresult = append(result, i)\n\t}\n\treturn result\n}\n\n// Clamp clamps number within the inclusive lower and upper bounds.\n// Play: https://go.dev/play/p/RU4lJNC2hlI\nfunc Clamp[T constraints.Ordered](value T, min T, max T) T {\n\tif value < min {\n\t\treturn min\n\t} else if value > max {\n\t\treturn max\n\t}\n\treturn value\n}\n\n// Sum sums the values in a collection. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/upfeJVqs4Bt\nfunc Sum[T constraints.Float | constraints.Integer | constraints.Complex](collection []T) T {\n\tvar sum T = 0\n\tfor i := range collection {\n\t\tsum += collection[i]\n\t}\n\treturn sum\n}\n\n// SumBy summarizes the values in a collection using the given return value from the iteration function. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/Dz_a_7jN_ca\nfunc SumBy[T any, R constraints.Float | constraints.Integer | constraints.Complex](collection []T, iteratee func(item T) R) R {\n\tvar sum R = 0\n\tfor i := range collection {\n\t\tsum = sum + iteratee(collection[i])\n\t}\n\treturn sum\n}\n\n// Mean calculates the mean of a collection of numbers.\n\n\n\n\n\n\n\n\n\n// MeanBy calculates the mean of a collection of numbers using the given return value from the iteration function.\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func RangeFrom[T constraints.Integer | constraints.Float](start T, elementNum int) []T {\n# Natural Language Annotations: // RangeFrom creates an array of numbers from start with specified length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// RangeFrom creates an array of numbers from start with specified length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nfunc RangeFrom[T constraints.Integer | constraints.Float](start T, elementNum int) []T {\n\tresult := make([]T, elementNum)\n\tfor i := range result {\n\t\tresult[i] = start + T(i)\n\t}\n\treturn result\n}\n```", "```go\npackage lo\n\nimport \"github.com/samber/lo/internal/constraints\"\n\n// RangeFrom creates an array of numbers from start with specified length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nfunc RangeFrom[T constraints.Integer | constraints.Float](start T, elementNum int) []T {\n\tresult := make([]T, elementNum)\n\tfor i := 0; i < elementNum; i++ {\n\t\tresult[i] = start + T(i)\n\t}\n\treturn result\n}\n```", "```go\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// RangeFrom creates an array of numbers from start with specified length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nfunc RangeFrom[T constraints.Integer | constraints.Float](start T, elementNum int) []T {\n\tresult := make([]T, elementNum)\n\tfor i := 0; i < elementNum; i++ {\n\t\tresult[i] = start + T(i)\n\t}\n\treturn result\n}\n```"]}, "lo-math.go-Mean": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/math.go:\n```\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Range creates an array of numbers (positive and/or negative) with given length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeFrom creates an array of numbers from start with specified length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeWithSteps creates an array of numbers (positive and/or negative) progressing from start up to, but not including end.\n// step set to zero will return empty array.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nfunc RangeWithSteps[T constraints.Integer | constraints.Float](start, end, step T) []T {\n\tresult := []T{}\n\tif start == end || step == 0 {\n\t\treturn result\n\t}\n\tif start < end {\n\t\tif step < 0 {\n\t\t\treturn result\n\t\t}\n\t\tfor i := start; i < end; i += step {\n\t\t\tresult = append(result, i)\n\t\t}\n\t\treturn result\n\t}\n\tif step > 0 {\n\t\treturn result\n\t}\n\tfor i := start; i > end; i += step {\n\t\tresult = append(result, i)\n\t}\n\treturn result\n}\n\n// Clamp clamps number within the inclusive lower and upper bounds.\n// Play: https://go.dev/play/p/RU4lJNC2hlI\nfunc Clamp[T constraints.Ordered](value T, min T, max T) T {\n\tif value < min {\n\t\treturn min\n\t} else if value > max {\n\t\treturn max\n\t}\n\treturn value\n}\n\n// Sum sums the values in a collection. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/upfeJVqs4Bt\nfunc Sum[T constraints.Float | constraints.Integer | constraints.Complex](collection []T) T {\n\tvar sum T = 0\n\tfor i := range collection {\n\t\tsum += collection[i]\n\t}\n\treturn sum\n}\n\n// SumBy summarizes the values in a collection using the given return value from the iteration function. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/Dz_a_7jN_ca\nfunc SumBy[T any, R constraints.Float | constraints.Integer | constraints.Complex](collection []T, iteratee func(item T) R) R {\n\tvar sum R = 0\n\tfor i := range collection {\n\t\tsum = sum + iteratee(collection[i])\n\t}\n\treturn sum\n}\n\n// Mean calculates the mean of a collection of numbers.\n\n\n\n\n\n\n\n\n\n// MeanBy calculates the mean of a collection of numbers using the given return value from the iteration function.\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func Mean[T constraints.Float | constraints.Integer](collection []T) T {\n# Natural Language Annotations: // Mean calculates the mean of a collection of numbers.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\nfunc Mean[T constraints.Float | constraints.Integer](collection []T) T {\n\tif len(collection) == 0 {\n\t\treturn 0\n\t}\n\tvar sum T = 0\n\tfor _, value := range collection {\n\t\tsum += value\n\t}\n\treturn sum / T(len(collection))\n}\n```", "```go\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\nfunc Mean[T constraints.Float | constraints.Integer](collection []T) T {\n\tvar sum T = 0\n\tvar count T = T(len(collection))\n\tfor i := range collection {\n\t\tsum += collection[i]\n\t}\n\tif count > 0 {\n\t\treturn sum / count\n\t}\n\treturn 0\n}\n```", "```go\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Mean calculates the mean of a collection of numbers.\nfunc Mean[T constraints.Float | constraints.Integer](collection []T) T {\n\tvar sum T = 0\n\tvar length = len(collection)\n\tif length == 0 {\n\t\treturn 0\n\t}\n\tfor i := range collection {\n\t\tsum += collection[i]\n\t}\n\treturn sum / T(length)\n}\n```"]}, "lo-math.go-MeanBy": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/lo/math.go:\n```\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// Range creates an array of numbers (positive and/or negative) with given length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeFrom creates an array of numbers from start with specified length.\n// Play: https://go.dev/play/p/0r6VimXAi9H\n\n\n\n\n\n\n\n\n\n\n// RangeWithSteps creates an array of numbers (positive and/or negative) progressing from start up to, but not including end.\n// step set to zero will return empty array.\n// Play: https://go.dev/play/p/0r6VimXAi9H\nfunc RangeWithSteps[T constraints.Integer | constraints.Float](start, end, step T) []T {\n\tresult := []T{}\n\tif start == end || step == 0 {\n\t\treturn result\n\t}\n\tif start < end {\n\t\tif step < 0 {\n\t\t\treturn result\n\t\t}\n\t\tfor i := start; i < end; i += step {\n\t\t\tresult = append(result, i)\n\t\t}\n\t\treturn result\n\t}\n\tif step > 0 {\n\t\treturn result\n\t}\n\tfor i := start; i > end; i += step {\n\t\tresult = append(result, i)\n\t}\n\treturn result\n}\n\n// Clamp clamps number within the inclusive lower and upper bounds.\n// Play: https://go.dev/play/p/RU4lJNC2hlI\nfunc Clamp[T constraints.Ordered](value T, min T, max T) T {\n\tif value < min {\n\t\treturn min\n\t} else if value > max {\n\t\treturn max\n\t}\n\treturn value\n}\n\n// Sum sums the values in a collection. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/upfeJVqs4Bt\nfunc Sum[T constraints.Float | constraints.Integer | constraints.Complex](collection []T) T {\n\tvar sum T = 0\n\tfor i := range collection {\n\t\tsum += collection[i]\n\t}\n\treturn sum\n}\n\n// SumBy summarizes the values in a collection using the given return value from the iteration function. If collection is empty 0 is returned.\n// Play: https://go.dev/play/p/Dz_a_7jN_ca\nfunc SumBy[T any, R constraints.Float | constraints.Integer | constraints.Complex](collection []T, iteratee func(item T) R) R {\n\tvar sum R = 0\n\tfor i := range collection {\n\t\tsum = sum + iteratee(collection[i])\n\t}\n\treturn sum\n}\n\n// Mean calculates the mean of a collection of numbers.\n\n\n\n\n\n\n\n\n\n// MeanBy calculates the mean of a collection of numbers using the given return value from the iteration function.\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func MeanBy[T any, R constraints.Float | constraints.Integer](collection []T, iteratee func(item T) R) R {\n# Natural Language Annotations: // MeanBy calculates the mean of a collection of numbers using the given return value from the iteration function.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc MeanBy[T any, R constraints.Float | constraints.Integer](collection []T, iteratee func(item T) R) R {\n\t// Mean calculates the mean of a collection of numbers using the given return value from the iteration function.\n    var sum R = 0\n    for _, item := range collection {\n        sum += iteratee(item)\n    }\n    return sum / R(len(collection))\n}\n```", "```go\nfunc MeanBy[T any, R constraints.Float | constraints.Integer](collection []T, iteratee func(item T) R) R {\n\tvar sum R = 0\n\tfor i := range collection {\n\t\tsum = sum + iteratee(collection[i])\n\t}\n\tif len(collection) == 0 {\n\t\treturn 0\n\t}\n\treturn sum / R(len(collection))\n}\n```", "```go\npackage lo\n\nimport (\n\t\"github.com/samber/lo/internal/constraints\"\n)\n\n// MeanBy calculates the mean of a collection of numbers using the given return value from the iteration function.\nfunc MeanBy[T any, R constraints.Float | constraints.Integer](collection []T, iteratee func(item T) R) R {\n\tsum := SumBy(collection, iteratee)\n\tif len(collection) == 0 {\n\t\treturn 0\n\t}\n\treturn sum / R(len(collection))\n}\n```"]}, "conc-waitgroup.go-Wait": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/conc/waitgroup.go:\n```\npackage conc\n\nimport (\n\t\"sync\"\n\n\t\"github.com/sourcegraph/conc/panics\"\n)\n\n// NewWaitGroup creates a new WaitGroup.\nfunc NewWaitGroup() *WaitGroup {\n\treturn &WaitGroup{}\n}\n\n// WaitGroup is the primary building block for scoped concurrency.\n// Goroutines can be spawned in the WaitGroup with the Go method,\n// and calling Wait() will ensure that each of those goroutines exits\n// before continuing. Any panics in a child goroutine will be caught\n// and propagated to the caller of Wait().\n//\n// The zero value of WaitGroup is usable, just like sync.WaitGroup.\n// Also like sync.WaitGroup, it must not be copied after first use.\ntype WaitGroup struct {\n\twg sync.WaitGroup\n\tpc panics.Catcher\n}\n\n// Go spawns a new goroutine in the WaitGroup.\nfunc (h *WaitGroup) Go(f func()) {\n\th.wg.Add(1)\n\tgo func() {\n\t\tdefer h.wg.Done()\n\t\th.pc.Try(f)\n\t}()\n}\n\n// Wait will block until all goroutines spawned with Go exit and will\n// propagate any panics spawned in a child goroutine.\n\n\n\n\n\n\n\n// WaitAndRecover will block until all goroutines spawned with Go exit and\n// will return a *panics.Recovered if one of the child goroutines panics.\n\n\n\n\n\n\n\n```\n# Function Signature: func (h *WaitGroup) Wait() {\n# Natural Language Annotations: // Wait will block until all goroutines spawned with Go exit and will\n// propagate any panics spawned in a child goroutine.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (h *WaitGroup) Wait() {\n\t// Wait will block until all goroutines spawned with Go exit and will\n\t// propagate any panics spawned in a child goroutine.\n\th.wg.Wait()\n}\n```", "```go\nfunc (h *WaitGroup) Wait() {\n\th.wg.Wait()\n\tif h.pc.Captured() {\n\t\tpanic(h.pc.Error())\n\t}\n}\n```", "```go\n// Wait will block until all goroutines spawned with Go exit and will\n// propagate any panics spawned in a child goroutine.\nfunc (h *WaitGroup) Wait() {\n\th.wg.Wait()\n\th.pc.Propagate()\n}\n```"]}, "conc-waitgroup.go-WaitAndRecover": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/conc/waitgroup.go:\n```\npackage conc\n\nimport (\n\t\"sync\"\n\n\t\"github.com/sourcegraph/conc/panics\"\n)\n\n// NewWaitGroup creates a new WaitGroup.\nfunc NewWaitGroup() *WaitGroup {\n\treturn &WaitGroup{}\n}\n\n// WaitGroup is the primary building block for scoped concurrency.\n// Goroutines can be spawned in the WaitGroup with the Go method,\n// and calling Wait() will ensure that each of those goroutines exits\n// before continuing. Any panics in a child goroutine will be caught\n// and propagated to the caller of Wait().\n//\n// The zero value of WaitGroup is usable, just like sync.WaitGroup.\n// Also like sync.WaitGroup, it must not be copied after first use.\ntype WaitGroup struct {\n\twg sync.WaitGroup\n\tpc panics.Catcher\n}\n\n// Go spawns a new goroutine in the WaitGroup.\nfunc (h *WaitGroup) Go(f func()) {\n\th.wg.Add(1)\n\tgo func() {\n\t\tdefer h.wg.Done()\n\t\th.pc.Try(f)\n\t}()\n}\n\n// Wait will block until all goroutines spawned with Go exit and will\n// propagate any panics spawned in a child goroutine.\n\n\n\n\n\n\n\n// WaitAndRecover will block until all goroutines spawned with Go exit and\n// will return a *panics.Recovered if one of the child goroutines panics.\n\n\n\n\n\n\n\n```\n# Function Signature: func (h *WaitGroup) WaitAndRecover() *panics.Recovered {\n# Natural Language Annotations: // WaitAndRecover will block until all goroutines spawned with Go exit and\n// will return a *panics.Recovered if one of the child goroutines panics.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (h *WaitGroup) WaitAndRecover() *panics.Recovered {\n\th.wg.Wait()\n\tif h.pc.Recovered() {\n\t\treturn h.pc.Recovered()\n\t}\n\treturn nil\n}\n```", "```go\nfunc (h *WaitGroup) WaitAndRecover() *panics.Recovered {\n\th.wg.Wait()\n\tif h.pc.Recovered() != nil {\n\t\treturn h.pc.Recovered()\n\t}\n\treturn nil\n}\n```", "```go\nfunc (h *WaitGroup) WaitAndRecover() *panics.Recovered {\n\th.wg.Wait()\n\tif h.pc.Caught() {\n\t\treturn &h.pc.Recovered()\n\t}\n\treturn nil\n}\n```"]}, "conc-pool/result_context_pool.go-Wait": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/conc/pool/result_context_pool.go:\n```\npackage pool\n\nimport (\n\t\"context\"\n)\n\n// ResultContextPool is a pool that runs tasks that take a context and return a\n// result. The context passed to the task will be canceled if any of the tasks\n// return an error, which makes its functionality different than just capturing\n// a context with the task closure.\n//\n// The configuration methods (With*) will panic if they are used after calling\n// Go() for the first time.\ntype ResultContextPool[T any] struct {\n\tcontextPool    ContextPool\n\tagg            resultAggregator[T]\n\tcollectErrored bool\n}\n\n// Go submits a task to the pool. If all goroutines in the pool\n// are busy, a call to Go() will block until the task can be started.\nfunc (p *ResultContextPool[T]) Go(f func(context.Context) (T, error)) {\n\tidx := p.agg.nextIndex()\n\tp.contextPool.Go(func(ctx context.Context) error {\n\t\tres, err := f(ctx)\n\t\tp.agg.save(idx, res, err != nil)\n\t\treturn err\n\t})\n}\n\n// Wait cleans up all spawned goroutines, propagates any panics, and\n// returns an error if any of the tasks errored.\n\n\n\n\n\n\n\n// WithCollectErrored configures the pool to still collect the result of a task\n// even if the task returned an error. By default, the result of tasks that errored\n// are ignored and only the error is collected.\nfunc (p *ResultContextPool[T]) WithCollectErrored() *ResultContextPool[T] {\n\tp.panicIfInitialized()\n\tp.collectErrored = true\n\treturn p\n}\n\n// WithFirstError configures the pool to only return the first error\n// returned by a task. By default, Wait() will return a combined error.\nfunc (p *ResultContextPool[T]) WithFirstError() *ResultContextPool[T] {\n\tp.panicIfInitialized()\n\tp.contextPool.WithFirstError()\n\treturn p\n}\n\n// WithCancelOnError configures the pool to cancel its context as soon as\n// any task returns an error. By default, the pool's context is not\n// canceled until the parent context is canceled.\nfunc (p *ResultContextPool[T]) WithCancelOnError() *ResultContextPool[T] {\n\tp.panicIfInitialized()\n\tp.contextPool.WithCancelOnError()\n\treturn p\n}\n\n// WithFailFast is an alias for the combination of WithFirstError and\n// WithCancelOnError. By default, the errors from all tasks are returned and\n// the pool's context is not canceled until the parent context is canceled.\nfunc (p *ResultContextPool[T]) WithFailFast() *ResultContextPool[T] {\n\tp.panicIfInitialized()\n\tp.contextPool.WithFailFast()\n\treturn p\n}\n\n// WithMaxGoroutines limits the number of goroutines in a pool.\n// Defaults to unlimited. Panics if n < 1.\nfunc (p *ResultContextPool[T]) WithMaxGoroutines(n int) *ResultContextPool[T] {\n\tp.panicIfInitialized()\n\tp.contextPool.WithMaxGoroutines(n)\n\treturn p\n}\n\nfunc (p *ResultContextPool[T]) panicIfInitialized() {\n\tp.contextPool.panicIfInitialized()\n}\n\n```\n# Function Signature: func (p *ResultContextPool[T]) Wait() ([]T, error) {\n# Natural Language Annotations: // Wait cleans up all spawned goroutines, propagates any panics, and\n// returns an error if any of the tasks errored.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage pool\n\nimport (\n\t\"context\"\n\t\"sync\"\n)\n\n// Wait cleans up all spawned goroutines, propagates any panics, and\n// returns an error if any of the tasks errored.\nfunc (p *ResultContextPool[T]) Wait() ([]T, error) {\n\t// Wait for all tasks to finish\n\terr := p.contextPool.Wait()\n\n\t// Propagate any panics that occurred\n\tdefer p.contextPool.HandlePanic()\n\n\t// If any task errored, return the combined error\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// If no tasks errored, return the results\n\tres := p.agg.results()\n\treturn res, nil\n}\n```", "```go\nfunc (p *ResultContextPool[T]) Wait() ([]T, error) {\n\t// Wait cleans up all spawned goroutines, propagates any panics, and\n\t// returns an error if any of the tasks errored.\n\n\terr := p.contextPool.Wait()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn p.agg.results(), nil\n}\n```", "Sure, here's how you could implement the `Wait` function based on the provided information.\n\n```go\nfunc (p *ResultContextPool[T]) Wait() ([]T, error) {\n\terr := p.contextPool.Wait()\n\tresults := p.agg.results()\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tif p.collectErrored {\n\t\treturn results, nil\n\t}\n\n\tfilteredResults := make([]T, 0, len(results))\n\tfor _, result := range results {\n\t\tif result != nil {\n\t\t\tfilteredResults = append(filteredResults, result)\n\t\t}\n\t}\n\n\treturn filteredResults, nil\n}\n```\n\nThis function first waits for all tasks to finish and collects the errors. If there was an error, it returns it immediately. \n\nIf `collectErrored` is set to true, it returns all the results, otherwise it filters out tasks that returned an error.\n\nNote: This code assumes that the `resultAggregator` type has methods `nextIndex()`, `save(idx, res, err)`, and `results()`. These methods are not shown in the provided code, but they should be defined in the `resultAggregator` type's implementation."]}, "conc-stream/stream.go-Wait": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/conc/stream/stream.go:\n```\n// Package stream provides a concurrent, ordered stream implementation.\npackage stream\n\nimport (\n\t\"sync\"\n\n\t\"github.com/sourcegraph/conc\"\n\t\"github.com/sourcegraph/conc/panics\"\n\t\"github.com/sourcegraph/conc/pool\"\n)\n\n// New creates a new Stream with default settings.\nfunc New() *Stream {\n\treturn &Stream{\n\t\tpool: *pool.New(),\n\t}\n}\n\n// Stream is used to execute a stream of tasks concurrently while maintaining\n// the order of the results.\n//\n// To use a stream, you submit some number of `Task`s, each of which\n// return a callback. Each task will be executed concurrently in the stream's\n// associated Pool, and the callbacks will be executed sequentially in the\n// order the tasks were submitted.\n//\n// Once all your tasks have been submitted, Wait() must be called to clean up\n// running goroutines and propagate any panics.\n//\n// In the case of panic during execution of a task or a callback, all other\n// tasks and callbacks will still execute. The panic will be propagated to the\n// caller when Wait() is called.\n//\n// A Stream is efficient, but not zero cost. It should not be used for very\n// short tasks. Startup and teardown adds an overhead of a couple of\n// microseconds, and the overhead for each task is roughly 500ns. It should be\n// good enough for any task that requires a network call.\ntype Stream struct {\n\tpool             pool.Pool\n\tcallbackerHandle conc.WaitGroup\n\tqueue            chan callbackCh\n\n\tinitOnce sync.Once\n}\n\n// Task is a task that is submitted to the stream. Submitted tasks will\n// be executed concurrently. It returns a callback that will be called after\n// the task has completed.\ntype Task func() Callback\n\n// Callback is a function that is returned by a Task. Callbacks are\n// called in the same order that tasks are submitted.\ntype Callback func()\n\n// Go schedules a task to be run in the stream's pool. All submitted tasks\n// will be executed concurrently in worker goroutines. Then, the callbacks\n// returned by the tasks will be executed in the order that the tasks were\n// submitted. All callbacks will be executed by the same goroutine, so no\n// synchronization is necessary between callbacks. If all goroutines in the\n// stream's pool are busy, a call to Go() will block until the task can be\n// started.\nfunc (s *Stream) Go(f Task) {\n\ts.init()\n\n\t// Get a channel from the cache.\n\tch := getCh()\n\n\t// Queue the channel for the callbacker.\n\ts.queue <- ch\n\n\t// Submit the task for execution.\n\ts.pool.Go(func() {\n\t\tdefer func() {\n\t\t\t// In the case of a panic from f, we don't want the callbacker to\n\t\t\t// starve waiting for a callback from this channel, so give it an\n\t\t\t// empty callback.\n\t\t\tif r := recover(); r != nil {\n\t\t\t\tch <- func() {}\n\t\t\t\tpanic(r)\n\t\t\t}\n\t\t}()\n\n\t\t// Run the task, sending its callback down this task's channel.\n\t\tcallback := f()\n\t\tch <- callback\n\t})\n}\n\n// Wait signals to the stream that all tasks have been submitted. Wait will\n// not return until all tasks and callbacks have been run.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (s *Stream) WithMaxGoroutines(n int) *Stream {\n\ts.pool.WithMaxGoroutines(n)\n\treturn s\n}\n\nfunc (s *Stream) init() {\n\ts.initOnce.Do(func() {\n\t\ts.queue = make(chan callbackCh, s.pool.MaxGoroutines()+1)\n\n\t\t// Start the callbacker.\n\t\ts.callbackerHandle.Go(s.callbacker)\n\t})\n}\n\n// callbacker is responsible for calling the returned callbacks in the order\n// they were submitted. There is only a single instance of callbacker running.\nfunc (s *Stream) callbacker() {\n\tvar panicCatcher panics.Catcher\n\tdefer panicCatcher.Repanic()\n\n\t// For every scheduled task, read that tasks channel from the queue.\n\tfor callbackCh := range s.queue {\n\t\t// Wait for the task to complete and get its callback from the channel.\n\t\tcallback := <-callbackCh\n\n\t\t// Execute the callback (with panic protection).\n\t\tif callback != nil {\n\t\t\tpanicCatcher.Try(callback)\n\t\t}\n\n\t\t// Return the channel to the pool of unused channels.\n\t\tputCh(callbackCh)\n\t}\n}\n\ntype callbackCh chan func()\n\nvar callbackChPool = sync.Pool{\n\tNew: func() any {\n\t\treturn make(callbackCh, 1)\n\t},\n}\n\nfunc getCh() callbackCh {\n\treturn callbackChPool.Get().(callbackCh)\n}\n\nfunc putCh(ch callbackCh) {\n\tcallbackChPool.Put(ch)\n}\n\n```\n# Function Signature: func (s *Stream) Wait() {\n# Natural Language Annotations: // Wait signals to the stream that all tasks have been submitted. Wait will\n// not return until all tasks and callbacks have been run.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (s *Stream) Wait() {\n\t// Wait for all tasks to complete.\n\ts.pool.Wait()\n\n\t// Close the queue to signal that all tasks have been submitted.\n\tclose(s.queue)\n\n\t// Wait for the callbacker to complete.\n\ts.callbackerHandle.Wait()\n}\n```", "```go\nfunc (s *Stream) Wait() {\n\t// Close the queue channel to signal that no more tasks will be submitted.\n\tclose(s.queue)\n\n\t// Wait for the callbacker to finish.\n\ts.callbackerHandle.Wait()\n}\n```", "```go\nfunc (s *Stream) Wait() {\n\t// Signal the callbacker to stop.\n\tclose(s.queue)\n\n\t// Wait for the callbacker to finish.\n\ts.callbackerHandle.Wait()\n}\n```\nThis function is used to signal to the stream that all tasks have been submitted. It will wait until all tasks and callbacks have been run. It first signals to the callbacker to stop by closing the queue. Then it waits for the callbacker to finish by calling the `Wait` method on the `callbackerHandle`."]}, "OpenGFW-analyzer/udp/internal/quic/packet_protector.go-UnProtect": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/OpenGFW/analyzer/udp/internal/quic/packet_protector.go:\n```\npackage quic\n\nimport (\n\t\"crypto\"\n\t\"crypto/aes\"\n\t\"crypto/cipher\"\n\t\"crypto/sha256\"\n\t\"crypto/tls\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\n\t\"golang.org/x/crypto/chacha20\"\n\t\"golang.org/x/crypto/chacha20poly1305\"\n\t\"golang.org/x/crypto/cryptobyte\"\n\t\"golang.org/x/crypto/hkdf\"\n)\n\n// NewProtectionKey creates a new ProtectionKey.\nfunc NewProtectionKey(suite uint16, secret []byte, v uint32) (*ProtectionKey, error) {\n\treturn newProtectionKey(suite, secret, v)\n}\n\n// NewInitialProtectionKey is like NewProtectionKey, but the returned protection key\n// is used for encrypt/decrypt Initial Packet only.\n//\n// See: https://datatracker.ietf.org/doc/html/draft-ietf-quic-tls-32#name-initial-secrets\nfunc NewInitialProtectionKey(secret []byte, v uint32) (*ProtectionKey, error) {\n\treturn NewProtectionKey(tls.TLS_AES_128_GCM_SHA256, secret, v)\n}\n\n// NewPacketProtector creates a new PacketProtector.\nfunc NewPacketProtector(key *ProtectionKey) *PacketProtector {\n\treturn &PacketProtector{key: key}\n}\n\n// PacketProtector is used for protecting a QUIC packet.\n//\n// See: https://www.rfc-editor.org/rfc/rfc9001.html#name-packet-protection\ntype PacketProtector struct {\n\tkey *ProtectionKey\n}\n\n// UnProtect decrypts a QUIC packet.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ProtectionKey is the key used to protect a QUIC packet.\ntype ProtectionKey struct {\n\taead             cipher.AEAD\n\theaderProtection func(sample []byte) (mask []byte)\n\tiv               []byte\n}\n\n// https://datatracker.ietf.org/doc/html/draft-ietf-quic-tls-32#name-aead-usage\n//\n// \"The 62 bits of the reconstructed QUIC packet number in network byte order are\n// left-padded with zeros to the size of the IV. The exclusive OR of the padded\n// packet number and the IV forms the AEAD nonce.\"\nfunc (pk *ProtectionKey) nonce(pn int64) []byte {\n\tnonce := make([]byte, len(pk.iv))\n\tbinary.BigEndian.PutUint64(nonce[len(nonce)-8:], uint64(pn))\n\tfor i := range pk.iv {\n\t\tnonce[i] ^= pk.iv[i]\n\t}\n\treturn nonce\n}\n\nfunc newProtectionKey(suite uint16, secret []byte, v uint32) (*ProtectionKey, error) {\n\tswitch suite {\n\tcase tls.TLS_AES_128_GCM_SHA256:\n\t\tkey := hkdfExpandLabel(crypto.SHA256.New, secret, keyLabel(v), nil, 16)\n\t\tc, err := aes.NewCipher(key)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\taead, err := cipher.NewGCM(c)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tiv := hkdfExpandLabel(crypto.SHA256.New, secret, ivLabel(v), nil, aead.NonceSize())\n\t\thpKey := hkdfExpandLabel(crypto.SHA256.New, secret, headerProtectionLabel(v), nil, 16)\n\t\thp, err := aes.NewCipher(hpKey)\n\t\tif err != nil {\n\t\t\tpanic(err)\n\t\t}\n\t\tk := &ProtectionKey{}\n\t\tk.aead = aead\n\t\t// https://datatracker.ietf.org/doc/html/draft-ietf-quic-tls-32#name-aes-based-header-protection\n\t\tk.headerProtection = func(sample []byte) []byte {\n\t\t\tmask := make([]byte, hp.BlockSize())\n\t\t\thp.Encrypt(mask, sample)\n\t\t\treturn mask\n\t\t}\n\t\tk.iv = iv\n\t\treturn k, nil\n\tcase tls.TLS_CHACHA20_POLY1305_SHA256:\n\t\tkey := hkdfExpandLabel(crypto.SHA256.New, secret, keyLabel(v), nil, chacha20poly1305.KeySize)\n\t\taead, err := chacha20poly1305.New(key)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tiv := hkdfExpandLabel(crypto.SHA256.New, secret, ivLabel(v), nil, aead.NonceSize())\n\t\thpKey := hkdfExpandLabel(sha256.New, secret, headerProtectionLabel(v), nil, chacha20.KeySize)\n\t\tk := &ProtectionKey{}\n\t\tk.aead = aead\n\t\t// https://datatracker.ietf.org/doc/html/draft-ietf-quic-tls-32#name-chacha20-based-header-prote\n\t\tk.headerProtection = func(sample []byte) []byte {\n\t\t\tnonce := sample[4:16]\n\t\t\tc, err := chacha20.NewUnauthenticatedCipher(hpKey, nonce)\n\t\t\tif err != nil {\n\t\t\t\tpanic(err)\n\t\t\t}\n\t\t\tc.SetCounter(binary.LittleEndian.Uint32(sample[:4]))\n\t\t\tmask := make([]byte, 5)\n\t\t\tc.XORKeyStream(mask, mask)\n\t\t\treturn mask\n\t\t}\n\t\tk.iv = iv\n\t\treturn k, nil\n\t}\n\treturn nil, errors.New(\"not supported cipher suite\")\n}\n\n// decodePacketNumber decode the packet number after header protection removed.\n//\n// See: https://datatracker.ietf.org/doc/html/draft-ietf-quic-transport-32#section-appendix.a\nfunc decodePacketNumber(largest, truncated int64, nbits uint8) int64 {\n\texpected := largest + 1\n\twin := int64(1 << (nbits * 8))\n\thwin := win / 2\n\tmask := win - 1\n\tcandidate := (expected &^ mask) | truncated\n\tswitch {\n\tcase candidate <= expected-hwin && candidate < (1<<62)-win:\n\t\treturn candidate + win\n\tcase candidate > expected+hwin && candidate >= win:\n\t\treturn candidate - win\n\t}\n\treturn candidate\n}\n\n// Copied from crypto/tls/key_schedule.go.\nfunc hkdfExpandLabel(hash func() hash.Hash, secret []byte, label string, context []byte, length int) []byte {\n\tvar hkdfLabel cryptobyte.Builder\n\thkdfLabel.AddUint16(uint16(length))\n\thkdfLabel.AddUint8LengthPrefixed(func(b *cryptobyte.Builder) {\n\t\tb.AddBytes([]byte(\"tls13 \"))\n\t\tb.AddBytes([]byte(label))\n\t})\n\thkdfLabel.AddUint8LengthPrefixed(func(b *cryptobyte.Builder) {\n\t\tb.AddBytes(context)\n\t})\n\tout := make([]byte, length)\n\tn, err := hkdf.Expand(hash, secret, hkdfLabel.BytesOrPanic()).Read(out)\n\tif err != nil || n != length {\n\t\tpanic(\"quic: HKDF-Expand-Label invocation failed unexpectedly\")\n\t}\n\treturn out\n}\n\n```\n# Function Signature: func (pp *PacketProtector) UnProtect(packet []byte, pnOffset, pnMax int64) ([]byte, error) {\n# Natural Language Annotations: // UnProtect decrypts a QUIC packet.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (pp *PacketProtector) UnProtect(packet []byte, pnOffset, pnMax int64) ([]byte, error) {\n\t// Check if the packet is long enough to contain a header protection sample,\n\t// a packet number, and a minimum length for the AEAD ciphertext.\n\tif len(packet) < pp.key.aead.NonceSize()+8+pp.key.aead.Overhead() {\n\t\treturn nil, fmt.Errorf(\"packet is too short\")\n\t}\n\n\t// Extract the sample from the packet.\n\tsample := packet[:pp.key.aead.NonceSize()+8]\n\tmask := pp.key.headerProtection(sample)\n\n\t// Apply the mask to the packet number.\n\tpacket[pnOffset] ^= mask[0] & 0x1f\n\tpacket[pnOffset+1] ^= mask[1] & 0xfc\n\n\t// Decrypt the packet.\n\taead := pp.key.aead\n\tnonce := pp.key.nonce(pnOffset)\n\tplaintext, err := aead.Open(nil, nonce, packet[pp.key.aead.NonceSize():], nil)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to decrypt: %w\", err)\n\t}\n\n\treturn plaintext, nil\n}\n```", "```go\nfunc (pp *PacketProtector) UnProtect(packet []byte, pnOffset, pnMax int64) ([]byte, error) {\n\tif len(packet) < pnOffset+4 {\n\t\treturn nil, fmt.Errorf(\"packet is too short\")\n\t}\n\tpn := binary.BigEndian.Uint32(packet[pnOffset : pnOffset+4])\n\tpn &^= 0x80000000 // clear the highest bit\n\tpn &= uint32(pnMax)\n\tnonce := pp.key.nonce(int64(pn))\n\tplaintext, err := pp.key.aead.Open(nil, nonce, packet[pnOffset+4:], nil)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to decrypt packet: %w\", err)\n\t}\n\treturn plaintext, nil\n}\n```", "```go\nfunc (pp *PacketProtector) UnProtect(packet []byte, pnOffset, pnMax int64) ([]byte, error) {\n\tif pp.key == nil {\n\t\treturn nil, errors.New(\"ProtectionKey is nil\")\n\t}\n\tif len(packet) <= pnOffset {\n\t\treturn nil, fmt.Errorf(\"packet length is too small: %d\", len(packet))\n\t}\n\tsample := packet[:pnOffset]\n\tmask := pp.key.headerProtection(sample)\n\tfor i := range mask {\n\t\tpacket[i] ^= mask[i]\n\t}\n\tpn := binary.BigEndian.Uint64(packet[pnOffset : pnOffset+8])\n\tdecrypted, err := pp.key.aead.Open(nil, pp.key.nonce(int64(pn)), packet[pnOffset+8:], nil)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn decrypted, nil\n}\n```"]}, "hatchet-pkg/encryption/local.go-NewLocalEncryption": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/hatchet/pkg/encryption/local.go:\n```\npackage encryption\n\nimport (\n\t\"bytes\"\n\t\"encoding/base64\"\n\t\"fmt\"\n\n\t\"github.com/tink-crypto/tink-go/aead\"\n\t\"github.com/tink-crypto/tink-go/insecurecleartextkeyset\"\n\t\"github.com/tink-crypto/tink-go/jwt\"\n\t\"github.com/tink-crypto/tink-go/keyset\"\n\t\"github.com/tink-crypto/tink-go/tink\"\n)\n\ntype localEncryptionService struct {\n\tkey                *aead.KMSEnvelopeAEAD\n\tprivateEc256Handle *keyset.Handle\n\tpublicEc256Handle  *keyset.Handle\n}\n\n// NewLocalEncryption creates a new local encryption service. keysetBytes is the raw keyset in\n// base64-encoded JSON format. This can be generated by calling hatchet-admin keyset create-local.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc GenerateLocalKeys() (masterKey []byte, privateEc256 []byte, publicEc256 []byte, err error) {\n\tmasterKey, masterHandle, err := generateLocalMasterKey()\n\n\tif err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\n\ta, err := aead.New(masterHandle)\n\n\tif err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\n\tprivateEc256, publicEc256, err = generateJWTKeysets(a)\n\n\tif err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\n\treturn masterKey, privateEc256, publicEc256, nil\n}\n\nfunc generateLocalMasterKey() ([]byte, *keyset.Handle, error) {\n\taeadTemplate := aead.AES256GCMKeyTemplate()\n\n\taes256GcmHandle, err := keyset.NewHandle(aeadTemplate)\n\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"failed to create new keyset handle with AES256GCM template: %w\", err)\n\t}\n\n\tbytes, err := insecureBytesFromHandle(aes256GcmHandle)\n\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"failed to get bytes from handle: %w\", err)\n\t}\n\n\treturn bytes, aes256GcmHandle, nil\n}\n\n// generateJWTKeysets creates the keysets for JWT signing and verification encrypted with the\n// masterKey. The masterKey can be from a remote KMS service or a local keyset.\nfunc generateJWTKeysets(masterKey tink.AEAD) (privateEc256 []byte, publicEc256 []byte, err error) {\n\tprivateHandle, err := keyset.NewHandle(jwt.ES256Template())\n\n\tif err != nil {\n\t\terr = fmt.Errorf(\"failed to create new keyset handle with ES256 template: %w\", err)\n\t\treturn\n\t}\n\n\tprivateEc256, err = bytesFromHandle(privateHandle, masterKey)\n\n\tif err != nil {\n\t\treturn\n\t}\n\n\tpublicHandle, err := privateHandle.Public()\n\n\tif err != nil {\n\t\terr = fmt.Errorf(\"failed to get public keyset: %w\", err)\n\t\treturn\n\t}\n\n\tpublicEc256, err = bytesFromHandle(publicHandle, masterKey)\n\n\tif err != nil {\n\t\treturn\n\t}\n\n\treturn\n}\n\n// bytesFromHandle returns the encrypted keyset in base64-encoded JSON format, encrypted with the\n// masterKey\nfunc bytesFromHandle(kh *keyset.Handle, masterKey tink.AEAD) ([]byte, error) {\n\tbuf := new(bytes.Buffer)\n\twriter := keyset.NewJSONWriter(buf)\n\terr := kh.Write(writer, masterKey)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to write keyset: %w\", err)\n\t}\n\n\t// base64-encode bytes\n\tkeysetBytes := make([]byte, base64.RawStdEncoding.EncodedLen(len(buf.Bytes())))\n\tbase64.RawStdEncoding.Encode(keysetBytes, buf.Bytes())\n\n\treturn keysetBytes, nil\n}\n\n// insecureBytesFromHandle returns the raw (unencrypted) keyset in base64-encoded JSON format.\nfunc insecureBytesFromHandle(kh *keyset.Handle) ([]byte, error) {\n\tbuf := new(bytes.Buffer)\n\twriter := keyset.NewJSONWriter(buf)\n\terr := insecurecleartextkeyset.Write(kh, writer)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to write keyset: %w\", err)\n\t}\n\n\t// base64-encode bytes\n\tkeysetBytes := make([]byte, base64.RawStdEncoding.EncodedLen(len(buf.Bytes())))\n\tbase64.RawStdEncoding.Encode(keysetBytes, buf.Bytes())\n\n\treturn keysetBytes, nil\n}\n\nfunc handleFromBytes(keysetBytes []byte, masterKey tink.AEAD) (*keyset.Handle, error) {\n\t// base64-decode bytes\n\tkeysetJsonBytes := make([]byte, base64.RawStdEncoding.DecodedLen(len(keysetBytes)))\n\t_, err := base64.RawStdEncoding.Decode(keysetJsonBytes, keysetBytes)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to decode keyset bytes: %w\", err)\n\t}\n\n\t// read keyset\n\thandle, err := keyset.Read(keyset.NewJSONReader(bytes.NewReader(keysetJsonBytes)), masterKey)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to read keyset: %w\", err)\n\t}\n\n\treturn handle, nil\n}\n\nfunc insecureHandleFromBytes(keysetBytes []byte) (*keyset.Handle, error) {\n\t// base64-decode bytes\n\tkeysetJsonBytes := make([]byte, base64.RawStdEncoding.DecodedLen(len(keysetBytes)))\n\t_, err := base64.RawStdEncoding.Decode(keysetJsonBytes, keysetBytes)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to decode keyset bytes: %w\", err)\n\t}\n\n\t// read keyset\n\thandle, err := insecurecleartextkeyset.Read(keyset.NewJSONReader(bytes.NewReader(keysetJsonBytes)))\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to read keyset: %w\", err)\n\t}\n\n\treturn handle, nil\n}\n\nfunc (svc *localEncryptionService) Encrypt(plaintext []byte, dataId string) ([]byte, error) {\n\treturn encrypt(svc.key, plaintext, dataId)\n}\n\nfunc (svc *localEncryptionService) Decrypt(ciphertext []byte, dataId string) ([]byte, error) {\n\treturn decrypt(svc.key, ciphertext, dataId)\n}\n\nfunc (svc *localEncryptionService) EncryptString(data string, dataId string) (string, error) {\n\tb, err := encrypt(svc.key, []byte(data), dataId)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn base64.StdEncoding.EncodeToString(b), nil\n}\n\nfunc (svc *localEncryptionService) DecryptString(data string, dataId string) (string, error) {\n\tplain, err := base64.StdEncoding.DecodeString(data)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tb, err := decrypt(svc.key, plain, dataId)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn string(b), nil\n}\n\nfunc (svc *localEncryptionService) GetPrivateJWTHandle() *keyset.Handle {\n\treturn svc.privateEc256Handle\n}\n\nfunc (svc *localEncryptionService) GetPublicJWTHandle() *keyset.Handle {\n\treturn svc.publicEc256Handle\n}\n\n```\n# Function Signature: func NewLocalEncryption(masterKey []byte, privateEc256 []byte, publicEc256 []byte) (*localEncryptionService, error) {\n# Natural Language Annotations: // NewLocalEncryption creates a new local encryption service. keysetBytes is the raw keyset in\n// base64-encoded JSON format. This can be generated by calling hatchet-admin keyset create-local.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewLocalEncryption(masterKey []byte, privateEc256 []byte, publicEc256 []byte) (*localEncryptionService, error) {\n\ta, err := aead.New(masterHandle)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create new AEAD with master handle: %w\", err)\n\t}\n\n\tprivateHandle, err := handleFromBytes(privateEc256, a)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create private handle from bytes: %w\", err)\n\t}\n\n\tpublicHandle, err := handleFromBytes(publicEc256, a)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create public handle from bytes: %w\", err)\n\t}\n\n\treturn &localEncryptionService{\n\t\tkey:                a,\n\t\tprivateEc256Handle: privateHandle,\n\t\tpublicEc256Handle:  publicHandle,\n\t}, nil\n}\n```", "```go\nfunc NewLocalEncryption(masterKey []byte, privateEc256 []byte, publicEc256 []byte) (*localEncryptionService, error) {\n    masterHandle, err := handleFromBytes(masterKey, nil)\n    if err != nil {\n        return nil, err\n    }\n\n    a, err := aead.New(masterHandle)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to create new AEAD with master keyset: %w\", err)\n    }\n\n    privateHandle, err := handleFromBytes(privateEc256, a)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to create new keyset handle with private EC256 template: %w\", err)\n    }\n\n    publicHandle, err := handleFromBytes(publicEc256, a)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to create new keyset handle with public EC256 template: %w\", err)\n    }\n\n    return &localEncryptionService{\n        key:                a,\n        privateEc256Handle: privateHandle,\n        publicEc256Handle:  publicHandle,\n    }, nil\n}\n```", "```go\nfunc NewLocalEncryption(masterKey []byte, privateEc256 []byte, publicEc256 []byte) (*localEncryptionService, error) {\n\tprivateHandle, err := handleFromBytes(privateEc256, masterKey)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create private handle: %w\", err)\n\t}\n\n\tpublicHandle, err := handleFromBytes(publicEc256, masterKey)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create public handle: %w\", err)\n\t}\n\n\tmasterHandle, err := insecureHandleFromBytes(masterKey)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create master handle: %w\", err)\n\t}\n\n\taead, err := aead.New(masterHandle)\n\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"failed to create AEAD: %w\", err)\n\t}\n\n\treturn &localEncryptionService{\n\t\tkey:                aead,\n\t\tprivateEc256Handle: privateHandle,\n\t\tpublicEc256Handle:  publicHandle,\n\t}, nil\n}\n```"]}, "hatchet-pkg/encryption/cloudkms.go-NewCloudKMSEncryption": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/hatchet/pkg/encryption/cloudkms.go:\n```\npackage encryption\n\nimport (\n\t\"context\"\n\t\"encoding/base64\"\n\t\"fmt\"\n\n\t\"github.com/tink-crypto/tink-go-gcpkms/integration/gcpkms\"\n\t\"github.com/tink-crypto/tink-go/aead\"\n\t\"github.com/tink-crypto/tink-go/core/registry\"\n\t\"github.com/tink-crypto/tink-go/keyset\"\n\t\"google.golang.org/api/option\"\n)\n\ntype cloudkmsEncryptionService struct {\n\tkey                *aead.KMSEnvelopeAEAD\n\tprivateEc256Handle *keyset.Handle\n\tpublicEc256Handle  *keyset.Handle\n}\n\n// NewCloudKMSEncryption creates a GCP CloudKMS-backed encryption service.\n\n\n\n\n\n\n\n\n\n\nfunc GenerateJWTKeysetsFromCloudKMS(keyUri string, credentialsJSON []byte) (privateEc256 []byte, publicEc256 []byte, err error) {\n\tclient, err := gcpkms.NewClientWithOptions(context.Background(), keyUri, option.WithCredentialsJSON(credentialsJSON))\n\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\treturn generateJWTKeysetsWithClient(keyUri, client)\n}\n\nfunc generateJWTKeysetsWithClient(keyUri string, client registry.KMSClient) (privateEc256 []byte, publicEc256 []byte, err error) {\n\tregistry.RegisterKMSClient(client)\n\n\tremote, err := client.GetAEAD(keyUri)\n\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\treturn generateJWTKeysets(remote)\n}\n\nfunc newWithClient(client registry.KMSClient, keyUri string, privateEc256, publicEc256 []byte) (*cloudkmsEncryptionService, error) {\n\tregistry.RegisterKMSClient(client)\n\n\tdek := aead.AES128CTRHMACSHA256KeyTemplate()\n\ttemplate, err := aead.CreateKMSEnvelopeAEADKeyTemplate(keyUri, dek)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// get the remote KEK from the client\n\tremote, err := client.GetAEAD(keyUri)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tenvelope := aead.NewKMSEnvelopeAEAD2(template, remote)\n\n\tif envelope == nil {\n\t\treturn nil, fmt.Errorf(\"failed to create envelope\")\n\t}\n\n\tprivateEc256Handle, err := handleFromBytes(privateEc256, remote)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tpublicEc256Handle, err := handleFromBytes(publicEc256, remote)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &cloudkmsEncryptionService{\n\t\tkey:                envelope,\n\t\tprivateEc256Handle: privateEc256Handle,\n\t\tpublicEc256Handle:  publicEc256Handle,\n\t}, nil\n}\n\nfunc (svc *cloudkmsEncryptionService) Encrypt(plaintext []byte, dataId string) ([]byte, error) {\n\treturn encrypt(svc.key, plaintext, dataId)\n}\n\nfunc (svc *cloudkmsEncryptionService) Decrypt(ciphertext []byte, dataId string) ([]byte, error) {\n\treturn decrypt(svc.key, ciphertext, dataId)\n}\n\nfunc (svc *cloudkmsEncryptionService) EncryptString(plaintext string, dataId string) (string, error) {\n\tb, err := encrypt(svc.key, []byte(plaintext), dataId)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn base64.StdEncoding.EncodeToString(b), nil\n}\n\nfunc (svc *cloudkmsEncryptionService) DecryptString(ciphertext string, dataId string) (string, error) {\n\tdecoded, err := base64.StdEncoding.DecodeString(ciphertext)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\tb, err := decrypt(svc.key, decoded, dataId)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\treturn string(b), nil\n}\n\nfunc (svc *cloudkmsEncryptionService) GetPrivateJWTHandle() *keyset.Handle {\n\treturn svc.privateEc256Handle\n}\n\nfunc (svc *cloudkmsEncryptionService) GetPublicJWTHandle() *keyset.Handle {\n\treturn svc.publicEc256Handle\n}\n\n```\n# Function Signature: func NewCloudKMSEncryption(keyUri string, credentialsJSON, privateEc256, publicEc256 []byte) (*cloudkmsEncryptionService, error) {\n# Natural Language Annotations: // NewCloudKMSEncryption creates a GCP CloudKMS-backed encryption service.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewCloudKMSEncryption(keyUri string, credentialsJSON, privateEc256, publicEc256 []byte) (*cloudkmsEncryptionService, error) {\n\tclient, err := gcpkms.NewClientWithOptions(context.Background(), keyUri, option.WithCredentialsJSON(credentialsJSON))\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn newWithClient(client, keyUri, privateEc256, publicEc256)\n}\n```", "```go\nfunc NewCloudKMSEncryption(keyUri string, credentialsJSON, privateEc256, publicEc256 []byte) (*cloudkmsEncryptionService, error) {\n\tclient, err := gcpkms.NewClientWithOptions(context.Background(), keyUri, option.WithCredentialsJSON(credentialsJSON))\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tregistry.RegisterKMSClient(client)\n\n\tdek := aead.AES128CTRHMACSHA256KeyTemplate()\n\ttemplate, err := aead.CreateKMSEnvelopeAEADKeyTemplate(keyUri, dek)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// get the remote KEK from the client\n\tremote, err := client.GetAEAD(keyUri)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tenvelope := aead.NewKMSEnvelopeAEAD2(template, remote)\n\n\tif envelope == nil {\n\t\treturn nil, fmt.Errorf(\"failed to create envelope\")\n\t}\n\n\tprivateEc256Handle, err := handleFromBytes(privateEc256, remote)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tpublicEc256Handle, err := handleFromBytes(publicEc256, remote)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &cloudkmsEncryptionService{\n\t\tkey:                envelope,\n\t\tprivateEc256Handle: privateEc256Handle,\n\t\tpublicEc256Handle:  publicEc256Handle,\n\t}, nil\n}\n```", "```go\nfunc NewCloudKMSEncryption(keyUri string, credentialsJSON, privateEc256, publicEc256 []byte) (*cloudkmsEncryptionService, error) {\n\tclient, err := gcpkms.NewClientWithOptions(context.Background(), keyUri, option.WithCredentialsJSON(credentialsJSON))\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn newWithClient(client, keyUri, privateEc256, publicEc256)\n}\n```\nThis function creates a new CloudKMSEncryptionService with the provided keyUri and credentialsJSON. It first creates a new client using the provided credentials, then uses that client to create and return a new cloudkmsEncryptionService."]}, "hatchet-internal/datautils/render.go-RenderTemplateFields": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/hatchet/internal/datautils/render.go:\n```\npackage datautils\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"text/template\"\n)\n\n// RenderTemplateFields recursively processes the input map, rendering any string fields using the data map.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func RenderTemplateFields(data map[string]interface{}, input map[string]interface{}) (map[string]interface{}, error) {\n# Natural Language Annotations: // RenderTemplateFields recursively processes the input map, rendering any string fields using the data map.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage datautils\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"text/template\"\n)\n\n// RenderTemplateFields recursively processes the input map, rendering any string fields using the data map.\nfunc RenderTemplateFields(data map[string]interface{}, input map[string]interface{}) (map[string]interface{}, error) {\n\tvar err error\n\tfor key, value := range input {\n\t\tswitch v := value.(type) {\n\t\tcase string:\n\t\t\tinput[key], err = RenderString(v, data)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\tcase map[string]interface{}:\n\t\t\tinput[key], err = RenderTemplateFields(data, v)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\tcase []interface{}:\n\t\t\tfor i, item := range v {\n\t\t\t\tswitch item := item.(type) {\n\t\t\t\tcase string:\n\t\t\t\t\tv[i], err = RenderString(item, data)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn nil, err\n\t\t\t\t\t}\n\t\t\t\tcase map[string]interface{}:\n\t\t\t\t\tv[i], err = RenderTemplateFields(data, item)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn nil, err\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tinput[key] = v\n\t\tdefault:\n\t\t\tcontinue\n\t\t}\n\t}\n\treturn input, nil\n}\n\nfunc RenderString(s string, data map[string]interface{}) (string, error) {\n\tt, err := template.New(\"\").Parse(s)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"failed to parse template: %v\", err)\n\t}\n\tvar buf bytes.Buffer\n\terr = t.Execute(&buf, data)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"failed to execute template: %v\", err)\n\t}\n\treturn buf.String(), nil\n}\n```", "```go\npackage datautils\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"text/template\"\n)\n\n// RenderTemplateFields recursively processes the input map, rendering any string fields using the data map.\nfunc RenderTemplateFields(data map[string]interface{}, input map[string]interface{}) (map[string]interface{}, error) {\n\toutput := make(map[string]interface{})\n\n\tfor key, value := range input {\n\t\tif reflect.ValueOf(value).Kind() == reflect.String {\n\t\t\ttmpl, err := template.New(key).Parse(value.(string))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"failed to parse template: %w\", err)\n\t\t\t}\n\n\t\t\tvar buf bytes.Buffer\n\t\t\terr = tmpl.Execute(&buf, data)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"failed to execute template: %w\", err)\n\t\t\t}\n\n\t\t\toutput[key] = buf.String()\n\t\t} else if reflect.ValueOf(value).Kind() == reflect.Map {\n\t\t\trendered, err := RenderTemplateFields(data, value.(map[string]interface{}))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\toutput[key] = rendered\n\t\t} else {\n\t\t\toutput[key] = value\n\t\t}\n\t}\n\n\treturn output, nil\n}\n```", "```go\npackage datautils\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"reflect\"\n\t\"text/template\"\n)\n\n// RenderTemplateFields recursively processes the input map, rendering any string fields using the data map.\nfunc RenderTemplateFields(data map[string]interface{}, input map[string]interface{}) (map[string]interface{}, error) {\n\toutput := make(map[string]interface{})\n\n\tfor key, value := range input {\n\t\tif reflect.TypeOf(value).Kind() == reflect.String {\n\t\t\ttmpl, err := template.New(key).Parse(value.(string))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"error parsing template for key '%s': %v\", key, err)\n\t\t\t}\n\t\t\tvar buf bytes.Buffer\n\t\t\terr = tmpl.Execute(&buf, data)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"error executing template for key '%s': %v\", key, err)\n\t\t\t}\n\t\t\toutput[key] = buf.String()\n\t\t} else if reflect.TypeOf(value).Kind() == reflect.Map {\n\t\t\trenderedMap, err := RenderTemplateFields(data, value.(map[string]interface{}))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\toutput[key] = renderedMap\n\t\t} else {\n\t\t\toutput[key] = value\n\t\t}\n\t}\n\n\treturn output, nil\n}\n```"]}, "mox-ratelimit/ratelimit.go-Reset": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/ratelimit/ratelimit.go:\n```\n// Package ratelimit provides a simple window-based rate limiter.\npackage ratelimit\n\nimport (\n\t\"net\"\n\t\"sync\"\n\t\"time\"\n)\n\n// Limiter is a simple rate limiter with one or more fixed windows, e.g. the\n// last minute/hour/day/week, working on three classes/subnets of an IP.\ntype Limiter struct {\n\tsync.Mutex\n\tWindowLimits []WindowLimit\n\tipmasked     [3][16]byte\n}\n\n// WindowLimit holds counters for one window, with limits for each IP class/subnet.\ntype WindowLimit struct {\n\tWindow time.Duration\n\tLimits [3]int64 // For \"ipmasked1\" through \"ipmasked3\".\n\tTime   uint32   // Time/Window.\n\tCounts map[struct {\n\t\tIndex    uint8\n\t\tIPMasked [16]byte\n\t}]int64\n}\n\n// Add attempts to consume \"n\" items from the rate limiter. If the total for this\n// key and this interval would exceed limit, \"n\" is not counted and false is\n// returned. If now represents a different time interval, all counts are reset.\nfunc (l *Limiter) Add(ip net.IP, tm time.Time, n int64) bool {\n\treturn l.checkAdd(true, ip, tm, n)\n}\n\n// CanAdd returns if n could be added to the limiter.\nfunc (l *Limiter) CanAdd(ip net.IP, tm time.Time, n int64) bool {\n\treturn l.checkAdd(false, ip, tm, n)\n}\n\nfunc (l *Limiter) checkAdd(add bool, ip net.IP, tm time.Time, n int64) bool {\n\tl.Lock()\n\tdefer l.Unlock()\n\n\t// First check.\n\tfor i, pl := range l.WindowLimits {\n\t\tt := uint32(tm.UnixNano() / int64(pl.Window))\n\n\t\tif t > pl.Time || pl.Counts == nil {\n\t\t\tl.WindowLimits[i].Time = t\n\t\t\tpl.Counts = map[struct {\n\t\t\t\tIndex    uint8\n\t\t\t\tIPMasked [16]byte\n\t\t\t}]int64{} // Used below.\n\t\t\tl.WindowLimits[i].Counts = pl.Counts\n\t\t}\n\n\t\tfor j := 0; j < 3; j++ {\n\t\t\tif i == 0 {\n\t\t\t\tl.ipmasked[j] = l.maskIP(j, ip)\n\t\t\t}\n\n\t\t\tv := pl.Counts[struct {\n\t\t\t\tIndex    uint8\n\t\t\t\tIPMasked [16]byte\n\t\t\t}{uint8(j), l.ipmasked[j]}]\n\t\t\tif v+n > pl.Limits[j] {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\tif !add {\n\t\treturn true\n\t}\n\t// Finally record.\n\tfor _, pl := range l.WindowLimits {\n\t\tfor j := 0; j < 3; j++ {\n\t\t\tpl.Counts[struct {\n\t\t\t\tIndex    uint8\n\t\t\t\tIPMasked [16]byte\n\t\t\t}{uint8(j), l.ipmasked[j]}] += n\n\t\t}\n\t}\n\treturn true\n}\n\n// Reset sets the counter to 0 for key and ip, and subtracts from the ipmasked counts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (l *Limiter) maskIP(i int, ip net.IP) [16]byte {\n\tisv4 := ip.To4() != nil\n\n\tvar ipmasked net.IP\n\tif isv4 {\n\t\tswitch i {\n\t\tcase 0:\n\t\t\tipmasked = ip\n\t\tcase 1:\n\t\t\tipmasked = ip.Mask(net.CIDRMask(26, 32))\n\t\tcase 2:\n\t\t\tipmasked = ip.Mask(net.CIDRMask(21, 32))\n\t\tdefault:\n\t\t\tpanic(\"missing case for maskip ipv4\")\n\t\t}\n\t} else {\n\t\tswitch i {\n\t\tcase 0:\n\t\t\tipmasked = ip.Mask(net.CIDRMask(64, 128))\n\t\tcase 1:\n\t\t\tipmasked = ip.Mask(net.CIDRMask(48, 128))\n\t\tcase 2:\n\t\t\tipmasked = ip.Mask(net.CIDRMask(32, 128))\n\t\tdefault:\n\t\t\tpanic(\"missing case for masking ipv6\")\n\t\t}\n\t}\n\treturn *(*[16]byte)(ipmasked.To16())\n}\n\n```\n# Function Signature: func (l *Limiter) Reset(ip net.IP, tm time.Time) {\n# Natural Language Annotations: // Reset sets the counter to 0 for key and ip, and subtracts from the ipmasked counts.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (l *Limiter) Reset(ip net.IP, tm time.Time) {\n\tl.Lock()\n\tdefer l.Unlock()\n\t// First check.\n\tfor i, pl := range l.WindowLimits {\n\t\tt := uint32(tm.UnixNano() / int64(pl.Window))\n\t\tif t > pl.Time {\n\t\t\tl.WindowLimits[i].Time = t\n\t\t\tpl.Counts = map[struct {\n\t\t\t\tIndex    uint8\n\t\t\t\tIPMasked [16]byte\n\t\t\t}]int64{} // Used below.\n\t\t\tl.WindowLimits[i].Counts = pl.Counts\n\t\t}\n\t}\n\t// Reset counts.\n\tfor _, pl := range l.WindowLimits {\n\t\tfor j := 0; j < 3; j++ {\n\t\t\tpl.Counts[struct {\n\t\t\t\tIndex    uint8\n\t\t\t\tIPMasked [16]byte\n\t\t\t}{uint8(j), l.ipmasked[j]}] = 0\n\t\t}\n\t}\n}\n```", "```go\nfunc (l *Limiter) Reset(ip net.IP, tm time.Time) {\n\tl.Lock()\n\tdefer l.Unlock()\n\n\t// Reset all counts for the given IP.\n\tfor _, pl := range l.WindowLimits {\n\t\tt := uint32(tm.UnixNano() / int64(pl.Window))\n\n\t\tif t > pl.Time || pl.Counts == nil {\n\t\t\tl.WindowLimits[i].Time = t\n\t\t\tpl.Counts = map[struct {\n\t\t\t\tIndex    uint8\n\t\t\t\tIPMasked [16]byte\n\t\t\t}]int64{} // Used below.\n\t\t\tl.WindowLimits[i].Counts = pl.Counts\n\t\t}\n\n\t\tfor j := 0; j < 3; j++ {\n\t\t\tif i == 0 {\n\t\t\t\tl.ipmasked[j] = l.maskIP(j, ip)\n\t\t\t}\n\n\t\t\tpl.Counts[struct {\n\t\t\t\tIndex    uint8\n\t\t\t\tIPMasked [16]byte\n\t\t\t}{uint8(j), l.ipmasked[j]}] = 0\n\t\t}\n\t}\n}\n```", "```go\nfunc (l *Limiter) Reset(ip net.IP, tm time.Time) {\n\tl.Lock()\n\tdefer l.Unlock()\n\n\t// First check.\n\tfor i, pl := range l.WindowLimits {\n\t\tt := uint32(tm.UnixNano() / int64(pl.Window))\n\n\t\tif t > pl.Time || pl.Counts == nil {\n\t\t\tl.WindowLimits[i].Time = t\n\t\t\tpl.Counts = map[struct {\n\t\t\t\tIndex    uint8\n\t\t\t\tIPMasked [16]byte\n\t\t\t}]int64{} // Used below.\n\t\t\tl.WindowLimits[i].Counts = pl.Counts\n\t\t}\n\t}\n\n\t// Reset counts.\n\tfor _, pl := range l.WindowLimits {\n\t\tfor j := 0; j < 3; j++ {\n\t\t\tpl.Counts[struct {\n\t\t\t\tIndex    uint8\n\t\t\t\tIPMasked [16]byte\n\t\t\t}{uint8(j), l.ipmasked[j]}] = 0\n\t\t}\n\t}\n}\n```"]}, "mox-dns/dns.go-ParseDomainLax": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dns/dns.go:\n```\n// Package dns helps parse internationalized domain names (IDNA), canonicalize\n// names and provides a strict and metrics-keeping logging DNS resolver.\npackage dns\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"golang.org/x/net/idna\"\n\n\t\"github.com/mjl-/adns\"\n)\n\n// Pedantic enables stricter parsing.\nvar Pedantic bool\n\nvar (\n\terrTrailingDot = errors.New(\"dns name has trailing dot\")\n\terrUnderscore  = errors.New(\"domain name with underscore\")\n\terrIDNA        = errors.New(\"idna\")\n)\n\n// Domain is a domain name, with one or more labels, with at least an ASCII\n// representation, and for IDNA non-ASCII domains a unicode representation.\n// The ASCII string must be used for DNS lookups. The strings do not have a\n// trailing dot. When using with StrictResolver, add the trailing dot.\ntype Domain struct {\n\t// A non-unicode domain, e.g. with A-labels (xn--...) or NR-LDH (non-reserved\n\t// letters/digits/hyphens) labels. Always in lower case. No trailing dot.\n\tASCII string\n\n\t// Name as U-labels, in Unicode NFC. Empty if this is an ASCII-only domain. No\n\t// trailing dot.\n\tUnicode string\n}\n\n// Name returns the unicode name if set, otherwise the ASCII name.\nfunc (d Domain) Name() string {\n\tif d.Unicode != \"\" {\n\t\treturn d.Unicode\n\t}\n\treturn d.ASCII\n}\n\n// XName is like Name, but only returns a unicode name when utf8 is true.\nfunc (d Domain) XName(utf8 bool) string {\n\tif utf8 && d.Unicode != \"\" {\n\t\treturn d.Unicode\n\t}\n\treturn d.ASCII\n}\n\n// ASCIIExtra returns the ASCII version of the domain name if smtputf8 is true and\n// this is a unicode domain name. Otherwise it returns an empty string.\n//\n// This function is used to add the punycode name in a comment to SMTP message\n// headers, e.g. Received and Authentication-Results.\nfunc (d Domain) ASCIIExtra(smtputf8 bool) string {\n\tif smtputf8 && d.Unicode != \"\" {\n\t\treturn d.ASCII\n\t}\n\treturn \"\"\n}\n\n// Strings returns a human-readable string.\n// For IDNA names, the string contains both the unicode and ASCII name.\nfunc (d Domain) String() string {\n\treturn d.LogString()\n}\n\n// LogString returns a domain for logging.\n// For IDNA names, the string is the slash-separated Unicode and ASCII name.\n// For ASCII-only domain names, just the ASCII string is returned.\nfunc (d Domain) LogString() string {\n\tif d.Unicode == \"\" {\n\t\treturn d.ASCII\n\t}\n\treturn d.Unicode + \"/\" + d.ASCII\n}\n\n// IsZero returns if this is an empty Domain.\nfunc (d Domain) IsZero() bool {\n\treturn d == Domain{}\n}\n\n// ParseDomain parses a domain name that can consist of ASCII-only labels or U\n// labels (unicode).\n// Names are IDN-canonicalized and lower-cased.\n// Characters in unicode can be replaced by equivalents. E.g. \"\u24c7\" to \"r\". This\n// means you should only compare parsed domain names, never unparsed strings\n// directly.\nfunc ParseDomain(s string) (Domain, error) {\n\tif strings.HasSuffix(s, \".\") {\n\t\treturn Domain{}, errTrailingDot\n\t}\n\n\tascii, err := idna.Lookup.ToASCII(s)\n\tif err != nil {\n\t\treturn Domain{}, fmt.Errorf(\"%w: to ascii: %v\", errIDNA, err)\n\t}\n\tunicode, err := idna.Lookup.ToUnicode(s)\n\tif err != nil {\n\t\treturn Domain{}, fmt.Errorf(\"%w: to unicode: %w\", errIDNA, err)\n\t}\n\t// todo: should we cause errors for unicode domains that were not in\n\t// canonical form? we are now accepting all kinds of obscure spellings\n\t// for even a basic ASCII domain name.\n\t// Also see https://daniel.haxx.se/blog/2022/12/14/idn-is-crazy/\n\tif ascii == unicode {\n\t\treturn Domain{ascii, \"\"}, nil\n\t}\n\treturn Domain{ascii, unicode}, nil\n}\n\n// ParseDomainLax parses a domain like ParseDomain, but allows labels with\n// underscores if the entire domain name is ASCII-only non-IDNA and Pedantic mode\n// is not enabled. Used for interoperability, e.g. domains may specify MX\n// targets with underscores.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// IsNotFound returns whether an error is an adns.DNSError with IsNotFound set.\n// IsNotFound means the requested type does not exist for the given domain (a\n// nodata or nxdomain response). It doesn't not necessarily mean no other types for\n// that name exist.\n//\n// A DNS server can respond to a lookup with an error \"nxdomain\" to indicate a\n// name does not exist (at all), or with a success status with an empty list.\n// The adns resolver (just like the Go resolver) returns an IsNotFound error for\n// both cases, there is no need to explicitly check for zero entries.\nfunc IsNotFound(err error) bool {\n\tvar dnsErr *adns.DNSError\n\treturn err != nil && errors.As(err, &dnsErr) && dnsErr.IsNotFound\n}\n\n```\n# Function Signature: func ParseDomainLax(s string) (Domain, error) {\n# Natural Language Annotations: // ParseDomainLax parses a domain like ParseDomain, but allows labels with\n// underscores if the entire domain name is ASCII-only non-IDNA and Pedantic mode\n// is not enabled. Used for interoperability, e.g. domains may specify MX\n// targets with underscores.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ParseDomainLax(s string) (Domain, error) {\n\tif strings.HasSuffix(s, \".\") {\n\t\treturn Domain{}, errTrailingDot\n\t}\n\n\t// Check if the domain is ASCII only\n\tasciiOnly := true\n\tfor _, c := range s {\n\t\tif c > unicode.MaxASCII {\n\t\t\tasciiOnly = false\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// If domain is ASCII only and Pedantic mode is not enabled, allow labels with underscores\n\tif asciiOnly && !Pedantic {\n\t\t// Replace underscores with hyphens\n\t\ts = strings.ReplaceAll(s, \"_\", \"-\")\n\t}\n\n\tascii, err := idna.Lookup.ToASCII(s)\n\tif err != nil {\n\t\treturn Domain{}, fmt.Errorf(\"%w: to ascii: %v\", errIDNA, err)\n\t}\n\tunicode, err := idna.Lookup.ToUnicode(s)\n\tif err != nil {\n\t\treturn Domain{}, fmt.Errorf(\"%w: to unicode: %w\", errIDNA, err)\n\t}\n\t// todo: should we cause errors for unicode domains that were not in\n\t// canonical form? we are now accepting all kinds of obscure spellings\n\t// for even a basic ASCII domain name.\n\t// Also see https://daniel.haxx.se/blog/2022/12/14/idn-is-crazy/\n\tif ascii == unicode {\n\t\treturn Domain{ascii, \"\"}, nil\n\t}\n\treturn Domain{ascii, unicode}, nil\n}\n```", "```go\nfunc ParseDomainLax(s string) (Domain, error) {\n\tif strings.HasSuffix(s, \".\") {\n\t\treturn Domain{}, errTrailingDot\n\t}\n\n\tascii, err := idna.Lookup.ToASCII(s)\n\tif err != nil {\n\t\treturn Domain{}, fmt.Errorf(\"%w: to ascii: %v\", errIDNA, err)\n\t}\n\tunicode, err := idna.Lookup.ToUnicode(s)\n\tif err != nil {\n\t\treturn Domain{}, fmt.Errorf(\"%w: to unicode: %w\", errIDNA, err)\n\t}\n\tif ascii != unicode && !Pedantic {\n\t\ts = ascii\n\t\tunicode = \"\"\n\t}\n\treturn Domain{s, unicode}, nil\n}\n```", "```go\nfunc ParseDomainLax(s string) (Domain, error) {\n\tif strings.HasSuffix(s, \".\") {\n\t\treturn Domain{}, errTrailingDot\n\t}\n\n\tascii, err := idna.Lookup.ToASCII(s)\n\tif err != nil {\n\t\treturn Domain{}, fmt.Errorf(\"%w: to ascii: %v\", errIDNA, err)\n\t}\n\tif Pedantic && !strings.Contains(ascii, \"_\") {\n\t\tunicode, err := idna.Lookup.ToUnicode(s)\n\t\tif err != nil {\n\t\t\treturn Domain{}, fmt.Errorf(\"%w: to unicode: %w\", errIDNA, err)\n\t\t}\n\t\tif ascii == unicode {\n\t\t\treturn Domain{ascii, \"\"}, nil\n\t\t}\n\t\treturn Domain{ascii, unicode}, nil\n\t}\n\treturn Domain{ascii, \"\"}, nil\n}\n```"]}, "mox-dmarcdb/eval.go-AddEvaluation": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dmarcdb/eval.go:\n```\npackage dmarcdb\n\n// Sending TLS reports and DMARC reports is very similar. See ../dmarcdb/eval.go:/similar and ../tlsrptsend/send.go:/similar.\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"mime/multipart\"\n\t\"net/textproto\"\n\t\"net/url\"\n\t\"os\"\n\t\"runtime/debug\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/exp/maps\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/dkim\"\n\t\"github.com/mjl-/mox/dmarc\"\n\t\"github.com/mjl-/mox/dmarcrpt\"\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/metrics\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/moxvar\"\n\t\"github.com/mjl-/mox/publicsuffix\"\n\t\"github.com/mjl-/mox/queue\"\n\t\"github.com/mjl-/mox/smtp\"\n\t\"github.com/mjl-/mox/store\"\n)\n\nvar (\n\tmetricReport = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_queued_total\",\n\t\t\tHelp: \"Total messages with DMARC aggregate/error reports queued.\",\n\t\t},\n\t)\n\tmetricReportError = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_error_total\",\n\t\t\tHelp: \"Total errors while composing or queueing DMARC aggregate/error reports.\",\n\t\t},\n\t)\n)\n\nvar (\n\tEvalDBTypes = []any{Evaluation{}, SuppressAddress{}} // Types stored in DB.\n\t// Exported for backups. For incoming deliveries the SMTP server adds evaluations\n\t// to the database. Every hour, a goroutine wakes up that gathers evaluations from\n\t// the last hour(s), sends a report, and removes the evaluations from the database.\n\tEvalDB *bstore.DB\n)\n\n// Evaluation is the result of an evaluation of a DMARC policy, to be included\n// in a DMARC report.\ntype Evaluation struct {\n\tID int64\n\n\t// Domain where DMARC policy was found, could be the organizational domain while\n\t// evaluation was for a subdomain. Unicode. Same as domain found in\n\t// PolicyPublished. A separate field for its index.\n\tPolicyDomain string `bstore:\"index\"`\n\n\t// Time of evaluation, determines which report (covering whole hours) this\n\t// evaluation will be included in.\n\tEvaluated time.Time `bstore:\"default now\"`\n\n\t// If optional, this evaluation is not a reason to send a DMARC report, but it will\n\t// be included when a report is sent due to other non-optional evaluations. Set for\n\t// evaluations of incoming DMARC reports. We don't want such deliveries causing us to\n\t// send a report, or we would keep exchanging reporting messages forever. Also set\n\t// for when evaluation is a DMARC reject for domains we haven't positively\n\t// interacted with, to prevent being used to flood an unsuspecting domain with\n\t// reports.\n\tOptional bool\n\n\t// Effective aggregate reporting interval in hours. Between 1 and 24, rounded up\n\t// from seconds from policy to first number that can divide 24.\n\tIntervalHours int\n\n\t// \"rua\" in DMARC record, we only store evaluations for records with aggregate reporting addresses, so always non-empty.\n\tAddresses []string\n\n\t// Policy used for evaluation. We don't store the \"fo\" field for failure reporting\n\t// options, since we don't send failure reports for individual messages.\n\tPolicyPublished dmarcrpt.PolicyPublished\n\n\t// For \"row\" in a report record.\n\tSourceIP        string\n\tDisposition     dmarcrpt.Disposition\n\tAlignedDKIMPass bool\n\tAlignedSPFPass  bool\n\tOverrideReasons []dmarcrpt.PolicyOverrideReason\n\n\t// For \"identifiers\" in a report record.\n\tEnvelopeTo   string\n\tEnvelopeFrom string\n\tHeaderFrom   string\n\n\t// For \"auth_results\" in a report record.\n\tDKIMResults []dmarcrpt.DKIMAuthResult\n\tSPFResults  []dmarcrpt.SPFAuthResult\n}\n\n// SuppressAddress is a reporting address for which outgoing DMARC reports\n// will be suppressed for a period.\ntype SuppressAddress struct {\n\tID               int64\n\tInserted         time.Time `bstore:\"default now\"`\n\tReportingAddress string    `bstore:\"unique\"`\n\tUntil            time.Time `bstore:\"nonzero\"`\n\tComment          string\n}\n\nvar dmarcResults = map[bool]dmarcrpt.DMARCResult{\n\tfalse: dmarcrpt.DMARCFail,\n\ttrue:  dmarcrpt.DMARCPass,\n}\n\n// ReportRecord turns an evaluation into a record that can be included in a\n// report.\nfunc (e Evaluation) ReportRecord(count int) dmarcrpt.ReportRecord {\n\treturn dmarcrpt.ReportRecord{\n\t\tRow: dmarcrpt.Row{\n\t\t\tSourceIP: e.SourceIP,\n\t\t\tCount:    count,\n\t\t\tPolicyEvaluated: dmarcrpt.PolicyEvaluated{\n\t\t\t\tDisposition: e.Disposition,\n\t\t\t\tDKIM:        dmarcResults[e.AlignedDKIMPass],\n\t\t\t\tSPF:         dmarcResults[e.AlignedSPFPass],\n\t\t\t\tReasons:     e.OverrideReasons,\n\t\t\t},\n\t\t},\n\t\tIdentifiers: dmarcrpt.Identifiers{\n\t\t\tEnvelopeTo:   e.EnvelopeTo,\n\t\t\tEnvelopeFrom: e.EnvelopeFrom,\n\t\t\tHeaderFrom:   e.HeaderFrom,\n\t\t},\n\t\tAuthResults: dmarcrpt.AuthResults{\n\t\t\tDKIM: e.DKIMResults,\n\t\t\tSPF:  e.SPFResults,\n\t\t},\n\t}\n}\n\nvar intervalOpts = []int{24, 12, 8, 6, 4, 3, 2}\n\nfunc intervalHours(seconds int) int {\n\thours := (seconds + 3600 - 1) / 3600\n\tfor _, opt := range intervalOpts {\n\t\tif hours >= opt {\n\t\t\treturn opt\n\t\t}\n\t}\n\treturn 1\n}\n\n// AddEvaluation adds the result of a DMARC evaluation for an incoming message\n// to the database.\n//\n// AddEvaluation sets Evaluation.IntervalHours based on\n// aggregateReportingIntervalSeconds.\n\n\n\n\n\n\n\n// Evaluations returns all evaluations in the database.\nfunc Evaluations(ctx context.Context) ([]Evaluation, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.SortAsc(\"Evaluated\")\n\treturn q.List()\n}\n\n// EvaluationStat summarizes stored evaluations, for inclusion in an upcoming\n// aggregate report, for a domain.\ntype EvaluationStat struct {\n\tDomain       dns.Domain\n\tDispositions []string\n\tCount        int\n\tSendReport   bool\n}\n\n// EvaluationStats returns evaluation counts and report-sending status per domain.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// EvaluationsDomain returns all evaluations for a domain.\n\n\n\n\n\n\n\n// RemoveEvaluationsDomain removes evaluations for domain so they won't be sent in\n// an aggregate report.\n\n\n\n\n\n\n\nvar jitterRand = mox.NewPseudoRand()\n\n// time to sleep until next whole hour t, replaced by tests.\n// Jitter so we don't cause load at exactly whole hours, other processes may\n// already be doing that.\nvar jitteredTimeUntil = func(t time.Time) time.Duration {\n\treturn time.Until(t.Add(time.Duration(30+jitterRand.Intn(60)) * time.Second))\n}\n\n// Start launches a goroutine that wakes up at each whole hour (plus jitter) and\n// sends DMARC reports to domains that requested them.\nfunc Start(resolver dns.Resolver) {\n\tgo func() {\n\t\tlog := mlog.New(\"dmarcdb\", nil)\n\n\t\tdefer func() {\n\t\t\t// In case of panic don't take the whole program down.\n\t\t\tx := recover()\n\t\t\tif x != nil {\n\t\t\t\tlog.Error(\"recover from panic\", slog.Any(\"panic\", x))\n\t\t\t\tdebug.PrintStack()\n\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t}\n\t\t}()\n\n\t\ttimer := time.NewTimer(time.Hour)\n\t\tdefer timer.Stop()\n\n\t\tctx := mox.Shutdown\n\n\t\tfor {\n\t\t\tnow := time.Now()\n\t\t\tnextEnd := nextWholeHour(now)\n\t\t\ttimer.Reset(jitteredTimeUntil(nextEnd))\n\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tlog.Info(\"dmarc aggregate report sender shutting down\")\n\t\t\t\treturn\n\t\t\tcase <-timer.C:\n\t\t\t}\n\n\t\t\t// Gather report intervals we want to process now. Multiples of hours that can\n\t\t\t// divide 24, starting from UTC.\n\t\t\t// ../rfc/7489:1750\n\t\t\tutchour := nextEnd.UTC().Hour()\n\t\t\tif utchour == 0 {\n\t\t\t\tutchour = 24\n\t\t\t}\n\t\t\tintervals := []int{}\n\t\t\tfor _, ival := range intervalOpts {\n\t\t\t\tif ival*(utchour/ival) == utchour {\n\t\t\t\t\tintervals = append(intervals, ival)\n\t\t\t\t}\n\t\t\t}\n\t\t\tintervals = append(intervals, 1)\n\n\t\t\t// Remove evaluations older than 48 hours (2 reports with the default and maximum\n\t\t\t// 24 hour interval). They should have been processed by now. We may have kept them\n\t\t\t// during temporary errors, but persistent temporary errors shouldn't fill up our\n\t\t\t// database. This also cleans up evaluations that were all optional for a domain.\n\t\t\t_, err := bstore.QueryDB[Evaluation](ctx, EvalDB).FilterLess(\"Evaluated\", nextEnd.Add(-48*time.Hour)).Delete()\n\t\t\tlog.Check(err, \"removing stale dmarc evaluations from database\")\n\n\t\t\tclog := log.WithCid(mox.Cid())\n\t\t\tclog.Info(\"sending dmarc aggregate reports\", slog.Time(\"end\", nextEnd.UTC()), slog.Any(\"intervals\", intervals))\n\t\t\tif err := sendReports(ctx, clog, resolver, EvalDB, nextEnd, intervals); err != nil {\n\t\t\t\tclog.Errorx(\"sending dmarc aggregate reports\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t} else {\n\t\t\t\tclog.Info(\"finished sending dmarc aggregate reports\")\n\t\t\t}\n\t\t}\n\t}()\n}\n\nfunc nextWholeHour(now time.Time) time.Time {\n\tt := now\n\tt = t.Add(time.Hour)\n\treturn time.Date(t.Year(), t.Month(), t.Day(), t.Hour(), 0, 0, 0, t.Location())\n}\n\n// We don't send reports at full speed. In the future, we could try to stretch out\n// reports a bit smarter. E.g. over 5 minutes with some minimum interval, and\n// perhaps faster and in parallel when there are lots of reports. Perhaps also\n// depending on reporting interval (faster for 1h, slower for 24h).\n// Replaced by tests.\nvar sleepBetween = func(ctx context.Context, between time.Duration) (ok bool) {\n\tt := time.NewTimer(between)\n\tselect {\n\tcase <-ctx.Done():\n\t\tt.Stop()\n\t\treturn false\n\tcase <-t.C:\n\t\treturn true\n\t}\n}\n\n// sendReports gathers all policy domains that have evaluations that should\n// receive a DMARC report and sends a report to each.\nfunc sendReports(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, intervals []int) error {\n\tivals := make([]any, len(intervals))\n\tfor i, v := range intervals {\n\t\tivals[i] = v\n\t}\n\n\tdestDomains := map[string]bool{}\n\n\t// Gather all domains that we plan to send to.\n\tnsend := 0\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterEqual(\"IntervalHours\", ivals...)\n\terr := q.ForEach(func(e Evaluation) error {\n\t\tif !e.Optional && !destDomains[e.PolicyPublished.Domain] {\n\t\t\tnsend++\n\t\t}\n\t\tdestDomains[e.PolicyPublished.Domain] = destDomains[e.PolicyPublished.Domain] || !e.Optional\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"looking for domains to send reports to: %v\", err)\n\t}\n\n\tvar wg sync.WaitGroup\n\n\t// Sleep in between sending reports. We process hourly, and spread the reports over\n\t// the hour, but with max 5 minute interval.\n\tbetween := 45 * time.Minute\n\tif nsend > 0 {\n\t\tbetween /= time.Duration(nsend)\n\t}\n\tif between > 5*time.Minute {\n\t\tbetween = 5 * time.Minute\n\t}\n\n\t// Attempt to send report to each domain.\n\tn := 0\n\tfor d, send := range destDomains {\n\t\t// Cleanup evaluations for domain with only optionals.\n\t\tif !send {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, d)\n\t\t\tcontinue\n\t\t}\n\n\t\tif n > 0 {\n\t\t\tif ok := sleepBetween(ctx, between); !ok {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t\tn++\n\n\t\t// Send in goroutine, so a slow process doesn't block progress.\n\t\twg.Add(1)\n\t\tgo func(domain string) {\n\t\t\tdefer func() {\n\t\t\t\t// In case of panic don't take the whole program down.\n\t\t\t\tx := recover()\n\t\t\t\tif x != nil {\n\t\t\t\t\tlog.Error(\"unhandled panic in dmarcdb sendReports\", slog.Any(\"panic\", x))\n\t\t\t\t\tdebug.PrintStack()\n\t\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t\t}\n\t\t\t}()\n\t\t\tdefer wg.Done()\n\n\t\t\trlog := log.WithCid(mox.Cid()).With(slog.Any(\"domain\", domain))\n\t\t\trlog.Info(\"sending dmarc report\")\n\t\t\tif _, err := sendReportDomain(ctx, rlog, resolver, db, endTime, domain); err != nil {\n\t\t\t\trlog.Errorx(\"sending dmarc aggregate report to domain\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t}\n\t\t}(d)\n\t}\n\n\twg.Wait()\n\n\treturn nil\n}\n\ntype recipient struct {\n\taddress smtp.Address\n\tmaxSize uint64\n}\n\nfunc parseRecipient(log mlog.Log, uri dmarc.URI) (r recipient, ok bool) {\n\tlog = log.With(slog.Any(\"uri\", uri.Address))\n\n\tu, err := url.Parse(uri.Address)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\tif !strings.EqualFold(u.Scheme, \"mailto\") {\n\t\tlog.Debug(\"skipping unrecognized scheme in dmarc record rua value\")\n\t\treturn r, false\n\t}\n\taddr, err := smtp.ParseAddress(u.Opaque)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing mailto uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\n\tr = recipient{addr, uri.MaxSize}\n\t// ../rfc/7489:1197\n\tswitch uri.Unit {\n\tcase \"k\", \"K\":\n\t\tr.maxSize *= 1024\n\tcase \"m\", \"M\":\n\t\tr.maxSize *= 1024 * 1024\n\tcase \"g\", \"G\":\n\t\tr.maxSize *= 1024 * 1024 * 1024\n\tcase \"t\", \"T\":\n\t\t// Oh yeah, terabyte-sized reports!\n\t\tr.maxSize *= 1024 * 1024 * 1024 * 1024\n\tcase \"\":\n\tdefault:\n\t\tlog.Debug(\"unrecognized max size unit in dmarc record rua value\", slog.String(\"unit\", uri.Unit))\n\t\treturn r, false\n\t}\n\n\treturn r, true\n}\n\nfunc removeEvaluations(ctx context.Context, log mlog.Log, db *bstore.DB, endTime time.Time, domain string) {\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterNonzero(Evaluation{PolicyDomain: domain})\n\t_, err := q.Delete()\n\tlog.Check(err, \"removing evaluations after processing for dmarc aggregate report\")\n}\n\n// replaceable for testing.\nvar queueAdd = queue.Add\n\nfunc sendReportDomain(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, domain string) (cleanup bool, rerr error) {\n\tdom, err := dns.ParseDomain(domain)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"parsing domain for sending reports: %v\", err)\n\t}\n\n\t// We'll cleanup records by default.\n\tcleanup = true\n\t// If we encounter a temporary error we cancel cleanup of evaluations on error.\n\ttempError := false\n\n\tdefer func() {\n\t\tif !cleanup || tempError {\n\t\t\tlog.Debug(\"not cleaning up evaluations after attempting to send dmarc aggregate report\")\n\t\t} else {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, domain)\n\t\t}\n\t}()\n\n\t// We're going to build up this report.\n\treport := dmarcrpt.Feedback{\n\t\tVersion: \"1.0\",\n\t\tReportMetadata: dmarcrpt.ReportMetadata{\n\t\t\tOrgName: mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\tEmail:   \"postmaster@\" + mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\t// ReportID and DateRange are set after we've seen evaluations.\n\t\t\t// Errors is filled below when we encounter problems.\n\t\t},\n\t\t// We'll fill the records below.\n\t\tRecords: []dmarcrpt.ReportRecord{},\n\t}\n\n\tvar errors []string // For report.ReportMetaData.Errors\n\n\t// Check if we should be sending a report at all: if there are rua URIs in the\n\t// current DMARC record. The interval may have changed too, but we'll flush out our\n\t// evaluations regardless. We always use the latest DMARC record when sending, but\n\t// we'll lump all policies of the last interval into one report.\n\t// ../rfc/7489:1714\n\tstatus, _, record, _, _, err := dmarc.Lookup(ctx, log.Logger, resolver, dom)\n\tif err != nil {\n\t\t// todo future: we could perhaps still send this report, assuming the values we know. in case of temporary error, we could also schedule again regardless of next interval hour (we would now only retry a 24h-interval report after 24h passed).\n\t\t// Remove records unless it was a temporary error. We'll try again next round.\n\t\tcleanup = status != dmarc.StatusTemperror\n\t\treturn cleanup, fmt.Errorf(\"looking up current dmarc record for reporting address: %v\", err)\n\t}\n\n\tvar recipients []recipient\n\n\t// Gather all aggregate reporting addresses to try to send to. We'll start with\n\t// those in the initial DMARC record, but will follow external reporting addresses\n\t// and possibly update the list.\n\tfor _, uri := range record.AggregateReportAddresses {\n\t\tr, ok := parseRecipient(log, uri)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Check if domain of rua recipient has the same organizational domain as for the\n\t\t// evaluations. If not, we need to verify we are allowed to send.\n\t\truaOrgDom := publicsuffix.Lookup(ctx, log.Logger, r.address.Domain)\n\t\tevalOrgDom := publicsuffix.Lookup(ctx, log.Logger, dom)\n\n\t\tif ruaOrgDom == evalOrgDom {\n\t\t\trecipients = append(recipients, r)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Verify and follow addresses in other organizational domain through\n\t\t// <policydomain>._report._dmarc.<host> lookup.\n\t\t// ../rfc/7489:1556\n\t\taccepts, status, records, _, _, err := dmarc.LookupExternalReportsAccepted(ctx, log.Logger, resolver, evalOrgDom, r.address.Domain)\n\t\tlog.Debugx(\"checking if rua address with different organization domain has opted into receiving dmarc reports\", err,\n\t\t\tslog.Any(\"policydomain\", evalOrgDom),\n\t\t\tslog.Any(\"destinationdomain\", r.address.Domain),\n\t\t\tslog.Bool(\"accepts\", accepts),\n\t\t\tslog.Any(\"status\", status))\n\t\tif status == dmarc.StatusTemperror {\n\t\t\t// With a temporary error, we'll try to get the report the delivered anyway,\n\t\t\t// perhaps there are multiple recipients.\n\t\t\t// ../rfc/7489:1578\n\t\t\ttempError = true\n\t\t\terrors = append(errors, \"temporary error checking authorization for report delegation to external address\")\n\t\t}\n\t\tif !accepts {\n\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain that does not opt-in to receiving dmarc records through _report dmarc record\", r.address))\n\t\t\tcontinue\n\t\t}\n\n\t\t// We can follow a _report DMARC DNS record once. In that record, a domain may\n\t\t// specify alternative addresses that we should send reports to instead. Such\n\t\t// alternative address(es) must have the same host. If not, we ignore the new\n\t\t// value. Behaviour for multiple records and/or multiple new addresses is\n\t\t// underspecified. We'll replace an address with one or more new addresses, and\n\t\t// keep the original if there was no candidate (which covers the case of invalid\n\t\t// alternative addresses and no new address specified).\n\t\t// ../rfc/7489:1600\n\t\tfoundReplacement := false\n\t\trlog := log.With(slog.Any(\"followedaddress\", uri.Address))\n\t\tfor _, record := range records {\n\t\t\tfor _, exturi := range record.AggregateReportAddresses {\n\t\t\t\textr, ok := parseRecipient(rlog, exturi)\n\t\t\t\tif !ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif extr.address.Domain != r.address.Domain {\n\t\t\t\t\trlog.Debug(\"rua address in external _report dmarc record has different host than initial dmarc record, ignoring new name\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain with a replacement address %s with different host\", r.address, extr.address))\n\t\t\t\t} else {\n\t\t\t\t\trlog.Debug(\"using replacement rua address from external _report dmarc record\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\tfoundReplacement = true\n\t\t\t\t\trecipients = append(recipients, extr)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif !foundReplacement {\n\t\t\trecipients = append(recipients, r)\n\t\t}\n\t}\n\n\tif len(recipients) == 0 {\n\t\t// No reports requested, perfectly fine, no work to do for us.\n\t\tlog.Debug(\"no aggregate reporting addresses configured\")\n\t\treturn true, nil\n\t}\n\n\t// We count idential records. Can be common with a domain sending quite some email.\n\t// Though less if the sending domain has many IPs. In the future, we may want to\n\t// remove some details from records so we can aggregate them into fewer rows.\n\ttype recordCount struct {\n\t\tdmarcrpt.ReportRecord\n\t\tcount int\n\t}\n\tcounts := map[string]recordCount{}\n\n\n```\n# Function Signature: func AddEvaluation(ctx context.Context, aggregateReportingIntervalSeconds int, e *Evaluation) error {\n# Natural Language Annotations: // AddEvaluation adds the result of a DMARC evaluation for an incoming message\n// to the database.\n//\n// AddEvaluation sets Evaluation.IntervalHours based on\n// aggregateReportingIntervalSeconds.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc AddEvaluation(ctx context.Context, aggregateReportingIntervalSeconds int, e *Evaluation) error {\n\t// Calculate the interval hours and set it in the evaluation\n\te.IntervalHours = intervalHours(aggregateReportingIntervalSeconds)\n\n\t// Add the evaluation to the database\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\treturn q.Insert(e)\n}\n```", "```go\nfunc AddEvaluation(ctx context.Context, aggregateReportingIntervalSeconds int, e *Evaluation) error {\n\te.IntervalHours = intervalHours(aggregateReportingIntervalSeconds)\n\n\t_, err := bstore.QueryDB[Evaluation](ctx, EvalDB).Insert(e)\n\treturn err\n}\n```", "```go\npackage dmarcdb\n\nimport (\n\t\"context\"\n\t\"time\"\n)\n\nfunc AddEvaluation(ctx context.Context, aggregateReportingIntervalSeconds int, e *Evaluation) error {\n\te.IntervalHours = intervalHours(aggregateReportingIntervalSeconds)\n\t_, err := EvalDB.Insert(ctx, e)\n\treturn err\n}\n```"]}, "mox-dmarcdb/eval.go-EvaluationStats": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dmarcdb/eval.go:\n```\npackage dmarcdb\n\n// Sending TLS reports and DMARC reports is very similar. See ../dmarcdb/eval.go:/similar and ../tlsrptsend/send.go:/similar.\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"mime/multipart\"\n\t\"net/textproto\"\n\t\"net/url\"\n\t\"os\"\n\t\"runtime/debug\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/exp/maps\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/dkim\"\n\t\"github.com/mjl-/mox/dmarc\"\n\t\"github.com/mjl-/mox/dmarcrpt\"\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/metrics\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/moxvar\"\n\t\"github.com/mjl-/mox/publicsuffix\"\n\t\"github.com/mjl-/mox/queue\"\n\t\"github.com/mjl-/mox/smtp\"\n\t\"github.com/mjl-/mox/store\"\n)\n\nvar (\n\tmetricReport = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_queued_total\",\n\t\t\tHelp: \"Total messages with DMARC aggregate/error reports queued.\",\n\t\t},\n\t)\n\tmetricReportError = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_error_total\",\n\t\t\tHelp: \"Total errors while composing or queueing DMARC aggregate/error reports.\",\n\t\t},\n\t)\n)\n\nvar (\n\tEvalDBTypes = []any{Evaluation{}, SuppressAddress{}} // Types stored in DB.\n\t// Exported for backups. For incoming deliveries the SMTP server adds evaluations\n\t// to the database. Every hour, a goroutine wakes up that gathers evaluations from\n\t// the last hour(s), sends a report, and removes the evaluations from the database.\n\tEvalDB *bstore.DB\n)\n\n// Evaluation is the result of an evaluation of a DMARC policy, to be included\n// in a DMARC report.\ntype Evaluation struct {\n\tID int64\n\n\t// Domain where DMARC policy was found, could be the organizational domain while\n\t// evaluation was for a subdomain. Unicode. Same as domain found in\n\t// PolicyPublished. A separate field for its index.\n\tPolicyDomain string `bstore:\"index\"`\n\n\t// Time of evaluation, determines which report (covering whole hours) this\n\t// evaluation will be included in.\n\tEvaluated time.Time `bstore:\"default now\"`\n\n\t// If optional, this evaluation is not a reason to send a DMARC report, but it will\n\t// be included when a report is sent due to other non-optional evaluations. Set for\n\t// evaluations of incoming DMARC reports. We don't want such deliveries causing us to\n\t// send a report, or we would keep exchanging reporting messages forever. Also set\n\t// for when evaluation is a DMARC reject for domains we haven't positively\n\t// interacted with, to prevent being used to flood an unsuspecting domain with\n\t// reports.\n\tOptional bool\n\n\t// Effective aggregate reporting interval in hours. Between 1 and 24, rounded up\n\t// from seconds from policy to first number that can divide 24.\n\tIntervalHours int\n\n\t// \"rua\" in DMARC record, we only store evaluations for records with aggregate reporting addresses, so always non-empty.\n\tAddresses []string\n\n\t// Policy used for evaluation. We don't store the \"fo\" field for failure reporting\n\t// options, since we don't send failure reports for individual messages.\n\tPolicyPublished dmarcrpt.PolicyPublished\n\n\t// For \"row\" in a report record.\n\tSourceIP        string\n\tDisposition     dmarcrpt.Disposition\n\tAlignedDKIMPass bool\n\tAlignedSPFPass  bool\n\tOverrideReasons []dmarcrpt.PolicyOverrideReason\n\n\t// For \"identifiers\" in a report record.\n\tEnvelopeTo   string\n\tEnvelopeFrom string\n\tHeaderFrom   string\n\n\t// For \"auth_results\" in a report record.\n\tDKIMResults []dmarcrpt.DKIMAuthResult\n\tSPFResults  []dmarcrpt.SPFAuthResult\n}\n\n// SuppressAddress is a reporting address for which outgoing DMARC reports\n// will be suppressed for a period.\ntype SuppressAddress struct {\n\tID               int64\n\tInserted         time.Time `bstore:\"default now\"`\n\tReportingAddress string    `bstore:\"unique\"`\n\tUntil            time.Time `bstore:\"nonzero\"`\n\tComment          string\n}\n\nvar dmarcResults = map[bool]dmarcrpt.DMARCResult{\n\tfalse: dmarcrpt.DMARCFail,\n\ttrue:  dmarcrpt.DMARCPass,\n}\n\n// ReportRecord turns an evaluation into a record that can be included in a\n// report.\nfunc (e Evaluation) ReportRecord(count int) dmarcrpt.ReportRecord {\n\treturn dmarcrpt.ReportRecord{\n\t\tRow: dmarcrpt.Row{\n\t\t\tSourceIP: e.SourceIP,\n\t\t\tCount:    count,\n\t\t\tPolicyEvaluated: dmarcrpt.PolicyEvaluated{\n\t\t\t\tDisposition: e.Disposition,\n\t\t\t\tDKIM:        dmarcResults[e.AlignedDKIMPass],\n\t\t\t\tSPF:         dmarcResults[e.AlignedSPFPass],\n\t\t\t\tReasons:     e.OverrideReasons,\n\t\t\t},\n\t\t},\n\t\tIdentifiers: dmarcrpt.Identifiers{\n\t\t\tEnvelopeTo:   e.EnvelopeTo,\n\t\t\tEnvelopeFrom: e.EnvelopeFrom,\n\t\t\tHeaderFrom:   e.HeaderFrom,\n\t\t},\n\t\tAuthResults: dmarcrpt.AuthResults{\n\t\t\tDKIM: e.DKIMResults,\n\t\t\tSPF:  e.SPFResults,\n\t\t},\n\t}\n}\n\nvar intervalOpts = []int{24, 12, 8, 6, 4, 3, 2}\n\nfunc intervalHours(seconds int) int {\n\thours := (seconds + 3600 - 1) / 3600\n\tfor _, opt := range intervalOpts {\n\t\tif hours >= opt {\n\t\t\treturn opt\n\t\t}\n\t}\n\treturn 1\n}\n\n// AddEvaluation adds the result of a DMARC evaluation for an incoming message\n// to the database.\n//\n// AddEvaluation sets Evaluation.IntervalHours based on\n// aggregateReportingIntervalSeconds.\n\n\n\n\n\n\n\n// Evaluations returns all evaluations in the database.\nfunc Evaluations(ctx context.Context) ([]Evaluation, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.SortAsc(\"Evaluated\")\n\treturn q.List()\n}\n\n// EvaluationStat summarizes stored evaluations, for inclusion in an upcoming\n// aggregate report, for a domain.\ntype EvaluationStat struct {\n\tDomain       dns.Domain\n\tDispositions []string\n\tCount        int\n\tSendReport   bool\n}\n\n// EvaluationStats returns evaluation counts and report-sending status per domain.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// EvaluationsDomain returns all evaluations for a domain.\n\n\n\n\n\n\n\n// RemoveEvaluationsDomain removes evaluations for domain so they won't be sent in\n// an aggregate report.\n\n\n\n\n\n\n\nvar jitterRand = mox.NewPseudoRand()\n\n// time to sleep until next whole hour t, replaced by tests.\n// Jitter so we don't cause load at exactly whole hours, other processes may\n// already be doing that.\nvar jitteredTimeUntil = func(t time.Time) time.Duration {\n\treturn time.Until(t.Add(time.Duration(30+jitterRand.Intn(60)) * time.Second))\n}\n\n// Start launches a goroutine that wakes up at each whole hour (plus jitter) and\n// sends DMARC reports to domains that requested them.\nfunc Start(resolver dns.Resolver) {\n\tgo func() {\n\t\tlog := mlog.New(\"dmarcdb\", nil)\n\n\t\tdefer func() {\n\t\t\t// In case of panic don't take the whole program down.\n\t\t\tx := recover()\n\t\t\tif x != nil {\n\t\t\t\tlog.Error(\"recover from panic\", slog.Any(\"panic\", x))\n\t\t\t\tdebug.PrintStack()\n\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t}\n\t\t}()\n\n\t\ttimer := time.NewTimer(time.Hour)\n\t\tdefer timer.Stop()\n\n\t\tctx := mox.Shutdown\n\n\t\tfor {\n\t\t\tnow := time.Now()\n\t\t\tnextEnd := nextWholeHour(now)\n\t\t\ttimer.Reset(jitteredTimeUntil(nextEnd))\n\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tlog.Info(\"dmarc aggregate report sender shutting down\")\n\t\t\t\treturn\n\t\t\tcase <-timer.C:\n\t\t\t}\n\n\t\t\t// Gather report intervals we want to process now. Multiples of hours that can\n\t\t\t// divide 24, starting from UTC.\n\t\t\t// ../rfc/7489:1750\n\t\t\tutchour := nextEnd.UTC().Hour()\n\t\t\tif utchour == 0 {\n\t\t\t\tutchour = 24\n\t\t\t}\n\t\t\tintervals := []int{}\n\t\t\tfor _, ival := range intervalOpts {\n\t\t\t\tif ival*(utchour/ival) == utchour {\n\t\t\t\t\tintervals = append(intervals, ival)\n\t\t\t\t}\n\t\t\t}\n\t\t\tintervals = append(intervals, 1)\n\n\t\t\t// Remove evaluations older than 48 hours (2 reports with the default and maximum\n\t\t\t// 24 hour interval). They should have been processed by now. We may have kept them\n\t\t\t// during temporary errors, but persistent temporary errors shouldn't fill up our\n\t\t\t// database. This also cleans up evaluations that were all optional for a domain.\n\t\t\t_, err := bstore.QueryDB[Evaluation](ctx, EvalDB).FilterLess(\"Evaluated\", nextEnd.Add(-48*time.Hour)).Delete()\n\t\t\tlog.Check(err, \"removing stale dmarc evaluations from database\")\n\n\t\t\tclog := log.WithCid(mox.Cid())\n\t\t\tclog.Info(\"sending dmarc aggregate reports\", slog.Time(\"end\", nextEnd.UTC()), slog.Any(\"intervals\", intervals))\n\t\t\tif err := sendReports(ctx, clog, resolver, EvalDB, nextEnd, intervals); err != nil {\n\t\t\t\tclog.Errorx(\"sending dmarc aggregate reports\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t} else {\n\t\t\t\tclog.Info(\"finished sending dmarc aggregate reports\")\n\t\t\t}\n\t\t}\n\t}()\n}\n\nfunc nextWholeHour(now time.Time) time.Time {\n\tt := now\n\tt = t.Add(time.Hour)\n\treturn time.Date(t.Year(), t.Month(), t.Day(), t.Hour(), 0, 0, 0, t.Location())\n}\n\n// We don't send reports at full speed. In the future, we could try to stretch out\n// reports a bit smarter. E.g. over 5 minutes with some minimum interval, and\n// perhaps faster and in parallel when there are lots of reports. Perhaps also\n// depending on reporting interval (faster for 1h, slower for 24h).\n// Replaced by tests.\nvar sleepBetween = func(ctx context.Context, between time.Duration) (ok bool) {\n\tt := time.NewTimer(between)\n\tselect {\n\tcase <-ctx.Done():\n\t\tt.Stop()\n\t\treturn false\n\tcase <-t.C:\n\t\treturn true\n\t}\n}\n\n// sendReports gathers all policy domains that have evaluations that should\n// receive a DMARC report and sends a report to each.\nfunc sendReports(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, intervals []int) error {\n\tivals := make([]any, len(intervals))\n\tfor i, v := range intervals {\n\t\tivals[i] = v\n\t}\n\n\tdestDomains := map[string]bool{}\n\n\t// Gather all domains that we plan to send to.\n\tnsend := 0\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterEqual(\"IntervalHours\", ivals...)\n\terr := q.ForEach(func(e Evaluation) error {\n\t\tif !e.Optional && !destDomains[e.PolicyPublished.Domain] {\n\t\t\tnsend++\n\t\t}\n\t\tdestDomains[e.PolicyPublished.Domain] = destDomains[e.PolicyPublished.Domain] || !e.Optional\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"looking for domains to send reports to: %v\", err)\n\t}\n\n\tvar wg sync.WaitGroup\n\n\t// Sleep in between sending reports. We process hourly, and spread the reports over\n\t// the hour, but with max 5 minute interval.\n\tbetween := 45 * time.Minute\n\tif nsend > 0 {\n\t\tbetween /= time.Duration(nsend)\n\t}\n\tif between > 5*time.Minute {\n\t\tbetween = 5 * time.Minute\n\t}\n\n\t// Attempt to send report to each domain.\n\tn := 0\n\tfor d, send := range destDomains {\n\t\t// Cleanup evaluations for domain with only optionals.\n\t\tif !send {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, d)\n\t\t\tcontinue\n\t\t}\n\n\t\tif n > 0 {\n\t\t\tif ok := sleepBetween(ctx, between); !ok {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t\tn++\n\n\t\t// Send in goroutine, so a slow process doesn't block progress.\n\t\twg.Add(1)\n\t\tgo func(domain string) {\n\t\t\tdefer func() {\n\t\t\t\t// In case of panic don't take the whole program down.\n\t\t\t\tx := recover()\n\t\t\t\tif x != nil {\n\t\t\t\t\tlog.Error(\"unhandled panic in dmarcdb sendReports\", slog.Any(\"panic\", x))\n\t\t\t\t\tdebug.PrintStack()\n\t\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t\t}\n\t\t\t}()\n\t\t\tdefer wg.Done()\n\n\t\t\trlog := log.WithCid(mox.Cid()).With(slog.Any(\"domain\", domain))\n\t\t\trlog.Info(\"sending dmarc report\")\n\t\t\tif _, err := sendReportDomain(ctx, rlog, resolver, db, endTime, domain); err != nil {\n\t\t\t\trlog.Errorx(\"sending dmarc aggregate report to domain\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t}\n\t\t}(d)\n\t}\n\n\twg.Wait()\n\n\treturn nil\n}\n\ntype recipient struct {\n\taddress smtp.Address\n\tmaxSize uint64\n}\n\nfunc parseRecipient(log mlog.Log, uri dmarc.URI) (r recipient, ok bool) {\n\tlog = log.With(slog.Any(\"uri\", uri.Address))\n\n\tu, err := url.Parse(uri.Address)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\tif !strings.EqualFold(u.Scheme, \"mailto\") {\n\t\tlog.Debug(\"skipping unrecognized scheme in dmarc record rua value\")\n\t\treturn r, false\n\t}\n\taddr, err := smtp.ParseAddress(u.Opaque)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing mailto uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\n\tr = recipient{addr, uri.MaxSize}\n\t// ../rfc/7489:1197\n\tswitch uri.Unit {\n\tcase \"k\", \"K\":\n\t\tr.maxSize *= 1024\n\tcase \"m\", \"M\":\n\t\tr.maxSize *= 1024 * 1024\n\tcase \"g\", \"G\":\n\t\tr.maxSize *= 1024 * 1024 * 1024\n\tcase \"t\", \"T\":\n\t\t// Oh yeah, terabyte-sized reports!\n\t\tr.maxSize *= 1024 * 1024 * 1024 * 1024\n\tcase \"\":\n\tdefault:\n\t\tlog.Debug(\"unrecognized max size unit in dmarc record rua value\", slog.String(\"unit\", uri.Unit))\n\t\treturn r, false\n\t}\n\n\treturn r, true\n}\n\nfunc removeEvaluations(ctx context.Context, log mlog.Log, db *bstore.DB, endTime time.Time, domain string) {\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterNonzero(Evaluation{PolicyDomain: domain})\n\t_, err := q.Delete()\n\tlog.Check(err, \"removing evaluations after processing for dmarc aggregate report\")\n}\n\n// replaceable for testing.\nvar queueAdd = queue.Add\n\nfunc sendReportDomain(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, domain string) (cleanup bool, rerr error) {\n\tdom, err := dns.ParseDomain(domain)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"parsing domain for sending reports: %v\", err)\n\t}\n\n\t// We'll cleanup records by default.\n\tcleanup = true\n\t// If we encounter a temporary error we cancel cleanup of evaluations on error.\n\ttempError := false\n\n\tdefer func() {\n\t\tif !cleanup || tempError {\n\t\t\tlog.Debug(\"not cleaning up evaluations after attempting to send dmarc aggregate report\")\n\t\t} else {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, domain)\n\t\t}\n\t}()\n\n\t// We're going to build up this report.\n\treport := dmarcrpt.Feedback{\n\t\tVersion: \"1.0\",\n\t\tReportMetadata: dmarcrpt.ReportMetadata{\n\t\t\tOrgName: mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\tEmail:   \"postmaster@\" + mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\t// ReportID and DateRange are set after we've seen evaluations.\n\t\t\t// Errors is filled below when we encounter problems.\n\t\t},\n\t\t// We'll fill the records below.\n\t\tRecords: []dmarcrpt.ReportRecord{},\n\t}\n\n\tvar errors []string // For report.ReportMetaData.Errors\n\n\t// Check if we should be sending a report at all: if there are rua URIs in the\n\t// current DMARC record. The interval may have changed too, but we'll flush out our\n\t// evaluations regardless. We always use the latest DMARC record when sending, but\n\t// we'll lump all policies of the last interval into one report.\n\t// ../rfc/7489:1714\n\tstatus, _, record, _, _, err := dmarc.Lookup(ctx, log.Logger, resolver, dom)\n\tif err != nil {\n\t\t// todo future: we could perhaps still send this report, assuming the values we know. in case of temporary error, we could also schedule again regardless of next interval hour (we would now only retry a 24h-interval report after 24h passed).\n\t\t// Remove records unless it was a temporary error. We'll try again next round.\n\t\tcleanup = status != dmarc.StatusTemperror\n\t\treturn cleanup, fmt.Errorf(\"looking up current dmarc record for reporting address: %v\", err)\n\t}\n\n\tvar recipients []recipient\n\n\t// Gather all aggregate reporting addresses to try to send to. We'll start with\n\t// those in the initial DMARC record, but will follow external reporting addresses\n\t// and possibly update the list.\n\tfor _, uri := range record.AggregateReportAddresses {\n\t\tr, ok := parseRecipient(log, uri)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Check if domain of rua recipient has the same organizational domain as for the\n\t\t// evaluations. If not, we need to verify we are allowed to send.\n\t\truaOrgDom := publicsuffix.Lookup(ctx, log.Logger, r.address.Domain)\n\t\tevalOrgDom := publicsuffix.Lookup(ctx, log.Logger, dom)\n\n\t\tif ruaOrgDom == evalOrgDom {\n\t\t\trecipients = append(recipients, r)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Verify and follow addresses in other organizational domain through\n\t\t// <policydomain>._report._dmarc.<host> lookup.\n\t\t// ../rfc/7489:1556\n\t\taccepts, status, records, _, _, err := dmarc.LookupExternalReportsAccepted(ctx, log.Logger, resolver, evalOrgDom, r.address.Domain)\n\t\tlog.Debugx(\"checking if rua address with different organization domain has opted into receiving dmarc reports\", err,\n\t\t\tslog.Any(\"policydomain\", evalOrgDom),\n\t\t\tslog.Any(\"destinationdomain\", r.address.Domain),\n\t\t\tslog.Bool(\"accepts\", accepts),\n\t\t\tslog.Any(\"status\", status))\n\t\tif status == dmarc.StatusTemperror {\n\t\t\t// With a temporary error, we'll try to get the report the delivered anyway,\n\t\t\t// perhaps there are multiple recipients.\n\t\t\t// ../rfc/7489:1578\n\t\t\ttempError = true\n\t\t\terrors = append(errors, \"temporary error checking authorization for report delegation to external address\")\n\t\t}\n\t\tif !accepts {\n\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain that does not opt-in to receiving dmarc records through _report dmarc record\", r.address))\n\t\t\tcontinue\n\t\t}\n\n\t\t// We can follow a _report DMARC DNS record once. In that record, a domain may\n\t\t// specify alternative addresses that we should send reports to instead. Such\n\t\t// alternative address(es) must have the same host. If not, we ignore the new\n\t\t// value. Behaviour for multiple records and/or multiple new addresses is\n\t\t// underspecified. We'll replace an address with one or more new addresses, and\n\t\t// keep the original if there was no candidate (which covers the case of invalid\n\t\t// alternative addresses and no new address specified).\n\t\t// ../rfc/7489:1600\n\t\tfoundReplacement := false\n\t\trlog := log.With(slog.Any(\"followedaddress\", uri.Address))\n\t\tfor _, record := range records {\n\t\t\tfor _, exturi := range record.AggregateReportAddresses {\n\t\t\t\textr, ok := parseRecipient(rlog, exturi)\n\t\t\t\tif !ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif extr.address.Domain != r.address.Domain {\n\t\t\t\t\trlog.Debug(\"rua address in external _report dmarc record has different host than initial dmarc record, ignoring new name\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain with a replacement address %s with different host\", r.address, extr.address))\n\t\t\t\t} else {\n\t\t\t\t\trlog.Debug(\"using replacement rua address from external _report dmarc record\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\tfoundReplacement = true\n\t\t\t\t\trecipients = append(recipients, extr)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif !foundReplacement {\n\t\t\trecipients = append(recipients, r)\n\t\t}\n\t}\n\n\tif len(recipients) == 0 {\n\t\t// No reports requested, perfectly fine, no work to do for us.\n\t\tlog.Debug(\"no aggregate reporting addresses configured\")\n\t\treturn true, nil\n\t}\n\n\t// We count idential records. Can be common with a domain sending quite some email.\n\t// Though less if the sending domain has many IPs. In the future, we may want to\n\t// remove some details from records so we can aggregate them into fewer rows.\n\ttype recordCount struct {\n\t\tdmarcrpt.ReportRecord\n\t\tcount int\n\t}\n\tcounts := map[string]recordCount{}\n\n\n```\n# Function Signature: func EvaluationStats(ctx context.Context) (map[string]EvaluationStat, error) {\n# Natural Language Annotations: // EvaluationStats returns evaluation counts and report-sending status per domain.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc EvaluationStats(ctx context.Context) (map[string]EvaluationStat, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.SortAsc(\"Evaluated\")\n\tevals, err := q.List()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tstats := make(map[string]EvaluationStat)\n\tfor _, eval := range evals {\n\t\tstat, ok := stats[eval.PolicyDomain]\n\t\tif !ok {\n\t\t\tstat = EvaluationStat{\n\t\t\t\tDomain:       dns.Domain(eval.PolicyDomain),\n\t\t\t\tDispositions: []string{string(eval.Disposition)},\n\t\t\t\tCount:        1,\n\t\t\t\tSendReport:   !eval.Optional,\n\t\t\t}\n\t\t} else {\n\t\t\tstat.Dispositions = append(stat.Dispositions, string(eval.Disposition))\n\t\t\tstat.Count++\n\t\t\tstat.SendReport = stat.SendReport || !eval.Optional\n\t\t}\n\t\tstats[eval.PolicyDomain] = stat\n\t}\n\n\treturn stats, nil\n}\n```", "```go\npackage dmarcdb\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"time\"\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"github.com/mjl-/bstore\"\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/dmarcrpt\"\n)\n\nfunc EvaluationStats(ctx context.Context) (map[string]EvaluationStat, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tevaluations, err := q.List()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"error fetching evaluations: %v\", err)\n\t}\n\n\tstats := make(map[string]EvaluationStat)\n\tfor _, evaluation := range evaluations {\n\t\tstat, ok := stats[evaluation.PolicyDomain]\n\t\tif !ok {\n\t\t\tstat = EvaluationStat{\n\t\t\t\tDomain:       dns.Domain{ASCII: evaluation.PolicyDomain},\n\t\t\t\tDispositions: []string{},\n\t\t\t\tCount:        0,\n\t\t\t\tSendReport:   true, // Default value\n\t\t\t}\n\t\t}\n\n\t\t// Check if report sending should be suppressed for this domain\n\t\tsuppressions := getSuppression(ctx, evaluation.PolicyDomain)\n\t\tif len(suppressions) > 0 {\n\t\t\tuntil := suppressions[0].Until\n\t\t\tif until.After(time.Now()) {\n\t\t\t\tstat.SendReport = false\n\t\t\t}\n\t\t}\n\n\t\tstat.Dispositions = append(stat.Dispositions, evaluation.Disposition.String())\n\t\tstat.Count++\n\t\tstats[evaluation.PolicyDomain] = stat\n\t}\n\n\treturn stats, nil\n}\n\nfunc getSuppression(ctx context.Context, domain string) ([]SuppressAddress) {\n\tq := bstore.QueryDB[SuppressAddress](ctx, EvalDB)\n\tsuppressions, err := q.FilterEqual(\"ReportingAddress\", domain).List()\n\tif err != nil {\n\t\tmlog.Log(ctx).Error(\"error fetching suppressions\", err)\n\t\treturn []SuppressAddress{}\n\t}\n\treturn suppressions\n}\n```", "```go\nfunc EvaluationStats(ctx context.Context) (map[string]EvaluationStat, error) {\n\t// Prepare a map to hold the statistics\n\tstats := make(map[string]EvaluationStat)\n\n\t// Query the database for all evaluations\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\terr := q.ForEach(func(e Evaluation) error {\n\t\t// If the domain for the evaluation is already in the stats, increment the count\n\t\tif stat, ok := stats[e.PolicyDomain]; ok {\n\t\t\tstat.Count++\n\t\t\tstats[e.PolicyDomain] = stat\n\t\t} else {\n\t\t\t// Otherwise, initialize a new EvaluationStat for the domain\n\t\t\tstats[e.PolicyDomain] = EvaluationStat{\n\t\t\t\tDomain:       dns.Domain{ASCII: e.PolicyDomain},\n\t\t\t\tDispositions: []string{string(e.Disposition)},\n\t\t\t\tCount:        1,\n\t\t\t\tSendReport:   !e.Optional,\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"retrieving evaluations from database: %v\", err)\n\t}\n\n\treturn stats, nil\n}\n```"]}, "mox-dmarcdb/eval.go-EvaluationsDomain": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dmarcdb/eval.go:\n```\npackage dmarcdb\n\n// Sending TLS reports and DMARC reports is very similar. See ../dmarcdb/eval.go:/similar and ../tlsrptsend/send.go:/similar.\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"mime/multipart\"\n\t\"net/textproto\"\n\t\"net/url\"\n\t\"os\"\n\t\"runtime/debug\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/exp/maps\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/dkim\"\n\t\"github.com/mjl-/mox/dmarc\"\n\t\"github.com/mjl-/mox/dmarcrpt\"\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/metrics\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/moxvar\"\n\t\"github.com/mjl-/mox/publicsuffix\"\n\t\"github.com/mjl-/mox/queue\"\n\t\"github.com/mjl-/mox/smtp\"\n\t\"github.com/mjl-/mox/store\"\n)\n\nvar (\n\tmetricReport = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_queued_total\",\n\t\t\tHelp: \"Total messages with DMARC aggregate/error reports queued.\",\n\t\t},\n\t)\n\tmetricReportError = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_error_total\",\n\t\t\tHelp: \"Total errors while composing or queueing DMARC aggregate/error reports.\",\n\t\t},\n\t)\n)\n\nvar (\n\tEvalDBTypes = []any{Evaluation{}, SuppressAddress{}} // Types stored in DB.\n\t// Exported for backups. For incoming deliveries the SMTP server adds evaluations\n\t// to the database. Every hour, a goroutine wakes up that gathers evaluations from\n\t// the last hour(s), sends a report, and removes the evaluations from the database.\n\tEvalDB *bstore.DB\n)\n\n// Evaluation is the result of an evaluation of a DMARC policy, to be included\n// in a DMARC report.\ntype Evaluation struct {\n\tID int64\n\n\t// Domain where DMARC policy was found, could be the organizational domain while\n\t// evaluation was for a subdomain. Unicode. Same as domain found in\n\t// PolicyPublished. A separate field for its index.\n\tPolicyDomain string `bstore:\"index\"`\n\n\t// Time of evaluation, determines which report (covering whole hours) this\n\t// evaluation will be included in.\n\tEvaluated time.Time `bstore:\"default now\"`\n\n\t// If optional, this evaluation is not a reason to send a DMARC report, but it will\n\t// be included when a report is sent due to other non-optional evaluations. Set for\n\t// evaluations of incoming DMARC reports. We don't want such deliveries causing us to\n\t// send a report, or we would keep exchanging reporting messages forever. Also set\n\t// for when evaluation is a DMARC reject for domains we haven't positively\n\t// interacted with, to prevent being used to flood an unsuspecting domain with\n\t// reports.\n\tOptional bool\n\n\t// Effective aggregate reporting interval in hours. Between 1 and 24, rounded up\n\t// from seconds from policy to first number that can divide 24.\n\tIntervalHours int\n\n\t// \"rua\" in DMARC record, we only store evaluations for records with aggregate reporting addresses, so always non-empty.\n\tAddresses []string\n\n\t// Policy used for evaluation. We don't store the \"fo\" field for failure reporting\n\t// options, since we don't send failure reports for individual messages.\n\tPolicyPublished dmarcrpt.PolicyPublished\n\n\t// For \"row\" in a report record.\n\tSourceIP        string\n\tDisposition     dmarcrpt.Disposition\n\tAlignedDKIMPass bool\n\tAlignedSPFPass  bool\n\tOverrideReasons []dmarcrpt.PolicyOverrideReason\n\n\t// For \"identifiers\" in a report record.\n\tEnvelopeTo   string\n\tEnvelopeFrom string\n\tHeaderFrom   string\n\n\t// For \"auth_results\" in a report record.\n\tDKIMResults []dmarcrpt.DKIMAuthResult\n\tSPFResults  []dmarcrpt.SPFAuthResult\n}\n\n// SuppressAddress is a reporting address for which outgoing DMARC reports\n// will be suppressed for a period.\ntype SuppressAddress struct {\n\tID               int64\n\tInserted         time.Time `bstore:\"default now\"`\n\tReportingAddress string    `bstore:\"unique\"`\n\tUntil            time.Time `bstore:\"nonzero\"`\n\tComment          string\n}\n\nvar dmarcResults = map[bool]dmarcrpt.DMARCResult{\n\tfalse: dmarcrpt.DMARCFail,\n\ttrue:  dmarcrpt.DMARCPass,\n}\n\n// ReportRecord turns an evaluation into a record that can be included in a\n// report.\nfunc (e Evaluation) ReportRecord(count int) dmarcrpt.ReportRecord {\n\treturn dmarcrpt.ReportRecord{\n\t\tRow: dmarcrpt.Row{\n\t\t\tSourceIP: e.SourceIP,\n\t\t\tCount:    count,\n\t\t\tPolicyEvaluated: dmarcrpt.PolicyEvaluated{\n\t\t\t\tDisposition: e.Disposition,\n\t\t\t\tDKIM:        dmarcResults[e.AlignedDKIMPass],\n\t\t\t\tSPF:         dmarcResults[e.AlignedSPFPass],\n\t\t\t\tReasons:     e.OverrideReasons,\n\t\t\t},\n\t\t},\n\t\tIdentifiers: dmarcrpt.Identifiers{\n\t\t\tEnvelopeTo:   e.EnvelopeTo,\n\t\t\tEnvelopeFrom: e.EnvelopeFrom,\n\t\t\tHeaderFrom:   e.HeaderFrom,\n\t\t},\n\t\tAuthResults: dmarcrpt.AuthResults{\n\t\t\tDKIM: e.DKIMResults,\n\t\t\tSPF:  e.SPFResults,\n\t\t},\n\t}\n}\n\nvar intervalOpts = []int{24, 12, 8, 6, 4, 3, 2}\n\nfunc intervalHours(seconds int) int {\n\thours := (seconds + 3600 - 1) / 3600\n\tfor _, opt := range intervalOpts {\n\t\tif hours >= opt {\n\t\t\treturn opt\n\t\t}\n\t}\n\treturn 1\n}\n\n// AddEvaluation adds the result of a DMARC evaluation for an incoming message\n// to the database.\n//\n// AddEvaluation sets Evaluation.IntervalHours based on\n// aggregateReportingIntervalSeconds.\n\n\n\n\n\n\n\n// Evaluations returns all evaluations in the database.\nfunc Evaluations(ctx context.Context) ([]Evaluation, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.SortAsc(\"Evaluated\")\n\treturn q.List()\n}\n\n// EvaluationStat summarizes stored evaluations, for inclusion in an upcoming\n// aggregate report, for a domain.\ntype EvaluationStat struct {\n\tDomain       dns.Domain\n\tDispositions []string\n\tCount        int\n\tSendReport   bool\n}\n\n// EvaluationStats returns evaluation counts and report-sending status per domain.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// EvaluationsDomain returns all evaluations for a domain.\n\n\n\n\n\n\n\n// RemoveEvaluationsDomain removes evaluations for domain so they won't be sent in\n// an aggregate report.\n\n\n\n\n\n\n\nvar jitterRand = mox.NewPseudoRand()\n\n// time to sleep until next whole hour t, replaced by tests.\n// Jitter so we don't cause load at exactly whole hours, other processes may\n// already be doing that.\nvar jitteredTimeUntil = func(t time.Time) time.Duration {\n\treturn time.Until(t.Add(time.Duration(30+jitterRand.Intn(60)) * time.Second))\n}\n\n// Start launches a goroutine that wakes up at each whole hour (plus jitter) and\n// sends DMARC reports to domains that requested them.\nfunc Start(resolver dns.Resolver) {\n\tgo func() {\n\t\tlog := mlog.New(\"dmarcdb\", nil)\n\n\t\tdefer func() {\n\t\t\t// In case of panic don't take the whole program down.\n\t\t\tx := recover()\n\t\t\tif x != nil {\n\t\t\t\tlog.Error(\"recover from panic\", slog.Any(\"panic\", x))\n\t\t\t\tdebug.PrintStack()\n\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t}\n\t\t}()\n\n\t\ttimer := time.NewTimer(time.Hour)\n\t\tdefer timer.Stop()\n\n\t\tctx := mox.Shutdown\n\n\t\tfor {\n\t\t\tnow := time.Now()\n\t\t\tnextEnd := nextWholeHour(now)\n\t\t\ttimer.Reset(jitteredTimeUntil(nextEnd))\n\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tlog.Info(\"dmarc aggregate report sender shutting down\")\n\t\t\t\treturn\n\t\t\tcase <-timer.C:\n\t\t\t}\n\n\t\t\t// Gather report intervals we want to process now. Multiples of hours that can\n\t\t\t// divide 24, starting from UTC.\n\t\t\t// ../rfc/7489:1750\n\t\t\tutchour := nextEnd.UTC().Hour()\n\t\t\tif utchour == 0 {\n\t\t\t\tutchour = 24\n\t\t\t}\n\t\t\tintervals := []int{}\n\t\t\tfor _, ival := range intervalOpts {\n\t\t\t\tif ival*(utchour/ival) == utchour {\n\t\t\t\t\tintervals = append(intervals, ival)\n\t\t\t\t}\n\t\t\t}\n\t\t\tintervals = append(intervals, 1)\n\n\t\t\t// Remove evaluations older than 48 hours (2 reports with the default and maximum\n\t\t\t// 24 hour interval). They should have been processed by now. We may have kept them\n\t\t\t// during temporary errors, but persistent temporary errors shouldn't fill up our\n\t\t\t// database. This also cleans up evaluations that were all optional for a domain.\n\t\t\t_, err := bstore.QueryDB[Evaluation](ctx, EvalDB).FilterLess(\"Evaluated\", nextEnd.Add(-48*time.Hour)).Delete()\n\t\t\tlog.Check(err, \"removing stale dmarc evaluations from database\")\n\n\t\t\tclog := log.WithCid(mox.Cid())\n\t\t\tclog.Info(\"sending dmarc aggregate reports\", slog.Time(\"end\", nextEnd.UTC()), slog.Any(\"intervals\", intervals))\n\t\t\tif err := sendReports(ctx, clog, resolver, EvalDB, nextEnd, intervals); err != nil {\n\t\t\t\tclog.Errorx(\"sending dmarc aggregate reports\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t} else {\n\t\t\t\tclog.Info(\"finished sending dmarc aggregate reports\")\n\t\t\t}\n\t\t}\n\t}()\n}\n\nfunc nextWholeHour(now time.Time) time.Time {\n\tt := now\n\tt = t.Add(time.Hour)\n\treturn time.Date(t.Year(), t.Month(), t.Day(), t.Hour(), 0, 0, 0, t.Location())\n}\n\n// We don't send reports at full speed. In the future, we could try to stretch out\n// reports a bit smarter. E.g. over 5 minutes with some minimum interval, and\n// perhaps faster and in parallel when there are lots of reports. Perhaps also\n// depending on reporting interval (faster for 1h, slower for 24h).\n// Replaced by tests.\nvar sleepBetween = func(ctx context.Context, between time.Duration) (ok bool) {\n\tt := time.NewTimer(between)\n\tselect {\n\tcase <-ctx.Done():\n\t\tt.Stop()\n\t\treturn false\n\tcase <-t.C:\n\t\treturn true\n\t}\n}\n\n// sendReports gathers all policy domains that have evaluations that should\n// receive a DMARC report and sends a report to each.\nfunc sendReports(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, intervals []int) error {\n\tivals := make([]any, len(intervals))\n\tfor i, v := range intervals {\n\t\tivals[i] = v\n\t}\n\n\tdestDomains := map[string]bool{}\n\n\t// Gather all domains that we plan to send to.\n\tnsend := 0\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterEqual(\"IntervalHours\", ivals...)\n\terr := q.ForEach(func(e Evaluation) error {\n\t\tif !e.Optional && !destDomains[e.PolicyPublished.Domain] {\n\t\t\tnsend++\n\t\t}\n\t\tdestDomains[e.PolicyPublished.Domain] = destDomains[e.PolicyPublished.Domain] || !e.Optional\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"looking for domains to send reports to: %v\", err)\n\t}\n\n\tvar wg sync.WaitGroup\n\n\t// Sleep in between sending reports. We process hourly, and spread the reports over\n\t// the hour, but with max 5 minute interval.\n\tbetween := 45 * time.Minute\n\tif nsend > 0 {\n\t\tbetween /= time.Duration(nsend)\n\t}\n\tif between > 5*time.Minute {\n\t\tbetween = 5 * time.Minute\n\t}\n\n\t// Attempt to send report to each domain.\n\tn := 0\n\tfor d, send := range destDomains {\n\t\t// Cleanup evaluations for domain with only optionals.\n\t\tif !send {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, d)\n\t\t\tcontinue\n\t\t}\n\n\t\tif n > 0 {\n\t\t\tif ok := sleepBetween(ctx, between); !ok {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t\tn++\n\n\t\t// Send in goroutine, so a slow process doesn't block progress.\n\t\twg.Add(1)\n\t\tgo func(domain string) {\n\t\t\tdefer func() {\n\t\t\t\t// In case of panic don't take the whole program down.\n\t\t\t\tx := recover()\n\t\t\t\tif x != nil {\n\t\t\t\t\tlog.Error(\"unhandled panic in dmarcdb sendReports\", slog.Any(\"panic\", x))\n\t\t\t\t\tdebug.PrintStack()\n\t\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t\t}\n\t\t\t}()\n\t\t\tdefer wg.Done()\n\n\t\t\trlog := log.WithCid(mox.Cid()).With(slog.Any(\"domain\", domain))\n\t\t\trlog.Info(\"sending dmarc report\")\n\t\t\tif _, err := sendReportDomain(ctx, rlog, resolver, db, endTime, domain); err != nil {\n\t\t\t\trlog.Errorx(\"sending dmarc aggregate report to domain\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t}\n\t\t}(d)\n\t}\n\n\twg.Wait()\n\n\treturn nil\n}\n\ntype recipient struct {\n\taddress smtp.Address\n\tmaxSize uint64\n}\n\nfunc parseRecipient(log mlog.Log, uri dmarc.URI) (r recipient, ok bool) {\n\tlog = log.With(slog.Any(\"uri\", uri.Address))\n\n\tu, err := url.Parse(uri.Address)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\tif !strings.EqualFold(u.Scheme, \"mailto\") {\n\t\tlog.Debug(\"skipping unrecognized scheme in dmarc record rua value\")\n\t\treturn r, false\n\t}\n\taddr, err := smtp.ParseAddress(u.Opaque)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing mailto uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\n\tr = recipient{addr, uri.MaxSize}\n\t// ../rfc/7489:1197\n\tswitch uri.Unit {\n\tcase \"k\", \"K\":\n\t\tr.maxSize *= 1024\n\tcase \"m\", \"M\":\n\t\tr.maxSize *= 1024 * 1024\n\tcase \"g\", \"G\":\n\t\tr.maxSize *= 1024 * 1024 * 1024\n\tcase \"t\", \"T\":\n\t\t// Oh yeah, terabyte-sized reports!\n\t\tr.maxSize *= 1024 * 1024 * 1024 * 1024\n\tcase \"\":\n\tdefault:\n\t\tlog.Debug(\"unrecognized max size unit in dmarc record rua value\", slog.String(\"unit\", uri.Unit))\n\t\treturn r, false\n\t}\n\n\treturn r, true\n}\n\nfunc removeEvaluations(ctx context.Context, log mlog.Log, db *bstore.DB, endTime time.Time, domain string) {\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterNonzero(Evaluation{PolicyDomain: domain})\n\t_, err := q.Delete()\n\tlog.Check(err, \"removing evaluations after processing for dmarc aggregate report\")\n}\n\n// replaceable for testing.\nvar queueAdd = queue.Add\n\nfunc sendReportDomain(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, domain string) (cleanup bool, rerr error) {\n\tdom, err := dns.ParseDomain(domain)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"parsing domain for sending reports: %v\", err)\n\t}\n\n\t// We'll cleanup records by default.\n\tcleanup = true\n\t// If we encounter a temporary error we cancel cleanup of evaluations on error.\n\ttempError := false\n\n\tdefer func() {\n\t\tif !cleanup || tempError {\n\t\t\tlog.Debug(\"not cleaning up evaluations after attempting to send dmarc aggregate report\")\n\t\t} else {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, domain)\n\t\t}\n\t}()\n\n\t// We're going to build up this report.\n\treport := dmarcrpt.Feedback{\n\t\tVersion: \"1.0\",\n\t\tReportMetadata: dmarcrpt.ReportMetadata{\n\t\t\tOrgName: mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\tEmail:   \"postmaster@\" + mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\t// ReportID and DateRange are set after we've seen evaluations.\n\t\t\t// Errors is filled below when we encounter problems.\n\t\t},\n\t\t// We'll fill the records below.\n\t\tRecords: []dmarcrpt.ReportRecord{},\n\t}\n\n\tvar errors []string // For report.ReportMetaData.Errors\n\n\t// Check if we should be sending a report at all: if there are rua URIs in the\n\t// current DMARC record. The interval may have changed too, but we'll flush out our\n\t// evaluations regardless. We always use the latest DMARC record when sending, but\n\t// we'll lump all policies of the last interval into one report.\n\t// ../rfc/7489:1714\n\tstatus, _, record, _, _, err := dmarc.Lookup(ctx, log.Logger, resolver, dom)\n\tif err != nil {\n\t\t// todo future: we could perhaps still send this report, assuming the values we know. in case of temporary error, we could also schedule again regardless of next interval hour (we would now only retry a 24h-interval report after 24h passed).\n\t\t// Remove records unless it was a temporary error. We'll try again next round.\n\t\tcleanup = status != dmarc.StatusTemperror\n\t\treturn cleanup, fmt.Errorf(\"looking up current dmarc record for reporting address: %v\", err)\n\t}\n\n\tvar recipients []recipient\n\n\t// Gather all aggregate reporting addresses to try to send to. We'll start with\n\t// those in the initial DMARC record, but will follow external reporting addresses\n\t// and possibly update the list.\n\tfor _, uri := range record.AggregateReportAddresses {\n\t\tr, ok := parseRecipient(log, uri)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Check if domain of rua recipient has the same organizational domain as for the\n\t\t// evaluations. If not, we need to verify we are allowed to send.\n\t\truaOrgDom := publicsuffix.Lookup(ctx, log.Logger, r.address.Domain)\n\t\tevalOrgDom := publicsuffix.Lookup(ctx, log.Logger, dom)\n\n\t\tif ruaOrgDom == evalOrgDom {\n\t\t\trecipients = append(recipients, r)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Verify and follow addresses in other organizational domain through\n\t\t// <policydomain>._report._dmarc.<host> lookup.\n\t\t// ../rfc/7489:1556\n\t\taccepts, status, records, _, _, err := dmarc.LookupExternalReportsAccepted(ctx, log.Logger, resolver, evalOrgDom, r.address.Domain)\n\t\tlog.Debugx(\"checking if rua address with different organization domain has opted into receiving dmarc reports\", err,\n\t\t\tslog.Any(\"policydomain\", evalOrgDom),\n\t\t\tslog.Any(\"destinationdomain\", r.address.Domain),\n\t\t\tslog.Bool(\"accepts\", accepts),\n\t\t\tslog.Any(\"status\", status))\n\t\tif status == dmarc.StatusTemperror {\n\t\t\t// With a temporary error, we'll try to get the report the delivered anyway,\n\t\t\t// perhaps there are multiple recipients.\n\t\t\t// ../rfc/7489:1578\n\t\t\ttempError = true\n\t\t\terrors = append(errors, \"temporary error checking authorization for report delegation to external address\")\n\t\t}\n\t\tif !accepts {\n\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain that does not opt-in to receiving dmarc records through _report dmarc record\", r.address))\n\t\t\tcontinue\n\t\t}\n\n\t\t// We can follow a _report DMARC DNS record once. In that record, a domain may\n\t\t// specify alternative addresses that we should send reports to instead. Such\n\t\t// alternative address(es) must have the same host. If not, we ignore the new\n\t\t// value. Behaviour for multiple records and/or multiple new addresses is\n\t\t// underspecified. We'll replace an address with one or more new addresses, and\n\t\t// keep the original if there was no candidate (which covers the case of invalid\n\t\t// alternative addresses and no new address specified).\n\t\t// ../rfc/7489:1600\n\t\tfoundReplacement := false\n\t\trlog := log.With(slog.Any(\"followedaddress\", uri.Address))\n\t\tfor _, record := range records {\n\t\t\tfor _, exturi := range record.AggregateReportAddresses {\n\t\t\t\textr, ok := parseRecipient(rlog, exturi)\n\t\t\t\tif !ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif extr.address.Domain != r.address.Domain {\n\t\t\t\t\trlog.Debug(\"rua address in external _report dmarc record has different host than initial dmarc record, ignoring new name\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain with a replacement address %s with different host\", r.address, extr.address))\n\t\t\t\t} else {\n\t\t\t\t\trlog.Debug(\"using replacement rua address from external _report dmarc record\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\tfoundReplacement = true\n\t\t\t\t\trecipients = append(recipients, extr)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif !foundReplacement {\n\t\t\trecipients = append(recipients, r)\n\t\t}\n\t}\n\n\tif len(recipients) == 0 {\n\t\t// No reports requested, perfectly fine, no work to do for us.\n\t\tlog.Debug(\"no aggregate reporting addresses configured\")\n\t\treturn true, nil\n\t}\n\n\t// We count idential records. Can be common with a domain sending quite some email.\n\t// Though less if the sending domain has many IPs. In the future, we may want to\n\t// remove some details from records so we can aggregate them into fewer rows.\n\ttype recordCount struct {\n\t\tdmarcrpt.ReportRecord\n\t\tcount int\n\t}\n\tcounts := map[string]recordCount{}\n\n\n```\n# Function Signature: func EvaluationsDomain(ctx context.Context, domain dns.Domain) ([]Evaluation, error) {\n# Natural Language Annotations: // EvaluationsDomain returns all evaluations for a domain.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc EvaluationsDomain(ctx context.Context, domain dns.Domain) ([]Evaluation, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.FilterNonzero(Evaluation{PolicyDomain: domain.String()})\n\treturn q.List()\n}\n```", "```go\nfunc EvaluationsDomain(ctx context.Context, domain dns.Domain) ([]Evaluation, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.FilterNonzero(Evaluation{PolicyDomain: domain.String()})\n\treturn q.List()\n}\n```", "```go\nfunc EvaluationsDomain(ctx context.Context, domain dns.Domain) ([]Evaluation, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.FilterEqual(\"PolicyDomain\", domain.String())\n\treturn q.List()\n}\n```"]}, "mox-dmarcdb/eval.go-RemoveEvaluationsDomain": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dmarcdb/eval.go:\n```\npackage dmarcdb\n\n// Sending TLS reports and DMARC reports is very similar. See ../dmarcdb/eval.go:/similar and ../tlsrptsend/send.go:/similar.\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"mime/multipart\"\n\t\"net/textproto\"\n\t\"net/url\"\n\t\"os\"\n\t\"runtime/debug\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/exp/maps\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/dkim\"\n\t\"github.com/mjl-/mox/dmarc\"\n\t\"github.com/mjl-/mox/dmarcrpt\"\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/metrics\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/moxvar\"\n\t\"github.com/mjl-/mox/publicsuffix\"\n\t\"github.com/mjl-/mox/queue\"\n\t\"github.com/mjl-/mox/smtp\"\n\t\"github.com/mjl-/mox/store\"\n)\n\nvar (\n\tmetricReport = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_queued_total\",\n\t\t\tHelp: \"Total messages with DMARC aggregate/error reports queued.\",\n\t\t},\n\t)\n\tmetricReportError = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_report_error_total\",\n\t\t\tHelp: \"Total errors while composing or queueing DMARC aggregate/error reports.\",\n\t\t},\n\t)\n)\n\nvar (\n\tEvalDBTypes = []any{Evaluation{}, SuppressAddress{}} // Types stored in DB.\n\t// Exported for backups. For incoming deliveries the SMTP server adds evaluations\n\t// to the database. Every hour, a goroutine wakes up that gathers evaluations from\n\t// the last hour(s), sends a report, and removes the evaluations from the database.\n\tEvalDB *bstore.DB\n)\n\n// Evaluation is the result of an evaluation of a DMARC policy, to be included\n// in a DMARC report.\ntype Evaluation struct {\n\tID int64\n\n\t// Domain where DMARC policy was found, could be the organizational domain while\n\t// evaluation was for a subdomain. Unicode. Same as domain found in\n\t// PolicyPublished. A separate field for its index.\n\tPolicyDomain string `bstore:\"index\"`\n\n\t// Time of evaluation, determines which report (covering whole hours) this\n\t// evaluation will be included in.\n\tEvaluated time.Time `bstore:\"default now\"`\n\n\t// If optional, this evaluation is not a reason to send a DMARC report, but it will\n\t// be included when a report is sent due to other non-optional evaluations. Set for\n\t// evaluations of incoming DMARC reports. We don't want such deliveries causing us to\n\t// send a report, or we would keep exchanging reporting messages forever. Also set\n\t// for when evaluation is a DMARC reject for domains we haven't positively\n\t// interacted with, to prevent being used to flood an unsuspecting domain with\n\t// reports.\n\tOptional bool\n\n\t// Effective aggregate reporting interval in hours. Between 1 and 24, rounded up\n\t// from seconds from policy to first number that can divide 24.\n\tIntervalHours int\n\n\t// \"rua\" in DMARC record, we only store evaluations for records with aggregate reporting addresses, so always non-empty.\n\tAddresses []string\n\n\t// Policy used for evaluation. We don't store the \"fo\" field for failure reporting\n\t// options, since we don't send failure reports for individual messages.\n\tPolicyPublished dmarcrpt.PolicyPublished\n\n\t// For \"row\" in a report record.\n\tSourceIP        string\n\tDisposition     dmarcrpt.Disposition\n\tAlignedDKIMPass bool\n\tAlignedSPFPass  bool\n\tOverrideReasons []dmarcrpt.PolicyOverrideReason\n\n\t// For \"identifiers\" in a report record.\n\tEnvelopeTo   string\n\tEnvelopeFrom string\n\tHeaderFrom   string\n\n\t// For \"auth_results\" in a report record.\n\tDKIMResults []dmarcrpt.DKIMAuthResult\n\tSPFResults  []dmarcrpt.SPFAuthResult\n}\n\n// SuppressAddress is a reporting address for which outgoing DMARC reports\n// will be suppressed for a period.\ntype SuppressAddress struct {\n\tID               int64\n\tInserted         time.Time `bstore:\"default now\"`\n\tReportingAddress string    `bstore:\"unique\"`\n\tUntil            time.Time `bstore:\"nonzero\"`\n\tComment          string\n}\n\nvar dmarcResults = map[bool]dmarcrpt.DMARCResult{\n\tfalse: dmarcrpt.DMARCFail,\n\ttrue:  dmarcrpt.DMARCPass,\n}\n\n// ReportRecord turns an evaluation into a record that can be included in a\n// report.\nfunc (e Evaluation) ReportRecord(count int) dmarcrpt.ReportRecord {\n\treturn dmarcrpt.ReportRecord{\n\t\tRow: dmarcrpt.Row{\n\t\t\tSourceIP: e.SourceIP,\n\t\t\tCount:    count,\n\t\t\tPolicyEvaluated: dmarcrpt.PolicyEvaluated{\n\t\t\t\tDisposition: e.Disposition,\n\t\t\t\tDKIM:        dmarcResults[e.AlignedDKIMPass],\n\t\t\t\tSPF:         dmarcResults[e.AlignedSPFPass],\n\t\t\t\tReasons:     e.OverrideReasons,\n\t\t\t},\n\t\t},\n\t\tIdentifiers: dmarcrpt.Identifiers{\n\t\t\tEnvelopeTo:   e.EnvelopeTo,\n\t\t\tEnvelopeFrom: e.EnvelopeFrom,\n\t\t\tHeaderFrom:   e.HeaderFrom,\n\t\t},\n\t\tAuthResults: dmarcrpt.AuthResults{\n\t\t\tDKIM: e.DKIMResults,\n\t\t\tSPF:  e.SPFResults,\n\t\t},\n\t}\n}\n\nvar intervalOpts = []int{24, 12, 8, 6, 4, 3, 2}\n\nfunc intervalHours(seconds int) int {\n\thours := (seconds + 3600 - 1) / 3600\n\tfor _, opt := range intervalOpts {\n\t\tif hours >= opt {\n\t\t\treturn opt\n\t\t}\n\t}\n\treturn 1\n}\n\n// AddEvaluation adds the result of a DMARC evaluation for an incoming message\n// to the database.\n//\n// AddEvaluation sets Evaluation.IntervalHours based on\n// aggregateReportingIntervalSeconds.\n\n\n\n\n\n\n\n// Evaluations returns all evaluations in the database.\nfunc Evaluations(ctx context.Context) ([]Evaluation, error) {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\tq.SortAsc(\"Evaluated\")\n\treturn q.List()\n}\n\n// EvaluationStat summarizes stored evaluations, for inclusion in an upcoming\n// aggregate report, for a domain.\ntype EvaluationStat struct {\n\tDomain       dns.Domain\n\tDispositions []string\n\tCount        int\n\tSendReport   bool\n}\n\n// EvaluationStats returns evaluation counts and report-sending status per domain.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// EvaluationsDomain returns all evaluations for a domain.\n\n\n\n\n\n\n\n// RemoveEvaluationsDomain removes evaluations for domain so they won't be sent in\n// an aggregate report.\n\n\n\n\n\n\n\nvar jitterRand = mox.NewPseudoRand()\n\n// time to sleep until next whole hour t, replaced by tests.\n// Jitter so we don't cause load at exactly whole hours, other processes may\n// already be doing that.\nvar jitteredTimeUntil = func(t time.Time) time.Duration {\n\treturn time.Until(t.Add(time.Duration(30+jitterRand.Intn(60)) * time.Second))\n}\n\n// Start launches a goroutine that wakes up at each whole hour (plus jitter) and\n// sends DMARC reports to domains that requested them.\nfunc Start(resolver dns.Resolver) {\n\tgo func() {\n\t\tlog := mlog.New(\"dmarcdb\", nil)\n\n\t\tdefer func() {\n\t\t\t// In case of panic don't take the whole program down.\n\t\t\tx := recover()\n\t\t\tif x != nil {\n\t\t\t\tlog.Error(\"recover from panic\", slog.Any(\"panic\", x))\n\t\t\t\tdebug.PrintStack()\n\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t}\n\t\t}()\n\n\t\ttimer := time.NewTimer(time.Hour)\n\t\tdefer timer.Stop()\n\n\t\tctx := mox.Shutdown\n\n\t\tfor {\n\t\t\tnow := time.Now()\n\t\t\tnextEnd := nextWholeHour(now)\n\t\t\ttimer.Reset(jitteredTimeUntil(nextEnd))\n\n\t\t\tselect {\n\t\t\tcase <-ctx.Done():\n\t\t\t\tlog.Info(\"dmarc aggregate report sender shutting down\")\n\t\t\t\treturn\n\t\t\tcase <-timer.C:\n\t\t\t}\n\n\t\t\t// Gather report intervals we want to process now. Multiples of hours that can\n\t\t\t// divide 24, starting from UTC.\n\t\t\t// ../rfc/7489:1750\n\t\t\tutchour := nextEnd.UTC().Hour()\n\t\t\tif utchour == 0 {\n\t\t\t\tutchour = 24\n\t\t\t}\n\t\t\tintervals := []int{}\n\t\t\tfor _, ival := range intervalOpts {\n\t\t\t\tif ival*(utchour/ival) == utchour {\n\t\t\t\t\tintervals = append(intervals, ival)\n\t\t\t\t}\n\t\t\t}\n\t\t\tintervals = append(intervals, 1)\n\n\t\t\t// Remove evaluations older than 48 hours (2 reports with the default and maximum\n\t\t\t// 24 hour interval). They should have been processed by now. We may have kept them\n\t\t\t// during temporary errors, but persistent temporary errors shouldn't fill up our\n\t\t\t// database. This also cleans up evaluations that were all optional for a domain.\n\t\t\t_, err := bstore.QueryDB[Evaluation](ctx, EvalDB).FilterLess(\"Evaluated\", nextEnd.Add(-48*time.Hour)).Delete()\n\t\t\tlog.Check(err, \"removing stale dmarc evaluations from database\")\n\n\t\t\tclog := log.WithCid(mox.Cid())\n\t\t\tclog.Info(\"sending dmarc aggregate reports\", slog.Time(\"end\", nextEnd.UTC()), slog.Any(\"intervals\", intervals))\n\t\t\tif err := sendReports(ctx, clog, resolver, EvalDB, nextEnd, intervals); err != nil {\n\t\t\t\tclog.Errorx(\"sending dmarc aggregate reports\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t} else {\n\t\t\t\tclog.Info(\"finished sending dmarc aggregate reports\")\n\t\t\t}\n\t\t}\n\t}()\n}\n\nfunc nextWholeHour(now time.Time) time.Time {\n\tt := now\n\tt = t.Add(time.Hour)\n\treturn time.Date(t.Year(), t.Month(), t.Day(), t.Hour(), 0, 0, 0, t.Location())\n}\n\n// We don't send reports at full speed. In the future, we could try to stretch out\n// reports a bit smarter. E.g. over 5 minutes with some minimum interval, and\n// perhaps faster and in parallel when there are lots of reports. Perhaps also\n// depending on reporting interval (faster for 1h, slower for 24h).\n// Replaced by tests.\nvar sleepBetween = func(ctx context.Context, between time.Duration) (ok bool) {\n\tt := time.NewTimer(between)\n\tselect {\n\tcase <-ctx.Done():\n\t\tt.Stop()\n\t\treturn false\n\tcase <-t.C:\n\t\treturn true\n\t}\n}\n\n// sendReports gathers all policy domains that have evaluations that should\n// receive a DMARC report and sends a report to each.\nfunc sendReports(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, intervals []int) error {\n\tivals := make([]any, len(intervals))\n\tfor i, v := range intervals {\n\t\tivals[i] = v\n\t}\n\n\tdestDomains := map[string]bool{}\n\n\t// Gather all domains that we plan to send to.\n\tnsend := 0\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterEqual(\"IntervalHours\", ivals...)\n\terr := q.ForEach(func(e Evaluation) error {\n\t\tif !e.Optional && !destDomains[e.PolicyPublished.Domain] {\n\t\t\tnsend++\n\t\t}\n\t\tdestDomains[e.PolicyPublished.Domain] = destDomains[e.PolicyPublished.Domain] || !e.Optional\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"looking for domains to send reports to: %v\", err)\n\t}\n\n\tvar wg sync.WaitGroup\n\n\t// Sleep in between sending reports. We process hourly, and spread the reports over\n\t// the hour, but with max 5 minute interval.\n\tbetween := 45 * time.Minute\n\tif nsend > 0 {\n\t\tbetween /= time.Duration(nsend)\n\t}\n\tif between > 5*time.Minute {\n\t\tbetween = 5 * time.Minute\n\t}\n\n\t// Attempt to send report to each domain.\n\tn := 0\n\tfor d, send := range destDomains {\n\t\t// Cleanup evaluations for domain with only optionals.\n\t\tif !send {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, d)\n\t\t\tcontinue\n\t\t}\n\n\t\tif n > 0 {\n\t\t\tif ok := sleepBetween(ctx, between); !ok {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t}\n\t\tn++\n\n\t\t// Send in goroutine, so a slow process doesn't block progress.\n\t\twg.Add(1)\n\t\tgo func(domain string) {\n\t\t\tdefer func() {\n\t\t\t\t// In case of panic don't take the whole program down.\n\t\t\t\tx := recover()\n\t\t\t\tif x != nil {\n\t\t\t\t\tlog.Error(\"unhandled panic in dmarcdb sendReports\", slog.Any(\"panic\", x))\n\t\t\t\t\tdebug.PrintStack()\n\t\t\t\t\tmetrics.PanicInc(metrics.Dmarcdb)\n\t\t\t\t}\n\t\t\t}()\n\t\t\tdefer wg.Done()\n\n\t\t\trlog := log.WithCid(mox.Cid()).With(slog.Any(\"domain\", domain))\n\t\t\trlog.Info(\"sending dmarc report\")\n\t\t\tif _, err := sendReportDomain(ctx, rlog, resolver, db, endTime, domain); err != nil {\n\t\t\t\trlog.Errorx(\"sending dmarc aggregate report to domain\", err)\n\t\t\t\tmetricReportError.Inc()\n\t\t\t}\n\t\t}(d)\n\t}\n\n\twg.Wait()\n\n\treturn nil\n}\n\ntype recipient struct {\n\taddress smtp.Address\n\tmaxSize uint64\n}\n\nfunc parseRecipient(log mlog.Log, uri dmarc.URI) (r recipient, ok bool) {\n\tlog = log.With(slog.Any(\"uri\", uri.Address))\n\n\tu, err := url.Parse(uri.Address)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\tif !strings.EqualFold(u.Scheme, \"mailto\") {\n\t\tlog.Debug(\"skipping unrecognized scheme in dmarc record rua value\")\n\t\treturn r, false\n\t}\n\taddr, err := smtp.ParseAddress(u.Opaque)\n\tif err != nil {\n\t\tlog.Debugx(\"parsing mailto uri in dmarc record rua value\", err)\n\t\treturn r, false\n\t}\n\n\tr = recipient{addr, uri.MaxSize}\n\t// ../rfc/7489:1197\n\tswitch uri.Unit {\n\tcase \"k\", \"K\":\n\t\tr.maxSize *= 1024\n\tcase \"m\", \"M\":\n\t\tr.maxSize *= 1024 * 1024\n\tcase \"g\", \"G\":\n\t\tr.maxSize *= 1024 * 1024 * 1024\n\tcase \"t\", \"T\":\n\t\t// Oh yeah, terabyte-sized reports!\n\t\tr.maxSize *= 1024 * 1024 * 1024 * 1024\n\tcase \"\":\n\tdefault:\n\t\tlog.Debug(\"unrecognized max size unit in dmarc record rua value\", slog.String(\"unit\", uri.Unit))\n\t\treturn r, false\n\t}\n\n\treturn r, true\n}\n\nfunc removeEvaluations(ctx context.Context, log mlog.Log, db *bstore.DB, endTime time.Time, domain string) {\n\tq := bstore.QueryDB[Evaluation](ctx, db)\n\tq.FilterLess(\"Evaluated\", endTime)\n\tq.FilterNonzero(Evaluation{PolicyDomain: domain})\n\t_, err := q.Delete()\n\tlog.Check(err, \"removing evaluations after processing for dmarc aggregate report\")\n}\n\n// replaceable for testing.\nvar queueAdd = queue.Add\n\nfunc sendReportDomain(ctx context.Context, log mlog.Log, resolver dns.Resolver, db *bstore.DB, endTime time.Time, domain string) (cleanup bool, rerr error) {\n\tdom, err := dns.ParseDomain(domain)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"parsing domain for sending reports: %v\", err)\n\t}\n\n\t// We'll cleanup records by default.\n\tcleanup = true\n\t// If we encounter a temporary error we cancel cleanup of evaluations on error.\n\ttempError := false\n\n\tdefer func() {\n\t\tif !cleanup || tempError {\n\t\t\tlog.Debug(\"not cleaning up evaluations after attempting to send dmarc aggregate report\")\n\t\t} else {\n\t\t\tremoveEvaluations(ctx, log, db, endTime, domain)\n\t\t}\n\t}()\n\n\t// We're going to build up this report.\n\treport := dmarcrpt.Feedback{\n\t\tVersion: \"1.0\",\n\t\tReportMetadata: dmarcrpt.ReportMetadata{\n\t\t\tOrgName: mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\tEmail:   \"postmaster@\" + mox.Conf.Static.HostnameDomain.ASCII,\n\t\t\t// ReportID and DateRange are set after we've seen evaluations.\n\t\t\t// Errors is filled below when we encounter problems.\n\t\t},\n\t\t// We'll fill the records below.\n\t\tRecords: []dmarcrpt.ReportRecord{},\n\t}\n\n\tvar errors []string // For report.ReportMetaData.Errors\n\n\t// Check if we should be sending a report at all: if there are rua URIs in the\n\t// current DMARC record. The interval may have changed too, but we'll flush out our\n\t// evaluations regardless. We always use the latest DMARC record when sending, but\n\t// we'll lump all policies of the last interval into one report.\n\t// ../rfc/7489:1714\n\tstatus, _, record, _, _, err := dmarc.Lookup(ctx, log.Logger, resolver, dom)\n\tif err != nil {\n\t\t// todo future: we could perhaps still send this report, assuming the values we know. in case of temporary error, we could also schedule again regardless of next interval hour (we would now only retry a 24h-interval report after 24h passed).\n\t\t// Remove records unless it was a temporary error. We'll try again next round.\n\t\tcleanup = status != dmarc.StatusTemperror\n\t\treturn cleanup, fmt.Errorf(\"looking up current dmarc record for reporting address: %v\", err)\n\t}\n\n\tvar recipients []recipient\n\n\t// Gather all aggregate reporting addresses to try to send to. We'll start with\n\t// those in the initial DMARC record, but will follow external reporting addresses\n\t// and possibly update the list.\n\tfor _, uri := range record.AggregateReportAddresses {\n\t\tr, ok := parseRecipient(log, uri)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Check if domain of rua recipient has the same organizational domain as for the\n\t\t// evaluations. If not, we need to verify we are allowed to send.\n\t\truaOrgDom := publicsuffix.Lookup(ctx, log.Logger, r.address.Domain)\n\t\tevalOrgDom := publicsuffix.Lookup(ctx, log.Logger, dom)\n\n\t\tif ruaOrgDom == evalOrgDom {\n\t\t\trecipients = append(recipients, r)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Verify and follow addresses in other organizational domain through\n\t\t// <policydomain>._report._dmarc.<host> lookup.\n\t\t// ../rfc/7489:1556\n\t\taccepts, status, records, _, _, err := dmarc.LookupExternalReportsAccepted(ctx, log.Logger, resolver, evalOrgDom, r.address.Domain)\n\t\tlog.Debugx(\"checking if rua address with different organization domain has opted into receiving dmarc reports\", err,\n\t\t\tslog.Any(\"policydomain\", evalOrgDom),\n\t\t\tslog.Any(\"destinationdomain\", r.address.Domain),\n\t\t\tslog.Bool(\"accepts\", accepts),\n\t\t\tslog.Any(\"status\", status))\n\t\tif status == dmarc.StatusTemperror {\n\t\t\t// With a temporary error, we'll try to get the report the delivered anyway,\n\t\t\t// perhaps there are multiple recipients.\n\t\t\t// ../rfc/7489:1578\n\t\t\ttempError = true\n\t\t\terrors = append(errors, \"temporary error checking authorization for report delegation to external address\")\n\t\t}\n\t\tif !accepts {\n\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain that does not opt-in to receiving dmarc records through _report dmarc record\", r.address))\n\t\t\tcontinue\n\t\t}\n\n\t\t// We can follow a _report DMARC DNS record once. In that record, a domain may\n\t\t// specify alternative addresses that we should send reports to instead. Such\n\t\t// alternative address(es) must have the same host. If not, we ignore the new\n\t\t// value. Behaviour for multiple records and/or multiple new addresses is\n\t\t// underspecified. We'll replace an address with one or more new addresses, and\n\t\t// keep the original if there was no candidate (which covers the case of invalid\n\t\t// alternative addresses and no new address specified).\n\t\t// ../rfc/7489:1600\n\t\tfoundReplacement := false\n\t\trlog := log.With(slog.Any(\"followedaddress\", uri.Address))\n\t\tfor _, record := range records {\n\t\t\tfor _, exturi := range record.AggregateReportAddresses {\n\t\t\t\textr, ok := parseRecipient(rlog, exturi)\n\t\t\t\tif !ok {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif extr.address.Domain != r.address.Domain {\n\t\t\t\t\trlog.Debug(\"rua address in external _report dmarc record has different host than initial dmarc record, ignoring new name\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\terrors = append(errors, fmt.Sprintf(\"rua %s is external domain with a replacement address %s with different host\", r.address, extr.address))\n\t\t\t\t} else {\n\t\t\t\t\trlog.Debug(\"using replacement rua address from external _report dmarc record\", slog.Any(\"externaladdress\", extr.address))\n\t\t\t\t\tfoundReplacement = true\n\t\t\t\t\trecipients = append(recipients, extr)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif !foundReplacement {\n\t\t\trecipients = append(recipients, r)\n\t\t}\n\t}\n\n\tif len(recipients) == 0 {\n\t\t// No reports requested, perfectly fine, no work to do for us.\n\t\tlog.Debug(\"no aggregate reporting addresses configured\")\n\t\treturn true, nil\n\t}\n\n\t// We count idential records. Can be common with a domain sending quite some email.\n\t// Though less if the sending domain has many IPs. In the future, we may want to\n\t// remove some details from records so we can aggregate them into fewer rows.\n\ttype recordCount struct {\n\t\tdmarcrpt.ReportRecord\n\t\tcount int\n\t}\n\tcounts := map[string]recordCount{}\n\n\n```\n# Function Signature: func RemoveEvaluationsDomain(ctx context.Context, domain dns.Domain) error {\n# Natural Language Annotations: // RemoveEvaluationsDomain removes evaluations for domain so they won't be sent in\n// an aggregate report.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc RemoveEvaluationsDomain(ctx context.Context, domain dns.Domain) error {\n\tremoveEvaluations(ctx, domain)\n\treturn nil\n}\n```", "```go\nfunc RemoveEvaluationsDomain(ctx context.Context, domain dns.Domain) error {\n\tq := bstore.QueryDB[Evaluation](ctx, EvalDB)\n\t_, err := q.FilterEqual(\"PolicyDomain\", domain.String()).Delete()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error removing evaluations for domain: %v\", err)\n\t}\n\treturn nil\n}\n```", "```go\nfunc RemoveEvaluationsDomain(ctx context.Context, domain dns.Domain) error {\n\tremoveEvaluations(ctx, mlog.New(\"dmarcdb\", nil), EvalDB, time.Now(), domain.String())\n\treturn nil\n}\n```"]}, "mox-dmarcdb/reports.go-AddReport": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dmarcdb/reports.go:\n```\npackage dmarcdb\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/dmarcrpt\"\n\t\"github.com/mjl-/mox/dns\"\n)\n\nvar (\n\tReportsDBTypes = []any{DomainFeedback{}} // Types stored in DB.\n\tReportsDB      *bstore.DB                // Exported for backups.\n)\n\nvar (\n\tmetricEvaluated = promauto.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_policy_evaluated_total\",\n\t\t\tHelp: \"Number of policy evaluations.\",\n\t\t},\n\t\t// We only register validated domains for which we have a config.\n\t\t[]string{\"domain\", \"disposition\", \"dkim\", \"spf\"},\n\t)\n\tmetricDKIM = promauto.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_dkim_result_total\",\n\t\t\tHelp: \"Number of DKIM results.\",\n\t\t},\n\t\t[]string{\"result\"},\n\t)\n\tmetricSPF = promauto.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_dmarcdb_spf_result_total\",\n\t\t\tHelp: \"Number of SPF results.\",\n\t\t},\n\t\t[]string{\"result\"},\n\t)\n)\n\n// DomainFeedback is a single report stored in the database.\ntype DomainFeedback struct {\n\tID int64\n\t// Domain where DMARC DNS record was found, could be organizational domain.\n\tDomain string `bstore:\"index\"`\n\t// Domain in From-header.\n\tFromDomain string `bstore:\"index\"`\n\tdmarcrpt.Feedback\n}\n\n// AddReport adds a DMARC aggregate feedback report from an email to the database,\n// and updates prometheus metrics.\n//\n// fromDomain is the domain in the report message From header.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Records returns all reports in the database.\nfunc Records(ctx context.Context) ([]DomainFeedback, error) {\n\treturn bstore.QueryDB[DomainFeedback](ctx, ReportsDB).List()\n}\n\n// RecordID returns the report for the ID.\nfunc RecordID(ctx context.Context, id int64) (DomainFeedback, error) {\n\te := DomainFeedback{ID: id}\n\terr := ReportsDB.Get(ctx, &e)\n\treturn e, err\n}\n\n// RecordsPeriodDomain returns the reports overlapping start and end, for the given\n// domain. If domain is empty, all records match for domain.\nfunc RecordsPeriodDomain(ctx context.Context, start, end time.Time, domain string) ([]DomainFeedback, error) {\n\ts := start.Unix()\n\te := end.Unix()\n\n\tq := bstore.QueryDB[DomainFeedback](ctx, ReportsDB)\n\tif domain != \"\" {\n\t\tq.FilterNonzero(DomainFeedback{Domain: domain})\n\t}\n\tq.FilterFn(func(d DomainFeedback) bool {\n\t\tm := d.Feedback.ReportMetadata.DateRange\n\t\treturn m.Begin >= s && m.Begin < e || m.End > s && m.End <= e\n\t})\n\treturn q.List()\n}\n\n```\n# Function Signature: func AddReport(ctx context.Context, f *dmarcrpt.Feedback, fromDomain dns.Domain) error {\n# Natural Language Annotations: // AddReport adds a DMARC aggregate feedback report from an email to the database,\n// and updates prometheus metrics.\n//\n// fromDomain is the domain in the report message From header.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc AddReport(ctx context.Context, f *dmarcrpt.Feedback, fromDomain dns.Domain) error {\n\t// AddReport adds a DMARC aggregate feedback report from an email to the database,\n\t// and updates prometheus metrics.\n\t//\n\t// fromDomain is the domain in the report message From header.\n\n\t// Create a new DomainFeedback record.\n\trecord := DomainFeedback{\n\t\tDomain:     string(fromDomain),\n\t\tFromDomain: string(fromDomain),\n\t\tFeedback:   *f,\n\t}\n\n\t// Add the record to the database.\n\tif err := ReportsDB.Insert(ctx, &record); err != nil {\n\t\treturn err\n\t}\n\n\t// Update the prometheus metrics based on the feedback.\n\tfor _, rec := range f.Records {\n\t\tif rec.DKIMAuthResult != nil {\n\t\t\tmetricDKIM.WithLabelValues(string(*rec.DKIMAuthResult)).Inc()\n\t\t}\n\t\tif rec.SPFAuthResult != nil {\n\t\t\tmetricSPF.WithLabelValues(string(*rec.SPFAuthResult)).Inc()\n\t\t}\n\t}\n\n\treturn nil\n}\n```", "```go\nfunc AddReport(ctx context.Context, f *dmarcrpt.Feedback, fromDomain dns.Domain) error {\n\tdomain := string(fromDomain)\n\n\tfeedback := f.PCT\n\tdisposition := feedback.PolicyDisposition\n\tdkim := feedback.PolicyDKIM\n\tspf := feedback.PolicySPF\n\n\t// Add the report to the database.\n\tid, err := bstore.MutateDB(ctx, ReportsDB, func(tx *bstore.Tx) (any, error) {\n\t\tvar df DomainFeedback\n\t\tdf.Domain = domain\n\t\tdf.FromDomain = string(f.ReportMetadata.Domain)\n\t\tdf.Feedback = *f\n\t\treturn tx.Insert(df)\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"adding report to database: %v\", err)\n\t}\n\n\t// Update the Prometheus metrics.\n\tmetricEvaluated.WithLabelValues(domain, string(disposition), string(dkim), string(spf)).Inc()\n\tmetricDKIM.WithLabelValues(string(dkim)).Inc()\n\tmetricSPF.WithLabelValues(string(spf)).Inc()\n\n\treturn nil\n}\n```", "```go\nfunc AddReport(ctx context.Context, f *dmarcrpt.Feedback, fromDomain dns.Domain) error {\n\tdomain := fromDomain.ASCII()\n\tfeedback := *f\n\treport := DomainFeedback{\n\t\tDomain:      domain,\n\t\tFromDomain:  fromDomain.ASCII(),\n\t\tFeedback:    feedback,\n\t}\n\n\terr := ReportsDB.Insert(ctx, &report)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdisposition := \"\"\n\tif feedback.PolicyPublished.Domain != \"\" {\n\t\tdisposition = feedback.PolicyPublished.DKIM.IdentifierAlignment\n\t}\n\n\tdkimResult := \"\"\n\tif feedback.Record.Row.IdentifiersAlignment != nil {\n\t\tdkimResult = *feedback.Record.Row.IdentifiersAlignment\n\t}\n\n\tspfResult := \"\"\n\tif feedback.Record.Row.AuthResult != nil {\n\t\tspfResult = *feedback.Record.Row.AuthResult\n\t}\n\n\tmetricEvaluated.WithLabelValues(domain, disposition, dkimResult, spfResult).Inc()\n\tmetricDKIM.WithLabelValues(dkimResult).Inc()\n\tmetricSPF.WithLabelValues(spfResult).Inc()\n\n\treturn nil\n}\n```\nThis new function `AddReport` adds a DMARC aggregate feedback report from an email to the database, and updates the Prometheus metrics. It takes in a context, a feedback pointer, and a fromDomain. It then creates a DomainFeedback struct with the provided data, inserts it into the database, and updates the Prometheus metrics."]}, "mox-junk/bloom.go-NewBloom": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/junk/bloom.go:\n```\npackage junk\n\nimport (\n\t\"errors\"\n\t\"os\"\n\n\t\"golang.org/x/crypto/blake2b\"\n)\n\n// see https://en.wikipedia.org/wiki/Bloom_filter\n\nvar errWidth = errors.New(\"k and width wider than 256 bits and width not more than 32\")\nvar errPowerOfTwo = errors.New(\"data not a power of two\")\n\n// Bloom is a bloom filter.\ntype Bloom struct {\n\tdata     []byte\n\tk        int // Number of bits we store/lookup in the bloom filter per value.\n\tw        int // Number of bits needed to address a single bit position.\n\tmodified bool\n}\n\nfunc bloomWidth(fileSize int) int {\n\tw := 0\n\tfor bits := uint32(fileSize * 8); bits > 1; bits >>= 1 {\n\t\tw++\n\t}\n\treturn w\n}\n\n// BloomValid returns an error if the bloom file parameters are not correct.\nfunc BloomValid(fileSize int, k int) error {\n\t_, err := bloomValid(fileSize, k)\n\treturn err\n}\n\nfunc bloomValid(fileSize, k int) (int, error) {\n\tw := bloomWidth(fileSize)\n\tif 1<<w != fileSize*8 {\n\t\treturn 0, errPowerOfTwo\n\t}\n\tif k*w > 256 || w > 32 {\n\t\treturn 0, errWidth\n\t}\n\treturn w, nil\n}\n\n// NewBloom returns a bloom filter with given initial data.\n//\n// The number of bits in data must be a power of 2.\n// K is the number of \"hashes\" (bits) to store/lookup for each value stored.\n// Width is calculated as the number of bits needed to represent a single bit/hash\n// position in the data.\n//\n// For each value stored/looked up, a hash over the value is calculated. The hash\n// is split into \"k\" values that are \"width\" bits wide, each used to lookup a bit.\n// K * width must not exceed 256.\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (b *Bloom) Add(s string) {\n\th := hash([]byte(s), b.w)\n\tfor i := 0; i < b.k; i++ {\n\t\tb.set(h.nextPos())\n\t}\n}\n\nfunc (b *Bloom) Has(s string) bool {\n\th := hash([]byte(s), b.w)\n\tfor i := 0; i < b.k; i++ {\n\t\tif !b.has(h.nextPos()) {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc (b *Bloom) Bytes() []byte {\n\treturn b.data\n}\n\nfunc (b *Bloom) Modified() bool {\n\treturn b.modified\n}\n\n// Ones returns the number of ones.\nfunc (b *Bloom) Ones() (n int) {\n\tfor _, d := range b.data {\n\t\tfor i := 0; i < 8; i++ {\n\t\t\tif d&1 != 0 {\n\t\t\t\tn++\n\t\t\t}\n\t\t\td >>= 1\n\t\t}\n\t}\n\treturn n\n}\n\nfunc (b *Bloom) Write(path string) error {\n\tf, err := os.OpenFile(path, os.O_CREATE|os.O_WRONLY, 0660)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif _, err := f.Write(b.data); err != nil {\n\t\tf.Close()\n\t\treturn err\n\t}\n\tif err := f.Close(); err != nil {\n\t\treturn err\n\t}\n\tb.modified = false\n\treturn nil\n}\n\nfunc (b *Bloom) has(p int) bool {\n\tv := b.data[p>>3] >> (7 - (p & 7))\n\treturn v&1 != 0\n}\n\nfunc (b *Bloom) set(p int) {\n\tby := p >> 3\n\tbi := p & 0x7\n\tvar v byte = 1 << (7 - bi)\n\tif b.data[by]&v == 0 {\n\t\tb.data[by] |= v\n\t\tb.modified = true\n\t}\n}\n\ntype bits struct {\n\twidth int    // Number of bits for each position.\n\tbuf   []byte // Remaining bytes to use for next position.\n\tcur   uint64 // Bits to read next position from. Replenished from buf.\n\tncur  int    // Number of bits available in cur. We consume the highest bits first.\n}\n\nfunc hash(v []byte, width int) *bits {\n\tbuf := blake2b.Sum256(v)\n\treturn &bits{width: width, buf: buf[:]}\n}\n\n// nextPos returns the next bit position.\nfunc (b *bits) nextPos() (v int) {\n\tif b.width > b.ncur {\n\t\tfor len(b.buf) > 0 && b.ncur < 64-8 {\n\t\t\tb.cur <<= 8\n\t\t\tb.cur |= uint64(b.buf[0])\n\t\t\tb.ncur += 8\n\t\t\tb.buf = b.buf[1:]\n\t\t}\n\t}\n\tv = int((b.cur >> (b.ncur - b.width)) & ((1 << b.width) - 1))\n\tb.ncur -= b.width\n\treturn v\n}\n\n```\n# Function Signature: func NewBloom(data []byte, k int) (*Bloom, error) {\n# Natural Language Annotations: // NewBloom returns a bloom filter with given initial data.\n//\n// The number of bits in data must be a power of 2.\n// K is the number of \"hashes\" (bits) to store/lookup for each value stored.\n// Width is calculated as the number of bits needed to represent a single bit/hash\n// position in the data.\n//\n// For each value stored/looked up, a hash over the value is calculated. The hash\n// is split into \"k\" values that are \"width\" bits wide, each used to lookup a bit.\n// K * width must not exceed 256.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewBloom(data []byte, k int) (*Bloom, error) {\n\tfileSize := len(data) * 8\n\tw, err := bloomValid(fileSize, k)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &Bloom{\n\t\tdata: data,\n\t\tk:    k,\n\t\tw:    w,\n\t}, nil\n}\n```", "```go\nfunc NewBloom(data []byte, k int) (*Bloom, error) {\n\tfileSize := len(data) * 8 // Convert bytes to bits\n\n\tw, err := bloomValid(fileSize, k)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn &Bloom{\n\t\tdata: data,\n\t\tk:    k,\n\t\tw:    w,\n\t}, nil\n}\n```", "```go\nfunc NewBloom(data []byte, k int) (*Bloom, error) {\n\tfileSize := len(data) * 8 // Convert byte size to bit size\n\tw, err := bloomValid(fileSize, k)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &Bloom{data: data, k: k, w: w, modified: false}, nil\n}\n```"]}, "mox-junk/filter.go-NewFilter": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/junk/filter.go:\n```\n// Package junk implements a bayesian spam filter.\n//\n// A message can be parsed into words. Words (or pairs or triplets) can be used\n// to train the filter or to classify the message as ham or spam.  Training\n// records the words in the database as ham/spam. Classifying consists of\n// calculating the ham/spam probability by combining the words in the message\n// with their ham/spam status.\npackage junk\n\n// todo: look at inverse chi-square function? see https://www.linuxjournal.com/article/6467\n// todo: perhaps: whether anchor text in links in html are different from the url\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"math\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"time\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n)\n\nvar (\n\t// errBadContentType = errors.New(\"bad content-type\") // sure sign of spam, todo: use this error\n\terrClosed = errors.New(\"filter is closed\")\n)\n\ntype word struct {\n\tHam  uint32\n\tSpam uint32\n}\n\ntype wordscore struct {\n\tWord string\n\tHam  uint32\n\tSpam uint32\n}\n\n// Params holds parameters for the filter. Most are at test-time. The first are\n// used during parsing and training.\ntype Params struct {\n\tOnegrams    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for single words.\"`\n\tTwograms    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each two consecutive words.\"`\n\tThreegrams  bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each three consecutive words.\"`\n\tMaxPower    float64 `sconf-doc:\"Maximum power a word (combination) can have. If spaminess is 0.99, and max power is 0.1, spaminess of the word will be set to 0.9. Similar for ham words.\"`\n\tTopWords    int     `sconf-doc:\"Number of most spammy/hammy words to use for calculating probability. E.g. 10.\"`\n\tIgnoreWords float64 `sconf:\"optional\" sconf-doc:\"Ignore words that are this much away from 0.5 haminess/spaminess. E.g. 0.1, causing word (combinations) of 0.4 to 0.6 to be ignored.\"`\n\tRareWords   int     `sconf:\"optional\" sconf-doc:\"Occurrences in word database until a word is considered rare and its influence in calculating probability reduced. E.g. 1 or 2.\"`\n}\n\nvar DBTypes = []any{wordscore{}} // Stored in DB.\n\ntype Filter struct {\n\tParams\n\n\tlog               mlog.Log // For logging cid.\n\tclosed            bool\n\tmodified          bool            // Whether any modifications are pending. Cleared by Save.\n\thams, spams       uint32          // Message count, stored in db under word \"-\".\n\tcache             map[string]word // Words read from database or during training.\n\tchanged           map[string]word // Words modified during training.\n\tdbPath, bloomPath string\n\tdb                *bstore.DB // Always open on a filter.\n\tbloom             *Bloom     // Only opened when writing.\n\tisNew             bool       // Set for new filters until their first sync to disk. For faster writing.\n}\n\nfunc (f *Filter) ensureBloom() error {\n\tif f.bloom != nil {\n\t\treturn nil\n\t}\n\tvar err error\n\tf.bloom, err = openBloom(f.bloomPath)\n\treturn err\n}\n\n// CloseDiscard closes the filter, discarding any changes.\nfunc (f *Filter) CloseDiscard() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\terr := f.db.Close()\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\n// Close first saves the filter if it has modifications, then closes the database\n// connection and releases the bloom filter.\nfunc (f *Filter) Close() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\tvar err error\n\tif f.modified {\n\t\terr = f.Save()\n\t}\n\tif err != nil {\n\t\tf.db.Close()\n\t} else {\n\t\terr = f.db.Close()\n\t}\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\nfunc OpenFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string, loadBloom bool) (*Filter, error) {\n\tvar bloom *Bloom\n\tif loadBloom {\n\t\tvar err error\n\t\tbloom, err = openBloom(bloomPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else if fi, err := os.Stat(bloomPath); err == nil {\n\t\tif err := BloomValid(int(fi.Size()), bloomK); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"bloom: %s\", err)\n\t\t}\n\t}\n\n\tdb, err := openDB(ctx, log, dbPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open database: %s\", err)\n\t}\n\n\tf := &Filter{\n\t\tParams:    params,\n\t\tlog:       log,\n\t\tcache:     map[string]word{},\n\t\tchanged:   map[string]word{},\n\t\tdbPath:    dbPath,\n\t\tbloomPath: bloomPath,\n\t\tdb:        db,\n\t\tbloom:     bloom,\n\t}\n\terr = f.db.Read(ctx, func(tx *bstore.Tx) error {\n\t\twc := wordscore{Word: \"-\"}\n\t\terr := tx.Get(&wc)\n\t\tf.hams = wc.Ham\n\t\tf.spams = wc.Spam\n\t\treturn err\n\t})\n\tif err != nil {\n\t\tcerr := f.Close()\n\t\tlog.Check(cerr, \"closing filter after error\")\n\t\treturn nil, fmt.Errorf(\"looking up ham/spam message count: %s\", err)\n\t}\n\treturn f, nil\n}\n\n// NewFilter creates a new filter with empty bloom filter and database files. The\n// filter is marked as new until the first save, will be done automatically if\n// TrainDirs is called. If the bloom and/or database files exist, an error is\n// returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconst bloomK = 10\n\nfunc openBloom(path string) (*Bloom, error) {\n\tbuf, err := os.ReadFile(path)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading bloom file: %w\", err)\n\t}\n\treturn NewBloom(buf, bloomK)\n}\n\nfunc newDB(ctx context.Context, log mlog.Log, path string) (db *bstore.DB, rerr error) {\n\t// Remove any existing files.\n\tos.Remove(path)\n\n\tdefer func() {\n\t\tif rerr != nil {\n\t\t\terr := os.Remove(path)\n\t\t\tlog.Check(err, \"removing db file after init error\")\n\t\t}\n\t}()\n\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\tdb, err := bstore.Open(ctx, path, &opts, DBTypes...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open new database: %w\", err)\n\t}\n\treturn db, nil\n}\n\nfunc openDB(ctx context.Context, log mlog.Log, path string) (*bstore.DB, error) {\n\tif _, err := os.Stat(path); err != nil {\n\t\treturn nil, fmt.Errorf(\"stat db file: %w\", err)\n\t}\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\treturn bstore.Open(ctx, path, &opts, DBTypes...)\n}\n\n// Save stores modifications, e.g. from training, to the database and bloom\n// filter files.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc loadWords(ctx context.Context, db *bstore.DB, l []string, dst map[string]word) error {\n\tsort.Slice(l, func(i, j int) bool {\n\t\treturn l[i] < l[j]\n\t})\n\n\terr := db.Read(ctx, func(tx *bstore.Tx) error {\n\t\tfor _, w := range l {\n\t\t\twc := wordscore{Word: w}\n\t\t\tif err := tx.Get(&wc); err == nil {\n\t\t\t\tdst[w] = word{wc.Ham, wc.Spam}\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"fetching words: %s\", err)\n\t}\n\treturn nil\n}\n\n// ClassifyWords returns the spam probability for the given words, and number of recognized ham and spam words.\nfunc (f *Filter) ClassifyWords(ctx context.Context, words map[string]struct{}) (probability float64, nham, nspam int, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, 0, errClosed\n\t}\n\n\ttype xword struct {\n\t\tWord string\n\t\tR    float64\n\t}\n\n\tvar hamHigh float64 = 0\n\tvar spamLow float64 = 1\n\tvar topHam []xword\n\tvar topSpam []xword\n\n\t// Find words that should be in the database.\n\tlookupWords := []string{}\n\texpect := map[string]struct{}{}\n\tunknowns := map[string]struct{}{}\n\ttotalUnknown := 0\n\tfor w := range words {\n\t\tif f.bloom != nil && !f.bloom.Has(w) {\n\t\t\ttotalUnknown++\n\t\t\tif len(unknowns) < 50 {\n\t\t\t\tunknowns[w] = struct{}{}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; ok {\n\t\t\tcontinue\n\t\t}\n\t\tlookupWords = append(lookupWords, w)\n\t\texpect[w] = struct{}{}\n\t}\n\tif len(unknowns) > 0 {\n\t\tf.log.Debug(\"unknown words in bloom filter, showing max 50\",\n\t\t\tslog.Any(\"words\", unknowns),\n\t\t\tslog.Any(\"totalunknown\", totalUnknown),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\t// Fetch words from database.\n\tfetched := map[string]word{}\n\tif len(lookupWords) > 0 {\n\t\tif err := loadWords(ctx, f.db, lookupWords, fetched); err != nil {\n\t\t\treturn 0, 0, 0, err\n\t\t}\n\t\tfor w, c := range fetched {\n\t\t\tdelete(expect, w)\n\t\t\tf.cache[w] = c\n\t\t}\n\t\tf.log.Debug(\"unknown words in db\",\n\t\t\tslog.Any(\"words\", expect),\n\t\t\tslog.Any(\"totalunknown\", len(expect)),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tvar wS, wH float64\n\t\tif f.spams > 0 {\n\t\t\twS = float64(c.Spam) / float64(f.spams)\n\t\t}\n\t\tif f.hams > 0 {\n\t\t\twH = float64(c.Ham) / float64(f.hams)\n\t\t}\n\t\tr := wS / (wS + wH)\n\n\t\tif r < f.MaxPower {\n\t\t\tr = f.MaxPower\n\t\t} else if r >= 1-f.MaxPower {\n\t\t\tr = 1 - f.MaxPower\n\t\t}\n\n\t\tif c.Ham+c.Spam <= uint32(f.RareWords) {\n\t\t\t// Reduce the power of rare words.\n\t\t\tr += float64(1+uint32(f.RareWords)-(c.Ham+c.Spam)) * (0.5 - r) / 10\n\t\t}\n\t\tif math.Abs(0.5-r) < f.IgnoreWords {\n\t\t\tcontinue\n\t\t}\n\t\tif r < 0.5 {\n\t\t\tif len(topHam) >= f.TopWords && r > hamHigh {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopHam = append(topHam, xword{w, r})\n\t\t\tif r > hamHigh {\n\t\t\t\thamHigh = r\n\t\t\t}\n\t\t} else if r > 0.5 {\n\t\t\tif len(topSpam) >= f.TopWords && r < spamLow {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopSpam = append(topSpam, xword{w, r})\n\t\t\tif r < spamLow {\n\t\t\t\tspamLow = r\n\t\t\t}\n\t\t}\n\t}\n\n\tsort.Slice(topHam, func(i, j int) bool {\n\t\ta, b := topHam[i], topHam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R < b.R\n\t})\n\tsort.Slice(topSpam, func(i, j int) bool {\n\t\ta, b := topSpam[i], topSpam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R > b.R\n\t})\n\n\tnham = f.TopWords\n\tif nham > len(topHam) {\n\t\tnham = len(topHam)\n\t}\n\tnspam = f.TopWords\n\tif nspam > len(topSpam) {\n\t\tnspam = len(topSpam)\n\t}\n\ttopHam = topHam[:nham]\n\ttopSpam = topSpam[:nspam]\n\n\tvar eta float64\n\tfor _, x := range topHam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\tfor _, x := range topSpam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\n\tf.log.Debug(\"top words\", slog.Any(\"hams\", topHam), slog.Any(\"spams\", topSpam))\n\n\tprob := 1 / (1 + math.Pow(math.E, eta))\n\treturn prob, len(topHam), len(topSpam), nil\n}\n\n// ClassifyMessagePath is a convenience wrapper for calling ClassifyMessage on a file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) ClassifyMessageReader(ctx context.Context, mf io.ReaderAt, size int64) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tm, err := message.EnsurePart(f.log.Logger, false, mf, size)\n\tif err != nil && errors.Is(err, message.ErrBadContentType) {\n\t\t// Invalid content-type header is a sure sign of spam.\n\t\t//f.log.Infox(\"parsing content\", err)\n\t\treturn 1, nil, 0, 0, nil\n\t}\n\treturn f.ClassifyMessage(ctx, m)\n}\n\n// ClassifyMessage parses the mail message in r and returns the spam probability\n// (between 0 and 1), along with the tokenized words found in the message, and the\n// number of recognized ham and spam words.\nfunc (f *Filter) ClassifyMessage(ctx context.Context, m message.Part) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tvar err error\n\twords, err = f.ParseMessage(m)\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, err\n\t}\n\n\tprobability, nham, nspam, err = f.ClassifyWords(ctx, words)\n\treturn probability, words, nham, nspam, err\n}\n\n// Train adds the words of a single message to the filter.\nfunc (f *Filter) Train(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\tvar lwords []string\n\n\tfor w := range words {\n\t\tif !f.bloom.Has(w) {\n\t\t\tf.bloom.Add(w)\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\tf.modified = true\n\tif ham {\n\t\tf.hams++\n\t} else {\n\t\tf.spams++\n\t}\n\n\tfor w := range words {\n\t\tc := f.cache[w]\n\t\tif ham {\n\t\t\tc.Ham++\n\t\t} else {\n\t\t\tc.Spam++\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\nfunc (f *Filter) TrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Train(ctx, ham, words)\n}\n\nfunc (f *Filter) UntrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Untrain(ctx, ham, words)\n}\n\nfunc (f *Filter) loadCache(ctx context.Context, lwords []string) error {\n\tif len(lwords) == 0 {\n\t\treturn nil\n\t}\n\n\treturn loadWords(ctx, f.db, lwords, f.cache)\n}\n\n// Untrain adjusts the filter to undo a previous training of the words.\nfunc (f *Filter) Untrain(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\t// Lookup any words from the db that aren't in the cache and put them in the cache for modification.\n\tvar lwords []string\n\tfor w := range words {\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\t// Modify the message count.\n\tf.modified = true\n\tif ham {\n\t\tf.hams--\n\t} else {\n\t\tf.spams--\n\t}\n\n\t// Decrease the word counts.\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif ham {\n\t\t\tc.Ham--\n\t\t} else {\n\t\t\tc.Spam--\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\n// TrainDir parses mail messages from files and trains the filter.\nfunc (f *Filter) TrainDir(dir string, files []string, ham bool) (n, malformed uint32, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, errClosed\n\t}\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\tfor _, name := range files {\n\t\tp := filepath.Join(dir, name)\n\t\tvalid, words, err := f.tokenizeMail(p)\n\t\tif err != nil {\n\t\t\t// f.log.Infox(\"tokenizing mail\", err, slog.Any(\"path\", p))\n\t\t\tmalformed++\n\t\t\tcontinue\n\t\t}\n\t\tif !valid {\n\t\t\tcontinue\n\t\t}\n\t\tn++\n\t\tfor w := range words {\n\t\t\tif !f.bloom.Has(w) {\n\t\t\t\tf.bloom.Add(w)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc := f.cache[w]\n\t\t\tf.modified = true\n\t\t\tif ham {\n\t\t\t\tc.Ham++\n\t\t\t} else {\n\t\t\t\tc.Spam++\n\t\t\t}\n\t\t\tf.cache[w] = c\n\t\t\tf.changed[w] = c\n\t\t}\n\t}\n\treturn\n}\n\n// TrainDirs trains and saves a filter with mail messages from different types\n// of directories.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) fileSize(p string) int {\n\tfi, err := os.Stat(p)\n\tif err != nil {\n\t\tf.log.Infox(\"stat\", err, slog.Any(\"path\", p))\n\t\treturn 0\n\t}\n\treturn int(fi.Size())\n}\n\n// DB returns the database, for backups.\nfunc (f *Filter) DB() *bstore.DB {\n\treturn f.db\n}\n\n```\n# Function Signature: func NewFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string) (*Filter, error) {\n# Natural Language Annotations: // NewFilter creates a new filter with empty bloom filter and database files. The\n// filter is marked as new until the first save, will be done automatically if\n// TrainDirs is called. If the bloom and/or database files exist, an error is\n// returned.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string) (*Filter, error) {\n\tif _, err := os.Stat(dbPath); err == nil {\n\t\treturn nil, fmt.Errorf(\"database file already exists: %s\", dbPath)\n\t}\n\n\tif _, err := os.Stat(bloomPath); err == nil {\n\t\treturn nil, fmt.Errorf(\"bloom file already exists: %s\", bloomPath)\n\t}\n\n\tbloom, err := newBloom(bloomPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"creating new bloom filter: %w\", err)\n\t}\n\n\tdb, err := newDB(ctx, log, dbPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"creating new database: %w\", err)\n\t}\n\n\tf := &Filter{\n\t\tParams:    params,\n\t\tlog:       log,\n\t\tcache:     map[string]word{},\n\t\tchanged:   map[string]word{},\n\t\tdbPath:    dbPath,\n\t\tbloomPath: bloomPath,\n\t\tdb:        db,\n\t\tbloom:     bloom,\n\t\tisNew:     true,\n\t}\n\n\terr = f.db.Write(ctx, func(tx *bstore.Tx) error {\n\t\treturn tx.Put(&wordscore{Word: \"-\"})\n\t})\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"initializing database: %w\", err)\n\t}\n\n\treturn f, nil\n}\n```\nPlease note that the function `newBloom` and `newDB` are not defined in the given code snippet. They are expected to be implemented elsewhere in your codebase.", "```go\nfunc NewFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string) (*Filter, error) {\n\tbloom, err := newDB(ctx, log, dbPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open new database: %w\", err)\n\t}\n\n\tf := &Filter{\n\t\tParams:    params,\n\t\tlog:       log,\n\t\tcache:     map[string]word{},\n\t\tchanged:   map[string]word{},\n\t\tdbPath:    dbPath,\n\t\tbloomPath: bloomPath,\n\t\tdb:        bloom,\n\t\tisNew:     true,\n\t}\n\treturn f, nil\n}\n```\nYou can see that the function creates a new database using the `bstore.DB` package and sets up a new `Filter` with the provided `Params`, `log`, `dbPath` and `bloomPath`. The `isNew` field is set to `true` to indicate that the filter is new and should be saved before it can be used for classification.", "```go\nfunc NewFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string) (*Filter, error) {\n\tif _, err := os.Stat(dbPath); !os.IsNotExist(err) {\n\t\treturn nil, fmt.Errorf(\"database file already exists: %s\", err)\n\t}\n\tif _, err := os.Stat(bloomPath); !os.IsNotExist(err) {\n\t\treturn nil, fmt.Errorf(\"bloom file already exists: %s\", err)\n\t}\n\n\tdb, err := newDB(ctx, log, dbPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"creating new database: %s\", err)\n\t}\n\n\tf := &Filter{\n\t\tParams:    params,\n\t\tlog:       log,\n\t\tcache:     map[string]word{},\n\t\tchanged:   map[string]word{},\n\t\tdbPath:    dbPath,\n\t\tbloomPath: bloomPath,\n\t\tdb:        db,\n\t\tisNew:     true,\n\t}\n\terr = f.db.Update(ctx, func(tx *bstore.Tx) error {\n\t\twc := wordscore{Word: \"-\"}\n\t\treturn tx.Put(&wc)\n\t})\n\tif err != nil {\n\t\tcerr := f.Close()\n\t\tlog.Check(cerr, \"closing filter after error\")\n\t\treturn nil, fmt.Errorf(\"initializing ham/spam message count: %s\", err)\n\t}\n\treturn f, nil\n}\n```\nThis function initializes a new filter with empty bloom filter and database files. It checks if the database and bloom files exist and returns an error if they do. It then creates a new database and bloom filter, and initializes the filter with the given parameters, database path, and bloom path. The filter is marked as new until the first save, which will be done automatically if `TrainDirs` is called."]}, "mox-junk/filter.go-Save": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/junk/filter.go:\n```\n// Package junk implements a bayesian spam filter.\n//\n// A message can be parsed into words. Words (or pairs or triplets) can be used\n// to train the filter or to classify the message as ham or spam.  Training\n// records the words in the database as ham/spam. Classifying consists of\n// calculating the ham/spam probability by combining the words in the message\n// with their ham/spam status.\npackage junk\n\n// todo: look at inverse chi-square function? see https://www.linuxjournal.com/article/6467\n// todo: perhaps: whether anchor text in links in html are different from the url\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"math\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"time\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n)\n\nvar (\n\t// errBadContentType = errors.New(\"bad content-type\") // sure sign of spam, todo: use this error\n\terrClosed = errors.New(\"filter is closed\")\n)\n\ntype word struct {\n\tHam  uint32\n\tSpam uint32\n}\n\ntype wordscore struct {\n\tWord string\n\tHam  uint32\n\tSpam uint32\n}\n\n// Params holds parameters for the filter. Most are at test-time. The first are\n// used during parsing and training.\ntype Params struct {\n\tOnegrams    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for single words.\"`\n\tTwograms    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each two consecutive words.\"`\n\tThreegrams  bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each three consecutive words.\"`\n\tMaxPower    float64 `sconf-doc:\"Maximum power a word (combination) can have. If spaminess is 0.99, and max power is 0.1, spaminess of the word will be set to 0.9. Similar for ham words.\"`\n\tTopWords    int     `sconf-doc:\"Number of most spammy/hammy words to use for calculating probability. E.g. 10.\"`\n\tIgnoreWords float64 `sconf:\"optional\" sconf-doc:\"Ignore words that are this much away from 0.5 haminess/spaminess. E.g. 0.1, causing word (combinations) of 0.4 to 0.6 to be ignored.\"`\n\tRareWords   int     `sconf:\"optional\" sconf-doc:\"Occurrences in word database until a word is considered rare and its influence in calculating probability reduced. E.g. 1 or 2.\"`\n}\n\nvar DBTypes = []any{wordscore{}} // Stored in DB.\n\ntype Filter struct {\n\tParams\n\n\tlog               mlog.Log // For logging cid.\n\tclosed            bool\n\tmodified          bool            // Whether any modifications are pending. Cleared by Save.\n\thams, spams       uint32          // Message count, stored in db under word \"-\".\n\tcache             map[string]word // Words read from database or during training.\n\tchanged           map[string]word // Words modified during training.\n\tdbPath, bloomPath string\n\tdb                *bstore.DB // Always open on a filter.\n\tbloom             *Bloom     // Only opened when writing.\n\tisNew             bool       // Set for new filters until their first sync to disk. For faster writing.\n}\n\nfunc (f *Filter) ensureBloom() error {\n\tif f.bloom != nil {\n\t\treturn nil\n\t}\n\tvar err error\n\tf.bloom, err = openBloom(f.bloomPath)\n\treturn err\n}\n\n// CloseDiscard closes the filter, discarding any changes.\nfunc (f *Filter) CloseDiscard() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\terr := f.db.Close()\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\n// Close first saves the filter if it has modifications, then closes the database\n// connection and releases the bloom filter.\nfunc (f *Filter) Close() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\tvar err error\n\tif f.modified {\n\t\terr = f.Save()\n\t}\n\tif err != nil {\n\t\tf.db.Close()\n\t} else {\n\t\terr = f.db.Close()\n\t}\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\nfunc OpenFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string, loadBloom bool) (*Filter, error) {\n\tvar bloom *Bloom\n\tif loadBloom {\n\t\tvar err error\n\t\tbloom, err = openBloom(bloomPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else if fi, err := os.Stat(bloomPath); err == nil {\n\t\tif err := BloomValid(int(fi.Size()), bloomK); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"bloom: %s\", err)\n\t\t}\n\t}\n\n\tdb, err := openDB(ctx, log, dbPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open database: %s\", err)\n\t}\n\n\tf := &Filter{\n\t\tParams:    params,\n\t\tlog:       log,\n\t\tcache:     map[string]word{},\n\t\tchanged:   map[string]word{},\n\t\tdbPath:    dbPath,\n\t\tbloomPath: bloomPath,\n\t\tdb:        db,\n\t\tbloom:     bloom,\n\t}\n\terr = f.db.Read(ctx, func(tx *bstore.Tx) error {\n\t\twc := wordscore{Word: \"-\"}\n\t\terr := tx.Get(&wc)\n\t\tf.hams = wc.Ham\n\t\tf.spams = wc.Spam\n\t\treturn err\n\t})\n\tif err != nil {\n\t\tcerr := f.Close()\n\t\tlog.Check(cerr, \"closing filter after error\")\n\t\treturn nil, fmt.Errorf(\"looking up ham/spam message count: %s\", err)\n\t}\n\treturn f, nil\n}\n\n// NewFilter creates a new filter with empty bloom filter and database files. The\n// filter is marked as new until the first save, will be done automatically if\n// TrainDirs is called. If the bloom and/or database files exist, an error is\n// returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconst bloomK = 10\n\nfunc openBloom(path string) (*Bloom, error) {\n\tbuf, err := os.ReadFile(path)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading bloom file: %w\", err)\n\t}\n\treturn NewBloom(buf, bloomK)\n}\n\nfunc newDB(ctx context.Context, log mlog.Log, path string) (db *bstore.DB, rerr error) {\n\t// Remove any existing files.\n\tos.Remove(path)\n\n\tdefer func() {\n\t\tif rerr != nil {\n\t\t\terr := os.Remove(path)\n\t\t\tlog.Check(err, \"removing db file after init error\")\n\t\t}\n\t}()\n\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\tdb, err := bstore.Open(ctx, path, &opts, DBTypes...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open new database: %w\", err)\n\t}\n\treturn db, nil\n}\n\nfunc openDB(ctx context.Context, log mlog.Log, path string) (*bstore.DB, error) {\n\tif _, err := os.Stat(path); err != nil {\n\t\treturn nil, fmt.Errorf(\"stat db file: %w\", err)\n\t}\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\treturn bstore.Open(ctx, path, &opts, DBTypes...)\n}\n\n// Save stores modifications, e.g. from training, to the database and bloom\n// filter files.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc loadWords(ctx context.Context, db *bstore.DB, l []string, dst map[string]word) error {\n\tsort.Slice(l, func(i, j int) bool {\n\t\treturn l[i] < l[j]\n\t})\n\n\terr := db.Read(ctx, func(tx *bstore.Tx) error {\n\t\tfor _, w := range l {\n\t\t\twc := wordscore{Word: w}\n\t\t\tif err := tx.Get(&wc); err == nil {\n\t\t\t\tdst[w] = word{wc.Ham, wc.Spam}\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"fetching words: %s\", err)\n\t}\n\treturn nil\n}\n\n// ClassifyWords returns the spam probability for the given words, and number of recognized ham and spam words.\nfunc (f *Filter) ClassifyWords(ctx context.Context, words map[string]struct{}) (probability float64, nham, nspam int, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, 0, errClosed\n\t}\n\n\ttype xword struct {\n\t\tWord string\n\t\tR    float64\n\t}\n\n\tvar hamHigh float64 = 0\n\tvar spamLow float64 = 1\n\tvar topHam []xword\n\tvar topSpam []xword\n\n\t// Find words that should be in the database.\n\tlookupWords := []string{}\n\texpect := map[string]struct{}{}\n\tunknowns := map[string]struct{}{}\n\ttotalUnknown := 0\n\tfor w := range words {\n\t\tif f.bloom != nil && !f.bloom.Has(w) {\n\t\t\ttotalUnknown++\n\t\t\tif len(unknowns) < 50 {\n\t\t\t\tunknowns[w] = struct{}{}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; ok {\n\t\t\tcontinue\n\t\t}\n\t\tlookupWords = append(lookupWords, w)\n\t\texpect[w] = struct{}{}\n\t}\n\tif len(unknowns) > 0 {\n\t\tf.log.Debug(\"unknown words in bloom filter, showing max 50\",\n\t\t\tslog.Any(\"words\", unknowns),\n\t\t\tslog.Any(\"totalunknown\", totalUnknown),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\t// Fetch words from database.\n\tfetched := map[string]word{}\n\tif len(lookupWords) > 0 {\n\t\tif err := loadWords(ctx, f.db, lookupWords, fetched); err != nil {\n\t\t\treturn 0, 0, 0, err\n\t\t}\n\t\tfor w, c := range fetched {\n\t\t\tdelete(expect, w)\n\t\t\tf.cache[w] = c\n\t\t}\n\t\tf.log.Debug(\"unknown words in db\",\n\t\t\tslog.Any(\"words\", expect),\n\t\t\tslog.Any(\"totalunknown\", len(expect)),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tvar wS, wH float64\n\t\tif f.spams > 0 {\n\t\t\twS = float64(c.Spam) / float64(f.spams)\n\t\t}\n\t\tif f.hams > 0 {\n\t\t\twH = float64(c.Ham) / float64(f.hams)\n\t\t}\n\t\tr := wS / (wS + wH)\n\n\t\tif r < f.MaxPower {\n\t\t\tr = f.MaxPower\n\t\t} else if r >= 1-f.MaxPower {\n\t\t\tr = 1 - f.MaxPower\n\t\t}\n\n\t\tif c.Ham+c.Spam <= uint32(f.RareWords) {\n\t\t\t// Reduce the power of rare words.\n\t\t\tr += float64(1+uint32(f.RareWords)-(c.Ham+c.Spam)) * (0.5 - r) / 10\n\t\t}\n\t\tif math.Abs(0.5-r) < f.IgnoreWords {\n\t\t\tcontinue\n\t\t}\n\t\tif r < 0.5 {\n\t\t\tif len(topHam) >= f.TopWords && r > hamHigh {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopHam = append(topHam, xword{w, r})\n\t\t\tif r > hamHigh {\n\t\t\t\thamHigh = r\n\t\t\t}\n\t\t} else if r > 0.5 {\n\t\t\tif len(topSpam) >= f.TopWords && r < spamLow {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopSpam = append(topSpam, xword{w, r})\n\t\t\tif r < spamLow {\n\t\t\t\tspamLow = r\n\t\t\t}\n\t\t}\n\t}\n\n\tsort.Slice(topHam, func(i, j int) bool {\n\t\ta, b := topHam[i], topHam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R < b.R\n\t})\n\tsort.Slice(topSpam, func(i, j int) bool {\n\t\ta, b := topSpam[i], topSpam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R > b.R\n\t})\n\n\tnham = f.TopWords\n\tif nham > len(topHam) {\n\t\tnham = len(topHam)\n\t}\n\tnspam = f.TopWords\n\tif nspam > len(topSpam) {\n\t\tnspam = len(topSpam)\n\t}\n\ttopHam = topHam[:nham]\n\ttopSpam = topSpam[:nspam]\n\n\tvar eta float64\n\tfor _, x := range topHam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\tfor _, x := range topSpam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\n\tf.log.Debug(\"top words\", slog.Any(\"hams\", topHam), slog.Any(\"spams\", topSpam))\n\n\tprob := 1 / (1 + math.Pow(math.E, eta))\n\treturn prob, len(topHam), len(topSpam), nil\n}\n\n// ClassifyMessagePath is a convenience wrapper for calling ClassifyMessage on a file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) ClassifyMessageReader(ctx context.Context, mf io.ReaderAt, size int64) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tm, err := message.EnsurePart(f.log.Logger, false, mf, size)\n\tif err != nil && errors.Is(err, message.ErrBadContentType) {\n\t\t// Invalid content-type header is a sure sign of spam.\n\t\t//f.log.Infox(\"parsing content\", err)\n\t\treturn 1, nil, 0, 0, nil\n\t}\n\treturn f.ClassifyMessage(ctx, m)\n}\n\n// ClassifyMessage parses the mail message in r and returns the spam probability\n// (between 0 and 1), along with the tokenized words found in the message, and the\n// number of recognized ham and spam words.\nfunc (f *Filter) ClassifyMessage(ctx context.Context, m message.Part) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tvar err error\n\twords, err = f.ParseMessage(m)\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, err\n\t}\n\n\tprobability, nham, nspam, err = f.ClassifyWords(ctx, words)\n\treturn probability, words, nham, nspam, err\n}\n\n// Train adds the words of a single message to the filter.\nfunc (f *Filter) Train(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\tvar lwords []string\n\n\tfor w := range words {\n\t\tif !f.bloom.Has(w) {\n\t\t\tf.bloom.Add(w)\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\tf.modified = true\n\tif ham {\n\t\tf.hams++\n\t} else {\n\t\tf.spams++\n\t}\n\n\tfor w := range words {\n\t\tc := f.cache[w]\n\t\tif ham {\n\t\t\tc.Ham++\n\t\t} else {\n\t\t\tc.Spam++\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\nfunc (f *Filter) TrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Train(ctx, ham, words)\n}\n\nfunc (f *Filter) UntrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Untrain(ctx, ham, words)\n}\n\nfunc (f *Filter) loadCache(ctx context.Context, lwords []string) error {\n\tif len(lwords) == 0 {\n\t\treturn nil\n\t}\n\n\treturn loadWords(ctx, f.db, lwords, f.cache)\n}\n\n// Untrain adjusts the filter to undo a previous training of the words.\nfunc (f *Filter) Untrain(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\t// Lookup any words from the db that aren't in the cache and put them in the cache for modification.\n\tvar lwords []string\n\tfor w := range words {\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\t// Modify the message count.\n\tf.modified = true\n\tif ham {\n\t\tf.hams--\n\t} else {\n\t\tf.spams--\n\t}\n\n\t// Decrease the word counts.\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif ham {\n\t\t\tc.Ham--\n\t\t} else {\n\t\t\tc.Spam--\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\n// TrainDir parses mail messages from files and trains the filter.\nfunc (f *Filter) TrainDir(dir string, files []string, ham bool) (n, malformed uint32, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, errClosed\n\t}\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\tfor _, name := range files {\n\t\tp := filepath.Join(dir, name)\n\t\tvalid, words, err := f.tokenizeMail(p)\n\t\tif err != nil {\n\t\t\t// f.log.Infox(\"tokenizing mail\", err, slog.Any(\"path\", p))\n\t\t\tmalformed++\n\t\t\tcontinue\n\t\t}\n\t\tif !valid {\n\t\t\tcontinue\n\t\t}\n\t\tn++\n\t\tfor w := range words {\n\t\t\tif !f.bloom.Has(w) {\n\t\t\t\tf.bloom.Add(w)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc := f.cache[w]\n\t\t\tf.modified = true\n\t\t\tif ham {\n\t\t\t\tc.Ham++\n\t\t\t} else {\n\t\t\t\tc.Spam++\n\t\t\t}\n\t\t\tf.cache[w] = c\n\t\t\tf.changed[w] = c\n\t\t}\n\t}\n\treturn\n}\n\n// TrainDirs trains and saves a filter with mail messages from different types\n// of directories.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) fileSize(p string) int {\n\tfi, err := os.Stat(p)\n\tif err != nil {\n\t\tf.log.Infox(\"stat\", err, slog.Any(\"path\", p))\n\t\treturn 0\n\t}\n\treturn int(fi.Size())\n}\n\n// DB returns the database, for backups.\nfunc (f *Filter) DB() *bstore.DB {\n\treturn f.db\n}\n\n```\n# Function Signature: func (f *Filter) Save() error {\n# Natural Language Annotations: // Save stores modifications, e.g. from training, to the database and bloom\n// filter files.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (f *Filter) Save() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\n\tif f.bloom != nil {\n\t\tdata := f.bloom.Bytes()\n\t\tif err := os.WriteFile(f.bloomPath, data, 0660); err != nil {\n\t\t\treturn fmt.Errorf(\"writing bloom filter: %w\", err)\n\t\t}\n\t}\n\n\tif len(f.changed) == 0 {\n\t\tif f.bloom == nil {\n\t\t\treturn nil\n\t\t}\n\t}\n\n\terr := f.db.Write(func(tx *bstore.Tx) error {\n\t\tfor w, c := range f.changed {\n\t\t\tif err := tx.Put(wordscore{w, c.Ham, c.Spam}); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tf.changed = map[string]word{}\n\n\t\twc := wordscore{\"-\", f.hams, f.spams}\n\t\treturn tx.Put(wc)\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"writing to database: %w\", err)\n\t}\n\n\tf.modified = false\n\treturn nil\n}\n```\nThis function saves the changes made to the filter to the database and bloom filter files. It first checks if the filter is closed, and if so, returns an error. It then checks if the bloom filter exists and writes it to the file. After that, it checks if any changes have been made to the filter. If no changes are detected, it returns without doing anything. If changes are detected, it proceeds to write them to the database and updates the message count in the database. Finally, it sets the modified flag to false and returns.", "```go\nfunc (f *Filter) Save() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\n\t// Write changes to database.\n\terr := f.db.Write(func(tx *bstore.Tx) error {\n\t\tfor w, c := range f.changed {\n\t\t\tif err := tx.Put(wordscore{w, c.Ham, c.Spam}); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif err := tx.Put(wordscore{\"-\", f.hams, f.spams}); err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"writing to database: %w\", err)\n\t}\n\n\t// Write the bloom filter to a file.\n\tif f.bloom != nil {\n\t\terr = os.WriteFile(f.bloomPath, f.bloom.Bytes(), 0666)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"writing bloom filter to file: %w\", err)\n\t\t}\n\t}\n\n\t// Mark the filter as saved.\n\tf.modified = false\n\tf.changed = map[string]word{}\n\treturn nil\n}\n```", "```go\nfunc (f *Filter) Save() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\t\n\tif f.bloom != nil {\n\t\tif err := f.bloom.WriteFile(f.bloomPath); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif f.isNew {\n\t\t// Only set the new flag after the bloom filter is written, because the new db file will be written if it doesn't exist.\n\t\tf.isNew = false\n\t}\n\n\terr := f.db.Write(func(tx *bstore.Tx) error {\n\t\tfor w, c := range f.changed {\n\t\t\twc := wordscore{Word: w, Ham: c.Ham, Spam: c.Spam}\n\t\t\terr := tx.Put(&wc)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\twc := wordscore{Word: \"-\", Ham: f.hams, Spam: f.spams}\n\t\terr = tx.Put(&wc)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tf.modified = false\n\tf.changed = map[string]word{}\n\n\treturn nil\n}\n```\n\nThis function `Save` is a method for the `Filter` struct. It saves the modifications made to the database and bloom filter files. It first checks if the filter is closed. If it's not, it proceeds to write the bloom filter to the file. After writing the bloom filter, the function checks if it's a new filter. If it is, it sets the `isNew` flag to false.\n\nThe modifications are then written to the database in a transaction. The changes are iterated over and saved to the database. The final modification is the word count, which is also saved.\n\nFinally, if the changes are successfully written to the database, the `modified` flag is set to false and the `changed` map is cleared. This indicates that the filter is no longer modified and no modifications are pending."]}, "mox-junk/filter.go-ClassifyMessagePath": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/junk/filter.go:\n```\n// Package junk implements a bayesian spam filter.\n//\n// A message can be parsed into words. Words (or pairs or triplets) can be used\n// to train the filter or to classify the message as ham or spam.  Training\n// records the words in the database as ham/spam. Classifying consists of\n// calculating the ham/spam probability by combining the words in the message\n// with their ham/spam status.\npackage junk\n\n// todo: look at inverse chi-square function? see https://www.linuxjournal.com/article/6467\n// todo: perhaps: whether anchor text in links in html are different from the url\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"math\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"time\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n)\n\nvar (\n\t// errBadContentType = errors.New(\"bad content-type\") // sure sign of spam, todo: use this error\n\terrClosed = errors.New(\"filter is closed\")\n)\n\ntype word struct {\n\tHam  uint32\n\tSpam uint32\n}\n\ntype wordscore struct {\n\tWord string\n\tHam  uint32\n\tSpam uint32\n}\n\n// Params holds parameters for the filter. Most are at test-time. The first are\n// used during parsing and training.\ntype Params struct {\n\tOnegrams    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for single words.\"`\n\tTwograms    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each two consecutive words.\"`\n\tThreegrams  bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each three consecutive words.\"`\n\tMaxPower    float64 `sconf-doc:\"Maximum power a word (combination) can have. If spaminess is 0.99, and max power is 0.1, spaminess of the word will be set to 0.9. Similar for ham words.\"`\n\tTopWords    int     `sconf-doc:\"Number of most spammy/hammy words to use for calculating probability. E.g. 10.\"`\n\tIgnoreWords float64 `sconf:\"optional\" sconf-doc:\"Ignore words that are this much away from 0.5 haminess/spaminess. E.g. 0.1, causing word (combinations) of 0.4 to 0.6 to be ignored.\"`\n\tRareWords   int     `sconf:\"optional\" sconf-doc:\"Occurrences in word database until a word is considered rare and its influence in calculating probability reduced. E.g. 1 or 2.\"`\n}\n\nvar DBTypes = []any{wordscore{}} // Stored in DB.\n\ntype Filter struct {\n\tParams\n\n\tlog               mlog.Log // For logging cid.\n\tclosed            bool\n\tmodified          bool            // Whether any modifications are pending. Cleared by Save.\n\thams, spams       uint32          // Message count, stored in db under word \"-\".\n\tcache             map[string]word // Words read from database or during training.\n\tchanged           map[string]word // Words modified during training.\n\tdbPath, bloomPath string\n\tdb                *bstore.DB // Always open on a filter.\n\tbloom             *Bloom     // Only opened when writing.\n\tisNew             bool       // Set for new filters until their first sync to disk. For faster writing.\n}\n\nfunc (f *Filter) ensureBloom() error {\n\tif f.bloom != nil {\n\t\treturn nil\n\t}\n\tvar err error\n\tf.bloom, err = openBloom(f.bloomPath)\n\treturn err\n}\n\n// CloseDiscard closes the filter, discarding any changes.\nfunc (f *Filter) CloseDiscard() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\terr := f.db.Close()\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\n// Close first saves the filter if it has modifications, then closes the database\n// connection and releases the bloom filter.\nfunc (f *Filter) Close() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\tvar err error\n\tif f.modified {\n\t\terr = f.Save()\n\t}\n\tif err != nil {\n\t\tf.db.Close()\n\t} else {\n\t\terr = f.db.Close()\n\t}\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\nfunc OpenFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string, loadBloom bool) (*Filter, error) {\n\tvar bloom *Bloom\n\tif loadBloom {\n\t\tvar err error\n\t\tbloom, err = openBloom(bloomPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else if fi, err := os.Stat(bloomPath); err == nil {\n\t\tif err := BloomValid(int(fi.Size()), bloomK); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"bloom: %s\", err)\n\t\t}\n\t}\n\n\tdb, err := openDB(ctx, log, dbPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open database: %s\", err)\n\t}\n\n\tf := &Filter{\n\t\tParams:    params,\n\t\tlog:       log,\n\t\tcache:     map[string]word{},\n\t\tchanged:   map[string]word{},\n\t\tdbPath:    dbPath,\n\t\tbloomPath: bloomPath,\n\t\tdb:        db,\n\t\tbloom:     bloom,\n\t}\n\terr = f.db.Read(ctx, func(tx *bstore.Tx) error {\n\t\twc := wordscore{Word: \"-\"}\n\t\terr := tx.Get(&wc)\n\t\tf.hams = wc.Ham\n\t\tf.spams = wc.Spam\n\t\treturn err\n\t})\n\tif err != nil {\n\t\tcerr := f.Close()\n\t\tlog.Check(cerr, \"closing filter after error\")\n\t\treturn nil, fmt.Errorf(\"looking up ham/spam message count: %s\", err)\n\t}\n\treturn f, nil\n}\n\n// NewFilter creates a new filter with empty bloom filter and database files. The\n// filter is marked as new until the first save, will be done automatically if\n// TrainDirs is called. If the bloom and/or database files exist, an error is\n// returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconst bloomK = 10\n\nfunc openBloom(path string) (*Bloom, error) {\n\tbuf, err := os.ReadFile(path)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading bloom file: %w\", err)\n\t}\n\treturn NewBloom(buf, bloomK)\n}\n\nfunc newDB(ctx context.Context, log mlog.Log, path string) (db *bstore.DB, rerr error) {\n\t// Remove any existing files.\n\tos.Remove(path)\n\n\tdefer func() {\n\t\tif rerr != nil {\n\t\t\terr := os.Remove(path)\n\t\t\tlog.Check(err, \"removing db file after init error\")\n\t\t}\n\t}()\n\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\tdb, err := bstore.Open(ctx, path, &opts, DBTypes...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open new database: %w\", err)\n\t}\n\treturn db, nil\n}\n\nfunc openDB(ctx context.Context, log mlog.Log, path string) (*bstore.DB, error) {\n\tif _, err := os.Stat(path); err != nil {\n\t\treturn nil, fmt.Errorf(\"stat db file: %w\", err)\n\t}\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\treturn bstore.Open(ctx, path, &opts, DBTypes...)\n}\n\n// Save stores modifications, e.g. from training, to the database and bloom\n// filter files.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc loadWords(ctx context.Context, db *bstore.DB, l []string, dst map[string]word) error {\n\tsort.Slice(l, func(i, j int) bool {\n\t\treturn l[i] < l[j]\n\t})\n\n\terr := db.Read(ctx, func(tx *bstore.Tx) error {\n\t\tfor _, w := range l {\n\t\t\twc := wordscore{Word: w}\n\t\t\tif err := tx.Get(&wc); err == nil {\n\t\t\t\tdst[w] = word{wc.Ham, wc.Spam}\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"fetching words: %s\", err)\n\t}\n\treturn nil\n}\n\n// ClassifyWords returns the spam probability for the given words, and number of recognized ham and spam words.\nfunc (f *Filter) ClassifyWords(ctx context.Context, words map[string]struct{}) (probability float64, nham, nspam int, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, 0, errClosed\n\t}\n\n\ttype xword struct {\n\t\tWord string\n\t\tR    float64\n\t}\n\n\tvar hamHigh float64 = 0\n\tvar spamLow float64 = 1\n\tvar topHam []xword\n\tvar topSpam []xword\n\n\t// Find words that should be in the database.\n\tlookupWords := []string{}\n\texpect := map[string]struct{}{}\n\tunknowns := map[string]struct{}{}\n\ttotalUnknown := 0\n\tfor w := range words {\n\t\tif f.bloom != nil && !f.bloom.Has(w) {\n\t\t\ttotalUnknown++\n\t\t\tif len(unknowns) < 50 {\n\t\t\t\tunknowns[w] = struct{}{}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; ok {\n\t\t\tcontinue\n\t\t}\n\t\tlookupWords = append(lookupWords, w)\n\t\texpect[w] = struct{}{}\n\t}\n\tif len(unknowns) > 0 {\n\t\tf.log.Debug(\"unknown words in bloom filter, showing max 50\",\n\t\t\tslog.Any(\"words\", unknowns),\n\t\t\tslog.Any(\"totalunknown\", totalUnknown),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\t// Fetch words from database.\n\tfetched := map[string]word{}\n\tif len(lookupWords) > 0 {\n\t\tif err := loadWords(ctx, f.db, lookupWords, fetched); err != nil {\n\t\t\treturn 0, 0, 0, err\n\t\t}\n\t\tfor w, c := range fetched {\n\t\t\tdelete(expect, w)\n\t\t\tf.cache[w] = c\n\t\t}\n\t\tf.log.Debug(\"unknown words in db\",\n\t\t\tslog.Any(\"words\", expect),\n\t\t\tslog.Any(\"totalunknown\", len(expect)),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tvar wS, wH float64\n\t\tif f.spams > 0 {\n\t\t\twS = float64(c.Spam) / float64(f.spams)\n\t\t}\n\t\tif f.hams > 0 {\n\t\t\twH = float64(c.Ham) / float64(f.hams)\n\t\t}\n\t\tr := wS / (wS + wH)\n\n\t\tif r < f.MaxPower {\n\t\t\tr = f.MaxPower\n\t\t} else if r >= 1-f.MaxPower {\n\t\t\tr = 1 - f.MaxPower\n\t\t}\n\n\t\tif c.Ham+c.Spam <= uint32(f.RareWords) {\n\t\t\t// Reduce the power of rare words.\n\t\t\tr += float64(1+uint32(f.RareWords)-(c.Ham+c.Spam)) * (0.5 - r) / 10\n\t\t}\n\t\tif math.Abs(0.5-r) < f.IgnoreWords {\n\t\t\tcontinue\n\t\t}\n\t\tif r < 0.5 {\n\t\t\tif len(topHam) >= f.TopWords && r > hamHigh {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopHam = append(topHam, xword{w, r})\n\t\t\tif r > hamHigh {\n\t\t\t\thamHigh = r\n\t\t\t}\n\t\t} else if r > 0.5 {\n\t\t\tif len(topSpam) >= f.TopWords && r < spamLow {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopSpam = append(topSpam, xword{w, r})\n\t\t\tif r < spamLow {\n\t\t\t\tspamLow = r\n\t\t\t}\n\t\t}\n\t}\n\n\tsort.Slice(topHam, func(i, j int) bool {\n\t\ta, b := topHam[i], topHam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R < b.R\n\t})\n\tsort.Slice(topSpam, func(i, j int) bool {\n\t\ta, b := topSpam[i], topSpam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R > b.R\n\t})\n\n\tnham = f.TopWords\n\tif nham > len(topHam) {\n\t\tnham = len(topHam)\n\t}\n\tnspam = f.TopWords\n\tif nspam > len(topSpam) {\n\t\tnspam = len(topSpam)\n\t}\n\ttopHam = topHam[:nham]\n\ttopSpam = topSpam[:nspam]\n\n\tvar eta float64\n\tfor _, x := range topHam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\tfor _, x := range topSpam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\n\tf.log.Debug(\"top words\", slog.Any(\"hams\", topHam), slog.Any(\"spams\", topSpam))\n\n\tprob := 1 / (1 + math.Pow(math.E, eta))\n\treturn prob, len(topHam), len(topSpam), nil\n}\n\n// ClassifyMessagePath is a convenience wrapper for calling ClassifyMessage on a file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) ClassifyMessageReader(ctx context.Context, mf io.ReaderAt, size int64) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tm, err := message.EnsurePart(f.log.Logger, false, mf, size)\n\tif err != nil && errors.Is(err, message.ErrBadContentType) {\n\t\t// Invalid content-type header is a sure sign of spam.\n\t\t//f.log.Infox(\"parsing content\", err)\n\t\treturn 1, nil, 0, 0, nil\n\t}\n\treturn f.ClassifyMessage(ctx, m)\n}\n\n// ClassifyMessage parses the mail message in r and returns the spam probability\n// (between 0 and 1), along with the tokenized words found in the message, and the\n// number of recognized ham and spam words.\nfunc (f *Filter) ClassifyMessage(ctx context.Context, m message.Part) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tvar err error\n\twords, err = f.ParseMessage(m)\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, err\n\t}\n\n\tprobability, nham, nspam, err = f.ClassifyWords(ctx, words)\n\treturn probability, words, nham, nspam, err\n}\n\n// Train adds the words of a single message to the filter.\nfunc (f *Filter) Train(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\tvar lwords []string\n\n\tfor w := range words {\n\t\tif !f.bloom.Has(w) {\n\t\t\tf.bloom.Add(w)\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\tf.modified = true\n\tif ham {\n\t\tf.hams++\n\t} else {\n\t\tf.spams++\n\t}\n\n\tfor w := range words {\n\t\tc := f.cache[w]\n\t\tif ham {\n\t\t\tc.Ham++\n\t\t} else {\n\t\t\tc.Spam++\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\nfunc (f *Filter) TrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Train(ctx, ham, words)\n}\n\nfunc (f *Filter) UntrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Untrain(ctx, ham, words)\n}\n\nfunc (f *Filter) loadCache(ctx context.Context, lwords []string) error {\n\tif len(lwords) == 0 {\n\t\treturn nil\n\t}\n\n\treturn loadWords(ctx, f.db, lwords, f.cache)\n}\n\n// Untrain adjusts the filter to undo a previous training of the words.\nfunc (f *Filter) Untrain(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\t// Lookup any words from the db that aren't in the cache and put them in the cache for modification.\n\tvar lwords []string\n\tfor w := range words {\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\t// Modify the message count.\n\tf.modified = true\n\tif ham {\n\t\tf.hams--\n\t} else {\n\t\tf.spams--\n\t}\n\n\t// Decrease the word counts.\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif ham {\n\t\t\tc.Ham--\n\t\t} else {\n\t\t\tc.Spam--\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\n// TrainDir parses mail messages from files and trains the filter.\nfunc (f *Filter) TrainDir(dir string, files []string, ham bool) (n, malformed uint32, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, errClosed\n\t}\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\tfor _, name := range files {\n\t\tp := filepath.Join(dir, name)\n\t\tvalid, words, err := f.tokenizeMail(p)\n\t\tif err != nil {\n\t\t\t// f.log.Infox(\"tokenizing mail\", err, slog.Any(\"path\", p))\n\t\t\tmalformed++\n\t\t\tcontinue\n\t\t}\n\t\tif !valid {\n\t\t\tcontinue\n\t\t}\n\t\tn++\n\t\tfor w := range words {\n\t\t\tif !f.bloom.Has(w) {\n\t\t\t\tf.bloom.Add(w)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc := f.cache[w]\n\t\t\tf.modified = true\n\t\t\tif ham {\n\t\t\t\tc.Ham++\n\t\t\t} else {\n\t\t\t\tc.Spam++\n\t\t\t}\n\t\t\tf.cache[w] = c\n\t\t\tf.changed[w] = c\n\t\t}\n\t}\n\treturn\n}\n\n// TrainDirs trains and saves a filter with mail messages from different types\n// of directories.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) fileSize(p string) int {\n\tfi, err := os.Stat(p)\n\tif err != nil {\n\t\tf.log.Infox(\"stat\", err, slog.Any(\"path\", p))\n\t\treturn 0\n\t}\n\treturn int(fi.Size())\n}\n\n// DB returns the database, for backups.\nfunc (f *Filter) DB() *bstore.DB {\n\treturn f.db\n}\n\n```\n# Function Signature: func (f *Filter) ClassifyMessagePath(ctx context.Context, path string) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n# Natural Language Annotations: // ClassifyMessagePath is a convenience wrapper for calling ClassifyMessage on a file.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (f *Filter) ClassifyMessagePath(ctx context.Context, path string) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tfile, err := os.Open(path)\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, err\n\t}\n\tdefer file.Close()\n\n\tfi, err := file.Stat()\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, err\n\t}\n\n\treturn f.ClassifyMessageReader(ctx, file, fi.Size())\n}\n```", "```go\nfunc (f *Filter) ClassifyMessagePath(ctx context.Context, path string) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tmf, err := os.Open(path)\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, err\n\t}\n\tdefer mf.Close()\n\n\tfi, err := mf.Stat()\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, err\n\t}\n\n\treturn f.ClassifyMessageReader(ctx, mf, fi.Size())\n}\n```", "```go\nfunc (f *Filter) ClassifyMessagePath(ctx context.Context, path string) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tfh, err := os.Open(path)\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, fmt.Errorf(\"opening file: %v\", err)\n\t}\n\tdefer func() {\n\t\tif err := fh.Close(); err != nil {\n\t\t\trerr = fmt.Errorf(\"closing file: %v\", err)\n\t\t}\n\t}()\n\n\tfi, err := fh.Stat()\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, fmt.Errorf(\"getting file info: %v\", err)\n\t}\n\n\treturn f.ClassifyMessageReader(ctx, fh, fi.Size())\n}\n```"]}, "mox-junk/filter.go-TrainDirs": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/junk/filter.go:\n```\n// Package junk implements a bayesian spam filter.\n//\n// A message can be parsed into words. Words (or pairs or triplets) can be used\n// to train the filter or to classify the message as ham or spam.  Training\n// records the words in the database as ham/spam. Classifying consists of\n// calculating the ham/spam probability by combining the words in the message\n// with their ham/spam status.\npackage junk\n\n// todo: look at inverse chi-square function? see https://www.linuxjournal.com/article/6467\n// todo: perhaps: whether anchor text in links in html are different from the url\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"math\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"time\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n)\n\nvar (\n\t// errBadContentType = errors.New(\"bad content-type\") // sure sign of spam, todo: use this error\n\terrClosed = errors.New(\"filter is closed\")\n)\n\ntype word struct {\n\tHam  uint32\n\tSpam uint32\n}\n\ntype wordscore struct {\n\tWord string\n\tHam  uint32\n\tSpam uint32\n}\n\n// Params holds parameters for the filter. Most are at test-time. The first are\n// used during parsing and training.\ntype Params struct {\n\tOnegrams    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for single words.\"`\n\tTwograms    bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each two consecutive words.\"`\n\tThreegrams  bool    `sconf:\"optional\" sconf-doc:\"Track ham/spam ranking for each three consecutive words.\"`\n\tMaxPower    float64 `sconf-doc:\"Maximum power a word (combination) can have. If spaminess is 0.99, and max power is 0.1, spaminess of the word will be set to 0.9. Similar for ham words.\"`\n\tTopWords    int     `sconf-doc:\"Number of most spammy/hammy words to use for calculating probability. E.g. 10.\"`\n\tIgnoreWords float64 `sconf:\"optional\" sconf-doc:\"Ignore words that are this much away from 0.5 haminess/spaminess. E.g. 0.1, causing word (combinations) of 0.4 to 0.6 to be ignored.\"`\n\tRareWords   int     `sconf:\"optional\" sconf-doc:\"Occurrences in word database until a word is considered rare and its influence in calculating probability reduced. E.g. 1 or 2.\"`\n}\n\nvar DBTypes = []any{wordscore{}} // Stored in DB.\n\ntype Filter struct {\n\tParams\n\n\tlog               mlog.Log // For logging cid.\n\tclosed            bool\n\tmodified          bool            // Whether any modifications are pending. Cleared by Save.\n\thams, spams       uint32          // Message count, stored in db under word \"-\".\n\tcache             map[string]word // Words read from database or during training.\n\tchanged           map[string]word // Words modified during training.\n\tdbPath, bloomPath string\n\tdb                *bstore.DB // Always open on a filter.\n\tbloom             *Bloom     // Only opened when writing.\n\tisNew             bool       // Set for new filters until their first sync to disk. For faster writing.\n}\n\nfunc (f *Filter) ensureBloom() error {\n\tif f.bloom != nil {\n\t\treturn nil\n\t}\n\tvar err error\n\tf.bloom, err = openBloom(f.bloomPath)\n\treturn err\n}\n\n// CloseDiscard closes the filter, discarding any changes.\nfunc (f *Filter) CloseDiscard() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\terr := f.db.Close()\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\n// Close first saves the filter if it has modifications, then closes the database\n// connection and releases the bloom filter.\nfunc (f *Filter) Close() error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\tvar err error\n\tif f.modified {\n\t\terr = f.Save()\n\t}\n\tif err != nil {\n\t\tf.db.Close()\n\t} else {\n\t\terr = f.db.Close()\n\t}\n\t*f = Filter{log: f.log, closed: true}\n\treturn err\n}\n\nfunc OpenFilter(ctx context.Context, log mlog.Log, params Params, dbPath, bloomPath string, loadBloom bool) (*Filter, error) {\n\tvar bloom *Bloom\n\tif loadBloom {\n\t\tvar err error\n\t\tbloom, err = openBloom(bloomPath)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else if fi, err := os.Stat(bloomPath); err == nil {\n\t\tif err := BloomValid(int(fi.Size()), bloomK); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"bloom: %s\", err)\n\t\t}\n\t}\n\n\tdb, err := openDB(ctx, log, dbPath)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open database: %s\", err)\n\t}\n\n\tf := &Filter{\n\t\tParams:    params,\n\t\tlog:       log,\n\t\tcache:     map[string]word{},\n\t\tchanged:   map[string]word{},\n\t\tdbPath:    dbPath,\n\t\tbloomPath: bloomPath,\n\t\tdb:        db,\n\t\tbloom:     bloom,\n\t}\n\terr = f.db.Read(ctx, func(tx *bstore.Tx) error {\n\t\twc := wordscore{Word: \"-\"}\n\t\terr := tx.Get(&wc)\n\t\tf.hams = wc.Ham\n\t\tf.spams = wc.Spam\n\t\treturn err\n\t})\n\tif err != nil {\n\t\tcerr := f.Close()\n\t\tlog.Check(cerr, \"closing filter after error\")\n\t\treturn nil, fmt.Errorf(\"looking up ham/spam message count: %s\", err)\n\t}\n\treturn f, nil\n}\n\n// NewFilter creates a new filter with empty bloom filter and database files. The\n// filter is marked as new until the first save, will be done automatically if\n// TrainDirs is called. If the bloom and/or database files exist, an error is\n// returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nconst bloomK = 10\n\nfunc openBloom(path string) (*Bloom, error) {\n\tbuf, err := os.ReadFile(path)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading bloom file: %w\", err)\n\t}\n\treturn NewBloom(buf, bloomK)\n}\n\nfunc newDB(ctx context.Context, log mlog.Log, path string) (db *bstore.DB, rerr error) {\n\t// Remove any existing files.\n\tos.Remove(path)\n\n\tdefer func() {\n\t\tif rerr != nil {\n\t\t\terr := os.Remove(path)\n\t\t\tlog.Check(err, \"removing db file after init error\")\n\t\t}\n\t}()\n\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\tdb, err := bstore.Open(ctx, path, &opts, DBTypes...)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"open new database: %w\", err)\n\t}\n\treturn db, nil\n}\n\nfunc openDB(ctx context.Context, log mlog.Log, path string) (*bstore.DB, error) {\n\tif _, err := os.Stat(path); err != nil {\n\t\treturn nil, fmt.Errorf(\"stat db file: %w\", err)\n\t}\n\topts := bstore.Options{Timeout: 5 * time.Second, Perm: 0660, RegisterLogger: log.Logger}\n\treturn bstore.Open(ctx, path, &opts, DBTypes...)\n}\n\n// Save stores modifications, e.g. from training, to the database and bloom\n// filter files.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc loadWords(ctx context.Context, db *bstore.DB, l []string, dst map[string]word) error {\n\tsort.Slice(l, func(i, j int) bool {\n\t\treturn l[i] < l[j]\n\t})\n\n\terr := db.Read(ctx, func(tx *bstore.Tx) error {\n\t\tfor _, w := range l {\n\t\t\twc := wordscore{Word: w}\n\t\t\tif err := tx.Get(&wc); err == nil {\n\t\t\t\tdst[w] = word{wc.Ham, wc.Spam}\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"fetching words: %s\", err)\n\t}\n\treturn nil\n}\n\n// ClassifyWords returns the spam probability for the given words, and number of recognized ham and spam words.\nfunc (f *Filter) ClassifyWords(ctx context.Context, words map[string]struct{}) (probability float64, nham, nspam int, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, 0, errClosed\n\t}\n\n\ttype xword struct {\n\t\tWord string\n\t\tR    float64\n\t}\n\n\tvar hamHigh float64 = 0\n\tvar spamLow float64 = 1\n\tvar topHam []xword\n\tvar topSpam []xword\n\n\t// Find words that should be in the database.\n\tlookupWords := []string{}\n\texpect := map[string]struct{}{}\n\tunknowns := map[string]struct{}{}\n\ttotalUnknown := 0\n\tfor w := range words {\n\t\tif f.bloom != nil && !f.bloom.Has(w) {\n\t\t\ttotalUnknown++\n\t\t\tif len(unknowns) < 50 {\n\t\t\t\tunknowns[w] = struct{}{}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; ok {\n\t\t\tcontinue\n\t\t}\n\t\tlookupWords = append(lookupWords, w)\n\t\texpect[w] = struct{}{}\n\t}\n\tif len(unknowns) > 0 {\n\t\tf.log.Debug(\"unknown words in bloom filter, showing max 50\",\n\t\t\tslog.Any(\"words\", unknowns),\n\t\t\tslog.Any(\"totalunknown\", totalUnknown),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\t// Fetch words from database.\n\tfetched := map[string]word{}\n\tif len(lookupWords) > 0 {\n\t\tif err := loadWords(ctx, f.db, lookupWords, fetched); err != nil {\n\t\t\treturn 0, 0, 0, err\n\t\t}\n\t\tfor w, c := range fetched {\n\t\t\tdelete(expect, w)\n\t\t\tf.cache[w] = c\n\t\t}\n\t\tf.log.Debug(\"unknown words in db\",\n\t\t\tslog.Any(\"words\", expect),\n\t\t\tslog.Any(\"totalunknown\", len(expect)),\n\t\t\tslog.Any(\"totalwords\", len(words)))\n\t}\n\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tvar wS, wH float64\n\t\tif f.spams > 0 {\n\t\t\twS = float64(c.Spam) / float64(f.spams)\n\t\t}\n\t\tif f.hams > 0 {\n\t\t\twH = float64(c.Ham) / float64(f.hams)\n\t\t}\n\t\tr := wS / (wS + wH)\n\n\t\tif r < f.MaxPower {\n\t\t\tr = f.MaxPower\n\t\t} else if r >= 1-f.MaxPower {\n\t\t\tr = 1 - f.MaxPower\n\t\t}\n\n\t\tif c.Ham+c.Spam <= uint32(f.RareWords) {\n\t\t\t// Reduce the power of rare words.\n\t\t\tr += float64(1+uint32(f.RareWords)-(c.Ham+c.Spam)) * (0.5 - r) / 10\n\t\t}\n\t\tif math.Abs(0.5-r) < f.IgnoreWords {\n\t\t\tcontinue\n\t\t}\n\t\tif r < 0.5 {\n\t\t\tif len(topHam) >= f.TopWords && r > hamHigh {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopHam = append(topHam, xword{w, r})\n\t\t\tif r > hamHigh {\n\t\t\t\thamHigh = r\n\t\t\t}\n\t\t} else if r > 0.5 {\n\t\t\tif len(topSpam) >= f.TopWords && r < spamLow {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttopSpam = append(topSpam, xword{w, r})\n\t\t\tif r < spamLow {\n\t\t\t\tspamLow = r\n\t\t\t}\n\t\t}\n\t}\n\n\tsort.Slice(topHam, func(i, j int) bool {\n\t\ta, b := topHam[i], topHam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R < b.R\n\t})\n\tsort.Slice(topSpam, func(i, j int) bool {\n\t\ta, b := topSpam[i], topSpam[j]\n\t\tif a.R == b.R {\n\t\t\treturn len(a.Word) > len(b.Word)\n\t\t}\n\t\treturn a.R > b.R\n\t})\n\n\tnham = f.TopWords\n\tif nham > len(topHam) {\n\t\tnham = len(topHam)\n\t}\n\tnspam = f.TopWords\n\tif nspam > len(topSpam) {\n\t\tnspam = len(topSpam)\n\t}\n\ttopHam = topHam[:nham]\n\ttopSpam = topSpam[:nspam]\n\n\tvar eta float64\n\tfor _, x := range topHam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\tfor _, x := range topSpam {\n\t\teta += math.Log(1-x.R) - math.Log(x.R)\n\t}\n\n\tf.log.Debug(\"top words\", slog.Any(\"hams\", topHam), slog.Any(\"spams\", topSpam))\n\n\tprob := 1 / (1 + math.Pow(math.E, eta))\n\treturn prob, len(topHam), len(topSpam), nil\n}\n\n// ClassifyMessagePath is a convenience wrapper for calling ClassifyMessage on a file.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) ClassifyMessageReader(ctx context.Context, mf io.ReaderAt, size int64) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tm, err := message.EnsurePart(f.log.Logger, false, mf, size)\n\tif err != nil && errors.Is(err, message.ErrBadContentType) {\n\t\t// Invalid content-type header is a sure sign of spam.\n\t\t//f.log.Infox(\"parsing content\", err)\n\t\treturn 1, nil, 0, 0, nil\n\t}\n\treturn f.ClassifyMessage(ctx, m)\n}\n\n// ClassifyMessage parses the mail message in r and returns the spam probability\n// (between 0 and 1), along with the tokenized words found in the message, and the\n// number of recognized ham and spam words.\nfunc (f *Filter) ClassifyMessage(ctx context.Context, m message.Part) (probability float64, words map[string]struct{}, nham, nspam int, rerr error) {\n\tvar err error\n\twords, err = f.ParseMessage(m)\n\tif err != nil {\n\t\treturn 0, nil, 0, 0, err\n\t}\n\n\tprobability, nham, nspam, err = f.ClassifyWords(ctx, words)\n\treturn probability, words, nham, nspam, err\n}\n\n// Train adds the words of a single message to the filter.\nfunc (f *Filter) Train(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\tvar lwords []string\n\n\tfor w := range words {\n\t\tif !f.bloom.Has(w) {\n\t\t\tf.bloom.Add(w)\n\t\t\tcontinue\n\t\t}\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\tf.modified = true\n\tif ham {\n\t\tf.hams++\n\t} else {\n\t\tf.spams++\n\t}\n\n\tfor w := range words {\n\t\tc := f.cache[w]\n\t\tif ham {\n\t\t\tc.Ham++\n\t\t} else {\n\t\t\tc.Spam++\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\nfunc (f *Filter) TrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Train(ctx, ham, words)\n}\n\nfunc (f *Filter) UntrainMessage(ctx context.Context, r io.ReaderAt, size int64, ham bool) error {\n\tp, _ := message.EnsurePart(f.log.Logger, false, r, size)\n\twords, err := f.ParseMessage(p)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"parsing mail contents: %v\", err)\n\t}\n\treturn f.Untrain(ctx, ham, words)\n}\n\nfunc (f *Filter) loadCache(ctx context.Context, lwords []string) error {\n\tif len(lwords) == 0 {\n\t\treturn nil\n\t}\n\n\treturn loadWords(ctx, f.db, lwords, f.cache)\n}\n\n// Untrain adjusts the filter to undo a previous training of the words.\nfunc (f *Filter) Untrain(ctx context.Context, ham bool, words map[string]struct{}) error {\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\t// Lookup any words from the db that aren't in the cache and put them in the cache for modification.\n\tvar lwords []string\n\tfor w := range words {\n\t\tif _, ok := f.cache[w]; !ok {\n\t\t\tlwords = append(lwords, w)\n\t\t}\n\t}\n\tif err := f.loadCache(ctx, lwords); err != nil {\n\t\treturn err\n\t}\n\n\t// Modify the message count.\n\tf.modified = true\n\tif ham {\n\t\tf.hams--\n\t} else {\n\t\tf.spams--\n\t}\n\n\t// Decrease the word counts.\n\tfor w := range words {\n\t\tc, ok := f.cache[w]\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tif ham {\n\t\t\tc.Ham--\n\t\t} else {\n\t\t\tc.Spam--\n\t\t}\n\t\tf.cache[w] = c\n\t\tf.changed[w] = c\n\t}\n\treturn nil\n}\n\n// TrainDir parses mail messages from files and trains the filter.\nfunc (f *Filter) TrainDir(dir string, files []string, ham bool) (n, malformed uint32, rerr error) {\n\tif f.closed {\n\t\treturn 0, 0, errClosed\n\t}\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\tfor _, name := range files {\n\t\tp := filepath.Join(dir, name)\n\t\tvalid, words, err := f.tokenizeMail(p)\n\t\tif err != nil {\n\t\t\t// f.log.Infox(\"tokenizing mail\", err, slog.Any(\"path\", p))\n\t\t\tmalformed++\n\t\t\tcontinue\n\t\t}\n\t\tif !valid {\n\t\t\tcontinue\n\t\t}\n\t\tn++\n\t\tfor w := range words {\n\t\t\tif !f.bloom.Has(w) {\n\t\t\t\tf.bloom.Add(w)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc := f.cache[w]\n\t\t\tf.modified = true\n\t\t\tif ham {\n\t\t\t\tc.Ham++\n\t\t\t} else {\n\t\t\t\tc.Spam++\n\t\t\t}\n\t\t\tf.cache[w] = c\n\t\t\tf.changed[w] = c\n\t\t}\n\t}\n\treturn\n}\n\n// TrainDirs trains and saves a filter with mail messages from different types\n// of directories.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (f *Filter) fileSize(p string) int {\n\tfi, err := os.Stat(p)\n\tif err != nil {\n\t\tf.log.Infox(\"stat\", err, slog.Any(\"path\", p))\n\t\treturn 0\n\t}\n\treturn int(fi.Size())\n}\n\n// DB returns the database, for backups.\nfunc (f *Filter) DB() *bstore.DB {\n\treturn f.db\n}\n\n```\n# Function Signature: func (f *Filter) TrainDirs(hamDir, sentDir, spamDir string, hamFiles, sentFiles, spamFiles []string) error {\n# Natural Language Annotations: // TrainDirs trains and saves a filter with mail messages from different types\n// of directories.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (f *Filter) TrainDirs(hamDir, sentDir, spamDir string, hamFiles, sentFiles, spamFiles []string) error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\t// Train ham messages.\n\tnham := uint32(0)\n\tmalformedham := uint32(0)\n\tfor _, name := range hamFiles {\n\t\tp := filepath.Join(hamDir, name)\n\t\tvalid, words, err := f.tokenizeMail(p)\n\t\tif err != nil {\n\t\t\t// f.log.Infox(\"tokenizing ham mail\", err, slog.Any(\"path\", p))\n\t\t\tmalformedham++\n\t\t\tcontinue\n\t\t}\n\t\tif !valid {\n\t\t\tcontinue\n\t\t}\n\t\tnham++\n\t\tif err := f.Train(f.log.Context(), true, words); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Train sent messages.\n\tnsent := uint32(0)\n\tmalformedsent := uint32(0)\n\tfor _, name := range sentFiles {\n\t\tp := filepath.Join(sentDir, name)\n\t\tvalid, words, err := f.tokenizeMail(p)\n\t\tif err != nil {\n\t\t\t// f.log.Infox(\"tokenizing sent mail\", err, slog.Any(\"path\", p))\n\t\t\tmalformedsent++\n\t\t\tcontinue\n\t\t}\n\t\tif !valid {\n\t\t\tcontinue\n\t\t}\n\t\tnsent++\n\t\tif err := f.Train(f.log.Context(), true, words); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Train spam messages.\n\tnspam := uint32(0)\n\tmalformedspam := uint32(0)\n\tfor _, name := range spamFiles {\n\t\tp := filepath.Join(spamDir, name)\n\t\tvalid, words, err := f.tokenizeMail(p)\n\t\tif err != nil {\n\t\t\t// f.log.Infox(\"tokenizing spam mail\", err, slog.Any(\"path\", p))\n\t\t\tmalformedspam++\n\t\t\tcontinue\n\t\t}\n\t\tif !valid {\n\t\t\tcontinue\n\t\t}\n\t\tnspam++\n\t\tif err := f.Train(f.log.Context(), false, words); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tf.log.Debugx(\"trained mail\",\n\t\tslog.Any(\"nham\", nham),\n\t\tslog.Any(\"malformedham\", malformedham),\n\t\tslog.Any(\"nsent\", nsent),\n\t\tslog.Any(\"malformedsent\", malformedsent),\n\t\tslog.Any(\"nspam\", nspam),\n\t\tslog.Any(\"malformedspam\", malformedspam),\n\t)\n\n\treturn nil\n}\n```", "```go\nfunc (f *Filter) TrainDirs(hamDir, sentDir, spamDir string, hamFiles, sentFiles, spamFiles []string) error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\t// Train ham messages\n\tif len(hamFiles) > 0 {\n\t\tn, malformed, err := f.TrainDir(hamDir, hamFiles, true)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tf.log.Info(\"trained ham messages\", slog.Int(\"total\", len(hamFiles)), slog.Int(\"trained\", n), slog.Int(\"malformed\", malformed))\n\t}\n\n\t// Train sent messages\n\tif len(sentFiles) > 0 {\n\t\tn, malformed, err := f.TrainDir(sentDir, sentFiles, true)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tf.log.Info(\"trained sent messages\", slog.Int(\"total\", len(sentFiles)), slog.Int(\"trained\", n), slog.Int(\"malformed\", malformed))\n\t}\n\n\t// Train spam messages\n\tif len(spamFiles) > 0 {\n\t\tn, malformed, err := f.TrainDir(spamDir, spamFiles, false)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tf.log.Info(\"trained spam messages\", slog.Int(\"total\", len(spamFiles)), slog.Int(\"trained\", n), slog.Int(\"malformed\", malformed))\n\t}\n\n\treturn f.Save()\n}\n```", "```go\nfunc (f *Filter) TrainDirs(hamDir, sentDir, spamDir string, hamFiles, sentFiles, spamFiles []string) error {\n\tif f.closed {\n\t\treturn errClosed\n\t}\n\tif err := f.ensureBloom(); err != nil {\n\t\treturn err\n\t}\n\n\t// Train ham messages.\n\tfor _, name := range hamFiles {\n\t\tp := filepath.Join(hamDir, name)\n\t\tvalid, words, err := f.tokenizeMail(p)\n\t\tif err != nil {\n\t\t\t// f.log.Infox(\"tokenizing ham mail\", err, slog.Any(\"path\", p))\n\t\t\tcontinue\n\t\t}\n\t\tif !valid {\n\t\t\tcontinue\n\t\t}\n\t\tfor w := range words {\n\t\t\tif !f.bloom.Has(w) {\n\t\t\t\tf.bloom.Add(w)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc := f.cache[w]\n\t\t\tf.modified = true\n\t\t\tc.Ham++\n\t\t\tf.cache[w] = c\n\t\t\tf.changed[w] = c\n\t\t}\n\t}\n\n\t// Train spam messages.\n\tfor _, name := range spamFiles {\n\t\tp := filepath.Join(spamDir, name)\n\t\tvalid, words, err := f.tokenizeMail(p)\n\t\tif err != nil {\n\t\t\t// f.log.Infox(\"tokenizing spam mail\", err, slog.Any(\"path\", p))\n\t\t\tcontinue\n\t\t}\n\t\tif !valid {\n\t\t\tcontinue\n\t\t}\n\t\tfor w := range words {\n\t\t\tif !f.bloom.Has(w) {\n\t\t\t\tf.bloom.Add(w)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc := f.cache[w]\n\t\t\tf.modified = true\n\t\t\tc.Spam++\n\t\t\tf.cache[w] = c\n\t\t\tf.changed[w] = c\n\t\t}\n\t}\n\n\t// Train sent messages.\n\tfor _, name := range sentFiles {\n\t\tp := filepath.Join(sentDir, name)\n\t\tvalid, words, err := f.tokenizeMail(p)\n\t\tif err != nil {\n\t\t\t// f.log.Infox(\"tokenizing sent mail\", err, slog.Any(\"path\", p))\n\t\t\tcontinue\n\t\t}\n\t\tif !valid {\n\t\t\tcontinue\n\t\t}\n\t\tfor w := range words {\n\t\t\tif !f.bloom.Has(w) {\n\t\t\t\tf.bloom.Add(w)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tc := f.cache[w]\n\t\t\tf.modified = true\n\t\t\tc.Spam = c.Spam / 2\n\t\t\tc.Ham = c.Ham / 2\n\t\t\tf.cache[w] = c\n\t\t\tf.changed[w] = c\n\t\t}\n\t}\n\n\t// Save the changes to the database.\n\tif f.modified {\n\t\tf.hams /= 2\n\t\tf.spams /= 2\n\t\tf.modified = false\n\t\terr := f.db.Write(func(tx *bstore.Tx) error {\n\t\t\tfor w, c := range f.changed {\n\t\t\t\terr := tx.Put(wordscore{w, c.Ham, c.Spam})\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t}\n\t\t\twc := wordscore{Word: \"-\", Ham: f.hams, Spam: f.spams}\n\t\t\treturn tx.Put(wc)\n\t\t})\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"saving changes to database: %s\", err)\n\t\t}\n\t}\n\treturn nil\n}\n```"]}, "mox-webmail/message.go-parseListPostAddress": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/webmail/message.go:\n```\npackage webmail\n\nimport (\n\t\"bufio\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"net/url\"\n\t\"strings\"\n\n\t\"golang.org/x/text/encoding/ianaindex\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/smtp\"\n\t\"github.com/mjl-/mox/store\"\n)\n\n// todo: we should have all needed information for messageItem in store.Message (perhaps some data in message.Part) for fast access, not having to parse the on-disk message file.\n\nvar wordDecoder = mime.WordDecoder{\n\tCharsetReader: func(charset string, r io.Reader) (io.Reader, error) {\n\t\tswitch strings.ToLower(charset) {\n\t\tcase \"\", \"us-ascii\", \"utf-8\":\n\t\t\treturn r, nil\n\t\t}\n\t\tenc, _ := ianaindex.MIME.Encoding(charset)\n\t\tif enc == nil {\n\t\t\tenc, _ = ianaindex.IANA.Encoding(charset)\n\t\t}\n\t\tif enc == nil {\n\t\t\treturn r, fmt.Errorf(\"unknown charset %q\", charset)\n\t\t}\n\t\treturn enc.NewDecoder().Reader(r), nil\n\t},\n}\n\n// Attempt q/b-word-decode name, coming from Content-Type \"name\" field or\n// Content-Disposition \"filename\" field.\n//\n// RFC 2231 specify an encoding for non-ascii values in mime header parameters. But\n// it appears common practice to instead just q/b-word encode the values.\n// Thunderbird and gmail.com do this for the Content-Type \"name\" parameter.\n// gmail.com also does that for the Content-Disposition \"filename\" parameter, where\n// Thunderbird uses the RFC 2231-defined encoding. Go's mime.ParseMediaType parses\n// the mechanism specified in RFC 2231 only. The value for \"name\" we get here would\n// already be decoded properly for standards-compliant headers, like\n// \"filename*0*=UTF-8\u201d%...; filename*1*=%.... We'll look for Q/B-word encoding\n// markers (\"=?\"-prefix or \"?=\"-suffix) and try to decode if present. This would\n// only cause trouble for filenames having this prefix/suffix.\nfunc tryDecodeParam(log mlog.Log, name string) string {\n\tif name == \"\" || !strings.HasPrefix(name, \"=?\") && !strings.HasSuffix(name, \"?=\") {\n\t\treturn name\n\t}\n\t// todo: find where this is allowed. it seems quite common. perhaps we should remove the pedantic check?\n\tif mox.Pedantic {\n\t\tlog.Debug(\"attachment contains rfc2047 q/b-word-encoded mime parameter instead of rfc2231-encoded\", slog.String(\"name\", name))\n\t\treturn name\n\t}\n\ts, err := wordDecoder.DecodeHeader(name)\n\tif err != nil {\n\t\tlog.Debugx(\"q/b-word decoding mime parameter\", err, slog.String(\"name\", name))\n\t\treturn name\n\t}\n\treturn s\n}\n\n// todo: mime.FormatMediaType does not wrap long lines. should do it ourselves, and split header into several parts (if commonly supported).\n\nfunc messageItem(log mlog.Log, m store.Message, state *msgState) (MessageItem, error) {\n\tpm, err := parsedMessage(log, m, state, false, true)\n\tif err != nil {\n\t\treturn MessageItem{}, fmt.Errorf(\"parsing message %d for item: %v\", m.ID, err)\n\t}\n\t// Clear largish unused data.\n\tm.MsgPrefix = nil\n\tm.ParsedBuf = nil\n\treturn MessageItem{m, pm.envelope, pm.attachments, pm.isSigned, pm.isEncrypted, pm.firstLine, true}, nil\n}\n\n// formatFirstLine returns a line the client can display next to the subject line\n// in a mailbox. It will replace quoted text, and any prefixing \"On ... write:\"\n// line with \"[...]\" so only new and useful information will be displayed.\n// Trailing signatures are not included.\nfunc formatFirstLine(r io.Reader) (string, error) {\n\t// We look quite a bit of lines ahead for trailing signatures with trailing empty lines.\n\tvar lines []string\n\tscanner := bufio.NewScanner(r)\n\tensureLines := func() {\n\t\tfor len(lines) < 10 && scanner.Scan() {\n\t\t\tlines = append(lines, strings.TrimSpace(scanner.Text()))\n\t\t}\n\t}\n\tensureLines()\n\n\tisSnipped := func(s string) bool {\n\t\treturn s == \"[...]\" || s == \"[\u2026]\" || s == \"...\"\n\t}\n\n\tnextLineQuoted := func(i int) bool {\n\t\tif i+1 < len(lines) && lines[i+1] == \"\" {\n\t\t\ti++\n\t\t}\n\t\treturn i+1 < len(lines) && (strings.HasPrefix(lines[i+1], \">\") || isSnipped(lines[i+1]))\n\t}\n\n\t// Remainder is signature if we see a line with only and minimum 2 dashes, and\n\t// there are no more empty lines, and there aren't more than 5 lines left.\n\tisSignature := func() bool {\n\t\tif len(lines) == 0 || !strings.HasPrefix(lines[0], \"--\") || strings.Trim(strings.TrimSpace(lines[0]), \"-\") != \"\" {\n\t\t\treturn false\n\t\t}\n\t\tl := lines[1:]\n\t\tfor len(l) > 0 && l[len(l)-1] == \"\" {\n\t\t\tl = l[:len(l)-1]\n\t\t}\n\t\tif len(l) >= 5 {\n\t\t\treturn false\n\t\t}\n\t\tfor _, line := range l {\n\t\t\tif line == \"\" {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\treturn true\n\t}\n\n\tresult := \"\"\n\n\tresultSnipped := func() bool {\n\t\treturn strings.HasSuffix(result, \"[...]\\n\") || strings.HasSuffix(result, \"[\u2026]\")\n\t}\n\n\t// Quick check for initial wrapped \"On ... wrote:\" line.\n\tif len(lines) > 3 && strings.HasPrefix(lines[0], \"On \") && !strings.HasSuffix(lines[0], \"wrote:\") && strings.HasSuffix(lines[1], \":\") && nextLineQuoted(1) {\n\t\tresult = \"[...]\\n\"\n\t\tlines = lines[3:]\n\t\tensureLines()\n\t}\n\n\tfor ; len(lines) > 0 && !isSignature(); ensureLines() {\n\t\tline := lines[0]\n\t\tif strings.HasPrefix(line, \">\") {\n\t\t\tif !resultSnipped() {\n\t\t\t\tresult += \"[...]\\n\"\n\t\t\t}\n\t\t\tlines = lines[1:]\n\t\t\tcontinue\n\t\t}\n\t\tif line == \"\" {\n\t\t\tlines = lines[1:]\n\t\t\tcontinue\n\t\t}\n\t\t// Check for a \"On <date>, <person> wrote:\", we require digits before a quoted\n\t\t// line, with an optional empty line in between. If we don't have any text yet, we\n\t\t// don't require the digits.\n\t\tif strings.HasSuffix(line, \":\") && (strings.ContainsAny(line, \"0123456789\") || result == \"\") && nextLineQuoted(0) {\n\t\t\tif !resultSnipped() {\n\t\t\t\tresult += \"[...]\\n\"\n\t\t\t}\n\t\t\tlines = lines[1:]\n\t\t\tcontinue\n\t\t}\n\t\t// Skip possibly duplicate snipping by author.\n\t\tif !isSnipped(line) || !resultSnipped() {\n\t\t\tresult += line + \"\\n\"\n\t\t}\n\t\tlines = lines[1:]\n\t\tif len(result) > 250 {\n\t\t\tbreak\n\t\t}\n\t}\n\tif len(result) > 250 {\n\t\tresult = result[:230] + \"...\"\n\t}\n\treturn result, scanner.Err()\n}\n\nfunc parsedMessage(log mlog.Log, m store.Message, state *msgState, full, msgitem bool) (pm ParsedMessage, rerr error) {\n\tif full || msgitem {\n\t\tif !state.ensurePart(m, true) {\n\t\t\treturn pm, state.err\n\t\t}\n\t\tif full {\n\t\t\tpm.Part = *state.part\n\t\t}\n\t} else {\n\t\tif !state.ensurePart(m, false) {\n\t\t\treturn pm, state.err\n\t\t}\n\t}\n\n\t// todo: we should store this form in message.Part, requires a data structure update.\n\n\tconvertAddrs := func(l []message.Address) []MessageAddress {\n\t\tr := make([]MessageAddress, len(l))\n\t\tfor i, a := range l {\n\t\t\td, err := dns.ParseDomain(a.Host)\n\t\t\tlog.Check(err, \"parsing domain\")\n\t\t\tif err != nil {\n\t\t\t\td = dns.Domain{ASCII: a.Host}\n\t\t\t}\n\t\t\tr[i] = MessageAddress{a.Name, a.User, d}\n\t\t}\n\t\treturn r\n\t}\n\n\tif full || msgitem {\n\t\tenv := MessageEnvelope{}\n\t\tif state.part.Envelope != nil {\n\t\t\te := *state.part.Envelope\n\t\t\tenv.Date = e.Date\n\t\t\tenv.Subject = e.Subject\n\t\t\tenv.InReplyTo = e.InReplyTo\n\t\t\tenv.MessageID = e.MessageID\n\t\t\tenv.From = convertAddrs(e.From)\n\t\t\tenv.Sender = convertAddrs(e.Sender)\n\t\t\tenv.ReplyTo = convertAddrs(e.ReplyTo)\n\t\t\tenv.To = convertAddrs(e.To)\n\t\t\tenv.CC = convertAddrs(e.CC)\n\t\t\tenv.BCC = convertAddrs(e.BCC)\n\t\t}\n\t\tpm.envelope = env\n\t}\n\n\tif full && state.part.BodyOffset > 0 {\n\t\thdrs, err := state.part.Header()\n\t\tif err != nil {\n\t\t\treturn ParsedMessage{}, fmt.Errorf(\"parsing headers: %v\", err)\n\t\t}\n\t\tpm.Headers = hdrs\n\n\t\tpm.ListReplyAddress = parseListPostAddress(hdrs.Get(\"List-Post\"))\n\t} else {\n\t\tpm.Headers = map[string][]string{}\n\t}\n\n\tpm.Texts = []string{}\n\n\t// We track attachments from multipart/mixed differently from other attachments.\n\t// The others are often inline, sometimes just some logo's in HTML alternative\n\t// messages. We want to have our mixed attachments at the start of the list, but\n\t// our descent-first parsing would result in inline messages first in the typical\n\t// message.\n\tvar attachmentsMixed []Attachment\n\tvar attachmentsOther []Attachment\n\n\taddAttachment := func(a Attachment, isMixed bool) {\n\t\tif isMixed {\n\t\t\tattachmentsMixed = append(attachmentsMixed, a)\n\t\t} else {\n\t\t\tattachmentsOther = append(attachmentsOther, a)\n\t\t}\n\t}\n\n\t// todo: how should we handle messages where a user prefers html, and we want to show it, but it's a DSN that also has textual-only parts? e.g. gmail's dsn where the first part is multipart/related with multipart/alternative, and second part is the regular message/delivery-status. we want to display both the html and the text.\n\n\tvar usePart func(p message.Part, index int, parent *message.Part, path []int, parentMixed bool)\n\tusePart = func(p message.Part, index int, parent *message.Part, path []int, parentMixed bool) {\n\t\tmt := p.MediaType + \"/\" + p.MediaSubType\n\t\tnewParentMixed := mt == \"MULTIPART/MIXED\"\n\t\tfor i, sp := range p.Parts {\n\t\t\tif mt == \"MULTIPART/SIGNED\" && i >= 1 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tusePart(sp, i, &p, append(append([]int{}, path...), i), newParentMixed)\n\t\t}\n\t\tswitch mt {\n\t\tcase \"TEXT/PLAIN\", \"/\":\n\t\t\t// Don't include if Content-Disposition attachment.\n\t\t\tif full || msgitem {\n\t\t\t\t// todo: should have this, and perhaps all content-* headers, preparsed in message.Part?\n\t\t\t\th, err := p.Header()\n\t\t\t\tlog.Check(err, \"parsing attachment headers\", slog.Int64(\"msgid\", m.ID))\n\t\t\t\tcp := h.Get(\"Content-Disposition\")\n\t\t\t\tif cp != \"\" {\n\t\t\t\t\tdisp, params, err := mime.ParseMediaType(cp)\n\t\t\t\t\tlog.Check(err, \"parsing content-disposition\", slog.String(\"cp\", cp))\n\t\t\t\t\tif strings.EqualFold(disp, \"attachment\") {\n\t\t\t\t\t\tname := tryDecodeParam(log, p.ContentTypeParams[\"name\"])\n\t\t\t\t\t\tif name == \"\" {\n\t\t\t\t\t\t\tname = tryDecodeParam(log, params[\"filename\"])\n\t\t\t\t\t\t}\n\t\t\t\t\t\taddAttachment(Attachment{path, name, p}, parentMixed)\n\t\t\t\t\t\treturn\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif full {\n\t\t\t\tbuf, err := io.ReadAll(&moxio.LimitReader{R: p.ReaderUTF8OrBinary(), Limit: 2 * 1024 * 1024})\n\t\t\t\tif err != nil {\n\t\t\t\t\trerr = fmt.Errorf(\"reading text part: %v\", err)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tpm.Texts = append(pm.Texts, string(buf))\n\t\t\t}\n\t\t\tif msgitem && pm.firstLine == \"\" {\n\t\t\t\tpm.firstLine, rerr = formatFirstLine(p.ReaderUTF8OrBinary())\n\t\t\t\tif rerr != nil {\n\t\t\t\t\trerr = fmt.Errorf(\"reading text for first line snippet: %v\", rerr)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase \"TEXT/HTML\":\n\t\t\tpm.HasHTML = true\n\n\t\tdefault:\n\t\t\t// todo: see if there is a common nesting messages that are both signed and encrypted.\n\t\t\tif parent == nil && mt == \"MULTIPART/SIGNED\" {\n\t\t\t\tpm.isSigned = true\n\t\t\t}\n\t\t\tif parent == nil && mt == \"MULTIPART/ENCRYPTED\" {\n\t\t\t\tpm.isEncrypted = true\n\t\t\t}\n\t\t\t// todo: possibly do not include anything below multipart/alternative that starts with text/html, they may be cids. perhaps have a separate list of attachments for the text vs html version?\n\t\t\tif p.MediaType != \"MULTIPART\" {\n\t\t\t\tvar parentct string\n\t\t\t\tif parent != nil {\n\t\t\t\t\tparentct = parent.MediaType + \"/\" + parent.MediaSubType\n\t\t\t\t}\n\n\t\t\t\t// Recognize DSNs.\n\t\t\t\tif parentct == \"MULTIPART/REPORT\" && index == 1 && (mt == \"MESSAGE/GLOBAL-DELIVERY-STATUS\" || mt == \"MESSAGE/DELIVERY-STATUS\") {\n\t\t\t\t\tif full {\n\t\t\t\t\t\tbuf, err := io.ReadAll(&moxio.LimitReader{R: p.ReaderUTF8OrBinary(), Limit: 1024 * 1024})\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\trerr = fmt.Errorf(\"reading text part: %v\", err)\n\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t}\n\t\t\t\t\t\tpm.Texts = append(pm.Texts, string(buf))\n\t\t\t\t\t}\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif parentct == \"MULTIPART/REPORT\" && index == 2 && (mt == \"MESSAGE/GLOBAL-HEADERS\" || mt == \"TEXT/RFC822-HEADERS\") {\n\t\t\t\t\tif full {\n\t\t\t\t\t\tbuf, err := io.ReadAll(&moxio.LimitReader{R: p.ReaderUTF8OrBinary(), Limit: 1024 * 1024})\n\t\t\t\t\t\tif err != nil {\n\t\t\t\t\t\t\trerr = fmt.Errorf(\"reading text part: %v\", err)\n\t\t\t\t\t\t\treturn\n\t\t\t\t\t\t}\n\t\t\t\t\t\tpm.Texts = append(pm.Texts, string(buf))\n\t\t\t\t\t}\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif parentct == \"MULTIPART/REPORT\" && index == 2 && (mt == \"MESSAGE/GLOBAL\" || mt == \"TEXT/RFC822\") {\n\t\t\t\t\taddAttachment(Attachment{path, \"original.eml\", p}, parentMixed)\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tname := tryDecodeParam(log, p.ContentTypeParams[\"name\"])\n\t\t\t\tif name == \"\" && (full || msgitem) {\n\t\t\t\t\t// todo: should have this, and perhaps all content-* headers, preparsed in message.Part?\n\t\t\t\t\th, err := p.Header()\n\t\t\t\t\tlog.Check(err, \"parsing attachment headers\", slog.Int64(\"msgid\", m.ID))\n\t\t\t\t\tcp := h.Get(\"Content-Disposition\")\n\t\t\t\t\tif cp != \"\" {\n\t\t\t\t\t\t_, params, err := mime.ParseMediaType(cp)\n\t\t\t\t\t\tlog.Check(err, \"parsing content-disposition\", slog.String(\"cp\", cp))\n\t\t\t\t\t\tname = tryDecodeParam(log, params[\"filename\"])\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\taddAttachment(Attachment{path, name, p}, parentMixed)\n\t\t\t}\n\t\t}\n\t}\n\tusePart(*state.part, -1, nil, []int{}, false)\n\n\tpm.attachments = []Attachment{}\n\tpm.attachments = append(pm.attachments, attachmentsMixed...)\n\tpm.attachments = append(pm.attachments, attachmentsOther...)\n\n\tif rerr == nil {\n\t\tpm.ID = m.ID\n\t}\n\treturn\n}\n\n// parses List-Post header, returning an address if it could be found, and nil otherwise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func parseListPostAddress(s string) *MessageAddress {\n# Natural Language Annotations: // parses List-Post header, returning an address if it could be found, and nil otherwise.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc parseListPostAddress(s string) *MessageAddress {\n\t// If the string is empty, return nil\n\tif s == \"\" {\n\t\treturn nil\n\t}\n\n\t// Split the string by semicolon to get key-value pairs\n\tpairs := strings.Split(s, \";\")\n\t\n\t// Initialize a map to store the key-value pairs\n\tparams := make(map[string]string)\n\n\t// Process each key-value pair\n\tfor _, pair := range pairs {\n\t\t// Split the pair by equals sign to separate key and value\n\t\tkv := strings.Split(pair, \"=\")\n\t\tif len(kv) != 2 {\n\t\t\tcontinue // Skip if the pair does not contain an equals sign\n\t\t}\n\t\tkey := strings.TrimSpace(kv[0])\n\t\tvalue := strings.TrimSpace(kv[1])\n\t\tparams[key] = value\n\t}\n\n\t// Extract the address and name from the params\n\taddrStr, ok := params[\"mailto\"]\n\tif !ok {\n\t\treturn nil // Return nil if there's no mailto parameter\n\t}\n\taddrStr = strings.Trim(addrStr, \"<>\") // Trim angle brackets from the address\n\taddrParts := strings.Split(addrStr, \"@\")\n\tif len(addrParts) != 2 {\n\t\treturn nil // Return nil if the address is not in the correct format\n\t}\n\tuser := addrParts[0]\n\thost := addrParts[1]\n\tname, _ := params[\"display-name\"] // name is optional, so we ignore the error if it's not found\n\n\t// Parse the host into a dns.Domain\n\tdomain, err := dns.ParseDomain(host)\n\tif err != nil {\n\t\t// If parsing the domain fails, use the ASCII version\n\t\tdomain = dns.Domain{ASCII: host}\n\t}\n\n\t// Return the MessageAddress\n\treturn &MessageAddress{Name: name, User: user, Host: domain}\n}\n```", "```go\nfunc parseListPostAddress(s string) *MessageAddress {\n\tif s == \"\" {\n\t\treturn nil\n\t}\n\n\tfields := strings.Split(s, \";\")\n\n\tvar address MessageAddress\n\n\tfor _, field := range fields {\n\t\tkeyValue := strings.Split(field, \"=\")\n\t\tif len(keyValue) != 2 {\n\t\t\tcontinue\n\t\t}\n\n\t\tkey := strings.TrimSpace(keyValue[0])\n\t\tvalue := strings.TrimSpace(keyValue[1])\n\n\t\tswitch key {\n\t\tcase \"mailto\":\n\t\t\taddr, err := message.ParseAddress(value)\n\t\t\tif err != nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\td, err := dns.ParseDomain(addr.Host)\n\t\t\tif err != nil {\n\t\t\t\td = dns.Domain{ASCII: addr.Host}\n\t\t\t}\n\t\t\taddress.Domain = d\n\t\t\taddress.User = addr.User\n\t\t\taddress.Name = addr.Name\n\t\t}\n\t}\n\n\tif address.Domain.ASCII == \"\" {\n\t\treturn nil\n\t}\n\n\treturn &address\n}\n```\nThis function parses the List-Post header and returns an address if it could be found, and nil otherwise. It uses the message.ParseAddress function to parse the mailto value and the dns.ParseDomain function to parse the domain.", "```go\nfunc parseListPostAddress(s string) *MessageAddress {\n\taddr, params, err := mime.ParseMediaType(s)\n\tif err != nil {\n\t\treturn nil\n\t}\n\tparts := strings.Split(params[\"mailto\"], \";\")\n\tmailto := parts[0]\n\tname := params[\"name\"]\n\tuser, host := parseAddress(mailto)\n\tdomain, err := dns.ParseDomain(host)\n\tif err != nil {\n\t\tdomain = dns.Domain{ASCII: host}\n\t}\n\treturn &MessageAddress{name, user, domain}\n}\n\nfunc parseAddress(addr string) (user, host string) {\n\ti := strings.LastIndex(addr, \"@\")\n\tif i < 0 {\n\t\treturn \"\", \"\"\n\t}\n\treturn addr[:i], addr[i+1:]\n}\n```"]}, "mox-webmail/webmail.go-sanitizeNode": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/webmail/webmail.go:\n```\n// Package webmail implements a webmail client, serving html/js and providing an API for message actions and SSE endpoint for receiving real-time updates.\npackage webmail\n\n// todo: should we be serving the messages/parts on a separate (sub)domain for user-content? to limit damage if the csp rules aren't enough.\n\nimport (\n\t\"archive/zip\"\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/base64\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"net/http\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"regexp\"\n\t\"runtime/debug\"\n\t\"strconv\"\n\t\"strings\"\n\n\t_ \"embed\"\n\n\t\"golang.org/x/net/html\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/bstore\"\n\t\"github.com/mjl-/sherpa\"\n\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/metrics\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/store\"\n\t\"github.com/mjl-/mox/webauth\"\n\t\"github.com/mjl-/mox/webops\"\n)\n\nvar pkglog = mlog.New(\"webmail\", nil)\n\ntype ctxKey string\n\n// We pass the request to the sherpa handler so the TLS info can be used for\n// the Received header in submitted messages. Most API calls need just the\n// account name.\nvar requestInfoCtxKey ctxKey = \"requestInfo\"\n\ntype requestInfo struct {\n\tLog          mlog.Log\n\tLoginAddress string\n\tAccount      *store.Account // Nil only for methods Login and LoginPrep.\n\tSessionToken store.SessionToken\n\tResponse     http.ResponseWriter\n\tRequest      *http.Request // For Proto and TLS connection state during message submit.\n}\n\n//go:embed webmail.html\nvar webmailHTML []byte\n\n//go:embed webmail.js\nvar webmailJS []byte\n\n//go:embed msg.html\nvar webmailmsgHTML []byte\n\n//go:embed msg.js\nvar webmailmsgJS []byte\n\n//go:embed text.html\nvar webmailtextHTML []byte\n\n//go:embed text.js\nvar webmailtextJS []byte\n\nvar (\n\t// Similar between ../webmail/webmail.go:/metricSubmission and ../smtpserver/server.go:/metricSubmission and ../webapisrv/server.go:/metricSubmission\n\tmetricSubmission = promauto.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_webmail_submission_total\",\n\t\t\tHelp: \"Webmail message submission results, known values (those ending with error are server errors): ok, badfrom, messagelimiterror, recipientlimiterror, queueerror, storesenterror.\",\n\t\t},\n\t\t[]string{\n\t\t\t\"result\",\n\t\t},\n\t)\n\tmetricServerErrors = promauto.NewCounterVec(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_webmail_errors_total\",\n\t\t\tHelp: \"Webmail server errors, known values: dkimsign, submit.\",\n\t\t},\n\t\t[]string{\n\t\t\t\"error\",\n\t\t},\n\t)\n\tmetricSSEConnections = promauto.NewGauge(\n\t\tprometheus.GaugeOpts{\n\t\t\tName: \"mox_webmail_sse_connections\",\n\t\t\tHelp: \"Number of active webmail SSE connections.\",\n\t\t},\n\t)\n)\n\nfunc xcheckf(ctx context.Context, err error, format string, args ...any) {\n\tif err == nil {\n\t\treturn\n\t}\n\tmsg := fmt.Sprintf(format, args...)\n\terrmsg := fmt.Sprintf(\"%s: %s\", msg, err)\n\tpkglog.WithContext(ctx).Errorx(msg, err)\n\tcode := \"server:error\"\n\tif errors.Is(err, context.Canceled) || errors.Is(err, context.DeadlineExceeded) {\n\t\tcode = \"user:error\"\n\t}\n\tpanic(&sherpa.Error{Code: code, Message: errmsg})\n}\n\nfunc xcheckuserf(ctx context.Context, err error, format string, args ...any) {\n\tif err == nil {\n\t\treturn\n\t}\n\tmsg := fmt.Sprintf(format, args...)\n\terrmsg := fmt.Sprintf(\"%s: %s\", msg, err)\n\tpkglog.WithContext(ctx).Errorx(msg, err)\n\tpanic(&sherpa.Error{Code: \"user:error\", Message: errmsg})\n}\n\nfunc xdbwrite(ctx context.Context, acc *store.Account, fn func(tx *bstore.Tx)) {\n\terr := acc.DB.Write(ctx, func(tx *bstore.Tx) error {\n\t\tfn(tx)\n\t\treturn nil\n\t})\n\txcheckf(ctx, err, \"transaction\")\n}\n\nfunc xdbread(ctx context.Context, acc *store.Account, fn func(tx *bstore.Tx)) {\n\terr := acc.DB.Read(ctx, func(tx *bstore.Tx) error {\n\t\tfn(tx)\n\t\treturn nil\n\t})\n\txcheckf(ctx, err, \"transaction\")\n}\n\nvar webmailFile = &mox.WebappFile{\n\tHTML:     webmailHTML,\n\tJS:       webmailJS,\n\tHTMLPath: filepath.FromSlash(\"webmail/webmail.html\"),\n\tJSPath:   filepath.FromSlash(\"webmail/webmail.js\"),\n}\n\n// Serve content, either from a file, or return the fallback data. Caller\n// should already have set the content-type. We use this to return a file from\n// the local file system (during development), or embedded in the binary (when\n// deployed).\nfunc serveContentFallback(log mlog.Log, w http.ResponseWriter, r *http.Request, path string, fallback []byte) {\n\tf, err := os.Open(path)\n\tif err == nil {\n\t\tdefer f.Close()\n\t\tst, err := f.Stat()\n\t\tif err == nil {\n\t\t\thttp.ServeContent(w, r, \"\", st.ModTime(), f)\n\t\t\treturn\n\t\t}\n\t}\n\thttp.ServeContent(w, r, \"\", mox.FallbackMtime(log), bytes.NewReader(fallback))\n}\n\nfunc init() {\n\tmox.NewWebmailHandler = func(maxMsgSize int64, basePath string, isForwarded bool, accountPath string) http.Handler {\n\t\treturn http.HandlerFunc(Handler(maxMsgSize, basePath, isForwarded, accountPath))\n\t}\n}\n\n// Handler returns a handler for the webmail endpoints, customized for the max\n// message size coming from the listener and cookiePath.\nfunc Handler(maxMessageSize int64, cookiePath string, isForwarded bool, accountPath string) func(w http.ResponseWriter, r *http.Request) {\n\tsh, err := makeSherpaHandler(maxMessageSize, cookiePath, isForwarded)\n\treturn func(w http.ResponseWriter, r *http.Request) {\n\t\tif err != nil {\n\t\t\thttp.Error(w, \"500 - internal server error - cannot handle requests\", http.StatusInternalServerError)\n\t\t\treturn\n\t\t}\n\t\thandle(sh, isForwarded, accountPath, w, r)\n\t}\n}\n\nfunc handle(apiHandler http.Handler, isForwarded bool, accountPath string, w http.ResponseWriter, r *http.Request) {\n\tctx := r.Context()\n\tlog := pkglog.WithContext(ctx).With(slog.String(\"userauth\", \"\"))\n\n\t// Server-sent event connection, for all initial data (list of mailboxes), list of\n\t// messages, and all events afterwards. Authenticated through a token in the query\n\t// string, which it got from a Token API call.\n\tif r.URL.Path == \"/events\" {\n\t\tserveEvents(ctx, log, accountPath, w, r)\n\t\treturn\n\t}\n\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\terr, ok := x.(*sherpa.Error)\n\t\tif !ok {\n\t\t\tlog.WithContext(ctx).Error(\"handle panic\", slog.Any(\"err\", x))\n\t\t\tdebug.PrintStack()\n\t\t\tmetrics.PanicInc(metrics.Webmailhandle)\n\t\t\tpanic(x)\n\t\t}\n\t\tif strings.HasPrefix(err.Code, \"user:\") {\n\t\t\tlog.Debugx(\"webmail user error\", err)\n\t\t\thttp.Error(w, \"400 - bad request - \"+err.Message, http.StatusBadRequest)\n\t\t} else {\n\t\t\tlog.Errorx(\"webmail server error\", err)\n\t\t\thttp.Error(w, \"500 - internal server error - \"+err.Message, http.StatusInternalServerError)\n\t\t}\n\t}()\n\n\tswitch r.URL.Path {\n\tcase \"/\":\n\t\tswitch r.Method {\n\t\tcase \"GET\", \"HEAD\":\n\t\t\th := w.Header()\n\t\t\th.Set(\"X-Frame-Options\", \"deny\")\n\t\t\th.Set(\"Referrer-Policy\", \"same-origin\")\n\t\t\twebmailFile.Serve(ctx, log, w, r)\n\t\tdefault:\n\t\t\thttp.Error(w, \"405 - method not allowed - use get\", http.StatusMethodNotAllowed)\n\t\t}\n\t\treturn\n\n\tcase \"/msg.js\", \"/text.js\":\n\t\tswitch r.Method {\n\t\tdefault:\n\t\t\thttp.Error(w, \"405 - method not allowed - use get\", http.StatusMethodNotAllowed)\n\t\t\treturn\n\t\tcase \"GET\", \"HEAD\":\n\t\t}\n\n\t\tpath := filepath.Join(\"webmail\", r.URL.Path[1:])\n\t\tvar fallback = webmailmsgJS\n\t\tif r.URL.Path == \"/text.js\" {\n\t\t\tfallback = webmailtextJS\n\t\t}\n\n\t\tw.Header().Set(\"Content-Type\", \"application/javascript; charset=utf-8\")\n\t\tserveContentFallback(log, w, r, path, fallback)\n\t\treturn\n\t}\n\n\tisAPI := strings.HasPrefix(r.URL.Path, \"/api/\")\n\t// Only allow POST for calls, they will not work cross-domain without CORS.\n\tif isAPI && r.URL.Path != \"/api/\" && r.Method != \"POST\" {\n\t\thttp.Error(w, \"405 - method not allowed - use post\", http.StatusMethodNotAllowed)\n\t\treturn\n\t}\n\n\tvar loginAddress, accName string\n\tvar sessionToken store.SessionToken\n\t// All other URLs, except the login endpoint require some authentication.\n\tif r.URL.Path != \"/api/LoginPrep\" && r.URL.Path != \"/api/Login\" {\n\t\tvar ok bool\n\t\tisExport := r.URL.Path == \"/export\"\n\t\trequireCSRF := isAPI || isExport\n\t\taccName, sessionToken, loginAddress, ok = webauth.Check(ctx, log, webauth.Accounts, \"webmail\", isForwarded, w, r, isAPI, requireCSRF, isExport)\n\t\tif !ok {\n\t\t\t// Response has been written already.\n\t\t\treturn\n\t\t}\n\t}\n\n\tif isAPI {\n\t\tvar acc *store.Account\n\t\tif accName != \"\" {\n\t\t\tlog = log.With(slog.String(\"account\", accName))\n\t\t\tvar err error\n\t\t\tacc, err = store.OpenAccount(log, accName)\n\t\t\tif err != nil {\n\t\t\t\tlog.Errorx(\"open account\", err)\n\t\t\t\thttp.Error(w, \"500 - internal server error - error opening account\", http.StatusInternalServerError)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tdefer func() {\n\t\t\t\terr := acc.Close()\n\t\t\t\tlog.Check(err, \"closing account\")\n\t\t\t}()\n\t\t}\n\t\treqInfo := requestInfo{log, loginAddress, acc, sessionToken, w, r}\n\t\tctx = context.WithValue(ctx, requestInfoCtxKey, reqInfo)\n\t\tapiHandler.ServeHTTP(w, r.WithContext(ctx))\n\t\treturn\n\t}\n\n\t// We are now expecting the following URLs:\n\t// .../export\n\t// .../msg/<msgid>/{attachments.zip,parsedmessage.js,raw}\n\t// .../msg/<msgid>/{,msg}{text,html,htmlexternal}\n\t// .../msg/<msgid>/{view,viewtext,download}/<partid>\n\n\tif r.URL.Path == \"/export\" {\n\t\twebops.Export(log, accName, w, r)\n\t\treturn\n\t}\n\n\tif !strings.HasPrefix(r.URL.Path, \"/msg/\") {\n\t\thttp.NotFound(w, r)\n\t\treturn\n\t}\n\n\tt := strings.Split(r.URL.Path[len(\"/msg/\"):], \"/\")\n\tif len(t) < 2 {\n\t\thttp.NotFound(w, r)\n\t\treturn\n\t}\n\n\tid, err := strconv.ParseInt(t[0], 10, 64)\n\tif err != nil || id == 0 {\n\t\thttp.NotFound(w, r)\n\t\treturn\n\t}\n\n\t// Many of the requests need either a message or a parsed part. Make it easy to\n\t// fetch/prepare and cleanup. We only do all the work when the request seems legit\n\t// (valid HTTP route and method).\n\txprepare := func() (acc *store.Account, m store.Message, msgr *store.MsgReader, p message.Part, cleanup func(), ok bool) {\n\t\tif r.Method != \"GET\" {\n\t\t\thttp.Error(w, \"405 - method not allowed - post required\", http.StatusMethodNotAllowed)\n\t\t\treturn\n\t\t}\n\n\t\tdefer func() {\n\t\t\tif ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif msgr != nil {\n\t\t\t\terr := msgr.Close()\n\t\t\t\tlog.Check(err, \"closing message reader\")\n\t\t\t\tmsgr = nil\n\t\t\t}\n\t\t\tif acc != nil {\n\t\t\t\terr := acc.Close()\n\t\t\t\tlog.Check(err, \"closing account\")\n\t\t\t\tacc = nil\n\t\t\t}\n\t\t}()\n\n\t\tvar err error\n\n\t\tacc, err = store.OpenAccount(log, accName)\n\t\txcheckf(ctx, err, \"open account\")\n\n\t\tm = store.Message{ID: id}\n\t\terr = acc.DB.Get(ctx, &m)\n\t\tif err == bstore.ErrAbsent || err == nil && m.Expunged {\n\t\t\thttp.NotFound(w, r)\n\t\t\treturn\n\t\t}\n\t\txcheckf(ctx, err, \"get message\")\n\n\t\tmsgr = acc.MessageReader(m)\n\n\t\tp, err = m.LoadPart(msgr)\n\t\txcheckf(ctx, err, \"load parsed message\")\n\n\t\tcleanup = func() {\n\t\t\terr := msgr.Close()\n\t\t\tlog.Check(err, \"closing message reader\")\n\t\t\terr = acc.Close()\n\t\t\tlog.Check(err, \"closing account\")\n\t\t}\n\t\tok = true\n\t\treturn\n\t}\n\n\th := w.Header()\n\n\t// We set a Content-Security-Policy header that is as strict as possible, depending\n\t// on the type of message/part/html/js. We have to be careful because we are\n\t// returning data that is coming in from external places. E.g. HTML could contain\n\t// javascripts that we don't want to execute, especially not on our domain. We load\n\t// resources in an iframe. The CSP policy starts out  with default-src 'none' to\n\t// disallow loading anything, then start allowing what is safe, such as inlined\n\t// datauri images and inline styles. Data can only be loaded when the request is\n\t// coming from the same origin (so other sites cannot include resources\n\t// (messages/parts)).\n\t//\n\t// We want to load resources in sandbox-mode, causing the page to be loaded as from\n\t// a different origin. If sameOrigin is set, we have a looser CSP policy:\n\t// allow-same-origin is set so resources are loaded as coming from this same\n\t// origin. This is needed for the msg* endpoints that render a message, where we\n\t// load the message body in a separate iframe again (with stricter CSP again),\n\t// which we need to access for its inner height. If allowSelfScript is also set\n\t// (for \"msgtext\"), the CSP leaves out the sandbox entirely.\n\t//\n\t// If allowExternal is set, we allow loading image, media (audio/video), styles and\n\t// fronts from external URLs as well as inline URI's. By default we don't allow any\n\t// loading of content, except inlined images (we do that ourselves for images\n\t// embedded in the email), and we allow inline styles (which are safely constrained\n\t// to an iframe).\n\t//\n\t// If allowSelfScript is set, inline scripts and scripts from our origin are\n\t// allowed. Used to display a message including header. The header is rendered with\n\t// javascript, the content is rendered in a separate iframe with a CSP that doesn't\n\t// have allowSelfScript.\n\theaders := func(sameOrigin, allowExternal, allowSelfScript, allowSelfImg bool) {\n\t\t// allow-popups is needed to make opening links in new tabs work.\n\t\tsb := \"sandbox allow-popups allow-popups-to-escape-sandbox; \"\n\t\tif sameOrigin && allowSelfScript {\n\t\t\t// Sandbox with both allow-same-origin and allow-script would not provide security,\n\t\t\t// and would give warning in console about that.\n\t\t\tsb = \"\"\n\t\t} else if sameOrigin {\n\t\t\tsb = \"sandbox allow-popups allow-popups-to-escape-sandbox allow-same-origin; \"\n\t\t}\n\t\tscript := \"\"\n\t\tif allowSelfScript {\n\t\t\tscript = \"; script-src 'unsafe-inline' 'self'; frame-src 'self'; connect-src 'self'\"\n\t\t}\n\t\tvar csp string\n\t\tif allowExternal {\n\t\t\tcsp = sb + \"frame-ancestors 'self'; default-src 'none'; img-src data: http: https: 'unsafe-inline'; style-src 'unsafe-inline' data: http: https:; font-src data: http: https: 'unsafe-inline'; media-src 'unsafe-inline' data: http: https:\" + script\n\t\t} else if allowSelfImg {\n\t\t\tcsp = sb + \"frame-ancestors 'self'; default-src 'none'; img-src data: 'self'; style-src 'unsafe-inline'\" + script\n\t\t} else {\n\t\t\tcsp = sb + \"frame-ancestors 'self'; default-src 'none'; img-src data:; style-src 'unsafe-inline'\" + script\n\t\t}\n\t\th.Set(\"Content-Security-Policy\", csp)\n\t\th.Set(\"X-Frame-Options\", \"sameorigin\") // Duplicate with CSP, but better too much than too little.\n\t\th.Set(\"X-Content-Type-Options\", \"nosniff\")\n\t\th.Set(\"Referrer-Policy\", \"no-referrer\")\n\t}\n\n\tswitch {\n\tcase len(t) == 2 && t[1] == \"attachments.zip\":\n\t\tacc, m, msgr, p, cleanup, ok := xprepare()\n\t\tif !ok {\n\t\t\treturn\n\t\t}\n\t\tdefer cleanup()\n\t\tstate := msgState{acc: acc, m: m, msgr: msgr, part: &p}\n\t\t// note: state is cleared by cleanup\n\n\t\tmi, err := messageItem(log, m, &state)\n\t\txcheckf(ctx, err, \"parsing message\")\n\n\t\theaders(false, false, false, false)\n\t\th.Set(\"Content-Type\", \"application/zip\")\n\t\th.Set(\"Cache-Control\", \"no-store, max-age=0\")\n\t\tvar subjectSlug string\n\t\tif p.Envelope != nil {\n\t\t\ts := p.Envelope.Subject\n\t\t\ts = strings.ToLower(s)\n\t\t\ts = regexp.MustCompile(\"[^a-z0-9_.-]\").ReplaceAllString(s, \"-\")\n\t\t\ts = regexp.MustCompile(\"--*\").ReplaceAllString(s, \"-\")\n\t\t\ts = strings.TrimLeft(s, \"-\")\n\t\t\ts = strings.TrimRight(s, \"-\")\n\t\t\tif s != \"\" {\n\t\t\t\ts = \"-\" + s\n\t\t\t}\n\t\t\tsubjectSlug = s\n\t\t}\n\t\tfilename := fmt.Sprintf(\"email-%d-attachments-%s%s.zip\", m.ID, m.Received.Format(\"20060102-150405\"), subjectSlug)\n\t\tcd := mime.FormatMediaType(\"attachment\", map[string]string{\"filename\": filename})\n\t\th.Set(\"Content-Disposition\", cd)\n\n\t\tzw := zip.NewWriter(w)\n\t\tnames := map[string]bool{}\n\t\tfor _, a := range mi.Attachments {\n\t\t\tap := a.Part\n\t\t\tname := tryDecodeParam(log, ap.ContentTypeParams[\"name\"])\n\t\t\tif name == \"\" {\n\t\t\t\t// We don't check errors, this is all best-effort.\n\t\t\t\th, _ := ap.Header()\n\t\t\t\tdisposition := h.Get(\"Content-Disposition\")\n\t\t\t\t_, params, _ := mime.ParseMediaType(disposition)\n\t\t\t\tname = tryDecodeParam(log, params[\"filename\"])\n\t\t\t}\n\t\t\tif name != \"\" {\n\t\t\t\tname = filepath.Base(name)\n\t\t\t}\n\t\t\tmt := strings.ToLower(ap.MediaType + \"/\" + ap.MediaSubType)\n\t\t\tif name == \"\" || names[name] {\n\t\t\t\text := filepath.Ext(name)\n\t\t\t\tif ext == \"\" {\n\t\t\t\t\t// Handle just a few basic types.\n\t\t\t\t\textensions := map[string]string{\n\t\t\t\t\t\t\"text/plain\":      \".txt\",\n\t\t\t\t\t\t\"text/html\":       \".html\",\n\t\t\t\t\t\t\"image/jpeg\":      \".jpg\",\n\t\t\t\t\t\t\"image/png\":       \".png\",\n\t\t\t\t\t\t\"image/gif\":       \".gif\",\n\t\t\t\t\t\t\"application/zip\": \".zip\",\n\t\t\t\t\t}\n\t\t\t\t\text = extensions[mt]\n\t\t\t\t\tif ext == \"\" {\n\t\t\t\t\t\text = \".bin\"\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tvar stem string\n\t\t\t\tif name != \"\" && strings.HasSuffix(name, ext) {\n\t\t\t\t\tstem = strings.TrimSuffix(name, ext)\n\t\t\t\t} else {\n\t\t\t\t\tstem = \"attachment\"\n\t\t\t\t\tfor _, index := range a.Path {\n\t\t\t\t\t\tstem += fmt.Sprintf(\"-%d\", index)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tname = stem + ext\n\t\t\t\tseq := 0\n\t\t\t\tfor names[name] {\n\t\t\t\t\tseq++\n\t\t\t\t\tname = stem + fmt.Sprintf(\"-%d\", seq) + ext\n\t\t\t\t}\n\t\t\t}\n\t\t\tnames[name] = true\n\n\t\t\tfh := zip.FileHeader{\n\t\t\t\tName:     name,\n\t\t\t\tModified: m.Received,\n\t\t\t}\n\t\t\tnodeflate := map[string]bool{\n\t\t\t\t\"application/x-bzip2\":          true,\n\t\t\t\t\"application/zip\":              true,\n\t\t\t\t\"application/x-zip-compressed\": true,\n\t\t\t\t\"application/gzip\":             true,\n\t\t\t\t\"application/x-gzip\":           true,\n\t\t\t\t\"application/vnd.rar\":          true,\n\t\t\t\t\"application/x-rar-compressed\": true,\n\t\t\t\t\"application/x-7z-compressed\":  true,\n\t\t\t}\n\t\t\t// Sniff content-type as well for compressed data.\n\t\t\tbuf := make([]byte, 512)\n\t\t\tn, _ := io.ReadFull(ap.Reader(), buf)\n\t\t\tvar sniffmt string\n\t\t\tif n > 0 {\n\t\t\t\tsniffmt = strings.ToLower(http.DetectContentType(buf[:n]))\n\t\t\t}\n\t\t\tdeflate := ap.MediaType != \"VIDEO\" && ap.MediaType != \"AUDIO\" && (ap.MediaType != \"IMAGE\" || ap.MediaSubType == \"BMP\") && !nodeflate[mt] && !nodeflate[sniffmt]\n\t\t\tif deflate {\n\t\t\t\tfh.Method = zip.Deflate\n\t\t\t}\n\t\t\t// We cannot return errors anymore: we have already sent an application/zip header.\n\t\t\tif zf, err := zw.CreateHeader(&fh); err != nil {\n\t\t\t\tlog.Check(err, \"adding to zip file\")\n\t\t\t\treturn\n\t\t\t} else if _, err := io.Copy(zf, ap.Reader()); err != nil {\n\t\t\t\tlog.Check(err, \"writing to zip file\")\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\terr = zw.Close()\n\t\tlog.Check(err, \"final write to zip file\")\n\n\t// Raw display of a message, as text/plain.\n\tcase len(t) == 2 && t[1] == \"raw\":\n\t\t_, _, msgr, p, cleanup, ok := xprepare()\n\t\tif !ok {\n\t\t\treturn\n\t\t}\n\t\tdefer cleanup()\n\n\t\t// We intentially use text/plain. We certainly don't want to return a format that\n\t\t// browsers or users would think of executing. We do set the charset if available\n\n```\n# Function Signature: func sanitizeNode(node *html.Node) {\n# Natural Language Annotations: // sanitizeNode removes script elements, on* attributes, javascript: href\n// attributes, adds target=\"_blank\" to all links and to a base tag.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc sanitizeNode(node *html.Node) {\n\tswitch node.Type {\n\tcase html.ElementNode:\n\t\tswitch node.Data {\n\t\tcase \"script\":\n\t\t\tnode.Type = html.TextNode\n\t\tcase \"a\":\n\t\t\tfor i, attr := range node.Attr {\n\t\t\t\tif attr.Key == \"onclick\" || attr.Key == \"onmouseover\" || attr.Key == \"onload\" || attr.Key == \"href\" {\n\t\t\t\t\tnode.Attr = append(node.Attr[:i], node.Attr[i+1:]...)\n\t\t\t\t}\n\t\t\t\tif attr.Key == \"href\" {\n\t\t\t\t\tif strings.HasPrefix(attr.Val, \"javascript:\") {\n\t\t\t\t\t\tnode.Attr[i].Val = \"\"\n\t\t\t\t\t}\n\t\t\t\t\tif attr.Val != \"#\" {\n\t\t\t\t\t\tnode.Attr[i].Val = \"#\" + attr.Val\n\t\t\t\t\t\tnode.Attr = append(node.Attr, html.Attribute{Key: \"target\", Val: \"_blank\"})\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tfor c := node.FirstChild; c != nil; c = c.NextSibling {\n\t\tsanitizeNode(c)\n\t}\n}\n```\nThis function goes through each node in a HTML document, and if the node is an element node (\"a\" tag), it checks each attribute in the node. If the attribute key is \"onclick\", \"onmouseover\", \"onload\" or \"href\", it removes the attribute from the node. If the attribute key is \"href\" and it starts with \"javascript:\", it sets the attribute value to an empty string. If the attribute value is not \"#\", it prepends \"#\" to the attribute value and adds a new attribute \"target=_blank\" to the node.", "```go\nfunc sanitizeNode(node *html.Node) {\n\tswitch node.Type {\n\tcase html.ElementNode:\n\t\tswitch node.Data {\n\t\tcase \"script\":\n\t\t\tnode.Type = html.TextNode\n\t\t\tnode.Data = \"\"\n\t\t\treturn\n\t\tcase \"a\":\n\t\t\tfor i, attr := range node.Attr {\n\t\t\t\tif attr.Key == \"href\" && strings.HasPrefix(attr.Val, \"javascript:\") {\n\t\t\t\t\tnode.Attr[i].Val = \"\"\n\t\t\t\t}\n\t\t\t\tif strings.HasPrefix(attr.Key, \"on\") {\n\t\t\t\t\tnode.Attr[i].Key = \"\"\n\t\t\t\t\tnode.Attr[i].Val = \"\"\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tfor i, attr := range node.Attr {\n\t\t\tif attr.Key == \"target\" {\n\t\t\t\tnode.Attr[i].Val = \"_blank\"\n\t\t\t}\n\t\t}\n\t\tnode.Attr = append(node.Attr, html.Attribute{Key: \"target\", Val: \"_blank\"})\n\tcase html.DocumentNode:\n\t\tfor c := node.FirstChild; c != nil; c = c.NextSibling {\n\t\t\tsanitizeNode(c)\n\t\t}\n\tcase html.ElementNode:\n\t\tfor c := node.FirstChild; c != nil; c = c.NextSibling {\n\t\t\tsanitizeNode(c)\n\t\t}\n\t}\n}\n```", "```go\nfunc sanitizeNode(node *html.Node) {\n\tif node.Type == html.ElementNode {\n\t\tswitch node.Data {\n\t\tcase \"script\":\n\t\t\tnode.Type = html.TextNode\n\t\t\tnode.Data = \"\"\n\t\tcase \"a\":\n\t\t\tfor i, a := range node.Attr {\n\t\t\t\tif a.Key == \"href\" && strings.HasPrefix(a.Val, \"javascript:\") {\n\t\t\t\t\tnode.Attr[i].Val = \"#\"\n\t\t\t\t}\n\t\t\t\tif a.Key == \"onclick\" || a.Key == \"onmouseover\" || a.Key == \"onload\" || a.Key == \"onerror\" || a.Key == \"onfocus\" || a.Key == \"onblur\" || a.Key == \"onclick\" || a.Key == \"ondblclick\" || a.Key == \"onmousedown\" || a.Key == \"onmouseup\" || a.Key == \"onmousemove\" || a.Key == \"onmouseout\" || a.Key == \"onmouseover\" || a.Key == \"onmouseup\" || a.Key == \"onreset\" || a.Key == \"onselect\" || a.Key == \"onchange\" || a.Key == \"onsubmit\" || a.Key == \"onkeydown\" || a.Key == \"onkeypress\" || a.Key == \"onkeyup\" || a.Key == \"onunload\" {\n\t\t\t\t\tnode.Attr = append(node.Attr[:i], node.Attr[i+1:]...)\n\t\t\t\t}\n\t\t\t}\n\t\t\ttargetFound := false\n\t\t\tfor _, a := range node.Attr {\n\t\t\t\tif a.Key == \"target\" {\n\t\t\t\t\ttargetFound = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !targetFound {\n\t\t\t\tnode.Attr = append(node.Attr, html.Attribute{Key: \"target\", Val: \"_blank\"})\n\t\t\t}\n\t\t}\n\t}\n\tfor c := node.FirstChild; c != nil; c = c.NextSibling {\n\t\tsanitizeNode(c)\n\t}\n}\n```"]}, "mox-dnsbl/dnsbl.go-Lookup": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dnsbl/dnsbl.go:\n```\n// Package dnsbl implements DNS block lists (RFC 5782), for checking incoming messages from sources without reputation.\n//\n// A DNS block list contains IP addresses that should be blocked. The DNSBL is\n// queried using DNS \"A\" lookups. The DNSBL starts at a \"zone\", e.g.\n// \"dnsbl.example\". To look up whether an IP address is listed, a DNS name is\n// composed: For 10.11.12.13, that name would be \"13.12.11.10.dnsbl.example\". If\n// the lookup returns \"record does not exist\", the IP is not listed. If an IP\n// address is returned, the IP is listed. If an IP is listed, an additional TXT\n// lookup is done for more information about the block. IPv6 addresses are also\n// looked up with an DNS \"A\" lookup of a name similar to an IPv4 address, but with\n// 4-bit hexadecimal dot-separated characters, in reverse.\n//\n// The health of a DNSBL \"zone\" can be check through a lookup of 127.0.0.1\n// (must not be present) and 127.0.0.2 (must be present).\npackage dnsbl\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/stub\"\n)\n\nvar (\n\tMetricLookup stub.HistogramVec = stub.HistogramVecIgnore{}\n)\n\nvar ErrDNS = errors.New(\"dnsbl: dns error\") // Temporary error.\n\n// Status is the result of a DNSBL lookup.\ntype Status string\n\nvar (\n\tStatusTemperr Status = \"temperror\" // Temporary failure.\n\tStatusPass    Status = \"pass\"      // Not present in block list.\n\tStatusFail    Status = \"fail\"      // Present in block list.\n)\n\n// Lookup checks if \"ip\" occurs in the DNS block list \"zone\" (e.g. dnsbl.example.org).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// CheckHealth checks whether the DNSBL \"zone\" is operating correctly by\n// querying for 127.0.0.2 (must be present) and 127.0.0.1 (must not be present).\n// Users of a DNSBL should periodically check if the DNSBL is still operating\n// properly.\n// For temporary errors, ErrDNS is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func Lookup(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain, ip net.IP) (rstatus Status, rexplanation string, rerr error) {\n# Natural Language Annotations: // Lookup checks if \"ip\" occurs in the DNS block list \"zone\" (e.g. dnsbl.example.org).\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc Lookup(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain, ip net.IP) (rstatus Status, rexplanation string, rerr error) {\n\tstatus := StatusPass\n\tvar explanation string\n\tvar err error\n\n\t// Reverse the IP address.\n\tipstr := ip.String()\n\tipparts := strings.Split(ipstr, \".\")\n\tfor i, j := 0, len(ipparts)-1; i < j; i, j = i+1, j-1 {\n\t\tipparts[i], ipparts[j] = ipparts[j], ipparts[i]\n\t}\n\trevipstr := strings.Join(ipparts, \".\")\n\n\t// Construct the DNS name to look up.\n\tdnsname := fmt.Sprintf(\"%s.%s\", revipstr, zone)\n\n\t// Perform the DNS lookup.\n\tstart := time.Now()\n\trr, err := resolver.LookupA(ctx, dnsname)\n\tMetricLookup.Observe(time.Since(start).Seconds())\n\tif err != nil {\n\t\tif err == dns.ErrDNS {\n\t\t\treturn StatusTemperr, \"\", err\n\t\t}\n\t\treturn StatusFail, \"\", err\n\t}\n\n\t// If an IP address is returned, the IP is listed.\n\tif len(rr) > 0 {\n\t\tstatus = StatusFail\n\t\t// Perform an additional TXT lookup for more information about the block.\n\t\trr, err = resolver.LookupTXT(ctx, dnsname)\n\t\tif err != nil {\n\t\t\tmlog.Printf(\"dnsbl: can't lookup txt for %s\", dnsname)\n\t\t} else {\n\t\t\texplanation = strings.Join(rr, \", \")\n\t\t}\n\t}\n\n\treturn status, explanation, nil\n}\n```", "```go\npackage dnsbl\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/dns\"\n)\n\n// Lookup checks if \"ip\" occurs in the DNS block list \"zone\" (e.g. dnsbl.example.org).\nfunc Lookup(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain, ip net.IP) (rstatus Status, rexplanation string, rerr error) {\n\t// Convert the IP to a string.\n\tipStr := ip.String()\n\n\t// Convert the IP to a reversed dotted quad string.\n\tipDottedQuad := strings.Split(ipStr, \".\")\n\treversedIp := fmt.Sprintf(\"%s.%s.%s.%s\", ipDottedQuad[3], ipDottedQuad[2], ipDottedQuad[1], ipDottedQuad[0])\n\n\t// Create the DNS name to lookup.\n\tlookupName := fmt.Sprintf(\"%s.%s\", reversedIp, zone)\n\n\t// Perform the DNS lookup.\n\taddrs, err := resolver.LookupHost(ctx, lookupName)\n\tif err != nil {\n\t\t// If there was a temporary error, return it.\n\t\tif dnsErr, ok := err.(*dns.Error); ok && dnsErr.Temporary() {\n\t\t\treturn StatusTemperr, \"\", ErrDNS\n\t\t}\n\n\t\t// If there was a permanent error, return it.\n\t\treturn \"\", \"\", err\n\t}\n\n\t// If the IP was found in the block list, return a fail status.\n\tif len(addrs) > 0 {\n\t\treturn StatusFail, \"\", nil\n\t}\n\n\t// If the IP was not found in the block list, return a pass status.\n\treturn StatusPass, \"\", nil\n}\n```", "```go\npackage dnsbl\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\n\t\"github.com/mjl-/mox/dns\"\n)\n\n// Lookup checks if \"ip\" occurs in the DNS block list \"zone\" (e.g. dnsbl.example.org).\nfunc Lookup(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain, ip net.IP) (rstatus Status, rexplanation string, rerr error) {\n\tipStr := ip.String()\n\trevIPStr := reverseIP(ipStr)\n\tnames := []string{\n\t\tfmt.Sprintf(\"%s.%s\", revIPStr, zone),\n\t\tfmt.Sprintf(\"%s.%s\", revIPStr, zone),\n\t}\n\n\tfor _, name := range names {\n\t\trr, err := resolver.LookupA(ctx, name)\n\t\tif err != nil {\n\t\t\tif dns.IsNotFound(err) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\treturn StatusTemperr, \"\", fmt.Errorf(\"%w: %v\", ErrDNS, err)\n\t\t}\n\n\t\tfor _, r := range rr {\n\t\t\trexplanation, _ = resolver.LookupTXT(ctx, r.Name)\n\t\t\trstatus = StatusFail\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif rstatus != StatusFail {\n\t\trstatus = StatusPass\n\t}\n\n\treturn rstatus, rexplanation, nil\n}\n\nfunc reverseIP(ip string) string {\n\tparts := strings.Split(ip, \".\")\n\tfor i, j := 0, len(parts)-1; i < j; i, j = i+1, j-1 {\n\t\tparts[i], parts[j] = parts[j], parts[i]\n\t}\n\treturn strings.Join(parts, \".\")\n}\n```"]}, "mox-dnsbl/dnsbl.go-CheckHealth": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dnsbl/dnsbl.go:\n```\n// Package dnsbl implements DNS block lists (RFC 5782), for checking incoming messages from sources without reputation.\n//\n// A DNS block list contains IP addresses that should be blocked. The DNSBL is\n// queried using DNS \"A\" lookups. The DNSBL starts at a \"zone\", e.g.\n// \"dnsbl.example\". To look up whether an IP address is listed, a DNS name is\n// composed: For 10.11.12.13, that name would be \"13.12.11.10.dnsbl.example\". If\n// the lookup returns \"record does not exist\", the IP is not listed. If an IP\n// address is returned, the IP is listed. If an IP is listed, an additional TXT\n// lookup is done for more information about the block. IPv6 addresses are also\n// looked up with an DNS \"A\" lookup of a name similar to an IPv4 address, but with\n// 4-bit hexadecimal dot-separated characters, in reverse.\n//\n// The health of a DNSBL \"zone\" can be check through a lookup of 127.0.0.1\n// (must not be present) and 127.0.0.2 (must be present).\npackage dnsbl\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/stub\"\n)\n\nvar (\n\tMetricLookup stub.HistogramVec = stub.HistogramVecIgnore{}\n)\n\nvar ErrDNS = errors.New(\"dnsbl: dns error\") // Temporary error.\n\n// Status is the result of a DNSBL lookup.\ntype Status string\n\nvar (\n\tStatusTemperr Status = \"temperror\" // Temporary failure.\n\tStatusPass    Status = \"pass\"      // Not present in block list.\n\tStatusFail    Status = \"fail\"      // Present in block list.\n)\n\n// Lookup checks if \"ip\" occurs in the DNS block list \"zone\" (e.g. dnsbl.example.org).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// CheckHealth checks whether the DNSBL \"zone\" is operating correctly by\n// querying for 127.0.0.2 (must be present) and 127.0.0.1 (must not be present).\n// Users of a DNSBL should periodically check if the DNSBL is still operating\n// properly.\n// For temporary errors, ErrDNS is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func CheckHealth(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain) (rerr error) {\n# Natural Language Annotations: // CheckHealth checks whether the DNSBL \"zone\" is operating correctly by\n// querying for 127.0.0.2 (must be present) and 127.0.0.1 (must not be present).\n// Users of a DNSBL should periodically check if the DNSBL is still operating\n// properly.\n// For temporary errors, ErrDNS is returned.\nPlease return the generated code surrounded by ```\n", "response": ["Sure, here is the code for the `CheckHealth` function based on the given function signature and annotations:\n\n```go\nfunc CheckHealth(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain) (rerr error) {\n\t// CheckHealth checks whether the DNSBL \"zone\" is operating correctly by\n\t// querying for 127.0.0.2 (must be present) and 127.0.0.1 (must not be present).\n\t// Users of a DNSBL should periodically check if the DNSBL is still operating\n\t// properly.\n\t// For temporary errors, ErrDNS is returned.\n\n\t// Construct the DNS names for the health checks\n\thealthCheckGood := net.JoinHostPort(\"127.0.0.2\", zone.String())\n\thealthCheckBad := net.JoinHostPort(\"127.0.0.1\", zone.String())\n\n\t// Perform the health checks\n\t_, errGood := resolver.LookupA(ctx, healthCheckGood, true)\n\t_, errBad := resolver.LookupA(ctx, healthCheckBad, true)\n\n\t// Check for errors and return ErrDNS if any\n\tif errGood != nil || errBad == nil {\n\t\treturn ErrDNS\n\t}\n\n\treturn nil\n}\n```\n\nThis function uses a DNS resolver to perform the health checks. If either of the health checks fail, it returns `ErrDNS`.", "```go\nfunc CheckHealth(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain) (rerr error) {\n\tr := &net.Resolver{\n\t\tPreferGo: true,\n\t\tDial:     func(ctx context.Context, network, address string) (net.Conn, error) { return resolver.Dial(ctx, network, address) },\n\t}\n\n\tip := net.ParseIP(\"127.0.0.1\")\n\treverseIP := reverseIP(ip)\n\tname := fmt.Sprintf(\"%s.%s\", reverseIP, zone)\n\n\t_, err := r.LookupHost(ctx, name)\n\tif err == nil {\n\t\telog.Error(\"dnsbl: lookup of 127.0.0.1 succeeded\", slog.String(\"name\", name))\n\t\trerr = ErrDNS\n\t\treturn\n\t}\n\tvar dnsError *net.DNSError\n\tif !errors.As(err, &dnsError) || !dnsError.IsNotFound {\n\t\telog.Error(\"dnsbl: lookup of 127.0.0.1 failed\", slog.String(\"name\", name), mlog.ErrField(err))\n\t\trerr = ErrDNS\n\t\treturn\n\t}\n\n\tip = net.ParseIP(\"127.0.0.2\")\n\treverseIP = reverseIP(ip)\n\tname = fmt.Sprintf(\"%s.%s\", reverseIP, zone)\n\n\taddrs, err := r.LookupHost(ctx, name)\n\tif err != nil {\n\t\tvar dnsError *net.DNSError\n\t\tif !errors.As(err, &dnsError) || !dnsError.IsNotFound {\n\t\t\telog.Error(\"dnsbl: lookup of 127.0.0.2 failed\", slog.String(\"name\", name), mlog.ErrField(err))\n\t\t\trerr = ErrDNS\n\t\t\treturn\n\t\t}\n\t\telog.Error(\"dnsbl: lookup of 127.0.0.2 failed, but that's expected\", slog.String(\"name\", name))\n\t\treturn\n\t}\n\n\tif len(addrs) != 1 {\n\t\telog.Error(\"dnsbl: lookup of 127.0.0.2 returned multiple addresses\", slog.String(\"name\", name), slog.String(\"addrs\", strings.Join(addrs, \", \")))\n\t\trerr = ErrDNS\n\t\treturn\n\t}\n\n\telog.Info(\"dnsbl: healthcheck succeeded\", slog.String(\"name\", name))\n\treturn\n}\n\nfunc reverseIP(ip net.IP) string {\n\tip = ip.To4()\n\tif ip == nil {\n\t\treturn \"\"\n\t}\n\treturn strconv.Itoa(int(ip[3])) + \".\" + strconv.Itoa(int(ip[2])) + \".\" + strconv.Itoa(int(ip[1])) + \".\" + strconv.Itoa(int(ip[0]))\n}\n```\nPlease note that this function uses a custom `net.Resolver` to use the provided DNS resolver. The `reverseIP` function is used to reverse the IP address for the DNSBL lookup.", "```go\nfunc CheckHealth(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, zone dns.Domain) (rerr error) {\n\tctx, cancel := context.WithTimeout(ctx, 10*time.Second)\n\tdefer cancel()\n\n\tips := []net.IP{net.ParseIP(\"127.0.0.1\"), net.ParseIP(\"127.0.0.2\")}\n\tresults := make([]net.IP, 0)\n\n\tfor _, ip := range ips {\n\t\treverseIP := reverseIP(ip)\n\t\tname := reverseIP + \".\" + zone.String()\n\t\trrs, err := resolver.LookupTXT(ctx, name)\n\n\t\tif err != nil {\n\t\t\tmlog.LogDns(ctx, elog, \"CheckHealth: lookup error\",\n\t\t\t\tmlog.Field(\"zone\", zone),\n\t\t\t\tmlog.Field(\"ip\", reverseIP),\n\t\t\t\tmlog.Field(\"err\", err))\n\t\t\tif dns.IsNotFound(err) {\n\t\t\t\trerr = nil\n\t\t\t} else if dns.IsTemporary(err) {\n\t\t\t\trerr = ErrDNS\n\t\t\t} else {\n\t\t\t\trerr = err\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\n\t\tif len(rrs) > 0 {\n\t\t\tresults = append(results, ip)\n\t\t}\n\t}\n\n\tif rerr != nil {\n\t\treturn rerr\n\t}\n\n\tif len(results) != len(ips) {\n\t\treturn fmt.Errorf(\"DNSBL %s health check failed\", zone)\n\t}\n\n\treturn nil\n}\n\nfunc reverseIP(ip net.IP) string {\n\tif ip.To4() != nil {\n\t\t// IPv4\n\t\tsplitAddress := strings.Split(ip.String(), \".\")\n\t\tfor i, j := 0, len(splitAddress)-1; i < j; i, j = i+1, j-1 {\n\t\t\tsplitAddress[i], splitAddress[j] = splitAddress[j], splitAddress[i]\n\t\t}\n\t\treturn strings.Join(splitAddress, \".\")\n\t} else {\n\t\t// IPv6\n\t\texpandedAddress := strings.Replace(ip.String(), \":\", \"\", -1)\n\t\treturn reverseIP(net.ParseIP(expandedAddress))\n\t}\n}\n```"]}, "mox-dmarcrpt/parse.go-ParseMessageReport": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dmarcrpt/parse.go:\n```\n// Package dmarcrpt parses DMARC aggregate feedback reports.\npackage dmarcrpt\n\nimport (\n\t\"archive/zip\"\n\t\"bytes\"\n\t\"compress/gzip\"\n\t\"encoding/xml\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net/http\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/moxio\"\n)\n\nvar ErrNoReport = errors.New(\"no dmarc aggregate report found in message\")\n\n// ParseReport parses an XML aggregate feedback report.\n// The maximum report size is 20MB.\nfunc ParseReport(r io.Reader) (*Feedback, error) {\n\tr = &moxio.LimitReader{R: r, Limit: 20 * 1024 * 1024}\n\tvar feedback Feedback\n\td := xml.NewDecoder(r)\n\tif err := d.Decode(&feedback); err != nil {\n\t\treturn nil, err\n\t}\n\treturn &feedback, nil\n}\n\n// ParseMessageReport parses an aggregate feedback report from a mail message. The\n// maximum message size is 15MB, the maximum report size after decompression is\n// 20MB.\n\n\n\n\n\n\n\n\n\n\n\nfunc parseMessageReport(log mlog.Log, p message.Part) (*Feedback, error) {\n\t// Pretty much any mime structure is allowed. ../rfc/7489:1861\n\t// In practice, some parties will send the report as the only (non-multipart)\n\t// content of the message.\n\n\tif p.MediaType != \"MULTIPART\" {\n\t\treturn parseReport(p)\n\t}\n\n\tfor {\n\t\tsp, err := p.ParseNextPart(log.Logger)\n\t\tif err == io.EOF {\n\t\t\treturn nil, ErrNoReport\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treport, err := parseMessageReport(log, *sp)\n\t\tif err == ErrNoReport {\n\t\t\tcontinue\n\t\t} else if err != nil || report != nil {\n\t\t\treturn report, err\n\t\t}\n\t}\n}\n\nfunc parseReport(p message.Part) (*Feedback, error) {\n\tct := strings.ToLower(p.MediaType + \"/\" + p.MediaSubType)\n\tr := p.Reader()\n\n\t// If no (useful) content-type is set, try to detect it.\n\tif ct == \"\" || ct == \"application/octet-stream\" {\n\t\tdata := make([]byte, 512)\n\t\tn, err := io.ReadFull(r, data)\n\t\tif err == io.EOF {\n\t\t\treturn nil, ErrNoReport\n\t\t} else if err != nil && err != io.ErrUnexpectedEOF {\n\t\t\treturn nil, fmt.Errorf(\"reading application/octet-stream for content-type detection: %v\", err)\n\t\t}\n\t\tdata = data[:n]\n\t\tct = http.DetectContentType(data)\n\t\tr = io.MultiReader(bytes.NewReader(data), r)\n\t}\n\n\tswitch ct {\n\tcase \"application/zip\":\n\t\t// Google sends messages with direct application/zip content-type.\n\t\treturn parseZip(r)\n\tcase \"application/gzip\", \"application/x-gzip\":\n\t\tgzr, err := gzip.NewReader(r)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"decoding gzip xml report: %s\", err)\n\t\t}\n\t\treturn ParseReport(gzr)\n\tcase \"text/xml\", \"application/xml\":\n\t\treturn ParseReport(r)\n\t}\n\treturn nil, ErrNoReport\n}\n\nfunc parseZip(r io.Reader) (*Feedback, error) {\n\tbuf, err := io.ReadAll(r)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading feedback: %s\", err)\n\t}\n\tzr, err := zip.NewReader(bytes.NewReader(buf), int64(len(buf)))\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"parsing zip file: %s\", err)\n\t}\n\tif len(zr.File) != 1 {\n\t\treturn nil, fmt.Errorf(\"zip contains %d files, expected 1\", len(zr.File))\n\t}\n\tf, err := zr.File[0].Open()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"opening file in zip: %s\", err)\n\t}\n\tdefer f.Close()\n\treturn ParseReport(f)\n}\n\n```\n# Function Signature: func ParseMessageReport(elog *slog.Logger, r io.ReaderAt) (*Feedback, error) {\n# Natural Language Annotations: // ParseMessageReport parses an aggregate feedback report from a mail message. The\n// maximum message size is 15MB, the maximum report size after decompression is\n// 20MB.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ParseMessageReport(elog *slog.Logger, r io.ReaderAt) (*Feedback, error) {\n\t// Pretty much any mime structure is allowed. ../rfc/7489:1861\n\t// In practice, some parties will send the report as the only (non-multipart)\n\t// content of the message.\n\n\tp := message.Part{MediaType: \"MULTIPART\", Reader: r}\n\tif p.MediaType != \"MULTIPART\" {\n\t\treturn parseReport(p)\n\t}\n\n\tfor {\n\t\tsp, err := p.ParseNextPart(elog)\n\t\tif err == io.EOF {\n\t\t\treturn nil, ErrNoReport\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treport, err := parseMessageReport(elog, *sp)\n\t\tif err == ErrNoReport {\n\t\t\tcontinue\n\t\t} else if err != nil || report != nil {\n\t\t\treturn report, err\n\t\t}\n\t}\n}\n```", "```go\nfunc ParseMessageReport(elog *slog.Logger, r io.ReaderAt) (*Feedback, error) {\n\tvar log mlog.Log\n\tlog.Logger = elog\n\n\tp := message.Part{\n\t\tMediaType: \"MULTIPART\",\n\t\tReader: func() io.Reader {\n\t\t\treturn io.NewSectionReader(r, 0, 15*1024*1024) // Max message size is 15MB\n\t\t},\n\t}\n\n\treturn parseMessageReport(log, p)\n}\n```", "```go\nfunc ParseMessageReport(elog *slog.Logger, r io.ReaderAt) (*Feedback, error) {\n\telog = &moxio.LimitReader{R: elog, Limit: 15 * 1024 * 1024}\n\tvar feedback Feedback\n\td := xml.NewDecoder(r)\n\tif err := d.Decode(&feedback); err != nil {\n\t\treturn nil, err\n\t}\n\treturn &feedback, nil\n}\n```"]}, "mox-mox-/lifecycle.go-Register": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/mox-/lifecycle.go:\n```\npackage mox\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"net\"\n\t\"os\"\n\t\"runtime\"\n\t\"runtime/debug\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n)\n\n// We start up as root, bind to sockets, open private key/cert files and fork and\n// exec as unprivileged user. During startup as root, we gather the fd's for the\n// listen addresses in passedListeners and files in passedFiles, and pass their\n// addresses and paths in environment variables to the new process.\nvar passedListeners = map[string]*os.File{} // Listen address to file descriptor.\nvar passedFiles = map[string][]*os.File{}   // Path to file descriptors.\n\n// RestorePassedFiles reads addresses from $MOX_SOCKETS and paths from $MOX_FILES\n// and prepares an os.File for each file descriptor, which are used by later calls\n// of Listen or opening files.\nfunc RestorePassedFiles() {\n\ts := os.Getenv(\"MOX_SOCKETS\")\n\tif s == \"\" {\n\t\tvar linuxhint string\n\t\tif runtime.GOOS == \"linux\" {\n\t\t\tlinuxhint = \" If you updated from v0.0.1, update the mox.service file to start as root (privileges are dropped): ./mox config printservice >mox.service && sudo systemctl daemon-reload && sudo systemctl restart mox.\"\n\t\t}\n\t\tpkglog.Fatal(\"mox must be started as root, and will drop privileges after binding required sockets (missing environment variable MOX_SOCKETS).\" + linuxhint)\n\t}\n\n\t// 0,1,2 are stdin,stdout,stderr, 3 is the first passed fd (first listeners, then files).\n\tvar o uintptr = 3\n\tfor _, addr := range strings.Split(s, \",\") {\n\t\tpassedListeners[addr] = os.NewFile(o, addr)\n\t\to++\n\t}\n\n\tfiles := os.Getenv(\"MOX_FILES\")\n\tif files == \"\" {\n\t\treturn\n\t}\n\tfor _, path := range strings.Split(files, \",\") {\n\t\tpassedFiles[path] = append(passedFiles[path], os.NewFile(o, path))\n\t\to++\n\t}\n}\n\n// CleanupPassedFiles closes the listening socket file descriptors and files passed\n// in by the parent process. To be called by the unprivileged child after listeners\n// have been recreated (they dup the file descriptor), and by the privileged\n// process after starting its child.\nfunc CleanupPassedFiles() {\n\tfor _, f := range passedListeners {\n\t\terr := f.Close()\n\t\tpkglog.Check(err, \"closing listener socket file descriptor\")\n\t}\n\tfor _, fl := range passedFiles {\n\t\tfor _, f := range fl {\n\t\t\terr := f.Close()\n\t\t\tpkglog.Check(err, \"closing path file descriptor\")\n\t\t}\n\t}\n}\n\n// For privileged file descriptor operations (listen and opening privileged files),\n// perform them immediately, regardless of running as root or other user, in case\n// ForkExecUnprivileged is not used.\nvar FilesImmediate bool\n\n// Listen returns a newly created network listener when starting as root, and\n// otherwise (not root) returns a network listener from a file descriptor that was\n// passed by the parent root process.\nfunc Listen(network, addr string) (net.Listener, error) {\n\tif os.Getuid() != 0 && !FilesImmediate {\n\t\tf, ok := passedListeners[addr]\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"no file descriptor for listener %s\", addr)\n\t\t}\n\t\tln, err := net.FileListener(f)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"making network listener from file descriptor for address %s: %v\", addr, err)\n\t\t}\n\t\treturn ln, nil\n\t}\n\n\tif _, ok := passedListeners[addr]; ok {\n\t\treturn nil, fmt.Errorf(\"duplicate listener: %s\", addr)\n\t}\n\n\tln, err := net.Listen(network, addr)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// On windows, we cannot duplicate a socket. We don't need to for mox localserve\n\t// with FilesImmediate.\n\tif !FilesImmediate {\n\t\ttcpln, ok := ln.(*net.TCPListener)\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"listener not a tcp listener, but %T, for network %s, address %s\", ln, network, addr)\n\t\t}\n\t\tf, err := tcpln.File()\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"dup listener: %v\", err)\n\t\t}\n\t\tpassedListeners[addr] = f\n\t}\n\treturn ln, err\n}\n\n// Open a privileged file, such as a TLS private key. When running as root\n// (during startup), the file is opened and the file descriptor is stored.\n// These file descriptors are passed to the unprivileged process. When in the\n// unprivileged processed, we lookup a passed file descriptor.\n// The same calls should be made in the privileged and unprivileged process.\nfunc OpenPrivileged(path string) (*os.File, error) {\n\tif os.Getuid() != 0 && !FilesImmediate {\n\t\tfl := passedFiles[path]\n\t\tif len(fl) == 0 {\n\t\t\treturn nil, fmt.Errorf(\"no file descriptor for file %s\", path)\n\t\t}\n\t\tf := fl[0]\n\t\tpassedFiles[path] = fl[1:]\n\t\treturn f, nil\n\t}\n\n\tf, err := os.Open(path)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tpassedFiles[path] = append(passedFiles[path], f)\n\n\t// Open again, the caller will be closing this file.\n\treturn os.Open(path)\n}\n\n// Shutdown is canceled when a graceful shutdown is initiated. SMTP, IMAP, periodic\n// processes should check this before starting a new operation. If this context is\n// canaceled, the operation should not be started, and new connections/commands should\n// receive a message that the service is currently not available.\nvar Shutdown context.Context\nvar ShutdownCancel func()\n\n// This context should be used as parent by most operations. It is canceled 1\n// second after graceful shutdown was initiated with the cancelation of the\n// Shutdown context. This should abort active operations.\n//\n// Operations typically have context timeouts, 30s for single i/o like DNS queries,\n// and 1 minute for operations with more back and forth. These are set through a\n// context.WithTimeout based on this context, so those contexts are still canceled\n// when shutting down.\n//\n// HTTP servers don't get graceful shutdown, their connections are just aborted.\n// todo: should shut down http connections as well, and shut down the listener and/or return 503 for new requests.\nvar Context context.Context\nvar ContextCancel func()\n\n// Connections holds all active protocol sockets (smtp, imap). They will be given\n// an immediate read/write deadline shortly after initiating mox shutdown, after\n// which the connections get 1 more second for error handling before actual\n// shutdown.\nvar Connections = &connections{\n\tconns:  map[net.Conn]connKind{},\n\tgauges: map[connKind]prometheus.GaugeFunc{},\n\tactive: map[connKind]int64{},\n}\n\ntype connKind struct {\n\tprotocol string\n\tlistener string\n}\n\ntype connections struct {\n\tsync.Mutex\n\tconns  map[net.Conn]connKind\n\tdones  []chan struct{}\n\tgauges map[connKind]prometheus.GaugeFunc\n\n\tactiveMutex sync.Mutex\n\tactive      map[connKind]int64\n}\n\n// Register adds a connection for receiving an immediate i/o deadline on shutdown.\n// When the connection is closed, Remove must be called to cancel the registration.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Unregister removes a connection for shutdown.\nfunc (c *connections) Unregister(nc net.Conn) {\n\tc.Lock()\n\tdefer c.Unlock()\n\tck := c.conns[nc]\n\n\tdefer func() {\n\t\tc.activeMutex.Lock()\n\t\tc.active[ck]--\n\t\tc.activeMutex.Unlock()\n\t}()\n\n\tdelete(c.conns, nc)\n\tif len(c.conns) > 0 {\n\t\treturn\n\t}\n\tfor _, done := range c.dones {\n\t\tdone <- struct{}{}\n\t}\n\tc.dones = nil\n}\n\n// Shutdown sets an immediate i/o deadline on all open registered sockets. Called\n// some time after mox shutdown is initiated.\n// The deadline will cause i/o's to be aborted, which should result in the\n// connection being unregistered.\nfunc (c *connections) Shutdown() {\n\tnow := time.Now()\n\tc.Lock()\n\tdefer c.Unlock()\n\tfor nc := range c.conns {\n\t\tif err := nc.SetDeadline(now); err != nil {\n\t\t\tpkglog.Errorx(\"setting immediate read/write deadline for shutdown\", err)\n\t\t}\n\t}\n}\n\n// Done returns a new channel on which a value is sent when no more sockets are\n// open, which could be immediate.\nfunc (c *connections) Done() chan struct{} {\n\tc.Lock()\n\tdefer c.Unlock()\n\tdone := make(chan struct{}, 1)\n\tif len(c.conns) == 0 {\n\t\tdone <- struct{}{}\n\t\treturn done\n\t}\n\tc.dones = append(c.dones, done)\n\treturn done\n}\n\n```\n# Function Signature: func (c *connections) Register(nc net.Conn, protocol, listener string) {\n# Natural Language Annotations: // Register adds a connection for receiving an immediate i/o deadline on shutdown.\n// When the connection is closed, Remove must be called to cancel the registration.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (c *connections) Register(nc net.Conn, protocol, listener string) {\n\tc.Lock()\n\tdefer c.Unlock()\n\n\tck := connKind{protocol, listener}\n\tc.conns[nc] = ck\n\n\tif _, ok := c.gauges[ck]; !ok {\n\t\tc.gauges[ck] = promauto.NewGaugeFunc(prometheus.GaugeOpts{\n\t\t\tNamespace: \"mox\",\n\t\t\tSubsystem: \"connections\",\n\t\t\tName:      ck.protocol + \"_\" + ck.listener,\n\t\t\tHelp:      \"Number of active connections for \" + ck.protocol + \" on \" + ck.listener,\n\t\t}, func() float64 {\n\t\t\tc.activeMutex.Lock()\n\t\t\tdefer c.activeMutex.Unlock()\n\t\t\treturn float64(c.active[ck])\n\t\t})\n\t}\n\n\tc.activeMutex.Lock()\n\tc.active[ck]++\n\tc.activeMutex.Unlock()\n\n\tgo func() {\n\t\t<-Context.Done()\n\t\tc.Unregister(nc)\n\t}()\n}\n```\nHere, the function `Register` registers a new connection `nc` for receiving an immediate I/O deadline on shutdown. When the connection is closed, the connection should be unregistered using the `Unregister` method. This function also maintains a count of active connections for each unique combination of protocol and listener, and updates a Prometheus gauge with this count.", "```go\nfunc (c *connections) Register(nc net.Conn, protocol, listener string) {\n\tc.Lock()\n\tdefer c.Unlock()\n\n\tck := connKind{protocol, listener}\n\tc.conns[nc] = ck\n\n\tc.activeMutex.Lock()\n\tdefer c.activeMutex.Unlock()\n\tif _, ok := c.active[ck]; !ok {\n\t\tc.active[ck] = 0\n\t\tc.gauges[ck] = promauto.NewGaugeFunc(prometheus.GaugeOpts{\n\t\t\tName:        \"mox_connections_active\",\n\t\t\tHelp:        \"Number of active connections\",\n\t\t\tConstLabels: prometheus.Labels{\"protocol\": protocol, \"listener\": listener},\n\t\t}, func() float64 {\n\t\t\tc.activeMutex.Lock()\n\t\t\tdefer c.activeMutex.Unlock()\n\t\t\treturn float64(c.active[ck])\n\t\t})\n\t}\n\n\tc.active[ck]++\n}\n```\nThis code defines a method `Register` on the `connections` struct which adds a new connection to the map of active connections. It also creates a new gauge for the connection kind, if it doesn't already exist. The gauge is a function that returns the number of active connections for a given protocol and listener. The connection is added to the map of connections and the count of active connections for that kind is incremented. The locking ensures that the map of connections and the gauge are accessed and modified atomically.", "```go\nfunc (c *connections) Register(nc net.Conn, protocol, listener string) {\n\tc.Lock()\n\tdefer c.Unlock()\n\n\tck := connKind{protocol: protocol, listener: listener}\n\tc.conns[nc] = ck\n\n\tc.activeMutex.Lock()\n\tc.active[ck]++\n\tc.activeMutex.Unlock()\n\n\tnc.SetDeadline(time.Now().Add(time.Second * 60))\n\n\tnc.Close()\n\tc.Unregister(nc)\n}\n```\nThis code registers a new connection `nc` for receiving an immediate I/O deadline on shutdown. The connection's protocol and listener are added to the `connKind` struct and stored in the `connections` map. The connection gets an immediate read/write deadline of 60 seconds. If the connection is closed, `Unregister` must be called to cancel the registration."]}, "mox-smtpclient/dial.go-Dial": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/smtpclient/dial.go:\n```\npackage smtpclient\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"time\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// DialHook can be used during tests to override the regular dialer from being used.\nvar DialHook func(ctx context.Context, dialer Dialer, timeout time.Duration, addr string, laddr net.Addr) (net.Conn, error)\n\nfunc dial(ctx context.Context, dialer Dialer, timeout time.Duration, addr string, laddr net.Addr) (net.Conn, error) {\n\t// todo: see if we can remove this function and DialHook in favor of the Dialer interface.\n\n\tif DialHook != nil {\n\t\treturn DialHook(ctx, dialer, timeout, addr, laddr)\n\t}\n\n\t// If this is a net.Dialer, use its settings and add the timeout and localaddr.\n\t// This is the typical case, but SOCKS5 support can use a different dialer.\n\tif d, ok := dialer.(*net.Dialer); ok {\n\t\tnd := *d\n\t\tnd.Timeout = timeout\n\t\tnd.LocalAddr = laddr\n\t\treturn nd.DialContext(ctx, \"tcp\", addr)\n\t}\n\treturn dialer.DialContext(ctx, \"tcp\", addr)\n}\n\n// Dialer is used to dial mail servers, an interface to facilitate testing.\ntype Dialer interface {\n\tDialContext(ctx context.Context, network, addr string) (c net.Conn, err error)\n}\n\n// Dial connects to host by dialing ips, taking previous attempts in dialedIPs into\n// accounts (for greylisting, blocklisting and ipv4/ipv6).\n//\n// If the previous attempt used IPv4, this attempt will use IPv6 (useful in case\n// one of the IPs is in a DNSBL).\n//\n// The second attempt for an address family we prefer the same IP as earlier, to\n// increase our chances if remote is doing greylisting.\n//\n// Dial updates dialedIPs, callers may want to save it so it can be taken into\n// account for future delivery attempts.\n//\n// The first matching protocol family from localIPs is set for the local side\n// of the TCP connection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func Dial(ctx context.Context, elog *slog.Logger, dialer Dialer, host dns.IPDomain, ips []net.IP, port int, dialedIPs map[string][]net.IP, localIPs []net.IP) (conn net.Conn, ip net.IP, rerr error) {\n# Natural Language Annotations: // Dial connects to host by dialing ips, taking previous attempts in dialedIPs into\n// accounts (for greylisting, blocklisting and ipv4/ipv6).\n//\n// If the previous attempt used IPv4, this attempt will use IPv6 (useful in case\n// one of the IPs is in a DNSBL).\n//\n// The second attempt for an address family we prefer the same IP as earlier, to\n// increase our chances if remote is doing greylisting.\n//\n// Dial updates dialedIPs, callers may want to save it so it can be taken into\n// account for future delivery attempts.\n//\n// The first matching protocol family from localIPs is set for the local side\n// of the TCP connection.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc Dial(ctx context.Context, elog *slog.Logger, dialer Dialer, host dns.IPDomain, ips []net.IP, port int, dialedIPs map[string][]net.IP, localIPs []net.IP) (conn net.Conn, ip net.IP, rerr error) {\n\t// Dial connects to host by dialing ips, taking previous attempts in dialedIPs into\n\t// accounts (for greylisting, blocklisting and ipv4/ipv6).\n\t//\n\t// If the previous attempt used IPv4, this attempt will use IPv6 (useful in case\n\t// one of the IPs is in a DNSBL).\n\t//\n\t// The second attempt for an address family we prefer the same IP as earlier, to\n\t// increase our chances if remote is doing greylisting.\n\t//\n\t// Dial updates dialedIPs, callers may want to save it so it can be taken into\n\t// account for future delivery attempts.\n\t//\n\t// The first matching protocol family from localIPs is set for the local side\n\t// of the TCP connection.\n\n\tvar err error\n\tvar lastIPFamily string\n\tif dialedIPs[host.Domain] != nil {\n\t\tlastIP := dialedIPs[host.Domain][len(dialedIPs[host.Domain])-1]\n\t\tif lastIP.To4() != nil {\n\t\t\tlastIPFamily = \"IPv4\"\n\t\t} else {\n\t\t\tlastIPFamily = \"IPv6\"\n\t\t}\n\t}\n\n\tfor _, ip := range ips {\n\t\t// If the previous attempt used IPv4, this attempt will use IPv6 (useful in case\n\t\t// one of the IPs is in a DNSBL).\n\t\tif lastIPFamily == \"IPv4\" && ip.To4() == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tipStr := fmt.Sprintf(\"%s:%d\", ip.String(), port)\n\t\tconn, err = dial(ctx, dialer, 10*time.Second, ipStr, nil)\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\n\t\t// If this is a net.Dialer, use its settings and add the timeout and localaddr.\n\t\t// This is the typical case, but SOCKS5 support can use a different dialer.\n\t\tif d, ok := dialer.(*net.Dialer); ok {\n\t\t\tnd := *d\n\t\t\tnd.LocalAddr = nil\n\t\t\tfor _, localIP := range localIPs {\n\t\t\t\t// The first matching protocol family from localIPs is set for the local side\n\t\t\t\t// of the TCP connection.\n\t\t\t\tif (localIP.To4() != nil && ip.To4() != nil) || (localIP.To4() == nil && ip.To4() == nil) {\n\t\t\t\t\tnd.LocalAddr = localIP\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tconn, err = dialer.DialContext(ctx, \"tcp\", ipStr)\n\t\tif err != nil {\n\t\t\tcontinue\n\t\t}\n\n\t\tdialedIPs[host.Domain] = append(dialedIPs[host.Domain], ip)\n\t\treturn conn, ip, nil\n\t}\n\n\treturn nil, nil, fmt.Errorf(\"failed to dial %s\", host.Domain)\n}\n```", "```go\nfunc Dial(ctx context.Context, elog *slog.Logger, dialer Dialer, host dns.IPDomain, ips []net.IP, port int, dialedIPs map[string][]net.IP, localIPs []net.IP) (conn net.Conn, ip net.IP, rerr error) {\n\tusedIPv4 := false\n\tusedIPv6 := false\n\tlastIP := \"\"\n\n\tfor _, ip := range ips {\n\t\tif usedIPv4 && ip.To4() != nil {\n\t\t\tcontinue\n\t\t} else if usedIPv6 && ip.To4() == nil {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Dialer will set the timeout and local address\n\t\tconn, err := dial(ctx, dialer, 30*time.Second, fmt.Sprintf(\"%s:%d\", ip.String(), port), nil)\n\t\tif err != nil {\n\t\t\tmlog.Log(elog, mlog.LevelInfo, \"dial\", map[string]any{\"host\": host, \"ip\": ip, \"err\": err})\n\t\t\tcontinue\n\t\t}\n\n\t\tusedIPv4 = usedIPv4 || ip.To4() != nil\n\t\tusedIPv6 = usedIPv6 || ip.To4() == nil\n\t\tlastIP = ip.String()\n\t\tdialedIPs[host.Domain] = append(dialedIPs[host.Domain], ip)\n\t\treturn conn, ip, nil\n\t}\n\n\tif lastIP != \"\" && len(ips) > 1 {\n\t\tfor _, ip := range ips {\n\t\t\tif ip.String() != lastIP && (usedIPv4 == (ip.To4() != nil) || usedIPv6 == (ip.To4() == nil)) {\n\t\t\t\t// Dialer will set the timeout and local address\n\t\t\t\tconn, err := dial(ctx, dialer, 30*time.Second, fmt.Sprintf(\"%s:%d\", ip.String(), port), nil)\n\t\t\t\tif err != nil {\n\t\t\t\t\tmlog.Log(elog, mlog.LevelInfo, \"dial\", map[string]any{\"host\": host, \"ip\": ip, \"err\": err})\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tdialedIPs[host.Domain] = append(dialedIPs[host.Domain], ip)\n\t\t\t\treturn conn, ip, nil\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil, nil, fmt.Errorf(\"failed to dial host %s\", host)\n}\n```", "```go\nfunc Dial(ctx context.Context, elog *slog.Logger, dialer Dialer, host dns.IPDomain, ips []net.IP, port int, dialedIPs map[string][]net.IP, localIPs []net.IP) (conn net.Conn, ip net.IP, rerr error) {\n\tvar err error\n\tipv4 := false\n\n\tif dialedIPs[host.Domain] != nil && len(dialedIPs[host.Domain]) > 0 {\n\t\tipv4 = dialedIPs[host.Domain][0].To4() != nil\n\t}\n\n\tfor _, ip = range ips {\n\t\tif ipv4 == (ip.To4() != nil) {\n\t\t\taddr := fmt.Sprintf(\"%s:%d\", ip, port)\n\t\t\tconn, err = dial(ctx, dialer, 10*time.Second, addr, nil)\n\t\t\tif err == nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tmlog.LogNewMail(ctx, elog).With(\"err\", err).Info(\"dial attempt failed\")\n\t\t}\n\t}\n\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tdialedIPs[host.Domain] = append(dialedIPs[host.Domain], ip)\n\n\treturn conn, ip, nil\n}\n```\nThe function `Dial` is used to dial a mail server. It attempts to connect to a host by dialing IP addresses. It takes into account previous attempts in `dialedIPs` for greylisting, blocklisting, and IP version selection. It also updates `dialedIPs` to save the information for future delivery attempts. The first matching protocol family from `localIPs` is set for the local side of the TCP connection."]}, "mox-smtpclient/gather.go-GatherIPs": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/smtpclient/gather.go:\n```\npackage smtpclient\n\nimport (\n\t\"context\"\n\t\"crypto/sha256\"\n\t\"crypto/sha512\"\n\t\"crypto/x509\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/mjl-/adns\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n)\n\nvar (\n\terrCNAMELoop  = errors.New(\"cname loop\")\n\terrCNAMELimit = errors.New(\"too many cname records\")\n\terrDNS        = errors.New(\"dns lookup error\")\n\terrNoMail     = errors.New(\"domain does not accept email as indicated with single dot for mx record\")\n)\n\n// GatherDestinations looks up the hosts to deliver email to a domain (\"next-hop\").\n// If it is an IP address, it is the only destination to try. Otherwise CNAMEs of\n// the domain are followed. Then MX records for the expanded CNAME are looked up.\n// If no MX record is present, the original domain is returned. If an MX record is\n// present but indicates the domain does not accept email, ErrNoMail is returned.\n// If valid MX records were found, the MX target hosts are returned.\n//\n// haveMX indicates if an MX record was found.\n//\n// origNextHopAuthentic indicates if the DNS record for the initial domain name was\n// DNSSEC secure (CNAME, MX).\n//\n// expandedNextHopAuthentic indicates if the DNS records after following CNAMEs were\n// DNSSEC secure.\n//\n// These authentic results are needed for DANE, to determine where to look up TLSA\n// records, and which names to allow in the remote TLS certificate. If MX records\n// were found, both the original and expanded next-hops must be authentic for DANE\n// to be option. For a non-IP with no MX records found, the authentic result can\n// be used to decide which of the names to use as TLSA base domain.\nfunc GatherDestinations(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, origNextHop dns.IPDomain) (haveMX, origNextHopAuthentic, expandedNextHopAuthentic bool, expandedNextHop dns.Domain, hosts []dns.IPDomain, permanent bool, err error) {\n\t// ../rfc/5321:3824\n\n\tlog := mlog.New(\"smtpclient\", elog)\n\n\t// IP addresses are dialed directly, and don't have TLSA records.\n\tif len(origNextHop.IP) > 0 {\n\t\treturn false, false, false, expandedNextHop, []dns.IPDomain{origNextHop}, false, nil\n\t}\n\n\t// We start out assuming the result is authentic. Updated with each lookup.\n\torigNextHopAuthentic = true\n\texpandedNextHopAuthentic = true\n\n\t// We start out delivering to the recipient domain. We follow CNAMEs.\n\trcptDomain := origNextHop.Domain\n\t// Domain we are actually delivering to, after following CNAME record(s).\n\texpandedNextHop = rcptDomain\n\t// Keep track of CNAMEs we have followed, to detect loops.\n\tdomainsSeen := map[string]bool{}\n\tfor i := 0; ; i++ {\n\t\tif domainsSeen[expandedNextHop.ASCII] {\n\t\t\t// todo: only mark as permanent failure if TTLs for all records are beyond latest possibly delivery retry we would do.\n\t\t\terr := fmt.Errorf(\"%w: recipient domain %s: already saw %s\", errCNAMELoop, rcptDomain, expandedNextHop)\n\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, false, err\n\t\t}\n\t\tdomainsSeen[expandedNextHop.ASCII] = true\n\n\t\t// note: The Go resolver returns the requested name if the domain has no CNAME\n\t\t// record but has a host record.\n\t\tif i == 16 {\n\t\t\t// We have a maximum number of CNAME records we follow. There is no hard limit for\n\t\t\t// DNS, and you might think folks wouldn't configure CNAME chains at all, but for\n\t\t\t// (non-mail) domains, CNAME chains of 10 records have been encountered according\n\t\t\t// to the internet.\n\t\t\t// todo: only mark as permanent failure if TTLs for all records are beyond latest possibly delivery retry we would do.\n\t\t\terr := fmt.Errorf(\"%w: recipient domain %s, last resolved domain %s\", errCNAMELimit, rcptDomain, expandedNextHop)\n\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, false, err\n\t\t}\n\n\t\t// Do explicit CNAME lookup. Go's LookupMX also resolves CNAMEs, but we want to\n\t\t// know the final name, and we're interested in learning if the first vs later\n\t\t// results were DNSSEC-(in)secure.\n\t\t// ../rfc/5321:3838 ../rfc/3974:197\n\t\tcctx, ccancel := context.WithTimeout(ctx, 30*time.Second)\n\t\tdefer ccancel()\n\t\tcname, cnameResult, err := resolver.LookupCNAME(cctx, expandedNextHop.ASCII+\".\")\n\t\tccancel()\n\t\tif i == 0 {\n\t\t\torigNextHopAuthentic = origNextHopAuthentic && cnameResult.Authentic\n\t\t}\n\t\texpandedNextHopAuthentic = expandedNextHopAuthentic && cnameResult.Authentic\n\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\terr = fmt.Errorf(\"%w: cname lookup for %s: %v\", errDNS, expandedNextHop, err)\n\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, false, err\n\t\t}\n\t\tif err == nil && cname != expandedNextHop.ASCII+\".\" {\n\t\t\td, err := dns.ParseDomain(strings.TrimSuffix(cname, \".\"))\n\t\t\tif err != nil {\n\t\t\t\t// todo: only mark as permanent failure if TTLs for all records are beyond latest possibly delivery retry we would do.\n\t\t\t\terr = fmt.Errorf(\"%w: parsing cname domain %s: %v\", errDNS, expandedNextHop, err)\n\t\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, false, err\n\t\t\t}\n\t\t\texpandedNextHop = d\n\t\t\t// Start again with new domain.\n\t\t\tcontinue\n\t\t}\n\n\t\t// Not a CNAME, so lookup MX record.\n\t\tmctx, mcancel := context.WithTimeout(ctx, 30*time.Second)\n\t\tdefer mcancel()\n\t\t// Note: LookupMX can return an error and still return records: Invalid records are\n\t\t// filtered out and an error returned. We must process any records that are valid.\n\t\t// Only if all are unusable will we return an error. ../rfc/5321:3851\n\t\tmxl, mxResult, err := resolver.LookupMX(mctx, expandedNextHop.ASCII+\".\")\n\t\tmcancel()\n\t\tif i == 0 {\n\t\t\torigNextHopAuthentic = origNextHopAuthentic && mxResult.Authentic\n\t\t}\n\t\texpandedNextHopAuthentic = expandedNextHopAuthentic && mxResult.Authentic\n\t\tif err != nil && len(mxl) == 0 {\n\t\t\tif !dns.IsNotFound(err) {\n\t\t\t\terr = fmt.Errorf(\"%w: mx lookup for %s: %v\", errDNS, expandedNextHop, err)\n\t\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, false, err\n\t\t\t}\n\n\t\t\t// No MX record, attempt delivery directly to host. ../rfc/5321:3842\n\t\t\thosts = []dns.IPDomain{{Domain: expandedNextHop}}\n\t\t\treturn false, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, hosts, false, nil\n\t\t} else if err != nil {\n\t\t\tlog.Infox(\"mx record has some invalid records, keeping only the valid mx records\", err)\n\t\t}\n\n\t\t// ../rfc/7505:122\n\t\tif err == nil && len(mxl) == 1 && mxl[0].Host == \".\" {\n\t\t\t// Note: Depending on MX record TTL, this record may be replaced with a more\n\t\t\t// receptive MX record before our final delivery attempt. But it's clearly the\n\t\t\t// explicit desire not to be bothered with email delivery attempts, so mark failure\n\t\t\t// as permanent.\n\t\t\treturn true, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, true, errNoMail\n\t\t}\n\n\t\t// The Go resolver already sorts by preference, randomizing records of same\n\t\t// preference. ../rfc/5321:3885\n\t\tfor _, mx := range mxl {\n\t\t\t// Parsing lax (unless pedantic mode) for MX targets with underscores as seen in the wild.\n\t\t\thost, err := dns.ParseDomainLax(strings.TrimSuffix(mx.Host, \".\"))\n\t\t\tif err != nil {\n\t\t\t\t// note: should not happen because Go resolver already filters these out.\n\t\t\t\terr = fmt.Errorf(\"%w: invalid host name in mx record %q: %v\", errDNS, mx.Host, err)\n\t\t\t\treturn true, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, nil, true, err\n\t\t\t}\n\t\t\thosts = append(hosts, dns.IPDomain{Domain: host})\n\t\t}\n\t\tif len(hosts) > 0 {\n\t\t\terr = nil\n\t\t}\n\t\treturn true, origNextHopAuthentic, expandedNextHopAuthentic, expandedNextHop, hosts, false, err\n\t}\n}\n\n// GatherIPs looks up the IPs to try for connecting to host, with the IPs ordered\n// to take previous attempts into account. For use with DANE, the CNAME-expanded\n// name is returned, and whether the DNS responses were authentic.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// GatherTLSA looks up TLSA record for either expandedHost or host, and returns\n// records usable for DANE with SMTP, and host names to allow in DANE-TA\n// certificate name verification.\n//\n// If no records are found, this isn't necessarily an error. It can just indicate\n// the domain/host does not opt-in to DANE, and nil records and a nil error are\n// returned.\n//\n// Only usable records are returned. If any record was found, DANE is required and\n// this is indicated with daneRequired. If no usable records remain, the caller\n// must do TLS, but not verify the remote TLS certificate.\n//\n// Returned values are always meaningful, also when an error was returned.\nfunc GatherTLSA(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, host dns.Domain, expandedAuthentic bool, expandedHost dns.Domain) (daneRequired bool, daneRecords []adns.TLSA, tlsaBaseDomain dns.Domain, err error) {\n\tlog := mlog.New(\"smtpclient\", elog)\n\n\t// ../rfc/7672:912\n\t// This function is only called when the lookup of host was authentic.\n\n\tvar l []adns.TLSA\n\n\ttlsaBaseDomain = host\n\tif host == expandedHost || !expandedAuthentic {\n\t\tl, err = lookupTLSACNAME(ctx, log, resolver, 25, \"tcp\", host)\n\t} else if expandedAuthentic {\n\t\t// ../rfc/7672:934\n\t\ttlsaBaseDomain = expandedHost\n\t\tl, err = lookupTLSACNAME(ctx, log, resolver, 25, \"tcp\", expandedHost)\n\t\tif err == nil && len(l) == 0 {\n\t\t\ttlsaBaseDomain = host\n\t\t\tl, err = lookupTLSACNAME(ctx, log, resolver, 25, \"tcp\", host)\n\t\t}\n\t}\n\tif len(l) == 0 || err != nil {\n\t\tdaneRequired = err != nil\n\t\tlog.Debugx(\"gathering tlsa records failed\", err, slog.Bool(\"danerequired\", daneRequired), slog.Any(\"basedomain\", tlsaBaseDomain))\n\t\treturn daneRequired, nil, tlsaBaseDomain, err\n\t}\n\tdaneRequired = len(l) > 0\n\tl = filterUsableTLSARecords(log, l)\n\tlog.Debug(\"tlsa records exist\",\n\t\tslog.Bool(\"danerequired\", daneRequired),\n\t\tslog.Any(\"records\", l),\n\t\tslog.Any(\"basedomain\", tlsaBaseDomain))\n\treturn daneRequired, l, tlsaBaseDomain, err\n}\n\n// lookupTLSACNAME composes a TLSA domain name to lookup, follows CNAMEs and looks\n// up TLSA records. no TLSA records exist, a nil error is returned as it means\n// the host does not opt-in to DANE.\nfunc lookupTLSACNAME(ctx context.Context, log mlog.Log, resolver dns.Resolver, port int, protocol string, host dns.Domain) (l []adns.TLSA, rerr error) {\n\tname := fmt.Sprintf(\"_%d._%s.%s\", port, protocol, host.ASCII+\".\")\n\tfor i := 0; ; i++ {\n\t\tcname, result, err := resolver.LookupCNAME(ctx, name)\n\t\tif dns.IsNotFound(err) {\n\t\t\tif !result.Authentic {\n\t\t\t\tlog.Debugx(\"cname nxdomain result during tlsa lookup not authentic, not doing dane for host\", err, slog.Any(\"host\", host), slog.String(\"name\", name))\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t\tbreak\n\t\t} else if err != nil {\n\t\t\treturn nil, fmt.Errorf(\"looking up cname for tlsa candidate base domain: %w\", err)\n\t\t} else if !result.Authentic {\n\t\t\tlog.Debugx(\"cname result during tlsa lookup not authentic, not doing dane for host\", err, slog.Any(\"host\", host), slog.String(\"name\", name))\n\t\t\treturn nil, nil\n\t\t}\n\t\tif i == 10 {\n\t\t\treturn nil, fmt.Errorf(\"looking up cname for tlsa candidate base domain: %w\", errCNAMELimit)\n\t\t}\n\t\tname = strings.TrimSuffix(cname, \".\") + \".\"\n\t}\n\tvar result adns.Result\n\tvar err error\n\tl, result, err = resolver.LookupTLSA(ctx, 0, \"\", name)\n\tif dns.IsNotFound(err) || err == nil && len(l) == 0 {\n\t\tlog.Debugx(\"no tlsa records for host, not doing dane\", err,\n\t\t\tslog.Any(\"host\", host),\n\t\t\tslog.String(\"name\", name),\n\t\t\tslog.Bool(\"authentic\", result.Authentic))\n\t\treturn nil, nil\n\t} else if err != nil {\n\t\treturn nil, fmt.Errorf(\"looking up tlsa records for tlsa candidate base domain: %w\", err)\n\t} else if !result.Authentic {\n\t\tlog.Debugx(\"tlsa lookup not authentic, not doing dane for host\", err, slog.Any(\"host\", host), slog.String(\"name\", name))\n\t\treturn nil, nil\n\t}\n\treturn l, nil\n}\n\nfunc filterUsableTLSARecords(log mlog.Log, l []adns.TLSA) []adns.TLSA {\n\t// Gather \"usable\" records. ../rfc/7672:708\n\to := 0\n\tfor _, r := range l {\n\t\t// A record is not usable when we don't recognize parameters. ../rfc/6698:649\n\n\t\tswitch r.Usage {\n\t\tcase adns.TLSAUsageDANETA, adns.TLSAUsageDANEEE:\n\t\tdefault:\n\t\t\t// We can regard PKIX-TA and PKIX-EE as \"unusable\" with SMTP DANE. ../rfc/7672:1304\n\t\t\tcontinue\n\t\t}\n\t\tswitch r.Selector {\n\t\tcase adns.TLSASelectorCert, adns.TLSASelectorSPKI:\n\t\tdefault:\n\t\t\tcontinue\n\t\t}\n\t\tswitch r.MatchType {\n\t\tcase adns.TLSAMatchTypeFull:\n\t\t\tif r.Selector == adns.TLSASelectorCert {\n\t\t\t\tif _, err := x509.ParseCertificate(r.CertAssoc); err != nil {\n\t\t\t\t\tlog.Debugx(\"parsing certificate in dane tlsa record, ignoring\", err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t} else if r.Selector == adns.TLSASelectorSPKI {\n\t\t\t\tif _, err := x509.ParsePKIXPublicKey(r.CertAssoc); err != nil {\n\t\t\t\t\tlog.Debugx(\"parsing certificate in dane tlsa record, ignoring\", err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\tcase adns.TLSAMatchTypeSHA256:\n\t\t\tif len(r.CertAssoc) != sha256.Size {\n\t\t\t\tlog.Debug(\"dane tlsa record with wrong data size for sha2-256\", slog.Int(\"got\", len(r.CertAssoc)), slog.Int(\"expect\", sha256.Size))\n\t\t\t\tcontinue\n\t\t\t}\n\t\tcase adns.TLSAMatchTypeSHA512:\n\t\t\tif len(r.CertAssoc) != sha512.Size {\n\t\t\t\tlog.Debug(\"dane tlsa record with wrong data size for sha2-512\", slog.Int(\"got\", len(r.CertAssoc)), slog.Int(\"expect\", sha512.Size))\n\t\t\t\tcontinue\n\t\t\t}\n\t\tdefault:\n\t\t\tcontinue\n\t\t}\n\n\t\tl[o] = r\n\t\to++\n\t}\n\treturn l[:o]\n}\n\n// GatherTLSANames returns the allowed names in TLS certificates for verification\n// with PKIX-* or DANE-TA. The first name should be used for SNI.\n//\n// If there was no MX record, the next-hop domain parameters (i.e. the original\n// email destination host, and its CNAME-expanded host, that has MX records) are\n// ignored and only the base domain parameters are taken into account.\nfunc GatherTLSANames(haveMX, expandedNextHopAuthentic, expandedTLSABaseDomainAuthentic bool, origNextHop, expandedNextHop, origTLSABaseDomain, expandedTLSABaseDomain dns.Domain) []dns.Domain {\n\t// Gather the names to check against TLS certificate. ../rfc/7672:1318\n\tif !haveMX {\n\t\t// ../rfc/7672:1336\n\t\tif !expandedTLSABaseDomainAuthentic || origTLSABaseDomain == expandedTLSABaseDomain {\n\t\t\treturn []dns.Domain{origTLSABaseDomain}\n\t\t}\n\t\treturn []dns.Domain{expandedTLSABaseDomain, origTLSABaseDomain}\n\t} else if expandedNextHopAuthentic {\n\t\t// ../rfc/7672:1326\n\t\tvar l []dns.Domain\n\t\tif expandedTLSABaseDomainAuthentic {\n\t\t\tl = []dns.Domain{expandedTLSABaseDomain}\n\t\t}\n\t\tif expandedTLSABaseDomain != origTLSABaseDomain {\n\t\t\tl = append(l, origTLSABaseDomain)\n\t\t}\n\t\tl = append(l, origNextHop)\n\t\tif origNextHop != expandedNextHop {\n\t\t\tl = append(l, expandedNextHop)\n\t\t}\n\t\treturn l\n\t} else {\n\t\t// We don't attempt DANE after insecure MX, but behaviour for it is specified.\n\t\t// ../rfc/7672:1332\n\t\treturn []dns.Domain{origNextHop}\n\t}\n}\n\n```\n# Function Signature: func GatherIPs(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, network string, host dns.IPDomain, dialedIPs map[string][]net.IP) (authentic bool, expandedAuthentic bool, expandedHost dns.Domain, ips []net.IP, dualstack bool, rerr error) {\n# Natural Language Annotations: // GatherIPs looks up the IPs to try for connecting to host, with the IPs ordered\n// to take previous attempts into account. For use with DANE, the CNAME-expanded\n// name is returned, and whether the DNS responses were authentic.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc GatherIPs(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, network string, host dns.IPDomain, dialedIPs map[string][]net.IP) (authentic bool, expandedAuthentic bool, expandedHost dns.Domain, ips []net.IP, dualstack bool, rerr error) {\n\tlog := mlog.New(\"smtpclient\", elog)\n\n\t// ../rfc/7505:122\n\n\tauthentic = true\n\texpandedAuthentic = true\n\texpandedHost = host.Domain\n\tips = []net.IP{}\n\tdualstack = false\n\trerr = nil\n\n\t// We start out delivering to the recipient domain. We follow CNAMEs.\n\trcptDomain := host.Domain\n\t// Domain we are actually delivering to, after following CNAME record(s).\n\texpandedNextHop := rcptDomain\n\t// Keep track of CNAMEs we have followed, to detect loops.\n\tdomainsSeen := map[string]bool{}\n\tfor i := 0; ; i++ {\n\t\tif domainsSeen[expandedNextHop.ASCII] {\n\t\t\t// todo: only mark as permanent failure if TTLs for all records are beyond latest possibly delivery retry we would do.\n\t\t\terr := fmt.Errorf(\"%w: recipient domain %s: already saw %s\", errCNAMELoop, rcptDomain, expandedNextHop)\n\t\t\treturn false, authentic, expandedAuthentic, expandedHost, ips, dualstack, err\n\t\t}\n\t\tdomainsSeen[expandedNextHop.ASCII] = true\n\n\t\t// note: The Go resolver returns the requested name if the domain has no CNAME\n\t\t// record but has a host record.\n\t\tif i == 16 {\n\t\t\t// We have a maximum number of CNAME records we follow. There is no hard limit for\n\t\t\t// DNS, and you might think folks wouldn't configure CNAME chains at all, but for\n\t\t\t// (non-mail) domains, CNAME chains of 10 records have been encountered according\n\t\t\t// to the internet.\n\t\t\t// todo: only mark as permanent failure if TTLs for all records are beyond latest possibly delivery retry we would do.\n\t\t\terr := fmt.Errorf(\"%w: recipient domain %s, last resolved domain %s\", errCNAMELimit, rcptDomain, expandedNextHop)\n\t\t\treturn false, authentic, expandedAuthentic, expandedHost, ips, dualstack, err\n\t\t}\n\n\t\t// Do explicit CNAME lookup. Go's LookupMX also resolves CNAMEs, but we want to\n\t\t// know the final name, and we're interested in learning if the first vs later\n\t\t// results were DNSSEC-(in)secure.\n\t\t// ../rfc/5321:3838 ../rfc/3974:197\n\t\tcctx, ccancel := context.WithTimeout(ctx, 30*time.Second)\n\t\tdefer ccancel()\n\t\tcname, cnameResult, err := resolver.LookupCNAME(cctx, expandedNextHop.ASCII+\".\")\n\t\tccancel()\n\t\tauthentic = authentic && cnameResult.Authentic\n\t\texpandedAuthentic = expandedAuthentic && cnameResult.Authentic\n\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\terr = fmt.Errorf(\"%w: cname lookup for %s: %v\", errDNS, expandedNextHop, err)\n\t\t\treturn false, authentic, expandedAuthentic, expandedHost, ips, dualstack, err\n\t\t}\n\t\tif err == nil && cname != expandedNextHop.ASCII+\".\" {\n\t\t\td, err := dns.ParseDomain(strings.TrimSuffix(cname, \".\"))\n\t\t\tif err != nil {\n\t\t\t\t// todo: only mark as permanent failure if TTLs for all records are beyond latest possibly delivery retry we would do.\n\t\t\t\terr = fmt.Errorf(\"%w: parsing cname domain %s: %v\", errDNS, expandedNextHop, err)\n\t\t\t\treturn false, authentic, expandedAuthentic, expandedHost, ips, dualstack, err\n\t\t\t}\n\t\t\texpandedNextHop = d\n\t\t\t// Start again with new domain.\n\t\t\tcontinue\n\t\t}\n\n\t\t// Not a CNAME, so lookup MX record.\n\t\tmctx, mcancel := context.WithTimeout(ctx, 30*time.Second)\n\t\tdefer mcancel()\n\t\t// Note: LookupMX can return an error and still return records: Invalid records are\n\t\t// filtered out and an error returned. We must process any records that are valid.\n\t\t// Only if all are unusable will we return an error. ../rfc/5321:3851\n\t\tmxl, mxResult, err := resolver.LookupMX(mctx, expandedNextHop.ASCII+\".\")\n\t\tmcancel()\n\t\tauthentic = authentic && mxResult.Authentic\n\t\texpandedAuthentic = expandedAuthentic && mxResult.Authentic\n\t\tif err != nil && len(mxl) == 0 {\n\t\t\tif !dns.IsNotFound(err) {\n\t\t\t\terr = fmt.Errorf(\"%w: mx lookup for %s: %v\", errDNS, expandedNextHop, err)\n\t\t\t\treturn false, authentic, expandedAuthentic, expandedHost, ips, dualstack, err\n\t\t\t}\n\n\t\t\t// No MX record, attempt delivery directly to host. ../rfc/5321:3842\n\t\t\tips = dialedIPs[expandedNextHop.ASCII]\n\t\t\tdualstack, err = isDualStack(ips)\n\t\t\tif err != nil {\n\t\t\t\terr = fmt.Errorf(\"%w: checking for dualstack: %v\", errDNS, err)\n\t\t\t}\n\t\t\treturn authentic, expandedAuthentic, expandedHost, ips, dualstack, err\n\t\t} else if err != nil {\n\t\t\tlog.Infox(\"mx record has some invalid records, keeping only the valid mx records\", err)\n\t\t}\n\n\t\t// The Go resolver already sorts by preference, randomizing records of same\n\t\t// preference. ../rfc/5321:3885\n\t\tfor _, mx := range mxl {\n\t\t\t// Parsing lax (unless pedantic mode) for MX targets with underscores as seen in the wild.\n\t\t\thost, err := dns.ParseDomainLax(strings.TrimSuffix(mx.Host, \".\"))\n\t\t\tif err != nil {\n\t\t\t\t// note: should not happen because Go resolver already filters these out.\n\t\t\t\terr = fmt.Errorf(\"%w: invalid host name in mx record %q: %v\", errDNS, mx.Host, err)\n\t\t\t\treturn authentic, expandedAuthentic, expandedHost, ips, dualstack, err\n\t\t\t}\n\t\t\tips = append(ips, dialedIPs[host.ASCII]...)\n\t\t}\n\t\tif len(ips) > 0 {\n\t\t\terr = nil\n\t\t}\n\t\tdualstack, err = isDualStack(ips)\n\t\tif err != nil {\n\t\t\terr = fmt.Errorf(\"%w: checking for dualstack: %v\", errDNS, err)\n\t\t}\n\t\treturn authentic, expandedAuthentic, expandedHost, ips, dualstack, err\n\t}\n}\n\nfunc isDualStack(ips []net.IP) (bool, error) {\n\tdualstack := true\n\tfor _, ip := range ips {\n\t\tif ip.To4() != nil { // IPv4\n\t\t\tif !dualstack {\n\t\t\t\treturn false, errors.New(\"inconsistent IP versioning\")\n\t\t\t}\n\t\t} else { // IPv6\n\t\t\tdualstack = false\n\t\t}\n\t}\n\treturn dualstack, nil\n}\n```", "```go\npackage smtpclient\n\nimport (\n\t\"context\"\n\t\"net\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// GatherIPs looks up the IPs to try for connecting to host, with the IPs ordered\n// to take previous attempts into account. For use with DANE, the CNAME-expanded\n// name is returned, and whether the DNS responses were authentic.\nfunc GatherIPs(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, network string, host dns.IPDomain, dialedIPs map[string][]net.IP) (authentic bool, expandedAuthentic bool, expandedHost dns.Domain, ips []net.IP, dualstack bool, rerr error) {\n\tlog := mlog.New(\"smtpclient\", elog)\n\n\t// IP addresses are dialed directly, and don't have TLSA records.\n\tif len(host.IP) > 0 {\n\t\treturn false, false, expandedHost, []net.IP{host.IP}, false, nil\n\t}\n\n\t// We start out assuming the result is authentic. Updated with each lookup.\n\tauthentic = true\n\texpandedAuthentic = true\n\n\t// We start out delivering to the recipient domain. We follow CNAMEs.\n\trcptDomain := host.Domain\n\t// Domain we are actually delivering to, after following CNAME record(s).\n\texpandedHost = rcptDomain\n\n\t// If no CNAME lookup is needed, we can directly look up A/AAAA records.\n\tif ips, ok := dialedIPs[expandedHost.ASCII]; ok {\n\t\treturn authentic, expandedAuthentic, expandedHost, ips, false, nil\n\t}\n\n\t// Do explicit CNAME lookup. Go's LookupMX also resolves CNAMEs, but we want to\n\t// know the final name, and we're interested in learning if the first vs later\n\t// results were DNSSEC-(in)secure.\n\t// ../rfc/5321:3838 ../rfc/3974:197\n\tcctx, ccancel := context.WithTimeout(ctx, 30*time.Second)\n\tdefer ccancel()\n\tcname, cnameResult, err := resolver.LookupCNAME(cctx, expandedHost.ASCII+\".\")\n\tif err != nil && !dns.IsNotFound(err) {\n\t\terr = fmt.Errorf(\"%w: cname lookup for %s: %v\", errDNS, expandedHost, err)\n\t\treturn false, authentic, expandedHost, nil, false, err\n\t}\n\tauthentic = authentic && cnameResult.Authentic\n\texpandedAuthentic = expandedAuthentic && cnameResult.Authentic\n\tif err == nil && cname != expandedHost.ASCII+\".\" {\n\t\td, err := dns.ParseDomain(strings.TrimSuffix(cname, \".\"))\n\t\tif err != nil {\n\t\t\terr = fmt.Errorf(\"%w: parsing cname domain %s: %v\", errDNS, expandedHost, err)\n\t\t\treturn false, authentic, expandedHost, nil, false, err\n\t\t}\n\t\texpandedHost = d\n\t}\n\n\t// Lookup A and AAAA records.\n\tactx, acancel := context.WithTimeout(ctx, 30*time.Second)\n\tdefer acancel()\n\taddrs, result, err := resolver.LookupIP(actx, network, expandedHost.ASCII+\".\")\n\tif err != nil && !dns.IsNotFound(err) {\n\t\terr = fmt.Errorf(\"%w: ip lookup for %s: %v\", errDNS, expandedHost, err)\n\t\treturn authentic, expandedAuthentic, expandedHost, nil, false, err\n\t}\n\tauthentic = authentic && result.Authentic\n\texpandedAuthentic = expandedAuthentic && result.Authentic\n\tif err != nil {\n\t\treturn authentic, expandedAuthentic, expandedHost, nil, false, nil\n\t}\n\n\t// Filter out any dialed IPs.\n\tfilteredIPs := make([]net.IP, 0, len(addrs))\n\tfor _, ip := range addrs {\n\t\tdialed := false\n\t\tfor _, dialedIP := range dialedIPs[expandedHost.ASCII] {\n\t\t\tif dialedIP.Equal(ip) {\n\t\t\t\tdialed = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif !dialed {\n\t\t\tfilteredIPs = append(filteredIPs, ip)\n\t\t}\n\t}\n\n\t// Sort the IPs to take previous attempts into account.\n\tsortIPs(filteredIPs)\n\n\t// Check if we have a dual-stack setup.\n\tdualstack = hasDualStack(filteredIPs)\n\n\treturn authentic, expandedAuthentic, expandedHost, filteredIPs, dualstack, nil\n}\n```", "```go\nfunc GatherIPs(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, network string, host dns.IPDomain, dialedIPs map[string][]net.IP) (authentic bool, expandedAuthentic bool, expandedHost dns.Domain, ips []net.IP, dualstack bool, rerr error) {\n\tlog := mlog.New(\"smtpclient\", elog)\n\n\t// IP addresses are dialed directly, and don't have TLSA records.\n\tif len(host.IP) > 0 {\n\t\treturn false, false, dns.IPDomain{}, []net.IP{host.IP}, false, nil\n\t}\n\n\t// We start out assuming the result is authentic. Updated with each lookup.\n\tauthentic = true\n\texpandedAuthentic = true\n\n\t// We start out delivering to the recipient domain. We follow CNAMEs.\n\trcptDomain := host\n\t// Domain we are actually delivering to, after following CNAME record(s).\n\texpandedHost = rcptDomain\n\t// Keep track of CNAMEs we have followed, to detect loops.\n\tdomainsSeen := map[string]bool{}\n\tfor i := 0; ; i++ {\n\t\tif domainsSeen[expandedHost.ASCII] {\n\t\t\t// todo: only mark as permanent failure if TTLs for all records are beyond latest possibly delivery retry we would do.\n\t\t\terr := fmt.Errorf(\"%w: recipient domain %s: already saw %s\", errCNAMELoop, rcptDomain, expandedHost)\n\t\t\treturn false, authentic, expandedAuthentic, expandedHost, nil, false, err\n\t\t}\n\t\tdomainsSeen[expandedHost.ASCII] = true\n\n\t\t// note: The Go resolver returns the requested name if the domain has no CNAME\n\t\t// record but has a host record.\n\t\tif i == 16 {\n\t\t\t// We have a maximum number of CNAME records we follow. There is no hard limit for\n\t\t\t// DNS, and you might think folks wouldn't configure CNAME chains at all, but for\n\t\t\t// (non-mail) domains, CNAME chains of 10 records have been encountered according\n\t\t\t// to the internet.\n\t\t\t// todo: only mark as permanent failure if TTLs for all records are beyond latest possibly delivery retry we would do.\n\t\t\terr := fmt.Errorf(\"%w: recipient domain %s, last resolved domain %s\", errCNAMELimit, rcptDomain, expandedHost)\n\t\t\treturn false, authentic, expandedAuthentic, expandedHost, nil, false, err\n\t\t}\n\n\t\t// Do explicit CNAME lookup. Go's LookupMX also resolves CNAMEs, but we want to\n\t\t// know the final name, and we're interested in learning if the first vs later\n\t\t// results were DNSSEC-(in)secure.\n\t\t// ../rfc/5321:3838 ../rfc/3974:197\n\t\tcctx, ccancel := context.WithTimeout(ctx, 30*time.Second)\n\t\tdefer ccancel()\n\t\tcname, cnameResult, err := resolver.LookupCNAME(cctx, expandedHost.ASCII+\".\")\n\t\tccancel()\n\t\tauthentic = authentic && cnameResult.Authentic\n\t\texpandedAuthentic = expandedAuthentic && cnameResult.Authentic\n\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\terr = fmt.Errorf(\"%w: cname lookup for %s: %v\", errDNS, expandedHost, err)\n\t\t\treturn false, authentic, expandedAuthentic, expandedHost, nil, false, err\n\t\t}\n\t\tif err == nil && cname != expandedHost.ASCII+\".\" {\n\t\t\td, err := dns.ParseDomain(strings.TrimSuffix(cname, \".\"))\n\t\t\tif err != nil {\n\t\t\t\t// todo: only mark as permanent failure if TTLs for all records are beyond latest possibly delivery retry we would do.\n\t\t\t\terr = fmt.Errorf(\"%w: parsing cname domain %s: %v\", errDNS, expandedHost, err)\n\t\t\t\treturn false, authentic, expandedAuthentic, expandedHost, nil, false, err\n\t\t\t}\n\t\t\texpandedHost = d\n\t\t\t// Start again with new domain.\n\t\t\tcontinue\n\t\t}\n\n\t\t// Not a CNAME, so lookup MX record.\n\t\tmctx, mcancel := context.WithTimeout(ctx, 30*time.Second)\n\t\tdefer mcancel()\n\t\t// Note: LookupMX can return an error and still return records: Invalid records are\n\t\t// filtered out and an error returned. We must process any records that are valid.\n\t\t// Only if all are unusable will we return an error. ../rfc/5321:3851\n\t\t_, _, err = resolver.LookupMX(mctx, expandedHost.ASCII+\".\")\n\t\tmcancel()\n\t\tauthentic = authentic && cnameResult.Authentic\n\t\texpandedAuthentic = expandedAuthentic && cnameResult.Authentic\n\t\tif err != nil {\n\t\t\treturn false, authentic, expandedAuthentic, expandedHost, nil, false, nil\n\t\t}\n\n\t\t// The Go resolver already sorts by preference, randomizing records of same\n\t\t// preference. ../rfc/5321:3885\n\t\tips = make([]net.IP, 0)\n\t\tfor _, ip := range dialedIPs[expandedHost.ASCII] {\n\t\t\t// Try to filter the IPs based on the network type\n\t\t\tswitch network {\n\t\t\tcase \"tcp4\":\n\t\t\t\tif ip.To4() != nil {\n\t\t\t\t\tips = append(ips, ip)\n\t\t\t\t}\n\t\t\tcase \"tcp6\":\n\t\t\t\tif ip.To4() == nil {\n\t\t\t\t\tips = append(ips, ip)\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\tips = append(ips, ip)\n\t\t\t}\n\t\t}\n\t\tdualstack = len(ips) > 0 && len(dialedIPs[expandedHost.ASCII]) > 0\n\t\treturn true, authentic, expandedAuthentic, expandedHost, ips, dualstack, nil\n\t}\n}\n```"]}, "mox-tlsrpt/report.go-TLSFailureDetails": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/tlsrpt/report.go:\n```\npackage tlsrpt\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net\"\n\t\"os\"\n\t\"reflect\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/mjl-/adns\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/moxio\"\n)\n\nvar ErrNoReport = errors.New(\"no tlsrpt report found\")\n\n// ../rfc/8460:628\n\n// Report is a TLSRPT report.\ntype Report struct {\n\tOrganizationName string\n\tDateRange        TLSRPTDateRange\n\tContactInfo      string\n\tReportID         string\n\tPolicies         []Result\n}\n\n// ReportJSON is a TLS report with field names as used in the specification. These field names are inconvenient to use in JavaScript, so after parsing a ReportJSON is turned into a Report.\ntype ReportJSON struct {\n\tOrganizationName string              `json:\"organization-name\"`\n\tDateRange        TLSRPTDateRangeJSON `json:\"date-range\"`\n\tContactInfo      string              `json:\"contact-info\"` // Email address.\n\tReportID         string              `json:\"report-id\"`\n\tPolicies         []ResultJSON        `json:\"policies\"`\n}\n\nfunc convertSlice[T interface{ Convert() S }, S any](l []T) []S {\n\tif l == nil {\n\t\treturn nil\n\t}\n\tr := make([]S, len(l))\n\tfor i, e := range l {\n\t\tr[i] = e.Convert()\n\t}\n\treturn r\n}\n\nfunc (v Report) Convert() ReportJSON {\n\treturn ReportJSON{v.OrganizationName, v.DateRange.Convert(), v.ContactInfo, v.ReportID, convertSlice[Result, ResultJSON](v.Policies)}\n}\n\nfunc (v ReportJSON) Convert() Report {\n\treturn Report{v.OrganizationName, v.DateRange.Convert(), v.ContactInfo, v.ReportID, convertSlice[ResultJSON, Result](v.Policies)}\n}\n\n// Merge combines the counts and failure details of results into the report.\n// Policies are merged if identical and added otherwise. Same for failure details\n// within a result.\nfunc (r *Report) Merge(results ...Result) {\nMerge:\n\tfor _, nr := range results {\n\t\tfor i, p := range r.Policies {\n\t\t\tif !p.Policy.equal(nr.Policy) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tr.Policies[i].Add(nr.Summary.TotalSuccessfulSessionCount, nr.Summary.TotalFailureSessionCount, nr.FailureDetails...)\n\t\t\tcontinue Merge\n\t\t}\n\n\t\tr.Policies = append(r.Policies, nr)\n\t}\n}\n\n// Add increases the success/failure counts of a result, and adds any failure\n// details.\nfunc (r *Result) Add(success, failure int64, fds ...FailureDetails) {\n\tr.Summary.TotalSuccessfulSessionCount += success\n\tr.Summary.TotalFailureSessionCount += failure\n\n\t// In smtpclient we can compensate with a negative success, after failed read after\n\t// successful handshake. Sanity check that we never get negative counts.\n\tif r.Summary.TotalSuccessfulSessionCount < 0 {\n\t\tr.Summary.TotalSuccessfulSessionCount = 0\n\t}\n\tif r.Summary.TotalFailureSessionCount < 0 {\n\t\tr.Summary.TotalFailureSessionCount = 0\n\t}\n\nMerge:\n\tfor _, nfd := range fds {\n\t\tfor i, fd := range r.FailureDetails {\n\t\t\tif !fd.equalKey(nfd) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tfd.FailedSessionCount += nfd.FailedSessionCount\n\t\t\tr.FailureDetails[i] = fd\n\t\t\tcontinue Merge\n\t\t}\n\t\tr.FailureDetails = append(r.FailureDetails, nfd)\n\t}\n}\n\n// Add is a convenience function for merging making a Result and merging it into\n// the report.\nfunc (r *Report) Add(policy ResultPolicy, success, failure int64, fds ...FailureDetails) {\n\tr.Merge(Result{policy, Summary{success, failure}, fds})\n}\n\n// TLSAPolicy returns a policy for DANE.\nfunc TLSAPolicy(records []adns.TLSA, tlsaBaseDomain dns.Domain) ResultPolicy {\n\t// The policy domain is the TLSA base domain. ../rfc/8460:251\n\n\tl := make([]string, len(records))\n\tfor i, r := range records {\n\t\tl[i] = r.Record()\n\t}\n\tsort.Strings(l) // For consistent equals.\n\treturn ResultPolicy{\n\t\tType:   TLSA,\n\t\tString: l,\n\t\tDomain: tlsaBaseDomain.ASCII,\n\t\tMXHost: []string{},\n\t}\n}\n\nfunc MakeResult(policyType PolicyType, domain dns.Domain, fds ...FailureDetails) Result {\n\tif fds == nil {\n\t\tfds = []FailureDetails{}\n\t}\n\treturn Result{\n\t\tPolicy:         ResultPolicy{Type: policyType, Domain: domain.ASCII, String: []string{}, MXHost: []string{}},\n\t\tFailureDetails: fds,\n\t}\n}\n\n// note: with TLSRPT prefix to prevent clash in sherpadoc types.\ntype TLSRPTDateRange struct {\n\tStart time.Time\n\tEnd   time.Time\n}\n\nfunc (v TLSRPTDateRange) Convert() TLSRPTDateRangeJSON {\n\treturn TLSRPTDateRangeJSON(v)\n}\n\ntype TLSRPTDateRangeJSON struct {\n\tStart time.Time `json:\"start-datetime\"`\n\tEnd   time.Time `json:\"end-datetime\"`\n}\n\nfunc (v TLSRPTDateRangeJSON) Convert() TLSRPTDateRange {\n\treturn TLSRPTDateRange(v)\n}\n\n// UnmarshalJSON is defined on the date range, not the individual time.Time fields\n// because it is easier to keep the unmodified time.Time fields stored in the\n// database.\nfunc (dr *TLSRPTDateRangeJSON) UnmarshalJSON(buf []byte) error {\n\tvar v struct {\n\t\tStart xtime `json:\"start-datetime\"`\n\t\tEnd   xtime `json:\"end-datetime\"`\n\t}\n\tif err := json.Unmarshal(buf, &v); err != nil {\n\t\treturn err\n\t}\n\tdr.Start = time.Time(v.Start)\n\tdr.End = time.Time(v.End)\n\treturn nil\n}\n\n// xtime and its UnmarshalJSON exists to work around a specific invalid date-time encoding seen in the wild.\ntype xtime time.Time\n\nfunc (x *xtime) UnmarshalJSON(buf []byte) error {\n\tvar t time.Time\n\terr := t.UnmarshalJSON(buf)\n\tif err == nil {\n\t\t*x = xtime(t)\n\t\treturn nil\n\t}\n\n\t// Microsoft is sending reports with invalid start-datetime/end-datetime (missing\n\t// timezone, ../rfc/8460:682 ../rfc/3339:415). We compensate.\n\tvar s string\n\tif err := json.Unmarshal(buf, &s); err != nil {\n\t\treturn err\n\t}\n\tt, err = time.Parse(\"2006-01-02T15:04:05\", s)\n\tif err != nil {\n\t\treturn err\n\t}\n\t*x = xtime(t)\n\treturn nil\n}\n\ntype Result struct {\n\tPolicy         ResultPolicy\n\tSummary        Summary\n\tFailureDetails []FailureDetails\n}\n\nfunc (r Result) Convert() ResultJSON {\n\treturn ResultJSON{ResultPolicyJSON(r.Policy), SummaryJSON(r.Summary), convertSlice[FailureDetails, FailureDetailsJSON](r.FailureDetails)}\n}\n\ntype ResultJSON struct {\n\tPolicy         ResultPolicyJSON     `json:\"policy\"`\n\tSummary        SummaryJSON          `json:\"summary\"`\n\tFailureDetails []FailureDetailsJSON `json:\"failure-details\"`\n}\n\nfunc (r ResultJSON) Convert() Result {\n\treturn Result{ResultPolicy(r.Policy), Summary(r.Summary), convertSlice[FailureDetailsJSON, FailureDetails](r.FailureDetails)}\n}\n\n// todo spec: ../rfc/8460:437 says policy is a string, with rules for turning dane records into a single string. perhaps a remnant of an earlier version (for mtasts a single string would have made more sense). i doubt the intention is to always have a single element in policy-string (though the field name is singular).\n\ntype ResultPolicy struct {\n\tType   PolicyType\n\tString []string\n\tDomain string // ASCII/A-labels, ../rfc/8460:704\n\tMXHost []string\n}\n\ntype ResultPolicyJSON struct {\n\tType   PolicyType `json:\"policy-type\"`\n\tString []string   `json:\"policy-string\"`\n\tDomain string     `json:\"policy-domain\"`\n\tMXHost []string   `json:\"mx-host\"` // Example in RFC has errata, it originally was a single string. ../rfc/8460-eid6241 ../rfc/8460:1779\n}\n\n// PolicyType indicates the policy success/failure results are for.\ntype PolicyType string\n\nconst (\n\t// For DANE, against a mail host (not recipient domain).\n\tTLSA PolicyType = \"tlsa\"\n\n\t// For MTA-STS, against a recipient domain (not a mail host).\n\tSTS PolicyType = \"sts\"\n\n\t// Recipient domain did not have MTA-STS policy, or mail host (TSLA base domain)\n\t// did not have DANE TLSA records.\n\tNoPolicyFound PolicyType = \"no-policy-found\"\n\t// todo spec: ../rfc/8460:440 ../rfc/8460:697 suggest to replace with values like \"no-sts-found\" and \"no-tlsa-found\" to make it explicit which policy isn't found. also easier to implement, because you don't have to handle leaving out an sts no-policy-found result for a mail host when a tlsa policy is present.\n)\n\nfunc (rp ResultPolicy) equal(orp ResultPolicy) bool {\n\treturn rp.Type == orp.Type && slices.Equal(rp.String, orp.String) && rp.Domain == orp.Domain && slices.Equal(rp.MXHost, orp.MXHost)\n}\n\ntype Summary struct {\n\tTotalSuccessfulSessionCount int64\n\tTotalFailureSessionCount    int64\n}\n\ntype SummaryJSON struct {\n\tTotalSuccessfulSessionCount int64 `json:\"total-successful-session-count\"`\n\tTotalFailureSessionCount    int64 `json:\"total-failure-session-count\"`\n}\n\n// ResultType represents a TLS error.\ntype ResultType string\n\n// ../rfc/8460:1377\n// https://www.iana.org/assignments/starttls-validation-result-types/starttls-validation-result-types.xhtml\n\nconst (\n\tResultSTARTTLSNotSupported    ResultType = \"starttls-not-supported\"\n\tResultCertificateHostMismatch ResultType = \"certificate-host-mismatch\"\n\tResultCertificateExpired      ResultType = \"certificate-expired\"\n\tResultTLSAInvalid             ResultType = \"tlsa-invalid\"\n\tResultDNSSECInvalid           ResultType = \"dnssec-invalid\"\n\tResultDANERequired            ResultType = \"dane-required\"\n\tResultCertificateNotTrusted   ResultType = \"certificate-not-trusted\"\n\tResultSTSPolicyInvalid        ResultType = \"sts-policy-invalid\"\n\tResultSTSWebPKIInvalid        ResultType = \"sts-webpki-invalid\"\n\tResultValidationFailure       ResultType = \"validation-failure\" // Other error.\n\tResultSTSPolicyFetch          ResultType = \"sts-policy-fetch-error\"\n)\n\n// todo spec: ../rfc/8460:719 more of these fields should be optional. some sts failure details, like failed policy fetches, won't have an ip or mx, the failure happens earlier in the delivery process.\n\ntype FailureDetails struct {\n\tResultType            ResultType\n\tSendingMTAIP          string\n\tReceivingMXHostname   string\n\tReceivingMXHelo       string\n\tReceivingIP           string\n\tFailedSessionCount    int64\n\tAdditionalInformation string\n\tFailureReasonCode     string\n}\n\nfunc (v FailureDetails) Convert() FailureDetailsJSON { return FailureDetailsJSON(v) }\n\ntype FailureDetailsJSON struct {\n\tResultType            ResultType `json:\"result-type\"`\n\tSendingMTAIP          string     `json:\"sending-mta-ip\"`\n\tReceivingMXHostname   string     `json:\"receiving-mx-hostname\"`\n\tReceivingMXHelo       string     `json:\"receiving-mx-helo,omitempty\"`\n\tReceivingIP           string     `json:\"receiving-ip\"`\n\tFailedSessionCount    int64      `json:\"failed-session-count\"`\n\tAdditionalInformation string     `json:\"additional-information\"`\n\tFailureReasonCode     string     `json:\"failure-reason-code\"`\n}\n\nfunc (v FailureDetailsJSON) Convert() FailureDetails { return FailureDetails(v) }\n\n// equalKey returns whether FailureDetails have the same values, expect for\n// FailedSessionCount. Useful for aggregating FailureDetails.\nfunc (fd FailureDetails) equalKey(ofd FailureDetails) bool {\n\tfd.FailedSessionCount = 0\n\tofd.FailedSessionCount = 0\n\treturn fd == ofd\n}\n\n// Details is a convenience function to compose a FailureDetails.\nfunc Details(t ResultType, r string) FailureDetails {\n\treturn FailureDetails{ResultType: t, FailedSessionCount: 1, FailureReasonCode: r}\n}\n\nvar invalidReasons = map[x509.InvalidReason]string{\n\tx509.NotAuthorizedToSign:           \"not-authorized-to-sign\",\n\tx509.Expired:                       \"certificate-expired\",\n\tx509.CANotAuthorizedForThisName:    \"ca-not-authorized-for-this-name\",\n\tx509.TooManyIntermediates:          \"too-many-intermediates\",\n\tx509.IncompatibleUsage:             \"incompatible-key-usage\",\n\tx509.NameMismatch:                  \"parent-subject-child-issuer-mismatch\",\n\tx509.NameConstraintsWithoutSANs:    \"name-constraint-without-sans\",\n\tx509.UnconstrainedName:             \"unconstrained-name\",\n\tx509.TooManyConstraints:            \"too-many-constraints\",\n\tx509.CANotAuthorizedForExtKeyUsage: \"ca-not-authorized-for-ext-key-usage\",\n}\n\n// TLSFailureDetails turns errors encountered during TLS handshakes into a result\n// type and failure reason code for use with FailureDetails.\n//\n// Errors from crypto/tls, including local and remote alerts, from crypto/x509,\n// and generic i/o and timeout errors are recognized.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Parse parses a Report.\n// The maximum size is 20MB.\nfunc Parse(r io.Reader) (*ReportJSON, error) {\n\tr = &moxio.LimitReader{R: r, Limit: 20 * 1024 * 1024}\n\tvar report ReportJSON\n\tif err := json.NewDecoder(r).Decode(&report); err != nil {\n\t\treturn nil, err\n\t}\n\t// note: there may be leftover data, we ignore it.\n\treturn &report, nil\n}\n\n// ParseMessage parses a Report from a mail message.\n// The maximum size of the message is 15MB, the maximum size of the\n// decompressed report is 20MB.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc parseMessageReport(log mlog.Log, p message.Part, allow bool) (*ReportJSON, error) {\n\tif p.MediaType != \"MULTIPART\" {\n\t\tif !allow {\n\t\t\treturn nil, ErrNoReport\n\t\t}\n\t\treturn parseReport(p)\n\t}\n\n\tfor {\n\t\tsp, err := p.ParseNextPart(log.Logger)\n\t\tif err == io.EOF {\n\t\t\treturn nil, ErrNoReport\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif p.MediaSubType == \"REPORT\" && p.ContentTypeParams[\"report-type\"] != \"tlsrpt\" {\n\t\t\treturn nil, fmt.Errorf(\"unknown report-type parameter %q\", p.ContentTypeParams[\"report-type\"])\n\t\t}\n\t\treport, err := parseMessageReport(log, *sp, p.MediaSubType == \"REPORT\")\n\t\tif err == ErrNoReport {\n\t\t\tcontinue\n\t\t} else if err != nil || report != nil {\n\t\t\treturn report, err\n\t\t}\n\t}\n}\n\nfunc parseReport(p message.Part) (*ReportJSON, error) {\n\tmt := strings.ToLower(p.MediaType + \"/\" + p.MediaSubType)\n\tswitch mt {\n\tcase \"application/tlsrpt+json\":\n\t\treturn Parse(p.Reader())\n\tcase \"application/tlsrpt+gzip\":\n\t\tgzr, err := gzip.NewReader(p.Reader())\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"decoding gzip TLSRPT report: %s\", err)\n\t\t}\n\t\treturn Parse(gzr)\n\t}\n\treturn nil, ErrNoReport\n}\n\n```\n# Function Signature: func TLSFailureDetails(err error) (ResultType, string) {\n# Natural Language Annotations: // TLSFailureDetails turns errors encountered during TLS handshakes into a result\n// type and failure reason code for use with FailureDetails.\n//\n// Errors from crypto/tls, including local and remote alerts, from crypto/x509,\n// and generic i/o and timeout errors are recognized.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc TLSFailureDetails(err error) (ResultType, string) {\n\tswitch err {\n\tcase tls.ErrSSLAlert, tls.ErrSSLNegotiation, tls.ErrSSLNegotiationTimeout:\n\t\treturn ResultValidationFailure, \"local-or-remote-alert\"\n\tcase net.Error:\n\t\tif err.(net.Error).Timeout() {\n\t\t\treturn ResultValidationFailure, \"timeout\"\n\t\t}\n\tcase x509.UnknownAuthorityError:\n\t\treturn ResultCertificateNotTrusted, \"unknown-ca\"\n\tcase x509.CertificateInvalidError:\n\t\t// This is the most common case, so default to this case.\n\t\treason := \"unknown-error\"\n\t\tif invalid, ok := invalidReasons[err.(x509.CertificateInvalidError).Reason]; ok {\n\t\t\treason = invalid\n\t\t}\n\t\treturn ResultCertificateExpired, reason\n\tcase x509.HostnameError:\n\t\treturn ResultCertificateHostMismatch, \"hostname-mismatch\"\n\tcase x509.SystemRootsError:\n\t\treturn ResultCertificateNotTrusted, \"system-roots-error\"\n\tcase x509.ConstraintViolationError:\n\t\treturn ResultCertificateNotTrusted, \"constraint-violation\"\n\tcase x509.InsecureAlgorithmError:\n\t\treturn ResultCertificateNotTrusted, \"insecure-algorithm\"\n\tcase x509.UnsupportedFeatureError:\n\t\treturn ResultCertificateNotTrusted, \"unsupported-feature\"\n\t}\n\n\treturn ResultValidationFailure, \"unknown-error\"\n}\n```", "```go\nfunc TLSFailureDetails(err error) (ResultType, string) {\n\tvar resultType ResultType\n\tvar reasonCode string\n\n\tswitch e := err.(type) {\n\tcase x509.CertificateInvalidError:\n\t\tresultType = ResultCertificateNotTrusted\n\t\treasonCode = invalidReasons[e.Reason]\n\tcase x509.UnknownAuthorityError:\n\t\tresultType = ResultCertificateNotTrusted\n\t\treasonCode = \"unknown-ca\"\n\tcase x509.HostnameError:\n\t\tresultType = ResultCertificateHostMismatch\n\t\treasonCode = \"hostname-mismatch\"\n\tcase x509.SystemRootsError:\n\t\tresultType = ResultCertificateNotTrusted\n\t\treasonCode = \"system-roots-error\"\n\tcase x509.UnhandledCriticalExtension:\n\t\tresultType = ResultValidationFailure\n\t\treasonCode = \"unhandled-critical-extension\"\n\tcase x509.UnknownSignatureAlgorithmError:\n\t\tresultType = ResultValidationFailure\n\t\treasonCode = \"unknown-signature-algorithm\"\n\tcase x509.SystemRootsError:\n\t\tresultType = ResultValidationFailure\n\t\treasonCode = \"system-roots-error\"\n\tcase x509.InsecureAlgorithmError:\n\t\tresultType = ResultValidationFailure\n\t\treasonCode = \"insecure-algorithm\"\n\tcase x509.ConstraintViolationError:\n\t\tresultType = ResultValidationFailure\n\t\treasonCode = \"constraint-violation\"\n\tcase x509.UnsupportedFeatureError:\n\t\tresultType = ResultValidationFailure\n\t\treasonCode = \"unsupported-feature\"\n\tcase x509.SystemRootsError:\n\t\tresultType = ResultValidationFailure\n\t\treasonCode = \"system-roots-error\"\n\tdefault:\n\t\tswitch e := err.(type) {\n\t\tcase tls.RecordHeaderError:\n\t\t\tresultType = ResultValidationFailure\n\t\t\treasonCode = e.Error()\n\t\tcase net.Error:\n\t\t\tif e.Timeout() {\n\t\t\t\tresultType = ResultValidationFailure\n\t\t\t\treasonCode = \"timeout\"\n\t\t\t} else {\n\t\t\t\tresultType = ResultValidationFailure\n\t\t\t\treasonCode = \"network-error\"\n\t\t\t}\n\t\tdefault:\n\t\t\tresultType = ResultValidationFailure\n\t\t\treasonCode = \"unknown-error\"\n\t\t}\n\t}\n\n\treturn resultType, reasonCode\n}\n```", "Here is the generated function based on your request:\n\n```go\nfunc TLSFailureDetails(err error) (ResultType, string) {\n\tvar reasonCode string\n\tvar resultType ResultType\n\n\tswitch e := err.(type) {\n\tcase x509.CertificateInvalidError:\n\t\treasonCode = invalidReasons[e.Reason]\n\t\tresultType = ResultCertificateHostMismatch\n\tcase x509.UnknownAuthorityError, x509.SystemRootsError:\n\t\treasonCode = \"certificate-not-trusted\"\n\t\tresultType = ResultCertificateNotTrusted\n\tcase x509.InsecureAlgorithmError:\n\t\treasonCode = \"insecure-algorithm\"\n\t\tresultType = ResultDANERequired\n\tcase x509.ConstraintViolationError:\n\t\treasonCode = \"constraint-violation\"\n\t\tresultType = ResultDANERequired\n\tcase x509.UnsupportedCertificateError:\n\t\treasonCode = \"unsupported-certificate\"\n\t\tresultType = ResultValidationFailure\n\tcase net.Error:\n\t\tif e.Timeout() {\n\t\t\treasonCode = \"timeout\"\n\t\t\tresultType = ResultValidationFailure\n\t\t} else {\n\t\t\treasonCode = \"network-error\"\n\t\t\tresultType = ResultValidationFailure\n\t\t}\n\tcase io.EOF:\n\t\treasonCode = \"eof\"\n\t\tresultType = ResultValidationFailure\n\tdefault:\n\t\treasonCode = \"unknown-error\"\n\t\tresultType = ResultValidationFailure\n\t}\n\n\treturn resultType, reasonCode\n}\n```\n\nThis function takes an error as input and returns a TLSRPT result type and a reason code based on the type of the error. It supports errors from crypto/tls, crypto/x509, and generic i/o and timeout errors. For unsupported types of errors, it defaults to returning a ResultValidationFailure with a reason code of \"unknown-error\"."]}, "mox-tlsrpt/report.go-ParseMessage": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/tlsrpt/report.go:\n```\npackage tlsrpt\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net\"\n\t\"os\"\n\t\"reflect\"\n\t\"slices\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/mjl-/adns\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/moxio\"\n)\n\nvar ErrNoReport = errors.New(\"no tlsrpt report found\")\n\n// ../rfc/8460:628\n\n// Report is a TLSRPT report.\ntype Report struct {\n\tOrganizationName string\n\tDateRange        TLSRPTDateRange\n\tContactInfo      string\n\tReportID         string\n\tPolicies         []Result\n}\n\n// ReportJSON is a TLS report with field names as used in the specification. These field names are inconvenient to use in JavaScript, so after parsing a ReportJSON is turned into a Report.\ntype ReportJSON struct {\n\tOrganizationName string              `json:\"organization-name\"`\n\tDateRange        TLSRPTDateRangeJSON `json:\"date-range\"`\n\tContactInfo      string              `json:\"contact-info\"` // Email address.\n\tReportID         string              `json:\"report-id\"`\n\tPolicies         []ResultJSON        `json:\"policies\"`\n}\n\nfunc convertSlice[T interface{ Convert() S }, S any](l []T) []S {\n\tif l == nil {\n\t\treturn nil\n\t}\n\tr := make([]S, len(l))\n\tfor i, e := range l {\n\t\tr[i] = e.Convert()\n\t}\n\treturn r\n}\n\nfunc (v Report) Convert() ReportJSON {\n\treturn ReportJSON{v.OrganizationName, v.DateRange.Convert(), v.ContactInfo, v.ReportID, convertSlice[Result, ResultJSON](v.Policies)}\n}\n\nfunc (v ReportJSON) Convert() Report {\n\treturn Report{v.OrganizationName, v.DateRange.Convert(), v.ContactInfo, v.ReportID, convertSlice[ResultJSON, Result](v.Policies)}\n}\n\n// Merge combines the counts and failure details of results into the report.\n// Policies are merged if identical and added otherwise. Same for failure details\n// within a result.\nfunc (r *Report) Merge(results ...Result) {\nMerge:\n\tfor _, nr := range results {\n\t\tfor i, p := range r.Policies {\n\t\t\tif !p.Policy.equal(nr.Policy) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tr.Policies[i].Add(nr.Summary.TotalSuccessfulSessionCount, nr.Summary.TotalFailureSessionCount, nr.FailureDetails...)\n\t\t\tcontinue Merge\n\t\t}\n\n\t\tr.Policies = append(r.Policies, nr)\n\t}\n}\n\n// Add increases the success/failure counts of a result, and adds any failure\n// details.\nfunc (r *Result) Add(success, failure int64, fds ...FailureDetails) {\n\tr.Summary.TotalSuccessfulSessionCount += success\n\tr.Summary.TotalFailureSessionCount += failure\n\n\t// In smtpclient we can compensate with a negative success, after failed read after\n\t// successful handshake. Sanity check that we never get negative counts.\n\tif r.Summary.TotalSuccessfulSessionCount < 0 {\n\t\tr.Summary.TotalSuccessfulSessionCount = 0\n\t}\n\tif r.Summary.TotalFailureSessionCount < 0 {\n\t\tr.Summary.TotalFailureSessionCount = 0\n\t}\n\nMerge:\n\tfor _, nfd := range fds {\n\t\tfor i, fd := range r.FailureDetails {\n\t\t\tif !fd.equalKey(nfd) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tfd.FailedSessionCount += nfd.FailedSessionCount\n\t\t\tr.FailureDetails[i] = fd\n\t\t\tcontinue Merge\n\t\t}\n\t\tr.FailureDetails = append(r.FailureDetails, nfd)\n\t}\n}\n\n// Add is a convenience function for merging making a Result and merging it into\n// the report.\nfunc (r *Report) Add(policy ResultPolicy, success, failure int64, fds ...FailureDetails) {\n\tr.Merge(Result{policy, Summary{success, failure}, fds})\n}\n\n// TLSAPolicy returns a policy for DANE.\nfunc TLSAPolicy(records []adns.TLSA, tlsaBaseDomain dns.Domain) ResultPolicy {\n\t// The policy domain is the TLSA base domain. ../rfc/8460:251\n\n\tl := make([]string, len(records))\n\tfor i, r := range records {\n\t\tl[i] = r.Record()\n\t}\n\tsort.Strings(l) // For consistent equals.\n\treturn ResultPolicy{\n\t\tType:   TLSA,\n\t\tString: l,\n\t\tDomain: tlsaBaseDomain.ASCII,\n\t\tMXHost: []string{},\n\t}\n}\n\nfunc MakeResult(policyType PolicyType, domain dns.Domain, fds ...FailureDetails) Result {\n\tif fds == nil {\n\t\tfds = []FailureDetails{}\n\t}\n\treturn Result{\n\t\tPolicy:         ResultPolicy{Type: policyType, Domain: domain.ASCII, String: []string{}, MXHost: []string{}},\n\t\tFailureDetails: fds,\n\t}\n}\n\n// note: with TLSRPT prefix to prevent clash in sherpadoc types.\ntype TLSRPTDateRange struct {\n\tStart time.Time\n\tEnd   time.Time\n}\n\nfunc (v TLSRPTDateRange) Convert() TLSRPTDateRangeJSON {\n\treturn TLSRPTDateRangeJSON(v)\n}\n\ntype TLSRPTDateRangeJSON struct {\n\tStart time.Time `json:\"start-datetime\"`\n\tEnd   time.Time `json:\"end-datetime\"`\n}\n\nfunc (v TLSRPTDateRangeJSON) Convert() TLSRPTDateRange {\n\treturn TLSRPTDateRange(v)\n}\n\n// UnmarshalJSON is defined on the date range, not the individual time.Time fields\n// because it is easier to keep the unmodified time.Time fields stored in the\n// database.\nfunc (dr *TLSRPTDateRangeJSON) UnmarshalJSON(buf []byte) error {\n\tvar v struct {\n\t\tStart xtime `json:\"start-datetime\"`\n\t\tEnd   xtime `json:\"end-datetime\"`\n\t}\n\tif err := json.Unmarshal(buf, &v); err != nil {\n\t\treturn err\n\t}\n\tdr.Start = time.Time(v.Start)\n\tdr.End = time.Time(v.End)\n\treturn nil\n}\n\n// xtime and its UnmarshalJSON exists to work around a specific invalid date-time encoding seen in the wild.\ntype xtime time.Time\n\nfunc (x *xtime) UnmarshalJSON(buf []byte) error {\n\tvar t time.Time\n\terr := t.UnmarshalJSON(buf)\n\tif err == nil {\n\t\t*x = xtime(t)\n\t\treturn nil\n\t}\n\n\t// Microsoft is sending reports with invalid start-datetime/end-datetime (missing\n\t// timezone, ../rfc/8460:682 ../rfc/3339:415). We compensate.\n\tvar s string\n\tif err := json.Unmarshal(buf, &s); err != nil {\n\t\treturn err\n\t}\n\tt, err = time.Parse(\"2006-01-02T15:04:05\", s)\n\tif err != nil {\n\t\treturn err\n\t}\n\t*x = xtime(t)\n\treturn nil\n}\n\ntype Result struct {\n\tPolicy         ResultPolicy\n\tSummary        Summary\n\tFailureDetails []FailureDetails\n}\n\nfunc (r Result) Convert() ResultJSON {\n\treturn ResultJSON{ResultPolicyJSON(r.Policy), SummaryJSON(r.Summary), convertSlice[FailureDetails, FailureDetailsJSON](r.FailureDetails)}\n}\n\ntype ResultJSON struct {\n\tPolicy         ResultPolicyJSON     `json:\"policy\"`\n\tSummary        SummaryJSON          `json:\"summary\"`\n\tFailureDetails []FailureDetailsJSON `json:\"failure-details\"`\n}\n\nfunc (r ResultJSON) Convert() Result {\n\treturn Result{ResultPolicy(r.Policy), Summary(r.Summary), convertSlice[FailureDetailsJSON, FailureDetails](r.FailureDetails)}\n}\n\n// todo spec: ../rfc/8460:437 says policy is a string, with rules for turning dane records into a single string. perhaps a remnant of an earlier version (for mtasts a single string would have made more sense). i doubt the intention is to always have a single element in policy-string (though the field name is singular).\n\ntype ResultPolicy struct {\n\tType   PolicyType\n\tString []string\n\tDomain string // ASCII/A-labels, ../rfc/8460:704\n\tMXHost []string\n}\n\ntype ResultPolicyJSON struct {\n\tType   PolicyType `json:\"policy-type\"`\n\tString []string   `json:\"policy-string\"`\n\tDomain string     `json:\"policy-domain\"`\n\tMXHost []string   `json:\"mx-host\"` // Example in RFC has errata, it originally was a single string. ../rfc/8460-eid6241 ../rfc/8460:1779\n}\n\n// PolicyType indicates the policy success/failure results are for.\ntype PolicyType string\n\nconst (\n\t// For DANE, against a mail host (not recipient domain).\n\tTLSA PolicyType = \"tlsa\"\n\n\t// For MTA-STS, against a recipient domain (not a mail host).\n\tSTS PolicyType = \"sts\"\n\n\t// Recipient domain did not have MTA-STS policy, or mail host (TSLA base domain)\n\t// did not have DANE TLSA records.\n\tNoPolicyFound PolicyType = \"no-policy-found\"\n\t// todo spec: ../rfc/8460:440 ../rfc/8460:697 suggest to replace with values like \"no-sts-found\" and \"no-tlsa-found\" to make it explicit which policy isn't found. also easier to implement, because you don't have to handle leaving out an sts no-policy-found result for a mail host when a tlsa policy is present.\n)\n\nfunc (rp ResultPolicy) equal(orp ResultPolicy) bool {\n\treturn rp.Type == orp.Type && slices.Equal(rp.String, orp.String) && rp.Domain == orp.Domain && slices.Equal(rp.MXHost, orp.MXHost)\n}\n\ntype Summary struct {\n\tTotalSuccessfulSessionCount int64\n\tTotalFailureSessionCount    int64\n}\n\ntype SummaryJSON struct {\n\tTotalSuccessfulSessionCount int64 `json:\"total-successful-session-count\"`\n\tTotalFailureSessionCount    int64 `json:\"total-failure-session-count\"`\n}\n\n// ResultType represents a TLS error.\ntype ResultType string\n\n// ../rfc/8460:1377\n// https://www.iana.org/assignments/starttls-validation-result-types/starttls-validation-result-types.xhtml\n\nconst (\n\tResultSTARTTLSNotSupported    ResultType = \"starttls-not-supported\"\n\tResultCertificateHostMismatch ResultType = \"certificate-host-mismatch\"\n\tResultCertificateExpired      ResultType = \"certificate-expired\"\n\tResultTLSAInvalid             ResultType = \"tlsa-invalid\"\n\tResultDNSSECInvalid           ResultType = \"dnssec-invalid\"\n\tResultDANERequired            ResultType = \"dane-required\"\n\tResultCertificateNotTrusted   ResultType = \"certificate-not-trusted\"\n\tResultSTSPolicyInvalid        ResultType = \"sts-policy-invalid\"\n\tResultSTSWebPKIInvalid        ResultType = \"sts-webpki-invalid\"\n\tResultValidationFailure       ResultType = \"validation-failure\" // Other error.\n\tResultSTSPolicyFetch          ResultType = \"sts-policy-fetch-error\"\n)\n\n// todo spec: ../rfc/8460:719 more of these fields should be optional. some sts failure details, like failed policy fetches, won't have an ip or mx, the failure happens earlier in the delivery process.\n\ntype FailureDetails struct {\n\tResultType            ResultType\n\tSendingMTAIP          string\n\tReceivingMXHostname   string\n\tReceivingMXHelo       string\n\tReceivingIP           string\n\tFailedSessionCount    int64\n\tAdditionalInformation string\n\tFailureReasonCode     string\n}\n\nfunc (v FailureDetails) Convert() FailureDetailsJSON { return FailureDetailsJSON(v) }\n\ntype FailureDetailsJSON struct {\n\tResultType            ResultType `json:\"result-type\"`\n\tSendingMTAIP          string     `json:\"sending-mta-ip\"`\n\tReceivingMXHostname   string     `json:\"receiving-mx-hostname\"`\n\tReceivingMXHelo       string     `json:\"receiving-mx-helo,omitempty\"`\n\tReceivingIP           string     `json:\"receiving-ip\"`\n\tFailedSessionCount    int64      `json:\"failed-session-count\"`\n\tAdditionalInformation string     `json:\"additional-information\"`\n\tFailureReasonCode     string     `json:\"failure-reason-code\"`\n}\n\nfunc (v FailureDetailsJSON) Convert() FailureDetails { return FailureDetails(v) }\n\n// equalKey returns whether FailureDetails have the same values, expect for\n// FailedSessionCount. Useful for aggregating FailureDetails.\nfunc (fd FailureDetails) equalKey(ofd FailureDetails) bool {\n\tfd.FailedSessionCount = 0\n\tofd.FailedSessionCount = 0\n\treturn fd == ofd\n}\n\n// Details is a convenience function to compose a FailureDetails.\nfunc Details(t ResultType, r string) FailureDetails {\n\treturn FailureDetails{ResultType: t, FailedSessionCount: 1, FailureReasonCode: r}\n}\n\nvar invalidReasons = map[x509.InvalidReason]string{\n\tx509.NotAuthorizedToSign:           \"not-authorized-to-sign\",\n\tx509.Expired:                       \"certificate-expired\",\n\tx509.CANotAuthorizedForThisName:    \"ca-not-authorized-for-this-name\",\n\tx509.TooManyIntermediates:          \"too-many-intermediates\",\n\tx509.IncompatibleUsage:             \"incompatible-key-usage\",\n\tx509.NameMismatch:                  \"parent-subject-child-issuer-mismatch\",\n\tx509.NameConstraintsWithoutSANs:    \"name-constraint-without-sans\",\n\tx509.UnconstrainedName:             \"unconstrained-name\",\n\tx509.TooManyConstraints:            \"too-many-constraints\",\n\tx509.CANotAuthorizedForExtKeyUsage: \"ca-not-authorized-for-ext-key-usage\",\n}\n\n// TLSFailureDetails turns errors encountered during TLS handshakes into a result\n// type and failure reason code for use with FailureDetails.\n//\n// Errors from crypto/tls, including local and remote alerts, from crypto/x509,\n// and generic i/o and timeout errors are recognized.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Parse parses a Report.\n// The maximum size is 20MB.\nfunc Parse(r io.Reader) (*ReportJSON, error) {\n\tr = &moxio.LimitReader{R: r, Limit: 20 * 1024 * 1024}\n\tvar report ReportJSON\n\tif err := json.NewDecoder(r).Decode(&report); err != nil {\n\t\treturn nil, err\n\t}\n\t// note: there may be leftover data, we ignore it.\n\treturn &report, nil\n}\n\n// ParseMessage parses a Report from a mail message.\n// The maximum size of the message is 15MB, the maximum size of the\n// decompressed report is 20MB.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc parseMessageReport(log mlog.Log, p message.Part, allow bool) (*ReportJSON, error) {\n\tif p.MediaType != \"MULTIPART\" {\n\t\tif !allow {\n\t\t\treturn nil, ErrNoReport\n\t\t}\n\t\treturn parseReport(p)\n\t}\n\n\tfor {\n\t\tsp, err := p.ParseNextPart(log.Logger)\n\t\tif err == io.EOF {\n\t\t\treturn nil, ErrNoReport\n\t\t}\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif p.MediaSubType == \"REPORT\" && p.ContentTypeParams[\"report-type\"] != \"tlsrpt\" {\n\t\t\treturn nil, fmt.Errorf(\"unknown report-type parameter %q\", p.ContentTypeParams[\"report-type\"])\n\t\t}\n\t\treport, err := parseMessageReport(log, *sp, p.MediaSubType == \"REPORT\")\n\t\tif err == ErrNoReport {\n\t\t\tcontinue\n\t\t} else if err != nil || report != nil {\n\t\t\treturn report, err\n\t\t}\n\t}\n}\n\nfunc parseReport(p message.Part) (*ReportJSON, error) {\n\tmt := strings.ToLower(p.MediaType + \"/\" + p.MediaSubType)\n\tswitch mt {\n\tcase \"application/tlsrpt+json\":\n\t\treturn Parse(p.Reader())\n\tcase \"application/tlsrpt+gzip\":\n\t\tgzr, err := gzip.NewReader(p.Reader())\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"decoding gzip TLSRPT report: %s\", err)\n\t\t}\n\t\treturn Parse(gzr)\n\t}\n\treturn nil, ErrNoReport\n}\n\n```\n# Function Signature: func ParseMessage(elog *slog.Logger, r io.ReaderAt) (*ReportJSON, error) {\n# Natural Language Annotations: // ParseMessage parses a Report from a mail message.\n// The maximum size of the message is 15MB, the maximum size of the\n// decompressed report is 20MB.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ParseMessage(elog *slog.Logger, r io.ReaderAt) (*ReportJSON, error) {\n\tmsg, err := message.Read(r)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tlog := mlog.New(\"module\", \"tlsrpt\")\n\tif elog != nil {\n\t\tlog.Logger = elog\n\t}\n\n\treport, err := parseMessageReport(log, *msg, false)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn report, nil\n}\n```", "```go\nfunc ParseMessage(elog *slog.Logger, r io.ReaderAt) (*ReportJSON, error) {\n    p, err := message.Read(r, elog)\n    if err != nil {\n        return nil, err\n    }\n\n    if p.MediaType != \"MULTIPART\" {\n        return parseReport(p)\n    }\n\n    for {\n        sp, err := p.ParseNextPart(elog)\n        if err == io.EOF {\n            return nil, ErrNoReport\n        }\n        if err != nil {\n            return nil, err\n        }\n        if p.MediaSubType == \"REPORT\" && p.ContentTypeParams[\"report-type\"] != \"tlsrpt\" {\n            return nil, fmt.Errorf(\"unknown report-type parameter %q\", p.ContentTypeParams[\"report-type\"])\n        }\n        report, err := parseMessageReport(elog, *sp, p.MediaSubType == \"REPORT\")\n        if err == ErrNoReport {\n            continue\n        } else if err != nil || report != nil {\n            return report, err\n        }\n    }\n}\n```", "```go\nfunc ParseMessage(elog *slog.Logger, r io.ReaderAt) (*ReportJSON, error) {\n\tlog := mlog.New(elog)\n\tmsg, err := message.Read(log.Logger, r)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading message: %v\", err)\n\t}\n\t\n\t// Check if the message is too large\n\tif msg.Size > 15*1024*1024 {\n\t\treturn nil, fmt.Errorf(\"message size too large: %dMB\", msg.Size/(1024*1024))\n\t}\n\n\t// Parse the message\n\treport, err := parseMessageReport(log, *msg.Part, false)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"parsing message: %v\", err)\n\t}\n\n\treturn report, nil\n}\n```"]}, "mox-spf/received.go-Header": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/spf/received.go:\n```\npackage spf\n\nimport (\n\t\"net\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n)\n\n// ../rfc/7208:2083\n\n// Received represents a Received-SPF header with the SPF verify results, to be\n// prepended to a message.\n//\n// Example:\n//\n//\tReceived-SPF: pass (mybox.example.org: domain of\n//\t myname@example.com designates 192.0.2.1 as permitted sender)\n//\t receiver=mybox.example.org; client-ip=192.0.2.1;\n//\t envelope-from=\"myname@example.com\"; helo=foo.example.com;\ntype Received struct {\n\tResult       Status\n\tComment      string       // Additional free-form information about the verification result. Optional. Included in message header comment inside \"()\".\n\tClientIP     net.IP       // IP address of remote SMTP client, \"client-ip=\".\n\tEnvelopeFrom string       // Sender mailbox, typically SMTP MAIL FROM, but will be set to \"postmaster\" at SMTP EHLO if MAIL FROM is empty, \"envelop-from=\".\n\tHelo         dns.IPDomain // IP or host name from EHLO or HELO command, \"helo=\".\n\tProblem      string       // Optional. \"problem=\"\n\tReceiver     string       // Hostname of receiving mail server, \"receiver=\".\n\tIdentity     Identity     // The identity that was checked, \"mailfrom\" or \"helo\", for \"identity=\".\n\tMechanism    string       // Mechanism that caused the result, can be \"default\". Optional.\n}\n\n// Identity that was verified.\ntype Identity string\n\nconst (\n\tReceivedMailFrom Identity = \"mailfrom\"\n\tReceivedHELO     Identity = \"helo\"\n)\n\nfunc receivedValueEncode(s string) string {\n\tif s == \"\" {\n\t\treturn quotedString(\"\")\n\t}\n\tfor i, c := range s {\n\t\tif c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z' || c >= '0' && c <= '9' || c > 0x7f {\n\t\t\tcontinue\n\t\t}\n\t\t// ../rfc/5322:679\n\t\tconst atext = \"!#$%&'*+-/=?^_`{|}~\"\n\t\tif strings.IndexByte(atext, byte(c)) >= 0 {\n\t\t\tcontinue\n\t\t}\n\t\tif c != '.' || (i == 0 || i+1 == len(s)) {\n\t\t\treturn quotedString(s)\n\t\t}\n\t}\n\treturn s\n}\n\n// ../rfc/5322:736\nfunc quotedString(s string) string {\n\tw := &strings.Builder{}\n\tw.WriteByte('\"')\n\tfor _, c := range s {\n\t\tif c > ' ' && c < 0x7f && c != '\"' && c != '\\\\' || c > 0x7f || c == ' ' || c == '\\t' {\n\t\t\t// We allow utf-8. This should only be needed when the destination address has an\n\t\t\t// utf8 localpart, in which case we are already doing smtputf8.\n\t\t\t// We also allow unescaped space and tab. This is FWS, and the name of ABNF\n\t\t\t// production \"qcontent\" implies the FWS is not part of the string, but escaping\n\t\t\t// space and tab leads to ugly strings. ../rfc/5322:743\n\t\t\tw.WriteRune(c)\n\t\t\tcontinue\n\t\t}\n\t\tswitch c {\n\t\tcase ' ', '\\t', '\"', '\\\\':\n\t\t\tw.WriteByte('\\\\')\n\t\t\tw.WriteRune(c)\n\t\t}\n\t}\n\tw.WriteByte('\"')\n\treturn w.String()\n}\n\n// Header returns a Received-SPF header including trailing crlf that can be\n// prepended to an incoming message.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (r Received) Header() string {\n# Natural Language Annotations: // Header returns a Received-SPF header including trailing crlf that can be\n// prepended to an incoming message.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (r Received) Header() string {\n\tw := &strings.Builder{}\n\tw.WriteString(\"Received-SPF: \")\n\tw.WriteString(string(r.Result))\n\tif r.Comment != \"\" {\n\t\tw.WriteString(\" (\")\n\t\tw.WriteString(r.Comment)\n\t\tw.WriteString(\")\")\n\t}\n\tw.WriteString(\"; receiver=\")\n\tw.WriteString(receivedValueEncode(r.Receiver))\n\tw.WriteString(\"; client-ip=\")\n\tw.WriteString(receivedValueEncode(r.ClientIP.String()))\n\tw.WriteString(\"; envelope-from=\")\n\tw.WriteString(receivedValueEncode(r.EnvelopeFrom))\n\tw.WriteString(\"; helo=\")\n\tw.WriteString(receivedValueEncode(string(r.Helo)))\n\tif r.Problem != \"\" {\n\t\tw.WriteString(\"; problem=\")\n\t\tw.WriteString(receivedValueEncode(r.Problem))\n\t}\n\tw.WriteString(\"; identity=\")\n\tw.WriteString(receivedValueEncode(string(r.Identity)))\n\tif r.Mechanism != \"\" {\n\t\tw.WriteString(\"; mechanism=\")\n\t\tw.WriteString(receivedValueEncode(r.Mechanism))\n\t}\n\tw.WriteString(\"\\r\\n\")\n\treturn w.String()\n}\n```", "```go\nfunc (r Received) Header() string {\n\tw := &strings.Builder{}\n\tfmt.Fprintf(w, \"Received-SPF: %s \", r.Result)\n\n\tif r.Comment != \"\" {\n\t\tfmt.Fprintf(w, \"(%s)\", r.Comment)\n\t}\n\n\tfmt.Fprintf(w, \"receiver=%s; client-ip=%s; envelope-from=%s; helo=%s;\",\n\t\treceivedValueEncode(r.Receiver), receivedValueEncode(r.ClientIP.String()), receivedValueEncode(r.EnvelopeFrom), receivedValueEncode(string(r.Helo)))\n\n\tif r.Problem != \"\" {\n\t\tfmt.Fprintf(w, \" problem=%s;\", receivedValueEncode(r.Problem))\n\t}\n\n\tif r.Identity != \"\" {\n\t\tfmt.Fprintf(w, \" identity=%s;\", receivedValueEncode(string(r.Identity)))\n\t}\n\n\tif r.Mechanism != \"\" {\n\t\tfmt.Fprintf(w, \" mechanism=%s;\", receivedValueEncode(r.Mechanism))\n\t}\n\n\treturn w.String() + \"\\r\\n\"\n}\n```", "```go\nfunc (r Received) Header() string {\n\tw := &strings.Builder{}\n\tfmt.Fprintf(w, \"Received-SPF: %s\", r.Result)\n\tif r.Comment != \"\" {\n\t\tfmt.Fprintf(w, \" (%s)\", r.Comment)\n\t}\n\tfmt.Fprintf(w, \" receiver=%s; client-ip=%s; envelope-from=%s; helo=%s;\",\n\t\treceivedValueEncode(r.Receiver), receivedValueEncode(r.ClientIP.String()),\n\t\treceivedValueEncode(r.EnvelopeFrom), receivedValueEncode(string(r.Helo)))\n\tif r.Problem != \"\" {\n\t\tfmt.Fprintf(w, \" problem=%s\", receivedValueEncode(r.Problem))\n\t}\n\tif r.Identity != \"\" {\n\t\tfmt.Fprintf(w, \" identity=%s\", receivedValueEncode(string(r.Identity)))\n\t}\n\tif r.Mechanism != \"\" {\n\t\tfmt.Fprintf(w, \" mechanism=%s\", receivedValueEncode(r.Mechanism))\n\t}\n\tw.WriteString(\"\\r\\n\")\n\treturn w.String()\n}\n```"]}, "mox-spf/spf.go-Evaluate": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/spf/spf.go:\n```\n// Package spf implements Sender Policy Framework (SPF, RFC 7208) for verifying\n// remote mail server IPs with their published records.\n//\n// With SPF a domain can publish a policy as a DNS TXT record describing which IPs\n// are allowed to send email with SMTP with the domain in the MAIL FROM command,\n// and how to treat SMTP transactions coming from other IPs.\npackage spf\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log/slog\"\n\t\"net\"\n\t\"net/url\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/smtp\"\n\t\"github.com/mjl-/mox/stub\"\n)\n\n// The net package always returns DNS names in absolute, lower-case form. We make\n// sure we make names absolute when looking up. For verifying, we do not want to\n// verify names relative to our local search domain.\n\nvar (\n\tMetricVerify stub.HistogramVec = stub.HistogramVecIgnore{}\n)\n\n// cross-link rfc and errata\n// ../rfc/7208-eid5436 ../rfc/7208:2043\n// ../rfc/7208-eid6721 ../rfc/7208:1928\n// ../rfc/7208-eid5227 ../rfc/7208:1297\n// ../rfc/7208-eid6595 ../rfc/7208:984\n\nvar (\n\t// Lookup errors.\n\tErrName            = errors.New(\"spf: bad domain name\")\n\tErrNoRecord        = errors.New(\"spf: no txt record\")\n\tErrMultipleRecords = errors.New(\"spf: multiple spf txt records in dns\")\n\tErrDNS             = errors.New(\"spf: lookup of dns record\")\n\tErrRecordSyntax    = errors.New(\"spf: malformed spf txt record\")\n\n\t// Evaluation errors.\n\tErrTooManyDNSRequests = errors.New(\"spf: too many dns requests\")\n\tErrTooManyVoidLookups = errors.New(\"spf: too many void lookups\")\n\tErrMacroSyntax        = errors.New(\"spf: bad macro syntax\")\n)\n\nconst (\n\t// Maximum number of DNS requests to execute. This excludes some requests, such as\n\t// lookups of MX host results.\n\tdnsRequestsMax = 10\n\n\t// Maximum number of DNS lookups that result in no records before a StatusPermerror\n\t// is returned. This limit aims to prevent abuse.\n\tvoidLookupsMax = 2\n)\n\n// Status is the result of an SPF verification.\ntype Status string\n\n// ../rfc/7208:517\n// ../rfc/7208:1836\n\nconst (\n\tStatusNone      Status = \"none\"      // E.g. no DNS domain name in session, or no SPF record in DNS.\n\tStatusNeutral   Status = \"neutral\"   // Explicit statement that nothing is said about the IP, \"?\" qualifier. None and Neutral must be treated the same.\n\tStatusPass      Status = \"pass\"      // IP is authorized.\n\tStatusFail      Status = \"fail\"      // IP is exlicitly not authorized. \"-\" qualifier.\n\tStatusSoftfail  Status = \"softfail\"  // Weak statement that IP is probably not authorized, \"~\" qualifier.\n\tStatusTemperror Status = \"temperror\" // Trying again later may succeed, e.g. for temporary DNS lookup error.\n\tStatusPermerror Status = \"permerror\" // Error requiring some intervention to correct. E.g. invalid DNS record.\n)\n\n// Args are the parameters to the SPF verification algorithm (\"check_host\" in the RFC).\n//\n// All fields should be set as they can be required for macro expansions.\ntype Args struct {\n\t// RemoteIP will be checked as sender for email.\n\tRemoteIP net.IP\n\n\t// Address from SMTP MAIL FROM command. Zero values for a null reverse path (used for DSNs).\n\tMailFromLocalpart smtp.Localpart\n\tMailFromDomain    dns.Domain\n\n\t// HelloDomain is from the SMTP EHLO/HELO command.\n\tHelloDomain dns.IPDomain\n\n\tLocalIP       net.IP\n\tLocalHostname dns.Domain\n\n\t// Explanation string to use for failure. In case of \"include\", where explanation\n\t// from original domain must be used.\n\t// May be set for recursive calls.\n\texplanation *string\n\n\t// Domain to validate.\n\tdomain dns.Domain\n\n\t// Effective sender. Equal to MailFrom if non-zero, otherwise set to \"postmaster\" at HelloDomain.\n\tsenderLocalpart smtp.Localpart\n\tsenderDomain    dns.Domain\n\n\t// To enforce the limit on lookups. Initialized automatically if nil.\n\tdnsRequests *int\n\tvoidLookups *int\n}\n\n// Mocked for testing expanding \"t\" macro.\nvar timeNow = time.Now\n\n// Lookup looks up and parses an SPF TXT record for domain.\n//\n// Authentic indicates if the DNS results were DNSSEC-verified.\nfunc Lookup(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, domain dns.Domain) (rstatus Status, rtxt string, rrecord *Record, authentic bool, rerr error) {\n\tlog := mlog.New(\"spf\", elog)\n\tstart := time.Now()\n\tdefer func() {\n\t\tlog.Debugx(\"spf lookup result\", rerr,\n\t\t\tslog.Any(\"domain\", domain),\n\t\t\tslog.Any(\"status\", rstatus),\n\t\t\tslog.Any(\"record\", rrecord),\n\t\t\tslog.Duration(\"duration\", time.Since(start)))\n\t}()\n\n\t// ../rfc/7208:586\n\thost := domain.ASCII + \".\"\n\tif err := validateDNS(host); err != nil {\n\t\treturn StatusNone, \"\", nil, false, fmt.Errorf(\"%w: %s: %s\", ErrName, domain, err)\n\t}\n\n\t// Lookup spf record.\n\ttxts, result, err := dns.WithPackage(resolver, \"spf\").LookupTXT(ctx, host)\n\tif dns.IsNotFound(err) {\n\t\treturn StatusNone, \"\", nil, result.Authentic, fmt.Errorf(\"%w for %s\", ErrNoRecord, host)\n\t} else if err != nil {\n\t\treturn StatusTemperror, \"\", nil, result.Authentic, fmt.Errorf(\"%w: %s: %s\", ErrDNS, host, err)\n\t}\n\n\t// Parse the records. We only handle those that look like spf records.\n\tvar record *Record\n\tvar text string\n\tfor _, txt := range txts {\n\t\tvar isspf bool\n\t\tr, isspf, err := ParseRecord(txt)\n\t\tif !isspf {\n\t\t\t// ../rfc/7208:595\n\t\t\tcontinue\n\t\t} else if err != nil {\n\t\t\t// ../rfc/7208:852\n\t\t\treturn StatusPermerror, txt, nil, result.Authentic, fmt.Errorf(\"%w: %s\", ErrRecordSyntax, err)\n\t\t}\n\t\tif record != nil {\n\t\t\t// ../rfc/7208:576\n\t\t\treturn StatusPermerror, \"\", nil, result.Authentic, ErrMultipleRecords\n\t\t}\n\t\ttext = txt\n\t\trecord = r\n\t}\n\tif record == nil {\n\t\t// ../rfc/7208:837\n\t\treturn StatusNone, \"\", nil, result.Authentic, ErrNoRecord\n\t}\n\treturn StatusNone, text, record, result.Authentic, nil\n}\n\n// Verify checks if a remote IP is allowed to send email for a domain.\n//\n// If the SMTP \"MAIL FROM\" is set, it is used as identity (domain) to verify.\n// Otherwise, the EHLO domain is verified if it is a valid domain.\n//\n// The returned Received.Result status will always be set, regardless of whether an\n// error is returned.\n// For status Temperror and Permerror, an error is always returned.\n// For Fail, explanation may be set, and should be returned in the SMTP session if\n// it is the reason the message is rejected. The caller should ensure the\n// explanation is valid for use in SMTP, taking line length and ascii-only\n// requirement into account.\n//\n// Verify takes the maximum number of 10 DNS requests into account, and the maximum\n// of 2 lookups resulting in no records (\"void lookups\").\n//\n// Authentic indicates if the DNS results were DNSSEC-verified.\nfunc Verify(ctx context.Context, elog *slog.Logger, resolver dns.Resolver, args Args) (received Received, domain dns.Domain, explanation string, authentic bool, rerr error) {\n\tlog := mlog.New(\"spf\", elog)\n\tstart := time.Now()\n\tdefer func() {\n\t\tMetricVerify.ObserveLabels(float64(time.Since(start))/float64(time.Second), string(received.Result))\n\t\tlog.Debugx(\"spf verify result\", rerr,\n\t\t\tslog.Any(\"domain\", args.domain),\n\t\t\tslog.Any(\"ip\", args.RemoteIP),\n\t\t\tslog.Any(\"status\", received.Result),\n\t\t\tslog.String(\"explanation\", explanation),\n\t\t\tslog.Duration(\"duration\", time.Since(start)))\n\t}()\n\n\tisHello, ok := prepare(&args)\n\tif !ok {\n\t\treceived = Received{\n\t\t\tResult:       StatusNone,\n\t\t\tComment:      \"no domain, ehlo is an ip literal and mailfrom is empty\",\n\t\t\tClientIP:     args.RemoteIP,\n\t\t\tEnvelopeFrom: fmt.Sprintf(\"%s@%s\", args.senderLocalpart, args.HelloDomain.IP.String()),\n\t\t\tHelo:         args.HelloDomain,\n\t\t\tReceiver:     args.LocalHostname.ASCII,\n\t\t}\n\t\treturn received, dns.Domain{}, \"\", false, nil\n\t}\n\n\tstatus, mechanism, expl, authentic, err := checkHost(ctx, log, resolver, args)\n\tcomment := fmt.Sprintf(\"domain %s\", args.domain.ASCII)\n\tif isHello {\n\t\tcomment += \", from ehlo because mailfrom is empty\"\n\t}\n\treceived = Received{\n\t\tResult:       status,\n\t\tComment:      comment,\n\t\tClientIP:     args.RemoteIP,\n\t\tEnvelopeFrom: fmt.Sprintf(\"%s@%s\", args.senderLocalpart, args.senderDomain.ASCII), // ../rfc/7208:2090, explicitly \"sender\", not \"mailfrom\".\n\t\tHelo:         args.HelloDomain,\n\t\tReceiver:     args.LocalHostname.ASCII,\n\t\tMechanism:    mechanism,\n\t}\n\tif err != nil {\n\t\treceived.Problem = err.Error()\n\t}\n\tif isHello {\n\t\treceived.Identity = \"helo\"\n\t} else {\n\t\treceived.Identity = \"mailfrom\"\n\t}\n\treturn received, args.domain, expl, authentic, err\n}\n\n// prepare args, setting fields sender* and domain as required for checkHost.\nfunc prepare(args *Args) (isHello bool, ok bool) {\n\t// If MAIL FROM is set, that identity is used. Otherwise the EHLO identity is used.\n\t// MAIL FROM is preferred, because if we accept the message, and we have to send a\n\t// DSN, it helps to know it is a verified sender. If we would check an EHLO\n\t// identity, and it is different from the MAIL FROM, we may be sending the DSN to\n\t// an address with a domain that would not allow sending from the originating IP.\n\t// The RFC seems a bit confused, ../rfc/7208:778 implies MAIL FROM is preferred,\n\t// but ../rfc/7208:424 mentions that a MAIL FROM check can be avoided by first\n\t// doing HELO.\n\n\targs.explanation = nil\n\targs.dnsRequests = nil\n\targs.voidLookups = nil\n\tif args.MailFromDomain.IsZero() {\n\t\t// If there is on EHLO, and it is an IP, there is nothing to SPF-validate.\n\t\tif !args.HelloDomain.IsDomain() {\n\t\t\treturn false, false\n\t\t}\n\t\t// If we have a mailfrom, we also have a localpart. But for EHLO we won't. ../rfc/7208:810\n\t\targs.senderLocalpart = \"postmaster\"\n\t\targs.senderDomain = args.HelloDomain.Domain\n\t\tisHello = true\n\t} else {\n\t\targs.senderLocalpart = args.MailFromLocalpart\n\t\targs.senderDomain = args.MailFromDomain\n\t}\n\targs.domain = args.senderDomain\n\treturn isHello, true\n}\n\n// lookup spf record, then evaluate args against it.\nfunc checkHost(ctx context.Context, log mlog.Log, resolver dns.Resolver, args Args) (rstatus Status, mechanism, rexplanation string, rauthentic bool, rerr error) {\n\tstatus, _, record, rauthentic, err := Lookup(ctx, log.Logger, resolver, args.domain)\n\tif err != nil {\n\t\treturn status, \"\", \"\", rauthentic, err\n\t}\n\n\tvar evalAuthentic bool\n\trstatus, mechanism, rexplanation, evalAuthentic, rerr = evaluate(ctx, log, record, resolver, args)\n\trauthentic = rauthentic && evalAuthentic\n\treturn\n}\n\n// Evaluate evaluates the IP and names from args against the SPF DNS record for the domain.\n\n\n\n\n\n\n\n\n\n// evaluate RemoteIP against domain from args, given record.\nfunc evaluate(ctx context.Context, log mlog.Log, record *Record, resolver dns.Resolver, args Args) (rstatus Status, mechanism, rexplanation string, rauthentic bool, rerr error) {\n\tstart := time.Now()\n\tdefer func() {\n\t\tlog.Debugx(\"spf evaluate result\", rerr,\n\t\t\tslog.Int(\"dnsrequests\", *args.dnsRequests),\n\t\t\tslog.Int(\"voidlookups\", *args.voidLookups),\n\t\t\tslog.Any(\"domain\", args.domain),\n\t\t\tslog.Any(\"status\", rstatus),\n\t\t\tslog.String(\"mechanism\", mechanism),\n\t\t\tslog.String(\"explanation\", rexplanation),\n\t\t\tslog.Duration(\"duration\", time.Since(start)))\n\t}()\n\n\tif args.dnsRequests == nil {\n\t\targs.dnsRequests = new(int)\n\t\targs.voidLookups = new(int)\n\t}\n\n\t// Response is authentic until we find a non-authentic DNS response.\n\trauthentic = true\n\n\t// To4 returns nil for an IPv6 address. To16 will return an IPv4-to-IPv6-mapped address.\n\tvar remote6 net.IP\n\tremote4 := args.RemoteIP.To4()\n\tif remote4 == nil {\n\t\tremote6 = args.RemoteIP.To16()\n\t}\n\n\t// Check if ip matches remote ip, taking cidr mask into account.\n\tcheckIP := func(ip net.IP, d Directive) bool {\n\t\t// ../rfc/7208:1097\n\t\tif remote4 != nil {\n\t\t\tip4 := ip.To4()\n\t\t\tif ip4 == nil {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tones := 32\n\t\t\tif d.IP4CIDRLen != nil {\n\t\t\t\tones = *d.IP4CIDRLen\n\t\t\t}\n\t\t\tmask := net.CIDRMask(ones, 32)\n\t\t\treturn ip4.Mask(mask).Equal(remote4.Mask(mask))\n\t\t}\n\n\t\tip6 := ip.To16()\n\t\tif ip6 == nil {\n\t\t\treturn false\n\t\t}\n\t\tones := 128\n\t\tif d.IP6CIDRLen != nil {\n\t\t\tones = *d.IP6CIDRLen\n\t\t}\n\t\tmask := net.CIDRMask(ones, 128)\n\t\treturn ip6.Mask(mask).Equal(remote6.Mask(mask))\n\t}\n\n\t// Used for \"a\" and \"mx\".\n\tcheckHostIP := func(domain dns.Domain, d Directive, args *Args) (bool, Status, error) {\n\t\tips, result, err := resolver.LookupIP(ctx, \"ip\", domain.ASCII+\".\")\n\t\trauthentic = rauthentic && result.Authentic\n\t\ttrackVoidLookup(err, args)\n\t\t// If \"not found\", we must ignore the error and treat as zero records in answer. ../rfc/7208:1116\n\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\treturn false, StatusTemperror, err\n\t\t}\n\t\tfor _, ip := range ips {\n\t\t\tif checkIP(ip, d) {\n\t\t\t\treturn true, StatusPass, nil\n\t\t\t}\n\t\t}\n\t\treturn false, StatusNone, nil\n\t}\n\n\tfor _, d := range record.Directives {\n\t\tvar match bool\n\n\t\tswitch d.Mechanism {\n\t\tcase \"include\", \"a\", \"mx\", \"ptr\", \"exists\":\n\t\t\tif err := trackLookupLimits(&args); err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t}\n\n\t\tswitch d.Mechanism {\n\t\tcase \"all\":\n\t\t\t// ../rfc/7208:1127\n\t\t\tmatch = true\n\n\t\tcase \"include\":\n\t\t\t// ../rfc/7208:1143\n\t\t\tname, authentic, err := expandDomainSpecDNS(ctx, resolver, d.DomainSpec, args)\n\t\t\trauthentic = rauthentic && authentic\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"expanding domain-spec for include: %w\", err)\n\t\t\t}\n\t\t\tnargs := args\n\t\t\tnargs.domain = dns.Domain{ASCII: strings.TrimSuffix(name, \".\")}\n\t\t\tnargs.explanation = &record.Explanation // ../rfc/7208:1548\n\t\t\tstatus, _, _, authentic, err := checkHost(ctx, log, resolver, nargs)\n\t\t\trauthentic = rauthentic && authentic\n\t\t\t// ../rfc/7208:1202\n\t\t\tswitch status {\n\t\t\tcase StatusPass:\n\t\t\t\tmatch = true\n\t\t\tcase StatusTemperror:\n\t\t\t\treturn StatusTemperror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"include %q: %w\", name, err)\n\t\t\tcase StatusPermerror, StatusNone:\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"include %q resulted in status %q: %w\", name, status, err)\n\t\t\t}\n\n\t\tcase \"a\":\n\t\t\t// ../rfc/7208:1249\n\t\t\t// note: the syntax for DomainSpec hints that macros should be expanded. But\n\t\t\t// expansion is explicitly documented, and only for \"include\", \"exists\" and\n\t\t\t// \"redirect\". This reason for this could be low-effort reuse of the domain-spec\n\t\t\t// ABNF rule. It could be an oversight. We are not implementing expansion for the\n\t\t\t// mechanism for which it isn't specified.\n\t\t\thost, err := evaluateDomainSpec(d.DomainSpec, args.domain)\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\thmatch, status, err := checkHostIP(host, d, &args)\n\t\t\tif err != nil {\n\t\t\t\treturn status, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tmatch = hmatch\n\n\t\tcase \"mx\":\n\t\t\t// ../rfc/7208:1262\n\t\t\thost, err := evaluateDomainSpec(d.DomainSpec, args.domain)\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\t// Note: LookupMX can return an error and still return MX records.\n\t\t\tmxs, result, err := resolver.LookupMX(ctx, host.ASCII+\".\")\n\t\t\trauthentic = rauthentic && result.Authentic\n\t\t\ttrackVoidLookup(err, &args)\n\t\t\t// note: we handle \"not found\" simply as a result of zero mx records.\n\t\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\t\treturn StatusTemperror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tif err == nil && len(mxs) == 1 && mxs[0].Host == \".\" {\n\t\t\t\t// Explicitly no MX.\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfor i, mx := range mxs {\n\t\t\t\t// ../rfc/7208:947 says that each mx record cannot result in more than 10 DNS\n\t\t\t\t// requests. This seems independent of the overall limit of 10 DNS requests. So an\n\t\t\t\t// MX request resulting in 11 names is valid, but we must return a permerror if we\n\t\t\t\t// found no match before the 11th name.\n\t\t\t\t// ../rfc/7208:945\n\t\t\t\tif i >= 10 {\n\t\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, ErrTooManyDNSRequests\n\t\t\t\t}\n\t\t\t\t// Parsing lax (unless in pedantic mode) for MX targets with underscores as seen in the wild.\n\t\t\t\tmxd, err := dns.ParseDomainLax(strings.TrimSuffix(mx.Host, \".\"))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t\t}\n\t\t\t\thmatch, status, err := checkHostIP(mxd, d, &args)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn status, d.MechanismString(), \"\", rauthentic, err\n\t\t\t\t}\n\t\t\t\tif hmatch {\n\t\t\t\t\tmatch = hmatch\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase \"ptr\":\n\t\t\t// ../rfc/7208:1281\n\t\t\thost, err := evaluateDomainSpec(d.DomainSpec, args.domain)\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\n\t\t\trnames, result, err := resolver.LookupAddr(ctx, args.RemoteIP.String())\n\t\t\trauthentic = rauthentic && result.Authentic\n\t\t\ttrackVoidLookup(err, &args)\n\t\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\t\treturn StatusTemperror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tlookups := 0\n\t\tptrnames:\n\t\t\tfor _, rname := range rnames {\n\t\t\t\trd, err := dns.ParseDomain(strings.TrimSuffix(rname, \".\"))\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Errorx(\"bad address in ptr record\", err, slog.String(\"address\", rname))\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\t// ../rfc/7208-eid4751 ../rfc/7208:1323\n\t\t\t\tif rd.ASCII != host.ASCII && !strings.HasSuffix(rd.ASCII, \".\"+host.ASCII) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\t// ../rfc/7208:963, we must ignore entries after the first 10.\n\t\t\t\tif lookups >= 10 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tlookups++\n\t\t\t\tips, result, err := resolver.LookupIP(ctx, \"ip\", rd.ASCII+\".\")\n\t\t\t\trauthentic = rauthentic && result.Authentic\n\t\t\t\ttrackVoidLookup(err, &args)\n\t\t\t\tfor _, ip := range ips {\n\t\t\t\t\tif checkIP(ip, d) {\n\t\t\t\t\t\tmatch = true\n\t\t\t\t\t\tbreak ptrnames\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t// ../rfc/7208:1351\n\t\tcase \"ip4\":\n\t\t\tif remote4 != nil {\n\t\t\t\tmatch = checkIP(d.IP, d)\n\t\t\t}\n\t\tcase \"ip6\":\n\t\t\tif remote6 != nil {\n\t\t\t\tmatch = checkIP(d.IP, d)\n\t\t\t}\n\n\t\tcase \"exists\":\n\t\t\t// ../rfc/7208:1382\n\t\t\tname, authentic, err := expandDomainSpecDNS(ctx, resolver, d.DomainSpec, args)\n\t\t\trauthentic = rauthentic && authentic\n\t\t\tif err != nil {\n\t\t\t\treturn Status\n```\n# Function Signature: func Evaluate(ctx context.Context, elog *slog.Logger, record *Record, resolver dns.Resolver, args Args) (rstatus Status, mechanism, rexplanation string, rauthentic bool, rerr error) {\n# Natural Language Annotations: // Evaluate evaluates the IP and names from args against the SPF DNS record for the domain.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc Evaluate(ctx context.Context, elog *slog.Logger, record *Record, resolver dns.Resolver, args Args) (rstatus Status, mechanism string, rexplanation string, rauthentic bool, rerr error) {\n\tlog := mlog.New(\"spf\", elog)\n\tstart := time.Now()\n\tdefer func() {\n\t\tlog.Debugx(\"spf evaluate result\", rerr,\n\t\t\tslog.Int(\"dnsrequests\", *args.dnsRequests),\n\t\t\tslog.Int(\"voidlookups\", *args.voidLookups),\n\t\t\tslog.Any(\"domain\", args.domain),\n\t\t\tslog.Any(\"status\", rstatus),\n\t\t\tslog.String(\"mechanism\", mechanism),\n\t\t\tslog.String(\"explanation\", rexplanation),\n\t\t\tslog.Duration(\"duration\", time.Since(start)))\n\t}()\n\n\tif args.dnsRequests == nil {\n\t\targs.dnsRequests = new(int)\n\t\targs.voidLookups = new(int)\n\t}\n\n\t// Response is authentic until we find a non-authentic DNS response.\n\trauthentic = true\n\n\t// To4 returns nil for an IPv6 address. To16 will return an IPv4-to-IPv6-mapped address.\n\tvar remote6 net.IP\n\tremote4 := args.RemoteIP.To4()\n\tif remote4 == nil {\n\t\tremote6 = args.RemoteIP.To16()\n\t}\n\n\t// Check if ip matches remote ip, taking cidr mask into account.\n\tcheckIP := func(ip net.IP, d Directive) bool {\n\t\t// ../rfc/7208:1097\n\t\tif remote4 != nil {\n\t\t\tip4 := ip.To4()\n\t\t\tif ip4 == nil {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tones := 32\n\t\t\tif d.IP4CIDRLen != nil {\n\t\t\t\tones = *d.IP4CIDRLen\n\t\t\t}\n\t\t\tmask := net.CIDRMask(ones, 32)\n\t\t\treturn ip4.Mask(mask).Equal(remote4.Mask(mask))\n\t\t}\n\n\t\tip6 := ip.To16()\n\t\tif ip6 == nil {\n\t\t\treturn false\n\t\t}\n\t\tones := 128\n\t\tif d.IP6CIDRLen != nil {\n\t\t\tones = *d.IP6CIDRLen\n\t\t}\n\t\tmask := net.CIDRMask(ones, 128)\n\t\treturn ip6.Mask(mask).Equal(remote6.Mask(mask))\n\t}\n\n\t// Used for \"a\" and \"mx\".\n\tcheckHostIP := func(domain dns.Domain, d Directive, args *Args) (bool, Status, error) {\n\t\tips, result, err := resolver.LookupIP(ctx, \"ip\", domain.ASCII+\".\")\n\t\trauthentic = rauthentic && result.Authentic\n\t\ttrackVoidLookup(err, args)\n\t\t// If \"not found\", we must ignore the error and treat as zero records in answer. ../rfc/7208:1116\n\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\treturn false, StatusTemperror, err\n\t\t}\n\t\tfor _, ip := range ips {\n\t\t\tif checkIP(ip, d) {\n\t\t\t\treturn true, StatusPass, nil\n\t\t\t}\n\t\t}\n\t\treturn false, StatusNone, nil\n\t}\n\n\tfor _, d := range record.Directives {\n\t\tvar match bool\n\n\t\tswitch d.Mechanism {\n\t\tcase \"include\", \"a\", \"mx\", \"ptr\", \"exists\":\n\t\t\tif err := trackLookupLimits(&args); err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t}\n\n\t\tswitch d.Mechanism {\n\t\tcase \"all\":\n\t\t\t// ../rfc/7208:1127\n\t\t\tmatch = true\n\n\t\tcase \"include\":\n\t\t\t// ../rfc/7208:1143\n\t\t\tname, authentic, err := expandDomainSpecDNS(ctx, resolver, d.DomainSpec, args)\n\t\t\trauthentic = rauthentic && authentic\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"expanding domain-spec for include: %w\", err)\n\t\t\t}\n\t\t\tnargs := args\n\t\t\tnargs.domain = dns.Domain{ASCII: strings.TrimSuffix(name, \".\")}\n\t\t\tnargs.explanation = &record.Explanation // ../rfc/7208:1548\n\t\t\tstatus, _, _, authentic, err := checkHost(ctx, log, resolver, nargs)\n\t\t\trauthentic = rauthentic && authentic\n\t\t\t// ../rfc/7208:1202\n\t\t\tswitch status {\n\t\t\tcase StatusPass:\n\t\t\t\tmatch = true\n\t\t\tcase StatusTemperror:\n\t\t\t\treturn StatusTemperror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"include %q: %w\", name, err)\n\t\t\tcase StatusPermerror, StatusNone:\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"include %q resulted in status %q: %w\", name, status, err)\n\t\t\t}\n\n\t\tcase \"a\":\n\t\t\t// ../rfc/7208:1249\n\t\t\thost, err := evaluateDomainSpec(d.DomainSpec, args.domain)\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\thmatch, status, err := checkHostIP(host, d, &args)\n\t\t\tif err != nil {\n\t\t\t\treturn status, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tmatch = hmatch\n\n\t\tcase \"mx\":\n\t\t\t// ../rfc/7208:1262\n\t\t\thost, err := evaluateDomainSpec(d.DomainSpec, args.domain)\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\t// Note: LookupMX can return an error and still return MX records.\n\t\t\tmxs, result, err := resolver.LookupMX(ctx, host.ASCII+\".\")\n\t\t\trauthentic = rauthentic && result.Authentic\n\t\t\ttrackVoidLookup(err, &args)\n\t\t\t// note: we handle \"not found\" simply as a result of zero mx records.\n\t\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\t\treturn StatusTemperror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tif err == nil && len(mxs) == 1 && mxs[0].Host == \".\" {", "```go\nfunc Evaluate(ctx context.Context, elog *slog.Logger, record *Record, resolver dns.Resolver, args Args) (rstatus Status, mechanism string, rexplanation string, rauthentic bool, rerr error) {\n\tlog := mlog.New(\"spf\", elog)\n\tstart := time.Now()\n\tdefer func() {\n\t\tlog.Debugx(\"spf evaluate result\", rerr,\n\t\t\tslog.Int(\"dnsrequests\", *args.dnsRequests),\n\t\t\tslog.Int(\"voidlookups\", *args.voidLookups),\n\t\t\tslog.Any(\"domain\", args.domain),\n\t\t\tslog.Any(\"status\", rstatus),\n\t\t\tslog.String(\"mechanism\", mechanism),\n\t\t\tslog.String(\"explanation\", rexplanation),\n\t\t\tslog.Duration(\"duration\", time.Since(start)))\n\t}()\n\n\tif args.dnsRequests == nil {\n\t\targs.dnsRequests = new(int)\n\t\targs.voidLookups = new(int)\n\t}\n\n\t// Response is authentic until we find a non-authentic DNS response.\n\trauthentic = true\n\n\t// To4 returns nil for an IPv6 address. To16 will return an IPv4-to-IPv6-mapped address.\n\tvar remote6 net.IP\n\tremote4 := args.RemoteIP.To4()\n\tif remote4 == nil {\n\t\tremote6 = args.RemoteIP.To16()\n\t}\n\n\t// Check if ip matches remote ip, taking cidr mask into account.\n\tcheckIP := func(ip net.IP, d Directive) bool {\n\t\t// ../rfc/7208:1097\n\t\tif remote4 != nil {\n\t\t\tip4 := ip.To4()\n\t\t\tif ip4 == nil {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tones := 32\n\t\t\tif d.IP4CIDRLen != nil {\n\t\t\t\tones = *d.IP4CIDRLen\n\t\t\t}\n\t\t\tmask := net.CIDRMask(ones, 32)\n\t\t\treturn ip4.Mask(mask).Equal(remote4.Mask(mask))\n\t\t}\n\n\t\tip6 := ip.To16()\n\t\tif ip6 == nil {\n\t\t\treturn false\n\t\t}\n\t\tones := 128\n\t\tif d.IP6CIDRLen != nil {\n\t\t\tones = *d.IP6CIDRLen\n\t\t}\n\t\tmask := net.CIDRMask(ones, 128)\n\t\treturn ip6.Mask(mask).Equal(remote6.Mask(mask))\n\t}\n\n\t// Used for \"a\" and \"mx\".\n\tcheckHostIP := func(domain dns.Domain, d Directive, args *Args) (bool, Status, error) {\n\t\tips, result, err := resolver.LookupIP(ctx, \"ip\", domain.ASCII+\".\")\n\t\trauthentic = rauthentic && result.Authentic\n\t\ttrackVoidLookup(err, args)\n\t\t// If \"not found\", we must ignore the error and treat as zero records in answer. ../rfc/7208:1116\n\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\treturn false, StatusTemperror, err\n\t\t}\n\t\tfor _, ip := range ips {\n\t\t\tif checkIP(ip, d) {\n\t\t\t\treturn true, StatusPass, nil\n\t\t\t}\n\t\t}\n\t\treturn false, StatusNone, nil\n\t}\n\n\tfor _, d := range record.Directives {\n\t\tvar match bool\n\n\t\tswitch d.Mechanism {\n\t\tcase \"include\", \"a\", \"mx\", \"ptr\", \"exists\":\n\t\t\tif err := trackLookupLimits(&args); err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t}\n\n\t\tswitch d.Mechanism {\n\t\tcase \"all\":\n\t\t\t// ../rfc/7208:1127\n\t\t\tmatch = true\n\n\t\tcase \"include\":\n\t\t\t// ../rfc/7208:1143\n\t\t\tname, authentic, err := expandDomainSpecDNS(ctx, resolver, d.DomainSpec, args)\n\t\t\trauthentic = rauthentic && authentic\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"expanding domain-spec for include: %w\", err)\n\t\t\t}\n\t\t\tnargs := args\n\t\t\tnargs.domain = dns.Domain{ASCII: strings.TrimSuffix(name, \".\")}\n\t\t\tnargs.explanation = &record.Explanation // ../rfc/7208:1548\n\t\t\tstatus, _, _, authentic, err := checkHost(ctx, log, resolver, nargs)\n\t\t\trauthentic = rauthentic && authentic\n\t\t\t// ../rfc/7208:1202\n\t\t\tswitch status {\n\t\t\tcase StatusPass:\n\t\t\t\tmatch = true\n\t\t\tcase StatusTemperror:\n\t\t\t\treturn StatusTemperror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"include %q: %w\", name, err)\n\t\t\tcase StatusPermerror, StatusNone:\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"include %q resulted in status %q: %w\", name, status, err)\n\t\t\t}\n\n\t\tcase \"a\":\n\t\t\t// ../rfc/7208:1249\n\t\t\thost, err := evaluateDomainSpec(d.DomainSpec, args.domain)\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\thmatch, status, err := checkHostIP(host, d, &args)\n\t\t\tif err != nil {\n\t\t\t\treturn status, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tmatch = hmatch\n\n\t\tcase \"mx\":\n\t\t\t// ../rfc/7208:1262\n\t\t\thost, err := evaluateDomainSpec(d.DomainSpec, args.domain)\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\t// Note: LookupMX can return an error and still return MX records.\n\t\t\tmxs, result, err := resolver.LookupMX(ctx, host.ASCII+\".\")\n\t\t\trauthentic = rauthentic && result.Authentic\n\t\t\ttrackVoidLookup(err, &args)\n\t\t\t// note: we handle \"not found\" simply as a result of zero mx records.\n\t\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\t\treturn StatusTemperror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tif err == nil && len(mxs) == 1 && mxs[0].Host == \".\" {", "```go\nfunc Evaluate(ctx context.Context, elog *slog.Logger, record *Record, resolver dns.Resolver, args Args) (rstatus Status, mechanism string, rexplanation string, rauthentic bool, rerr error) {\n\tstart := time.Now()\n\tdefer func() {\n\t\tlog.Debugx(\"spf evaluate result\", rerr,\n\t\t\tslog.Int(\"dnsrequests\", *args.dnsRequests),\n\t\t\tslog.Int(\"voidlookups\", *args.voidLookups),\n\t\t\tslog.Any(\"domain\", args.domain),\n\t\t\tslog.Any(\"status\", rstatus),\n\t\t\tslog.String(\"mechanism\", mechanism),\n\t\t\tslog.String(\"explanation\", rexplanation),\n\t\t\tslog.Duration(\"duration\", time.Since(start)))\n\t}()\n\n\tif args.dnsRequests == nil {\n\t\targs.dnsRequests = new(int)\n\t\targs.voidLookups = new(int)\n\t}\n\n\t// Response is authentic until we find a non-authentic DNS response.\n\trauthentic = true\n\n\t// To4 returns nil for an IPv6 address. To16 will return an IPv4-to-IPv6-mapped address.\n\tvar remote6 net.IP\n\tremote4 := args.RemoteIP.To4()\n\tif remote4 == nil {\n\t\tremote6 = args.RemoteIP.To16()\n\t}\n\n\t// Check if ip matches remote ip, taking cidr mask into account.\n\tcheckIP := func(ip net.IP, d Directive) bool {\n\t\tif remote4 != nil {\n\t\t\tip4 := ip.To4()\n\t\t\tif ip4 == nil {\n\t\t\t\treturn false\n\t\t\t}\n\t\t\tones := 32\n\t\t\tif d.IP4CIDRLen != nil {\n\t\t\t\tones = *d.IP4CIDRLen\n\t\t\t}\n\t\t\tmask := net.CIDRMask(ones, 32)\n\t\t\treturn ip4.Mask(mask).Equal(remote4.Mask(mask))\n\t\t}\n\n\t\tip6 := ip.To16()\n\t\tif ip6 == nil {\n\t\t\treturn false\n\t\t}\n\t\tones := 128\n\t\tif d.IP6CIDRLen != nil {\n\t\t\tones = *d.IP6CIDRLen\n\t\t}\n\t\tmask := net.CIDRMask(ones, 128)\n\t\treturn ip6.Mask(mask).Equal(remote6.Mask(mask))\n\t}\n\n\t// Used for \"a\" and \"mx\".\n\tcheckHostIP := func(domain dns.Domain, d Directive, args *Args) (bool, Status, error) {\n\t\tips, result, err := resolver.LookupIP(ctx, \"ip\", domain.ASCII+\".\")\n\t\trauthentic = rauthentic && result.Authentic\n\t\ttrackVoidLookup(err, args)\n\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\treturn false, StatusTemperror, err\n\t\t}\n\t\tfor _, ip := range ips {\n\t\t\tif checkIP(ip, d) {\n\t\t\t\treturn true, StatusPass, nil\n\t\t\t}\n\t\t}\n\t\treturn false, StatusNone, nil\n\t}\n\n\tfor _, d := range record.Directives {\n\t\tvar match bool\n\n\t\tswitch d.Mechanism {\n\t\tcase \"include\", \"a\", \"mx\", \"ptr\", \"exists\":\n\t\t\tif err := trackLookupLimits(&args); err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t}\n\n\t\tswitch d.Mechanism {\n\t\tcase \"all\":\n\t\t\tmatch = true\n\n\t\tcase \"include\":\n\t\t\tname, authentic, err := expandDomainSpecDNS(ctx, resolver, d.DomainSpec, args)\n\t\t\trauthentic = rauthentic && authentic\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"expanding domain-spec for include: %w\", err)\n\t\t\t}\n\t\t\tnargs := args\n\t\t\tnargs.domain = dns.Domain{ASCII: strings.TrimSuffix(name, \".\")}\n\t\t\tnargs.explanation = &record.Explanation\n\t\t\tstatus, _, _, authentic, err := checkHost(ctx, log, resolver, nargs)\n\t\t\trauthentic = rauthentic && authentic\n\t\t\tswitch status {\n\t\t\tcase StatusPass:\n\t\t\t\tmatch = true\n\t\t\tcase StatusTemperror:\n\t\t\t\treturn StatusTemperror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"include %q: %w\", name, err)\n\t\t\tcase StatusPermerror, StatusNone:\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, fmt.Errorf(\"include %q resulted in status %q: %w\", name, status, err)\n\t\t\t}\n\n\t\tcase \"a\":\n\t\t\thost, err := evaluateDomainSpec(d.DomainSpec, args.domain)\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\thmatch, status, err := checkHostIP(host, d, &args)\n\t\t\tif err != nil {\n\t\t\t\treturn status, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tmatch = hmatch\n\n\t\tcase \"mx\":\n\t\t\thost, err := evaluateDomainSpec(d.DomainSpec, args.domain)\n\t\t\tif err != nil {\n\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tmxs, result, err := resolver.LookupMX(ctx, host.ASCII+\".\")\n\t\t\trauthentic = rauthentic && result.Authentic\n\t\t\ttrackVoidLookup(err, &args)\n\t\t\tif err != nil && !dns.IsNotFound(err) {\n\t\t\t\treturn StatusTemperror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t}\n\t\t\tif err == nil && len(mxs) == 1 && mxs[0].Host == \".\" {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfor i, mx := range mxs {\n\t\t\t\tif i >= 10 {\n\t\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, ErrTooManyDNSRequests\n\t\t\t\t}\n\t\t\t\tmxd, err := dns.ParseDomainLax(strings.TrimSuffix(mx.Host, \".\"))\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn StatusPermerror, d.MechanismString(), \"\", rauthentic, err\n\t\t\t\t}\n\t\t\t\thmatch, status, err := checkHostIP(mxd, d, &args)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn status, d.MechanismString(), \"\", rauthentic, err\n\t\t\t\t}\n\t\t\t\tif hmatch {\n\t\t\t\t\tmatch = hmatch\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase \"ptr\":\n\t\t\thost, err := evaluateDomain"]}, "mox-spf/parse.go-Record": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/spf/parse.go:\n```\npackage spf\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strconv\"\n\t\"strings\"\n)\n\n// Record is a parsed SPF DNS record.\n//\n// An example record for example.com:\n//\n//\tv=spf1 +mx a:colo.example.com/28 -all\ntype Record struct {\n\tVersion     string      // Must be \"spf1\".\n\tDirectives  []Directive // An IP is evaluated against each directive until a match is found.\n\tRedirect    string      // Modifier that redirects SPF checks to other domain after directives did not match. Optional. For \"redirect=\".\n\tExplanation string      // Modifier for creating a user-friendly error message when an IP results in status \"fail\".\n\tOther       []Modifier  // Other modifiers.\n}\n\n// Directive consists of a mechanism that describes how to check if an IP matches,\n// an (optional) qualifier indicating the policy for a match, and optional\n// parameters specific to the mechanism.\ntype Directive struct {\n\tQualifier  string // Sets the result if this directive matches. \"\" and \"+\" are \"pass\", \"-\" is \"fail\", \"?\" is \"neutral\", \"~\" is \"softfail\".\n\tMechanism  string // \"all\", \"include\", \"a\", \"mx\", \"ptr\", \"ip4\", \"ip6\", \"exists\".\n\tDomainSpec string // For include, a, mx, ptr, exists. Always in lower-case when parsed using ParseRecord.\n\tIP         net.IP `json:\"-\"` // For ip4, ip6.\n\tIPstr      string // Original string for IP, always with /subnet.\n\tIP4CIDRLen *int   // For a, mx, ip4.\n\tIP6CIDRLen *int   // For a, mx, ip6.\n}\n\n// MechanismString returns a directive in string form for use in the Received-SPF header.\nfunc (d Directive) MechanismString() string {\n\ts := d.Qualifier + d.Mechanism\n\tif d.DomainSpec != \"\" {\n\t\ts += \":\" + d.DomainSpec\n\t} else if d.IP != nil {\n\t\ts += \":\" + d.IP.String()\n\t}\n\tif d.IP4CIDRLen != nil {\n\t\ts += fmt.Sprintf(\"/%d\", *d.IP4CIDRLen)\n\t}\n\tif d.IP6CIDRLen != nil {\n\t\tif d.Mechanism != \"ip6\" {\n\t\t\ts += \"/\"\n\t\t}\n\t\ts += fmt.Sprintf(\"/%d\", *d.IP6CIDRLen)\n\t}\n\treturn s\n}\n\n// Modifier provides additional information for a policy.\n// \"redirect\" and \"exp\" are not represented as a Modifier but explicitly in a Record.\ntype Modifier struct {\n\tKey   string // Key is case-insensitive.\n\tValue string\n}\n\n// Record returns an DNS record, to be configured as a TXT record for a domain,\n// e.g. a TXT record for example.com.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype parser struct {\n\ts     string\n\tlower string\n\to     int\n}\n\ntype parseError string\n\nfunc (e parseError) Error() string {\n\treturn string(e)\n}\n\n// toLower lower cases bytes that are A-Z. strings.ToLower does too much. and\n// would replace invalid bytes with unicode replacement characters, which would\n// break our requirement that offsets into the original and upper case strings\n// point to the same character.\nfunc toLower(s string) string {\n\tr := []byte(s)\n\tfor i, c := range r {\n\t\tif c >= 'A' && c <= 'Z' {\n\t\t\tr[i] = c + 0x20\n\t\t}\n\t}\n\treturn string(r)\n}\n\n// ParseRecord parses an SPF DNS TXT record.\nfunc ParseRecord(s string) (r *Record, isspf bool, rerr error) {\n\tp := parser{s: s, lower: toLower(s)}\n\n\tr = &Record{\n\t\tVersion: \"spf1\",\n\t}\n\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\tif err, ok := x.(parseError); ok {\n\t\t\trerr = err\n\t\t\treturn\n\t\t}\n\t\tpanic(x)\n\t}()\n\n\tp.xtake(\"v=spf1\")\n\tfor !p.empty() {\n\t\tp.xtake(\" \")\n\t\tisspf = true // ../rfc/7208:825\n\t\tfor p.take(\" \") {\n\t\t}\n\t\tif p.empty() {\n\t\t\tbreak\n\t\t}\n\n\t\tqualifier := p.takelist(\"+\", \"-\", \"?\", \"~\")\n\t\tmechanism := p.takelist(\"all\", \"include:\", \"a\", \"mx\", \"ptr\", \"ip4:\", \"ip6:\", \"exists:\")\n\t\tif qualifier != \"\" && mechanism == \"\" {\n\t\t\tp.xerrorf(\"expected mechanism after qualifier\")\n\t\t}\n\t\tif mechanism == \"\" {\n\t\t\t// ../rfc/7208:2597\n\t\t\tmodifier := p.takelist(\"redirect=\", \"exp=\")\n\t\t\tif modifier == \"\" {\n\t\t\t\t// ../rfc/7208:2600\n\t\t\t\tname := p.xtakefn1(func(c rune, i int) bool {\n\t\t\t\t\talpha := c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z'\n\t\t\t\t\treturn alpha || i > 0 && (c >= '0' && c <= '9' || c == '-' || c == '_' || c == '.')\n\t\t\t\t})\n\t\t\t\tp.xtake(\"=\")\n\t\t\t\tv := p.xmacroString(true)\n\t\t\t\tr.Other = append(r.Other, Modifier{name, v})\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tv := p.xdomainSpec(true)\n\t\t\tmodifier = strings.TrimSuffix(modifier, \"=\")\n\t\t\tif modifier == \"redirect\" {\n\t\t\t\tif r.Redirect != \"\" {\n\t\t\t\t\t// ../rfc/7208:1419\n\t\t\t\t\tp.xerrorf(\"duplicate redirect modifier\")\n\t\t\t\t}\n\t\t\t\tr.Redirect = v\n\t\t\t}\n\t\t\tif modifier == \"exp\" {\n\t\t\t\tif r.Explanation != \"\" {\n\t\t\t\t\t// ../rfc/7208:1419\n\t\t\t\t\tp.xerrorf(\"duplicate exp modifier\")\n\t\t\t\t}\n\t\t\t\tr.Explanation = v\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\t// ../rfc/7208:2585\n\t\td := Directive{\n\t\t\tQualifier: qualifier,\n\t\t\tMechanism: strings.TrimSuffix(mechanism, \":\"),\n\t\t}\n\t\tswitch d.Mechanism {\n\t\tcase \"all\":\n\t\tcase \"include\":\n\t\t\td.DomainSpec = p.xdomainSpec(false)\n\t\tcase \"a\", \"mx\":\n\t\t\tif p.take(\":\") {\n\t\t\t\td.DomainSpec = p.xdomainSpec(false)\n\t\t\t}\n\t\t\tif p.take(\"/\") {\n\t\t\t\tif !p.take(\"/\") {\n\t\t\t\t\tnum, _ := p.xnumber()\n\t\t\t\t\tif num > 32 {\n\t\t\t\t\t\tp.xerrorf(\"invalid ip4 cidr length %d\", num)\n\t\t\t\t\t}\n\t\t\t\t\td.IP4CIDRLen = &num\n\t\t\t\t\tif !p.take(\"//\") {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tnum, _ := p.xnumber()\n\t\t\t\tif num > 128 {\n\t\t\t\t\tp.xerrorf(\"invalid ip6 cidr length %d\", num)\n\t\t\t\t}\n\t\t\t\td.IP6CIDRLen = &num\n\t\t\t}\n\t\tcase \"ptr\":\n\t\t\tif p.take(\":\") {\n\t\t\t\td.DomainSpec = p.xdomainSpec(false)\n\t\t\t}\n\t\tcase \"ip4\":\n\t\t\td.IP, d.IPstr = p.xip4address()\n\t\t\tif p.take(\"/\") {\n\t\t\t\tnum, _ := p.xnumber()\n\t\t\t\tif num > 32 {\n\t\t\t\t\tp.xerrorf(\"invalid ip4 cidr length %d\", num)\n\t\t\t\t}\n\t\t\t\td.IP4CIDRLen = &num\n\t\t\t\td.IPstr += fmt.Sprintf(\"/%d\", num)\n\t\t\t} else {\n\t\t\t\td.IPstr += \"/32\"\n\t\t\t}\n\t\tcase \"ip6\":\n\t\t\td.IP, d.IPstr = p.xip6address()\n\t\t\tif p.take(\"/\") {\n\t\t\t\tnum, _ := p.xnumber()\n\t\t\t\tif num > 128 {\n\t\t\t\t\tp.xerrorf(\"invalid ip6 cidr length %d\", num)\n\t\t\t\t}\n\t\t\t\td.IP6CIDRLen = &num\n\t\t\t\td.IPstr += fmt.Sprintf(\"/%d\", num)\n\t\t\t} else {\n\t\t\t\td.IPstr += \"/128\"\n\t\t\t}\n\t\tcase \"exists\":\n\t\t\td.DomainSpec = p.xdomainSpec(false)\n\t\tdefault:\n\t\t\treturn nil, true, fmt.Errorf(\"internal error, missing case for mechanism %q\", d.Mechanism)\n\t\t}\n\t\tr.Directives = append(r.Directives, d)\n\t}\n\treturn r, true, nil\n}\n\nfunc (p *parser) xerrorf(format string, args ...any) {\n\tmsg := fmt.Sprintf(format, args...)\n\tif !p.empty() {\n\t\tmsg += fmt.Sprintf(\" (leftover %q)\", p.s[p.o:])\n\t}\n\tpanic(parseError(msg))\n}\n\n// operates on original-cased characters.\nfunc (p *parser) xtakefn1(fn func(rune, int) bool) string {\n\tr := \"\"\n\tfor i, c := range p.s[p.o:] {\n\t\tif !fn(c, i) {\n\t\t\tbreak\n\t\t}\n\t\tr += string(c)\n\t}\n\tif r == \"\" {\n\t\tp.xerrorf(\"need at least 1 char\")\n\t}\n\tp.o += len(r)\n\treturn r\n}\n\n// caller should set includingSlash to false when parsing \"a\" or \"mx\", or the / would be consumed as valid macro literal.\nfunc (p *parser) xdomainSpec(includingSlash bool) string {\n\t// ../rfc/7208:1579\n\t// This also consumes the \"domain-end\" part, which we check below.\n\ts := p.xmacroString(includingSlash)\n\n\t// The ABNF says s must either end in macro-expand, or \".\" toplabel [\".\"]. The\n\t// toplabel rule implies the intention is to force a valid DNS name. We cannot just\n\t// check if the name is valid, because \"macro-expand\" is not a valid label. So we\n\t// recognize the macro-expand, and check for valid toplabel otherwise, because we\n\t// syntax errors must result in Permerror.\n\tfor _, suf := range []string{\"%%\", \"%_\", \"%-\", \"}\"} {\n\t\t// The check for \"}\" assumes a \"%{\" precedes it...\n\t\tif strings.HasSuffix(s, suf) {\n\t\t\treturn s\n\t\t}\n\t}\n\ttl := strings.Split(strings.TrimSuffix(s, \".\"), \".\")\n\tt := tl[len(tl)-1]\n\tif t == \"\" {\n\t\tp.xerrorf(\"invalid empty toplabel\")\n\t}\n\tnums := 0\n\tfor i, c := range t {\n\t\tswitch {\n\t\tcase c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z':\n\t\tcase c >= '0' && c <= '9':\n\t\t\tnums++\n\t\tcase c == '-':\n\t\t\tif i == 0 {\n\t\t\t\tp.xerrorf(\"bad toplabel, invalid leading dash\")\n\t\t\t}\n\t\t\tif i == len(t)-1 {\n\t\t\t\tp.xerrorf(\"bad toplabel, invalid trailing dash\")\n\t\t\t}\n\t\tdefault:\n\t\t\tp.xerrorf(\"bad toplabel, invalid character\")\n\t\t}\n\t}\n\tif nums == len(t) {\n\t\tp.xerrorf(\"bad toplabel, cannot be all digits\")\n\t}\n\treturn s\n}\n\nfunc (p *parser) xmacroString(includingSlash bool) string {\n\t// ../rfc/7208:1588\n\tr := \"\"\n\tfor !p.empty() {\n\t\tw := p.takelist(\"%{\", \"%%\", \"%_\", \"%-\") // \"macro-expand\"\n\t\tif w == \"\" {\n\t\t\t// \"macro-literal\"\n\t\t\tif !p.empty() {\n\t\t\t\tb := p.peekchar()\n\t\t\t\tif b > ' ' && b < 0x7f && b != '%' && (includingSlash || b != '/') {\n\t\t\t\t\tr += string(b)\n\t\t\t\t\tp.o++\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t\tr += w\n\t\tif w != \"%{\" {\n\t\t\tcontinue\n\t\t}\n\t\tr += p.xtakelist(\"s\", \"l\", \"o\", \"d\", \"i\", \"p\", \"h\", \"c\", \"r\", \"t\", \"v\") // \"macro-letter\"\n\t\tdigits := p.digits()\n\t\tif digits != \"\" {\n\t\t\tif v, err := strconv.Atoi(digits); err != nil {\n\t\t\t\tp.xerrorf(\"bad digits: %v\", err)\n\t\t\t} else if v == 0 {\n\t\t\t\tp.xerrorf(\"bad digits 0 for 0 labels\")\n\t\t\t}\n\t\t}\n\t\tr += digits\n\t\tif p.take(\"r\") {\n\t\t\tr += \"r\"\n\t\t}\n\t\tfor {\n\t\t\tdelimiter := p.takelist(\".\", \"-\", \"+\", \",\", \"/\", \"_\", \"=\")\n\t\t\tif delimiter == \"\" {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tr += delimiter\n\t\t}\n\t\tr += p.xtake(\"}\")\n\t}\n\treturn r\n}\n\nfunc (p *parser) empty() bool {\n\treturn p.o >= len(p.s)\n}\n\n// returns next original-cased character.\nfunc (p *parser) peekchar() byte {\n\treturn p.s[p.o]\n}\n\nfunc (p *parser) xtakelist(l ...string) string {\n\tw := p.takelist(l...)\n\tif w == \"\" {\n\t\tp.xerrorf(\"no match for %v\", l)\n\t}\n\treturn w\n}\n\nfunc (p *parser) takelist(l ...string) string {\n\tfor _, w := range l {\n\t\tif strings.HasPrefix(p.lower[p.o:], w) {\n\t\t\tp.o += len(w)\n\t\t\treturn w\n\t\t}\n\t}\n\treturn \"\"\n}\n\n// digits parses zero or more digits.\nfunc (p *parser) digits() string {\n\tr := \"\"\n\tfor !p.empty() {\n\t\tb := p.peekchar()\n\t\tif b >= '0' && b <= '9' {\n\t\t\tr += string(b)\n\t\t\tp.o++\n\t\t} else {\n\t\t\tbreak\n\t\t}\n\t}\n\treturn r\n}\n\nfunc (p *parser) take(s string) bool {\n\tif strings.HasPrefix(p.lower[p.o:], s) {\n\t\tp.o += len(s)\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (p *parser) xtake(s string) string {\n\tok := p.take(s)\n\tif !ok {\n\t\tp.xerrorf(\"expected %q\", s)\n\t}\n\treturn s\n}\n\nfunc (p *parser) xnumber() (int, string) {\n\ts := p.digits()\n\tif s == \"\" {\n\t\tp.xerrorf(\"expected number\")\n\t}\n\tif s == \"0\" {\n\t\treturn 0, s\n\t}\n\tif strings.HasPrefix(s, \"0\") {\n\t\tp.xerrorf(\"bogus leading 0 in number\")\n\t}\n\tv, err := strconv.Atoi(s)\n\tif err != nil {\n\t\tp.xerrorf(\"parsing number for %q: %s\", s, err)\n\t}\n\treturn v, s\n}\n\nfunc (p *parser) xip4address() (net.IP, string) {\n\t// ../rfc/7208:2607\n\tip4num := func() (byte, string) {\n\t\tv, vs := p.xnumber()\n\t\tif v > 255 {\n\t\t\tp.xerrorf(\"bad ip4 number %d\", v)\n\t\t}\n\t\treturn byte(v), vs\n\t}\n\ta, as := ip4num()\n\tp.xtake(\".\")\n\tb, bs := ip4num()\n\tp.xtake(\".\")\n\tc, cs := ip4num()\n\tp.xtake(\".\")\n\td, ds := ip4num()\n\treturn net.IPv4(a, b, c, d), as + \".\" + bs + \".\" + cs + \".\" + ds\n}\n\nfunc (p *parser) xip6address() (net.IP, string) {\n\t// ../rfc/7208:2614\n\t// We just take in a string that has characters that IPv6 uses, then parse it.\n\ts := p.xtakefn1(func(c rune, i int) bool {\n\t\treturn c >= '0' && c <= '9' || c >= 'a' && c <= 'f' || c >= 'A' && c <= 'F' || c == ':' || c == '.'\n\t})\n\tip := net.ParseIP(s)\n\tif ip == nil {\n\t\tp.xerrorf(\"ip6 address %q not valid\", s)\n\t}\n\treturn ip, s\n}\n\n```\n# Function Signature: func (r Record) Record() (string, error) {\n# Natural Language Annotations: // Record returns an DNS record, to be configured as a TXT record for a domain,\n// e.g. a TXT record for example.com.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (r Record) Record() (string, error) {\n\tif r.Version != \"spf1\" {\n\t\treturn \"\", fmt.Errorf(\"invalid SPF record version: %s\", r.Version)\n\t}\n\n\ts := \"v=spf1\"\n\tfor _, d := range r.Directives {\n\t\ts += \" \" + d.MechanismString()\n\t}\n\tif r.Redirect != \"\" {\n\t\ts += \" redirect=\" + r.Redirect\n\t}\n\tif r.Explanation != \"\" {\n\t\ts += \" exp=\" + r.Explanation\n\t}\n\tfor _, m := range r.Other {\n\t\ts += \" \" + m.Key + \"=\" + m.Value\n\t}\n\n\treturn s, nil\n}\n```\nThis function generates an SPF DNS TXT record from the `Record` struct. It starts by checking that the `Version` field is \"spf1\". If it's not, it returns an error. Then it concatenates the `Version` string to \"v=spf1\". For each `Directive` in the `Directives` slice, it appends the result of calling `MechanismString()` on it. If the `Redirect` and `Explanation` fields are not empty, it appends them to the string. Finally, for each `Modifier` in the `Other` slice, it appends it to the string. The function returns the generated string and `nil` if no errors occurred.", "```go\nfunc (r Record) Record() (string, error) {\n\tvar record string\n\n\tif r.Version != \"spf1\" {\n\t\treturn \"\", fmt.Errorf(\"invalid SPF record version\")\n\t}\n\n\trecord += \"v=spf1\"\n\n\tfor _, directive := range r.Directives {\n\t\trecord += \" \" + directive.MechanismString()\n\t}\n\n\tif r.Redirect != \"\" {\n\t\trecord += \" redirect=\" + r.Redirect\n\t}\n\n\tif r.Explanation != \"\" {\n\t\trecord += \" exp=\" + r.Explanation\n\t}\n\n\tfor _, modifier := range r.Other {\n\t\trecord += \" \" + modifier.Key + \"=\" + modifier.Value\n\t}\n\n\treturn record, nil\n}\n```", "```go\nfunc (r Record) Record() (string, error) {\n\t// Record returns an DNS record, to be configured as a TXT record for a domain,\n\t// e.g. a TXT record for example.com.\n\n\tvar record string\n\tif r.Version != \"spf1\" {\n\t\treturn \"\", fmt.Errorf(\"invalid SPF version, expected 'spf1'\")\n\t}\n\n\trecord = \"v=\" + r.Version\n\n\tfor _, directive := range r.Directives {\n\t\trecord += \" \" + directive.MechanismString()\n\t}\n\n\tif r.Redirect != \"\" {\n\t\trecord += \" redirect=\" + r.Redirect\n\t}\n\n\tif r.Explanation != \"\" {\n\t\trecord += \" exp=\" + r.Explanation\n\t}\n\n\tfor _, modifier := range r.Other {\n\t\trecord += \" \" + modifier.Key + \"=\" + modifier.Value\n\t}\n\n\treturn record, nil\n}\n```"]}, "mox-message/part.go-EnsurePart": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/message/part.go:\n```\npackage message\n\n// todo: allow more invalid content-type values, we now stop parsing on: empty media type (eg \"content-type: ; name=...\"), empty value for property (eg \"charset=\", missing quotes for characters that should be quoted (eg boundary containing \"=\" but without quotes), duplicate properties (two charsets), empty pairs (eg \"text/html;;\").\n// todo: should we be forgiving when closing boundary in multipart message is missing? seems like spam messages do this...\n// todo: should we allow base64 messages where a line starts with a space? and possibly more whitespace. is happening in messages. coreutils base64 accepts it, encoding/base64 does not.\n// todo: handle comments in headers?\n// todo: should we just always store messages with \\n instead of \\r\\n? \\r\\n seems easier for use with imap.\n// todo: can use a cleanup\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"mime/quotedprintable\"\n\t\"net/mail\"\n\t\"net/textproto\"\n\t\"strings\"\n\t\"time\"\n\n\t\"golang.org/x/text/encoding/ianaindex\"\n\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// Pedantic enables stricter parsing.\nvar Pedantic bool\n\nvar (\n\tErrBadContentType = errors.New(\"bad content-type\")\n)\n\nvar (\n\terrNotMultipart           = errors.New(\"not a multipart message\")\n\terrFirstBoundCloses       = errors.New(\"first boundary cannot be finishing boundary\")\n\terrLineTooLong            = errors.New(\"line too long\")\n\terrMissingBoundaryParam   = errors.New(\"missing/empty boundary content-type parameter\")\n\terrMissingClosingBoundary = errors.New(\"eof without closing boundary\")\n\terrBareLF                 = errors.New(\"invalid bare line feed\")\n\terrBareCR                 = errors.New(\"invalid bare carriage return\")\n\terrUnexpectedEOF          = errors.New(\"unexpected eof\")\n)\n\n// If set, during tests, attempts to reparse a part will cause an error, because sequentially reading parts should not lead to reparsing.\nvar enforceSequential bool\n\n// Part represents a whole mail message, or a part of a multipart message. It\n// is designed to handle IMAP requirements efficiently.\ntype Part struct {\n\tBoundaryOffset int64 // Offset in message where bound starts. -1 for top-level message.\n\tHeaderOffset   int64 // Offset in message file where header starts.\n\tBodyOffset     int64 // Offset in message file where body starts.\n\tEndOffset      int64 // Where body of part ends. Set when part is fully read.\n\tRawLineCount   int64 // Number of lines in raw, undecoded, body of part. Set when part is fully read.\n\tDecodedSize    int64 // Number of octets when decoded. If this is a text mediatype, lines ending only in LF are changed end in CRLF and DecodedSize reflects that.\n\n\tMediaType               string            // From Content-Type, upper case. E.g. \"TEXT\". Can be empty because content-type may be absent. In this case, the part may be treated as TEXT/PLAIN.\n\tMediaSubType            string            // From Content-Type, upper case. E.g. \"PLAIN\".\n\tContentTypeParams       map[string]string // E.g. holds \"boundary\" for multipart messages. Has lower-case keys, and original case values.\n\tContentID               string\n\tContentDescription      string\n\tContentTransferEncoding string    // In upper case.\n\tEnvelope                *Envelope // Email message headers. Not for non-message parts.\n\n\tParts []Part // Parts if this is a multipart.\n\n\t// Only for message/rfc822 and message/global. This part may have a buffer as\n\t// backing io.ReaderAt, because a message/global can have a non-identity\n\t// content-transfer-encoding. This part has a nil parent.\n\tMessage *Part\n\n\tr               io.ReaderAt\n\theader          textproto.MIMEHeader // Parsed header.\n\tnextBoundOffset int64                // If >= 0, the offset where the next part header starts. We can set this when a user fully reads each part.\n\tlastBoundOffset int64                // Start of header of last/previous part. Used to skip a part if ParseNextPart is called and nextBoundOffset is -1.\n\tparent          *Part                // Parent part, for getting bound from, and setting nextBoundOffset when a part has finished reading. Only for subparts, not top-level parts.\n\tbound           []byte               // Only set if valid multipart with boundary, includes leading --, excludes \\r\\n.\n\tstrict          bool                 // If set, valid crlf line endings are verified when reading body.\n}\n\n// todo: have all Content* fields in Part?\n// todo: make Address contain a type Localpart and dns.Domain?\n// todo: if we ever make a major change and reparse all parts, switch to lower-case values if not too troublesome.\n\n// Envelope holds the basic/common message headers as used in IMAP4.\ntype Envelope struct {\n\tDate      time.Time\n\tSubject   string // Q/B-word-decoded.\n\tFrom      []Address\n\tSender    []Address\n\tReplyTo   []Address\n\tTo        []Address\n\tCC        []Address\n\tBCC       []Address\n\tInReplyTo string // From In-Reply-To header, includes <>.\n\tMessageID string // From Message-Id header, includes <>.\n}\n\n// Address as used in From and To headers.\ntype Address struct {\n\tName string // Free-form name for display in mail applications.\n\tUser string // Localpart, encoded as string. Must be parsed before using as Localpart.\n\tHost string // Domain in ASCII.\n}\n\n// Parse reads the headers of the mail message and returns a part.\n// A part provides access to decoded and raw contents of a message and its multiple parts.\n//\n// If strict is set, fewer attempts are made to continue parsing when errors are\n// encountered, such as with invalid content-type headers or bare carriage returns.\nfunc Parse(elog *slog.Logger, strict bool, r io.ReaderAt) (Part, error) {\n\tlog := mlog.New(\"message\", elog)\n\treturn newPart(log, strict, r, 0, nil)\n}\n\n// EnsurePart parses a part as with Parse, but ensures a usable part is always\n// returned, even if error is non-nil. If a parse error occurs, the message is\n// returned as application/octet-stream, and headers can still be read if they\n// were valid.\n//\n// If strict is set, fewer attempts are made to continue parsing when errors are\n// encountered, such as with invalid content-type headers or bare carriage returns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc fallbackPart(p Part, r io.ReaderAt, size int64) (Part, error) {\n\tnp := Part{\n\t\tHeaderOffset:            p.HeaderOffset,\n\t\tBodyOffset:              p.BodyOffset,\n\t\tEndOffset:               size,\n\t\tMediaType:               \"APPLICATION\",\n\t\tMediaSubType:            \"OCTET-STREAM\",\n\t\tContentTypeParams:       p.ContentTypeParams,\n\t\tContentID:               p.ContentID,\n\t\tContentDescription:      p.ContentDescription,\n\t\tContentTransferEncoding: p.ContentTransferEncoding,\n\t\tEnvelope:                p.Envelope,\n\t\t// We don't keep:\n\t\t//   - BoundaryOffset: irrelevant for top-level message.\n\t\t//   - RawLineCount and DecodedSize: set below.\n\t\t//   - Parts: we are not treating this as a multipart message.\n\t}\n\tnp.SetReaderAt(r)\n\t// By reading body, the number of lines and decoded size will be set.\n\t_, err := io.Copy(io.Discard, np.Reader())\n\treturn np, err\n}\n\n// SetReaderAt sets r as reader for this part and all its sub parts, recursively.\n// No reader is set for any Message subpart, see SetMessageReaderAt.\nfunc (p *Part) SetReaderAt(r io.ReaderAt) {\n\tif r == nil {\n\t\tpanic(\"nil reader\")\n\t}\n\tp.r = r\n\tfor i := range p.Parts {\n\t\tpp := &p.Parts[i]\n\t\tpp.SetReaderAt(r)\n\t}\n}\n\n// SetMessageReaderAt sets a reader on p.Message, which must be non-nil.\nfunc (p *Part) SetMessageReaderAt() error {\n\t// todo: if p.Message does not contain any non-identity content-transfer-encoding, we should set an offsetReader of p.Message, recursively.\n\tbuf, err := io.ReadAll(p.Reader())\n\tif err != nil {\n\t\treturn err\n\t}\n\tp.Message.SetReaderAt(bytes.NewReader(buf))\n\treturn nil\n}\n\n// Walk through message, decoding along the way, and collecting mime part offsets and sizes, and line counts.\nfunc (p *Part) Walk(elog *slog.Logger, parent *Part) error {\n\tlog := mlog.New(\"message\", elog)\n\n\tif len(p.bound) == 0 {\n\t\tif p.MediaType == \"MESSAGE\" && (p.MediaSubType == \"RFC822\" || p.MediaSubType == \"GLOBAL\") {\n\t\t\t// todo: don't read whole submessage in memory...\n\t\t\tbuf, err := io.ReadAll(p.Reader())\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tbr := bytes.NewReader(buf)\n\t\t\tmp, err := Parse(log.Logger, p.strict, br)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"parsing embedded message: %w\", err)\n\t\t\t}\n\t\t\tif err := mp.Walk(log.Logger, nil); err != nil {\n\t\t\t\t// If this is a DSN and we are not in pedantic mode, accept unexpected end of\n\t\t\t\t// message. This is quite common because MTA's sometimes just truncate the original\n\t\t\t\t// message in a place that makes the message invalid.\n\t\t\t\tif errors.Is(err, errUnexpectedEOF) && !Pedantic && parent != nil && len(parent.Parts) >= 3 && p == &parent.Parts[2] && parent.MediaType == \"MULTIPART\" && parent.MediaSubType == \"REPORT\" {\n\t\t\t\t\tmp, err = fallbackPart(mp, br, int64(len(buf)))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn fmt.Errorf(\"parsing invalid embedded message: %w\", err)\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\treturn fmt.Errorf(\"parsing parts of embedded message: %w\", err)\n\t\t\t\t}\n\t\t\t}\n\t\t\t// todo: if mp does not contain any non-identity content-transfer-encoding, we should set an offsetReader of p.r on mp, recursively.\n\t\t\tp.Message = &mp\n\t\t\treturn nil\n\t\t}\n\t\t_, err := io.Copy(io.Discard, p.Reader())\n\t\treturn err\n\t}\n\n\tfor {\n\t\tpp, err := p.ParseNextPart(log.Logger)\n\t\tif err == io.EOF {\n\t\t\treturn nil\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := pp.Walk(log.Logger, p); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n}\n\n// String returns a debugging representation of the part.\nfunc (p *Part) String() string {\n\treturn fmt.Sprintf(\"&Part{%s/%s offsets %d/%d/%d/%d lines %d decodedsize %d next %d last %d bound %q parts %v}\", p.MediaType, p.MediaSubType, p.BoundaryOffset, p.HeaderOffset, p.BodyOffset, p.EndOffset, p.RawLineCount, p.DecodedSize, p.nextBoundOffset, p.lastBoundOffset, p.bound, p.Parts)\n}\n\n// newPart parses a new part, which can be the top-level message.\n// offset is the bound offset for parts, and the start of message for top-level messages. parent indicates if this is a top-level message or sub-part.\n// If an error occurs, p's exported values can still be relevant. EnsurePart uses these values.\nfunc newPart(log mlog.Log, strict bool, r io.ReaderAt, offset int64, parent *Part) (p Part, rerr error) {\n\tif r == nil {\n\t\tpanic(\"nil reader\")\n\t}\n\tp = Part{\n\t\tBoundaryOffset: -1,\n\t\tEndOffset:      -1,\n\t\tr:              r,\n\t\tparent:         parent,\n\t\tstrict:         strict,\n\t}\n\n\tb := &bufAt{strict: strict, r: r, offset: offset}\n\n\tif parent != nil {\n\t\tp.BoundaryOffset = offset\n\t\tif line, _, err := b.ReadLine(true); err != nil {\n\t\t\treturn p, err\n\t\t} else if match, finish := checkBound(line, parent.bound); !match {\n\t\t\treturn p, fmt.Errorf(\"missing bound\")\n\t\t} else if finish {\n\t\t\treturn p, fmt.Errorf(\"new part for closing boundary\")\n\t\t}\n\t}\n\n\t// Collect header.\n\tp.HeaderOffset = b.offset\n\tp.BodyOffset = b.offset\n\thb := &bytes.Buffer{}\n\tfor {\n\t\tline, _, err := b.ReadLine(true)\n\t\tif err == io.EOF {\n\t\t\t// No body is valid.\n\t\t\tbreak\n\t\t}\n\t\tif err != nil {\n\t\t\treturn p, fmt.Errorf(\"reading header line: %w\", err)\n\t\t}\n\t\thb.Write(line)\n\t\tif len(line) == 2 {\n\t\t\tbreak // crlf\n\t\t}\n\t}\n\tp.BodyOffset = b.offset\n\n\t// Don't attempt to parse empty header, mail.ReadMessage doesn't like it.\n\tif p.HeaderOffset == p.BodyOffset {\n\t\tp.header = textproto.MIMEHeader{}\n\t} else {\n\t\th, err := parseHeader(hb)\n\t\tif err != nil {\n\t\t\treturn p, fmt.Errorf(\"parsing header: %w\", err)\n\t\t}\n\t\tp.header = h\n\t}\n\n\tct := p.header.Get(\"Content-Type\")\n\tmt, params, err := mime.ParseMediaType(ct)\n\tif err != nil && ct != \"\" {\n\t\tif Pedantic || strict {\n\t\t\treturn p, fmt.Errorf(\"%w: %s: %q\", ErrBadContentType, err, ct)\n\t\t}\n\n\t\t// Try parsing just a content-type, ignoring parameters.\n\t\t// ../rfc/2045:628\n\t\tct = strings.TrimSpace(strings.SplitN(ct, \";\", 2)[0])\n\t\tt := strings.SplitN(ct, \"/\", 2)\n\t\tisToken := func(s string) bool {\n\t\t\tconst separators = `()<>@,;:\\\\\"/[]?= ` // ../rfc/2045:663\n\t\t\tfor _, c := range s {\n\t\t\t\tif c < 0x20 || c >= 0x80 || strings.ContainsRune(separators, c) {\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn len(s) > 0\n\t\t}\n\t\t// We cannot recover content-type of multipart, we won't have a boundary.\n\t\tif len(t) == 2 && isToken(t[0]) && !strings.EqualFold(t[0], \"multipart\") && isToken(t[1]) {\n\t\t\tp.MediaType = strings.ToUpper(t[0])\n\t\t\tp.MediaSubType = strings.ToUpper(t[1])\n\t\t} else {\n\t\t\tp.MediaType = \"APPLICATION\"\n\t\t\tp.MediaSubType = \"OCTET-STREAM\"\n\t\t}\n\t\tlog.Debugx(\"malformed content-type, attempting to recover and continuing\", err,\n\t\t\tslog.String(\"contenttype\", p.header.Get(\"Content-Type\")),\n\t\t\tslog.String(\"mediatype\", p.MediaType),\n\t\t\tslog.String(\"mediasubtype\", p.MediaSubType))\n\t} else if mt != \"\" {\n\t\tt := strings.SplitN(strings.ToUpper(mt), \"/\", 2)\n\t\tif len(t) != 2 {\n\t\t\tif Pedantic || strict {\n\t\t\t\treturn p, fmt.Errorf(\"bad content-type: %q (content-type %q)\", mt, ct)\n\t\t\t}\n\t\t\tlog.Debug(\"malformed media-type, ignoring and continuing\", slog.String(\"type\", mt))\n\t\t\tp.MediaType = \"APPLICATION\"\n\t\t\tp.MediaSubType = \"OCTET-STREAM\"\n\t\t} else {\n\t\t\tp.MediaType = t[0]\n\t\t\tp.MediaSubType = t[1]\n\t\t\tp.ContentTypeParams = params\n\t\t}\n\t}\n\n\tp.ContentID = p.header.Get(\"Content-Id\")\n\tp.ContentDescription = p.header.Get(\"Content-Description\")\n\tp.ContentTransferEncoding = strings.ToUpper(p.header.Get(\"Content-Transfer-Encoding\"))\n\n\tif parent == nil {\n\t\tp.Envelope, err = parseEnvelope(log, mail.Header(p.header))\n\t\tif err != nil {\n\t\t\treturn p, err\n\t\t}\n\t}\n\n\tif p.MediaType == \"MULTIPART\" {\n\t\ts := params[\"boundary\"]\n\t\tif s == \"\" {\n\t\t\treturn p, errMissingBoundaryParam\n\t\t}\n\t\tp.bound = append([]byte(\"--\"), s...)\n\n\t\t// Discard preamble, before first boundary.\n\t\tfor {\n\t\t\tline, _, err := b.PeekLine(true)\n\t\t\tif err != nil {\n\t\t\t\treturn p, fmt.Errorf(\"parsing line for part preamble: %w\", err)\n\t\t\t}\n\t\t\t// Line only needs boundary prefix, not exact match. ../rfc/2046:1103\n\t\t\t// Well, for compatibility, we require whitespace after the boundary. Because some\n\t\t\t// software use the same boundary but with text appended for sub parts.\n\t\t\tif match, finish := checkBound(line, p.bound); match {\n\t\t\t\tif finish {\n\t\t\t\t\treturn p, errFirstBoundCloses\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tb.ReadLine(true)\n\t\t}\n\t\tp.nextBoundOffset = b.offset\n\t\tp.lastBoundOffset = b.offset\n\t}\n\n\treturn p, nil\n}\n\n// Header returns the parsed header of this part.\nfunc (p *Part) Header() (textproto.MIMEHeader, error) {\n\tif p.header != nil {\n\t\treturn p.header, nil\n\t}\n\tif p.HeaderOffset == p.BodyOffset {\n\t\tp.header = textproto.MIMEHeader{}\n\t\treturn p.header, nil\n\t}\n\th, err := parseHeader(p.HeaderReader())\n\tp.header = h\n\treturn h, err\n}\n\n// HeaderReader returns a reader for the header section of this part, including ending bare CRLF.\nfunc (p *Part) HeaderReader() io.Reader {\n\treturn io.NewSectionReader(p.r, p.HeaderOffset, p.BodyOffset-p.HeaderOffset)\n}\n\n// parse a header, only call this on non-empty input (even though that is a valid header).\nfunc parseHeader(r io.Reader) (textproto.MIMEHeader, error) {\n\t// We read using mail.ReadMessage instead of textproto.ReadMIMEHeaders because the\n\t// first handles email messages properly, while the second only works for HTTP\n\t// headers.\n\tvar zero textproto.MIMEHeader\n\n\t// We read the header and add the optional \\r\\n header/body separator. If the \\r\\n\n\t// is missing, parsing with Go <1.21 results in an EOF error.\n\t// todo: directly parse from reader r when Go 1.20 is no longer supported.\n\tbuf, err := io.ReadAll(r)\n\tif err != nil {\n\t\treturn zero, err\n\t}\n\tif bytes.HasSuffix(buf, []byte(\"\\r\\n\")) && !bytes.HasSuffix(buf, []byte(\"\\r\\n\\r\\n\")) {\n\t\tbuf = append(buf, \"\\r\\n\"...)\n\t}\n\tmsg, err := mail.ReadMessage(bytes.NewReader(buf))\n\tif err != nil {\n\t\treturn zero, err\n\t}\n\treturn textproto.MIMEHeader(msg.Header), nil\n}\n\nvar wordDecoder = mime.WordDecoder{\n\tCharsetReader: func(charset string, r io.Reader) (io.Reader, error) {\n\t\tswitch strings.ToLower(charset) {\n\t\tcase \"\", \"us-ascii\", \"utf-8\":\n\t\t\treturn r, nil\n\t\t}\n\t\tenc, _ := ianaindex.MIME.Encoding(charset)\n\t\tif enc == nil {\n\t\t\tenc, _ = ianaindex.IANA.Encoding(charset)\n\t\t}\n\t\tif enc == nil {\n\t\t\treturn r, fmt.Errorf(\"unknown charset %q\", charset)\n\t\t}\n\t\treturn enc.NewDecoder().Reader(r), nil\n\t},\n}\n\nfunc parseEnvelope(log mlog.Log, h mail.Header) (*Envelope, error) {\n\tdate, _ := h.Date()\n\n\t// We currently marshal this field to JSON. But JSON cannot represent all\n\t// time.Time. Time zone of 24:00 was seen in the wild. We won't try for extreme\n\t// years, but we can readjust timezones.\n\t// todo: remove this once we no longer store using json.\n\t_, offset := date.Zone()\n\tif date.Year() > 9999 {\n\t\tdate = time.Time{}\n\t} else if offset <= -24*3600 || offset >= 24*3600 {\n\t\tdate = time.Unix(date.Unix(), 0).UTC()\n\t}\n\n\tsubject := h.Get(\"Subject\")\n\tif s, err := wordDecoder.DecodeHeader(subject); err == nil {\n\t\tsubject = s\n\t}\n\n\tenv := &Envelope{\n\t\tdate,\n\t\tsubject,\n\t\tparseAddressList(log, h, \"from\"),\n\t\tparseAddressList(log, h, \"sender\"),\n\t\tparseAddressList(log, h, \"reply-to\"),\n\t\tparseAddressList(log, h, \"to\"),\n\t\tparseAddressList(log, h, \"cc\"),\n\t\tparseAddressList(log, h, \"bcc\"),\n\t\th.Get(\"In-Reply-To\"),\n\t\th.Get(\"Message-Id\"),\n\t}\n\treturn env, nil\n}\n\nfunc parseAddressList(log mlog.Log, h mail.Header, k string) []Address {\n\t// todo: possibly work around ios mail generating incorrect q-encoded \"phrases\" with unencoded double quotes? ../rfc/2047:382\n\tl, err := h.AddressList(k)\n\tif err != nil {\n\t\treturn nil\n\t}\n\tvar r []Address\n\tfor _, a := range l {\n\t\t// todo: parse more fully according to ../rfc/5322:959\n\t\tvar user, host string\n\t\taddr, err := smtp.ParseAddress(a.Address)\n\t\tif err != nil {\n\t\t\tlog.Infox(\"parsing address (continuing)\", err, slog.Any(\"address\", a.Address))\n\t\t} else {\n\t\t\tuser = addr.Localpart.String()\n\t\t\thost = addr.Domain.ASCII\n\t\t}\n\t\tr = append(r, Address{a.Name, user, host})\n\t}\n\treturn r\n}\n\n// ParseNextPart parses the next (sub)part of this multipart message.\n// ParseNextPart returns io.EOF and a nil part when there are no more parts.\n// Only used for initial parsing of message. Once parsed, use p.Parts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// IsDSN returns whether the MIME structure of the part is a DSN.\nfunc (p *Part) IsDSN() bool {\n\treturn p.MediaType == \"MULTIPART\" &&\n\t\tp.MediaSubType == \"REPORT\" &&\n\t\tlen(p.Parts) >= 2 &&\n\t\tp.Parts[1].MediaType == \"MESSAGE\" &&\n\t\t(p.Parts[1].MediaSubType == \"DELIVERY-STATUS\" || p.Parts[1].MediaSubType == \"GLOBAL-DELIVERY-STATUS\")\n}\n\n// Reader returns a reader for the decoded body content.\nfunc (p *Part) Reader() io.Reader {\n\treturn p.bodyReader(p.RawReader())\n}\n\n// ReaderUTF8OrBinary returns a reader for the decoded body content, transformed to\n// utf-8 for known mime/iana encodings (only if they aren't us-ascii or utf-8\n// already). For unknown or missing character sets/encodings, the original reader\n// is returned.\nfunc (p *Part) ReaderUTF8OrBinary() io.Reader {\n\treturn DecodeReader(p.ContentType\n```\n# Function Signature: func EnsurePart(elog *slog.Logger, strict bool, r io.ReaderAt, size int64) (Part, error) {\n# Natural Language Annotations: // EnsurePart parses a part as with Parse, but ensures a usable part is always\n// returned, even if error is non-nil. If a parse error occurs, the message is\n// returned as application/octet-stream, and headers can still be read if they\n// were valid.\n//\n// If strict is set, fewer attempts are made to continue parsing when errors are\n// encountered, such as with invalid content-type headers or bare carriage returns.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc EnsurePart(elog *slog.Logger, strict bool, r io.ReaderAt, size int64) (Part, error) {\n\tpart, err := Parse(elog, strict, r)\n\tif err != nil {\n\t\tpart, err = fallbackPart(Part{}, r, size)\n\t}\n\treturn part, err\n}\n```", "```go\n// EnsurePart parses a part as with Parse, but ensures a usable part is always\n// returned, even if error is non-nil. If a parse error occurs, the message is\n// returned as application/octet-stream, and headers can still be read if they\n// were valid.\n//\n// If strict is set, fewer attempts are made to continue parsing when errors are\n// encountered, such as with invalid content-type headers or bare carriage returns.\nfunc EnsurePart(elog *slog.Logger, strict bool, r io.ReaderAt, size int64) (Part, error) {\n\tlog := mlog.New(\"message\", elog)\n\tp, err := newPart(log, strict, r, 0, nil)\n\tif err != nil {\n\t\tp, err = fallbackPart(p, r, size)\n\t\tif err != nil {\n\t\t\treturn Part{}, err\n\t\t}\n\t}\n\treturn p, nil\n}\n```", "```go\nfunc EnsurePart(elog *slog.Logger, strict bool, r io.ReaderAt, size int64) (Part, error) {\n\tlog := mlog.New(\"message\", elog)\n\t\n\tpart, err := Parse(log.Logger, strict, r)\n\tif err != nil {\n\t\tif !strict || !errors.Is(err, ErrBadContentType) {\n\t\t\tpart, err = fallbackPart(part, r, size)\n\t\t\tif err != nil {\n\t\t\t\treturn part, err\n\t\t\t}\n\t\t}\n\t}\n\t\n\treturn part, err\n}\n```"]}, "mox-message/part.go-ParseNextPart": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/message/part.go:\n```\npackage message\n\n// todo: allow more invalid content-type values, we now stop parsing on: empty media type (eg \"content-type: ; name=...\"), empty value for property (eg \"charset=\", missing quotes for characters that should be quoted (eg boundary containing \"=\" but without quotes), duplicate properties (two charsets), empty pairs (eg \"text/html;;\").\n// todo: should we be forgiving when closing boundary in multipart message is missing? seems like spam messages do this...\n// todo: should we allow base64 messages where a line starts with a space? and possibly more whitespace. is happening in messages. coreutils base64 accepts it, encoding/base64 does not.\n// todo: handle comments in headers?\n// todo: should we just always store messages with \\n instead of \\r\\n? \\r\\n seems easier for use with imap.\n// todo: can use a cleanup\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"mime\"\n\t\"mime/quotedprintable\"\n\t\"net/mail\"\n\t\"net/textproto\"\n\t\"strings\"\n\t\"time\"\n\n\t\"golang.org/x/text/encoding/ianaindex\"\n\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// Pedantic enables stricter parsing.\nvar Pedantic bool\n\nvar (\n\tErrBadContentType = errors.New(\"bad content-type\")\n)\n\nvar (\n\terrNotMultipart           = errors.New(\"not a multipart message\")\n\terrFirstBoundCloses       = errors.New(\"first boundary cannot be finishing boundary\")\n\terrLineTooLong            = errors.New(\"line too long\")\n\terrMissingBoundaryParam   = errors.New(\"missing/empty boundary content-type parameter\")\n\terrMissingClosingBoundary = errors.New(\"eof without closing boundary\")\n\terrBareLF                 = errors.New(\"invalid bare line feed\")\n\terrBareCR                 = errors.New(\"invalid bare carriage return\")\n\terrUnexpectedEOF          = errors.New(\"unexpected eof\")\n)\n\n// If set, during tests, attempts to reparse a part will cause an error, because sequentially reading parts should not lead to reparsing.\nvar enforceSequential bool\n\n// Part represents a whole mail message, or a part of a multipart message. It\n// is designed to handle IMAP requirements efficiently.\ntype Part struct {\n\tBoundaryOffset int64 // Offset in message where bound starts. -1 for top-level message.\n\tHeaderOffset   int64 // Offset in message file where header starts.\n\tBodyOffset     int64 // Offset in message file where body starts.\n\tEndOffset      int64 // Where body of part ends. Set when part is fully read.\n\tRawLineCount   int64 // Number of lines in raw, undecoded, body of part. Set when part is fully read.\n\tDecodedSize    int64 // Number of octets when decoded. If this is a text mediatype, lines ending only in LF are changed end in CRLF and DecodedSize reflects that.\n\n\tMediaType               string            // From Content-Type, upper case. E.g. \"TEXT\". Can be empty because content-type may be absent. In this case, the part may be treated as TEXT/PLAIN.\n\tMediaSubType            string            // From Content-Type, upper case. E.g. \"PLAIN\".\n\tContentTypeParams       map[string]string // E.g. holds \"boundary\" for multipart messages. Has lower-case keys, and original case values.\n\tContentID               string\n\tContentDescription      string\n\tContentTransferEncoding string    // In upper case.\n\tEnvelope                *Envelope // Email message headers. Not for non-message parts.\n\n\tParts []Part // Parts if this is a multipart.\n\n\t// Only for message/rfc822 and message/global. This part may have a buffer as\n\t// backing io.ReaderAt, because a message/global can have a non-identity\n\t// content-transfer-encoding. This part has a nil parent.\n\tMessage *Part\n\n\tr               io.ReaderAt\n\theader          textproto.MIMEHeader // Parsed header.\n\tnextBoundOffset int64                // If >= 0, the offset where the next part header starts. We can set this when a user fully reads each part.\n\tlastBoundOffset int64                // Start of header of last/previous part. Used to skip a part if ParseNextPart is called and nextBoundOffset is -1.\n\tparent          *Part                // Parent part, for getting bound from, and setting nextBoundOffset when a part has finished reading. Only for subparts, not top-level parts.\n\tbound           []byte               // Only set if valid multipart with boundary, includes leading --, excludes \\r\\n.\n\tstrict          bool                 // If set, valid crlf line endings are verified when reading body.\n}\n\n// todo: have all Content* fields in Part?\n// todo: make Address contain a type Localpart and dns.Domain?\n// todo: if we ever make a major change and reparse all parts, switch to lower-case values if not too troublesome.\n\n// Envelope holds the basic/common message headers as used in IMAP4.\ntype Envelope struct {\n\tDate      time.Time\n\tSubject   string // Q/B-word-decoded.\n\tFrom      []Address\n\tSender    []Address\n\tReplyTo   []Address\n\tTo        []Address\n\tCC        []Address\n\tBCC       []Address\n\tInReplyTo string // From In-Reply-To header, includes <>.\n\tMessageID string // From Message-Id header, includes <>.\n}\n\n// Address as used in From and To headers.\ntype Address struct {\n\tName string // Free-form name for display in mail applications.\n\tUser string // Localpart, encoded as string. Must be parsed before using as Localpart.\n\tHost string // Domain in ASCII.\n}\n\n// Parse reads the headers of the mail message and returns a part.\n// A part provides access to decoded and raw contents of a message and its multiple parts.\n//\n// If strict is set, fewer attempts are made to continue parsing when errors are\n// encountered, such as with invalid content-type headers or bare carriage returns.\nfunc Parse(elog *slog.Logger, strict bool, r io.ReaderAt) (Part, error) {\n\tlog := mlog.New(\"message\", elog)\n\treturn newPart(log, strict, r, 0, nil)\n}\n\n// EnsurePart parses a part as with Parse, but ensures a usable part is always\n// returned, even if error is non-nil. If a parse error occurs, the message is\n// returned as application/octet-stream, and headers can still be read if they\n// were valid.\n//\n// If strict is set, fewer attempts are made to continue parsing when errors are\n// encountered, such as with invalid content-type headers or bare carriage returns.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc fallbackPart(p Part, r io.ReaderAt, size int64) (Part, error) {\n\tnp := Part{\n\t\tHeaderOffset:            p.HeaderOffset,\n\t\tBodyOffset:              p.BodyOffset,\n\t\tEndOffset:               size,\n\t\tMediaType:               \"APPLICATION\",\n\t\tMediaSubType:            \"OCTET-STREAM\",\n\t\tContentTypeParams:       p.ContentTypeParams,\n\t\tContentID:               p.ContentID,\n\t\tContentDescription:      p.ContentDescription,\n\t\tContentTransferEncoding: p.ContentTransferEncoding,\n\t\tEnvelope:                p.Envelope,\n\t\t// We don't keep:\n\t\t//   - BoundaryOffset: irrelevant for top-level message.\n\t\t//   - RawLineCount and DecodedSize: set below.\n\t\t//   - Parts: we are not treating this as a multipart message.\n\t}\n\tnp.SetReaderAt(r)\n\t// By reading body, the number of lines and decoded size will be set.\n\t_, err := io.Copy(io.Discard, np.Reader())\n\treturn np, err\n}\n\n// SetReaderAt sets r as reader for this part and all its sub parts, recursively.\n// No reader is set for any Message subpart, see SetMessageReaderAt.\nfunc (p *Part) SetReaderAt(r io.ReaderAt) {\n\tif r == nil {\n\t\tpanic(\"nil reader\")\n\t}\n\tp.r = r\n\tfor i := range p.Parts {\n\t\tpp := &p.Parts[i]\n\t\tpp.SetReaderAt(r)\n\t}\n}\n\n// SetMessageReaderAt sets a reader on p.Message, which must be non-nil.\nfunc (p *Part) SetMessageReaderAt() error {\n\t// todo: if p.Message does not contain any non-identity content-transfer-encoding, we should set an offsetReader of p.Message, recursively.\n\tbuf, err := io.ReadAll(p.Reader())\n\tif err != nil {\n\t\treturn err\n\t}\n\tp.Message.SetReaderAt(bytes.NewReader(buf))\n\treturn nil\n}\n\n// Walk through message, decoding along the way, and collecting mime part offsets and sizes, and line counts.\nfunc (p *Part) Walk(elog *slog.Logger, parent *Part) error {\n\tlog := mlog.New(\"message\", elog)\n\n\tif len(p.bound) == 0 {\n\t\tif p.MediaType == \"MESSAGE\" && (p.MediaSubType == \"RFC822\" || p.MediaSubType == \"GLOBAL\") {\n\t\t\t// todo: don't read whole submessage in memory...\n\t\t\tbuf, err := io.ReadAll(p.Reader())\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tbr := bytes.NewReader(buf)\n\t\t\tmp, err := Parse(log.Logger, p.strict, br)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"parsing embedded message: %w\", err)\n\t\t\t}\n\t\t\tif err := mp.Walk(log.Logger, nil); err != nil {\n\t\t\t\t// If this is a DSN and we are not in pedantic mode, accept unexpected end of\n\t\t\t\t// message. This is quite common because MTA's sometimes just truncate the original\n\t\t\t\t// message in a place that makes the message invalid.\n\t\t\t\tif errors.Is(err, errUnexpectedEOF) && !Pedantic && parent != nil && len(parent.Parts) >= 3 && p == &parent.Parts[2] && parent.MediaType == \"MULTIPART\" && parent.MediaSubType == \"REPORT\" {\n\t\t\t\t\tmp, err = fallbackPart(mp, br, int64(len(buf)))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn fmt.Errorf(\"parsing invalid embedded message: %w\", err)\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\treturn fmt.Errorf(\"parsing parts of embedded message: %w\", err)\n\t\t\t\t}\n\t\t\t}\n\t\t\t// todo: if mp does not contain any non-identity content-transfer-encoding, we should set an offsetReader of p.r on mp, recursively.\n\t\t\tp.Message = &mp\n\t\t\treturn nil\n\t\t}\n\t\t_, err := io.Copy(io.Discard, p.Reader())\n\t\treturn err\n\t}\n\n\tfor {\n\t\tpp, err := p.ParseNextPart(log.Logger)\n\t\tif err == io.EOF {\n\t\t\treturn nil\n\t\t}\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err := pp.Walk(log.Logger, p); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n}\n\n// String returns a debugging representation of the part.\nfunc (p *Part) String() string {\n\treturn fmt.Sprintf(\"&Part{%s/%s offsets %d/%d/%d/%d lines %d decodedsize %d next %d last %d bound %q parts %v}\", p.MediaType, p.MediaSubType, p.BoundaryOffset, p.HeaderOffset, p.BodyOffset, p.EndOffset, p.RawLineCount, p.DecodedSize, p.nextBoundOffset, p.lastBoundOffset, p.bound, p.Parts)\n}\n\n// newPart parses a new part, which can be the top-level message.\n// offset is the bound offset for parts, and the start of message for top-level messages. parent indicates if this is a top-level message or sub-part.\n// If an error occurs, p's exported values can still be relevant. EnsurePart uses these values.\nfunc newPart(log mlog.Log, strict bool, r io.ReaderAt, offset int64, parent *Part) (p Part, rerr error) {\n\tif r == nil {\n\t\tpanic(\"nil reader\")\n\t}\n\tp = Part{\n\t\tBoundaryOffset: -1,\n\t\tEndOffset:      -1,\n\t\tr:              r,\n\t\tparent:         parent,\n\t\tstrict:         strict,\n\t}\n\n\tb := &bufAt{strict: strict, r: r, offset: offset}\n\n\tif parent != nil {\n\t\tp.BoundaryOffset = offset\n\t\tif line, _, err := b.ReadLine(true); err != nil {\n\t\t\treturn p, err\n\t\t} else if match, finish := checkBound(line, parent.bound); !match {\n\t\t\treturn p, fmt.Errorf(\"missing bound\")\n\t\t} else if finish {\n\t\t\treturn p, fmt.Errorf(\"new part for closing boundary\")\n\t\t}\n\t}\n\n\t// Collect header.\n\tp.HeaderOffset = b.offset\n\tp.BodyOffset = b.offset\n\thb := &bytes.Buffer{}\n\tfor {\n\t\tline, _, err := b.ReadLine(true)\n\t\tif err == io.EOF {\n\t\t\t// No body is valid.\n\t\t\tbreak\n\t\t}\n\t\tif err != nil {\n\t\t\treturn p, fmt.Errorf(\"reading header line: %w\", err)\n\t\t}\n\t\thb.Write(line)\n\t\tif len(line) == 2 {\n\t\t\tbreak // crlf\n\t\t}\n\t}\n\tp.BodyOffset = b.offset\n\n\t// Don't attempt to parse empty header, mail.ReadMessage doesn't like it.\n\tif p.HeaderOffset == p.BodyOffset {\n\t\tp.header = textproto.MIMEHeader{}\n\t} else {\n\t\th, err := parseHeader(hb)\n\t\tif err != nil {\n\t\t\treturn p, fmt.Errorf(\"parsing header: %w\", err)\n\t\t}\n\t\tp.header = h\n\t}\n\n\tct := p.header.Get(\"Content-Type\")\n\tmt, params, err := mime.ParseMediaType(ct)\n\tif err != nil && ct != \"\" {\n\t\tif Pedantic || strict {\n\t\t\treturn p, fmt.Errorf(\"%w: %s: %q\", ErrBadContentType, err, ct)\n\t\t}\n\n\t\t// Try parsing just a content-type, ignoring parameters.\n\t\t// ../rfc/2045:628\n\t\tct = strings.TrimSpace(strings.SplitN(ct, \";\", 2)[0])\n\t\tt := strings.SplitN(ct, \"/\", 2)\n\t\tisToken := func(s string) bool {\n\t\t\tconst separators = `()<>@,;:\\\\\"/[]?= ` // ../rfc/2045:663\n\t\t\tfor _, c := range s {\n\t\t\t\tif c < 0x20 || c >= 0x80 || strings.ContainsRune(separators, c) {\n\t\t\t\t\treturn false\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn len(s) > 0\n\t\t}\n\t\t// We cannot recover content-type of multipart, we won't have a boundary.\n\t\tif len(t) == 2 && isToken(t[0]) && !strings.EqualFold(t[0], \"multipart\") && isToken(t[1]) {\n\t\t\tp.MediaType = strings.ToUpper(t[0])\n\t\t\tp.MediaSubType = strings.ToUpper(t[1])\n\t\t} else {\n\t\t\tp.MediaType = \"APPLICATION\"\n\t\t\tp.MediaSubType = \"OCTET-STREAM\"\n\t\t}\n\t\tlog.Debugx(\"malformed content-type, attempting to recover and continuing\", err,\n\t\t\tslog.String(\"contenttype\", p.header.Get(\"Content-Type\")),\n\t\t\tslog.String(\"mediatype\", p.MediaType),\n\t\t\tslog.String(\"mediasubtype\", p.MediaSubType))\n\t} else if mt != \"\" {\n\t\tt := strings.SplitN(strings.ToUpper(mt), \"/\", 2)\n\t\tif len(t) != 2 {\n\t\t\tif Pedantic || strict {\n\t\t\t\treturn p, fmt.Errorf(\"bad content-type: %q (content-type %q)\", mt, ct)\n\t\t\t}\n\t\t\tlog.Debug(\"malformed media-type, ignoring and continuing\", slog.String(\"type\", mt))\n\t\t\tp.MediaType = \"APPLICATION\"\n\t\t\tp.MediaSubType = \"OCTET-STREAM\"\n\t\t} else {\n\t\t\tp.MediaType = t[0]\n\t\t\tp.MediaSubType = t[1]\n\t\t\tp.ContentTypeParams = params\n\t\t}\n\t}\n\n\tp.ContentID = p.header.Get(\"Content-Id\")\n\tp.ContentDescription = p.header.Get(\"Content-Description\")\n\tp.ContentTransferEncoding = strings.ToUpper(p.header.Get(\"Content-Transfer-Encoding\"))\n\n\tif parent == nil {\n\t\tp.Envelope, err = parseEnvelope(log, mail.Header(p.header))\n\t\tif err != nil {\n\t\t\treturn p, err\n\t\t}\n\t}\n\n\tif p.MediaType == \"MULTIPART\" {\n\t\ts := params[\"boundary\"]\n\t\tif s == \"\" {\n\t\t\treturn p, errMissingBoundaryParam\n\t\t}\n\t\tp.bound = append([]byte(\"--\"), s...)\n\n\t\t// Discard preamble, before first boundary.\n\t\tfor {\n\t\t\tline, _, err := b.PeekLine(true)\n\t\t\tif err != nil {\n\t\t\t\treturn p, fmt.Errorf(\"parsing line for part preamble: %w\", err)\n\t\t\t}\n\t\t\t// Line only needs boundary prefix, not exact match. ../rfc/2046:1103\n\t\t\t// Well, for compatibility, we require whitespace after the boundary. Because some\n\t\t\t// software use the same boundary but with text appended for sub parts.\n\t\t\tif match, finish := checkBound(line, p.bound); match {\n\t\t\t\tif finish {\n\t\t\t\t\treturn p, errFirstBoundCloses\n\t\t\t\t}\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tb.ReadLine(true)\n\t\t}\n\t\tp.nextBoundOffset = b.offset\n\t\tp.lastBoundOffset = b.offset\n\t}\n\n\treturn p, nil\n}\n\n// Header returns the parsed header of this part.\nfunc (p *Part) Header() (textproto.MIMEHeader, error) {\n\tif p.header != nil {\n\t\treturn p.header, nil\n\t}\n\tif p.HeaderOffset == p.BodyOffset {\n\t\tp.header = textproto.MIMEHeader{}\n\t\treturn p.header, nil\n\t}\n\th, err := parseHeader(p.HeaderReader())\n\tp.header = h\n\treturn h, err\n}\n\n// HeaderReader returns a reader for the header section of this part, including ending bare CRLF.\nfunc (p *Part) HeaderReader() io.Reader {\n\treturn io.NewSectionReader(p.r, p.HeaderOffset, p.BodyOffset-p.HeaderOffset)\n}\n\n// parse a header, only call this on non-empty input (even though that is a valid header).\nfunc parseHeader(r io.Reader) (textproto.MIMEHeader, error) {\n\t// We read using mail.ReadMessage instead of textproto.ReadMIMEHeaders because the\n\t// first handles email messages properly, while the second only works for HTTP\n\t// headers.\n\tvar zero textproto.MIMEHeader\n\n\t// We read the header and add the optional \\r\\n header/body separator. If the \\r\\n\n\t// is missing, parsing with Go <1.21 results in an EOF error.\n\t// todo: directly parse from reader r when Go 1.20 is no longer supported.\n\tbuf, err := io.ReadAll(r)\n\tif err != nil {\n\t\treturn zero, err\n\t}\n\tif bytes.HasSuffix(buf, []byte(\"\\r\\n\")) && !bytes.HasSuffix(buf, []byte(\"\\r\\n\\r\\n\")) {\n\t\tbuf = append(buf, \"\\r\\n\"...)\n\t}\n\tmsg, err := mail.ReadMessage(bytes.NewReader(buf))\n\tif err != nil {\n\t\treturn zero, err\n\t}\n\treturn textproto.MIMEHeader(msg.Header), nil\n}\n\nvar wordDecoder = mime.WordDecoder{\n\tCharsetReader: func(charset string, r io.Reader) (io.Reader, error) {\n\t\tswitch strings.ToLower(charset) {\n\t\tcase \"\", \"us-ascii\", \"utf-8\":\n\t\t\treturn r, nil\n\t\t}\n\t\tenc, _ := ianaindex.MIME.Encoding(charset)\n\t\tif enc == nil {\n\t\t\tenc, _ = ianaindex.IANA.Encoding(charset)\n\t\t}\n\t\tif enc == nil {\n\t\t\treturn r, fmt.Errorf(\"unknown charset %q\", charset)\n\t\t}\n\t\treturn enc.NewDecoder().Reader(r), nil\n\t},\n}\n\nfunc parseEnvelope(log mlog.Log, h mail.Header) (*Envelope, error) {\n\tdate, _ := h.Date()\n\n\t// We currently marshal this field to JSON. But JSON cannot represent all\n\t// time.Time. Time zone of 24:00 was seen in the wild. We won't try for extreme\n\t// years, but we can readjust timezones.\n\t// todo: remove this once we no longer store using json.\n\t_, offset := date.Zone()\n\tif date.Year() > 9999 {\n\t\tdate = time.Time{}\n\t} else if offset <= -24*3600 || offset >= 24*3600 {\n\t\tdate = time.Unix(date.Unix(), 0).UTC()\n\t}\n\n\tsubject := h.Get(\"Subject\")\n\tif s, err := wordDecoder.DecodeHeader(subject); err == nil {\n\t\tsubject = s\n\t}\n\n\tenv := &Envelope{\n\t\tdate,\n\t\tsubject,\n\t\tparseAddressList(log, h, \"from\"),\n\t\tparseAddressList(log, h, \"sender\"),\n\t\tparseAddressList(log, h, \"reply-to\"),\n\t\tparseAddressList(log, h, \"to\"),\n\t\tparseAddressList(log, h, \"cc\"),\n\t\tparseAddressList(log, h, \"bcc\"),\n\t\th.Get(\"In-Reply-To\"),\n\t\th.Get(\"Message-Id\"),\n\t}\n\treturn env, nil\n}\n\nfunc parseAddressList(log mlog.Log, h mail.Header, k string) []Address {\n\t// todo: possibly work around ios mail generating incorrect q-encoded \"phrases\" with unencoded double quotes? ../rfc/2047:382\n\tl, err := h.AddressList(k)\n\tif err != nil {\n\t\treturn nil\n\t}\n\tvar r []Address\n\tfor _, a := range l {\n\t\t// todo: parse more fully according to ../rfc/5322:959\n\t\tvar user, host string\n\t\taddr, err := smtp.ParseAddress(a.Address)\n\t\tif err != nil {\n\t\t\tlog.Infox(\"parsing address (continuing)\", err, slog.Any(\"address\", a.Address))\n\t\t} else {\n\t\t\tuser = addr.Localpart.String()\n\t\t\thost = addr.Domain.ASCII\n\t\t}\n\t\tr = append(r, Address{a.Name, user, host})\n\t}\n\treturn r\n}\n\n// ParseNextPart parses the next (sub)part of this multipart message.\n// ParseNextPart returns io.EOF and a nil part when there are no more parts.\n// Only used for initial parsing of message. Once parsed, use p.Parts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// IsDSN returns whether the MIME structure of the part is a DSN.\nfunc (p *Part) IsDSN() bool {\n\treturn p.MediaType == \"MULTIPART\" &&\n\t\tp.MediaSubType == \"REPORT\" &&\n\t\tlen(p.Parts) >= 2 &&\n\t\tp.Parts[1].MediaType == \"MESSAGE\" &&\n\t\t(p.Parts[1].MediaSubType == \"DELIVERY-STATUS\" || p.Parts[1].MediaSubType == \"GLOBAL-DELIVERY-STATUS\")\n}\n\n// Reader returns a reader for the decoded body content.\nfunc (p *Part) Reader() io.Reader {\n\treturn p.bodyReader(p.RawReader())\n}\n\n// ReaderUTF8OrBinary returns a reader for the decoded body content, transformed to\n// utf-8 for known mime/iana encodings (only if they aren't us-ascii or utf-8\n// already). For unknown or missing character sets/encodings, the original reader\n// is returned.\nfunc (p *Part) ReaderUTF8OrBinary() io.Reader {\n\treturn DecodeReader(p.ContentType\n```\n# Function Signature: func (p *Part) ParseNextPart(elog *slog.Logger) (*Part, error) {\n# Natural Language Annotations: // ParseNextPart parses the next (sub)part of this multipart message.\n// ParseNextPart returns io.EOF and a nil part when there are no more parts.\n// Only used for initial parsing of message. Once parsed, use p.Parts.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (p *Part) ParseNextPart(elog *slog.Logger) (*Part, error) {\n\tlog := mlog.New(\"message\", elog)\n\n\tif p.nextBoundOffset == -1 {\n\t\treturn nil, io.EOF\n\t}\n\tif p.nextBoundOffset == p.lastBoundOffset {\n\t\t// todo: wait until next or return nil?\n\t\treturn nil, io.EOF\n\t}\n\n\tp.lastBoundOffset = p.nextBoundOffset\n\tnp, err := newPart(log.Logger, p.strict, p.r, p.nextBoundOffset, p)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tp.Parts = append(p.Parts, np)\n\n\tp.nextBoundOffset = -1\n\tif np.MediaType != \"MULTIPART\" {\n\t\t// todo: if np.MediaType == \"TEXT\", we could skip to next boundary.\n\t\treturn &np, nil\n\t}\n\n\t// If np is multipart, we don't know where next part starts until we reach the end of np.\n\t// So we can't set p.nextBoundOffset yet.\n\treturn &np, nil\n}\n```", "```go\nfunc (p *Part) ParseNextPart(elog *slog.Logger) (*Part, error) {\n\tif p.BoundaryOffset == -1 {\n\t\treturn nil, errNotMultipart\n\t}\n\n\tif p.nextBoundOffset == -1 {\n\t\treturn nil, io.EOF\n\t}\n\n\tlog := mlog.New(\"message\", elog)\n\tbr := bufAt{strict: p.strict, r: p.r, offset: p.nextBoundOffset}\n\tif p.lastBoundOffset != p.nextBoundOffset {\n\t\tbr.Offset(p.lastBoundOffset)\n\t}\n\n\tpp, err := newPart(log.Logger, p.strict, p.r, p.nextBoundOffset, p)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tpp.lastBoundOffset = pp.nextBoundOffset\n\tif line, _, err := br.ReadLine(true); err == io.EOF {\n\t\tp.nextBoundOffset = -1\n\t} else if err != nil {\n\t\treturn nil, err\n\t} else if match, finish := checkBound(line, p.bound); !match {\n\t\treturn nil, fmt.Errorf(\"missing bound\")\n\t} else if finish {\n\t\tp.nextBoundOffset = -1\n\t}\n\n\tp.Parts = append(p.Parts, pp)\n\treturn &pp, nil\n}\n```", "```go\nfunc (p *Part) ParseNextPart(elog *slog.Logger) (*Part, error) {\n\tlog := mlog.New(\"message\", elog)\n\t\n\tif p.nextBoundOffset == -1 {\n\t\treturn nil, io.EOF\n\t}\n\n\tif p.parent == nil {\n\t\treturn nil, errNotMultipart\n\t}\n\n\tif p.nextBoundOffset <= p.lastBoundOffset {\n\t\treturn nil, fmt.Errorf(\"boundary offset not increasing (%d vs %d)\", p.nextBoundOffset, p.lastBoundOffset)\n\t}\n\n\tpp, err := newPart(log.Logger, p.strict, p.r, p.nextBoundOffset, p.parent)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif pp.BoundaryOffset != p.nextBoundOffset {\n\t\treturn nil, fmt.Errorf(\"new part boundary offset does not match expected offset (got %d, expected %d)\", pp.BoundaryOffset, p.nextBoundOffset)\n\t}\n\n\tp.lastBoundOffset = pp.BoundaryOffset\n\tp.nextBoundOffset = pp.nextBoundOffset\n\n\tp.Parts = append(p.Parts, pp)\n\n\treturn &pp, nil\n}\n```"]}, "mox-message/messageid.go-MessageIDCanonical": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/message/messageid.go:\n```\npackage message\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/smtp\"\n)\n\nvar errBadMessageID = errors.New(\"not a message-id\")\n\n// MessageIDCanonical parses the Message-ID, returning a canonical value that is\n// lower-cased, without <>, and no unneeded quoting. For matching in threading,\n// with References/In-Reply-To. If the message-id is invalid (e.g. no <>), an error\n// is returned. If the message-id could not be parsed as address (localpart \"@\"\n// domain), the raw value and the bool return parameter true is returned. It is\n// quite common that message-id's don't adhere to the localpart @ domain\n// syntax.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func MessageIDCanonical(s string) (string, bool, error) {\n# Natural Language Annotations: // MessageIDCanonical parses the Message-ID, returning a canonical value that is\n// lower-cased, without <>, and no unneeded quoting. For matching in threading,\n// with References/In-Reply-To. If the message-id is invalid (e.g. no <>), an error\n// is returned. If the message-id could not be parsed as address (localpart \"@\"\n// domain), the raw value and the bool return parameter true is returned. It is\n// quite common that message-id's don't adhere to the localpart @ domain\n// syntax.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage message\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/smtp\"\n)\n\nvar errBadMessageID = errors.New(\"not a message-id\")\n\n// MessageIDCanonical parses the Message-ID, returning a canonical value that is\n// lower-cased, without <>, and no unneeded quoting. For matching in threading,\n// with References/In-Reply-To. If the message-id is invalid (e.g. no <>), an error\n// is returned. If the message-id could not be parsed as address (localpart \"@\"\n// domain), the raw value and the bool return parameter true is returned. It is\n// quite common that message-id's don't adhere to the localpart @ domain\n// syntax.\nfunc MessageIDCanonical(s string) (string, bool, error) {\n\t// Check if the message-id is valid (i.e., starts and ends with \"<\" and \">\")\n\tif !strings.HasPrefix(s, \"<\") || !strings.HasSuffix(s, \">\") {\n\t\treturn \"\", false, errBadMessageID\n\t}\n\n\t// Remove the \"<\" and \">\" from the message-id\n\ts = s[1 : len(s)-1]\n\n\t// Check if the message-id is an address (localpart \"@\" domain)\n\tif !strings.Contains(s, \"@\") {\n\t\treturn s, true, nil\n\t}\n\n\t// Split the message-id into localpart and domain\n\tparts := strings.Split(s, \"@\")\n\tif len(parts) != 2 {\n\t\treturn s, true, nil\n\t}\n\n\t// Lower-case the localpart and the domain\n\tlocalpart := strings.ToLower(parts[0])\n\tdomain := strings.ToLower(parts[1])\n\n\t// Reconstruct the canonical message-id\n\tcanonical := localpart + \"@\" + domain\n\n\treturn canonical, false, nil\n}\n```", "```go\npackage message\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\t\"github.com/mjl-/mox/smtp\"\n)\n\nvar errBadMessageID = errors.New(\"not a message-id\")\n\n// MessageIDCanonical parses the Message-ID, returning a canonical value that is\n// lower-cased, without <>, and no unneeded quoting. For matching in threading,\n// with References/In-Reply-To. If the message-id is invalid (e.g. no <>), an error\n// is returned. If the message-id could not be parsed as address (localpart \"@\"\n// domain), the raw value and the bool return parameter true is returned. It is\n// quite common that message-id's don't adhere to the localpart @ domain\n// syntax.\nfunc MessageIDCanonical(s string) (string, bool, error) {\n\tif !strings.HasPrefix(s, \"<\") || !strings.HasSuffix(s, \">\") {\n\t\treturn \"\", false, errBadMessageID\n\t}\n\n\ts = strings.TrimSuffix(strings.TrimPrefix(s, \"<\"), \">\")\n\tif strings.Contains(s, \"@\") {\n\t\treturn s, true, nil\n\t}\n\n\treturn strings.ToLower(s), false, nil\n}\n```", "```go\npackage message\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/smtp\"\n)\n\nvar errBadMessageID = errors.New(\"not a message-id\")\n\n// MessageIDCanonical parses the Message-ID, returning a canonical value that is\n// lower-cased, without <>, and no unneeded quoting. For matching in threading,\n// with References/In-Reply-To. If the message-id is invalid (e.g. no <>), an error\n// is returned. If the message-id could not be parsed as address (localpart \"@\"\n// domain), the raw value and the bool return parameter true is returned. It is\n// quite common that message-id's don't adhere to the localpart @ domain\n// syntax.\nfunc MessageIDCanonical(s string) (string, bool, error) {\n\tif !strings.HasPrefix(s, \"<\") || !strings.HasSuffix(s, \">\") {\n\t\treturn s, true, errBadMessageID\n\t}\n\ts = strings.TrimSuffix(strings.TrimPrefix(s, \"<\"), \">\")\n\tif strings.Contains(s, \"@\") {\n\t\treturn s, false, nil\n\t}\n\treturn strings.ToLower(s), false, nil\n}\n```"]}, "mox-message/decode.go-DecodeReader": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/message/decode.go:\n```\npackage message\n\nimport (\n\t\"io\"\n\t\"strings\"\n\n\t\"golang.org/x/text/encoding/ianaindex\"\n)\n\n// DecodeReader returns a reader that reads from r, decoding as charset. If\n// charset is empty, us-ascii, utf-8 or unknown, the original reader is\n// returned and no decoding takes place.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func DecodeReader(charset string, r io.Reader) io.Reader {\n# Natural Language Annotations: // DecodeReader returns a reader that reads from r, decoding as charset. If\n// charset is empty, us-ascii, utf-8 or unknown, the original reader is\n// returned and no decoding takes place.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage message\n\nimport (\n\t\"io\"\n\t\"strings\"\n\t\"golang.org/x/text/encoding\"\n\t\"golang.org/x/text/encoding/ianaindex\"\n\t\"golang.org/x/text/transform\"\n)\n\n// DecodeReader returns a reader that reads from r, decoding as charset. If\n// charset is empty, us-ascii, utf-8 or unknown, the original reader is\n// returned and no decoding takes place.\nfunc DecodeReader(charset string, r io.Reader) io.Reader {\n\tif charset == \"\" || strings.EqualFold(charset, \"us-ascii\") || strings.EqualFold(charset, \"utf-8\") || strings.EqualFold(charset, \"unknown\") {\n\t\treturn r\n\t}\n\n\tenc, err := ianaindex.IANA.Encoding(charset)\n\tif err != nil {\n\t\treturn r\n\t}\n\n\treturn transform.NewReader(r, enc.NewDecoder())\n}\n```", "```go\npackage message\n\nimport (\n\t\"io\"\n\t\"strings\"\n\n\t\"golang.org/x/text/encoding/ianaindex\"\n)\n\n// DecodeReader returns a reader that reads from r, decoding as charset. If\n// charset is empty, us-ascii, utf-8 or unknown, the original reader is\n// returned and no decoding takes place.\nfunc DecodeReader(charset string, r io.Reader) io.Reader {\n\tcharset = strings.ToLower(charset)\n\tif charset == \"\" || charset == \"us-ascii\" || charset == \"utf-8\" || charset == \"unknown\" {\n\t\treturn r\n\t}\n\n\tencoding, err := ianaindex.IANA.Encoding(charset)\n\tif err != nil {\n\t\t// handle the error here, for example by returning nil\n\t\treturn nil\n\t}\n\n\treturn encoding.NewDecoder().Reader(r)\n}\n```", "```go\npackage message\n\nimport (\n\t\"io\"\n\t\"strings\"\n\t\"golang.org/x/text/encoding\"\n\t\"golang.org/x/text/encoding/ianaindex\"\n\t\"golang.org/x/text/transform\"\n)\n\n// DecodeReader returns a reader that reads from r, decoding as charset. If\n// charset is empty, us-ascii, utf-8 or unknown, the original reader is\n// returned and no decoding takes place.\nfunc DecodeReader(charset string, r io.Reader) io.Reader {\n\tcharset = strings.ToLower(charset)\n\tif charset == \"\" || charset == \"us-ascii\" || charset == \"utf-8\" || charset == \"unknown\" {\n\t\treturn r\n\t}\n\n\tenc, _ := ianaindex.IANA.Encoding(charset)\n\tif enc == nil {\n\t\treturn r\n\t}\n\n\treturn transform.NewReader(r, enc.NewDecoder())\n}\n```"]}, "mox-message/referencedids.go-ReferencedIDs": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/message/referencedids.go:\n```\npackage message\n\nimport (\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// ReferencedIDs returns the Message-IDs referenced from the References header(s),\n// with a fallback to the In-Reply-To header(s). The ids are canonicalized for\n// thread-matching, like with MessageIDCanonical. Empty message-id's are skipped.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func ReferencedIDs(references []string, inReplyTo []string) ([]string, error) {\n# Natural Language Annotations: // ReferencedIDs returns the Message-IDs referenced from the References header(s),\n// with a fallback to the In-Reply-To header(s). The ids are canonicalized for\n// thread-matching, like with MessageIDCanonical. Empty message-id's are skipped.\nPlease return the generated code surrounded by ```\n", "response": ["```go\npackage message\n\nimport (\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// ReferencedIDs returns the Message-IDs referenced from the References header(s),\n// with a fallback to the In-Reply-To header(s). The ids are canonicalized for\n// thread-matching, like with MessageIDCanonical. Empty message-id's are skipped.\nfunc ReferencedIDs(references []string, inReplyTo []string) ([]string, error) {\n\tvar ids []string\n\n\t// Process references first.\n\tfor _, ref := range references {\n\t\trefIDs := strings.Split(ref, \" \")\n\t\tfor _, id := range refIDs {\n\t\t\tif id != \"\" {\n\t\t\t\tcanonicalID, err := smtp.MessageIDCanonical(id)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tids = append(ids, canonicalID)\n\t\t\t}\n\t\t}\n\t}\n\n\t// If no references, use In-Reply-To.\n\tif len(ids) == 0 {\n\t\tfor _, replyTo := range inReplyTo {\n\t\t\tif replyTo != \"\" {\n\t\t\t\tcanonicalID, err := smtp.MessageIDCanonical(replyTo)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn nil, err\n\t\t\t\t}\n\t\t\t\tids = append(ids, canonicalID)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn ids, nil\n}\n```", "```go\npackage message\n\nimport (\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// ReferencedIDs returns the Message-IDs referenced from the References header(s),\n// with a fallback to the In-Reply-To header(s). The ids are canonicalized for\n// thread-matching, like with MessageIDCanonical. Empty message-id's are skipped.\nfunc ReferencedIDs(references []string, inReplyTo []string) ([]string, error) {\n\tids := make([]string, 0, len(references)+len(inReplyTo))\n\tfor _, s := range references {\n\t\tfor _, id := range strings.Split(s, \" \") {\n\t\t\tid = smtp.MessageIDCanonical(id)\n\t\t\tif id != \"\" {\n\t\t\t\tids = append(ids, id)\n\t\t\t}\n\t\t}\n\t}\n\tfor _, s := range inReplyTo {\n\t\tfor _, id := range strings.Split(s, \" \") {\n\t\t\tid = smtp.MessageIDCanonical(id)\n\t\t\tif id != \"\" {\n\t\t\t\tids = append(ids, id)\n\t\t\t}\n\t\t}\n\t}\n\treturn ids, nil\n}\n```", "```go\npackage message\n\nimport (\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// ReferencedIDs returns the Message-IDs referenced from the References header(s),\n// with a fallback to the In-Reply-To header(s). The ids are canonicalized for\n// thread-matching, like with MessageIDCanonical. Empty message-id's are skipped.\nfunc ReferencedIDs(references []string, inReplyTo []string) ([]string, error) {\n\tvar ids []string\n\tfor _, id := range references {\n\t\tid = strings.TrimSpace(id)\n\t\tif id == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\tid = smtp.MessageIDCanonical(id)\n\t\tids = append(ids, id)\n\t}\n\n\tif len(ids) == 0 {\n\t\tfor _, id := range inReplyTo {\n\t\t\tid = strings.TrimSpace(id)\n\t\t\tif id == \"\" {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tid = smtp.MessageIDCanonical(id)\n\t\t\tids = append(ids, id)\n\t\t}\n\t}\n\n\treturn ids, nil\n}\n```"]}, "mox-message/authresults.go-ParseAuthResults": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/message/authresults.go:\n```\npackage message\n\nimport (\n\t\"fmt\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/dns\"\n)\n\n// ../rfc/8601:577\n\n// Authentication-Results header, see RFC 8601.\ntype AuthResults struct {\n\tHostname string\n\t// Optional version of Authentication-Results header, assumed \"1\" when absent,\n\t// which is common.\n\tVersion string\n\tComment string       // If not empty, header comment without \"()\", added after Hostname.\n\tMethods []AuthMethod // Can be empty, in case of \"none\".\n}\n\n// ../rfc/8601:598\n\n// AuthMethod is a result for one authentication method.\n//\n// Example encoding in the header: \"spf=pass smtp.mailfrom=example.net\".\ntype AuthMethod struct {\n\t// E.g. \"dkim\", \"spf\", \"iprev\", \"auth\".\n\tMethod  string\n\tVersion string // For optional method version. \"1\" is implied when missing, which is common.\n\tResult  string // Each method has a set of known values, e.g. \"pass\", \"temperror\", etc.\n\tComment string // Optional, message header comment.\n\tReason  string // Optional.\n\tProps   []AuthProp\n}\n\n// ../rfc/8601:606\n\n// AuthProp describes properties for an authentication method.\n// Each method has a set of known properties.\n// Encoded in the header as \"type.property=value\", e.g. \"smtp.mailfrom=example.net\"\n// for spf.\ntype AuthProp struct {\n\t// Valid values maintained at https://www.iana.org/assignments/email-auth/email-auth.xhtml\n\tType     string\n\tProperty string\n\tValue    string\n\t// Whether value is address-like (localpart@domain, or domain). Or another value,\n\t// which is subject to escaping.\n\tIsAddrLike bool\n\tComment    string // If not empty, header comment without \"()\", added after Value.\n}\n\n// MakeAuthProp is a convenient way to make an AuthProp.\nfunc MakeAuthProp(typ, property, value string, isAddrLike bool, Comment string) AuthProp {\n\treturn AuthProp{typ, property, value, isAddrLike, Comment}\n}\n\n// todo future: we could store fields as dns.Domain, and when we encode as non-ascii also add the ascii version as a comment.\n\n// Header returns an Authentication-Results header, possibly spanning multiple\n// lines, always ending in crlf.\nfunc (h AuthResults) Header() string {\n\t// Escaping of values: ../rfc/8601:684 ../rfc/2045:661\n\n\toptComment := func(s string) string {\n\t\tif s != \"\" {\n\t\t\treturn \" (\" + s + \")\"\n\t\t}\n\t\treturn s\n\t}\n\n\tw := &HeaderWriter{}\n\tw.Add(\"\", \"Authentication-Results:\"+optComment(h.Comment)+\" \"+value(h.Hostname, false)+\";\")\n\tfor i, m := range h.Methods {\n\t\tw.Newline()\n\n\t\ttokens := []string{}\n\t\taddf := func(format string, args ...any) {\n\t\t\ts := fmt.Sprintf(format, args...)\n\t\t\ttokens = append(tokens, s)\n\t\t}\n\t\taddf(\"%s=%s\", m.Method, m.Result)\n\t\tif m.Comment != \"\" && (m.Reason != \"\" || len(m.Props) > 0) {\n\t\t\taddf(\"(%s)\", m.Comment)\n\t\t}\n\t\tif m.Reason != \"\" {\n\t\t\taddf(\"reason=%s\", value(m.Reason, false))\n\t\t}\n\t\tfor _, p := range m.Props {\n\t\t\tv := value(p.Value, p.IsAddrLike)\n\t\t\taddf(\"%s.%s=%s%s\", p.Type, p.Property, v, optComment(p.Comment))\n\t\t}\n\t\tfor j, t := range tokens {\n\t\t\tvar sep string\n\t\t\tif j > 0 {\n\t\t\t\tsep = \" \"\n\t\t\t}\n\t\t\tif j == len(tokens)-1 && i < len(h.Methods)-1 {\n\t\t\t\tt += \";\"\n\t\t\t}\n\t\t\tw.Add(sep, t)\n\t\t}\n\t}\n\treturn w.String()\n}\n\nfunc value(s string, isAddrLike bool) string {\n\tquote := s == \"\"\n\tfor _, c := range s {\n\t\t// utf-8 does not have to be quoted. ../rfc/6532:242\n\t\t// Characters outside of tokens do. ../rfc/2045:661\n\t\tif c <= ' ' || c == 0x7f || (c == '@' && !isAddrLike) || strings.ContainsRune(`()<>,;:\\\\\"/[]?= `, c) {\n\t\t\tquote = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif !quote {\n\t\treturn s\n\t}\n\tr := `\"`\n\tfor _, c := range s {\n\t\tif c == '\"' || c == '\\\\' {\n\t\t\tr += \"\\\\\"\n\t\t}\n\t\tr += string(c)\n\t}\n\tr += `\"`\n\treturn r\n}\n\n// ParseAuthResults parses a Authentication-Results header value.\n//\n// Comments are not populated in the returned AuthResults.\n// Both crlf and lf line-endings are accepted. The input string must end with\n// either crlf or lf.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype parser struct {\n\ts     string\n\tlower string // Like s, but with ascii characters lower-cased (utf-8 offsets preserved).\n\to     int\n}\n\ntype parseError struct{ err error }\n\nfunc (p *parser) recover(err *error) {\n\tx := recover()\n\tif x == nil {\n\t\treturn\n\t}\n\tperr, ok := x.(parseError)\n\tif ok {\n\t\t*err = perr.err\n\t\treturn\n\t}\n\tpanic(x)\n}\n\nfunc (p *parser) xerrorf(format string, args ...any) {\n\tpanic(parseError{fmt.Errorf(format, args...)})\n}\n\nfunc (p *parser) end() bool {\n\treturn p.s[p.o:] == \"\\r\\n\" || p.s[p.o:] == \"\\n\"\n}\n\n// ../rfc/5322:599\nfunc (p *parser) cfws() {\n\tp.fws()\n\tfor p.prefix(\"(\") {\n\t\tp.xcomment()\n\t}\n\tp.fws()\n}\n\nfunc (p *parser) fws() {\n\tfor p.take(\" \") || p.take(\"\\t\") {\n\t}\n\topts := []string{\"\\n \", \"\\n\\t\", \"\\r\\n \", \"\\r\\n\\t\"}\n\tfor _, o := range opts {\n\t\tif p.take(o) {\n\t\t\tbreak\n\t\t}\n\t}\n\tfor p.take(\" \") || p.take(\"\\t\") {\n\t}\n}\n\nfunc (p *parser) xcomment() {\n\tp.xtake(\"(\")\n\tp.fws()\n\tfor !p.take(\")\") {\n\t\tif p.empty() {\n\t\t\tp.xerrorf(\"unexpected end in comment\")\n\t\t}\n\t\tif p.prefix(\"(\") {\n\t\t\tp.xcomment()\n\t\t\tp.fws()\n\t\t\tcontinue\n\t\t}\n\t\tp.take(`\\`)\n\t\tif c := p.s[p.o]; c > ' ' && c < 0x7f {\n\t\t\tp.o++\n\t\t} else {\n\t\t\tp.xerrorf(\"bad character %c in comment\", c)\n\t\t}\n\t\tp.fws()\n\t}\n}\n\nfunc (p *parser) prefix(s string) bool {\n\treturn strings.HasPrefix(p.lower[p.o:], s)\n}\n\nfunc (p *parser) xvalue() string {\n\tif p.prefix(`\"`) {\n\t\treturn p.xquotedString()\n\t}\n\treturn p.xtakefn1(\"value token\", func(c rune, i int) bool {\n\t\t// ../rfc/2045:661\n\t\t// todo: token cannot contain utf-8? not updated in ../rfc/6532. however, we also use it for the localpart & domain parsing, so we'll allow it.\n\t\treturn c > ' ' && !strings.ContainsRune(`()<>@,;:\\\\\"/[]?= `, c)\n\t})\n}\n\nfunc (p *parser) xchar() rune {\n\t// We are careful to track invalid utf-8 properly.\n\tif p.empty() {\n\t\tp.xerrorf(\"need another character\")\n\t}\n\tvar r rune\n\tvar o int\n\tfor i, c := range p.s[p.o:] {\n\t\tif i > 0 {\n\t\t\to = i\n\t\t\tbreak\n\t\t}\n\t\tr = c\n\t}\n\tif o == 0 {\n\t\tp.o = len(p.s)\n\t} else {\n\t\tp.o += o\n\t}\n\treturn r\n}\n\nfunc (p *parser) xquotedString() string {\n\tp.xtake(`\"`)\n\tvar s string\n\tvar esc bool\n\tfor {\n\t\tc := p.xchar()\n\t\tif esc {\n\t\t\tif c >= ' ' && c < 0x7f {\n\t\t\t\ts += string(c)\n\t\t\t\tesc = false\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tp.xerrorf(\"bad escaped char %c in quoted string\", c)\n\t\t}\n\t\tif c == '\\\\' {\n\t\t\tesc = true\n\t\t\tcontinue\n\t\t}\n\t\tif c == '\"' {\n\t\t\treturn s\n\t\t}\n\t\tif c >= ' ' && c != '\\\\' && c != '\"' {\n\t\t\ts += string(c)\n\t\t\tcontinue\n\t\t}\n\t\tp.xerrorf(\"invalid quoted string, invalid character %c\", c)\n\t}\n}\n\nfunc (p *parser) digits() string {\n\to := p.o\n\tfor o < len(p.s) && p.s[o] >= '0' && p.s[o] <= '9' {\n\t\to++\n\t}\n\tp.o = o\n\treturn p.s[o:p.o]\n}\n\nfunc (p *parser) xdigits() string {\n\ts := p.digits()\n\tif s == \"\" {\n\t\tp.xerrorf(\"expected digits, remaining %q\", p.s[p.o:])\n\t}\n\treturn s\n}\n\nfunc (p *parser) xtake(s string) {\n\tif !p.prefix(s) {\n\t\tp.xerrorf(\"expected %q, remaining %q\", s, p.s[p.o:])\n\t}\n\tp.o += len(s)\n}\n\nfunc (p *parser) empty() bool {\n\treturn p.o >= len(p.s)\n}\n\nfunc (p *parser) take(s string) bool {\n\tif p.prefix(s) {\n\t\tp.o += len(s)\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (p *parser) xtakefn1(what string, fn func(c rune, i int) bool) string {\n\tif p.empty() {\n\t\tp.xerrorf(\"need at least one char for %s\", what)\n\t}\n\tfor i, c := range p.s[p.o:] {\n\t\tif !fn(c, i) {\n\t\t\tif i == 0 {\n\t\t\t\tp.xerrorf(\"expected at least one char for %s, remaining %q\", what, p.s[p.o:])\n\t\t\t}\n\t\t\ts := p.s[p.o : p.o+i]\n\t\t\tp.o += i\n\t\t\treturn s\n\t\t}\n\t}\n\ts := p.s[p.o:]\n\tp.o = len(p.s)\n\treturn s\n}\n\n// ../rfc/5321:2287\nfunc (p *parser) xkeyword(isResult bool) string {\n\ts := strings.ToLower(p.xtakefn1(\"keyword\", func(c rune, i int) bool {\n\t\t// Yahoo sends results like \"dkim=perm_fail\".\n\t\treturn c >= 'a' && c <= 'z' || c >= '0' && c <= '9' || c == '-' || isResult && !Pedantic && c == '_'\n\t}))\n\tif s == \"-\" {\n\t\tp.xerrorf(\"missing keyword\")\n\t} else if strings.HasSuffix(s, \"-\") {\n\t\tp.o--\n\t\ts = s[:len(s)-1]\n\t}\n\treturn s\n}\n\nfunc (p *parser) xmethodspec(methodKeyword string) (string, string, string) {\n\tp.cfws()\n\tvar methodDigits string\n\tif p.take(\"/\") {\n\t\tmethodDigits = p.xdigits()\n\t\tp.cfws()\n\t}\n\tp.xtake(\"=\")\n\tp.cfws()\n\tresult := p.xkeyword(true)\n\treturn methodKeyword, methodDigits, result\n}\n\nfunc (p *parser) xpropspec() (ap AuthProp) {\n\tap.Type = p.xkeyword(false)\n\tp.cfws()\n\tp.xtake(\".\")\n\tp.cfws()\n\tif p.take(\"mailfrom\") {\n\t\tap.Property = \"mailfrom\"\n\t} else if p.take(\"rcptto\") {\n\t\tap.Property = \"rcptto\"\n\t} else {\n\t\tap.Property = p.xkeyword(false)\n\t}\n\tp.cfws()\n\tp.xtake(\"=\")\n\tap.IsAddrLike, ap.Value = p.xpvalue()\n\treturn\n}\n\n// method keyword has been parsed, method-version not yet.\nfunc (p *parser) xresinfo(methodKeyword string) (am AuthMethod) {\n\tp.cfws()\n\tam.Method, am.Version, am.Result = p.xmethodspec(methodKeyword)\n\tp.cfws()\n\tif p.take(\"reason\") {\n\t\tp.cfws()\n\t\tp.xtake(\"=\")\n\t\tp.cfws()\n\t\tam.Reason = p.xvalue()\n\t}\n\tp.cfws()\n\tfor !p.prefix(\";\") && !p.end() {\n\t\tam.Props = append(am.Props, p.xpropspec())\n\t\tp.cfws()\n\t}\n\treturn\n}\n\n// todo: could keep track whether this is a localpart.\nfunc (p *parser) xpvalue() (bool, string) {\n\tp.cfws()\n\tif p.take(\"@\") {\n\t\t// Bare domain.\n\t\tdom, _ := p.xdomain()\n\t\treturn true, \"@\" + dom\n\t}\n\ts := p.xvalue()\n\tif p.take(\"@\") {\n\t\tdom, _ := p.xdomain()\n\t\ts += \"@\" + dom\n\t\treturn true, s\n\t}\n\treturn false, s\n}\n\n// ../rfc/5321:2291\nfunc (p *parser) xdomain() (string, dns.Domain) {\n\ts := p.xsubdomain()\n\tfor p.take(\".\") {\n\t\ts += \".\" + p.xsubdomain()\n\t}\n\td, err := dns.ParseDomain(s)\n\tif err != nil {\n\t\tp.xerrorf(\"parsing domain name %q: %s\", s, err)\n\t}\n\tif len(s) > 255 {\n\t\t// ../rfc/5321:3491\n\t\tp.xerrorf(\"domain longer than 255 octets\")\n\t}\n\treturn s, d\n}\n\n// ../rfc/5321:2303\n// ../rfc/5321:2303 ../rfc/6531:411\nfunc (p *parser) xsubdomain() string {\n\treturn p.xtakefn1(\"subdomain\", func(c rune, i int) bool {\n\t\treturn c >= '0' && c <= '9' || c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z' || i > 0 && c == '-' || c > 0x7f\n\t})\n}\n\n```\n# Function Signature: func ParseAuthResults(s string) (ar AuthResults, err error) {\n# Natural Language Annotations: // ParseAuthResults parses a Authentication-Results header value.\n//\n// Comments are not populated in the returned AuthResults.\n// Both crlf and lf line-endings are accepted. The input string must end with\n// either crlf or lf.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ParseAuthResults(s string) (ar AuthResults, err error) {\n\tdefer func() {\n\t\tif x := recover(); x != nil {\n\t\t\tperr, ok := x.(parseError)\n\t\t\tif ok {\n\t\t\t\terr = perr.err\n\t\t\t\treturn\n\t\t\t}\n\t\t\tpanic(x)\n\t\t}\n\t}()\n\n\tp := &parser{s: s, lower: strings.ToLower(s)}\n\tp.cfws()\n\tp.xtake(\"Authentication-Results:\")\n\tp.cfws()\n\tar.Hostname = p.xvalue()\n\tif p.prefix(\"(\") {\n\t\tp.xcomment()\n\t}\n\tp.cfws()\n\n\tfor !p.end() {\n\t\tvar m AuthMethod\n\t\tp.cfws()\n\t\tm.Method = p.xkeyword(false)\n\t\tm = p.xresinfo(m.Method)\n\t\tar.Methods = append(ar.Methods, m)\n\t\tp.cfws()\n\t}\n\treturn ar, nil\n}\n```", "```go\nfunc ParseAuthResults(s string) (ar AuthResults, err error) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr = r.(error)\n\t\t}\n\t}()\n\n\tp := &parser{s: s, lower: strings.ToLower(s)}\n\n\tar.Hostname = p.xvalue()\n\tif p.prefix(\";\") {\n\t\tp.o++\n\t}\n\tp.cfws()\n\tif p.prefix(\"spf\") || p.prefix(\"dkim\") || p.prefix(\"dmarc\") || p.prefix(\"auth\") {\n\t\tar.Methods = append(ar.Methods, p.xresinfo(\"\"))\n\t\tfor !p.empty() {\n\t\t\tp.cfws()\n\t\t\tif p.prefix(\"spf\") || p.prefix(\"dkim\") || p.prefix(\"dmarc\") || p.prefix(\"auth\") {\n\t\t\t\tar.Methods = append(ar.Methods, p.xresinfo(\"\"))\n\t\t\t}\n\t\t}\n\t}\n\tif !p.end() {\n\t\tp.xerrorf(\"expected end of input, remaining %q\", p.s[p.o:])\n\t}\n\n\treturn ar, nil\n}\n```", "```go\nfunc ParseAuthResults(s string) (ar AuthResults, err error) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\terr = r.(error)\n\t\t}\n\t}()\n\tp := &parser{s: s, lower: strings.ToLower(s)}\n\tp.cfws()\n\tp.xtake(\"Authentication-Results:\")\n\tp.cfws()\n\tar.Hostname = p.xvalue()\n\tif p.prefix(\"(\") {\n\t\tp.xcomment()\n\t}\n\tp.cfws()\n\tar.Version = \"1\"\n\tif p.prefix(\"/\") {\n\t\tar.Version = p.xdigits()\n\t\tp.cfws()\n\t}\n\tp.xtake(\";\")\n\tp.cfws()\n\tfor !p.end() {\n\t\tam := p.xresinfo(p.xkeyword(false))\n\t\tar.Methods = append(ar.Methods, am)\n\t\tp.cfws()\n\t\tif p.prefix(\";\") {\n\t\t\tp.xtake(\";\")\n\t\t\tp.cfws()\n\t\t}\n\t}\n\treturn ar, nil\n}\n```"]}, "mox-store/msgreader.go-FileMsgReader": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/store/msgreader.go:\n```\npackage store\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n)\n\n// MsgReader provides access to a message. Reads return the \"msg_prefix\" in the\n// database (typically received headers), followed by the on-disk msg file\n// contents. MsgReader is an io.Reader, io.ReaderAt and io.Closer.\ntype MsgReader struct {\n\tprefix []byte   // First part of the message. Typically contains received headers.\n\tpath   string   // To on-disk message file.\n\tsize   int64    // Total size of message, including prefix and contents from path.\n\toffset int64    // Current reading offset.\n\tf      *os.File // Opened path, automatically opened after prefix has been read.\n\terr    error    // If set, error to return for reads. Sets io.EOF for readers, but ReadAt ignores them.\n}\n\nvar errMsgClosed = errors.New(\"msg is closed\")\n\n// FileMsgReader makes a MsgReader for an open file.\n// If initialization fails, reads will return the error.\n// Only call close on the returned MsgReader if you want to close msgFile.\n\n\n\n\n\n\n\n\n\n\n\n// Read reads data from the msg, taking prefix and on-disk msg file into account.\n// The read offset is adjusted after the read.\nfunc (m *MsgReader) Read(buf []byte) (int, error) {\n\treturn m.read(buf, m.offset, false)\n}\n\n// ReadAt reads data from the msg, taking prefix and on-disk msg file into account.\n// The read offset is not affected by ReadAt.\nfunc (m *MsgReader) ReadAt(buf []byte, off int64) (n int, err error) {\n\treturn m.read(buf, off, true)\n}\n\n// read always fill buf as far as possible, for ReadAt semantics.\nfunc (m *MsgReader) read(buf []byte, off int64, pread bool) (int, error) {\n\t// If a reader has consumed the file and reached EOF, further ReadAt must not return eof.\n\tif m.err != nil && (!pread || m.err != io.EOF) {\n\t\treturn 0, m.err\n\t}\n\tvar o int\n\tfor o < len(buf) {\n\t\t// First attempt to read from m.prefix.\n\t\tpn := int64(len(m.prefix)) - off\n\t\tif pn > 0 {\n\t\t\tn := len(buf)\n\t\t\tif int64(n) > pn {\n\t\t\t\tn = int(pn)\n\t\t\t}\n\t\t\tcopy(buf[o:], m.prefix[int(off):int(off)+n])\n\t\t\to += n\n\t\t\toff += int64(n)\n\t\t\tif !pread {\n\t\t\t\tm.offset += int64(n)\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\n\t\t// Now we need to read from file. Ensure it is open.\n\t\tif m.f == nil {\n\t\t\tf, err := os.Open(m.path)\n\t\t\tif err != nil {\n\t\t\t\tm.err = err\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tm.f = f\n\t\t}\n\t\tn, err := m.f.ReadAt(buf[o:], off-int64(len(m.prefix)))\n\t\tif !pread && n > 0 {\n\t\t\tm.offset += int64(n)\n\t\t}\n\t\tif !pread || err != io.EOF {\n\t\t\tm.err = err\n\t\t}\n\t\tif n > 0 {\n\t\t\to += n\n\t\t\toff += int64(n)\n\t\t}\n\t\tif err == io.EOF {\n\t\t\tif off > m.size && (m.err == nil || m.err == io.EOF) {\n\t\t\t\terr = fmt.Errorf(\"on-disk message larger than expected (off %d, size %d)\", off, m.size)\n\t\t\t\tm.err = err\n\t\t\t}\n\t\t\treturn o, err\n\t\t}\n\t\tif n <= 0 {\n\t\t\tbreak\n\t\t}\n\t}\n\tif off > m.size && (m.err == nil || m.err == io.EOF) {\n\t\tm.err = fmt.Errorf(\"on-disk message larger than expected (off %d, size %d, prefix %d)\", off, m.size, len(m.prefix))\n\t}\n\treturn o, m.err\n}\n\n// Close ensures the msg file is closed. Further reads will fail.\nfunc (m *MsgReader) Close() error {\n\tif m.f != nil {\n\t\tif err := m.f.Close(); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tm.f = nil\n\t}\n\tif m.err == errMsgClosed {\n\t\treturn m.err\n\t}\n\tm.err = errMsgClosed\n\treturn nil\n}\n\n// Reset rewinds the offset and clears error conditions, making it usable as a fresh reader.\nfunc (m *MsgReader) Reset() {\n\tm.offset = 0\n\tm.err = nil\n}\n\n// Size returns the total size of the contents of the message.\nfunc (m *MsgReader) Size() int64 {\n\treturn m.size\n}\n\n```\n# Function Signature: func FileMsgReader(prefix []byte, msgFile *os.File) *MsgReader {\n# Natural Language Annotations: // FileMsgReader makes a MsgReader for an open file.\n// If initialization fails, reads will return the error.\n// Only call close on the returned MsgReader if you want to close msgFile.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc FileMsgReader(prefix []byte, msgFile *os.File) *MsgReader {\n\tm := &MsgReader{\n\t\tprefix: prefix,\n\t\tpath:   msgFile.Name(),\n\t\tsize:   -1, // Not known without reading the file.\n\t\tf:      msgFile,\n\t}\n\n\t// Try to get the size of the file.\n\tinfo, err := msgFile.Stat()\n\tif err != nil {\n\t\tm.err = err\n\t} else {\n\t\tm.size = info.Size()\n\t}\n\n\treturn m\n}\n```", "```go\nfunc FileMsgReader(prefix []byte, msgFile *os.File) *MsgReader {\n\tm := &MsgReader{\n\t\tprefix: prefix,\n\t\tpath:   msgFile.Name(),\n\t\tsize:   0, // Initialize size to 0 as it is not provided\n\t\toffset: 0,\n\t\tf:      msgFile,\n\t\terr:    nil,\n\t}\n\n\t// Get the size of the file\n\tif fileInfo, err := msgFile.Stat(); err == nil {\n\t\tm.size = fileInfo.Size()\n\t} else {\n\t\tm.err = err\n\t}\n\n\treturn m\n}\n```\nThis function creates a new `MsgReader` for an open file. It takes a prefix and an open file as arguments, and returns a `MsgReader`. The function also initializes the size of the file and sets it in the `MsgReader`. If there is an error while getting the file information, it sets the error in the `MsgReader`. Note that the function does not close the file, it is the responsibility of the caller to close the file if necessary.", "```go\nfunc FileMsgReader(prefix []byte, msgFile *os.File) *MsgReader {\n\tm := &MsgReader{\n\t\tprefix: prefix,\n\t\tpath:   msgFile.Name(),\n\t\tsize:   int64(len(prefix)) + getFileSize(msgFile),\n\t\toffset: 0,\n\t\tf:      msgFile,\n\t\terr:    nil,\n\t}\n\treturn m\n}\n\nfunc getFileSize(f *os.File) int64 {\n\tstat, err := f.Stat()\n\tif err != nil {\n\t\treturn 0\n\t}\n\treturn stat.Size()\n}\n```"]}, "mox-store/import.go-ParseDovecotKeywordsFlags": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/store/import.go:\n```\npackage store\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\t\"golang.org/x/exp/maps\"\n\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// MsgSource is implemented by readers for mailbox file formats.\ntype MsgSource interface {\n\t// Return next message, or io.EOF when there are no more.\n\tNext() (*Message, *os.File, string, error)\n}\n\n// MboxReader reads messages from an mbox file, implementing MsgSource.\ntype MboxReader struct {\n\tlog        mlog.Log\n\tcreateTemp func(log mlog.Log, pattern string) (*os.File, error)\n\tpath       string\n\tline       int\n\tr          *bufio.Reader\n\tprevempty  bool\n\tnonfirst   bool\n\teof        bool\n\tfromLine   string // \"From \"-line for this message.\n\theader     bool   // Now in header section.\n}\n\nfunc NewMboxReader(log mlog.Log, createTemp func(log mlog.Log, pattern string) (*os.File, error), filename string, r io.Reader) *MboxReader {\n\treturn &MboxReader{\n\t\tlog:        log,\n\t\tcreateTemp: createTemp,\n\t\tpath:       filename,\n\t\tline:       1,\n\t\tr:          bufio.NewReader(r),\n\t}\n}\n\n// Position returns \"<filename>:<lineno>\" for the current position.\nfunc (mr *MboxReader) Position() string {\n\treturn fmt.Sprintf(\"%s:%d\", mr.path, mr.line)\n}\n\n// Next returns the next message read from the mbox file. The file is a temporary\n// file and must be removed/consumed. The third return value is the position in the\n// file.\nfunc (mr *MboxReader) Next() (*Message, *os.File, string, error) {\n\tif mr.eof {\n\t\treturn nil, nil, \"\", io.EOF\n\t}\n\n\tfrom := []byte(\"From \")\n\n\tif !mr.nonfirst {\n\t\tmr.header = true\n\t\t// First read, we're at the beginning of the file.\n\t\tline, err := mr.r.ReadBytes('\\n')\n\t\tif err == io.EOF {\n\t\t\treturn nil, nil, \"\", io.EOF\n\t\t}\n\t\tmr.line++\n\n\t\tif !bytes.HasPrefix(line, from) {\n\t\t\treturn nil, nil, mr.Position(), fmt.Errorf(`first line does not start with \"From \"`)\n\t\t}\n\t\tmr.nonfirst = true\n\t\tmr.fromLine = strings.TrimSpace(string(line))\n\t}\n\n\tf, err := mr.createTemp(mr.log, \"mboxreader\")\n\tif err != nil {\n\t\treturn nil, nil, mr.Position(), err\n\t}\n\tdefer func() {\n\t\tif f != nil {\n\t\t\tCloseRemoveTempFile(mr.log, f, \"message after mbox read error\")\n\t\t}\n\t}()\n\n\tfromLine := mr.fromLine\n\tbf := bufio.NewWriter(f)\n\tvar flags Flags\n\tkeywords := map[string]bool{}\n\tvar size int64\n\tfor {\n\t\tline, err := mr.r.ReadBytes('\\n')\n\t\tif err != nil && err != io.EOF {\n\t\t\treturn nil, nil, mr.Position(), fmt.Errorf(\"reading from mbox: %v\", err)\n\t\t}\n\t\tif len(line) > 0 {\n\t\t\tmr.line++\n\t\t\t// We store data with crlf, adjust any imported messages with bare newlines.\n\t\t\tif !bytes.HasSuffix(line, []byte(\"\\r\\n\")) {\n\t\t\t\tline = append(line[:len(line)-1], \"\\r\\n\"...)\n\t\t\t}\n\n\t\t\tif mr.header {\n\t\t\t\t// See https://doc.dovecot.org/admin_manual/mailbox_formats/mbox/\n\t\t\t\tif bytes.HasPrefix(line, []byte(\"Status:\")) {\n\t\t\t\t\ts := strings.TrimSpace(strings.SplitN(string(line), \":\", 2)[1])\n\t\t\t\t\tfor _, c := range s {\n\t\t\t\t\t\tswitch c {\n\t\t\t\t\t\tcase 'R':\n\t\t\t\t\t\t\tflags.Seen = true\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if bytes.HasPrefix(line, []byte(\"X-Status:\")) {\n\t\t\t\t\ts := strings.TrimSpace(strings.SplitN(string(line), \":\", 2)[1])\n\t\t\t\t\tfor _, c := range s {\n\t\t\t\t\t\tswitch c {\n\t\t\t\t\t\tcase 'A':\n\t\t\t\t\t\t\tflags.Answered = true\n\t\t\t\t\t\tcase 'F':\n\t\t\t\t\t\t\tflags.Flagged = true\n\t\t\t\t\t\tcase 'T':\n\t\t\t\t\t\t\tflags.Draft = true\n\t\t\t\t\t\tcase 'D':\n\t\t\t\t\t\t\tflags.Deleted = true\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else if bytes.HasPrefix(line, []byte(\"X-Keywords:\")) {\n\t\t\t\t\ts := strings.TrimSpace(strings.SplitN(string(line), \":\", 2)[1])\n\t\t\t\t\tfor _, t := range strings.Split(s, \",\") {\n\t\t\t\t\t\tword := strings.ToLower(strings.TrimSpace(t))\n\t\t\t\t\t\tswitch word {\n\t\t\t\t\t\tcase \"forwarded\", \"$forwarded\":\n\t\t\t\t\t\t\tflags.Forwarded = true\n\t\t\t\t\t\tcase \"junk\", \"$junk\":\n\t\t\t\t\t\t\tflags.Junk = true\n\t\t\t\t\t\tcase \"notjunk\", \"$notjunk\", \"nonjunk\", \"$nonjunk\":\n\t\t\t\t\t\t\tflags.Notjunk = true\n\t\t\t\t\t\tcase \"phishing\", \"$phishing\":\n\t\t\t\t\t\t\tflags.Phishing = true\n\t\t\t\t\t\tcase \"mdnsent\", \"$mdnsent\":\n\t\t\t\t\t\t\tflags.MDNSent = true\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\tif err := CheckKeyword(word); err == nil {\n\t\t\t\t\t\t\t\tkeywords[word] = true\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif bytes.Equal(line, []byte(\"\\r\\n\")) {\n\t\t\t\tmr.header = false\n\t\t\t}\n\n\t\t\t// Next mail message starts at bare From word.\n\t\t\tif mr.prevempty && bytes.HasPrefix(line, from) {\n\t\t\t\tmr.fromLine = strings.TrimSpace(string(line))\n\t\t\t\tmr.header = true\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tif bytes.HasPrefix(line, []byte(\">\")) && bytes.HasPrefix(bytes.TrimLeft(line, \">\"), []byte(\"From \")) {\n\t\t\t\tline = line[1:]\n\t\t\t}\n\t\t\tn, err := bf.Write(line)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, nil, mr.Position(), fmt.Errorf(\"writing message to file: %v\", err)\n\t\t\t}\n\t\t\tsize += int64(n)\n\t\t\tmr.prevempty = bytes.Equal(line, []byte(\"\\r\\n\"))\n\t\t}\n\t\tif err == io.EOF {\n\t\t\tmr.eof = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif err := bf.Flush(); err != nil {\n\t\treturn nil, nil, mr.Position(), fmt.Errorf(\"flush: %v\", err)\n\t}\n\n\tm := &Message{Flags: flags, Keywords: maps.Keys(keywords), Size: size}\n\n\tif t := strings.SplitN(fromLine, \" \", 3); len(t) == 3 {\n\t\tlayouts := []string{time.ANSIC, time.UnixDate, time.RubyDate}\n\t\tfor _, l := range layouts {\n\t\t\tt, err := time.Parse(l, t[2])\n\t\t\tif err == nil {\n\t\t\t\tm.Received = t\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\t// Prevent cleanup by defer.\n\tmf := f\n\tf = nil\n\n\treturn m, mf, mr.Position(), nil\n}\n\ntype MaildirReader struct {\n\tlog          mlog.Log\n\tcreateTemp   func(log mlog.Log, pattern string) (*os.File, error)\n\tnewf, curf   *os.File\n\tf            *os.File // File we are currently reading from. We first read newf, then curf.\n\tdir          string   // Name of directory for f. Can be empty on first call.\n\tentries      []os.DirEntry\n\tdovecotFlags []string // Lower-case flags/keywords.\n}\n\nfunc NewMaildirReader(log mlog.Log, createTemp func(log mlog.Log, pattern string) (*os.File, error), newf, curf *os.File) *MaildirReader {\n\tmr := &MaildirReader{\n\t\tlog:        log,\n\t\tcreateTemp: createTemp,\n\t\tnewf:       newf,\n\t\tcurf:       curf,\n\t\tf:          newf,\n\t}\n\n\t// Best-effort parsing of dovecot keywords.\n\tkf, err := os.Open(filepath.Join(filepath.Dir(newf.Name()), \"dovecot-keywords\"))\n\tif err == nil {\n\t\tmr.dovecotFlags, err = ParseDovecotKeywordsFlags(kf, log)\n\t\tlog.Check(err, \"parsing dovecot keywords file\")\n\t\terr = kf.Close()\n\t\tlog.Check(err, \"closing dovecot-keywords file\")\n\t}\n\n\treturn mr\n}\n\nfunc (mr *MaildirReader) Next() (*Message, *os.File, string, error) {\n\tif mr.dir == \"\" {\n\t\tmr.dir = mr.f.Name()\n\t}\n\n\tif len(mr.entries) == 0 {\n\t\tvar err error\n\t\tmr.entries, err = mr.f.ReadDir(100)\n\t\tif err != nil && err != io.EOF {\n\t\t\treturn nil, nil, \"\", err\n\t\t}\n\t\tif len(mr.entries) == 0 {\n\t\t\tif mr.f == mr.curf {\n\t\t\t\treturn nil, nil, \"\", io.EOF\n\t\t\t}\n\t\t\tmr.f = mr.curf\n\t\t\tmr.dir = \"\"\n\t\t\treturn mr.Next()\n\t\t}\n\t}\n\n\tp := filepath.Join(mr.dir, mr.entries[0].Name())\n\tmr.entries = mr.entries[1:]\n\tsf, err := os.Open(p)\n\tif err != nil {\n\t\treturn nil, nil, p, fmt.Errorf(\"open message in maildir: %s\", err)\n\t}\n\tdefer func() {\n\t\terr := sf.Close()\n\t\tmr.log.Check(err, \"closing message file after error\")\n\t}()\n\tf, err := mr.createTemp(mr.log, \"maildirreader\")\n\tif err != nil {\n\t\treturn nil, nil, p, err\n\t}\n\tdefer func() {\n\t\tif f != nil {\n\t\t\tname := f.Name()\n\t\t\terr := f.Close()\n\t\t\tmr.log.Check(err, \"closing temporary message file after maildir read error\")\n\t\t\terr = os.Remove(name)\n\t\t\tmr.log.Check(err, \"removing temporary message file after maildir read error\", slog.String(\"path\", name))\n\t\t}\n\t}()\n\n\t// Copy data, changing bare \\n into \\r\\n.\n\tr := bufio.NewReader(sf)\n\tw := bufio.NewWriter(f)\n\tvar size int64\n\tfor {\n\t\tline, err := r.ReadBytes('\\n')\n\t\tif err != nil && err != io.EOF {\n\t\t\treturn nil, nil, p, fmt.Errorf(\"reading message: %v\", err)\n\t\t}\n\t\tif len(line) > 0 {\n\t\t\tif !bytes.HasSuffix(line, []byte(\"\\r\\n\")) {\n\t\t\t\tline = append(line[:len(line)-1], \"\\r\\n\"...)\n\t\t\t}\n\n\t\t\tif n, err := w.Write(line); err != nil {\n\t\t\t\treturn nil, nil, p, fmt.Errorf(\"writing message: %v\", err)\n\t\t\t} else {\n\t\t\t\tsize += int64(n)\n\t\t\t}\n\t\t}\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t}\n\t}\n\tif err := w.Flush(); err != nil {\n\t\treturn nil, nil, p, fmt.Errorf(\"writing message: %v\", err)\n\t}\n\n\t// Take received time from filename, falling back to mtime for maildirs\n\t// reconstructed some other sources of message files.\n\tvar received time.Time\n\tt := strings.SplitN(filepath.Base(sf.Name()), \".\", 3)\n\tif v, err := strconv.ParseInt(t[0], 10, 64); len(t) == 3 && err == nil {\n\t\treceived = time.Unix(v, 0)\n\t} else if fi, err := sf.Stat(); err == nil {\n\t\treceived = fi.ModTime()\n\t}\n\n\t// Parse flags. See https://cr.yp.to/proto/maildir.html.\n\tflags := Flags{}\n\tkeywords := map[string]bool{}\n\tt = strings.SplitN(filepath.Base(sf.Name()), \":2,\", 2)\n\tif len(t) == 2 {\n\t\tfor _, c := range t[1] {\n\t\t\tswitch c {\n\t\t\tcase 'P':\n\t\t\t\t// Passed, doesn't map to a common IMAP flag.\n\t\t\tcase 'R':\n\t\t\t\tflags.Answered = true\n\t\t\tcase 'S':\n\t\t\t\tflags.Seen = true\n\t\t\tcase 'T':\n\t\t\t\tflags.Deleted = true\n\t\t\tcase 'D':\n\t\t\t\tflags.Draft = true\n\t\t\tcase 'F':\n\t\t\t\tflags.Flagged = true\n\t\t\tdefault:\n\t\t\t\tif c >= 'a' && c <= 'z' {\n\t\t\t\t\tindex := int(c - 'a')\n\t\t\t\t\tif index >= len(mr.dovecotFlags) {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tkw := mr.dovecotFlags[index]\n\t\t\t\t\tswitch kw {\n\t\t\t\t\tcase \"$forwarded\", \"forwarded\":\n\t\t\t\t\t\tflags.Forwarded = true\n\t\t\t\t\tcase \"$junk\", \"junk\":\n\t\t\t\t\t\tflags.Junk = true\n\t\t\t\t\tcase \"$notjunk\", \"notjunk\", \"nonjunk\":\n\t\t\t\t\t\tflags.Notjunk = true\n\t\t\t\t\tcase \"$mdnsent\", \"mdnsent\":\n\t\t\t\t\t\tflags.MDNSent = true\n\t\t\t\t\tcase \"$phishing\", \"phishing\":\n\t\t\t\t\t\tflags.Phishing = true\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tkeywords[kw] = true\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tm := &Message{Received: received, Flags: flags, Keywords: maps.Keys(keywords), Size: size}\n\n\t// Prevent cleanup by defer.\n\tmf := f\n\tf = nil\n\n\treturn m, mf, p, nil\n}\n\n// ParseDovecotKeywordsFlags attempts to parse a dovecot-keywords file. It only\n// returns valid flags/keywords, as lower-case. If an error is encountered and\n// returned, any keywords that were found are still returned. The returned list has\n// both system/well-known flags and custom keywords.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func ParseDovecotKeywordsFlags(r io.Reader, log mlog.Log) ([]string, error) {\n# Natural Language Annotations: // ParseDovecotKeywordsFlags attempts to parse a dovecot-keywords file. It only\n// returns valid flags/keywords, as lower-case. If an error is encountered and\n// returned, any keywords that were found are still returned. The returned list has\n// both system/well-known flags and custom keywords.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ParseDovecotKeywordsFlags(r io.Reader, log mlog.Log) ([]string, error) {\n\tscanner := bufio.NewScanner(r)\n\tvar flags []string\n\tvar lineNumber int\n\tfor scanner.Scan() {\n\t\tlineNumber++\n\t\tline := scanner.Text()\n\t\tfor _, field := range strings.Fields(line) {\n\t\t\tfield = strings.ToLower(field)\n\t\t\tif err := CheckKeyword(field); err == nil {\n\t\t\t\tflags = append(flags, field)\n\t\t\t} else {\n\t\t\t\tlog.Error(\"invalid keyword\", slog.String(\"keyword\", field), slog.Int(\"line\", lineNumber))\n\t\t\t}\n\t\t}\n\t}\n\tif err := scanner.Err(); err != nil {\n\t\treturn nil, fmt.Errorf(\"scanning dovecot-keywords file: %v\", err)\n\t}\n\treturn flags, nil\n}\n```", "```go\nfunc ParseDovecotKeywordsFlags(r io.Reader, log mlog.Log) ([]string, error) {\n\tkeywords := []string{}\n\tscanner := bufio.NewScanner(r)\n\tline := 1\n\tfor scanner.Scan() {\n\t\ttokens := strings.Split(scanner.Text(), \" \")\n\t\tfor _, token := range tokens {\n\t\t\ttoken = strings.TrimSpace(token)\n\t\t\tif token != \"\" {\n\t\t\t\terr := CheckKeyword(token)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Printf(\"Error at line %d: %s\", line, err)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tkeywords = append(keywords, strings.ToLower(token))\n\t\t\t}\n\t\t}\n\t\tline++\n\t}\n\tif err := scanner.Err(); err != nil {\n\t\treturn keywords, fmt.Errorf(\"error reading dovecot-keywords: %v\", err)\n\t}\n\treturn keywords, nil\n}\n```", "```go\nfunc ParseDovecotKeywordsFlags(r io.Reader, log mlog.Log) ([]string, error) {\n\tscanner := bufio.NewScanner(r)\n\tvar flags []string\n\tfor scanner.Scan() {\n\t\tline := scanner.Text()\n\t\tparts := strings.Split(line, \"=\")\n\t\tif len(parts) != 2 {\n\t\t\treturn flags, fmt.Errorf(\"invalid line in dovecot-keywords file: %s\", line)\n\t\t}\n\t\tkey := strings.TrimSpace(parts[0])\n\t\tvalue := strings.TrimSpace(parts[1])\n\t\tif key != \"user_keywords\" {\n\t\t\treturn flags, fmt.Errorf(\"unexpected key in dovecot-keywords file: %s\", key)\n\t\t}\n\t\tflags = strings.Split(value, \" \")\n\t\tfor i := range flags {\n\t\t\tflags[i] = strings.ToLower(flags[i])\n\t\t}\n\t}\n\tif err := scanner.Err(); err != nil {\n\t\treturn flags, err\n\t}\n\treturn flags, nil\n}\n```"]}, "mox-store/account.go-MessageRuleset": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/store/account.go:\n```\n/*\nPackage store implements storage for accounts, their mailboxes, IMAP\nsubscriptions and messages, and broadcasts updates (e.g. mail delivery) to\ninterested sessions (e.g. IMAP connections).\n\nLayout of storage for accounts:\n\n\t<DataDir>/accounts/<name>/index.db\n\t<DataDir>/accounts/<name>/msg/[a-zA-Z0-9_-]+/<id>\n\nIndex.db holds tables for user information, mailboxes, and messages. Messages\nare stored in the msg/ subdirectory, each in their own file. The on-disk message\ndoes not contain headers generated during an incoming SMTP transaction, such as\nReceived and Authentication-Results headers. Those are in the database to\nprevent having to rewrite incoming messages (e.g. Authentication-Result for DKIM\nsignatures can only be determined after having read the message). Messages must\nbe read through MsgReader, which transparently adds the prefix from the\ndatabase.\n*/\npackage store\n\n// todo: make up a function naming scheme that indicates whether caller should broadcast changes.\n\nimport (\n\t\"context\"\n\t\"crypto/md5\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/sha1\"\n\t\"crypto/sha256\"\n\t\"encoding\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"io\"\n\t\"log/slog\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime/debug\"\n\t\"slices\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/crypto/bcrypt\"\n\t\"golang.org/x/text/secure/precis\"\n\t\"golang.org/x/text/unicode/norm\"\n\n\t\"github.com/mjl-/bstore\"\n\n\t\"github.com/mjl-/mox/config\"\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/metrics\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/moxio\"\n\t\"github.com/mjl-/mox/publicsuffix\"\n\t\"github.com/mjl-/mox/scram\"\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// If true, each time an account is closed its database file is checked for\n// consistency. If an inconsistency is found, panic is called. Set by default\n// because of all the packages with tests, the mox main function sets it to\n// false again.\nvar CheckConsistencyOnClose = true\n\nvar (\n\tErrUnknownMailbox     = errors.New(\"no such mailbox\")\n\tErrUnknownCredentials = errors.New(\"credentials not found\")\n\tErrAccountUnknown     = errors.New(\"no such account\")\n\tErrOverQuota          = errors.New(\"account over quota\")\n)\n\nvar DefaultInitialMailboxes = config.InitialMailboxes{\n\tSpecialUse: config.SpecialUseMailboxes{\n\t\tSent:    \"Sent\",\n\t\tArchive: \"Archive\",\n\t\tTrash:   \"Trash\",\n\t\tDraft:   \"Drafts\",\n\t\tJunk:    \"Junk\",\n\t},\n}\n\ntype SCRAM struct {\n\tSalt           []byte\n\tIterations     int\n\tSaltedPassword []byte\n}\n\n// CRAMMD5 holds HMAC ipad and opad hashes that are initialized with the first\n// block with (a derivation of) the key/password, so we don't store the password in plain\n// text.\ntype CRAMMD5 struct {\n\tIpad hash.Hash\n\tOpad hash.Hash\n}\n\n// BinaryMarshal is used by bstore to store the ipad/opad hash states.\nfunc (c CRAMMD5) MarshalBinary() ([]byte, error) {\n\tif c.Ipad == nil || c.Opad == nil {\n\t\treturn nil, nil\n\t}\n\n\tipad, err := c.Ipad.(encoding.BinaryMarshaler).MarshalBinary()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"marshal ipad: %v\", err)\n\t}\n\topad, err := c.Opad.(encoding.BinaryMarshaler).MarshalBinary()\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"marshal opad: %v\", err)\n\t}\n\tbuf := make([]byte, 2+len(ipad)+len(opad))\n\tipadlen := uint16(len(ipad))\n\tbuf[0] = byte(ipadlen >> 8)\n\tbuf[1] = byte(ipadlen >> 0)\n\tcopy(buf[2:], ipad)\n\tcopy(buf[2+len(ipad):], opad)\n\treturn buf, nil\n}\n\n// BinaryUnmarshal is used by bstore to restore the ipad/opad hash states.\nfunc (c *CRAMMD5) UnmarshalBinary(buf []byte) error {\n\tif len(buf) == 0 {\n\t\t*c = CRAMMD5{}\n\t\treturn nil\n\t}\n\tif len(buf) < 2 {\n\t\treturn fmt.Errorf(\"short buffer\")\n\t}\n\tipadlen := int(uint16(buf[0])<<8 | uint16(buf[1])<<0)\n\tif len(buf) < 2+ipadlen {\n\t\treturn fmt.Errorf(\"buffer too short for ipadlen\")\n\t}\n\tipad := md5.New()\n\topad := md5.New()\n\tif err := ipad.(encoding.BinaryUnmarshaler).UnmarshalBinary(buf[2 : 2+ipadlen]); err != nil {\n\t\treturn fmt.Errorf(\"unmarshal ipad: %v\", err)\n\t}\n\tif err := opad.(encoding.BinaryUnmarshaler).UnmarshalBinary(buf[2+ipadlen:]); err != nil {\n\t\treturn fmt.Errorf(\"unmarshal opad: %v\", err)\n\t}\n\t*c = CRAMMD5{ipad, opad}\n\treturn nil\n}\n\n// Password holds credentials in various forms, for logging in with SMTP/IMAP.\ntype Password struct {\n\tHash        string  // bcrypt hash for IMAP LOGIN, SASL PLAIN and HTTP basic authentication.\n\tCRAMMD5     CRAMMD5 // For SASL CRAM-MD5.\n\tSCRAMSHA1   SCRAM   // For SASL SCRAM-SHA-1.\n\tSCRAMSHA256 SCRAM   // For SASL SCRAM-SHA-256.\n}\n\n// Subjectpass holds the secret key used to sign subjectpass tokens.\ntype Subjectpass struct {\n\tEmail string // Our destination address (canonical, with catchall localpart stripped).\n\tKey   string\n}\n\n// NextUIDValidity is a singleton record in the database with the next UIDValidity\n// to use for the next mailbox.\ntype NextUIDValidity struct {\n\tID   int // Just a single record with ID 1.\n\tNext uint32\n}\n\n// SyncState track ModSeqs.\ntype SyncState struct {\n\tID int // Just a single record with ID 1.\n\n\t// Last used, next assigned will be one higher. The first value we hand out is 2.\n\t// That's because 0 (the default value for old existing messages, from before the\n\t// Message.ModSeq field) is special in IMAP, so we return it as 1.\n\tLastModSeq ModSeq `bstore:\"nonzero\"`\n\n\t// Highest ModSeq of expunged record that we deleted. When a clients synchronizes\n\t// and requests changes based on a modseq before this one, we don't have the\n\t// history to provide information about deletions. We normally keep these expunged\n\t// records around, but we may periodically truly delete them to reclaim storage\n\t// space. Initially set to -1 because we don't want to match with any ModSeq in the\n\t// database, which can be zero values.\n\tHighestDeletedModSeq ModSeq\n}\n\n// Mailbox is collection of messages, e.g. Inbox or Sent.\ntype Mailbox struct {\n\tID int64\n\n\t// \"Inbox\" is the name for the special IMAP \"INBOX\". Slash separated\n\t// for hierarchy.\n\tName string `bstore:\"nonzero,unique\"`\n\n\t// If UIDs are invalidated, e.g. when renaming a mailbox to a previously existing\n\t// name, UIDValidity must be changed. Used by IMAP for synchronization.\n\tUIDValidity uint32\n\n\t// UID likely to be assigned to next message. Used by IMAP to detect messages\n\t// delivered to a mailbox.\n\tUIDNext UID\n\n\tSpecialUse\n\n\t// Keywords as used in messages. Storing a non-system keyword for a message\n\t// automatically adds it to this list. Used in the IMAP FLAGS response. Only\n\t// \"atoms\" are allowed (IMAP syntax), keywords are case-insensitive, only stored in\n\t// lower case (for JMAP), sorted.\n\tKeywords []string\n\n\tHaveCounts    bool // Whether MailboxCounts have been initialized.\n\tMailboxCounts      // Statistics about messages, kept up to date whenever a change happens.\n}\n\n// MailboxCounts tracks statistics about messages for a mailbox.\ntype MailboxCounts struct {\n\tTotal   int64 // Total number of messages, excluding \\Deleted. For JMAP.\n\tDeleted int64 // Number of messages with \\Deleted flag. Used for IMAP message count that includes messages with \\Deleted.\n\tUnread  int64 // Messages without \\Seen, excluding those with \\Deleted, for JMAP.\n\tUnseen  int64 // Messages without \\Seen, including those with \\Deleted, for IMAP.\n\tSize    int64 // Number of bytes for all messages.\n}\n\nfunc (mc MailboxCounts) String() string {\n\treturn fmt.Sprintf(\"%d total, %d deleted, %d unread, %d unseen, size %d bytes\", mc.Total, mc.Deleted, mc.Unread, mc.Unseen, mc.Size)\n}\n\n// Add increases mailbox counts mc with those of delta.\nfunc (mc *MailboxCounts) Add(delta MailboxCounts) {\n\tmc.Total += delta.Total\n\tmc.Deleted += delta.Deleted\n\tmc.Unread += delta.Unread\n\tmc.Unseen += delta.Unseen\n\tmc.Size += delta.Size\n}\n\n// Add decreases mailbox counts mc with those of delta.\nfunc (mc *MailboxCounts) Sub(delta MailboxCounts) {\n\tmc.Total -= delta.Total\n\tmc.Deleted -= delta.Deleted\n\tmc.Unread -= delta.Unread\n\tmc.Unseen -= delta.Unseen\n\tmc.Size -= delta.Size\n}\n\n// SpecialUse identifies a specific role for a mailbox, used by clients to\n// understand where messages should go.\ntype SpecialUse struct {\n\tArchive bool\n\tDraft   bool\n\tJunk    bool\n\tSent    bool\n\tTrash   bool\n}\n\n// CalculateCounts calculates the full current counts for messages in the mailbox.\nfunc (mb *Mailbox) CalculateCounts(tx *bstore.Tx) (mc MailboxCounts, err error) {\n\tq := bstore.QueryTx[Message](tx)\n\tq.FilterNonzero(Message{MailboxID: mb.ID})\n\tq.FilterEqual(\"Expunged\", false)\n\terr = q.ForEach(func(m Message) error {\n\t\tmc.Add(m.MailboxCounts())\n\t\treturn nil\n\t})\n\treturn\n}\n\n// ChangeSpecialUse returns a change for special-use flags, for broadcasting to\n// other connections.\nfunc (mb Mailbox) ChangeSpecialUse() ChangeMailboxSpecialUse {\n\treturn ChangeMailboxSpecialUse{mb.ID, mb.Name, mb.SpecialUse}\n}\n\n// ChangeKeywords returns a change with new keywords for a mailbox (e.g. after\n// setting a new keyword on a message in the mailbox), for broadcasting to other\n// connections.\nfunc (mb Mailbox) ChangeKeywords() ChangeMailboxKeywords {\n\treturn ChangeMailboxKeywords{mb.ID, mb.Name, mb.Keywords}\n}\n\n// KeywordsChanged returns whether the keywords in a mailbox have changed.\nfunc (mb Mailbox) KeywordsChanged(origmb Mailbox) bool {\n\tif len(mb.Keywords) != len(origmb.Keywords) {\n\t\treturn true\n\t}\n\t// Keywords are stored sorted.\n\tfor i, kw := range mb.Keywords {\n\t\tif origmb.Keywords[i] != kw {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// CountsChange returns a change with mailbox counts.\nfunc (mb Mailbox) ChangeCounts() ChangeMailboxCounts {\n\treturn ChangeMailboxCounts{mb.ID, mb.Name, mb.MailboxCounts}\n}\n\n// Subscriptions are separate from existence of mailboxes.\ntype Subscription struct {\n\tName string\n}\n\n// Flags for a mail message.\ntype Flags struct {\n\tSeen      bool\n\tAnswered  bool\n\tFlagged   bool\n\tForwarded bool\n\tJunk      bool\n\tNotjunk   bool\n\tDeleted   bool\n\tDraft     bool\n\tPhishing  bool\n\tMDNSent   bool\n}\n\n// FlagsAll is all flags set, for use as mask.\nvar FlagsAll = Flags{true, true, true, true, true, true, true, true, true, true}\n\n// Validation of \"message From\" domain.\ntype Validation uint8\n\nconst (\n\tValidationUnknown   Validation = 0\n\tValidationStrict    Validation = 1 // Like DMARC, with strict policies.\n\tValidationDMARC     Validation = 2 // Actual DMARC policy.\n\tValidationRelaxed   Validation = 3 // Like DMARC, with relaxed policies.\n\tValidationPass      Validation = 4 // For SPF.\n\tValidationNeutral   Validation = 5 // For SPF.\n\tValidationTemperror Validation = 6\n\tValidationPermerror Validation = 7\n\tValidationFail      Validation = 8\n\tValidationSoftfail  Validation = 9  // For SPF.\n\tValidationNone      Validation = 10 // E.g. No records.\n)\n\n// Message stored in database and per-message file on disk.\n//\n// Contents are always the combined data from MsgPrefix and the on-disk file named\n// based on ID.\n//\n// Messages always have a header section, even if empty. Incoming messages without\n// header section must get an empty header section added before inserting.\ntype Message struct {\n\t// ID, unchanged over lifetime, determines path to on-disk msg file.\n\t// Set during deliver.\n\tID int64\n\n\tUID       UID   `bstore:\"nonzero\"` // UID, for IMAP. Set during deliver.\n\tMailboxID int64 `bstore:\"nonzero,unique MailboxID+UID,index MailboxID+Received,index MailboxID+ModSeq,ref Mailbox\"`\n\n\t// Modification sequence, for faster syncing with IMAP QRESYNC and JMAP.\n\t// ModSeq is the last modification. CreateSeq is the Seq the message was inserted,\n\t// always <= ModSeq. If Expunged is set, the message has been removed and should not\n\t// be returned to the user. In this case, ModSeq is the Seq where the message is\n\t// removed, and will never be changed again.\n\t// We have an index on both ModSeq (for JMAP that synchronizes per account) and\n\t// MailboxID+ModSeq (for IMAP that synchronizes per mailbox).\n\t// The index on CreateSeq helps efficiently finding created messages for JMAP.\n\t// The value of ModSeq is special for IMAP. Messages that existed before ModSeq was\n\t// added have 0 as value. But modseq 0 in IMAP is special, so we return it as 1. If\n\t// we get modseq 1 from a client, the IMAP server will translate it to 0. When we\n\t// return modseq to clients, we turn 0 into 1.\n\tModSeq    ModSeq `bstore:\"index\"`\n\tCreateSeq ModSeq `bstore:\"index\"`\n\tExpunged  bool\n\n\t// If set, this message was delivered to a Rejects mailbox. When it is moved to a\n\t// different mailbox, its MailboxOrigID is set to the destination mailbox and this\n\t// flag cleared.\n\tIsReject bool\n\n\t// If set, this is a forwarded message (through a ruleset with IsForward). This\n\t// causes fields used during junk analysis to be moved to their Orig variants, and\n\t// masked IP fields cleared, so they aren't used in junk classifications for\n\t// incoming messages. This ensures the forwarded messages don't cause negative\n\t// reputation for the forwarding mail server, which may also be sending regular\n\t// messages.\n\tIsForward bool\n\n\t// MailboxOrigID is the mailbox the message was originally delivered to. Typically\n\t// Inbox or Rejects, but can also be a mailbox configured in a Ruleset, or\n\t// Postmaster, TLS/DMARC reporting addresses. MailboxOrigID is not changed when the\n\t// message is moved to another mailbox, e.g. Archive/Trash/Junk. Used for\n\t// per-mailbox reputation.\n\t//\n\t// MailboxDestinedID is normally 0, but when a message is delivered to the Rejects\n\t// mailbox, it is set to the intended mailbox according to delivery rules,\n\t// typically that of Inbox. When such a message is moved out of Rejects, the\n\t// MailboxOrigID is corrected by setting it to MailboxDestinedID. This ensures the\n\t// message is used for reputation calculation for future deliveries to that\n\t// mailbox.\n\t//\n\t// These are not bstore references to prevent having to update all messages in a\n\t// mailbox when the original mailbox is removed. Use of these fields requires\n\t// checking if the mailbox still exists.\n\tMailboxOrigID     int64\n\tMailboxDestinedID int64\n\n\tReceived time.Time `bstore:\"default now,index\"`\n\n\t// Full IP address of remote SMTP server. Empty if not delivered over SMTP. The\n\t// masked IPs are used to classify incoming messages. They are left empty for\n\t// messages matching a ruleset for forwarded messages.\n\tRemoteIP        string\n\tRemoteIPMasked1 string `bstore:\"index RemoteIPMasked1+Received\"` // For IPv4 /32, for IPv6 /64, for reputation.\n\tRemoteIPMasked2 string `bstore:\"index RemoteIPMasked2+Received\"` // For IPv4 /26, for IPv6 /48.\n\tRemoteIPMasked3 string `bstore:\"index RemoteIPMasked3+Received\"` // For IPv4 /21, for IPv6 /32.\n\n\t// Only set if present and not an IP address. Unicode string. Empty for forwarded\n\t// messages.\n\tEHLODomain        string         `bstore:\"index EHLODomain+Received\"`\n\tMailFrom          string         // With localpart and domain. Can be empty.\n\tMailFromLocalpart smtp.Localpart // SMTP \"MAIL FROM\", can be empty.\n\t// Only set if it is a domain, not an IP. Unicode string. Empty for forwarded\n\t// messages, but see OrigMailFromDomain.\n\tMailFromDomain  string         `bstore:\"index MailFromDomain+Received\"`\n\tRcptToLocalpart smtp.Localpart // SMTP \"RCPT TO\", can be empty.\n\tRcptToDomain    string         // Unicode string.\n\n\t// Parsed \"From\" message header, used for reputation along with domain validation.\n\tMsgFromLocalpart smtp.Localpart\n\tMsgFromDomain    string `bstore:\"index MsgFromDomain+Received\"`    // Unicode string.\n\tMsgFromOrgDomain string `bstore:\"index MsgFromOrgDomain+Received\"` // Unicode string.\n\n\t// Simplified statements of the Validation fields below, used for incoming messages\n\t// to check reputation.\n\tEHLOValidated     bool\n\tMailFromValidated bool\n\tMsgFromValidated  bool\n\n\tEHLOValidation     Validation // Validation can also take reverse IP lookup into account, not only SPF.\n\tMailFromValidation Validation // Can have SPF-specific validations like ValidationSoftfail.\n\tMsgFromValidation  Validation // Desirable validations: Strict, DMARC, Relaxed. Will not be just Pass.\n\n\t// Domains with verified DKIM signatures. Unicode string. For forwarded messages, a\n\t// DKIM domain that matched a ruleset's verified domain is left out, but included\n\t// in OrigDKIMDomains.\n\tDKIMDomains []string `bstore:\"index DKIMDomains+Received\"`\n\n\t// For forwarded messages,\n\tOrigEHLODomain  string\n\tOrigDKIMDomains []string\n\n\t// Canonicalized Message-Id, always lower-case and normalized quoting, without\n\t// <>'s. Empty if missing. Used for matching message threads, and to prevent\n\t// duplicate reject delivery.\n\tMessageID string `bstore:\"index\"`\n\t// lower-case: ../rfc/5256:495\n\n\t// For matching threads in case there is no References/In-Reply-To header. It is\n\t// lower-cased, white-space collapsed, mailing list tags and re/fwd tags removed.\n\tSubjectBase string `bstore:\"index\"`\n\t// ../rfc/5256:90\n\n\t// Hash of message. For rejects delivery in case there is no Message-ID, only set\n\t// when delivered as reject.\n\tMessageHash []byte\n\n\t// ID of message starting this thread.\n\tThreadID int64 `bstore:\"index\"`\n\t// IDs of parent messages, from closest parent to the root message. Parent messages\n\t// may be in a different mailbox, or may no longer exist. ThreadParentIDs must\n\t// never contain the message id itself (a cycle), and parent messages must\n\t// reference the same ancestors.\n\tThreadParentIDs []int64\n\t// ThreadMissingLink is true if there is no match with a direct parent. E.g. first\n\t// ID in ThreadParentIDs is not the direct ancestor (an intermediate message may\n\t// have been deleted), or subject-based matching was done.\n\tThreadMissingLink bool\n\t// If set, newly delivered child messages are automatically marked as read. This\n\t// field is copied to new child messages. Changes are propagated to the webmail\n\t// client.\n\tThreadMuted bool\n\t// If set, this (sub)thread is collapsed in the webmail client, for threading mode\n\t// \"on\" (mode \"unread\" ignores it). This field is copied to new child message.\n\t// Changes are propagated to the webmail client.\n\tThreadCollapsed bool\n\n\t// If received message was known to match a mailing list rule (with modified junk\n\t// filtering).\n\tIsMailingList bool\n\n\t// If this message is a DSN, generated by us or received. For DSNs, we don't look\n\t// at the subject when matching threads.\n\tDSN bool\n\n\tReceivedTLSVersion     uint16 // 0 if unknown, 1 if plaintext/no TLS, otherwise TLS cipher suite.\n\tReceivedTLSCipherSuite uint16\n\tReceivedRequireTLS     bool // Whether RequireTLS was known to be used for incoming delivery.\n\n\tFlags\n\t// For keywords other than system flags or the basic well-known $-flags. Only in\n\t// \"atom\" syntax (IMAP), they are case-insensitive, always stored in lower-case\n\t// (for JMAP), sorted.\n\tKeywords    []string `bstore:\"index\"`\n\tSize        int64\n\tTrainedJunk *bool  // If nil, no training done yet. Otherwise, true is trained as junk, false trained as nonjunk.\n\tMsgPrefix   []byte // Typically holds received headers and/or header separator.\n\n\t// ParsedBuf message structure. Currently saved as JSON of message.Part because bstore\n\t// cannot yet store recursive types. Created when first needed, and saved in the\n\t// database.\n\t// todo: once replaced with non-json storage, remove date fixup in ../message/part.go.\n\tParsedBuf []byte\n}\n\n// MailboxCounts returns the delta to counts this message means for its\n// mailbox.\nfunc (m Message) MailboxCounts() (mc MailboxCounts) {\n\tif m.Expunged\n```\n# Function Signature: func MessageRuleset(log mlog.Log, dest config.Destination, m *Message, msgPrefix []byte, msgFile *os.File) *config.Ruleset {\n# Natural Language Annotations: // MessageRuleset returns the first ruleset (if any) that matches the message\n// represented by msgPrefix and msgFile, with smtp and validation fields from m.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc MessageRuleset(log mlog.Log, dest config.Destination, m *Message, msgPrefix []byte, msgFile *os.File) *config.Ruleset {\n    for _, ruleset := range dest.Rulesets {\n        if rulesetMatches(ruleset, m, msgPrefix, msgFile) {\n            return ruleset\n        }\n    }\n    return nil\n}\n```", "```go\nfunc MessageRuleset(log mlog.Log, dest config.Destination, m *Message, msgPrefix []byte, msgFile *os.File) *config.Ruleset {\n\t// Parse the message headers from msgPrefix and msgFile\n\theaders, err := message.ParseHeaders(msgPrefix, msgFile)\n\tif err != nil {\n\t\tlog.Printf(\"failed to parse headers: %v\", err)\n\t\treturn nil\n\t}\n\n\t// Check if the message matches any ruleset\n\tfor _, ruleset := range dest.Rulesets {\n\t\tif rulesetMatches(ruleset, m, headers) {\n\t\t\treturn &ruleset\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc rulesetMatches(ruleset config.Ruleset, m *Message, headers message.Headers) bool {\n\t// Check if the message headers match the ruleset\n\tfor _, match := range ruleset.Matches {\n\t\tif match.Field == \"From\" {\n\t\t\tif m.MsgFromLocalpart != match.Localpart || m.MsgFromDomain != match.Domain {\n\t\t\t\treturn false\n\t\t\t}\n\t\t} else if match.Field == \"To\" {\n\t\t\tif m.RcptToLocalpart != match.Localpart || m.RcptToDomain != match.Domain {\n\t\t\t\treturn false\n\t\t\t}\n\t\t} else if match.Field == \"EHLO\" {\n\t\t\tif m.EHLODomain != match.Domain {\n\t\t\t\treturn false\n\t\t\t}\n\t\t} else if match.Field == \"Header\" {\n\t\t\tif _, ok := headers[match.Header]; !ok {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t}\n\n\treturn true\n}\n```\nPlease note that the `message.ParseHeaders` function is not provided in the given context, and you may need to implement it based on your specific requirements.", "```go\nfunc MessageRuleset(log mlog.Log, dest config.Destination, m *Message, msgPrefix []byte, msgFile *os.File) *config.Ruleset {\n    for _, ruleset := range dest.Rulesets {\n        if ruleset.MatchMessage(log, m, msgPrefix, msgFile) {\n            return &ruleset\n        }\n    }\n    return nil\n}\n```"]}, "mox-smtp/address.go-ParseAddress": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/smtp/address.go:\n```\npackage smtp\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"golang.org/x/text/unicode/norm\"\n\n\t\"github.com/mjl-/mox/dns\"\n)\n\n// Pedantic enables stricter parsing.\nvar Pedantic bool\n\nvar ErrBadAddress = errors.New(\"invalid email address\")\n\n// Localpart is a decoded local part of an email address, before the \"@\".\n// For quoted strings, values do not hold the double quote or escaping backslashes.\n// An empty string can be a valid localpart.\n// Localparts are in Unicode NFC.\ntype Localpart string\n\n// String returns a packed representation of an address, with proper escaping/quoting, for use in SMTP.\nfunc (lp Localpart) String() string {\n\t// See ../rfc/5321:2322 ../rfc/6531:414\n\t// First we try as dot-string. If not possible we make a quoted-string.\n\tdotstr := true\n\tt := strings.Split(string(lp), \".\")\n\tfor _, e := range t {\n\t\tfor _, c := range e {\n\t\t\tif c >= '0' && c <= '9' || c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z' || c > 0x7f {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tswitch c {\n\t\t\tcase '!', '#', '$', '%', '&', '\\'', '*', '+', '-', '/', '=', '?', '^', '_', '`', '{', '|', '}', '~':\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tdotstr = false\n\t\t\tbreak\n\t\t}\n\t\tdotstr = dotstr && len(e) > 0\n\t}\n\tdotstr = dotstr && len(t) > 0\n\tif dotstr {\n\t\treturn string(lp)\n\t}\n\n\t// Make quoted-string.\n\tr := `\"`\n\tfor _, b := range lp {\n\t\tif b == '\"' || b == '\\\\' {\n\t\t\tr += \"\\\\\" + string(b)\n\t\t} else {\n\t\t\tr += string(b)\n\t\t}\n\t}\n\tr += `\"`\n\treturn r\n}\n\n// LogString returns the localpart as string for use in smtp, and an escaped\n// representation if it has non-ascii characters.\nfunc (lp Localpart) LogString() string {\n\ts := lp.String()\n\tqs := strconv.QuoteToASCII(s)\n\tif qs != `\"`+s+`\"` {\n\t\ts = \"/\" + qs\n\t}\n\treturn s\n}\n\n// DSNString returns the localpart as string for use in a DSN.\n// utf8 indicates if the remote MTA supports utf8 messaging. If not, the 7bit DSN\n// encoding for \"utf-8-addr-xtext\" from RFC 6533 is used.\nfunc (lp Localpart) DSNString(utf8 bool) string {\n\tif utf8 {\n\t\treturn lp.String()\n\t}\n\t// ../rfc/6533:259\n\tr := \"\"\n\tfor _, c := range lp {\n\t\tif c > 0x20 && c < 0x7f && c != '\\\\' && c != '+' && c != '=' {\n\t\t\tr += string(c)\n\t\t} else {\n\t\t\tr += fmt.Sprintf(`\\x{%x}`, c)\n\t\t}\n\t}\n\treturn r\n}\n\n// IsInternational returns if this is an internationalized local part, i.e. has\n// non-ASCII characters.\nfunc (lp Localpart) IsInternational() bool {\n\tfor _, c := range lp {\n\t\tif c > 0x7f {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Address is a parsed email address.\ntype Address struct {\n\tLocalpart Localpart\n\tDomain    dns.Domain // todo: shouldn't we accept an ip address here too? and merge this type into smtp.Path.\n}\n\n// NewAddress returns an address.\nfunc NewAddress(localpart Localpart, domain dns.Domain) Address {\n\treturn Address{localpart, domain}\n}\n\nfunc (a Address) Path() Path {\n\treturn Path{Localpart: a.Localpart, IPDomain: dns.IPDomain{Domain: a.Domain}}\n}\n\nfunc (a Address) IsZero() bool {\n\treturn a == Address{}\n}\n\n// Pack returns the address in string form. If smtputf8 is true, the domain is\n// formatted with non-ASCII characters. If localpart has non-ASCII characters,\n// they are returned regardless of smtputf8.\nfunc (a Address) Pack(smtputf8 bool) string {\n\tif a.IsZero() {\n\t\treturn \"\"\n\t}\n\treturn a.Localpart.String() + \"@\" + a.Domain.XName(smtputf8)\n}\n\n// String returns the address in string form with non-ASCII characters.\nfunc (a Address) String() string {\n\tif a.IsZero() {\n\t\treturn \"\"\n\t}\n\treturn a.Localpart.String() + \"@\" + a.Domain.Name()\n}\n\n// LogString returns the address with with utf-8 in localpart and/or domain. In\n// case of an IDNA domain and/or quotable characters in the localpart, an address\n// with quoted/escaped localpart and ASCII domain is also returned.\nfunc (a Address) LogString() string {\n\tif a.IsZero() {\n\t\treturn \"\"\n\t}\n\ts := a.Pack(true)\n\tlp := a.Localpart.String()\n\tqlp := strconv.QuoteToASCII(lp)\n\tescaped := qlp != `\"`+lp+`\"`\n\tif a.Domain.Unicode != \"\" || escaped {\n\t\tif escaped {\n\t\t\tlp = qlp\n\t\t}\n\t\ts += \"/\" + lp + \"@\" + a.Domain.ASCII\n\t}\n\treturn s\n}\n\n// ParseAddress parses an email address. UTF-8 is allowed.\n// Returns ErrBadAddress for invalid addresses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar ErrBadLocalpart = errors.New(\"invalid localpart\")\n\n// ParseLocalpart parses the local part.\n// UTF-8 is allowed.\n// Returns ErrBadAddress for invalid addresses.\n\n\n\n\n\n\n\n\n\n\n\nfunc parseLocalPart(s string) (localpart Localpart, remain string, err error) {\n\tp := &parser{s, 0}\n\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\te, ok := x.(error)\n\t\tif !ok {\n\t\t\tpanic(x)\n\t\t}\n\t\terr = fmt.Errorf(\"%w: %s\", ErrBadLocalpart, e)\n\t}()\n\n\tlp := p.xlocalpart()\n\treturn lp, p.remainder(), nil\n}\n\ntype parser struct {\n\ts string\n\to int\n}\n\nfunc (p *parser) xerrorf(format string, args ...any) {\n\tpanic(fmt.Errorf(format, args...))\n}\n\nfunc (p *parser) hasPrefix(s string) bool {\n\treturn strings.HasPrefix(p.s[p.o:], s)\n}\n\nfunc (p *parser) take(s string) bool {\n\tif p.hasPrefix(s) {\n\t\tp.o += len(s)\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (p *parser) xtake(s string) {\n\tif !p.take(s) {\n\t\tp.xerrorf(\"expected %q\", s)\n\t}\n}\n\nfunc (p *parser) empty() bool {\n\treturn p.o == len(p.s)\n}\n\nfunc (p *parser) xtaken(n int) string {\n\tr := p.s[p.o : p.o+n]\n\tp.o += n\n\treturn r\n}\n\nfunc (p *parser) remainder() string {\n\tr := p.s[p.o:]\n\tp.o = len(p.s)\n\treturn r\n}\n\n// todo: reduce duplication between implementations: ../smtp/address.go:/xlocalpart ../dkim/parser.go:/xlocalpart ../smtpserver/parse.go:/xlocalpart\nfunc (p *parser) xlocalpart() Localpart {\n\t// ../rfc/5321:2316\n\tvar s string\n\tif p.hasPrefix(`\"`) {\n\t\ts = p.xquotedString()\n\t} else {\n\t\ts = p.xatom()\n\t\tfor p.take(\".\") {\n\t\t\ts += \".\" + p.xatom()\n\t\t}\n\t}\n\t// In the wild, some services use large localparts for generated (bounce) addresses.\n\tif Pedantic && len(s) > 64 || len(s) > 128 {\n\t\t// ../rfc/5321:3486\n\t\tp.xerrorf(\"localpart longer than 64 octets\")\n\t}\n\treturn Localpart(norm.NFC.String(s))\n}\n\nfunc (p *parser) xquotedString() string {\n\tp.xtake(`\"`)\n\tvar s string\n\tvar esc bool\n\tfor {\n\t\tc := p.xchar()\n\t\tif esc {\n\t\t\tif c >= ' ' && c < 0x7f {\n\t\t\t\ts += string(c)\n\t\t\t\tesc = false\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tp.xerrorf(\"invalid localpart, bad escaped char %c\", c)\n\t\t}\n\t\tif c == '\\\\' {\n\t\t\tesc = true\n\t\t\tcontinue\n\t\t}\n\t\tif c == '\"' {\n\t\t\treturn s\n\t\t}\n\t\t// todo: should we be accepting utf8 for quoted strings?\n\t\tif c >= ' ' && c < 0x7f && c != '\\\\' && c != '\"' || c > 0x7f {\n\t\t\ts += string(c)\n\t\t\tcontinue\n\t\t}\n\t\tp.xerrorf(\"invalid localpart, invalid character %c\", c)\n\t}\n}\n\nfunc (p *parser) xchar() rune {\n\t// We are careful to track invalid utf-8 properly.\n\tif p.empty() {\n\t\tp.xerrorf(\"need another character\")\n\t}\n\tvar r rune\n\tvar o int\n\tfor i, c := range p.s[p.o:] {\n\t\tif i > 0 {\n\t\t\to = i\n\t\t\tbreak\n\t\t}\n\t\tr = c\n\t}\n\tif o == 0 {\n\t\tp.o = len(p.s)\n\t} else {\n\t\tp.o += o\n\t}\n\treturn r\n}\n\nfunc (p *parser) takefn1(what string, fn func(c rune, i int) bool) string {\n\tif p.empty() {\n\t\tp.xerrorf(\"need at least one char for %s\", what)\n\t}\n\tfor i, c := range p.s[p.o:] {\n\t\tif !fn(c, i) {\n\t\t\tif i == 0 {\n\t\t\t\tp.xerrorf(\"expected at least one char for %s, got char %c\", what, c)\n\t\t\t}\n\t\t\treturn p.xtaken(i)\n\t\t}\n\t}\n\treturn p.remainder()\n}\n\nfunc (p *parser) xatom() string {\n\treturn p.takefn1(\"atom\", func(c rune, i int) bool {\n\t\tswitch c {\n\t\tcase '!', '#', '$', '%', '&', '\\'', '*', '+', '-', '/', '=', '?', '^', '_', '`', '{', '|', '}', '~':\n\t\t\treturn true\n\t\t}\n\t\treturn isalphadigit(c) || c > 0x7f\n\t})\n}\n\nfunc isalpha(c rune) bool {\n\treturn c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z'\n}\n\nfunc isdigit(c rune) bool {\n\treturn c >= '0' && c <= '9'\n}\n\nfunc isalphadigit(c rune) bool {\n\treturn isalpha(c) || isdigit(c)\n}\n\n```\n# Function Signature: func ParseAddress(s string) (address Address, err error) {\n# Natural Language Annotations: // ParseAddress parses an email address. UTF-8 is allowed.\n// Returns ErrBadAddress for invalid addresses.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ParseAddress(s string) (address Address, err error) {\n\tp := &parser{s, 0}\n\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\te, ok := x.(error)\n\t\tif !ok {\n\t\t\tpanic(x)\n\t\t}\n\t\terr = fmt.Errorf(\"%w: %s\", ErrBadAddress, e)\n\t}()\n\n\tif p.empty() {\n\t\treturn Address{}, nil\n\t}\n\n\tlocalpart, remain := parseLocalPart(p.remainder())\n\n\tif remain == \"\" || remain[0] != '@' {\n\t\treturn Address{}, fmt.Errorf(\"%w: missing '@'\", ErrBadAddress)\n\t}\n\tremain = remain[1:]\n\n\tdomain, err := dns.ParseDomain(remain)\n\tif err != nil {\n\t\treturn Address{}, fmt.Errorf(\"%w: %s\", ErrBadAddress, err)\n\t}\n\n\treturn Address{localpart, domain}, nil\n}\n```", "```go\nfunc ParseAddress(s string) (address Address, err error) {\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\te, ok := x.(error)\n\t\tif !ok {\n\t\t\tpanic(x)\n\t\t}\n\t\terr = e\n\t}()\n\n\tdomain, remain, err := dns.ParseDomain(s)\n\tif err != nil {\n\t\treturn Address{}, ErrBadAddress\n\t}\n\tif len(remain) == 0 || remain[0] != '@' {\n\t\treturn Address{}, ErrBadAddress\n\t}\n\tlocalpart, remain, err := parseLocalPart(remain[1:])\n\tif err != nil {\n\t\treturn Address{}, ErrBadAddress\n\t}\n\tif len(remain) > 0 {\n\t\treturn Address{}, ErrBadAddress\n\t}\n\treturn NewAddress(localpart, domain), nil\n}\n```", "```go\nfunc ParseAddress(s string) (address Address, err error) {\n\tdefer func() {\n\t\tif x := recover(); x != nil {\n\t\t\te, ok := x.(error)\n\t\t\tif !ok {\n\t\t\t\tpanic(x)\n\t\t\t}\n\t\t\terr = e\n\t\t}\n\t}()\n\n\tlp, remain, err := parseLocalPart(s)\n\tif err != nil {\n\t\treturn Address{}, ErrBadAddress\n\t}\n\n\tdomain, err := dns.ParseDomain(remain)\n\tif err != nil {\n\t\treturn Address{}, fmt.Errorf(\"%w: %s\", ErrBadAddress, err)\n\t}\n\n\treturn Address{Localpart: lp, Domain: domain}, nil\n}\n```"]}, "mox-smtp/address.go-ParseLocalpart": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/smtp/address.go:\n```\npackage smtp\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"golang.org/x/text/unicode/norm\"\n\n\t\"github.com/mjl-/mox/dns\"\n)\n\n// Pedantic enables stricter parsing.\nvar Pedantic bool\n\nvar ErrBadAddress = errors.New(\"invalid email address\")\n\n// Localpart is a decoded local part of an email address, before the \"@\".\n// For quoted strings, values do not hold the double quote or escaping backslashes.\n// An empty string can be a valid localpart.\n// Localparts are in Unicode NFC.\ntype Localpart string\n\n// String returns a packed representation of an address, with proper escaping/quoting, for use in SMTP.\nfunc (lp Localpart) String() string {\n\t// See ../rfc/5321:2322 ../rfc/6531:414\n\t// First we try as dot-string. If not possible we make a quoted-string.\n\tdotstr := true\n\tt := strings.Split(string(lp), \".\")\n\tfor _, e := range t {\n\t\tfor _, c := range e {\n\t\t\tif c >= '0' && c <= '9' || c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z' || c > 0x7f {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tswitch c {\n\t\t\tcase '!', '#', '$', '%', '&', '\\'', '*', '+', '-', '/', '=', '?', '^', '_', '`', '{', '|', '}', '~':\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tdotstr = false\n\t\t\tbreak\n\t\t}\n\t\tdotstr = dotstr && len(e) > 0\n\t}\n\tdotstr = dotstr && len(t) > 0\n\tif dotstr {\n\t\treturn string(lp)\n\t}\n\n\t// Make quoted-string.\n\tr := `\"`\n\tfor _, b := range lp {\n\t\tif b == '\"' || b == '\\\\' {\n\t\t\tr += \"\\\\\" + string(b)\n\t\t} else {\n\t\t\tr += string(b)\n\t\t}\n\t}\n\tr += `\"`\n\treturn r\n}\n\n// LogString returns the localpart as string for use in smtp, and an escaped\n// representation if it has non-ascii characters.\nfunc (lp Localpart) LogString() string {\n\ts := lp.String()\n\tqs := strconv.QuoteToASCII(s)\n\tif qs != `\"`+s+`\"` {\n\t\ts = \"/\" + qs\n\t}\n\treturn s\n}\n\n// DSNString returns the localpart as string for use in a DSN.\n// utf8 indicates if the remote MTA supports utf8 messaging. If not, the 7bit DSN\n// encoding for \"utf-8-addr-xtext\" from RFC 6533 is used.\nfunc (lp Localpart) DSNString(utf8 bool) string {\n\tif utf8 {\n\t\treturn lp.String()\n\t}\n\t// ../rfc/6533:259\n\tr := \"\"\n\tfor _, c := range lp {\n\t\tif c > 0x20 && c < 0x7f && c != '\\\\' && c != '+' && c != '=' {\n\t\t\tr += string(c)\n\t\t} else {\n\t\t\tr += fmt.Sprintf(`\\x{%x}`, c)\n\t\t}\n\t}\n\treturn r\n}\n\n// IsInternational returns if this is an internationalized local part, i.e. has\n// non-ASCII characters.\nfunc (lp Localpart) IsInternational() bool {\n\tfor _, c := range lp {\n\t\tif c > 0x7f {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Address is a parsed email address.\ntype Address struct {\n\tLocalpart Localpart\n\tDomain    dns.Domain // todo: shouldn't we accept an ip address here too? and merge this type into smtp.Path.\n}\n\n// NewAddress returns an address.\nfunc NewAddress(localpart Localpart, domain dns.Domain) Address {\n\treturn Address{localpart, domain}\n}\n\nfunc (a Address) Path() Path {\n\treturn Path{Localpart: a.Localpart, IPDomain: dns.IPDomain{Domain: a.Domain}}\n}\n\nfunc (a Address) IsZero() bool {\n\treturn a == Address{}\n}\n\n// Pack returns the address in string form. If smtputf8 is true, the domain is\n// formatted with non-ASCII characters. If localpart has non-ASCII characters,\n// they are returned regardless of smtputf8.\nfunc (a Address) Pack(smtputf8 bool) string {\n\tif a.IsZero() {\n\t\treturn \"\"\n\t}\n\treturn a.Localpart.String() + \"@\" + a.Domain.XName(smtputf8)\n}\n\n// String returns the address in string form with non-ASCII characters.\nfunc (a Address) String() string {\n\tif a.IsZero() {\n\t\treturn \"\"\n\t}\n\treturn a.Localpart.String() + \"@\" + a.Domain.Name()\n}\n\n// LogString returns the address with with utf-8 in localpart and/or domain. In\n// case of an IDNA domain and/or quotable characters in the localpart, an address\n// with quoted/escaped localpart and ASCII domain is also returned.\nfunc (a Address) LogString() string {\n\tif a.IsZero() {\n\t\treturn \"\"\n\t}\n\ts := a.Pack(true)\n\tlp := a.Localpart.String()\n\tqlp := strconv.QuoteToASCII(lp)\n\tescaped := qlp != `\"`+lp+`\"`\n\tif a.Domain.Unicode != \"\" || escaped {\n\t\tif escaped {\n\t\t\tlp = qlp\n\t\t}\n\t\ts += \"/\" + lp + \"@\" + a.Domain.ASCII\n\t}\n\treturn s\n}\n\n// ParseAddress parses an email address. UTF-8 is allowed.\n// Returns ErrBadAddress for invalid addresses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar ErrBadLocalpart = errors.New(\"invalid localpart\")\n\n// ParseLocalpart parses the local part.\n// UTF-8 is allowed.\n// Returns ErrBadAddress for invalid addresses.\n\n\n\n\n\n\n\n\n\n\n\nfunc parseLocalPart(s string) (localpart Localpart, remain string, err error) {\n\tp := &parser{s, 0}\n\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\te, ok := x.(error)\n\t\tif !ok {\n\t\t\tpanic(x)\n\t\t}\n\t\terr = fmt.Errorf(\"%w: %s\", ErrBadLocalpart, e)\n\t}()\n\n\tlp := p.xlocalpart()\n\treturn lp, p.remainder(), nil\n}\n\ntype parser struct {\n\ts string\n\to int\n}\n\nfunc (p *parser) xerrorf(format string, args ...any) {\n\tpanic(fmt.Errorf(format, args...))\n}\n\nfunc (p *parser) hasPrefix(s string) bool {\n\treturn strings.HasPrefix(p.s[p.o:], s)\n}\n\nfunc (p *parser) take(s string) bool {\n\tif p.hasPrefix(s) {\n\t\tp.o += len(s)\n\t\treturn true\n\t}\n\treturn false\n}\n\nfunc (p *parser) xtake(s string) {\n\tif !p.take(s) {\n\t\tp.xerrorf(\"expected %q\", s)\n\t}\n}\n\nfunc (p *parser) empty() bool {\n\treturn p.o == len(p.s)\n}\n\nfunc (p *parser) xtaken(n int) string {\n\tr := p.s[p.o : p.o+n]\n\tp.o += n\n\treturn r\n}\n\nfunc (p *parser) remainder() string {\n\tr := p.s[p.o:]\n\tp.o = len(p.s)\n\treturn r\n}\n\n// todo: reduce duplication between implementations: ../smtp/address.go:/xlocalpart ../dkim/parser.go:/xlocalpart ../smtpserver/parse.go:/xlocalpart\nfunc (p *parser) xlocalpart() Localpart {\n\t// ../rfc/5321:2316\n\tvar s string\n\tif p.hasPrefix(`\"`) {\n\t\ts = p.xquotedString()\n\t} else {\n\t\ts = p.xatom()\n\t\tfor p.take(\".\") {\n\t\t\ts += \".\" + p.xatom()\n\t\t}\n\t}\n\t// In the wild, some services use large localparts for generated (bounce) addresses.\n\tif Pedantic && len(s) > 64 || len(s) > 128 {\n\t\t// ../rfc/5321:3486\n\t\tp.xerrorf(\"localpart longer than 64 octets\")\n\t}\n\treturn Localpart(norm.NFC.String(s))\n}\n\nfunc (p *parser) xquotedString() string {\n\tp.xtake(`\"`)\n\tvar s string\n\tvar esc bool\n\tfor {\n\t\tc := p.xchar()\n\t\tif esc {\n\t\t\tif c >= ' ' && c < 0x7f {\n\t\t\t\ts += string(c)\n\t\t\t\tesc = false\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tp.xerrorf(\"invalid localpart, bad escaped char %c\", c)\n\t\t}\n\t\tif c == '\\\\' {\n\t\t\tesc = true\n\t\t\tcontinue\n\t\t}\n\t\tif c == '\"' {\n\t\t\treturn s\n\t\t}\n\t\t// todo: should we be accepting utf8 for quoted strings?\n\t\tif c >= ' ' && c < 0x7f && c != '\\\\' && c != '\"' || c > 0x7f {\n\t\t\ts += string(c)\n\t\t\tcontinue\n\t\t}\n\t\tp.xerrorf(\"invalid localpart, invalid character %c\", c)\n\t}\n}\n\nfunc (p *parser) xchar() rune {\n\t// We are careful to track invalid utf-8 properly.\n\tif p.empty() {\n\t\tp.xerrorf(\"need another character\")\n\t}\n\tvar r rune\n\tvar o int\n\tfor i, c := range p.s[p.o:] {\n\t\tif i > 0 {\n\t\t\to = i\n\t\t\tbreak\n\t\t}\n\t\tr = c\n\t}\n\tif o == 0 {\n\t\tp.o = len(p.s)\n\t} else {\n\t\tp.o += o\n\t}\n\treturn r\n}\n\nfunc (p *parser) takefn1(what string, fn func(c rune, i int) bool) string {\n\tif p.empty() {\n\t\tp.xerrorf(\"need at least one char for %s\", what)\n\t}\n\tfor i, c := range p.s[p.o:] {\n\t\tif !fn(c, i) {\n\t\t\tif i == 0 {\n\t\t\t\tp.xerrorf(\"expected at least one char for %s, got char %c\", what, c)\n\t\t\t}\n\t\t\treturn p.xtaken(i)\n\t\t}\n\t}\n\treturn p.remainder()\n}\n\nfunc (p *parser) xatom() string {\n\treturn p.takefn1(\"atom\", func(c rune, i int) bool {\n\t\tswitch c {\n\t\tcase '!', '#', '$', '%', '&', '\\'', '*', '+', '-', '/', '=', '?', '^', '_', '`', '{', '|', '}', '~':\n\t\t\treturn true\n\t\t}\n\t\treturn isalphadigit(c) || c > 0x7f\n\t})\n}\n\nfunc isalpha(c rune) bool {\n\treturn c >= 'a' && c <= 'z' || c >= 'A' && c <= 'Z'\n}\n\nfunc isdigit(c rune) bool {\n\treturn c >= '0' && c <= '9'\n}\n\nfunc isalphadigit(c rune) bool {\n\treturn isalpha(c) || isdigit(c)\n}\n\n```\n# Function Signature: func ParseLocalpart(s string) (localpart Localpart, err error) {\n# Natural Language Annotations: // ParseLocalpart parses the local part.\n// UTF-8 is allowed.\n// Returns ErrBadAddress for invalid addresses.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc ParseLocalpart(s string) (localpart Localpart, err error) {\n\tp := &parser{s, 0}\n\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\te, ok := x.(error)\n\t\tif !ok {\n\t\t\tpanic(x)\n\t\t}\n\t\terr = fmt.Errorf(\"%w: %s\", ErrBadLocalpart, e)\n\t}()\n\n\tlp := p.xlocalpart()\n\treturn lp, nil\n}\n```", "```go\nfunc ParseLocalpart(s string) (localpart Localpart, err error) {\n\tp := &parser{s, 0}\n\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\te, ok := x.(error)\n\t\tif !ok {\n\t\t\tpanic(x)\n\t\t}\n\t\terr = fmt.Errorf(\"%w: %s\", ErrBadLocalpart, e)\n\t}()\n\n\tlp := p.xlocalpart()\n\treturn lp, nil\n}\n```", "```go\nfunc ParseLocalpart(s string) (localpart Localpart, err error) {\n\tp := &parser{s, 0}\n\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\te, ok := x.(error)\n\t\tif !ok {\n\t\t\tpanic(x)\n\t\t}\n\t\terr = fmt.Errorf(\"%w: %s\", ErrBadLocalpart, e)\n\t}()\n\n\tlp := p.xlocalpart()\n\treturn lp, nil\n}\n```"]}, "mox-smtp/data.go-DataWrite": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/smtp/data.go:\n```\npackage smtp\n\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"errors\"\n\t\"io\"\n)\n\nvar ErrCRLF = errors.New(\"invalid bare carriage return or newline\")\n\nvar errMissingCRLF = errors.New(\"missing crlf at end of message\")\n\n// DataWrite reads data (a mail message) from r, and writes it to smtp\n// connection w with dot stuffing, as required by the SMTP data command.\n//\n// Messages with bare carriage returns or bare newlines result in an error.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvar dotcrlf = []byte(\".\\r\\n\")\n\n// DataReader is an io.Reader that reads data from an SMTP DATA command, doing dot\n// unstuffing and returning io.EOF when a bare dot is received. Use NewDataReader.\n//\n// Bare carriage returns, and the sequences \"[^\\r]\\n.\" and \"\\n.\\n\" result in an\n// error.\ntype DataReader struct {\n\t// ../rfc/5321:2003\n\tr           *bufio.Reader\n\tplast, last byte\n\tbuf         []byte // From previous read.\n\terr         error  // Read error, for after r.buf is exhausted.\n\n\t// When we see invalid combinations of CR and LF, we keep reading, and report an\n\t// error at the final \"\\r\\n.\\r\\n\". We cannot just stop reading and return an error,\n\t// the SMTP protocol would become out of sync.\n\tbadcrlf bool\n}\n\n// NewDataReader returns an initialized DataReader.\nfunc NewDataReader(r *bufio.Reader) *DataReader {\n\treturn &DataReader{\n\t\tr: r,\n\t\t// Set up initial state to accept a message that is only \".\" and CRLF.\n\t\tplast: '\\r',\n\t\tlast:  '\\n',\n\t}\n}\n\n// Read implements io.Reader.\nfunc (r *DataReader) Read(p []byte) (int, error) {\n\twrote := 0\n\tfor len(p) > 0 {\n\t\t// Read until newline as long as it fits in the buffer.\n\t\tif len(r.buf) == 0 {\n\t\t\tif r.err != nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\t// todo: set a max length, eg 1000 octets including crlf excluding potential leading dot. ../rfc/5321:3512\n\t\t\tr.buf, r.err = r.r.ReadSlice('\\n')\n\t\t\tif r.err == bufio.ErrBufferFull {\n\t\t\t\tr.err = nil\n\t\t\t} else if r.err == io.EOF {\n\t\t\t\t// Mark EOF as bad for now. If we see the ending dotcrlf below, err becomes regular\n\t\t\t\t// io.EOF again.\n\t\t\t\tr.err = io.ErrUnexpectedEOF\n\t\t\t}\n\t\t}\n\t\tif len(r.buf) > 0 {\n\t\t\t// Reject bare \\r.\n\t\t\tfor i, c := range r.buf {\n\t\t\t\tif c == '\\r' && (i == len(r.buf) || r.buf[i+1] != '\\n') {\n\t\t\t\t\tr.badcrlf = true\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// We require crlf. A bare LF is not a line ending for the end of the SMTP\n\t\t\t// transaction. ../rfc/5321:2032\n\t\t\t// Bare newlines are accepted as message data, unless around a bare dot. The SMTP\n\t\t\t// server adds missing carriage returns. We don't reject bare newlines outright,\n\t\t\t// real-world messages like that occur.\n\t\t\tif r.plast == '\\r' && r.last == '\\n' {\n\t\t\t\tif bytes.Equal(r.buf, dotcrlf) {\n\t\t\t\t\tr.buf = nil\n\t\t\t\t\tr.err = io.EOF\n\t\t\t\t\tif r.badcrlf {\n\t\t\t\t\t\tr.err = ErrCRLF\n\t\t\t\t\t}\n\t\t\t\t\tbreak\n\t\t\t\t} else if r.buf[0] == '.' {\n\t\t\t\t\t// Reject \"\\r\\n.\\n\".\n\t\t\t\t\tif len(r.buf) >= 2 && r.buf[1] == '\\n' {\n\t\t\t\t\t\tr.badcrlf = true\n\t\t\t\t\t}\n\t\t\t\t\tr.buf = r.buf[1:]\n\t\t\t\t}\n\t\t\t} else if r.last == '\\n' && (bytes.HasPrefix(r.buf, []byte(\".\\n\")) || bytes.HasPrefix(r.buf, []byte(\".\\r\\n\"))) {\n\t\t\t\t// Reject \"[^\\r]\\n.\\n\" and \"[^\\r]\\n.\\r\\n\"\n\t\t\t\tr.badcrlf = true\n\t\t\t}\n\t\t\tn := len(r.buf)\n\t\t\tif n > len(p) {\n\t\t\t\tn = len(p)\n\t\t\t}\n\t\t\tcopy(p, r.buf[:n])\n\t\t\tif n == 1 {\n\t\t\t\tr.plast, r.last = r.last, r.buf[0]\n\t\t\t} else if n > 1 {\n\t\t\t\tr.plast, r.last = r.buf[n-2], r.buf[n-1]\n\t\t\t}\n\t\t\tp = p[n:]\n\t\t\tr.buf = r.buf[n:]\n\t\t\twrote += n\n\t\t}\n\t}\n\treturn wrote, r.err\n}\n\n```\n# Function Signature: func DataWrite(w io.Writer, r io.Reader) error {\n# Natural Language Annotations: // DataWrite reads data (a mail message) from r, and writes it to smtp\n// connection w with dot stuffing, as required by the SMTP data command.\n//\n// Messages with bare carriage returns or bare newlines result in an error.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nimport (\n\t\"bufio\"\n\t\"bytes\"\n\t\"errors\"\n\t\"io\"\n)\n\n// DataWrite reads data (a mail message) from r, and writes it to smtp\n// connection w with dot stuffing, as required by the SMTP data command.\n//\n// Messages with bare carriage returns or bare newlines result in an error.\nfunc DataWrite(w io.Writer, r io.Reader) error {\n\tbr := bufio.NewReader(r)\n\tbw := bufio.NewWriter(w)\n\n\tvar last byte\n\tbuf := make([]byte, 1)\n\tfor {\n\t\t_, err := br.Read(buf)\n\t\tif err != nil {\n\t\t\tif err == io.EOF {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\treturn err\n\t\t}\n\n\t\t// Bare carriage return or newline is an error.\n\t\tif (last == '\\r' && buf[0] != '\\n') || (last == '\\n' && buf[0] != '\\r') {\n\t\t\treturn errors.New(\"bare carriage return or newline detected\")\n\t\t}\n\n\t\t// Write to the writer.\n\t\tif _, err := bw.Write(buf); err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tlast = buf[0]\n\t}\n\n\t// Flush the buffer to ensure everything is written.\n\tif err := bw.Flush(); err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n```", "```go\nfunc DataWrite(w io.Writer, r io.Reader) error {\n\treader := bufio.NewReader(r)\n\tfor {\n\t\tline, err := reader.ReadBytes('\\n')\n\t\tif err != nil && err != io.EOF {\n\t\t\treturn err\n\t\t}\n\t\tif bytes.Contains(line, []byte{'\\r', '\\n'}) {\n\t\t\treturn ErrCRLF\n\t\t}\n\t\tif bytes.Contains(line, []byte{'\\r'}) {\n\t\t\treturn ErrCRLF\n\t\t}\n\t\tif bytes.Contains(line, []byte{'\\n'}) {\n\t\t\t_, err = w.Write([]byte{'.'})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\t_, err = w.Write(line)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif err == io.EOF {\n\t\t\tbreak\n\t\t}\n\t}\n\t_, err := w.Write(dotcrlf)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n```", "```go\nfunc DataWrite(w io.Writer, r io.Reader) error {\n\treader := bufio.NewReader(r)\n\tvar buf [512]byte\n\tvar err error\n\n\tfor {\n\t\tnr, er := reader.Read(buf[:])\n\t\tif nr > 0 {\n\t\t\tif buf[0] == '\\r' || buf[nr-1] == '\\n' {\n\t\t\t\treturn ErrCRLF\n\t\t\t}\n\n\t\t\t_, ew := w.Write(buf[0:nr])\n\t\t\tif ew != nil {\n\t\t\t\terr = ew\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif er != nil {\n\t\t\tif er != io.EOF {\n\t\t\t\terr = er\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t_, ew := w.Write([]byte(\"\\r\\n.\\r\\n\"))\n\tif ew != nil {\n\t\treturn ew\n\t}\n\n\treturn nil\n}\n```"]}, "mox-scram/scram.go-NewServer": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/scram/scram.go:\n```\n// Package scram implements the SCRAM-SHA-* SASL authentication mechanism, RFC 7677 and RFC 5802.\n//\n// SCRAM-SHA-256 and SCRAM-SHA-1 allow a client to authenticate to a server using a\n// password without handing plaintext password over to the server. The client also\n// verifies the server knows (a derivative of) the password. Both the client and\n// server side are implemented.\npackage scram\n\n// todo: test with messages that contains extensions\n// todo: some tests for the parser\n// todo: figure out how invalid parameters etc should be handled. just abort? perhaps mostly a problem for imap.\n\nimport (\n\t\"bytes\"\n\t\"crypto/hmac\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/tls\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"strings\"\n\n\t\"golang.org/x/crypto/pbkdf2\"\n\t\"golang.org/x/text/secure/precis\"\n\t\"golang.org/x/text/unicode/norm\"\n)\n\n// Errors at scram protocol level. Can be exchanged between client and server.\nvar (\n\tErrInvalidEncoding                 Error = \"invalid-encoding\"\n\tErrExtensionsNotSupported          Error = \"extensions-not-supported\"\n\tErrInvalidProof                    Error = \"invalid-proof\"\n\tErrChannelBindingsDontMatch        Error = \"channel-bindings-dont-match\"\n\tErrServerDoesSupportChannelBinding Error = \"server-does-support-channel-binding\"\n\tErrChannelBindingNotSupported      Error = \"channel-binding-not-supported\"\n\tErrUnsupportedChannelBindingType   Error = \"unsupported-channel-binding-type\"\n\tErrUnknownUser                     Error = \"unknown-user\"\n\tErrNoResources                     Error = \"no-resources\"\n\tErrOtherError                      Error = \"other-error\"\n)\n\nvar scramErrors = makeErrors()\n\nfunc makeErrors() map[string]Error {\n\tl := []Error{\n\t\tErrInvalidEncoding,\n\t\tErrExtensionsNotSupported,\n\t\tErrInvalidProof,\n\t\tErrChannelBindingsDontMatch,\n\t\tErrServerDoesSupportChannelBinding,\n\t\tErrChannelBindingNotSupported,\n\t\tErrUnsupportedChannelBindingType,\n\t\tErrUnknownUser,\n\t\tErrNoResources,\n\t\tErrOtherError,\n\t}\n\tm := map[string]Error{}\n\tfor _, e := range l {\n\t\tm[string(e)] = e\n\t}\n\treturn m\n}\n\nvar (\n\tErrNorm     = errors.New(\"parameter not unicode normalized\") // E.g. if client sends non-normalized username or authzid.\n\tErrUnsafe   = errors.New(\"unsafe parameter\")                 // E.g. salt, nonce too short, or too few iterations.\n\tErrProtocol = errors.New(\"protocol error\")                   // E.g. server responded with a nonce not prefixed by the client nonce.\n)\n\ntype Error string\n\nfunc (e Error) Error() string {\n\treturn string(e)\n}\n\n// MakeRandom returns a cryptographically random buffer for use as salt or as\n// nonce.\nfunc MakeRandom() []byte {\n\tbuf := make([]byte, 12)\n\t_, err := cryptorand.Read(buf)\n\tif err != nil {\n\t\tpanic(\"generate random\")\n\t}\n\treturn buf\n}\n\n// Cleanup password with precis, like remote should have done. If the password\n// appears invalid, we'll return the original, there is a chance the server also\n// doesn't enforce requirements and accepts it. ../rfc/8265:679\nfunc precisPassword(password string) string {\n\tpw, err := precis.OpaqueString.String(password)\n\tif err != nil {\n\t\treturn password\n\t}\n\treturn pw\n}\n\n// SaltPassword returns a salted password.\nfunc SaltPassword(h func() hash.Hash, password string, salt []byte, iterations int) []byte {\n\tpassword = precisPassword(password)\n\treturn pbkdf2.Key([]byte(password), salt, iterations, h().Size(), h)\n}\n\n// hmac0 returns the hmac with key over msg.\nfunc hmac0(h func() hash.Hash, key []byte, msg string) []byte {\n\tmac := hmac.New(h, key)\n\tmac.Write([]byte(msg))\n\treturn mac.Sum(nil)\n}\n\nfunc xor(a, b []byte) {\n\tfor i := range a {\n\t\ta[i] ^= b[i]\n\t}\n}\n\nfunc channelBindData(cs *tls.ConnectionState) ([]byte, error) {\n\tif cs.Version <= tls.VersionTLS12 {\n\t\tif cs.TLSUnique == nil {\n\t\t\treturn nil, fmt.Errorf(\"no channel binding data available\")\n\t\t}\n\t\treturn cs.TLSUnique, nil\n\t}\n\n\t// \"tls-exporter\", ../rfc/9266:95\n\t// Since TLS 1.3, a zero-length and absent context have the same behaviour. ../rfc/8446:5385 ../rfc/8446:5405\n\t// This is different from TLS 1.2 and earlier. ../rfc/5705:206 ../rfc/5705:245\n\treturn cs.ExportKeyingMaterial(\"EXPORTER-Channel-Binding\", []byte{}, 32)\n}\n\n// Server represents the server-side of a SCRAM-SHA-* authentication.\ntype Server struct {\n\tAuthentication string // Username for authentication, \"authc\". Always set and non-empty.\n\tAuthorization  string // If set, role of user to assume after authentication, \"authz\".\n\n\th func() hash.Hash // sha1.New or sha256.New\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\n\tgs2header           string\n\tclientNonce         string // Client-part of the nonce.\n\tserverNonceOverride string // If set, server does not generate random nonce, but uses this. For tests with the test vector.\n\tnonce               string // Full client + server nonce.\n\tchannelBinding      []byte\n}\n\n// NewServer returns a server given the first SCRAM message from a client.\n//\n// If cs is set, the PLUS variant can be negotiated, binding the authentication\n// exchange to the TLS channel (preventing MitM attempts). If a client\n// indicates it supports the PLUS variant, but thinks the server does not, the\n// authentication attempt will fail.\n//\n// If channelBindingRequired is set, the client has indicated it will do channel\n// binding and not doing so will cause the authentication to fail.\n//\n// The sequence for data and calls on a server:\n//\n//   - Read initial data from client, call NewServer (this call), then ServerFirst and write to the client.\n//   - Read response from client, call Finish or FinishFinal and write the resulting string.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst returns the string to send back to the client. To be called after NewServer.\n\n\n\n\n\n\n\n\n\n\n\n// Finish takes the final client message, and the salted password (probably\n// from server storage), verifies the client, and returns a message to return\n// to the client. If err is nil, authentication was successful. If the\n// authorization requested is not acceptable, the server should call\n// FinishError instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FinishError returns an error message to write to the client for the final\n// server message.\nfunc (s *Server) FinishError(err Error) string {\n\treturn \"e=\" + string(err)\n}\n\n// Client represents the client-side of a SCRAM-SHA-* authentication.\ntype Client struct {\n\tauthc string\n\tauthz string\n\n\th            func() hash.Hash     // sha1.New or sha256.New\n\tnoServerPlus bool                 // Server did not announce support for PLUS-variant.\n\tcs           *tls.ConnectionState // If set, use PLUS-variant.\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\tauthMessage             string\n\n\tgs2header       string\n\tclientNonce     string\n\tnonce           string // Full client + server nonce.\n\tsaltedPassword  []byte\n\tchannelBindData []byte // For PLUS-variant.\n}\n\n// NewClient returns a client for authentication authc, optionally for\n// authorization with role authz, for the hash (sha1.New or sha256.New).\n//\n// If noServerPlus is true, the client would like to have used the PLUS-variant,\n// that binds the authentication attempt to the TLS connection, but the client did\n// not see support for the PLUS variant announced by the server. Used during\n// negotiation to detect possible MitM attempt.\n//\n// If cs is not nil, the SCRAM PLUS-variant is negotiated, with channel binding to\n// the unique TLS connection, either using \"tls-exporter\" for TLS 1.3 and later, or\n// \"tls-unique\" otherwise.\n//\n// If cs is nil, no channel binding is done. If noServerPlus is also false, the\n// client is configured to not attempt/\"support\" the PLUS-variant, ensuring servers\n// that do support the PLUS-variant do not abort the connection.\n//\n// The sequence for data and calls on a client:\n//\n//   - ClientFirst, write result to server.\n//   - Read response from server, feed to ServerFirst, write response to server.\n//   - Read response from server, feed to ServerFinal.\nfunc NewClient(h func() hash.Hash, authc, authz string, noServerPlus bool, cs *tls.ConnectionState) *Client {\n\tauthc = norm.NFC.String(authc)\n\tauthz = norm.NFC.String(authz)\n\treturn &Client{authc: authc, authz: authz, h: h, noServerPlus: noServerPlus, cs: cs}\n}\n\n// ClientFirst returns the first client message to write to the server.\n// No channel binding is done/supported.\n// A random nonce is generated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst processes the first response message from the server. The\n// provided nonce, salt and iterations are checked. If valid, a final client\n// message is calculated and returned. This message must be written to the\n// server. It includes proof that the client knows the password.\nfunc (c *Client) ServerFirst(serverFirst []byte, password string) (clientFinal string, rerr error) {\n\tc.serverFirst = string(serverFirst)\n\tp := newParser(serverFirst)\n\tdefer p.recover(&rerr)\n\n\t// ../rfc/5802:632\n\t// ../rfc/5802:959\n\tif p.take(\"m=\") {\n\t\tp.xerrorf(\"unsupported mandatory extension: %w\", ErrExtensionsNotSupported) // ../rfc/5802:973\n\t}\n\n\tc.nonce = p.xnonce()\n\tp.xtake(\",\")\n\tsalt := p.xsalt()\n\tp.xtake(\",\")\n\titerations := p.xiterations()\n\t// We ignore extensions that we don't know about.\n\tfor p.take(\",\") {\n\t\tp.xattrval()\n\t}\n\tp.xempty()\n\n\tif !strings.HasPrefix(c.nonce, c.clientNonce) {\n\t\treturn \"\", fmt.Errorf(\"%w: server dropped our nonce\", ErrProtocol)\n\t}\n\tif len(c.nonce)-len(c.clientNonce) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: server nonce too short\", ErrUnsafe)\n\t}\n\tif len(salt) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: salt too short\", ErrUnsafe)\n\t}\n\tif iterations < 2048 {\n\t\treturn \"\", fmt.Errorf(\"%w: too few iterations\", ErrUnsafe)\n\t}\n\n\t// We send our channel binding data if present. If the server has different values,\n\t// we'll get an error. If any MitM would try to modify the channel binding data,\n\t// the server cannot verify our signature and will fail the attempt.\n\t// ../rfc/5802:925 ../rfc/5802:1015\n\tcbindInput := append([]byte(c.gs2header), c.channelBindData...)\n\tc.clientFinalWithoutProof = fmt.Sprintf(\"c=%s,r=%s\", base64.StdEncoding.EncodeToString(cbindInput), c.nonce)\n\n\tc.authMessage = c.clientFirstBare + \",\" + c.serverFirst + \",\" + c.clientFinalWithoutProof\n\n\tc.saltedPassword = SaltPassword(c.h, password, salt, iterations)\n\tclientKey := hmac0(c.h, c.saltedPassword, \"Client Key\")\n\th := c.h()\n\th.Write(clientKey)\n\tstoredKey := h.Sum(nil)\n\tclientSig := hmac0(c.h, storedKey, c.authMessage)\n\txor(clientSig, clientKey) // Now clientProof.\n\tclientProof := clientSig\n\n\tr := c.clientFinalWithoutProof + \",p=\" + base64.StdEncoding.EncodeToString(clientProof)\n\treturn r, nil\n}\n\n// ServerFinal processes the final message from the server, verifying that the\n// server knows the password.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Convert \",\" to =2C and \"=\" to =3D.\nfunc saslname(s string) string {\n\tvar r string\n\tfor _, c := range s {\n\t\tif c == ',' {\n\t\t\tr += \"=2C\"\n\t\t} else if c == '=' {\n\t\t\tr += \"=3D\"\n\t\t} else {\n\t\t\tr += string(c)\n\t\t}\n\t}\n\treturn r\n}\n\n```\n# Function Signature: func NewServer(h func() hash.Hash, clientFirst []byte, cs *tls.ConnectionState, channelBindingRequired bool) (server *Server, rerr error) {\n# Natural Language Annotations: // NewServer returns a server given the first SCRAM message from a client.\n//\n// If cs is set, the PLUS variant can be negotiated, binding the authentication\n// exchange to the TLS channel (preventing MitM attempts). If a client\n// indicates it supports the PLUS variant, but thinks the server does not, the\n// authentication attempt will fail.\n//\n// If channelBindingRequired is set, the client has indicated it will do channel\n// binding and not doing so will cause the authentication to fail.\n//\n// The sequence for data and calls on a server:\n//\n//   - Read initial data from client, call NewServer (this call), then ServerFirst and write to the client.\n//   - Read response from client, call Finish or FinishFinal and write the resulting string.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc NewServer(h func() hash.Hash, clientFirst []byte, cs *tls.ConnectionState, channelBindingRequired bool) (server *Server, rerr error) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\trerr = r.(error)\n\t\t}\n\t}()\n\n\tserver = &Server{h: h, cs: cs}\n\n\tp := newParser(clientFirst)\n\tserver.clientFirstBare = string(clientFirst)\n\tserver.clientNonce = p.xnonce()\n\tp.xtake(\",\")\n\tserver.gs2header = p.xgs2()\n\tp.xtake(\",\")\n\tserver.authentication = p.xattrval()\n\tp.xtake(\",\")\n\tserver.authorization = p.xattrval()\n\t// We ignore extensions that we don't know about.\n\tfor p.take(\",\") {\n\t\tp.xattrval()\n\t}\n\tp.xempty()\n\n\tserver.clientFirstBare = \"n=\" + saslname(p.username) + \",r=\" + p.clientNonce\n\n\tif channelBindingRequired {\n\t\tchannelBindData, err := channelBindData(cs)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"channel binding required but not supported by client: %w\", err)\n\t\t}\n\t\tserver.channelBinding = channelBindData\n\t}\n\n\tserverFirst := server.clientFirstBare + \",r=\" + server.nonce\n\tif cs != nil {\n\t\tserverFirst += \",c=biws\"\n\t}\n\tif channelBindingRequired {\n\t\tserverFirst += \",c=y\"\n\t}\n\n\treturn server, nil\n}\n```", "```go\nfunc NewServer(h func() hash.Hash, clientFirst []byte, cs *tls.ConnectionState, channelBindingRequired bool) (server *Server, rerr error) {\n\tserver = &Server{\n\t\th: h,\n\t\tclientFirstBare: string(clientFirst),\n\t\tcs: cs,\n\t}\n\n\tp := newParser(clientFirst)\n\tdefer p.recover(&rerr)\n\n\t// ../rfc/5802:644\n\t// ../rfc/5802:974\n\tif p.take(\"n=\") {\n\t\tp.xerrorf(\"unsupported mandatory extension: %w\", ErrExtensionsNotSupported)\n\t}\n\n\tserver.clientNonce = p.xnonce()\n\tp.xtake(\",\")\n\tserver.gs2header = p.xgs2header()\n\tp.xtake(\",\")\n\tserver.Authentication = p.xauthc()\n\tp.xtake(\",\")\n\tif p.take(\"a=\") {\n\t\tserver.Authorization = p.xa()\n\t\tp.xtake(\",\")\n\t}\n\n\tif channelBindingRequired {\n\t\tif cs == nil {\n\t\t\treturn nil, fmt.Errorf(\"%w: channel binding required\", ErrChannelBindingNotSupported)\n\t\t}\n\t\tchannelBindData, err := channelBindData(cs)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"%w: channel binding required\", ErrChannelBindingNotSupported)\n\t\t}\n\t\tserver.channelBinding = channelBindData\n\t}\n\n\tif p.take(\"c=\") {\n\t\treturn nil, fmt.Errorf(\"%w: channel binding not supported\", ErrServerDoesSupportChannelBinding)\n\t}\n\n\t// We ignore extensions that we don't know about.\n\tfor p.take(\",\") {\n\t\tp.xattrval()\n\t}\n\tp.xempty()\n\n\tserver.nonce = server.clientNonce + MakeRandom()\n\tserver.clientFirstBare = server.clientFirstBare + \",c=\" + base64.StdEncoding.EncodeToString(server.channelBinding) + \",r=\" + server.nonce\n\n\tserver.serverFirst = \"r=\" + server.nonce + \",s=\" + base64.StdEncoding.EncodeToString(MakeRandom())\n\n\tif channelBindingRequired && server.channelBinding == nil {\n\t\treturn nil, fmt.Errorf(\"%w: channel binding required\", ErrChannelBindingNotSupported)\n\t}\n\n\treturn server, nil\n}\n```", "```go\nfunc NewServer(h func() hash.Hash, clientFirst []byte, cs *tls.ConnectionState, channelBindingRequired bool) (server *Server, rerr error) {\n\tserver = &Server{h: h, clientFirstBare: string(clientFirst)}\n\tp := newParser(clientFirst)\n\tdefer p.recover(&rerr)\n\n\tif p.take(\"m=\") {\n\t\tp.xerrorf(\"unsupported mandatory extension: %w\", ErrExtensionsNotSupported)\n\t}\n\n\tserver.Authentication = p.xusername()\n\tp.xtake(\",\")\n\tserver.clientNonce = p.xnonce()\n\t// We ignore extensions that we don't know about.\n\tfor p.take(\",\") {\n\t\tp.xattrval()\n\t}\n\tp.xempty()\n\n\tif cs != nil {\n\t\tserver.channelBinding, rerr = channelBindData(cs)\n\t\tif rerr != nil {\n\t\t\treturn\n\t\t}\n\t\tif channelBindingRequired && len(server.channelBinding) == 0 {\n\t\t\trerr = fmt.Errorf(\"%w: client does not support channel binding\", ErrChannelBindingNotSupported)\n\t\t\treturn\n\t\t}\n\t}\n\n\tif strings.HasPrefix(server.clientNonce, \"n=\") {\n\t\tserver.clientNonce = server.clientNonce[2:]\n\t}\n\tserver.gs2header = \"n=\" + server.clientNonce + \",r=\" + base64.StdEncoding.EncodeToString(server.channelBinding)\n\n\tserver.serverNonceOverride = \"\"\n\n\tserver.serverFirst = \"r=\" + base64.StdEncoding.EncodeToString(MakeRandom()) + \",s=\" + base64.StdEncoding.EncodeToString(MakeRandom()) + \",i=4096\"\n\n\tserver.nonce = server.clientNonce + server.serverFirst[2:9]\n\tserver.clientFinalWithoutProof = \"c=\" + base64.StdEncoding.EncodeToString([]byte(server.gs2header)) + \",r=\" + server.nonce\n\n\treturn\n}\n```"]}, "mox-scram/scram.go-ServerFirst": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/scram/scram.go:\n```\n// Package scram implements the SCRAM-SHA-* SASL authentication mechanism, RFC 7677 and RFC 5802.\n//\n// SCRAM-SHA-256 and SCRAM-SHA-1 allow a client to authenticate to a server using a\n// password without handing plaintext password over to the server. The client also\n// verifies the server knows (a derivative of) the password. Both the client and\n// server side are implemented.\npackage scram\n\n// todo: test with messages that contains extensions\n// todo: some tests for the parser\n// todo: figure out how invalid parameters etc should be handled. just abort? perhaps mostly a problem for imap.\n\nimport (\n\t\"bytes\"\n\t\"crypto/hmac\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/tls\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"strings\"\n\n\t\"golang.org/x/crypto/pbkdf2\"\n\t\"golang.org/x/text/secure/precis\"\n\t\"golang.org/x/text/unicode/norm\"\n)\n\n// Errors at scram protocol level. Can be exchanged between client and server.\nvar (\n\tErrInvalidEncoding                 Error = \"invalid-encoding\"\n\tErrExtensionsNotSupported          Error = \"extensions-not-supported\"\n\tErrInvalidProof                    Error = \"invalid-proof\"\n\tErrChannelBindingsDontMatch        Error = \"channel-bindings-dont-match\"\n\tErrServerDoesSupportChannelBinding Error = \"server-does-support-channel-binding\"\n\tErrChannelBindingNotSupported      Error = \"channel-binding-not-supported\"\n\tErrUnsupportedChannelBindingType   Error = \"unsupported-channel-binding-type\"\n\tErrUnknownUser                     Error = \"unknown-user\"\n\tErrNoResources                     Error = \"no-resources\"\n\tErrOtherError                      Error = \"other-error\"\n)\n\nvar scramErrors = makeErrors()\n\nfunc makeErrors() map[string]Error {\n\tl := []Error{\n\t\tErrInvalidEncoding,\n\t\tErrExtensionsNotSupported,\n\t\tErrInvalidProof,\n\t\tErrChannelBindingsDontMatch,\n\t\tErrServerDoesSupportChannelBinding,\n\t\tErrChannelBindingNotSupported,\n\t\tErrUnsupportedChannelBindingType,\n\t\tErrUnknownUser,\n\t\tErrNoResources,\n\t\tErrOtherError,\n\t}\n\tm := map[string]Error{}\n\tfor _, e := range l {\n\t\tm[string(e)] = e\n\t}\n\treturn m\n}\n\nvar (\n\tErrNorm     = errors.New(\"parameter not unicode normalized\") // E.g. if client sends non-normalized username or authzid.\n\tErrUnsafe   = errors.New(\"unsafe parameter\")                 // E.g. salt, nonce too short, or too few iterations.\n\tErrProtocol = errors.New(\"protocol error\")                   // E.g. server responded with a nonce not prefixed by the client nonce.\n)\n\ntype Error string\n\nfunc (e Error) Error() string {\n\treturn string(e)\n}\n\n// MakeRandom returns a cryptographically random buffer for use as salt or as\n// nonce.\nfunc MakeRandom() []byte {\n\tbuf := make([]byte, 12)\n\t_, err := cryptorand.Read(buf)\n\tif err != nil {\n\t\tpanic(\"generate random\")\n\t}\n\treturn buf\n}\n\n// Cleanup password with precis, like remote should have done. If the password\n// appears invalid, we'll return the original, there is a chance the server also\n// doesn't enforce requirements and accepts it. ../rfc/8265:679\nfunc precisPassword(password string) string {\n\tpw, err := precis.OpaqueString.String(password)\n\tif err != nil {\n\t\treturn password\n\t}\n\treturn pw\n}\n\n// SaltPassword returns a salted password.\nfunc SaltPassword(h func() hash.Hash, password string, salt []byte, iterations int) []byte {\n\tpassword = precisPassword(password)\n\treturn pbkdf2.Key([]byte(password), salt, iterations, h().Size(), h)\n}\n\n// hmac0 returns the hmac with key over msg.\nfunc hmac0(h func() hash.Hash, key []byte, msg string) []byte {\n\tmac := hmac.New(h, key)\n\tmac.Write([]byte(msg))\n\treturn mac.Sum(nil)\n}\n\nfunc xor(a, b []byte) {\n\tfor i := range a {\n\t\ta[i] ^= b[i]\n\t}\n}\n\nfunc channelBindData(cs *tls.ConnectionState) ([]byte, error) {\n\tif cs.Version <= tls.VersionTLS12 {\n\t\tif cs.TLSUnique == nil {\n\t\t\treturn nil, fmt.Errorf(\"no channel binding data available\")\n\t\t}\n\t\treturn cs.TLSUnique, nil\n\t}\n\n\t// \"tls-exporter\", ../rfc/9266:95\n\t// Since TLS 1.3, a zero-length and absent context have the same behaviour. ../rfc/8446:5385 ../rfc/8446:5405\n\t// This is different from TLS 1.2 and earlier. ../rfc/5705:206 ../rfc/5705:245\n\treturn cs.ExportKeyingMaterial(\"EXPORTER-Channel-Binding\", []byte{}, 32)\n}\n\n// Server represents the server-side of a SCRAM-SHA-* authentication.\ntype Server struct {\n\tAuthentication string // Username for authentication, \"authc\". Always set and non-empty.\n\tAuthorization  string // If set, role of user to assume after authentication, \"authz\".\n\n\th func() hash.Hash // sha1.New or sha256.New\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\n\tgs2header           string\n\tclientNonce         string // Client-part of the nonce.\n\tserverNonceOverride string // If set, server does not generate random nonce, but uses this. For tests with the test vector.\n\tnonce               string // Full client + server nonce.\n\tchannelBinding      []byte\n}\n\n// NewServer returns a server given the first SCRAM message from a client.\n//\n// If cs is set, the PLUS variant can be negotiated, binding the authentication\n// exchange to the TLS channel (preventing MitM attempts). If a client\n// indicates it supports the PLUS variant, but thinks the server does not, the\n// authentication attempt will fail.\n//\n// If channelBindingRequired is set, the client has indicated it will do channel\n// binding and not doing so will cause the authentication to fail.\n//\n// The sequence for data and calls on a server:\n//\n//   - Read initial data from client, call NewServer (this call), then ServerFirst and write to the client.\n//   - Read response from client, call Finish or FinishFinal and write the resulting string.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst returns the string to send back to the client. To be called after NewServer.\n\n\n\n\n\n\n\n\n\n\n\n// Finish takes the final client message, and the salted password (probably\n// from server storage), verifies the client, and returns a message to return\n// to the client. If err is nil, authentication was successful. If the\n// authorization requested is not acceptable, the server should call\n// FinishError instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FinishError returns an error message to write to the client for the final\n// server message.\nfunc (s *Server) FinishError(err Error) string {\n\treturn \"e=\" + string(err)\n}\n\n// Client represents the client-side of a SCRAM-SHA-* authentication.\ntype Client struct {\n\tauthc string\n\tauthz string\n\n\th            func() hash.Hash     // sha1.New or sha256.New\n\tnoServerPlus bool                 // Server did not announce support for PLUS-variant.\n\tcs           *tls.ConnectionState // If set, use PLUS-variant.\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\tauthMessage             string\n\n\tgs2header       string\n\tclientNonce     string\n\tnonce           string // Full client + server nonce.\n\tsaltedPassword  []byte\n\tchannelBindData []byte // For PLUS-variant.\n}\n\n// NewClient returns a client for authentication authc, optionally for\n// authorization with role authz, for the hash (sha1.New or sha256.New).\n//\n// If noServerPlus is true, the client would like to have used the PLUS-variant,\n// that binds the authentication attempt to the TLS connection, but the client did\n// not see support for the PLUS variant announced by the server. Used during\n// negotiation to detect possible MitM attempt.\n//\n// If cs is not nil, the SCRAM PLUS-variant is negotiated, with channel binding to\n// the unique TLS connection, either using \"tls-exporter\" for TLS 1.3 and later, or\n// \"tls-unique\" otherwise.\n//\n// If cs is nil, no channel binding is done. If noServerPlus is also false, the\n// client is configured to not attempt/\"support\" the PLUS-variant, ensuring servers\n// that do support the PLUS-variant do not abort the connection.\n//\n// The sequence for data and calls on a client:\n//\n//   - ClientFirst, write result to server.\n//   - Read response from server, feed to ServerFirst, write response to server.\n//   - Read response from server, feed to ServerFinal.\nfunc NewClient(h func() hash.Hash, authc, authz string, noServerPlus bool, cs *tls.ConnectionState) *Client {\n\tauthc = norm.NFC.String(authc)\n\tauthz = norm.NFC.String(authz)\n\treturn &Client{authc: authc, authz: authz, h: h, noServerPlus: noServerPlus, cs: cs}\n}\n\n// ClientFirst returns the first client message to write to the server.\n// No channel binding is done/supported.\n// A random nonce is generated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst processes the first response message from the server. The\n// provided nonce, salt and iterations are checked. If valid, a final client\n// message is calculated and returned. This message must be written to the\n// server. It includes proof that the client knows the password.\nfunc (c *Client) ServerFirst(serverFirst []byte, password string) (clientFinal string, rerr error) {\n\tc.serverFirst = string(serverFirst)\n\tp := newParser(serverFirst)\n\tdefer p.recover(&rerr)\n\n\t// ../rfc/5802:632\n\t// ../rfc/5802:959\n\tif p.take(\"m=\") {\n\t\tp.xerrorf(\"unsupported mandatory extension: %w\", ErrExtensionsNotSupported) // ../rfc/5802:973\n\t}\n\n\tc.nonce = p.xnonce()\n\tp.xtake(\",\")\n\tsalt := p.xsalt()\n\tp.xtake(\",\")\n\titerations := p.xiterations()\n\t// We ignore extensions that we don't know about.\n\tfor p.take(\",\") {\n\t\tp.xattrval()\n\t}\n\tp.xempty()\n\n\tif !strings.HasPrefix(c.nonce, c.clientNonce) {\n\t\treturn \"\", fmt.Errorf(\"%w: server dropped our nonce\", ErrProtocol)\n\t}\n\tif len(c.nonce)-len(c.clientNonce) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: server nonce too short\", ErrUnsafe)\n\t}\n\tif len(salt) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: salt too short\", ErrUnsafe)\n\t}\n\tif iterations < 2048 {\n\t\treturn \"\", fmt.Errorf(\"%w: too few iterations\", ErrUnsafe)\n\t}\n\n\t// We send our channel binding data if present. If the server has different values,\n\t// we'll get an error. If any MitM would try to modify the channel binding data,\n\t// the server cannot verify our signature and will fail the attempt.\n\t// ../rfc/5802:925 ../rfc/5802:1015\n\tcbindInput := append([]byte(c.gs2header), c.channelBindData...)\n\tc.clientFinalWithoutProof = fmt.Sprintf(\"c=%s,r=%s\", base64.StdEncoding.EncodeToString(cbindInput), c.nonce)\n\n\tc.authMessage = c.clientFirstBare + \",\" + c.serverFirst + \",\" + c.clientFinalWithoutProof\n\n\tc.saltedPassword = SaltPassword(c.h, password, salt, iterations)\n\tclientKey := hmac0(c.h, c.saltedPassword, \"Client Key\")\n\th := c.h()\n\th.Write(clientKey)\n\tstoredKey := h.Sum(nil)\n\tclientSig := hmac0(c.h, storedKey, c.authMessage)\n\txor(clientSig, clientKey) // Now clientProof.\n\tclientProof := clientSig\n\n\tr := c.clientFinalWithoutProof + \",p=\" + base64.StdEncoding.EncodeToString(clientProof)\n\treturn r, nil\n}\n\n// ServerFinal processes the final message from the server, verifying that the\n// server knows the password.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Convert \",\" to =2C and \"=\" to =3D.\nfunc saslname(s string) string {\n\tvar r string\n\tfor _, c := range s {\n\t\tif c == ',' {\n\t\t\tr += \"=2C\"\n\t\t} else if c == '=' {\n\t\t\tr += \"=3D\"\n\t\t} else {\n\t\t\tr += string(c)\n\t\t}\n\t}\n\treturn r\n}\n\n```\n# Function Signature: func (s *Server) ServerFirst(iterations int, salt []byte) (string, error) {\n# Natural Language Annotations: // ServerFirst returns the string to send back to the client. To be called after NewServer.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (s *Server) ServerFirst(iterations int, salt []byte) (string, error) {\n\ts.h = sha1.New\n\ts.clientNonce = base64.StdEncoding.EncodeToString(MakeRandom())\n\ts.serverNonceOverride = \"\"\n\ts.nonce = s.clientNonce\n\ts.gs2header = \"n,,\"\n\ts.channelBinding = []byte{}\n\n\tserverNonce := s.clientNonce\n\tif s.serverNonceOverride != \"\" {\n\t\tserverNonce = s.serverNonceOverride\n\t}\n\n\tserverFirst := fmt.Sprintf(\"r=%s%s,s=%s,i=%d\", serverNonce, saslname(string(s.channelBinding)), base64.StdEncoding.EncodeToString(salt), iterations)\n\ts.serverFirst = serverFirst\n\n\tclientFirstBare := fmt.Sprintf(\"n,,n=%s,r=%s\", saslname(s.Authentication), s.clientNonce)\n\ts.clientFirstBare = clientFirstBare\n\n\tclientFinalWithoutProof := fmt.Sprintf(\"c=%s,r=%s\", base64.StdEncoding.EncodeToString(s.channelBinding), s.nonce)\n\ts.clientFinalWithoutProof = clientFinalWithoutProof\n\n\tr := clientFirstBare + \",\" + serverFirst + \",\" + clientFinalWithoutProof\n\treturn r, nil\n}\n```", "```go\nfunc (s *Server) ServerFirst(iterations int, salt []byte) (string, error) {\n\ts.clientNonce = base64.StdEncoding.EncodeToString(MakeRandom())\n\ts.nonce = s.clientNonce\n\tif s.serverNonceOverride != \"\" {\n\t\ts.nonce += s.serverNonceOverride\n\t}\n\t\n\ts.clientFirstBare = \"n=\" + saslname(s.Authentication) + \",r=\" + s.nonce\n\t\n\ts.gs2header = \"n,,\" + s.nonce\n\t\n\tvar err error\n\ts.channelBinding, err = channelBindData(nil)\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\t\n\tcbindInput := append([]byte(s.gs2header), s.channelBinding...)\n\ts.serverFirst = fmt.Sprintf(\"r=%s,s=%s,i=%d\", s.nonce, base64.StdEncoding.EncodeToString(salt), iterations)\n\t\n\tcbindInput = append([]byte(s.gs2header), cbindInput...)\n\ts.clientFinalWithoutProof = fmt.Sprintf(\"c=%s,r=%s\", base64.StdEncoding.EncodeToString(cbindInput), s.nonce)\n\t\n\ts.authMessage = s.clientFirstBare + \",\" + s.serverFirst + \",\" + s.clientFinalWithoutProof\n\t\n\treturn s.serverFirst, nil\n}\n```", "```go\nfunc (s *Server) ServerFirst(iterations int, salt []byte) (string, error) {\n\ts.clientNonce = base64.StdEncoding.EncodeToString(MakeRandom())\n\ts.serverNonce = base64.StdEncoding.EncodeToString(MakeRandom())\n\tnonce := s.clientNonce + \",\" + s.serverNonce\n\tif s.serverNonceOverride != \"\" {\n\t\tnonce = s.clientNonce + \",\" + s.serverNonceOverride\n\t}\n\ts.nonce = nonce\n\n\tauthz := s.Authorization\n\tif authz == \"\" {\n\t\tauthz = \"=\" // Means no authz.\n\t}\n\n\ts.clientFirstBare = \"n,,\" + authz\n\tserverFirst := \"r=\" + nonce + \",s=\" + base64.StdEncoding.EncodeToString(salt) + \",i=\" + fmt.Sprint(iterations)\n\n\tgs2header := \"n,,\" + authz + \",\" + serverFirst\n\thmac := hmac0(s.h, s.saltedPassword, gs2header)\n\ts.gs2header = base64.StdEncoding.EncodeToString(hmac)\n\n\tif s.cs != nil {\n\t\tvar err error\n\t\ts.channelBinding, err = channelBindData(s.cs)\n\t\tif err != nil {\n\t\t\treturn \"\", err\n\t\t}\n\t}\n\n\ts.serverFirst = serverFirst\n\treturn s.gs2header + \",\" + s.serverFirst, nil\n}\n```"]}, "mox-scram/scram.go-Finish": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/scram/scram.go:\n```\n// Package scram implements the SCRAM-SHA-* SASL authentication mechanism, RFC 7677 and RFC 5802.\n//\n// SCRAM-SHA-256 and SCRAM-SHA-1 allow a client to authenticate to a server using a\n// password without handing plaintext password over to the server. The client also\n// verifies the server knows (a derivative of) the password. Both the client and\n// server side are implemented.\npackage scram\n\n// todo: test with messages that contains extensions\n// todo: some tests for the parser\n// todo: figure out how invalid parameters etc should be handled. just abort? perhaps mostly a problem for imap.\n\nimport (\n\t\"bytes\"\n\t\"crypto/hmac\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/tls\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"strings\"\n\n\t\"golang.org/x/crypto/pbkdf2\"\n\t\"golang.org/x/text/secure/precis\"\n\t\"golang.org/x/text/unicode/norm\"\n)\n\n// Errors at scram protocol level. Can be exchanged between client and server.\nvar (\n\tErrInvalidEncoding                 Error = \"invalid-encoding\"\n\tErrExtensionsNotSupported          Error = \"extensions-not-supported\"\n\tErrInvalidProof                    Error = \"invalid-proof\"\n\tErrChannelBindingsDontMatch        Error = \"channel-bindings-dont-match\"\n\tErrServerDoesSupportChannelBinding Error = \"server-does-support-channel-binding\"\n\tErrChannelBindingNotSupported      Error = \"channel-binding-not-supported\"\n\tErrUnsupportedChannelBindingType   Error = \"unsupported-channel-binding-type\"\n\tErrUnknownUser                     Error = \"unknown-user\"\n\tErrNoResources                     Error = \"no-resources\"\n\tErrOtherError                      Error = \"other-error\"\n)\n\nvar scramErrors = makeErrors()\n\nfunc makeErrors() map[string]Error {\n\tl := []Error{\n\t\tErrInvalidEncoding,\n\t\tErrExtensionsNotSupported,\n\t\tErrInvalidProof,\n\t\tErrChannelBindingsDontMatch,\n\t\tErrServerDoesSupportChannelBinding,\n\t\tErrChannelBindingNotSupported,\n\t\tErrUnsupportedChannelBindingType,\n\t\tErrUnknownUser,\n\t\tErrNoResources,\n\t\tErrOtherError,\n\t}\n\tm := map[string]Error{}\n\tfor _, e := range l {\n\t\tm[string(e)] = e\n\t}\n\treturn m\n}\n\nvar (\n\tErrNorm     = errors.New(\"parameter not unicode normalized\") // E.g. if client sends non-normalized username or authzid.\n\tErrUnsafe   = errors.New(\"unsafe parameter\")                 // E.g. salt, nonce too short, or too few iterations.\n\tErrProtocol = errors.New(\"protocol error\")                   // E.g. server responded with a nonce not prefixed by the client nonce.\n)\n\ntype Error string\n\nfunc (e Error) Error() string {\n\treturn string(e)\n}\n\n// MakeRandom returns a cryptographically random buffer for use as salt or as\n// nonce.\nfunc MakeRandom() []byte {\n\tbuf := make([]byte, 12)\n\t_, err := cryptorand.Read(buf)\n\tif err != nil {\n\t\tpanic(\"generate random\")\n\t}\n\treturn buf\n}\n\n// Cleanup password with precis, like remote should have done. If the password\n// appears invalid, we'll return the original, there is a chance the server also\n// doesn't enforce requirements and accepts it. ../rfc/8265:679\nfunc precisPassword(password string) string {\n\tpw, err := precis.OpaqueString.String(password)\n\tif err != nil {\n\t\treturn password\n\t}\n\treturn pw\n}\n\n// SaltPassword returns a salted password.\nfunc SaltPassword(h func() hash.Hash, password string, salt []byte, iterations int) []byte {\n\tpassword = precisPassword(password)\n\treturn pbkdf2.Key([]byte(password), salt, iterations, h().Size(), h)\n}\n\n// hmac0 returns the hmac with key over msg.\nfunc hmac0(h func() hash.Hash, key []byte, msg string) []byte {\n\tmac := hmac.New(h, key)\n\tmac.Write([]byte(msg))\n\treturn mac.Sum(nil)\n}\n\nfunc xor(a, b []byte) {\n\tfor i := range a {\n\t\ta[i] ^= b[i]\n\t}\n}\n\nfunc channelBindData(cs *tls.ConnectionState) ([]byte, error) {\n\tif cs.Version <= tls.VersionTLS12 {\n\t\tif cs.TLSUnique == nil {\n\t\t\treturn nil, fmt.Errorf(\"no channel binding data available\")\n\t\t}\n\t\treturn cs.TLSUnique, nil\n\t}\n\n\t// \"tls-exporter\", ../rfc/9266:95\n\t// Since TLS 1.3, a zero-length and absent context have the same behaviour. ../rfc/8446:5385 ../rfc/8446:5405\n\t// This is different from TLS 1.2 and earlier. ../rfc/5705:206 ../rfc/5705:245\n\treturn cs.ExportKeyingMaterial(\"EXPORTER-Channel-Binding\", []byte{}, 32)\n}\n\n// Server represents the server-side of a SCRAM-SHA-* authentication.\ntype Server struct {\n\tAuthentication string // Username for authentication, \"authc\". Always set and non-empty.\n\tAuthorization  string // If set, role of user to assume after authentication, \"authz\".\n\n\th func() hash.Hash // sha1.New or sha256.New\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\n\tgs2header           string\n\tclientNonce         string // Client-part of the nonce.\n\tserverNonceOverride string // If set, server does not generate random nonce, but uses this. For tests with the test vector.\n\tnonce               string // Full client + server nonce.\n\tchannelBinding      []byte\n}\n\n// NewServer returns a server given the first SCRAM message from a client.\n//\n// If cs is set, the PLUS variant can be negotiated, binding the authentication\n// exchange to the TLS channel (preventing MitM attempts). If a client\n// indicates it supports the PLUS variant, but thinks the server does not, the\n// authentication attempt will fail.\n//\n// If channelBindingRequired is set, the client has indicated it will do channel\n// binding and not doing so will cause the authentication to fail.\n//\n// The sequence for data and calls on a server:\n//\n//   - Read initial data from client, call NewServer (this call), then ServerFirst and write to the client.\n//   - Read response from client, call Finish or FinishFinal and write the resulting string.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst returns the string to send back to the client. To be called after NewServer.\n\n\n\n\n\n\n\n\n\n\n\n// Finish takes the final client message, and the salted password (probably\n// from server storage), verifies the client, and returns a message to return\n// to the client. If err is nil, authentication was successful. If the\n// authorization requested is not acceptable, the server should call\n// FinishError instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FinishError returns an error message to write to the client for the final\n// server message.\nfunc (s *Server) FinishError(err Error) string {\n\treturn \"e=\" + string(err)\n}\n\n// Client represents the client-side of a SCRAM-SHA-* authentication.\ntype Client struct {\n\tauthc string\n\tauthz string\n\n\th            func() hash.Hash     // sha1.New or sha256.New\n\tnoServerPlus bool                 // Server did not announce support for PLUS-variant.\n\tcs           *tls.ConnectionState // If set, use PLUS-variant.\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\tauthMessage             string\n\n\tgs2header       string\n\tclientNonce     string\n\tnonce           string // Full client + server nonce.\n\tsaltedPassword  []byte\n\tchannelBindData []byte // For PLUS-variant.\n}\n\n// NewClient returns a client for authentication authc, optionally for\n// authorization with role authz, for the hash (sha1.New or sha256.New).\n//\n// If noServerPlus is true, the client would like to have used the PLUS-variant,\n// that binds the authentication attempt to the TLS connection, but the client did\n// not see support for the PLUS variant announced by the server. Used during\n// negotiation to detect possible MitM attempt.\n//\n// If cs is not nil, the SCRAM PLUS-variant is negotiated, with channel binding to\n// the unique TLS connection, either using \"tls-exporter\" for TLS 1.3 and later, or\n// \"tls-unique\" otherwise.\n//\n// If cs is nil, no channel binding is done. If noServerPlus is also false, the\n// client is configured to not attempt/\"support\" the PLUS-variant, ensuring servers\n// that do support the PLUS-variant do not abort the connection.\n//\n// The sequence for data and calls on a client:\n//\n//   - ClientFirst, write result to server.\n//   - Read response from server, feed to ServerFirst, write response to server.\n//   - Read response from server, feed to ServerFinal.\nfunc NewClient(h func() hash.Hash, authc, authz string, noServerPlus bool, cs *tls.ConnectionState) *Client {\n\tauthc = norm.NFC.String(authc)\n\tauthz = norm.NFC.String(authz)\n\treturn &Client{authc: authc, authz: authz, h: h, noServerPlus: noServerPlus, cs: cs}\n}\n\n// ClientFirst returns the first client message to write to the server.\n// No channel binding is done/supported.\n// A random nonce is generated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst processes the first response message from the server. The\n// provided nonce, salt and iterations are checked. If valid, a final client\n// message is calculated and returned. This message must be written to the\n// server. It includes proof that the client knows the password.\nfunc (c *Client) ServerFirst(serverFirst []byte, password string) (clientFinal string, rerr error) {\n\tc.serverFirst = string(serverFirst)\n\tp := newParser(serverFirst)\n\tdefer p.recover(&rerr)\n\n\t// ../rfc/5802:632\n\t// ../rfc/5802:959\n\tif p.take(\"m=\") {\n\t\tp.xerrorf(\"unsupported mandatory extension: %w\", ErrExtensionsNotSupported) // ../rfc/5802:973\n\t}\n\n\tc.nonce = p.xnonce()\n\tp.xtake(\",\")\n\tsalt := p.xsalt()\n\tp.xtake(\",\")\n\titerations := p.xiterations()\n\t// We ignore extensions that we don't know about.\n\tfor p.take(\",\") {\n\t\tp.xattrval()\n\t}\n\tp.xempty()\n\n\tif !strings.HasPrefix(c.nonce, c.clientNonce) {\n\t\treturn \"\", fmt.Errorf(\"%w: server dropped our nonce\", ErrProtocol)\n\t}\n\tif len(c.nonce)-len(c.clientNonce) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: server nonce too short\", ErrUnsafe)\n\t}\n\tif len(salt) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: salt too short\", ErrUnsafe)\n\t}\n\tif iterations < 2048 {\n\t\treturn \"\", fmt.Errorf(\"%w: too few iterations\", ErrUnsafe)\n\t}\n\n\t// We send our channel binding data if present. If the server has different values,\n\t// we'll get an error. If any MitM would try to modify the channel binding data,\n\t// the server cannot verify our signature and will fail the attempt.\n\t// ../rfc/5802:925 ../rfc/5802:1015\n\tcbindInput := append([]byte(c.gs2header), c.channelBindData...)\n\tc.clientFinalWithoutProof = fmt.Sprintf(\"c=%s,r=%s\", base64.StdEncoding.EncodeToString(cbindInput), c.nonce)\n\n\tc.authMessage = c.clientFirstBare + \",\" + c.serverFirst + \",\" + c.clientFinalWithoutProof\n\n\tc.saltedPassword = SaltPassword(c.h, password, salt, iterations)\n\tclientKey := hmac0(c.h, c.saltedPassword, \"Client Key\")\n\th := c.h()\n\th.Write(clientKey)\n\tstoredKey := h.Sum(nil)\n\tclientSig := hmac0(c.h, storedKey, c.authMessage)\n\txor(clientSig, clientKey) // Now clientProof.\n\tclientProof := clientSig\n\n\tr := c.clientFinalWithoutProof + \",p=\" + base64.StdEncoding.EncodeToString(clientProof)\n\treturn r, nil\n}\n\n// ServerFinal processes the final message from the server, verifying that the\n// server knows the password.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Convert \",\" to =2C and \"=\" to =3D.\nfunc saslname(s string) string {\n\tvar r string\n\tfor _, c := range s {\n\t\tif c == ',' {\n\t\t\tr += \"=2C\"\n\t\t} else if c == '=' {\n\t\t\tr += \"=3D\"\n\t\t} else {\n\t\t\tr += string(c)\n\t\t}\n\t}\n\treturn r\n}\n\n```\n# Function Signature: func (s *Server) Finish(clientFinal []byte, saltedPassword []byte) (serverFinal string, rerr error) {\n# Natural Language Annotations: // Finish takes the final client message, and the salted password (probably\n// from server storage), verifies the client, and returns a message to return\n// to the client. If err is nil, authentication was successful. If the\n// authorization requested is not acceptable, the server should call\n// FinishError instead.\nPlease return the generated code surrounded by ```\n", "response": ["Here is the generated code for the function:\n\n```go\nfunc (s *Server) Finish(clientFinal []byte, saltedPassword []byte) (serverFinal string, rerr error) {\n\ts.clientFinalWithoutProof = string(clientFinal)\n\tp := newParser(clientFinal)\n\tdefer p.recover(&rerr)\n\n\tp.xtake(\"p=\")\n\tproof := p.xbase64()\n\tp.xempty()\n\n\tclientSig := hmac0(s.h, saltedPassword, s.clientFirstBare+\",\"+s.serverFirst+\",\"+s.clientFinalWithoutProof)\n\tclientProof := clientSig\n\tclientKey := hmac0(s.h, saltedPassword, \"Client Key\")\n\txor(clientProof, clientKey)\n\n\tif !hmac.Equal(clientProof, proof) {\n\t\treturn \"\", fmt.Errorf(\"%w: invalid client proof\", ErrInvalidProof)\n\t}\n\n\tserverKey := hmac0(s.h, saltedPassword, \"Server Key\")\n\tserverSig := hmac0(s.h, serverKey, s.authMessage)\n\tserverProof := serverSig\n\n\tauthMessage := \"Server signature received\"\n\tif s.channelBinding != nil {\n\t\tif !bytes.Equal(s.channelBinding, p.channelBindingData) {\n\t\t\treturn \"\", fmt.Errorf(\"%w: channel binding data not equal\", ErrChannelBindingsDontMatch)\n\t\t}\n\t\tauthMessage = \"Server signature and channel binding received\"\n\t}\n\n\tserverFinal = fmt.Sprintf(\"v=%s,r=%s\", base64.StdEncoding.EncodeToString(serverProof), s.nonce)\n\treturn serverFinal, nil\n}\n```\n\nThis function takes the final client message and the salted password as input. It verifies the client proof, calculates the server proof, and returns the server final message if the client proof is valid. If the client proof is invalid, it returns an error. If the server channel binding data matches the client channel binding data, it also includes a message indicating that the server signature and channel binding were received.", "```go\nfunc (s *Server) Finish(clientFinal []byte, saltedPassword []byte) (serverFinal string, rerr error) {\n\ts.clientFinalWithoutProof = string(clientFinal)\n\tp := newParser(clientFinal)\n\tdefer p.recover(&rerr)\n\n\tp.xtake(\"p=\")\n\tclientProof := p.xproof()\n\tp.xempty()\n\n\tif !strings.HasPrefix(s.nonce, s.clientNonce) {\n\t\treturn \"\", fmt.Errorf(\"%w: client dropped our nonce\", ErrProtocol)\n\t}\n\n\tclientKey := hmac0(s.h, saltedPassword, \"Client Key\")\n\tstoredKey := hmac0(s.h, saltedPassword, \"StoredKey\")\n\tauthMessage := s.clientFirstBare + \",\" + s.serverFirst + \",\" + s.clientFinalWithoutProof\n\tclientSig := hmac0(s.h, storedKey, authMessage)\n\tclientProofCheck := clientProof\n\txor(clientProofCheck, clientKey)\n\tif !bytes.Equal(clientProofCheck, clientSig) {\n\t\treturn \"\", fmt.Errorf(\"%w: invalid proof\", ErrInvalidProof)\n\t}\n\n\tif s.channelBinding != nil {\n\t\tif !bytes.Equal(s.channelBinding, p.xchannelBinding) {\n\t\t\treturn \"\", fmt.Errorf(\"%w: channel binding does not match\", ErrChannelBindingsDontMatch)\n\t\t}\n\t}\n\n\tserverKey := hmac0(s.h, saltedPassword, \"Server Key\")\n\tserverSig := hmac0(s.h, serverKey, authMessage)\n\n\treturn \"v=\" + base64.StdEncoding.EncodeToString(serverSig), nil\n}\n```", "```go\nfunc (s *Server) Finish(clientFinal []byte, saltedPassword []byte) (serverFinal string, rerr error) {\n\tclientFinalStr := string(clientFinal)\n\tp := newParser(clientFinal)\n\tdefer p.recover(&rerr)\n\n\tp.take(\"c=\")\n\tchannelBindData := p.xbase64()\n\tp.xtake(\",r=\")\n\tnonce := p.xnonce()\n\tp.xtake(\",p=\")\n\tclientProof := p.xproof()\n\tp.xempty()\n\n\tif !bytes.Equal(channelBindData, s.channelBinding) {\n\t\treturn \"\", fmt.Errorf(\"%w: channel bindings don't match\", ErrChannelBindingsDontMatch)\n\t}\n\n\tif !strings.HasPrefix(nonce, s.nonce) {\n\t\treturn \"\", fmt.Errorf(\"%w: server dropped our nonce\", ErrProtocol)\n\t}\n\tif len(nonce)-len(s.nonce) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: server nonce too short\", ErrUnsafe)\n\t}\n\n\tclientKey := hmac0(s.h, saltedPassword, \"Client Key\")\n\tstoredKey := s.h().Sum(nil)\n\tclientSig := hmac0(s.h, storedKey, s.clientFinalWithoutProof)\n\tclientProofCheck := append([]byte{}, clientSig...)\n\txor(clientProofCheck, clientKey)\n\tif !hmac.Equal(clientProofCheck, clientProof) {\n\t\treturn \"\", fmt.Errorf(\"%w: proof doesn't match\", ErrInvalidProof)\n\t}\n\n\tserverKey := hmac0(s.h, saltedPassword, \"Server Key\")\n\tauthMessage := []byte(s.authMessage)\n\tserverSig := hmac0(s.h, serverKey, authMessage)\n\n\tserverFinal = \"v=\" + base64.StdEncoding.EncodeToString(serverSig)\n\treturn serverFinal, nil\n}\n```"]}, "mox-scram/scram.go-ClientFirst": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/scram/scram.go:\n```\n// Package scram implements the SCRAM-SHA-* SASL authentication mechanism, RFC 7677 and RFC 5802.\n//\n// SCRAM-SHA-256 and SCRAM-SHA-1 allow a client to authenticate to a server using a\n// password without handing plaintext password over to the server. The client also\n// verifies the server knows (a derivative of) the password. Both the client and\n// server side are implemented.\npackage scram\n\n// todo: test with messages that contains extensions\n// todo: some tests for the parser\n// todo: figure out how invalid parameters etc should be handled. just abort? perhaps mostly a problem for imap.\n\nimport (\n\t\"bytes\"\n\t\"crypto/hmac\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/tls\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"strings\"\n\n\t\"golang.org/x/crypto/pbkdf2\"\n\t\"golang.org/x/text/secure/precis\"\n\t\"golang.org/x/text/unicode/norm\"\n)\n\n// Errors at scram protocol level. Can be exchanged between client and server.\nvar (\n\tErrInvalidEncoding                 Error = \"invalid-encoding\"\n\tErrExtensionsNotSupported          Error = \"extensions-not-supported\"\n\tErrInvalidProof                    Error = \"invalid-proof\"\n\tErrChannelBindingsDontMatch        Error = \"channel-bindings-dont-match\"\n\tErrServerDoesSupportChannelBinding Error = \"server-does-support-channel-binding\"\n\tErrChannelBindingNotSupported      Error = \"channel-binding-not-supported\"\n\tErrUnsupportedChannelBindingType   Error = \"unsupported-channel-binding-type\"\n\tErrUnknownUser                     Error = \"unknown-user\"\n\tErrNoResources                     Error = \"no-resources\"\n\tErrOtherError                      Error = \"other-error\"\n)\n\nvar scramErrors = makeErrors()\n\nfunc makeErrors() map[string]Error {\n\tl := []Error{\n\t\tErrInvalidEncoding,\n\t\tErrExtensionsNotSupported,\n\t\tErrInvalidProof,\n\t\tErrChannelBindingsDontMatch,\n\t\tErrServerDoesSupportChannelBinding,\n\t\tErrChannelBindingNotSupported,\n\t\tErrUnsupportedChannelBindingType,\n\t\tErrUnknownUser,\n\t\tErrNoResources,\n\t\tErrOtherError,\n\t}\n\tm := map[string]Error{}\n\tfor _, e := range l {\n\t\tm[string(e)] = e\n\t}\n\treturn m\n}\n\nvar (\n\tErrNorm     = errors.New(\"parameter not unicode normalized\") // E.g. if client sends non-normalized username or authzid.\n\tErrUnsafe   = errors.New(\"unsafe parameter\")                 // E.g. salt, nonce too short, or too few iterations.\n\tErrProtocol = errors.New(\"protocol error\")                   // E.g. server responded with a nonce not prefixed by the client nonce.\n)\n\ntype Error string\n\nfunc (e Error) Error() string {\n\treturn string(e)\n}\n\n// MakeRandom returns a cryptographically random buffer for use as salt or as\n// nonce.\nfunc MakeRandom() []byte {\n\tbuf := make([]byte, 12)\n\t_, err := cryptorand.Read(buf)\n\tif err != nil {\n\t\tpanic(\"generate random\")\n\t}\n\treturn buf\n}\n\n// Cleanup password with precis, like remote should have done. If the password\n// appears invalid, we'll return the original, there is a chance the server also\n// doesn't enforce requirements and accepts it. ../rfc/8265:679\nfunc precisPassword(password string) string {\n\tpw, err := precis.OpaqueString.String(password)\n\tif err != nil {\n\t\treturn password\n\t}\n\treturn pw\n}\n\n// SaltPassword returns a salted password.\nfunc SaltPassword(h func() hash.Hash, password string, salt []byte, iterations int) []byte {\n\tpassword = precisPassword(password)\n\treturn pbkdf2.Key([]byte(password), salt, iterations, h().Size(), h)\n}\n\n// hmac0 returns the hmac with key over msg.\nfunc hmac0(h func() hash.Hash, key []byte, msg string) []byte {\n\tmac := hmac.New(h, key)\n\tmac.Write([]byte(msg))\n\treturn mac.Sum(nil)\n}\n\nfunc xor(a, b []byte) {\n\tfor i := range a {\n\t\ta[i] ^= b[i]\n\t}\n}\n\nfunc channelBindData(cs *tls.ConnectionState) ([]byte, error) {\n\tif cs.Version <= tls.VersionTLS12 {\n\t\tif cs.TLSUnique == nil {\n\t\t\treturn nil, fmt.Errorf(\"no channel binding data available\")\n\t\t}\n\t\treturn cs.TLSUnique, nil\n\t}\n\n\t// \"tls-exporter\", ../rfc/9266:95\n\t// Since TLS 1.3, a zero-length and absent context have the same behaviour. ../rfc/8446:5385 ../rfc/8446:5405\n\t// This is different from TLS 1.2 and earlier. ../rfc/5705:206 ../rfc/5705:245\n\treturn cs.ExportKeyingMaterial(\"EXPORTER-Channel-Binding\", []byte{}, 32)\n}\n\n// Server represents the server-side of a SCRAM-SHA-* authentication.\ntype Server struct {\n\tAuthentication string // Username for authentication, \"authc\". Always set and non-empty.\n\tAuthorization  string // If set, role of user to assume after authentication, \"authz\".\n\n\th func() hash.Hash // sha1.New or sha256.New\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\n\tgs2header           string\n\tclientNonce         string // Client-part of the nonce.\n\tserverNonceOverride string // If set, server does not generate random nonce, but uses this. For tests with the test vector.\n\tnonce               string // Full client + server nonce.\n\tchannelBinding      []byte\n}\n\n// NewServer returns a server given the first SCRAM message from a client.\n//\n// If cs is set, the PLUS variant can be negotiated, binding the authentication\n// exchange to the TLS channel (preventing MitM attempts). If a client\n// indicates it supports the PLUS variant, but thinks the server does not, the\n// authentication attempt will fail.\n//\n// If channelBindingRequired is set, the client has indicated it will do channel\n// binding and not doing so will cause the authentication to fail.\n//\n// The sequence for data and calls on a server:\n//\n//   - Read initial data from client, call NewServer (this call), then ServerFirst and write to the client.\n//   - Read response from client, call Finish or FinishFinal and write the resulting string.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst returns the string to send back to the client. To be called after NewServer.\n\n\n\n\n\n\n\n\n\n\n\n// Finish takes the final client message, and the salted password (probably\n// from server storage), verifies the client, and returns a message to return\n// to the client. If err is nil, authentication was successful. If the\n// authorization requested is not acceptable, the server should call\n// FinishError instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FinishError returns an error message to write to the client for the final\n// server message.\nfunc (s *Server) FinishError(err Error) string {\n\treturn \"e=\" + string(err)\n}\n\n// Client represents the client-side of a SCRAM-SHA-* authentication.\ntype Client struct {\n\tauthc string\n\tauthz string\n\n\th            func() hash.Hash     // sha1.New or sha256.New\n\tnoServerPlus bool                 // Server did not announce support for PLUS-variant.\n\tcs           *tls.ConnectionState // If set, use PLUS-variant.\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\tauthMessage             string\n\n\tgs2header       string\n\tclientNonce     string\n\tnonce           string // Full client + server nonce.\n\tsaltedPassword  []byte\n\tchannelBindData []byte // For PLUS-variant.\n}\n\n// NewClient returns a client for authentication authc, optionally for\n// authorization with role authz, for the hash (sha1.New or sha256.New).\n//\n// If noServerPlus is true, the client would like to have used the PLUS-variant,\n// that binds the authentication attempt to the TLS connection, but the client did\n// not see support for the PLUS variant announced by the server. Used during\n// negotiation to detect possible MitM attempt.\n//\n// If cs is not nil, the SCRAM PLUS-variant is negotiated, with channel binding to\n// the unique TLS connection, either using \"tls-exporter\" for TLS 1.3 and later, or\n// \"tls-unique\" otherwise.\n//\n// If cs is nil, no channel binding is done. If noServerPlus is also false, the\n// client is configured to not attempt/\"support\" the PLUS-variant, ensuring servers\n// that do support the PLUS-variant do not abort the connection.\n//\n// The sequence for data and calls on a client:\n//\n//   - ClientFirst, write result to server.\n//   - Read response from server, feed to ServerFirst, write response to server.\n//   - Read response from server, feed to ServerFinal.\nfunc NewClient(h func() hash.Hash, authc, authz string, noServerPlus bool, cs *tls.ConnectionState) *Client {\n\tauthc = norm.NFC.String(authc)\n\tauthz = norm.NFC.String(authz)\n\treturn &Client{authc: authc, authz: authz, h: h, noServerPlus: noServerPlus, cs: cs}\n}\n\n// ClientFirst returns the first client message to write to the server.\n// No channel binding is done/supported.\n// A random nonce is generated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst processes the first response message from the server. The\n// provided nonce, salt and iterations are checked. If valid, a final client\n// message is calculated and returned. This message must be written to the\n// server. It includes proof that the client knows the password.\nfunc (c *Client) ServerFirst(serverFirst []byte, password string) (clientFinal string, rerr error) {\n\tc.serverFirst = string(serverFirst)\n\tp := newParser(serverFirst)\n\tdefer p.recover(&rerr)\n\n\t// ../rfc/5802:632\n\t// ../rfc/5802:959\n\tif p.take(\"m=\") {\n\t\tp.xerrorf(\"unsupported mandatory extension: %w\", ErrExtensionsNotSupported) // ../rfc/5802:973\n\t}\n\n\tc.nonce = p.xnonce()\n\tp.xtake(\",\")\n\tsalt := p.xsalt()\n\tp.xtake(\",\")\n\titerations := p.xiterations()\n\t// We ignore extensions that we don't know about.\n\tfor p.take(\",\") {\n\t\tp.xattrval()\n\t}\n\tp.xempty()\n\n\tif !strings.HasPrefix(c.nonce, c.clientNonce) {\n\t\treturn \"\", fmt.Errorf(\"%w: server dropped our nonce\", ErrProtocol)\n\t}\n\tif len(c.nonce)-len(c.clientNonce) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: server nonce too short\", ErrUnsafe)\n\t}\n\tif len(salt) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: salt too short\", ErrUnsafe)\n\t}\n\tif iterations < 2048 {\n\t\treturn \"\", fmt.Errorf(\"%w: too few iterations\", ErrUnsafe)\n\t}\n\n\t// We send our channel binding data if present. If the server has different values,\n\t// we'll get an error. If any MitM would try to modify the channel binding data,\n\t// the server cannot verify our signature and will fail the attempt.\n\t// ../rfc/5802:925 ../rfc/5802:1015\n\tcbindInput := append([]byte(c.gs2header), c.channelBindData...)\n\tc.clientFinalWithoutProof = fmt.Sprintf(\"c=%s,r=%s\", base64.StdEncoding.EncodeToString(cbindInput), c.nonce)\n\n\tc.authMessage = c.clientFirstBare + \",\" + c.serverFirst + \",\" + c.clientFinalWithoutProof\n\n\tc.saltedPassword = SaltPassword(c.h, password, salt, iterations)\n\tclientKey := hmac0(c.h, c.saltedPassword, \"Client Key\")\n\th := c.h()\n\th.Write(clientKey)\n\tstoredKey := h.Sum(nil)\n\tclientSig := hmac0(c.h, storedKey, c.authMessage)\n\txor(clientSig, clientKey) // Now clientProof.\n\tclientProof := clientSig\n\n\tr := c.clientFinalWithoutProof + \",p=\" + base64.StdEncoding.EncodeToString(clientProof)\n\treturn r, nil\n}\n\n// ServerFinal processes the final message from the server, verifying that the\n// server knows the password.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Convert \",\" to =2C and \"=\" to =3D.\nfunc saslname(s string) string {\n\tvar r string\n\tfor _, c := range s {\n\t\tif c == ',' {\n\t\t\tr += \"=2C\"\n\t\t} else if c == '=' {\n\t\t\tr += \"=3D\"\n\t\t} else {\n\t\t\tr += string(c)\n\t\t}\n\t}\n\treturn r\n}\n\n```\n# Function Signature: func (c *Client) ClientFirst() (clientFirst string, rerr error) {\n# Natural Language Annotations: // ClientFirst returns the first client message to write to the server.\n// No channel binding is done/supported.\n// A random nonce is generated.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (c *Client) ClientFirst() (clientFirst string, rerr error) {\n\tdefer func() {\n\t\tif r := recover(); r != nil {\n\t\t\trerr = fmt.Errorf(\"panic: %v\", r)\n\t\t}\n\t}()\n\n\tc.clientNonce = base64.StdEncoding.EncodeToString(MakeRandom())\n\tc.gs2header = \"n,,\"\n\tc.clientFirstBare = fmt.Sprintf(\"n,,n=%s,r=%s\", c.gs2header, c.clientNonce)\n\treturn c.clientFirstBare, nil\n}\n```\n\nIn this function, we first generate a random nonce using the `MakeRandom` function provided in the `scram` package. We then encode this nonce using base64. The `gs2header` is set as \"n,,\" and the `clientFirstBare` is the first part of the client message which is constructed using the `n`, `gs2header`, and `clientNonce`.\n\nNote: We are using a panic recovery function to handle any potential panics that might occur during execution of this function. If a panic occurs, we capture the error and set it to `rerr`.", "```go\nfunc (c *Client) ClientFirst() (clientFirst string, rerr error) {\n\tc.clientNonce = base64.StdEncoding.EncodeToString(MakeRandom())\n\tc.clientFirstBare = \"n=\" + saslname(c.authc) + \",r=\" + c.clientNonce\n\tc.gs2header = \"n,,\"\n\n\tif c.authz != \"\" {\n\t\tc.clientFirstBare += \",a=\" + saslname(c.authz)\n\t\tc.gs2header += \"a=\" + c.authz\n\t}\n\n\tif c.cs != nil {\n\t\tc.channelBindData, rerr = channelBindData(c.cs)\n\t\tif rerr != nil {\n\t\t\treturn \"\", fmt.Errorf(\"get channel binding data: %w\", rerr)\n\t\t}\n\t\tc.gs2header += \",c=biws\"\n\t}\n\n\treturn \"n,,n=\" + c.clientFirstBare, nil\n}\n```", "```go\nfunc (c *Client) ClientFirst() (clientFirst string, rerr error) {\n\tc.clientNonce = base64.StdEncoding.EncodeToString(MakeRandom())\n\tc.clientFirstBare = fmt.Sprintf(\"n=%s,r=%s\", saslname(c.authc), c.clientNonce)\n\tif c.authz != \"\" {\n\t\tc.clientFirstBare += fmt.Sprintf(\",a=%s\", saslname(c.authz))\n\t}\n\tc.gs2header = \"n,,n=\" + saslname(c.authc)\n\tif c.authz != \"\" {\n\t\tc.gs2header += \",a=\" + saslname(c.authz)\n\t}\n\tclientFirst = c.clientFirstBare\n\treturn\n}\n```"]}, "mox-scram/scram.go-ServerFinal": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/scram/scram.go:\n```\n// Package scram implements the SCRAM-SHA-* SASL authentication mechanism, RFC 7677 and RFC 5802.\n//\n// SCRAM-SHA-256 and SCRAM-SHA-1 allow a client to authenticate to a server using a\n// password without handing plaintext password over to the server. The client also\n// verifies the server knows (a derivative of) the password. Both the client and\n// server side are implemented.\npackage scram\n\n// todo: test with messages that contains extensions\n// todo: some tests for the parser\n// todo: figure out how invalid parameters etc should be handled. just abort? perhaps mostly a problem for imap.\n\nimport (\n\t\"bytes\"\n\t\"crypto/hmac\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/tls\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"strings\"\n\n\t\"golang.org/x/crypto/pbkdf2\"\n\t\"golang.org/x/text/secure/precis\"\n\t\"golang.org/x/text/unicode/norm\"\n)\n\n// Errors at scram protocol level. Can be exchanged between client and server.\nvar (\n\tErrInvalidEncoding                 Error = \"invalid-encoding\"\n\tErrExtensionsNotSupported          Error = \"extensions-not-supported\"\n\tErrInvalidProof                    Error = \"invalid-proof\"\n\tErrChannelBindingsDontMatch        Error = \"channel-bindings-dont-match\"\n\tErrServerDoesSupportChannelBinding Error = \"server-does-support-channel-binding\"\n\tErrChannelBindingNotSupported      Error = \"channel-binding-not-supported\"\n\tErrUnsupportedChannelBindingType   Error = \"unsupported-channel-binding-type\"\n\tErrUnknownUser                     Error = \"unknown-user\"\n\tErrNoResources                     Error = \"no-resources\"\n\tErrOtherError                      Error = \"other-error\"\n)\n\nvar scramErrors = makeErrors()\n\nfunc makeErrors() map[string]Error {\n\tl := []Error{\n\t\tErrInvalidEncoding,\n\t\tErrExtensionsNotSupported,\n\t\tErrInvalidProof,\n\t\tErrChannelBindingsDontMatch,\n\t\tErrServerDoesSupportChannelBinding,\n\t\tErrChannelBindingNotSupported,\n\t\tErrUnsupportedChannelBindingType,\n\t\tErrUnknownUser,\n\t\tErrNoResources,\n\t\tErrOtherError,\n\t}\n\tm := map[string]Error{}\n\tfor _, e := range l {\n\t\tm[string(e)] = e\n\t}\n\treturn m\n}\n\nvar (\n\tErrNorm     = errors.New(\"parameter not unicode normalized\") // E.g. if client sends non-normalized username or authzid.\n\tErrUnsafe   = errors.New(\"unsafe parameter\")                 // E.g. salt, nonce too short, or too few iterations.\n\tErrProtocol = errors.New(\"protocol error\")                   // E.g. server responded with a nonce not prefixed by the client nonce.\n)\n\ntype Error string\n\nfunc (e Error) Error() string {\n\treturn string(e)\n}\n\n// MakeRandom returns a cryptographically random buffer for use as salt or as\n// nonce.\nfunc MakeRandom() []byte {\n\tbuf := make([]byte, 12)\n\t_, err := cryptorand.Read(buf)\n\tif err != nil {\n\t\tpanic(\"generate random\")\n\t}\n\treturn buf\n}\n\n// Cleanup password with precis, like remote should have done. If the password\n// appears invalid, we'll return the original, there is a chance the server also\n// doesn't enforce requirements and accepts it. ../rfc/8265:679\nfunc precisPassword(password string) string {\n\tpw, err := precis.OpaqueString.String(password)\n\tif err != nil {\n\t\treturn password\n\t}\n\treturn pw\n}\n\n// SaltPassword returns a salted password.\nfunc SaltPassword(h func() hash.Hash, password string, salt []byte, iterations int) []byte {\n\tpassword = precisPassword(password)\n\treturn pbkdf2.Key([]byte(password), salt, iterations, h().Size(), h)\n}\n\n// hmac0 returns the hmac with key over msg.\nfunc hmac0(h func() hash.Hash, key []byte, msg string) []byte {\n\tmac := hmac.New(h, key)\n\tmac.Write([]byte(msg))\n\treturn mac.Sum(nil)\n}\n\nfunc xor(a, b []byte) {\n\tfor i := range a {\n\t\ta[i] ^= b[i]\n\t}\n}\n\nfunc channelBindData(cs *tls.ConnectionState) ([]byte, error) {\n\tif cs.Version <= tls.VersionTLS12 {\n\t\tif cs.TLSUnique == nil {\n\t\t\treturn nil, fmt.Errorf(\"no channel binding data available\")\n\t\t}\n\t\treturn cs.TLSUnique, nil\n\t}\n\n\t// \"tls-exporter\", ../rfc/9266:95\n\t// Since TLS 1.3, a zero-length and absent context have the same behaviour. ../rfc/8446:5385 ../rfc/8446:5405\n\t// This is different from TLS 1.2 and earlier. ../rfc/5705:206 ../rfc/5705:245\n\treturn cs.ExportKeyingMaterial(\"EXPORTER-Channel-Binding\", []byte{}, 32)\n}\n\n// Server represents the server-side of a SCRAM-SHA-* authentication.\ntype Server struct {\n\tAuthentication string // Username for authentication, \"authc\". Always set and non-empty.\n\tAuthorization  string // If set, role of user to assume after authentication, \"authz\".\n\n\th func() hash.Hash // sha1.New or sha256.New\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\n\tgs2header           string\n\tclientNonce         string // Client-part of the nonce.\n\tserverNonceOverride string // If set, server does not generate random nonce, but uses this. For tests with the test vector.\n\tnonce               string // Full client + server nonce.\n\tchannelBinding      []byte\n}\n\n// NewServer returns a server given the first SCRAM message from a client.\n//\n// If cs is set, the PLUS variant can be negotiated, binding the authentication\n// exchange to the TLS channel (preventing MitM attempts). If a client\n// indicates it supports the PLUS variant, but thinks the server does not, the\n// authentication attempt will fail.\n//\n// If channelBindingRequired is set, the client has indicated it will do channel\n// binding and not doing so will cause the authentication to fail.\n//\n// The sequence for data and calls on a server:\n//\n//   - Read initial data from client, call NewServer (this call), then ServerFirst and write to the client.\n//   - Read response from client, call Finish or FinishFinal and write the resulting string.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst returns the string to send back to the client. To be called after NewServer.\n\n\n\n\n\n\n\n\n\n\n\n// Finish takes the final client message, and the salted password (probably\n// from server storage), verifies the client, and returns a message to return\n// to the client. If err is nil, authentication was successful. If the\n// authorization requested is not acceptable, the server should call\n// FinishError instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// FinishError returns an error message to write to the client for the final\n// server message.\nfunc (s *Server) FinishError(err Error) string {\n\treturn \"e=\" + string(err)\n}\n\n// Client represents the client-side of a SCRAM-SHA-* authentication.\ntype Client struct {\n\tauthc string\n\tauthz string\n\n\th            func() hash.Hash     // sha1.New or sha256.New\n\tnoServerPlus bool                 // Server did not announce support for PLUS-variant.\n\tcs           *tls.ConnectionState // If set, use PLUS-variant.\n\n\t// Messages used in hash calculations.\n\tclientFirstBare         string\n\tserverFirst             string\n\tclientFinalWithoutProof string\n\tauthMessage             string\n\n\tgs2header       string\n\tclientNonce     string\n\tnonce           string // Full client + server nonce.\n\tsaltedPassword  []byte\n\tchannelBindData []byte // For PLUS-variant.\n}\n\n// NewClient returns a client for authentication authc, optionally for\n// authorization with role authz, for the hash (sha1.New or sha256.New).\n//\n// If noServerPlus is true, the client would like to have used the PLUS-variant,\n// that binds the authentication attempt to the TLS connection, but the client did\n// not see support for the PLUS variant announced by the server. Used during\n// negotiation to detect possible MitM attempt.\n//\n// If cs is not nil, the SCRAM PLUS-variant is negotiated, with channel binding to\n// the unique TLS connection, either using \"tls-exporter\" for TLS 1.3 and later, or\n// \"tls-unique\" otherwise.\n//\n// If cs is nil, no channel binding is done. If noServerPlus is also false, the\n// client is configured to not attempt/\"support\" the PLUS-variant, ensuring servers\n// that do support the PLUS-variant do not abort the connection.\n//\n// The sequence for data and calls on a client:\n//\n//   - ClientFirst, write result to server.\n//   - Read response from server, feed to ServerFirst, write response to server.\n//   - Read response from server, feed to ServerFinal.\nfunc NewClient(h func() hash.Hash, authc, authz string, noServerPlus bool, cs *tls.ConnectionState) *Client {\n\tauthc = norm.NFC.String(authc)\n\tauthz = norm.NFC.String(authz)\n\treturn &Client{authc: authc, authz: authz, h: h, noServerPlus: noServerPlus, cs: cs}\n}\n\n// ClientFirst returns the first client message to write to the server.\n// No channel binding is done/supported.\n// A random nonce is generated.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// ServerFirst processes the first response message from the server. The\n// provided nonce, salt and iterations are checked. If valid, a final client\n// message is calculated and returned. This message must be written to the\n// server. It includes proof that the client knows the password.\nfunc (c *Client) ServerFirst(serverFirst []byte, password string) (clientFinal string, rerr error) {\n\tc.serverFirst = string(serverFirst)\n\tp := newParser(serverFirst)\n\tdefer p.recover(&rerr)\n\n\t// ../rfc/5802:632\n\t// ../rfc/5802:959\n\tif p.take(\"m=\") {\n\t\tp.xerrorf(\"unsupported mandatory extension: %w\", ErrExtensionsNotSupported) // ../rfc/5802:973\n\t}\n\n\tc.nonce = p.xnonce()\n\tp.xtake(\",\")\n\tsalt := p.xsalt()\n\tp.xtake(\",\")\n\titerations := p.xiterations()\n\t// We ignore extensions that we don't know about.\n\tfor p.take(\",\") {\n\t\tp.xattrval()\n\t}\n\tp.xempty()\n\n\tif !strings.HasPrefix(c.nonce, c.clientNonce) {\n\t\treturn \"\", fmt.Errorf(\"%w: server dropped our nonce\", ErrProtocol)\n\t}\n\tif len(c.nonce)-len(c.clientNonce) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: server nonce too short\", ErrUnsafe)\n\t}\n\tif len(salt) < 8 {\n\t\treturn \"\", fmt.Errorf(\"%w: salt too short\", ErrUnsafe)\n\t}\n\tif iterations < 2048 {\n\t\treturn \"\", fmt.Errorf(\"%w: too few iterations\", ErrUnsafe)\n\t}\n\n\t// We send our channel binding data if present. If the server has different values,\n\t// we'll get an error. If any MitM would try to modify the channel binding data,\n\t// the server cannot verify our signature and will fail the attempt.\n\t// ../rfc/5802:925 ../rfc/5802:1015\n\tcbindInput := append([]byte(c.gs2header), c.channelBindData...)\n\tc.clientFinalWithoutProof = fmt.Sprintf(\"c=%s,r=%s\", base64.StdEncoding.EncodeToString(cbindInput), c.nonce)\n\n\tc.authMessage = c.clientFirstBare + \",\" + c.serverFirst + \",\" + c.clientFinalWithoutProof\n\n\tc.saltedPassword = SaltPassword(c.h, password, salt, iterations)\n\tclientKey := hmac0(c.h, c.saltedPassword, \"Client Key\")\n\th := c.h()\n\th.Write(clientKey)\n\tstoredKey := h.Sum(nil)\n\tclientSig := hmac0(c.h, storedKey, c.authMessage)\n\txor(clientSig, clientKey) // Now clientProof.\n\tclientProof := clientSig\n\n\tr := c.clientFinalWithoutProof + \",p=\" + base64.StdEncoding.EncodeToString(clientProof)\n\treturn r, nil\n}\n\n// ServerFinal processes the final message from the server, verifying that the\n// server knows the password.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Convert \",\" to =2C and \"=\" to =3D.\nfunc saslname(s string) string {\n\tvar r string\n\tfor _, c := range s {\n\t\tif c == ',' {\n\t\t\tr += \"=2C\"\n\t\t} else if c == '=' {\n\t\t\tr += \"=3D\"\n\t\t} else {\n\t\t\tr += string(c)\n\t\t}\n\t}\n\treturn r\n}\n\n```\n# Function Signature: func (c *Client) ServerFinal(serverFinal []byte) (rerr error) {\n# Natural Language Annotations: // ServerFinal processes the final message from the server, verifying that the\n// server knows the password.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (c *Client) ServerFinal(serverFinal []byte) (rerr error) {\n\tserverFinalMessage := string(serverFinal)\n\tp := newParser(serverFinal)\n\tdefer p.recover(&rerr)\n\n\tp.xtake(\"v=\")\n\tserverSignature := p.xsig()\n\tp.xempty()\n\n\tserverKey := hmac0(c.h, c.saltedPassword, \"Server Key\")\n\th := c.h()\n\th.Write(serverKey)\n\tstoredKey := h.Sum(nil)\n\n\tserverSig := hmac0(c.h, storedKey, c.authMessage)\n\tif !bytes.Equal(serverSignature, serverSig) {\n\t\treturn fmt.Errorf(\"%w: invalid proof\", ErrInvalidProof)\n\t}\n\n\treturn nil\n}\n```", "```go\nfunc (c *Client) ServerFinal(serverFinal []byte) (rerr error) {\n\tp := newParser(serverFinal)\n\tdefer p.recover(&rerr)\n\n\tp.xtake(\"v=\")\n\tverifier := p.xsaslname()\n\tp.xempty()\n\n\tserverKey := hmac0(c.h, c.saltedPassword, \"Server Key\")\n\tserverSig := hmac0(c.h, serverKey, c.authMessage)\n\n\tif verifier != base64.StdEncoding.EncodeToString(serverSig) {\n\t\treturn fmt.Errorf(\"%w: invalid proof\", ErrInvalidProof)\n\t}\n\n\treturn nil\n}\n```\nThis function processes the final message from the server, verifying that the server knows the password. It uses the `hmac0` function to calculate the server signature and compares it with the verifier received from the server. If they match, the function returns nil, otherwise it returns an error.", "```go\nfunc (c *Client) ServerFinal(serverFinal []byte) (rerr error) {\n\tp := newParser(serverFinal)\n\tdefer p.recover(&rerr)\n\n\tp.xtake(\"v=\")\n\tverifier := p.xsasl()\n\tp.xempty()\n\n\tif bytes.Compare(hmac0(c.h, c.saltedPassword, c.authMessage), verifier) != 0 {\n\t\treturn fmt.Errorf(\"%w: server proof does not match\", ErrInvalidProof)\n\t}\n\n\treturn nil\n}\n```"]}, "mox-autotls/autotls.go-SetAllowedHostnames": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/autotls/autotls.go:\n```\n// Package autotls automatically configures TLS (for SMTP, IMAP, HTTP) by\n// requesting certificates with ACME, typically from Let's Encrypt.\npackage autotls\n\n// We do tls-alpn-01, and also http-01. For DNS we would need a third party tool\n// with an API that can make the DNS changes, as we don't want to link in dozens of\n// bespoke API's for DNS record manipulation into mox.\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto\"\n\t\"crypto/ecdsa\"\n\t\"crypto/elliptic\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/rsa\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"encoding/pem\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/crypto/acme\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/autocert\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/moxvar\"\n)\n\nvar (\n\tmetricCertput = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_autotls_certput_total\",\n\t\t\tHelp: \"Number of certificate store puts.\",\n\t\t},\n\t)\n)\n\n// Manager is in charge of a single ACME identity, and automatically requests\n// certificates for allowlisted hosts.\ntype Manager struct {\n\tACMETLSConfig *tls.Config // For serving HTTPS on port 443, which is required for certificate requests to succeed.\n\tTLSConfig     *tls.Config // For all TLS servers not used for validating ACME requests. Like SMTP and IMAP (including with STARTTLS) and HTTPS on ports other than 443.\n\tManager       *autocert.Manager\n\n\tshutdown <-chan struct{}\n\n\tsync.Mutex\n\thosts map[dns.Domain]struct{}\n}\n\n// Load returns an initialized autotls manager for \"name\" (used for the ACME key\n// file and requested certs and their keys). All files are stored within acmeDir.\n//\n// contactEmail must be a valid email address to which notifications about ACME can\n// be sent. directoryURL is the ACME starting point.\n//\n// eabKeyID and eabKey are for external account binding when making a new account,\n// which some ACME providers require.\n//\n// getPrivateKey is called to get the private key for the host and key type. It\n// can be used to deliver a specific (e.g. always the same) private key for a\n// host, or a newly generated key.\n//\n// When shutdown is closed, no new TLS connections can be created.\nfunc Load(name, acmeDir, contactEmail, directoryURL string, eabKeyID string, eabKey []byte, getPrivateKey func(host string, keyType autocert.KeyType) (crypto.Signer, error), shutdown <-chan struct{}) (*Manager, error) {\n\tif directoryURL == \"\" {\n\t\treturn nil, fmt.Errorf(\"empty ACME directory URL\")\n\t}\n\tif contactEmail == \"\" {\n\t\treturn nil, fmt.Errorf(\"empty contact email\")\n\t}\n\n\t// Load identity key if it exists. Otherwise, create a new key.\n\tp := filepath.Join(acmeDir, name+\".key\")\n\tvar key crypto.Signer\n\tf, err := os.Open(p)\n\tif f != nil {\n\t\tdefer f.Close()\n\t}\n\tif err != nil && os.IsNotExist(err) {\n\t\tkey, err = ecdsa.GenerateKey(elliptic.P256(), cryptorand.Reader)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"generating ecdsa identity key: %s\", err)\n\t\t}\n\t\tder, err := x509.MarshalPKCS8PrivateKey(key)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"marshal identity key: %s\", err)\n\t\t}\n\t\tblock := &pem.Block{\n\t\t\tType: \"PRIVATE KEY\",\n\t\t\tHeaders: map[string]string{\n\t\t\t\t\"Note\": fmt.Sprintf(\"PEM PKCS8 ECDSA private key generated for ACME provider %s by mox\", name),\n\t\t\t},\n\t\t\tBytes: der,\n\t\t}\n\t\tb := &bytes.Buffer{}\n\t\tif err := pem.Encode(b, block); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"pem encode: %s\", err)\n\t\t} else if err := os.WriteFile(p, b.Bytes(), 0660); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"writing identity key: %s\", err)\n\t\t}\n\t} else if err != nil {\n\t\treturn nil, fmt.Errorf(\"open identity key file: %s\", err)\n\t} else {\n\t\tvar privKey any\n\t\tif buf, err := io.ReadAll(f); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"reading identity key: %s\", err)\n\t\t} else if p, _ := pem.Decode(buf); p == nil {\n\t\t\treturn nil, fmt.Errorf(\"no pem data\")\n\t\t} else if p.Type != \"PRIVATE KEY\" {\n\t\t\treturn nil, fmt.Errorf(\"got PEM block %q, expected \\\"PRIVATE KEY\\\"\", p.Type)\n\t\t} else if privKey, err = x509.ParsePKCS8PrivateKey(p.Bytes); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"parsing PKCS8 private key: %s\", err)\n\t\t}\n\t\tswitch k := privKey.(type) {\n\t\tcase *ecdsa.PrivateKey:\n\t\t\tkey = k\n\t\tcase *rsa.PrivateKey:\n\t\t\tkey = k\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"unsupported private key type %T\", key)\n\t\t}\n\t}\n\n\tm := &autocert.Manager{\n\t\tCache:  dirCache(filepath.Join(acmeDir, \"keycerts\", name)),\n\t\tPrompt: autocert.AcceptTOS,\n\t\tEmail:  contactEmail,\n\t\tClient: &acme.Client{\n\t\t\tDirectoryURL: directoryURL,\n\t\t\tKey:          key,\n\t\t\tUserAgent:    \"mox/\" + moxvar.Version,\n\t\t},\n\t\tGetPrivateKey: getPrivateKey,\n\t\t// HostPolicy set below.\n\t}\n\t// If external account binding key is provided, use it for registering a new account.\n\t// todo: ideally the key and its id are provided temporarily by the admin when registering a new account. but we don't do that interactive setup yet. in the future, an interactive setup/quickstart would ask for the key once to register a new acme account.\n\tif eabKeyID != \"\" {\n\t\tm.ExternalAccountBinding = &acme.ExternalAccountBinding{\n\t\t\tKID: eabKeyID,\n\t\t\tKey: eabKey,\n\t\t}\n\t}\n\n\tloggingGetCertificate := func(hello *tls.ClientHelloInfo) (*tls.Certificate, error) {\n\t\tlog := mlog.New(\"autotls\", nil).WithContext(hello.Context())\n\n\t\t// We handle missing invalid hostnames/ip's by returning a nil certificate and nil\n\t\t// error, which crypto/tls turns into a TLS alert \"unrecognized name\", which can be\n\t\t// interpreted by clients as a hint that they are using the wrong hostname, or a\n\t\t// certificate is missing.\n\n\t\t// Handle missing SNI to prevent logging an error below.\n\t\t// At startup, during config initialization, we already adjust the tls config to\n\t\t// inject the listener hostname if there isn't one in the TLS client hello. This is\n\t\t// common for SMTP STARTTLS connections, which often do not care about the\n\t\t// verification of the certificate.\n\t\tif hello.ServerName == \"\" {\n\t\t\tlog.Debug(\"tls request without sni servername, rejecting\", slog.Any(\"localaddr\", hello.Conn.LocalAddr()), slog.Any(\"supportedprotos\", hello.SupportedProtos))\n\t\t\treturn nil, nil\n\t\t}\n\n\t\tcert, err := m.GetCertificate(hello)\n\t\tif err != nil && errors.Is(err, errHostNotAllowed) {\n\t\t\tlog.Debugx(\"requesting certificate\", err, slog.String(\"host\", hello.ServerName))\n\t\t\treturn nil, nil\n\t\t} else if err != nil {\n\t\t\tlog.Errorx(\"requesting certificate\", err, slog.String(\"host\", hello.ServerName))\n\t\t}\n\t\treturn cert, err\n\t}\n\n\tacmeTLSConfig := *m.TLSConfig()\n\tacmeTLSConfig.GetCertificate = loggingGetCertificate\n\n\ttlsConfig := tls.Config{\n\t\tGetCertificate: loggingGetCertificate,\n\t}\n\n\ta := &Manager{\n\t\tACMETLSConfig: &acmeTLSConfig,\n\t\tTLSConfig:     &tlsConfig,\n\t\tManager:       m,\n\t\tshutdown:      shutdown,\n\t\thosts:         map[dns.Domain]struct{}{},\n\t}\n\tm.HostPolicy = a.HostPolicy\n\treturn a, nil\n}\n\n// CertAvailable checks whether a non-expired ECDSA certificate is available in the\n// cache for host. No other checks than expiration are done.\nfunc (m *Manager) CertAvailable(ctx context.Context, log mlog.Log, host dns.Domain) (bool, error) {\n\tck := host.ASCII // Would be \"+rsa\" for rsa keys.\n\tdata, err := m.Manager.Cache.Get(ctx, ck)\n\tif err != nil && errors.Is(err, autocert.ErrCacheMiss) {\n\t\treturn false, nil\n\t} else if err != nil {\n\t\treturn false, fmt.Errorf(\"attempt to get certificate from cache: %v\", err)\n\t}\n\n\t// The cached keycert is of the form: private key, leaf certificate, intermediate certificates...\n\tprivb, rem := pem.Decode(data)\n\tif privb == nil {\n\t\treturn false, fmt.Errorf(\"missing private key in cached keycert file\")\n\t}\n\tpubb, _ := pem.Decode(rem)\n\tif pubb == nil {\n\t\treturn false, fmt.Errorf(\"missing certificate in cached keycert file\")\n\t} else if pubb.Type != \"CERTIFICATE\" {\n\t\treturn false, fmt.Errorf(\"second pem block is %q, expected CERTIFICATE\", pubb.Type)\n\t}\n\tcert, err := x509.ParseCertificate(pubb.Bytes)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"parsing certificate from cached keycert file: %v\", err)\n\t}\n\t// We assume the certificate has a matching hostname, and is properly CA-signed. We\n\t// only check the expiration time.\n\tif time.Until(cert.NotBefore) > 0 || time.Since(cert.NotAfter) > 0 {\n\t\treturn false, nil\n\t}\n\treturn true, nil\n}\n\n// SetAllowedHostnames sets a new list of allowed hostnames for automatic TLS.\n// After setting the host names, a goroutine is start to check that new host names\n// are fully served by publicIPs (only if non-empty and there is no unspecified\n// address in the list). If no, log an error with a warning that ACME validation\n// may fail.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Hostnames returns the allowed host names for use with ACME.\nfunc (m *Manager) Hostnames() []dns.Domain {\n\tm.Lock()\n\tdefer m.Unlock()\n\tvar l []dns.Domain\n\tfor h := range m.hosts {\n\t\tl = append(l, h)\n\t}\n\treturn l\n}\n\nvar errHostNotAllowed = errors.New(\"autotls: host not in allowlist\")\n\n// HostPolicy decides if a host is allowed for use with ACME, i.e. whether a\n// certificate will be returned if present and/or will be requested if not yet\n// present. Only hosts added with SetAllowedHostnames are allowed. During shutdown,\n// no new connections are allowed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype dirCache autocert.DirCache\n\nfunc (d dirCache) Delete(ctx context.Context, name string) (rerr error) {\n\tlog := mlog.New(\"autotls\", nil).WithContext(ctx)\n\tdefer func() {\n\t\tlog.Debugx(\"dircache delete result\", rerr, slog.String(\"name\", name))\n\t}()\n\terr := autocert.DirCache(d).Delete(ctx, name)\n\tif err != nil {\n\t\tlog.Errorx(\"deleting cert from dir cache\", err, slog.String(\"name\", name))\n\t} else if !strings.HasSuffix(name, \"+token\") {\n\t\tlog.Info(\"autotls cert delete\", slog.String(\"name\", name))\n\t}\n\treturn err\n}\n\nfunc (d dirCache) Get(ctx context.Context, name string) (rbuf []byte, rerr error) {\n\tlog := mlog.New(\"autotls\", nil).WithContext(ctx)\n\tdefer func() {\n\t\tlog.Debugx(\"dircache get result\", rerr, slog.String(\"name\", name))\n\t}()\n\tbuf, err := autocert.DirCache(d).Get(ctx, name)\n\tif err != nil && errors.Is(err, autocert.ErrCacheMiss) {\n\t\tlog.Infox(\"getting cert from dir cache\", err, slog.String(\"name\", name))\n\t} else if err != nil {\n\t\tlog.Errorx(\"getting cert from dir cache\", err, slog.String(\"name\", name))\n\t} else if !strings.HasSuffix(name, \"+token\") {\n\t\tlog.Debug(\"autotls cert get\", slog.String(\"name\", name))\n\t}\n\treturn buf, err\n}\n\nfunc (d dirCache) Put(ctx context.Context, name string, data []byte) (rerr error) {\n\tlog := mlog.New(\"autotls\", nil).WithContext(ctx)\n\tdefer func() {\n\t\tlog.Debugx(\"dircache put result\", rerr, slog.String(\"name\", name))\n\t}()\n\tmetricCertput.Inc()\n\terr := autocert.DirCache(d).Put(ctx, name, data)\n\tif err != nil {\n\t\tlog.Errorx(\"storing cert in dir cache\", err, slog.String(\"name\", name))\n\t} else if !strings.HasSuffix(name, \"+token\") {\n\t\tlog.Info(\"autotls cert store\", slog.String(\"name\", name))\n\t}\n\treturn err\n}\n\n```\n# Function Signature: func (m *Manager) SetAllowedHostnames(log mlog.Log, resolver dns.Resolver, hostnames map[dns.Domain]struct{}, publicIPs []string, checkHosts bool) {\n# Natural Language Annotations: // SetAllowedHostnames sets a new list of allowed hostnames for automatic TLS.\n// After setting the host names, a goroutine is start to check that new host names\n// are fully served by publicIPs (only if non-empty and there is no unspecified\n// address in the list). If no, log an error with a warning that ACME validation\n// may fail.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (m *Manager) SetAllowedHostnames(log mlog.Log, resolver dns.Resolver, hostnames map[dns.Domain]struct{}, publicIPs []string, checkHosts bool) {\n\tm.Lock()\n\tdefer m.Unlock()\n\tm.hosts = hostnames\n\n\tif checkHosts && len(publicIPs) > 0 {\n\t\tgo func() {\n\t\t\tfor host := range hostnames {\n\t\t\t\tips, err := resolver.LookupIPAddr(context.Background(), host.ASCII)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Errorx(\"resolving hostname for ACME validation\", err, slog.String(\"hostname\", host.ASCII))\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tipFound := false\n\t\t\t\tfor _, ip := range ips {\n\t\t\t\t\tif ip.IP.IsUnspecified() {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\n\t\t\t\t\tfor _, publicIP := range publicIPs {\n\t\t\t\t\t\tif ip.IP.String() == publicIP {\n\t\t\t\t\t\t\tipFound = true\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tif ipFound {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif !ipFound {\n\t\t\t\t\tlog.Warn(\"no public IP found for hostname, ACME validation may fail\", slog.String(\"hostname\", host.ASCII))\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t}\n}\n```\nThis function sets the allowed hostnames for automatic TLS. It also starts a goroutine to check if the new hostnames are fully served by the provided public IPs. If not, it logs a warning message stating that ACME validation may fail.", "Here is the generated code:\n\n```go\nfunc (m *Manager) SetAllowedHostnames(log mlog.Log, resolver dns.Resolver, hostnames map[dns.Domain]struct{}, publicIPs []string, checkHosts bool) {\n\tm.Lock()\n\tm.hosts = hostnames\n\tm.Unlock()\n\n\tif checkHosts && len(publicIPs) > 0 && !contains(publicIPs, \"\") {\n\t\tgo func() {\n\t\t\tfor h := range hostnames {\n\t\t\t\tips, err := resolver.LookupHost(h.ASCII)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Errorx(\"resolving hostnames for acme validation\", err, slog.String(\"host\", h.ASCII))\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tfor _, ip := range ips {\n\t\t\t\t\tif !contains(publicIPs, ip) {\n\t\t\t\t\t\tlog.Error(\"acme validation hostname not served by public IPs\", slog.String(\"host\", h.ASCII), slog.String(\"ip\", ip))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t}\n}\n\nfunc contains(slice []string, item string) bool {\n\tfor _, a := range slice {\n\t\tif a == item {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n```\n\nThis function first locks the manager to prevent concurrent access, then sets the allowed hostnames. If `checkHosts` is true and `publicIPs` is not empty and does not contain an unspecified address, a goroutine is started to check that each hostname is fully served by the public IPs. The function uses the provided resolver to resolve each hostname, and logs an error if any IP not in `publicIPs` is found. The `contains` helper function is used to check if a slice contains a given item.", "```go\nfunc (m *Manager) SetAllowedHostnames(log mlog.Log, resolver dns.Resolver, hostnames map[dns.Domain]struct{}, publicIPs []string, checkHosts bool) {\n\tm.Lock()\n\tdefer m.Unlock()\n\n\tm.hosts = hostnames\n\n\tif checkHosts && len(publicIPs) > 0 {\n\t\tgo func() {\n\t\t\tfor host := range hostnames {\n\t\t\t\tips, err := resolver.LookupIPAddr(context.Background(), host.ASCII)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Errorx(\"resolving host for ACME validation\", err, slog.String(\"host\", host.ASCII))\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tfound := false\n\t\t\t\tfor _, ip := range ips {\n\t\t\t\t\tif ip.IP.String() == publicIPs[0] || publicIPs[0] == \"\" {\n\t\t\t\t\t\tfound = true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif !found {\n\t\t\t\t\tlog.Warn(\"host not served by public IPs, ACME validation may fail\", slog.String(\"host\", host.ASCII), slog.String(\"public_ips\", strings.Join(publicIPs, \", \")))\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t}\n}\n```\n\nThis function sets the allowed hostnames for automatic TLS. It locks the manager to prevent concurrent access to the map of hostnames, then sets the hostnames to the provided map.\n\nIf the `checkHosts` flag is true and the `publicIPs` slice is not empty, a goroutine is started. This goroutine goes through each hostname and checks if it resolves to one of the provided public IPs. If it doesn't, a warning is logged stating that the host may not be served by the provided IPs, which could affect ACME validation."]}, "mox-autotls/autotls.go-HostPolicy": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/autotls/autotls.go:\n```\n// Package autotls automatically configures TLS (for SMTP, IMAP, HTTP) by\n// requesting certificates with ACME, typically from Let's Encrypt.\npackage autotls\n\n// We do tls-alpn-01, and also http-01. For DNS we would need a third party tool\n// with an API that can make the DNS changes, as we don't want to link in dozens of\n// bespoke API's for DNS record manipulation into mox.\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"crypto\"\n\t\"crypto/ecdsa\"\n\t\"crypto/elliptic\"\n\tcryptorand \"crypto/rand\"\n\t\"crypto/rsa\"\n\t\"crypto/tls\"\n\t\"crypto/x509\"\n\t\"encoding/pem\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"net\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"golang.org/x/crypto/acme\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\n\t\"github.com/mjl-/autocert\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/moxvar\"\n)\n\nvar (\n\tmetricCertput = promauto.NewCounter(\n\t\tprometheus.CounterOpts{\n\t\t\tName: \"mox_autotls_certput_total\",\n\t\t\tHelp: \"Number of certificate store puts.\",\n\t\t},\n\t)\n)\n\n// Manager is in charge of a single ACME identity, and automatically requests\n// certificates for allowlisted hosts.\ntype Manager struct {\n\tACMETLSConfig *tls.Config // For serving HTTPS on port 443, which is required for certificate requests to succeed.\n\tTLSConfig     *tls.Config // For all TLS servers not used for validating ACME requests. Like SMTP and IMAP (including with STARTTLS) and HTTPS on ports other than 443.\n\tManager       *autocert.Manager\n\n\tshutdown <-chan struct{}\n\n\tsync.Mutex\n\thosts map[dns.Domain]struct{}\n}\n\n// Load returns an initialized autotls manager for \"name\" (used for the ACME key\n// file and requested certs and their keys). All files are stored within acmeDir.\n//\n// contactEmail must be a valid email address to which notifications about ACME can\n// be sent. directoryURL is the ACME starting point.\n//\n// eabKeyID and eabKey are for external account binding when making a new account,\n// which some ACME providers require.\n//\n// getPrivateKey is called to get the private key for the host and key type. It\n// can be used to deliver a specific (e.g. always the same) private key for a\n// host, or a newly generated key.\n//\n// When shutdown is closed, no new TLS connections can be created.\nfunc Load(name, acmeDir, contactEmail, directoryURL string, eabKeyID string, eabKey []byte, getPrivateKey func(host string, keyType autocert.KeyType) (crypto.Signer, error), shutdown <-chan struct{}) (*Manager, error) {\n\tif directoryURL == \"\" {\n\t\treturn nil, fmt.Errorf(\"empty ACME directory URL\")\n\t}\n\tif contactEmail == \"\" {\n\t\treturn nil, fmt.Errorf(\"empty contact email\")\n\t}\n\n\t// Load identity key if it exists. Otherwise, create a new key.\n\tp := filepath.Join(acmeDir, name+\".key\")\n\tvar key crypto.Signer\n\tf, err := os.Open(p)\n\tif f != nil {\n\t\tdefer f.Close()\n\t}\n\tif err != nil && os.IsNotExist(err) {\n\t\tkey, err = ecdsa.GenerateKey(elliptic.P256(), cryptorand.Reader)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"generating ecdsa identity key: %s\", err)\n\t\t}\n\t\tder, err := x509.MarshalPKCS8PrivateKey(key)\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"marshal identity key: %s\", err)\n\t\t}\n\t\tblock := &pem.Block{\n\t\t\tType: \"PRIVATE KEY\",\n\t\t\tHeaders: map[string]string{\n\t\t\t\t\"Note\": fmt.Sprintf(\"PEM PKCS8 ECDSA private key generated for ACME provider %s by mox\", name),\n\t\t\t},\n\t\t\tBytes: der,\n\t\t}\n\t\tb := &bytes.Buffer{}\n\t\tif err := pem.Encode(b, block); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"pem encode: %s\", err)\n\t\t} else if err := os.WriteFile(p, b.Bytes(), 0660); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"writing identity key: %s\", err)\n\t\t}\n\t} else if err != nil {\n\t\treturn nil, fmt.Errorf(\"open identity key file: %s\", err)\n\t} else {\n\t\tvar privKey any\n\t\tif buf, err := io.ReadAll(f); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"reading identity key: %s\", err)\n\t\t} else if p, _ := pem.Decode(buf); p == nil {\n\t\t\treturn nil, fmt.Errorf(\"no pem data\")\n\t\t} else if p.Type != \"PRIVATE KEY\" {\n\t\t\treturn nil, fmt.Errorf(\"got PEM block %q, expected \\\"PRIVATE KEY\\\"\", p.Type)\n\t\t} else if privKey, err = x509.ParsePKCS8PrivateKey(p.Bytes); err != nil {\n\t\t\treturn nil, fmt.Errorf(\"parsing PKCS8 private key: %s\", err)\n\t\t}\n\t\tswitch k := privKey.(type) {\n\t\tcase *ecdsa.PrivateKey:\n\t\t\tkey = k\n\t\tcase *rsa.PrivateKey:\n\t\t\tkey = k\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"unsupported private key type %T\", key)\n\t\t}\n\t}\n\n\tm := &autocert.Manager{\n\t\tCache:  dirCache(filepath.Join(acmeDir, \"keycerts\", name)),\n\t\tPrompt: autocert.AcceptTOS,\n\t\tEmail:  contactEmail,\n\t\tClient: &acme.Client{\n\t\t\tDirectoryURL: directoryURL,\n\t\t\tKey:          key,\n\t\t\tUserAgent:    \"mox/\" + moxvar.Version,\n\t\t},\n\t\tGetPrivateKey: getPrivateKey,\n\t\t// HostPolicy set below.\n\t}\n\t// If external account binding key is provided, use it for registering a new account.\n\t// todo: ideally the key and its id are provided temporarily by the admin when registering a new account. but we don't do that interactive setup yet. in the future, an interactive setup/quickstart would ask for the key once to register a new acme account.\n\tif eabKeyID != \"\" {\n\t\tm.ExternalAccountBinding = &acme.ExternalAccountBinding{\n\t\t\tKID: eabKeyID,\n\t\t\tKey: eabKey,\n\t\t}\n\t}\n\n\tloggingGetCertificate := func(hello *tls.ClientHelloInfo) (*tls.Certificate, error) {\n\t\tlog := mlog.New(\"autotls\", nil).WithContext(hello.Context())\n\n\t\t// We handle missing invalid hostnames/ip's by returning a nil certificate and nil\n\t\t// error, which crypto/tls turns into a TLS alert \"unrecognized name\", which can be\n\t\t// interpreted by clients as a hint that they are using the wrong hostname, or a\n\t\t// certificate is missing.\n\n\t\t// Handle missing SNI to prevent logging an error below.\n\t\t// At startup, during config initialization, we already adjust the tls config to\n\t\t// inject the listener hostname if there isn't one in the TLS client hello. This is\n\t\t// common for SMTP STARTTLS connections, which often do not care about the\n\t\t// verification of the certificate.\n\t\tif hello.ServerName == \"\" {\n\t\t\tlog.Debug(\"tls request without sni servername, rejecting\", slog.Any(\"localaddr\", hello.Conn.LocalAddr()), slog.Any(\"supportedprotos\", hello.SupportedProtos))\n\t\t\treturn nil, nil\n\t\t}\n\n\t\tcert, err := m.GetCertificate(hello)\n\t\tif err != nil && errors.Is(err, errHostNotAllowed) {\n\t\t\tlog.Debugx(\"requesting certificate\", err, slog.String(\"host\", hello.ServerName))\n\t\t\treturn nil, nil\n\t\t} else if err != nil {\n\t\t\tlog.Errorx(\"requesting certificate\", err, slog.String(\"host\", hello.ServerName))\n\t\t}\n\t\treturn cert, err\n\t}\n\n\tacmeTLSConfig := *m.TLSConfig()\n\tacmeTLSConfig.GetCertificate = loggingGetCertificate\n\n\ttlsConfig := tls.Config{\n\t\tGetCertificate: loggingGetCertificate,\n\t}\n\n\ta := &Manager{\n\t\tACMETLSConfig: &acmeTLSConfig,\n\t\tTLSConfig:     &tlsConfig,\n\t\tManager:       m,\n\t\tshutdown:      shutdown,\n\t\thosts:         map[dns.Domain]struct{}{},\n\t}\n\tm.HostPolicy = a.HostPolicy\n\treturn a, nil\n}\n\n// CertAvailable checks whether a non-expired ECDSA certificate is available in the\n// cache for host. No other checks than expiration are done.\nfunc (m *Manager) CertAvailable(ctx context.Context, log mlog.Log, host dns.Domain) (bool, error) {\n\tck := host.ASCII // Would be \"+rsa\" for rsa keys.\n\tdata, err := m.Manager.Cache.Get(ctx, ck)\n\tif err != nil && errors.Is(err, autocert.ErrCacheMiss) {\n\t\treturn false, nil\n\t} else if err != nil {\n\t\treturn false, fmt.Errorf(\"attempt to get certificate from cache: %v\", err)\n\t}\n\n\t// The cached keycert is of the form: private key, leaf certificate, intermediate certificates...\n\tprivb, rem := pem.Decode(data)\n\tif privb == nil {\n\t\treturn false, fmt.Errorf(\"missing private key in cached keycert file\")\n\t}\n\tpubb, _ := pem.Decode(rem)\n\tif pubb == nil {\n\t\treturn false, fmt.Errorf(\"missing certificate in cached keycert file\")\n\t} else if pubb.Type != \"CERTIFICATE\" {\n\t\treturn false, fmt.Errorf(\"second pem block is %q, expected CERTIFICATE\", pubb.Type)\n\t}\n\tcert, err := x509.ParseCertificate(pubb.Bytes)\n\tif err != nil {\n\t\treturn false, fmt.Errorf(\"parsing certificate from cached keycert file: %v\", err)\n\t}\n\t// We assume the certificate has a matching hostname, and is properly CA-signed. We\n\t// only check the expiration time.\n\tif time.Until(cert.NotBefore) > 0 || time.Since(cert.NotAfter) > 0 {\n\t\treturn false, nil\n\t}\n\treturn true, nil\n}\n\n// SetAllowedHostnames sets a new list of allowed hostnames for automatic TLS.\n// After setting the host names, a goroutine is start to check that new host names\n// are fully served by publicIPs (only if non-empty and there is no unspecified\n// address in the list). If no, log an error with a warning that ACME validation\n// may fail.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Hostnames returns the allowed host names for use with ACME.\nfunc (m *Manager) Hostnames() []dns.Domain {\n\tm.Lock()\n\tdefer m.Unlock()\n\tvar l []dns.Domain\n\tfor h := range m.hosts {\n\t\tl = append(l, h)\n\t}\n\treturn l\n}\n\nvar errHostNotAllowed = errors.New(\"autotls: host not in allowlist\")\n\n// HostPolicy decides if a host is allowed for use with ACME, i.e. whether a\n// certificate will be returned if present and/or will be requested if not yet\n// present. Only hosts added with SetAllowedHostnames are allowed. During shutdown,\n// no new connections are allowed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype dirCache autocert.DirCache\n\nfunc (d dirCache) Delete(ctx context.Context, name string) (rerr error) {\n\tlog := mlog.New(\"autotls\", nil).WithContext(ctx)\n\tdefer func() {\n\t\tlog.Debugx(\"dircache delete result\", rerr, slog.String(\"name\", name))\n\t}()\n\terr := autocert.DirCache(d).Delete(ctx, name)\n\tif err != nil {\n\t\tlog.Errorx(\"deleting cert from dir cache\", err, slog.String(\"name\", name))\n\t} else if !strings.HasSuffix(name, \"+token\") {\n\t\tlog.Info(\"autotls cert delete\", slog.String(\"name\", name))\n\t}\n\treturn err\n}\n\nfunc (d dirCache) Get(ctx context.Context, name string) (rbuf []byte, rerr error) {\n\tlog := mlog.New(\"autotls\", nil).WithContext(ctx)\n\tdefer func() {\n\t\tlog.Debugx(\"dircache get result\", rerr, slog.String(\"name\", name))\n\t}()\n\tbuf, err := autocert.DirCache(d).Get(ctx, name)\n\tif err != nil && errors.Is(err, autocert.ErrCacheMiss) {\n\t\tlog.Infox(\"getting cert from dir cache\", err, slog.String(\"name\", name))\n\t} else if err != nil {\n\t\tlog.Errorx(\"getting cert from dir cache\", err, slog.String(\"name\", name))\n\t} else if !strings.HasSuffix(name, \"+token\") {\n\t\tlog.Debug(\"autotls cert get\", slog.String(\"name\", name))\n\t}\n\treturn buf, err\n}\n\nfunc (d dirCache) Put(ctx context.Context, name string, data []byte) (rerr error) {\n\tlog := mlog.New(\"autotls\", nil).WithContext(ctx)\n\tdefer func() {\n\t\tlog.Debugx(\"dircache put result\", rerr, slog.String(\"name\", name))\n\t}()\n\tmetricCertput.Inc()\n\terr := autocert.DirCache(d).Put(ctx, name, data)\n\tif err != nil {\n\t\tlog.Errorx(\"storing cert in dir cache\", err, slog.String(\"name\", name))\n\t} else if !strings.HasSuffix(name, \"+token\") {\n\t\tlog.Info(\"autotls cert store\", slog.String(\"name\", name))\n\t}\n\treturn err\n}\n\n```\n# Function Signature: func (m *Manager) HostPolicy(ctx context.Context, host string) (rerr error) {\n# Natural Language Annotations: // HostPolicy decides if a host is allowed for use with ACME, i.e. whether a\n// certificate will be returned if present and/or will be requested if not yet\n// present. Only hosts added with SetAllowedHostnames are allowed. During shutdown,\n// no new connections are allowed.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (m *Manager) HostPolicy(ctx context.Context, host string) (rerr error) {\n\tm.Lock()\n\tdefer m.Unlock()\n\tdom, err := dns.ParseDomain(host)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"host %s is not a valid domain name: %w\", host, err)\n\t}\n\tif _, ok := m.hosts[dom]; ok {\n\t\treturn nil\n\t}\n\treturn errHostNotAllowed\n}\n```", "```go\nfunc (m *Manager) HostPolicy(ctx context.Context, host string) (rerr error) {\n\tm.Lock()\n\tdefer m.Unlock()\n\n\t_, shutdown := <-m.shutdown\n\tif shutdown {\n\t\treturn fmt.Errorf(\"shutdown in progress, no new connections allowed\")\n\t}\n\n\tdomain := dns.Domain{ASCII: host}\n\tif _, ok := m.hosts[domain]; ok {\n\t\treturn nil\n\t}\n\n\treturn errHostNotAllowed\n}\n```", "```go\nfunc (m *Manager) HostPolicy(ctx context.Context, host string) (rerr error) {\n\t// Parse the host to a dns.Domain.\n\tdomain, err := dns.Parse(host)\n\tif err != nil {\n\t\trerr = fmt.Errorf(\"parsing host to domain: %w\", err)\n\t\treturn\n\t}\n\n\t// Check if the domain is in the allowlist.\n\tm.Lock()\n\t_, allowed := m.hosts[domain]\n\tm.Unlock()\n\n\tif !allowed {\n\t\t// If the domain is not in the allowlist, return an error.\n\t\trerr = errHostNotAllowed\n\t\treturn\n\t}\n\n\t// If the shutdown signal is received, return an error.\n\tselect {\n\tcase <-m.shutdown:\n\t\trerr = fmt.Errorf(\"shutdown signal received\")\n\tdefault:\n\t}\n\n\treturn\n}\n```"]}, "mox-imapserver/protocol.go-containsSeq": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/imapserver/protocol.go:\n```\npackage imapserver\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/mjl-/mox/store\"\n)\n\ntype numSet struct {\n\tsearchResult bool // \"$\"\n\tranges       []numRange\n}\n\ntype numRange struct {\n\tfirst setNumber\n\tlast  *setNumber // if nil, this numRange is just a setNumber in \"first\" and first.star will be false\n}\n\ntype setNumber struct {\n\tnumber uint32\n\tstar   bool // References last message (max sequence number/uid). ../rfc/9051:799\n}\n\n// containsSeq returns whether seq is in the numSet, given uids and (saved) searchResult.\n// uids and searchResult must be sorted. searchResult can have uids that are no longer in uids.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (ss numSet) containsUID(uid store.UID, uids []store.UID, searchResult []store.UID) bool {\n\tif len(uids) == 0 {\n\t\treturn false\n\t}\n\tif ss.searchResult {\n\t\treturn uidSearch(searchResult, uid) > 0 && uidSearch(uids, uid) > 0\n\t}\n\tfor _, r := range ss.ranges {\n\t\tfirst := store.UID(r.first.number)\n\t\tif r.first.star || first > uids[len(uids)-1] {\n\t\t\tfirst = uids[len(uids)-1]\n\t\t}\n\t\tlast := first\n\t\t// Num in <num>:* can be larger than last, but it still matches the last...\n\t\t// Similar for *:<num>. ../rfc/9051:4814\n\t\tif r.last != nil {\n\t\t\tlast = store.UID(r.last.number)\n\t\t\tif r.last.star || last > uids[len(uids)-1] {\n\t\t\t\tlast = uids[len(uids)-1]\n\t\t\t}\n\t\t}\n\t\tif first > last {\n\t\t\tfirst, last = last, first\n\t\t}\n\t\tif uid >= first && uid <= last && uidSearch(uids, uid) > 0 {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// contains returns whether the numset contains the number.\n// only allowed on basic, strictly increasing numsets.\nfunc (ss numSet) contains(v uint32) bool {\n\tfor _, r := range ss.ranges {\n\t\tif r.first.number == v || r.last != nil && v > r.first.number && v <= r.last.number {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc (ss numSet) empty() bool {\n\treturn !ss.searchResult && len(ss.ranges) == 0\n}\n\n// Strings returns the numset in zero or more strings of maxSize bytes. If\n// maxSize is <= 0, a single string is returned.\nfunc (ss numSet) Strings(maxSize int) []string {\n\tif ss.searchResult {\n\t\treturn []string{\"$\"}\n\t}\n\tvar l []string\n\tvar line string\n\tfor _, r := range ss.ranges {\n\t\ts := \"\"\n\t\tif r.first.star {\n\t\t\ts += \"*\"\n\t\t} else {\n\t\t\ts += fmt.Sprintf(\"%d\", r.first.number)\n\t\t}\n\t\tif r.last == nil {\n\t\t\tif r.first.star {\n\t\t\t\tpanic(\"invalid numSet range first star without last\")\n\t\t\t}\n\t\t} else {\n\t\t\ts += \":\"\n\t\t\tif r.last.star {\n\t\t\t\ts += \"*\"\n\t\t\t} else {\n\t\t\t\ts += fmt.Sprintf(\"%d\", r.last.number)\n\t\t\t}\n\t\t}\n\n\t\tnsize := len(line) + len(s)\n\t\tif line != \"\" {\n\t\t\tnsize++ // comma\n\t\t}\n\t\tif maxSize > 0 && nsize > maxSize {\n\t\t\tl = append(l, line)\n\t\t\tline = s\n\t\t\tcontinue\n\t\t}\n\t\tif line != \"\" {\n\t\t\tline += \",\"\n\t\t}\n\t\tline += s\n\t}\n\tif line != \"\" {\n\t\tl = append(l, line)\n\t}\n\treturn l\n}\n\nfunc (ss numSet) String() string {\n\tl := ss.Strings(0)\n\tif len(l) == 0 {\n\t\treturn \"\"\n\t}\n\treturn l[0]\n}\n\n// interpretStar returns a numset that interprets stars in a numset, returning a new\n// numset without stars with increasing first/last.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// whether numSet only has numbers (no star/search), and is strictly increasing.\nfunc (s *numSet) isBasicIncreasing() bool {\n\tif s.searchResult {\n\t\treturn false\n\t}\n\tvar last uint32\n\tfor _, r := range s.ranges {\n\t\tif r.first.star || r.first.number <= last || r.last != nil && (r.last.star || r.last.number < r.first.number) {\n\t\t\treturn false\n\t\t}\n\t\tlast = r.first.number\n\t\tif r.last != nil {\n\t\t\tlast = r.last.number\n\t\t}\n\t}\n\treturn true\n}\n\ntype numIter struct {\n\ts numSet\n\ti int\n\tr *rangeIter\n}\n\n// newIter must only be called on a numSet that is basic (no star/search) and ascending.\nfunc (s numSet) newIter() *numIter {\n\treturn &numIter{s: s}\n}\n\nfunc (i *numIter) Next() (uint32, bool) {\n\tif v, ok := i.r.Next(); ok {\n\t\treturn v, ok\n\t}\n\tif i.i >= len(i.s.ranges) {\n\t\treturn 0, false\n\t}\n\ti.r = i.s.ranges[i.i].newIter()\n\ti.i++\n\treturn i.r.Next()\n}\n\ntype rangeIter struct {\n\tr numRange\n\to int\n}\n\n// newIter must only be called on a range in a numSet that is basic (no star/search) and ascending.\nfunc (r numRange) newIter() *rangeIter {\n\treturn &rangeIter{r: r, o: 0}\n}\n\nfunc (r *rangeIter) Next() (uint32, bool) {\n\tif r == nil {\n\t\treturn 0, false\n\t}\n\tif r.o == 0 {\n\t\tr.o++\n\t\treturn r.r.first.number, true\n\t}\n\tif r.r.last == nil || r.r.first.number+uint32(r.o) > r.r.last.number {\n\t\treturn 0, false\n\t}\n\tv := r.r.first.number + uint32(r.o)\n\tr.o++\n\treturn v, true\n}\n\n// append adds a new number to the set, extending a range, or starting a new one (possibly the first).\n// can only be used on basic numsets, without star/searchResult.\nfunc (s *numSet) append(v uint32) {\n\tif len(s.ranges) == 0 {\n\t\ts.ranges = []numRange{{first: setNumber{number: v}}}\n\t\treturn\n\t}\n\tri := len(s.ranges) - 1\n\tr := s.ranges[ri]\n\tif v == r.first.number+1 && r.last == nil {\n\t\ts.ranges[ri].last = &setNumber{number: v}\n\t} else if r.last != nil && v == r.last.number+1 {\n\t\tr.last.number++\n\t} else {\n\t\ts.ranges = append(s.ranges, numRange{first: setNumber{number: v}})\n\t}\n}\n\ntype partial struct {\n\toffset uint32\n\tcount  uint32\n}\n\ntype sectionPart struct {\n\tpart []uint32\n\ttext *sectionText\n}\n\ntype sectionText struct {\n\tmime    bool // if \"MIME\"\n\tmsgtext *sectionMsgtext\n}\n\n// a non-nil *sectionSpec with nil msgtext & nil part means there were []'s, but nothing inside. e.g. \"BODY[]\".\ntype sectionSpec struct {\n\tmsgtext *sectionMsgtext\n\tpart    *sectionPart\n}\n\ntype sectionMsgtext struct {\n\ts       string   // \"HEADER\", \"HEADER.FIELDS\", \"HEADER.FIELDS.NOT\", \"TEXT\"\n\theaders []string // for \"HEADER.FIELDS\"*\n}\n\ntype fetchAtt struct {\n\tfield         string // uppercase, eg \"ENVELOPE\", \"BODY\". \".PEEK\" is removed.\n\tpeek          bool\n\tsection       *sectionSpec\n\tsectionBinary []uint32\n\tpartial       *partial\n}\n\ntype searchKey struct {\n\t// Only one of searchKeys, seqSet and op can be non-nil/non-empty.\n\tsearchKeys   []searchKey // In case of nested/multiple keys. Also for the top-level command.\n\tseqSet       *numSet     // In case of bare sequence set. For op UID, field uidSet contains the parameter.\n\top           string      // Determines which of the fields below are set.\n\theaderField  string\n\tastring      string\n\tdate         time.Time\n\tatom         string\n\tnumber       int64\n\tsearchKey    *searchKey\n\tsearchKey2   *searchKey\n\tuidSet       numSet\n\tclientModseq *int64\n}\n\nfunc compactUIDSet(l []store.UID) (r numSet) {\n\tfor len(l) > 0 {\n\t\te := 1\n\t\tfor ; e < len(l) && l[e] == l[e-1]+1; e++ {\n\t\t}\n\t\tfirst := setNumber{number: uint32(l[0])}\n\t\tvar last *setNumber\n\t\tif e > 1 {\n\t\t\tlast = &setNumber{number: uint32(l[e-1])}\n\t\t}\n\t\tr.ranges = append(r.ranges, numRange{first, last})\n\t\tl = l[e:]\n\t}\n\treturn\n}\n\n```\n# Function Signature: func (ss numSet) containsSeq(seq msgseq, uids []store.UID, searchResult []store.UID) bool {\n# Natural Language Annotations: // containsSeq returns whether seq is in the numSet, given uids and (saved) searchResult.\n// uids and searchResult must be sorted. searchResult can have uids that are no longer in uids.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (ss numSet) containsSeq(seq msgseq, uids []store.UID, searchResult []store.UID) bool {\n\tif len(uids) == 0 {\n\t\treturn false\n\t}\n\tif ss.searchResult {\n\t\treturn uidSearch(searchResult, seq.UID) > 0 && uidSearch(uids, seq.UID) > 0\n\t}\n\tfor _, r := range ss.ranges {\n\t\tfirst := store.UID(r.first.number)\n\t\tif r.first.star || first > uids[len(uids)-1] {\n\t\t\tfirst = uids[len(uids)-1]\n\t\t}\n\t\tlast := first\n\t\t// Num in <num>:* can be larger than last, but it still matches the last...\n\t\t// Similar for *:<num>. ../rfc/9051:4814\n\t\tif r.last != nil {\n\t\t\tlast = store.UID(r.last.number)\n\t\t\tif r.last.star || last > uids[len(uids)-1] {\n\t\t\t\tlast = uids[len(uids)-1]\n\t\t\t}\n\t\t}\n\t\tif first > last {\n\t\t\tfirst, last = last, first\n\t\t}\n\t\tif seq.UID >= first && seq.UID <= last && uidSearch(uids, seq.UID) > 0 {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n```", "```go\nfunc (ss numSet) containsSeq(seq msgseq, uids []store.UID, searchResult []store.UID) bool {\n\tif len(uids) == 0 {\n\t\treturn false\n\t}\n\tif ss.searchResult {\n\t\treturn uidSearch(searchResult, store.UID(seq)) > 0 && uidSearch(uids, store.UID(seq)) > 0\n\t}\n\tfor _, r := range ss.ranges {\n\t\tfirst := store.UID(r.first.number)\n\t\tif r.first.star || first > uids[len(uids)-1] {\n\t\t\tfirst = uids[len(uids)-1]\n\t\t}\n\t\tlast := first\n\t\t// Num in <num>:* can be larger than last, but it still matches the last...\n\t\t// Similar for *:<num>. ../rfc/9051:4814\n\t\tif r.last != nil {\n\t\t\tlast = store.UID(r.last.number)\n\t\t\tif r.last.star || last > uids[len(uids)-1] {\n\t\t\t\tlast = uids[len(uids)-1]\n\t\t\t}\n\t\t}\n\t\tif first > last {\n\t\t\tfirst, last = last, first\n\t\t}\n\t\tif store.UID(seq) >= first && store.UID(seq) <= last && uidSearch(uids, store.UID(seq)) > 0 {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n```", "```go\nfunc (ss numSet) containsSeq(seq msgseq, uids []store.UID, searchResult []store.UID) bool {\n\tif len(uids) == 0 {\n\t\treturn false\n\t}\n\tif ss.searchResult {\n\t\treturn uidSearch(searchResult, seq.UID) > 0 && uidSearch(uids, seq.UID) > 0\n\t}\n\tfor _, r := range ss.ranges {\n\t\tfirst := store.UID(r.first.number)\n\t\tif r.first.star || first > uids[len(uids)-1] {\n\t\t\tfirst = uids[len(uids)-1]\n\t\t}\n\t\tlast := first\n\t\t// Seq in <num>:* can be larger than last, but it still matches the last...\n\t\t// Similar for *:<num>. ../rfc/9051:4814\n\t\tif r.last != nil {\n\t\t\tlast = store.UID(r.last.number)\n\t\t\tif r.last.star || last > uids[len(uids)-1] {\n\t\t\t\tlast = uids[len(uids)-1]\n\t\t\t}\n\t\t}\n\t\tif first > last {\n\t\t\tfirst, last = last, first\n\t\t}\n\t\tif seq.UID >= first && seq.UID <= last && uidSearch(uids, seq.UID) > 0 {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n```"]}, "mox-imapserver/protocol.go-interpretStar": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/imapserver/protocol.go:\n```\npackage imapserver\n\nimport (\n\t\"fmt\"\n\t\"time\"\n\n\t\"github.com/mjl-/mox/store\"\n)\n\ntype numSet struct {\n\tsearchResult bool // \"$\"\n\tranges       []numRange\n}\n\ntype numRange struct {\n\tfirst setNumber\n\tlast  *setNumber // if nil, this numRange is just a setNumber in \"first\" and first.star will be false\n}\n\ntype setNumber struct {\n\tnumber uint32\n\tstar   bool // References last message (max sequence number/uid). ../rfc/9051:799\n}\n\n// containsSeq returns whether seq is in the numSet, given uids and (saved) searchResult.\n// uids and searchResult must be sorted. searchResult can have uids that are no longer in uids.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc (ss numSet) containsUID(uid store.UID, uids []store.UID, searchResult []store.UID) bool {\n\tif len(uids) == 0 {\n\t\treturn false\n\t}\n\tif ss.searchResult {\n\t\treturn uidSearch(searchResult, uid) > 0 && uidSearch(uids, uid) > 0\n\t}\n\tfor _, r := range ss.ranges {\n\t\tfirst := store.UID(r.first.number)\n\t\tif r.first.star || first > uids[len(uids)-1] {\n\t\t\tfirst = uids[len(uids)-1]\n\t\t}\n\t\tlast := first\n\t\t// Num in <num>:* can be larger than last, but it still matches the last...\n\t\t// Similar for *:<num>. ../rfc/9051:4814\n\t\tif r.last != nil {\n\t\t\tlast = store.UID(r.last.number)\n\t\t\tif r.last.star || last > uids[len(uids)-1] {\n\t\t\t\tlast = uids[len(uids)-1]\n\t\t\t}\n\t\t}\n\t\tif first > last {\n\t\t\tfirst, last = last, first\n\t\t}\n\t\tif uid >= first && uid <= last && uidSearch(uids, uid) > 0 {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// contains returns whether the numset contains the number.\n// only allowed on basic, strictly increasing numsets.\nfunc (ss numSet) contains(v uint32) bool {\n\tfor _, r := range ss.ranges {\n\t\tif r.first.number == v || r.last != nil && v > r.first.number && v <= r.last.number {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc (ss numSet) empty() bool {\n\treturn !ss.searchResult && len(ss.ranges) == 0\n}\n\n// Strings returns the numset in zero or more strings of maxSize bytes. If\n// maxSize is <= 0, a single string is returned.\nfunc (ss numSet) Strings(maxSize int) []string {\n\tif ss.searchResult {\n\t\treturn []string{\"$\"}\n\t}\n\tvar l []string\n\tvar line string\n\tfor _, r := range ss.ranges {\n\t\ts := \"\"\n\t\tif r.first.star {\n\t\t\ts += \"*\"\n\t\t} else {\n\t\t\ts += fmt.Sprintf(\"%d\", r.first.number)\n\t\t}\n\t\tif r.last == nil {\n\t\t\tif r.first.star {\n\t\t\t\tpanic(\"invalid numSet range first star without last\")\n\t\t\t}\n\t\t} else {\n\t\t\ts += \":\"\n\t\t\tif r.last.star {\n\t\t\t\ts += \"*\"\n\t\t\t} else {\n\t\t\t\ts += fmt.Sprintf(\"%d\", r.last.number)\n\t\t\t}\n\t\t}\n\n\t\tnsize := len(line) + len(s)\n\t\tif line != \"\" {\n\t\t\tnsize++ // comma\n\t\t}\n\t\tif maxSize > 0 && nsize > maxSize {\n\t\t\tl = append(l, line)\n\t\t\tline = s\n\t\t\tcontinue\n\t\t}\n\t\tif line != \"\" {\n\t\t\tline += \",\"\n\t\t}\n\t\tline += s\n\t}\n\tif line != \"\" {\n\t\tl = append(l, line)\n\t}\n\treturn l\n}\n\nfunc (ss numSet) String() string {\n\tl := ss.Strings(0)\n\tif len(l) == 0 {\n\t\treturn \"\"\n\t}\n\treturn l[0]\n}\n\n// interpretStar returns a numset that interprets stars in a numset, returning a new\n// numset without stars with increasing first/last.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// whether numSet only has numbers (no star/search), and is strictly increasing.\nfunc (s *numSet) isBasicIncreasing() bool {\n\tif s.searchResult {\n\t\treturn false\n\t}\n\tvar last uint32\n\tfor _, r := range s.ranges {\n\t\tif r.first.star || r.first.number <= last || r.last != nil && (r.last.star || r.last.number < r.first.number) {\n\t\t\treturn false\n\t\t}\n\t\tlast = r.first.number\n\t\tif r.last != nil {\n\t\t\tlast = r.last.number\n\t\t}\n\t}\n\treturn true\n}\n\ntype numIter struct {\n\ts numSet\n\ti int\n\tr *rangeIter\n}\n\n// newIter must only be called on a numSet that is basic (no star/search) and ascending.\nfunc (s numSet) newIter() *numIter {\n\treturn &numIter{s: s}\n}\n\nfunc (i *numIter) Next() (uint32, bool) {\n\tif v, ok := i.r.Next(); ok {\n\t\treturn v, ok\n\t}\n\tif i.i >= len(i.s.ranges) {\n\t\treturn 0, false\n\t}\n\ti.r = i.s.ranges[i.i].newIter()\n\ti.i++\n\treturn i.r.Next()\n}\n\ntype rangeIter struct {\n\tr numRange\n\to int\n}\n\n// newIter must only be called on a range in a numSet that is basic (no star/search) and ascending.\nfunc (r numRange) newIter() *rangeIter {\n\treturn &rangeIter{r: r, o: 0}\n}\n\nfunc (r *rangeIter) Next() (uint32, bool) {\n\tif r == nil {\n\t\treturn 0, false\n\t}\n\tif r.o == 0 {\n\t\tr.o++\n\t\treturn r.r.first.number, true\n\t}\n\tif r.r.last == nil || r.r.first.number+uint32(r.o) > r.r.last.number {\n\t\treturn 0, false\n\t}\n\tv := r.r.first.number + uint32(r.o)\n\tr.o++\n\treturn v, true\n}\n\n// append adds a new number to the set, extending a range, or starting a new one (possibly the first).\n// can only be used on basic numsets, without star/searchResult.\nfunc (s *numSet) append(v uint32) {\n\tif len(s.ranges) == 0 {\n\t\ts.ranges = []numRange{{first: setNumber{number: v}}}\n\t\treturn\n\t}\n\tri := len(s.ranges) - 1\n\tr := s.ranges[ri]\n\tif v == r.first.number+1 && r.last == nil {\n\t\ts.ranges[ri].last = &setNumber{number: v}\n\t} else if r.last != nil && v == r.last.number+1 {\n\t\tr.last.number++\n\t} else {\n\t\ts.ranges = append(s.ranges, numRange{first: setNumber{number: v}})\n\t}\n}\n\ntype partial struct {\n\toffset uint32\n\tcount  uint32\n}\n\ntype sectionPart struct {\n\tpart []uint32\n\ttext *sectionText\n}\n\ntype sectionText struct {\n\tmime    bool // if \"MIME\"\n\tmsgtext *sectionMsgtext\n}\n\n// a non-nil *sectionSpec with nil msgtext & nil part means there were []'s, but nothing inside. e.g. \"BODY[]\".\ntype sectionSpec struct {\n\tmsgtext *sectionMsgtext\n\tpart    *sectionPart\n}\n\ntype sectionMsgtext struct {\n\ts       string   // \"HEADER\", \"HEADER.FIELDS\", \"HEADER.FIELDS.NOT\", \"TEXT\"\n\theaders []string // for \"HEADER.FIELDS\"*\n}\n\ntype fetchAtt struct {\n\tfield         string // uppercase, eg \"ENVELOPE\", \"BODY\". \".PEEK\" is removed.\n\tpeek          bool\n\tsection       *sectionSpec\n\tsectionBinary []uint32\n\tpartial       *partial\n}\n\ntype searchKey struct {\n\t// Only one of searchKeys, seqSet and op can be non-nil/non-empty.\n\tsearchKeys   []searchKey // In case of nested/multiple keys. Also for the top-level command.\n\tseqSet       *numSet     // In case of bare sequence set. For op UID, field uidSet contains the parameter.\n\top           string      // Determines which of the fields below are set.\n\theaderField  string\n\tastring      string\n\tdate         time.Time\n\tatom         string\n\tnumber       int64\n\tsearchKey    *searchKey\n\tsearchKey2   *searchKey\n\tuidSet       numSet\n\tclientModseq *int64\n}\n\nfunc compactUIDSet(l []store.UID) (r numSet) {\n\tfor len(l) > 0 {\n\t\te := 1\n\t\tfor ; e < len(l) && l[e] == l[e-1]+1; e++ {\n\t\t}\n\t\tfirst := setNumber{number: uint32(l[0])}\n\t\tvar last *setNumber\n\t\tif e > 1 {\n\t\t\tlast = &setNumber{number: uint32(l[e-1])}\n\t\t}\n\t\tr.ranges = append(r.ranges, numRange{first, last})\n\t\tl = l[e:]\n\t}\n\treturn\n}\n\n```\n# Function Signature: func (s numSet) interpretStar(uids []store.UID) numSet {\n# Natural Language Annotations: // interpretStar returns a numset that interprets stars in a numset, returning a new\n// numset without stars with increasing first/last.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (s numSet) interpretStar(uids []store.UID) numSet {\n\tnewSet := numSet{}\n\tfor _, r := range s.ranges {\n\t\tif !r.first.star && r.last == nil {\n\t\t\tnewSet.ranges = append(newSet.ranges, r)\n\t\t} else {\n\t\t\tfirst := uint32(1)\n\t\t\tif !r.first.star {\n\t\t\t\tfirst = r.first.number\n\t\t\t}\n\t\t\tlast := uids[len(uids)-1]\n\t\t\tif r.last != nil && !r.last.star {\n\t\t\t\tlast = r.last.number\n\t\t\t}\n\t\t\tfor i := first; i <= last; i++ {\n\t\t\t\tnewSet.append(i)\n\t\t\t}\n\t\t}\n\t}\n\treturn newSet\n}\n```", "```go\nfunc (s numSet) interpretStar(uids []store.UID) numSet {\n\tvar result numSet\n\tlastUID := uids[len(uids)-1]\n\tfor _, r := range s.ranges {\n\t\tif r.first.star {\n\t\t\tr.first.number = uids[0]\n\t\t}\n\t\tif r.last != nil && r.last.star {\n\t\t\tr.last.number = lastUID\n\t\t}\n\t\tresult.ranges = append(result.ranges, r)\n\t}\n\treturn result\n}\n```", "```go\nfunc (s numSet) interpretStar(uids []store.UID) numSet {\n\tnewSet := numSet{}\n\tfor _, r := range s.ranges {\n\t\tif r.first.star {\n\t\t\tr.first.number = uids[0]\n\t\t}\n\t\tif r.last != nil && r.last.star {\n\t\t\tr.last.number = uids[len(uids)-1]\n\t\t}\n\t\tnewSet.ranges = append(newSet.ranges, r)\n\t}\n\treturn newSet\n}\n```"]}, "mox-http/web.go-ServeHTTP": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/http/web.go:\n```\n// Package http provides HTTP listeners/servers, for\n// autoconfiguration/autodiscovery, the account and admin web interface and\n// MTA-STS policies.\npackage http\n\nimport (\n\t\"compress/gzip\"\n\t\"context\"\n\t\"crypto/tls\"\n\t\"fmt\"\n\t\"io\"\n\tgolog \"log\"\n\t\"log/slog\"\n\t\"net\"\n\t\"net/http\"\n\t\"os\"\n\t\"path\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t_ \"embed\"\n\t_ \"net/http/pprof\"\n\n\t\"golang.org/x/exp/maps\"\n\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n\n\t\"github.com/mjl-/mox/autotls\"\n\t\"github.com/mjl-/mox/config\"\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/mlog\"\n\t\"github.com/mjl-/mox/mox-\"\n\t\"github.com/mjl-/mox/ratelimit\"\n\t\"github.com/mjl-/mox/webaccount\"\n\t\"github.com/mjl-/mox/webadmin\"\n\t\"github.com/mjl-/mox/webapisrv\"\n\t\"github.com/mjl-/mox/webmail\"\n)\n\nvar pkglog = mlog.New(\"http\", nil)\n\nvar (\n\t// metricRequest tracks performance (time to write response header) of server.\n\tmetricRequest = promauto.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"mox_httpserver_request_duration_seconds\",\n\t\t\tHelp:    \"HTTP(s) server request with handler name, protocol, method, result codes, and duration until response status code is written, in seconds.\",\n\t\t\tBuckets: []float64{0.001, 0.005, 0.01, 0.05, 0.100, 0.5, 1, 5, 10, 20, 30, 60, 120},\n\t\t},\n\t\t[]string{\n\t\t\t\"handler\", // Name from webhandler, can be empty.\n\t\t\t\"proto\",   // \"http\", \"https\", \"ws\", \"wss\"\n\t\t\t\"method\",  // \"(unknown)\" and otherwise only common verbs\n\t\t\t\"code\",\n\t\t},\n\t)\n\t// metricResponse tracks performance of entire request as experienced by users,\n\t// which also depends on their connection speed, so not necessarily something you\n\t// could act on.\n\tmetricResponse = promauto.NewHistogramVec(\n\t\tprometheus.HistogramOpts{\n\t\t\tName:    \"mox_httpserver_response_duration_seconds\",\n\t\t\tHelp:    \"HTTP(s) server response with handler name, protocol, method, result codes, and duration of entire response, in seconds.\",\n\t\t\tBuckets: []float64{0.001, 0.005, 0.01, 0.05, 0.100, 0.5, 1, 5, 10, 20, 30, 60, 120},\n\t\t},\n\t\t[]string{\n\t\t\t\"handler\", // Name from webhandler, can be empty.\n\t\t\t\"proto\",   // \"http\", \"https\", \"ws\", \"wss\"\n\t\t\t\"method\",  // \"(unknown)\" and otherwise only common verbs\n\t\t\t\"code\",\n\t\t},\n\t)\n)\n\n// We serve a favicon when webaccount/webmail/webadmin/webapi for account-related\n// domains. They are configured as \"service handler\", which have a lower priority\n// than web handler. Admins can configure a custom /favicon.ico route to override\n// the builtin favicon. In the future, we may want to make it easier to customize\n// the favicon, possibly per client settings domain.\n//\n//go:embed favicon.ico\nvar faviconIco string\nvar faviconModTime = time.Now()\n\nfunc init() {\n\tp, err := os.Executable()\n\tif err == nil {\n\t\tif st, err := os.Stat(p); err == nil {\n\t\t\tfaviconModTime = st.ModTime()\n\t\t}\n\t}\n}\n\nfunc faviconHandle(w http.ResponseWriter, r *http.Request) {\n\thttp.ServeContent(w, r, \"favicon.ico\", faviconModTime, strings.NewReader(faviconIco))\n}\n\ntype responseWriterFlusher interface {\n\thttp.ResponseWriter\n\thttp.Flusher\n}\n\n// http.ResponseWriter that writes access log and tracks metrics at end of response.\ntype loggingWriter struct {\n\tW                responseWriterFlusher // Calls are forwarded.\n\tStart            time.Time\n\tR                *http.Request\n\tWebsocketRequest bool // Whether request from was websocket.\n\n\t// Set by router.\n\tHandler  string\n\tCompress bool\n\n\t// Set by handlers.\n\tStatusCode                   int\n\tSize                         int64        // Of data served to client, for non-websocket responses.\n\tUncompressedSize             int64        // Can be set by a handler that already serves compressed data, and we update it while compressing.\n\tGzip                         *gzip.Writer // Only set if we transparently compress within loggingWriter (static handlers handle compression themselves, with a cache).\n\tErr                          error\n\tWebsocketResponse            bool        // If this was a successful websocket connection with backend.\n\tSizeFromClient, SizeToClient int64       // Websocket data.\n\tAttrs                        []slog.Attr // Additional fields to log.\n}\n\nfunc (w *loggingWriter) AddAttr(a slog.Attr) {\n\tw.Attrs = append(w.Attrs, a)\n}\n\nfunc (w *loggingWriter) Flush() {\n\tw.W.Flush()\n}\n\nfunc (w *loggingWriter) Header() http.Header {\n\treturn w.W.Header()\n}\n\n// protocol, for logging.\nfunc (w *loggingWriter) proto(websocket bool) string {\n\tproto := \"http\"\n\tif websocket {\n\t\tproto = \"ws\"\n\t}\n\tif w.R.TLS != nil {\n\t\tproto += \"s\"\n\t}\n\treturn proto\n}\n\nfunc (w *loggingWriter) Write(buf []byte) (int, error) {\n\tif w.StatusCode == 0 {\n\t\tw.WriteHeader(http.StatusOK)\n\t}\n\n\tvar n int\n\tvar err error\n\tif w.Gzip == nil {\n\t\tn, err = w.W.Write(buf)\n\t\tif n > 0 {\n\t\t\tw.Size += int64(n)\n\t\t}\n\t} else {\n\t\t// We flush after each write. Probably takes a few more bytes, but prevents any\n\t\t// issues due to buffering.\n\t\t// w.Gzip.Write updates w.Size with the compressed byte count.\n\t\tn, err = w.Gzip.Write(buf)\n\t\tif err == nil {\n\t\t\terr = w.Gzip.Flush()\n\t\t}\n\t\tif n > 0 {\n\t\t\tw.UncompressedSize += int64(n)\n\t\t}\n\t}\n\tif err != nil {\n\t\tw.error(err)\n\t}\n\treturn n, err\n}\n\nfunc (w *loggingWriter) setStatusCode(statusCode int) {\n\tif w.StatusCode != 0 {\n\t\treturn\n\t}\n\n\tw.StatusCode = statusCode\n\tmethod := metricHTTPMethod(w.R.Method)\n\tmetricRequest.WithLabelValues(w.Handler, w.proto(w.WebsocketRequest), method, fmt.Sprintf(\"%d\", w.StatusCode)).Observe(float64(time.Since(w.Start)) / float64(time.Second))\n}\n\n// SetUncompressedSize is used through an interface by\n// ../webmail/webmail.go:/WriteHeader, preventing an import cycle.\nfunc (w *loggingWriter) SetUncompressedSize(origSize int64) {\n\tw.UncompressedSize = origSize\n}\n\nfunc (w *loggingWriter) WriteHeader(statusCode int) {\n\tif w.StatusCode != 0 {\n\t\treturn\n\t}\n\n\tw.setStatusCode(statusCode)\n\n\t// We transparently gzip-compress responses for requests under these conditions, all must apply:\n\t//\n\t// - Enabled for handler (static handlers make their own decisions).\n\t// - Not a websocket request.\n\t// - Regular success responses (not errors, or partial content or redirects or \"not modified\", etc).\n\t// - Not already compressed, or any other Content-Encoding header (including \"identity\").\n\t// - Client accepts gzip encoded responses.\n\t// - The response has a content-type that is compressible (text/*, */*+{json,xml}, and a few common files (e.g. json, xml, javascript).\n\tif w.Compress && !w.WebsocketRequest && statusCode == http.StatusOK && w.W.Header().Values(\"Content-Encoding\") == nil && acceptsGzip(w.R) && compressibleContentType(w.W.Header().Get(\"Content-Type\")) {\n\t\t// todo: we should gather the first kb of data, see if it is compressible. if not, just return original. should set timer so we flush if it takes too long to gather 1kb. for smaller data we shouldn't compress at all.\n\n\t\t// We track the gzipped output for the access log.\n\t\tcw := countWriter{Writer: w.W, Size: &w.Size}\n\t\tw.Gzip, _ = gzip.NewWriterLevel(cw, gzip.BestSpeed)\n\t\tw.W.Header().Set(\"Content-Encoding\", \"gzip\")\n\t\tw.W.Header().Del(\"Content-Length\") // No longer valid, set again for small responses by net/http.\n\t}\n\tw.W.WriteHeader(statusCode)\n}\n\nfunc acceptsGzip(r *http.Request) bool {\n\ts := r.Header.Get(\"Accept-Encoding\")\n\tt := strings.Split(s, \",\")\n\tfor _, e := range t {\n\t\te = strings.TrimSpace(e)\n\t\ttt := strings.Split(e, \";\")\n\t\tif len(tt) > 1 && t[1] == \"q=0\" {\n\t\t\tcontinue\n\t\t}\n\t\tif tt[0] == \"gzip\" {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nvar compressibleTypes = map[string]bool{\n\t\"application/csv\":          true,\n\t\"application/javascript\":   true,\n\t\"application/json\":         true,\n\t\"application/x-javascript\": true,\n\t\"application/xml\":          true,\n\t\"image/vnd.microsoft.icon\": true,\n\t\"image/x-icon\":             true,\n\t\"font/ttf\":                 true,\n\t\"font/eot\":                 true,\n\t\"font/otf\":                 true,\n\t\"font/opentype\":            true,\n}\n\nfunc compressibleContentType(ct string) bool {\n\tct = strings.SplitN(ct, \";\", 2)[0]\n\tct = strings.TrimSpace(ct)\n\tct = strings.ToLower(ct)\n\tif compressibleTypes[ct] {\n\t\treturn true\n\t}\n\tt, st, _ := strings.Cut(ct, \"/\")\n\treturn t == \"text\" || strings.HasSuffix(st, \"+json\") || strings.HasSuffix(st, \"+xml\")\n}\n\nfunc compressibleContent(f *os.File) bool {\n\t// We don't want to store many small files. They take up too much disk overhead.\n\tif fi, err := f.Stat(); err != nil || fi.Size() < 1024 || fi.Size() > 10*1024*1024 {\n\t\treturn false\n\t}\n\n\tbuf := make([]byte, 512)\n\tn, err := f.ReadAt(buf, 0)\n\tif err != nil && err != io.EOF {\n\t\treturn false\n\t}\n\tct := http.DetectContentType(buf[:n])\n\treturn compressibleContentType(ct)\n}\n\ntype countWriter struct {\n\tWriter io.Writer\n\tSize   *int64\n}\n\nfunc (w countWriter) Write(buf []byte) (int, error) {\n\tn, err := w.Writer.Write(buf)\n\tif n > 0 {\n\t\t*w.Size += int64(n)\n\t}\n\treturn n, err\n}\n\nvar tlsVersions = map[uint16]string{\n\ttls.VersionTLS10: \"tls1.0\",\n\ttls.VersionTLS11: \"tls1.1\",\n\ttls.VersionTLS12: \"tls1.2\",\n\ttls.VersionTLS13: \"tls1.3\",\n}\n\nfunc metricHTTPMethod(method string) string {\n\t// https://www.iana.org/assignments/http-methods/http-methods.xhtml\n\tmethod = strings.ToLower(method)\n\tswitch method {\n\tcase \"acl\", \"baseline-control\", \"bind\", \"checkin\", \"checkout\", \"connect\", \"copy\", \"delete\", \"get\", \"head\", \"label\", \"link\", \"lock\", \"merge\", \"mkactivity\", \"mkcalendar\", \"mkcol\", \"mkredirectref\", \"mkworkspace\", \"move\", \"options\", \"orderpatch\", \"patch\", \"post\", \"pri\", \"propfind\", \"proppatch\", \"put\", \"rebind\", \"report\", \"search\", \"trace\", \"unbind\", \"uncheckout\", \"unlink\", \"unlock\", \"update\", \"updateredirectref\", \"version-control\":\n\t\treturn method\n\t}\n\treturn \"(other)\"\n}\n\nfunc (w *loggingWriter) error(err error) {\n\tif w.Err == nil {\n\t\tw.Err = err\n\t}\n}\n\nfunc (w *loggingWriter) Done() {\n\tif w.Err == nil && w.Gzip != nil {\n\t\tif err := w.Gzip.Close(); err != nil {\n\t\t\tw.error(err)\n\t\t}\n\t}\n\n\tmethod := metricHTTPMethod(w.R.Method)\n\tmetricResponse.WithLabelValues(w.Handler, w.proto(w.WebsocketResponse), method, fmt.Sprintf(\"%d\", w.StatusCode)).Observe(float64(time.Since(w.Start)) / float64(time.Second))\n\n\ttlsinfo := \"plain\"\n\tif w.R.TLS != nil {\n\t\tif v, ok := tlsVersions[w.R.TLS.Version]; ok {\n\t\t\ttlsinfo = v\n\t\t} else {\n\t\t\ttlsinfo = \"(other)\"\n\t\t}\n\t}\n\terr := w.Err\n\tif err == nil {\n\t\terr = w.R.Context().Err()\n\t}\n\tattrs := []slog.Attr{\n\t\tslog.String(\"httpaccess\", \"\"),\n\t\tslog.String(\"handler\", w.Handler),\n\t\tslog.String(\"method\", method),\n\t\tslog.Any(\"url\", w.R.URL),\n\t\tslog.String(\"host\", w.R.Host),\n\t\tslog.Duration(\"duration\", time.Since(w.Start)),\n\t\tslog.Int(\"statuscode\", w.StatusCode),\n\t\tslog.String(\"proto\", strings.ToLower(w.R.Proto)),\n\t\tslog.Any(\"remoteaddr\", w.R.RemoteAddr),\n\t\tslog.String(\"tlsinfo\", tlsinfo),\n\t\tslog.String(\"useragent\", w.R.Header.Get(\"User-Agent\")),\n\t\tslog.String(\"referrr\", w.R.Header.Get(\"Referrer\")),\n\t}\n\tif w.WebsocketRequest {\n\t\tattrs = append(attrs,\n\t\t\tslog.Bool(\"websocketrequest\", true),\n\t\t)\n\t}\n\tif w.WebsocketResponse {\n\t\tattrs = append(attrs,\n\t\t\tslog.Bool(\"websocket\", true),\n\t\t\tslog.Int64(\"sizetoclient\", w.SizeToClient),\n\t\t\tslog.Int64(\"sizefromclient\", w.SizeFromClient),\n\t\t)\n\t} else if w.UncompressedSize > 0 {\n\t\tattrs = append(attrs,\n\t\t\tslog.Int64(\"size\", w.Size),\n\t\t\tslog.Int64(\"uncompressedsize\", w.UncompressedSize),\n\t\t)\n\t} else {\n\t\tattrs = append(attrs,\n\t\t\tslog.Int64(\"size\", w.Size),\n\t\t)\n\t}\n\tattrs = append(attrs, w.Attrs...)\n\tpkglog.WithContext(w.R.Context()).Debugx(\"http request\", err, attrs...)\n}\n\n// Built-in handlers, e.g. mta-sts and autoconfig.\ntype pathHandler struct {\n\tName      string                       // For logging/metrics.\n\tHostMatch func(host dns.IPDomain) bool // If not nil, called to see if domain of requests matches. Host can be zero value for invalid domain/ip.\n\tPath      string                       // Path to register, like on http.ServeMux.\n\tHandler   http.Handler\n}\ntype serve struct {\n\tKinds     []string // Type of handler and protocol (e.g. acme-tls-alpn-01, account-http, admin-https).\n\tTLSConfig *tls.Config\n\tFavicon   bool\n\n\t// SystemHandlers are for MTA-STS, autoconfig, ACME validation. They can't be\n\t// overridden by WebHandlers. WebHandlers are evaluated next, and the internal\n\t// service handlers from Listeners in mox.conf (for admin, account, webmail, webapi\n\t// interfaces) last. WebHandlers can also pass requests to the internal servers.\n\t// This order allows admins to serve other content on domains serving the mox.conf\n\t// internal services.\n\tSystemHandlers  []pathHandler // Sorted, longest first.\n\tWebserver       bool\n\tServiceHandlers []pathHandler // Sorted, longest first.\n}\n\n// SystemHandle registers a named system handler for a path and optional host. If\n// path ends with a slash, it is used as prefix match, otherwise a full path match\n// is required. If hostOpt is set, only requests to those host are handled by this\n// handler.\nfunc (s *serve) SystemHandle(name string, hostMatch func(dns.IPDomain) bool, path string, fn http.Handler) {\n\ts.SystemHandlers = append(s.SystemHandlers, pathHandler{name, hostMatch, path, fn})\n}\n\n// Like SystemHandle, but for internal services \"admin\", \"account\", \"webmail\",\n// \"webapi\" configured in the mox.conf Listener.\nfunc (s *serve) ServiceHandle(name string, hostMatch func(dns.IPDomain) bool, path string, fn http.Handler) {\n\ts.ServiceHandlers = append(s.ServiceHandlers, pathHandler{name, hostMatch, path, fn})\n}\n\nvar (\n\tlimiterConnectionrate = &ratelimit.Limiter{\n\t\tWindowLimits: []ratelimit.WindowLimit{\n\t\t\t{\n\t\t\t\tWindow: time.Minute,\n\t\t\t\tLimits: [...]int64{1000, 3000, 9000},\n\t\t\t},\n\t\t\t{\n\t\t\t\tWindow: time.Hour,\n\t\t\t\tLimits: [...]int64{5000, 15000, 45000},\n\t\t\t},\n\t\t},\n\t}\n)\n\n// ServeHTTP is the starting point for serving HTTP requests. It dispatches to the\n// right pathHandler or WebHandler, and it generates access logs and tracks\n// metrics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc redirectToTrailingSlash(srv *serve, hostMatch func(dns.IPDomain) bool, name, path string) {\n\t// Helpfully redirect user to version with ending slash.\n\tif path != \"/\" && strings.HasSuffix(path, \"/\") {\n\t\thandler := mox.SafeHeaders(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\t\thttp.Redirect(w, r, path, http.StatusSeeOther)\n\t\t}))\n\t\tsrv.ServiceHandle(name, hostMatch, path[:len(path)-1], handler)\n\t}\n}\n\n// Listen binds to sockets for HTTP listeners, including those required for ACME to\n// generate TLS certificates. It stores the listeners so Serve can start serving them.\nfunc Listen() {\n\t// Initialize listeners in deterministic order for the same potential error\n\t// messages.\n\tnames := maps.Keys(mox.Conf.Static.Listeners)\n\tsort.Strings(names)\n\tfor _, name := range names {\n\t\tl := mox.Conf.Static.Listeners[name]\n\t\tportServe := portServes(l)\n\n\t\tports := maps.Keys(portServe)\n\t\tsort.Ints(ports)\n\t\tfor _, port := range ports {\n\t\t\tsrv := portServe[port]\n\t\t\tfor _, ip := range l.IPs {\n\t\t\t\tlisten1(ip, port, srv.TLSConfig, name, srv.Kinds, srv)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc portServes(l config.Listener) map[int]*serve {\n\tportServe := map[int]*serve{}\n\n\t// For system/services, we serve on host localhost too, for ssh tunnel scenario's.\n\tlocalhost := dns.Domain{ASCII: \"localhost\"}\n\n\tldom := l.HostnameDomain\n\tif l.Hostname == \"\" {\n\t\tldom = mox.Conf.Static.HostnameDomain\n\t}\n\tlistenerHostMatch := func(host dns.IPDomain) bool {\n\t\tif host.IsIP() {\n\t\t\treturn true\n\t\t}\n\t\treturn host.Domain == ldom || host.Domain == localhost\n\t}\n\taccountHostMatch := func(host dns.IPDomain) bool {\n\t\tif listenerHostMatch(host) {\n\t\t\treturn true\n\t\t}\n\t\treturn mox.Conf.IsClientSettingsDomain(host.Domain)\n\t}\n\n\tvar ensureServe func(https bool, port int, kind string, favicon bool) *serve\n\tensureServe = func(https bool, port int, kind string, favicon bool) *serve {\n\t\ts := portServe[port]\n\t\tif s == nil {\n\t\t\ts = &serve{nil, nil, false, nil, false, nil}\n\t\t\tportServe[port] = s\n\t\t}\n\t\ts.Kinds = append(s.Kinds, kind)\n\t\tif favicon && !s.Favicon {\n\t\t\ts.ServiceHandle(\"favicon\", accountHostMatch, \"/favicon.ico\", mox.SafeHeaders(http.HandlerFunc(faviconHandle)))\n\t\t\ts.Favicon = true\n\t\t}\n\n\t\tif https && l.TLS.ACME != \"\" {\n\t\t\ts.TLSConfig = l.TLS.ACMEConfig\n\t\t} else if https {\n\t\t\ts.TLSConfig = l.TLS.Config\n\t\t\tif l.TLS.ACME != \"\" {\n\t\t\t\ttlsport := config.Port(mox.Conf.Static.ACME[l.TLS.ACME].Port, 443)\n\t\t\t\tensureServe(true, tlsport, \"acme-tls-alpn-01\", false)\n\t\t\t}\n\t\t}\n\t\treturn s\n\t}\n\n\tif l.TLS != nil && l.TLS.ACME != \"\" && (l.SMTP.Enabled && !l.SMTP.NoSTARTTLS || l.Submissions.Enabled || l.IMAPS.Enabled) {\n\t\tport := config.Port(mox.Conf.Static.ACME[l.TLS.ACME].Port, 443)\n\t\tensureServe(true, port, \"acme-tls-alpn-01\", false)\n\t}\n\n\tif l.AccountHTTP.Enabled {\n\t\tport := config.Port(l.AccountHTTP.Port, 80)\n\t\tpath := \"/\"\n\t\tif l.AccountHTTP.Path != \"\" {\n\t\t\tpath = l.AccountHTTP.Path\n\t\t}\n\t\tsrv := ensureServe(false, port, \"account-http at \"+path, true)\n\t\thandler := mox.SafeHeaders(http.StripPrefix(path[:len(path)-1], http.HandlerFunc(webaccount.Handler(path, l.AccountHTTP.Forwarded))))\n\t\tsrv.ServiceHandle(\"account\", accountHostMatch, path, handler)\n\t\tredirectToTrailingSlash(srv, accountHostMatch, \"account\", path)\n\t}\n\tif l.AccountHTTPS.Enabled {\n\t\tport := config.Port(l.AccountHTTPS.Port, 443)\n\t\tpath := \"/\"\n\t\tif l.AccountHTTPS.Path != \"\" {\n\t\t\tpath = l.AccountHTTPS.Path\n\t\t}\n\t\tsrv := ensureServe(true, port, \"account-https at \"+path, true)\n\t\thandler := mox.SafeHeaders(http.StripPrefix(path[:len(path)-1], http.HandlerFunc(webaccount.Handler(path, l.AccountHTTPS.Forwarded))))\n\t\tsrv.ServiceHandle(\"account\", accountHostMatch, path, handler)\n\t\tredirectToTrailingSlash(srv, accountHostMatch, \"account\", path)\n\t}\n\n\tif l.AdminHTTP.Enabled {\n\t\tport := config.Port(l.AdminHTTP.Port, 80)\n\t\tpath := \"/admin/\"\n\t\tif l.AdminHTTP.Path != \"\" {\n\t\t\tpath = l.AdminHTTP.Path\n\t\t}\n\t\tsrv := ensureServe(false, port, \"admin-http at \"+path, true)\n\t\thandler := mox.SafeHeaders(http.StripPrefix(path[:len(path)-1], http.HandlerFunc(webadmin.Handler(path, l.AdminHTTP.Forwarded))))\n\t\tsrv.ServiceHandle(\"admin\", listenerHostMatch, path, handler)\n\t\tredirectToTrailingSlash(srv, listenerHostMatch, \"admin\", path)\n\t}\n\tif l.AdminHTTPS.Enabled {\n\t\tport := config.Port(l.AdminHTTPS.Port, \n```\n# Function Signature: func (s *serve) ServeHTTP(xw http.ResponseWriter, r *http.Request) {\n# Natural Language Annotations: // ServeHTTP is the starting point for serving HTTP requests. It dispatches to the\n// right pathHandler or WebHandler, and it generates access logs and tracks\n// metrics.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (s *serve) ServeHTTP(xw http.ResponseWriter, r *http.Request) {\n\t// Create the logging writer.\n\tw := &loggingWriter{\n\t\tW:       xw.(responseWriterFlusher),\n\t\tStart:   time.Now(),\n\t\tR:       r,\n\t\tHandler: \"serve\",\n\t}\n\tdefer w.Done()\n\n\t// Check if the request has been rate limited.\n\tif !limiterConnectionrate.Allow() {\n\t\tw.WriteHeader(http.StatusTooManyRequests)\n\t\treturn\n\t}\n\n\t// Check if the request is for a system handler.\n\tfor _, ph := range s.SystemHandlers {\n\t\tif strings.HasPrefix(r.URL.Path, ph.Path) && (ph.HostMatch == nil || ph.HostMatch(dns.IPDomain{Domain: r.Host})) {\n\t\t\tph.Handler.ServeHTTP(w, r)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Check if the request is for a service handler.\n\tfor _, ph := range s.ServiceHandlers {\n\t\tif strings.HasPrefix(r.URL.Path, ph.Path) && (ph.HostMatch == nil || ph.HostMatch(dns.IPDomain{Domain: r.Host})) {\n\t\t\tph.Handler.ServeHTTP(w, r)\n\t\t\treturn\n\t\t}\n\t}\n\n\t// If no handler was found, return a 404.\n\tw.WriteHeader(http.StatusNotFound)\n}\n```", "```go\nfunc (s *serve) ServeHTTP(xw http.ResponseWriter, r *http.Request) {\n\t// Create a loggingWriter that tracks metrics and logs for the request.\n\tw := &loggingWriter{\n\t\tW:       xw.(responseWriterFlusher),\n\t\tStart:   time.Now(),\n\t\tR:       r,\n\t\tHandler: \"unknown\",\n\t}\n\tdefer w.Done()\n\n\t// Dispatch to the correct handler.\n\tfor _, ph := range s.SystemHandlers {\n\t\tif ph.HostMatch == nil || ph.HostMatch(dns.DomainFromReq(r)) {\n\t\t\tif strings.HasSuffix(ph.Path, \"/\") {\n\t\t\t\tif strings.HasPrefix(r.URL.Path, ph.Path) {\n\t\t\t\t\tph.Handler.ServeHTTP(w, r)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t} else if r.URL.Path == ph.Path {\n\t\t\t\tph.Handler.ServeHTTP(w, r)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\tfor _, ph := range s.ServiceHandlers {\n\t\tif ph.HostMatch == nil || ph.HostMatch(dns.DomainFromReq(r)) {\n\t\t\tif strings.HasSuffix(ph.Path, \"/\") {\n\t\t\t\tif strings.HasPrefix(r.URL.Path, ph.Path) {\n\t\t\t\t\tph.Handler.ServeHTTP(w, r)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t} else if r.URL.Path == ph.Path {\n\t\t\t\tph.Handler.ServeHTTP(w, r)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\t// If no handler is found, return a 404.\n\thttp.NotFound(w, r)\n}\n```", "```go\nfunc (s *serve) ServeHTTP(xw http.ResponseWriter, r *http.Request) {\n\t// Create a logging writer.\n\tw := &loggingWriter{\n\t\tW: xw,\n\t\t// Add other necessary fields.\n\t}\n\n\t// Find the best matching pathHandler.\n\tvar ph *pathHandler\n\tfor _, p := range s.SystemHandlers {\n\t\tif strings.HasPrefix(r.URL.Path, p.Path) && (p.HostMatch == nil || p.HostMatch(dns.DomainFromURL(r.URL))) {\n\t\t\tph = &p\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// If no matching pathHandler found, try the webserver.\n\tif ph == nil && s.Webserver {\n\t\t// Handle the request as a webserver request.\n\t\t// Add necessary code here.\n\t}\n\n\t// If no matching pathHandler or webserver found, try the service handlers.\n\tif ph == nil {\n\t\tfor _, p := range s.ServiceHandlers {\n\t\t\tif strings.HasPrefix(r.URL.Path, p.Path) && (p.HostMatch == nil || p.HostMatch(dns.DomainFromURL(r.URL))) {\n\t\t\t\tph = &p\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\n\t// If a matching pathHandler or service handler was found, serve the request.\n\tif ph != nil {\n\t\tph.Handler.ServeHTTP(w, r)\n\t\treturn\n\t}\n\n\t// If no matching pathHandler or service handler was found, return a 404.\n\thttp.NotFound(xw, r)\n}\n```"]}, "mox-moxio/bufpool.go-put": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/moxio/bufpool.go:\n```\npackage moxio\n\nimport (\n\t\"bufio\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// todo: instead of a bufpool, should maybe just make an alternative to bufio.Reader with a big enough buffer that we can fully use to read a line.\n\nvar ErrLineTooLong = errors.New(\"line from remote too long\") // Returned by Bufpool.Readline.\n\n// Bufpool caches byte slices for reuse during parsing of line-terminated commands.\ntype Bufpool struct {\n\tc    chan []byte\n\tsize int\n}\n\n// NewBufpool makes a new pool, initially empty, but holding at most \"max\" buffers of \"size\" bytes each.\nfunc NewBufpool(max, size int) *Bufpool {\n\treturn &Bufpool{\n\t\tc:    make(chan []byte, max),\n\t\tsize: size,\n\t}\n}\n\n// get returns a buffer from the pool if available, otherwise allocates a new buffer.\n// The buffer should be returned with a call to put.\nfunc (b *Bufpool) get() []byte {\n\tvar buf []byte\n\n\t// Attempt to get buffer from pool. Otherwise create new buffer.\n\tselect {\n\tcase buf = <-b.c:\n\tdefault:\n\t}\n\tif buf == nil {\n\t\tbuf = make([]byte, b.size)\n\t}\n\treturn buf\n}\n\n// put puts a \"buf\" back in the pool. Put clears the first \"n\" bytes, which should\n// be all the bytes that have been read in the buffer. If the pool is full, the\n// buffer is discarded, and will be cleaned up by the garbage collector.\n// The caller should no longer reference \"buf\" after a call to put.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Readline reads a \\n- or \\r\\n-terminated line. Line is returned without \\n or \\r\\n.\n// If the line was too long, ErrLineTooLong is returned.\n// If an EOF is encountered before a \\n, io.ErrUnexpectedEOF is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (b *Bufpool) put(log mlog.Log, buf []byte, n int) {\n# Natural Language Annotations: // put puts a \"buf\" back in the pool. Put clears the first \"n\" bytes, which should\n// be all the bytes that have been read in the buffer. If the pool is full, the\n// buffer is discarded, and will be cleaned up by the garbage collector.\n// The caller should no longer reference \"buf\" after a call to put.\nPlease return the generated code surrounded by ```\n", "response": ["Sure, here is the generated code:\n\n```go\nfunc (b *Bufpool) put(log mlog.Log, buf []byte, n int) {\n\t// Clear the first \"n\" bytes.\n\tfor i := 0; i < n; i++ {\n\t\tbuf[i] = 0\n\t}\n\n\t// Try to put the buffer back in the pool.\n\tselect {\n\tcase b.c <- buf:\n\tdefault:\n\t\t// If the pool is full, the buffer is discarded.\n\t}\n}\n```\n\nThis function takes a buffer and an integer as arguments. It clears the first \"n\" bytes in the buffer. It then tries to put the buffer back in the pool. If the pool is full, the buffer is discarded, and will be cleaned up by the garbage collector. The caller should no longer reference the buffer after a call to `put`.", "Sure, here is the generated function based on the function signature and the natural language annotations:\n\n```go\nfunc (b *Bufpool) put(log mlog.Log, buf []byte, n int) {\n\t// clear the first n bytes\n\tfor i := 0; i < n; i++ {\n\t\tbuf[i] = 0\n\t}\n\n\t// attempt to put buffer back into the pool\n\tselect {\n\tcase b.c <- buf:\n\tdefault:\n\t\t// if the pool is full, discard the buffer\n\t\tbuf = nil\n\t}\n}\n```\n\nThis function clears the first `n` bytes in the buffer, and then tries to put the buffer back into the pool. If the pool is full (i.e., the channel is full), the buffer is discarded and set to `nil`. The caller is expected to no longer reference the buffer after a call to `put`.", "```go\nfunc (b *Bufpool) put(log mlog.Log, buf []byte, n int) {\n\t// Check if buffer is nil\n\tif buf == nil {\n\t\tlog.Error(\"buffer is nil, nothing to put back in the pool\")\n\t\treturn\n\t}\n\n\t// Clear the first \"n\" bytes\n\tfor i := 0; i < n; i++ {\n\t\tbuf[i] = 0\n\t}\n\n\t// Check if the pool is full\n\tif len(b.c) < cap(b.c) {\n\t\tb.c <- buf\n\t} else {\n\t\tlog.Debug(\"the pool is full, the buffer will be discarded\")\n\t}\n}\n```"]}, "mox-moxio/bufpool.go-Readline": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/moxio/bufpool.go:\n```\npackage moxio\n\nimport (\n\t\"bufio\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// todo: instead of a bufpool, should maybe just make an alternative to bufio.Reader with a big enough buffer that we can fully use to read a line.\n\nvar ErrLineTooLong = errors.New(\"line from remote too long\") // Returned by Bufpool.Readline.\n\n// Bufpool caches byte slices for reuse during parsing of line-terminated commands.\ntype Bufpool struct {\n\tc    chan []byte\n\tsize int\n}\n\n// NewBufpool makes a new pool, initially empty, but holding at most \"max\" buffers of \"size\" bytes each.\nfunc NewBufpool(max, size int) *Bufpool {\n\treturn &Bufpool{\n\t\tc:    make(chan []byte, max),\n\t\tsize: size,\n\t}\n}\n\n// get returns a buffer from the pool if available, otherwise allocates a new buffer.\n// The buffer should be returned with a call to put.\nfunc (b *Bufpool) get() []byte {\n\tvar buf []byte\n\n\t// Attempt to get buffer from pool. Otherwise create new buffer.\n\tselect {\n\tcase buf = <-b.c:\n\tdefault:\n\t}\n\tif buf == nil {\n\t\tbuf = make([]byte, b.size)\n\t}\n\treturn buf\n}\n\n// put puts a \"buf\" back in the pool. Put clears the first \"n\" bytes, which should\n// be all the bytes that have been read in the buffer. If the pool is full, the\n// buffer is discarded, and will be cleaned up by the garbage collector.\n// The caller should no longer reference \"buf\" after a call to put.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Readline reads a \\n- or \\r\\n-terminated line. Line is returned without \\n or \\r\\n.\n// If the line was too long, ErrLineTooLong is returned.\n// If an EOF is encountered before a \\n, io.ErrUnexpectedEOF is returned.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func (b *Bufpool) Readline(log mlog.Log, r *bufio.Reader) (line string, rerr error) {\n# Natural Language Annotations: // Readline reads a \\n- or \\r\\n-terminated line. Line is returned without \\n or \\r\\n.\n// If the line was too long, ErrLineTooLong is returned.\n// If an EOF is encountered before a \\n, io.ErrUnexpectedEOF is returned.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (b *Bufpool) Readline(log mlog.Log, r *bufio.Reader) (line string, rerr error) {\n\tbuf := b.get()\n\tdefer func() { b.put(buf) }()\n\n\tn, err := r.Read(buf)\n\tif err != nil {\n\t\tif err != io.EOF {\n\t\t\tlog.Error(\"read error\", slog.String(\"err\", err.Error()))\n\t\t}\n\t\treturn \"\", err\n\t}\n\n\thaveNL := false\n\tfor i, c := range buf[:n] {\n\t\tif c == '\\n' {\n\t\t\tline = string(buf[:i])\n\t\t\thaveNL = true\n\t\t\tbreak\n\t\t}\n\t}\n\tif !haveNL {\n\t\tif n == len(buf) {\n\t\t\treturn \"\", ErrLineTooLong\n\t\t}\n\t\treturn \"\", io.ErrUnexpectedEOF\n\t}\n\n\t// Trim \\r if present.\n\tif len(line) > 0 && line[len(line)-1] == '\\r' {\n\t\tline = line[:len(line)-1]\n\t}\n\treturn line, nil\n}\n```", "```go\nfunc (b *Bufpool) Readline(log mlog.Log, r *bufio.Reader) (line string, rerr error) {\n\tbuf := b.get()\n\tn, err := r.Read(buf)\n\tif err != nil {\n\t\tif err == io.EOF {\n\t\t\treturn \"\", io.ErrUnexpectedEOF\n\t\t}\n\t\treturn \"\", err\n\t}\n\n\t// Check if line is too long.\n\tif n == len(buf) && n != 0 && buf[n-1] != '\\n' {\n\t\treturn \"\", ErrLineTooLong\n\t}\n\n\t// Find the end of the line.\n\tend := n\n\tfor end > 0 && (buf[end-1] == '\\n' || buf[end-1] == '\\r') {\n\t\tend--\n\t}\n\n\t// Return the line.\n\tline = string(buf[:end])\n\tb.put(buf)\n\treturn line, nil\n}\n```", "```go\nfunc (b *Bufpool) Readline(log mlog.Log, r *bufio.Reader) (line string, rerr error) {\n\tbuf := b.get()\n\tdefer func() { b.put(buf[:cap(buf)]) }()\n\n\tn, err := r.Read(buf)\n\tif err != nil {\n\t\tif err == io.EOF {\n\t\t\treturn \"\", io.ErrUnexpectedEOF\n\t\t}\n\t\treturn \"\", err\n\t}\n\n\tif n == cap(buf) {\n\t\treturn \"\", ErrLineTooLong\n\t}\n\n\tend := bytes.IndexByte(buf[:n], '\\n')\n\tif end == -1 {\n\t\treturn \"\", ErrLineTooLong\n\t}\n\n\tadjust := 1\n\tif end > 0 && buf[end-1] == '\\r' {\n\t\tadjust++\n\t}\n\n\treturn string(buf[:end-adjust+1]), nil\n}\n```"]}, "mox-moxio/linkcopy.go-LinkOrCopy": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/moxio/linkcopy.go:\n```\npackage moxio\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"os\"\n\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// LinkOrCopy attempts to make a hardlink dst. If that fails, it will try to do a\n// regular file copy. If srcReaderOpt is not nil, it will be used for reading. If\n// sync is true and the file is copied, Sync is called on the file after writing to\n// ensure the file is written on disk. Callers should also sync the directory of\n// the destination file, but may want to do that after linking/copying multiple\n// files. If dst was created and an error occurred, it is removed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n# Function Signature: func LinkOrCopy(log mlog.Log, dst, src string, srcReaderOpt io.Reader, sync bool) (rerr error) {\n# Natural Language Annotations: // LinkOrCopy attempts to make a hardlink dst. If that fails, it will try to do a\n// regular file copy. If srcReaderOpt is not nil, it will be used for reading. If\n// sync is true and the file is copied, Sync is called on the file after writing to\n// ensure the file is written on disk. Callers should also sync the directory of\n// the destination file, but may want to do that after linking/copying multiple\n// files. If dst was created and an error occurred, it is removed.\nPlease return the generated code surrounded by ```\n", "response": ["Sure, here is a possible implementation of the `LinkOrCopy` function in Go:\n\n```go\npackage moxio\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"os\"\n\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// LinkOrCopy attempts to make a hardlink dst. If that fails, it will try to do a\n// regular file copy. If srcReaderOpt is not nil, it will be used for reading. If\n// sync is true and the file is copied, Sync is called on the file after writing to\n// ensure the file is written on disk. Callers should also sync the directory of\n// the destination file, but may want to do that after linking/copying multiple\n// files. If dst was created and an error occurred, it is removed.\nfunc LinkOrCopy(log mlog.Log, dst, src string, srcReaderOpt io.Reader, sync bool) (rerr error) {\n\tdefer func() {\n\t\tif rerr != nil && dst != \"\" {\n\t\t\tos.Remove(dst)\n\t\t}\n\t}()\n\n\tif srcReaderOpt != nil {\n\t\t// Use provided io.Reader for reading\n\t\treturn copyFile(dst, srcReaderOpt, sync)\n\t}\n\n\t// Try to make a hard link\n\tif err := os.Link(src, dst); err == nil {\n\t\treturn nil\n\t}\n\n\t// If hard link failed, try to open the source file and copy its contents\n\tsrcFile, err := os.Open(src)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to open source file: %w\", err)\n\t}\n\tdefer srcFile.Close()\n\n\treturn copyFile(dst, srcFile, sync)\n}\n\nfunc copyFile(dst string, src io.Reader, sync bool) error {\n\t// Create the destination file\n\tdstFile, err := os.Create(dst)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to create destination file: %w\", err)\n\t}\n\n\t// Copy the source file to the destination file\n\tif _, err := io.Copy(dstFile, src); err != nil {\n\t\treturn fmt.Errorf(\"failed to copy source file to destination: %w\", err)\n\t}\n\n\t// If sync is true, call Sync on the file to ensure it is written on disk\n\tif sync {\n\t\tif err := dstFile.Sync(); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to sync destination file: %w\", err)\n\t\t}\n\t}\n\n\t// Close the destination file\n\tif err := dstFile.Close(); err != nil {\n\t\treturn fmt.Errorf(\"failed to close destination file: %w\", err)\n\t}\n\n\treturn nil\n}\n```\nThis function attempts to create a hard link from `src` to `dst`. If it fails, it tries to open the source file and copy its contents to the destination. If `sync` is true and the file is copied, it calls `Sync` on the file after writing to ensure the file is written on disk. If an error occurs and `dst` was created, it removes the destination file.", "```go\npackage moxio\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"os\"\n\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// LinkOrCopy attempts to make a hardlink dst. If that fails, it will try to do a\n// regular file copy. If srcReaderOpt is not nil, it will be used for reading. If\n// sync is true and the file is copied, Sync is called on the file after writing to\n// ensure the file is written on disk. Callers should also sync the directory of\n// the destination file, but may want to do that after linking/copying multiple\n// files. If dst was created and an error occurred, it is removed.\nfunc LinkOrCopy(log mlog.Log, dst, src string, srcReaderOpt io.Reader, sync bool) (rerr error) {\n\t// Open the source file\n\tsrcFile, err := os.Open(src)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to open source file: %v\", err)\n\t}\n\tdefer srcFile.Close()\n\n\t// Create destination file\n\tdstFile, err := os.Create(dst)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to create destination file: %v\", err)\n\t}\n\tdefer func() {\n\t\tif err := dstFile.Close(); err != nil {\n\t\t\trerr = err\n\t\t}\n\t\tif rerr != nil {\n\t\t\tos.Remove(dst)\n\t\t}\n\t}()\n\n\t// Try to make a hard link\n\terr = os.Link(src, dst)\n\tif err == nil {\n\t\treturn nil\n\t}\n\n\t// If hard link failed, try to copy the file\n\tvar srcReader io.Reader\n\tif srcReaderOpt != nil {\n\t\tsrcReader = srcReaderOpt\n\t} else {\n\t\tsrcReader = srcFile\n\t}\n\n\t_, err = io.Copy(dstFile, srcReader)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to copy file: %v\", err)\n\t}\n\n\t// If sync is true, call Sync on the file\n\tif sync {\n\t\terr = dstFile.Sync()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to sync file: %v\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n```", "```go\npackage moxio\n\nimport (\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"os\"\n\n\t\"github.com/mjl-/mox/mlog\"\n)\n\n// LinkOrCopy attempts to make a hardlink dst. If that fails, it will try to do a\n// regular file copy. If srcReaderOpt is not nil, it will be used for reading. If\n// sync is true and the file is copied, Sync is called on the file after writing to\n// ensure the file is written on disk. Callers should also sync the directory of\n// the destination file, but may want to do that after linking/copying multiple\n// files. If dst was created and an error occurred, it is removed.\nfunc LinkOrCopy(log mlog.Log, dst, src string, srcReaderOpt io.Reader, sync bool) (rerr error) {\n\tdefer func() {\n\t\tif rerr != nil && dst != \"\" {\n\t\t\tos.Remove(dst)\n\t\t}\n\t}()\n\n\tvar srcFile *os.File\n\tif srcReaderOpt != nil {\n\t\tvar ok bool\n\t\tsrcFile, ok = srcReaderOpt.(*os.File)\n\t\tif !ok {\n\t\t\treturn fmt.Errorf(\"srcReaderOpt is not a file\")\n\t\t}\n\t} else {\n\t\tvar err error\n\t\tsrcFile, err = os.Open(src)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"opening source file: %w\", err)\n\t\t}\n\t\tdefer srcFile.Close()\n\t}\n\n\terr := os.Link(srcFile.Name(), dst)\n\tif err == nil {\n\t\treturn nil\n\t}\n\n\tdstFile, err := os.Create(dst)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"creating destination file: %w\", err)\n\t}\n\tdefer func() {\n\t\tif rerr != nil {\n\t\t\tos.Remove(dst)\n\t\t}\n\t}()\n\n\tif srcReaderOpt != nil {\n\t\t_, err = io.Copy(dstFile, srcReaderOpt)\n\t} else {\n\t\t_, err = io.Copy(dstFile, srcFile)\n\t}\n\tif err != nil {\n\t\treturn fmt.Errorf(\"copying file: %w\", err)\n\t}\n\n\tif sync {\n\t\terr = dstFile.Sync()\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"syncing file: %w\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n```"]}, "mox-moxio/base64writer.go-Base64Writer": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/moxio/base64writer.go:\n```\npackage moxio\n\nimport (\n\t\"encoding/base64\"\n\t\"io\"\n)\n\n// implement io.Closer\ntype closerFunc func() error\n\nfunc (f closerFunc) Close() error {\n\treturn f()\n}\n\n// Base64Writer turns a writer for data into one that writes base64 content on\n// \\r\\n separated lines of max 78+2 characters length.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntype lineWrapper struct {\n\tw io.Writer\n\tn int // Written on current line.\n}\n\nfunc (lw *lineWrapper) Write(buf []byte) (int, error) {\n\twrote := 0\n\tfor len(buf) > 0 {\n\t\tn := 78 - lw.n\n\t\tif n > len(buf) {\n\t\t\tn = len(buf)\n\t\t}\n\t\tnn, err := lw.w.Write(buf[:n])\n\t\tif nn > 0 {\n\t\t\twrote += nn\n\t\t\tbuf = buf[nn:]\n\t\t}\n\t\tif err != nil {\n\t\t\treturn wrote, err\n\t\t}\n\t\tlw.n += nn\n\t\tif lw.n == 78 {\n\t\t\t_, err := lw.w.Write([]byte(\"\\r\\n\"))\n\t\t\tif err != nil {\n\t\t\t\treturn wrote, err\n\t\t\t}\n\t\t\tlw.n = 0\n\t\t}\n\t}\n\treturn wrote, nil\n}\n\nfunc (lw *lineWrapper) Close() error {\n\tif lw.n > 0 {\n\t\tlw.n = 0\n\t\t_, err := lw.w.Write([]byte(\"\\r\\n\"))\n\t\treturn err\n\t}\n\treturn nil\n}\n\n```\n# Function Signature: func Base64Writer(w io.Writer) io.WriteCloser {\n# Natural Language Annotations: // Base64Writer turns a writer for data into one that writes base64 content on\n// \\r\\n separated lines of max 78+2 characters length.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc Base64Writer(w io.Writer) io.WriteCloser {\n\tb64Writer := base64.NewEncoder(base64.StdEncoding, &lineWrapper{w: w})\n\treturn closerFunc(func() error {\n\t\terr := b64Writer.Close()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn w.(io.Closer).Close()\n\t})\n}\n```", "```go\nfunc Base64Writer(w io.Writer) io.WriteCloser {\n\tb64w := base64.NewEncoder(base64.StdEncoding, &lineWrapper{w: w})\n\treturn closerFunc(func() error {\n\t\terr := b64w.Close()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif closer, ok := w.(io.Closer); ok {\n\t\t\treturn closer.Close()\n\t\t}\n\t\treturn nil\n\t})\n}\n```", "```go\npackage moxio\n\nimport (\n\t\"encoding/base64\"\n\t\"io\"\n)\n\nfunc Base64Writer(w io.Writer) io.WriteCloser {\n\tencoder := base64.NewEncoder(base64.StdEncoding, &lineWrapper{w: w})\n\treturn closerFunc(func() error {\n\t\terr := encoder.Close()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlnWriter, ok := w.(io.WriteCloser)\n\t\tif ok {\n\t\t\treturn lnWriter.Close()\n\t\t}\n\t\treturn nil\n\t})\n}\n```"]}, "mox-dkim/sig.go-Header": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dkim/sig.go:\n```\npackage dkim\n\nimport (\n\t\"bytes\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n\n\t\"github.com/mjl-/mox/dns\"\n\t\"github.com/mjl-/mox/message\"\n\t\"github.com/mjl-/mox/smtp\"\n)\n\n// Sig is a DKIM-Signature header.\n//\n// String values must be compared case insensitively.\ntype Sig struct {\n\t// Required fields.\n\tVersion       int        // Version, 1. Field \"v\". Always the first field.\n\tAlgorithmSign string     // \"rsa\" or \"ed25519\". Field \"a\".\n\tAlgorithmHash string     // \"sha256\" or the deprecated \"sha1\" (deprecated). Field \"a\".\n\tSignature     []byte     // Field \"b\".\n\tBodyHash      []byte     // Field \"bh\".\n\tDomain        dns.Domain // Field \"d\".\n\tSignedHeaders []string   // Duplicates are meaningful. Field \"h\".\n\tSelector      dns.Domain // Selector, for looking DNS TXT record at <s>._domainkey.<domain>. Field \"s\".\n\n\t// Optional fields.\n\t// Canonicalization is the transformation of header and/or body before hashing. The\n\t// value is in original case, but must be compared case-insensitively. Normally two\n\t// slash-separated values: header canonicalization and body canonicalization. But\n\t// the \"simple\" means \"simple/simple\" and \"relaxed\" means \"relaxed/simple\". Field\n\t// \"c\".\n\tCanonicalization string\n\tLength           int64     // Body length to verify, default -1 for whole body. Field \"l\".\n\tIdentity         *Identity // AUID (agent/user id). If nil and an identity is needed, should be treated as an Identity without localpart and Domain from d= field. Field \"i\".\n\tQueryMethods     []string  // For public key, currently known value is \"dns/txt\" (should be compared case-insensitively). If empty, dns/txt must be assumed. Field \"q\".\n\tSignTime         int64     // Unix epoch. -1 if unset. Field \"t\".\n\tExpireTime       int64     // Unix epoch. -1 if unset. Field \"x\".\n\tCopiedHeaders    []string  // Copied header fields. Field \"z\".\n}\n\n// Identity is used for the optional i= field in a DKIM-Signature header. It uses\n// the syntax of an email address, but does not necessarily represent one.\ntype Identity struct {\n\tLocalpart *smtp.Localpart // Optional.\n\tDomain    dns.Domain\n}\n\n// String returns a value for use in the i= DKIM-Signature field.\nfunc (i Identity) String() string {\n\ts := \"@\" + i.Domain.ASCII\n\t// We need localpart as pointer to indicate it is missing because localparts can be\n\t// \"\" which we store (decoded) as empty string and we need to differentiate.\n\tif i.Localpart != nil {\n\t\ts = i.Localpart.String() + s\n\t}\n\treturn s\n}\n\nfunc newSigWithDefaults() *Sig {\n\treturn &Sig{\n\t\tCanonicalization: \"simple/simple\",\n\t\tLength:           -1,\n\t\tSignTime:         -1,\n\t\tExpireTime:       -1,\n\t}\n}\n\n// Algorithm returns an algorithm string for use in the \"a\" field. E.g.\n// \"ed25519-sha256\".\nfunc (s Sig) Algorithm() string {\n\treturn s.AlgorithmSign + \"-\" + s.AlgorithmHash\n}\n\n// Header returns the DKIM-Signature header in string form, to be prepended to a\n// message, including DKIM-Signature field name and trailing \\r\\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n// Like quoted printable, but with \"|\" encoded as well.\n// We also encode \":\" because it is used as separator in DKIM headers which can\n// cause trouble for \"q\", even though it is listed in dkim-safe-char,\n// ../rfc/6376:497.\nfunc packQpHdrValue(s string) string {\n\t// ../rfc/6376:474\n\tconst hex = \"0123456789ABCDEF\"\n\tvar r string\n\tfor _, b := range []byte(s) {\n\t\tif b > ' ' && b < 0x7f && b != ';' && b != '=' && b != '|' && b != ':' {\n\t\t\tr += string(b)\n\t\t} else {\n\t\t\tr += \"=\" + string(hex[b>>4]) + string(hex[(b>>0)&0xf])\n\t\t}\n\t}\n\treturn r\n}\n\nvar (\n\terrSigHeader         = errors.New(\"not DKIM-Signature header\")\n\terrSigDuplicateTag   = errors.New(\"duplicate tag\")\n\terrSigMissingCRLF    = errors.New(\"missing crlf at end\")\n\terrSigExpired        = errors.New(\"signature timestamp (t=) must be before signature expiration (x=)\")\n\terrSigIdentityDomain = errors.New(\"identity domain (i=) not under domain (d=)\")\n\terrSigMissingTag     = errors.New(\"missing required tag\")\n\terrSigUnknownVersion = errors.New(\"unknown version\")\n\terrSigBodyHash       = errors.New(\"bad body hash size given algorithm\")\n)\n\n// parseSignatures returns the parsed form of a DKIM-Signature header.\n//\n// buf must end in crlf, as it should have occurred in the mail message.\n//\n// The dkim signature with signature left empty (\"b=\") and without trailing\n// crlf is returned, for use in verification.\nfunc parseSignature(buf []byte, smtputf8 bool) (sig *Sig, verifySig []byte, err error) {\n\tdefer func() {\n\t\tif x := recover(); x == nil {\n\t\t\treturn\n\t\t} else if xerr, ok := x.(error); ok {\n\t\t\tsig = nil\n\t\t\tverifySig = nil\n\t\t\terr = xerr\n\t\t} else {\n\t\t\tpanic(x)\n\t\t}\n\t}()\n\n\txerrorf := func(format string, args ...any) {\n\t\tpanic(fmt.Errorf(format, args...))\n\t}\n\n\tif !bytes.HasSuffix(buf, []byte(\"\\r\\n\")) {\n\t\txerrorf(\"%w\", errSigMissingCRLF)\n\t}\n\tbuf = buf[:len(buf)-2]\n\n\tds := newSigWithDefaults()\n\tseen := map[string]struct{}{}\n\tp := parser{s: string(buf), smtputf8: smtputf8}\n\tname := p.xhdrName(false)\n\tif !strings.EqualFold(name, \"DKIM-Signature\") {\n\t\txerrorf(\"%w\", errSigHeader)\n\t}\n\tp.wsp()\n\tp.xtake(\":\")\n\tp.wsp()\n\t// ../rfc/6376:655\n\t// ../rfc/6376:656 ../rfc/6376-eid5070\n\t// ../rfc/6376:658 ../rfc/6376-eid5070\n\tfor {\n\t\tp.fws()\n\t\tk := p.xtagName()\n\t\tp.fws()\n\t\tp.xtake(\"=\")\n\t\t// Special case for \"b\", see below.\n\t\tif k != \"b\" {\n\t\t\tp.fws()\n\t\t}\n\t\t// Keys are case-sensitive: ../rfc/6376:679\n\t\tif _, ok := seen[k]; ok {\n\t\t\t// Duplicates not allowed: ../rfc/6376:683\n\t\t\txerrorf(\"%w: %q\", errSigDuplicateTag, k)\n\t\t\tbreak\n\t\t}\n\t\tseen[k] = struct{}{}\n\n\t\t// ../rfc/6376:1021\n\t\tswitch k {\n\t\tcase \"v\":\n\t\t\t// ../rfc/6376:1025\n\t\t\tds.Version = int(p.xnumber(10))\n\t\t\tif ds.Version != 1 {\n\t\t\t\txerrorf(\"%w: version %d\", errSigUnknownVersion, ds.Version)\n\t\t\t}\n\t\tcase \"a\":\n\t\t\t// ../rfc/6376:1038\n\t\t\tds.AlgorithmSign, ds.AlgorithmHash = p.xalgorithm()\n\t\tcase \"b\":\n\t\t\t// ../rfc/6376:1054\n\t\t\t// To calculate the hash, we have to feed the DKIM-Signature header to the hash\n\t\t\t// function, but with the value for \"b=\" (the signature) left out. The parser\n\t\t\t// tracks all data that is read, except when drop is true.\n\t\t\t// ../rfc/6376:997\n\t\t\t// Surrounding whitespace must be cleared as well. ../rfc/6376:1659\n\t\t\t// Note: The RFC says \"surrounding\" whitespace, but whitespace is only allowed\n\t\t\t// before the value as part of the ABNF production for \"b\". Presumably the\n\t\t\t// intention is to ignore the trailing \"[FWS]\" for the tag-spec production,\n\t\t\t// ../rfc/6376:656\n\t\t\t// Another indication is the term \"value portion\", ../rfc/6376:1667. It appears to\n\t\t\t// mean everything after the \"b=\" part, instead of the actual value (either encoded\n\t\t\t// or decoded).\n\t\t\tp.drop = true\n\t\t\tp.fws()\n\t\t\tds.Signature = p.xbase64()\n\t\t\tp.fws()\n\t\t\tp.drop = false\n\t\tcase \"bh\":\n\t\t\t// ../rfc/6376:1076\n\t\t\tds.BodyHash = p.xbase64()\n\t\tcase \"c\":\n\t\t\t// ../rfc/6376:1088\n\t\t\tds.Canonicalization = p.xcanonical()\n\t\t\t// ../rfc/6376:810\n\t\tcase \"d\":\n\t\t\t// ../rfc/6376:1105\n\t\t\tds.Domain = p.xdomain()\n\t\tcase \"h\":\n\t\t\t// ../rfc/6376:1134\n\t\t\tds.SignedHeaders = p.xsignedHeaderFields()\n\t\tcase \"i\":\n\t\t\t// ../rfc/6376:1171\n\t\t\tid := p.xauid()\n\t\t\tds.Identity = &id\n\t\tcase \"l\":\n\t\t\t// ../rfc/6376:1244\n\t\t\tds.Length = p.xbodyLength()\n\t\tcase \"q\":\n\t\t\t// ../rfc/6376:1268\n\t\t\tds.QueryMethods = p.xqueryMethods()\n\t\tcase \"s\":\n\t\t\t// ../rfc/6376:1300\n\t\t\tds.Selector = p.xselector()\n\t\tcase \"t\":\n\t\t\t// ../rfc/6376:1310\n\t\t\tds.SignTime = p.xtimestamp()\n\t\tcase \"x\":\n\t\t\t// ../rfc/6376:1327\n\t\t\tds.ExpireTime = p.xtimestamp()\n\t\tcase \"z\":\n\t\t\t// ../rfc/6376:1361\n\t\t\tds.CopiedHeaders = p.xcopiedHeaderFields()\n\t\tdefault:\n\t\t\t// We must ignore unknown fields. ../rfc/6376:692 ../rfc/6376:1022\n\t\t\tp.xchar() // ../rfc/6376-eid5070\n\t\t\tfor !p.empty() && !p.hasPrefix(\";\") {\n\t\t\t\tp.xchar()\n\t\t\t}\n\t\t}\n\t\tp.fws()\n\n\t\tif p.empty() {\n\t\t\tbreak\n\t\t}\n\t\tp.xtake(\";\")\n\t\tif p.empty() {\n\t\t\tbreak\n\t\t}\n\t}\n\n\t// ../rfc/6376:2532\n\trequired := []string{\"v\", \"a\", \"b\", \"bh\", \"d\", \"h\", \"s\"}\n\tfor _, req := range required {\n\t\tif _, ok := seen[req]; !ok {\n\t\t\txerrorf(\"%w: %q\", errSigMissingTag, req)\n\t\t}\n\t}\n\n\tif strings.EqualFold(ds.AlgorithmHash, \"sha1\") && len(ds.BodyHash) != 20 {\n\t\txerrorf(\"%w: got %d bytes, must be 20 for sha1\", errSigBodyHash, len(ds.BodyHash))\n\t} else if strings.EqualFold(ds.AlgorithmHash, \"sha256\") && len(ds.BodyHash) != 32 {\n\t\txerrorf(\"%w: got %d bytes, must be 32 for sha256\", errSigBodyHash, len(ds.BodyHash))\n\t}\n\n\t// ../rfc/6376:1337\n\tif ds.SignTime >= 0 && ds.ExpireTime >= 0 && ds.SignTime >= ds.ExpireTime {\n\t\txerrorf(\"%w\", errSigExpired)\n\t}\n\n\t// Default identity is \"@\" plus domain. We don't set this value because we want to\n\t// keep the distinction between absent value.\n\t// ../rfc/6376:1172 ../rfc/6376:2537 ../rfc/6376:2541\n\tif ds.Identity != nil && ds.Identity.Domain.ASCII != ds.Domain.ASCII && !strings.HasSuffix(ds.Identity.Domain.ASCII, \".\"+ds.Domain.ASCII) {\n\t\txerrorf(\"%w: identity domain %q not under domain %q\", errSigIdentityDomain, ds.Identity.Domain.ASCII, ds.Domain.ASCII)\n\t}\n\n\treturn ds, []byte(p.tracked), nil\n}\n\n```\n# Function Signature: func (s *Sig) Header() (string, error) {\n# Natural Language Annotations: // Header returns the DKIM-Signature header in string form, to be prepended to a\n// message, including DKIM-Signature field name and trailing \\r\\n.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (s *Sig) Header() (string, error) {\n\theader := \"DKIM-Signature: v=1; a=\" + s.Algorithm() + \"; c=\" + s.Canonicalization + \"; h=\" + strings.Join(s.SignedHeaders, \":\") + \"; d=\" + s.Domain.ASCII + \"; s=\" + s.Selector.ASCII + \"; bh=\" + base64.StdEncoding.EncodeToString(s.BodyHash) + \"; b=\" + base64.StdEncoding.EncodeToString(s.Signature)\n\tif s.Identity != nil {\n\t\theader += \"; i=\" + s.Identity.String()\n\t}\n\tif len(s.CopiedHeaders) > 0 {\n\t\theader += \"; z=\" + strings.Join(s.CopiedHeaders, \":\")\n\t}\n\tif s.Length >= 0 {\n\t\theader += \"; l=\" + strconv.FormatInt(s.Length, 10)\n\t}\n\tif s.SignTime >= 0 {\n\t\theader += \"; t=\" + strconv.FormatInt(s.SignTime, 10)\n\t}\n\tif s.ExpireTime >= 0 {\n\t\theader += \"; x=\" + strconv.FormatInt(s.ExpireTime, 10)\n\t}\n\tif len(s.QueryMethods) > 0 {\n\t\theader += \"; q=\" + strings.Join(s.QueryMethods, \":\")\n\t}\n\theader += \"\\r\\n\"\n\treturn header, nil\n}\n```", "```go\nfunc (s *Sig) Header() (string, error) {\n\tvar b strings.Builder\n\n\tb.WriteString(\"DKIM-Signature: \")\n\n\tif s.Version != 0 {\n\t\tb.WriteString(\"v=\")\n\t\tb.WriteString(strconv.Itoa(s.Version))\n\t}\n\n\tif s.AlgorithmSign != \"\" {\n\t\tb.WriteString(\"; a=\")\n\t\tb.WriteString(packQpHdrValue(s.Algorithm()))\n\t}\n\n\tif len(s.Signature) != 0 {\n\t\tb.WriteString(\"; b=\")\n\t\tb.WriteString(base64.StdEncoding.EncodeToString(s.Signature))\n\t}\n\n\tif len(s.BodyHash) != 0 {\n\t\tb.WriteString(\"; bh=\")\n\t\tb.WriteString(base64.StdEncoding.EncodeToString(s.BodyHash))\n\t}\n\n\tif s.Domain.ASCII != \"\" {\n\t\tb.WriteString(\"; d=\")\n\t\tb.WriteString(packQpHdrValue(s.Domain.ASCII))\n\t}\n\n\tfor _, header := range s.SignedHeaders {\n\t\tb.WriteString(\"; h=\")\n\t\tb.WriteString(packQpHdrValue(header))\n\t}\n\n\tif s.Canonicalization != \"\" {\n\t\tb.WriteString(\"; c=\")\n\t\tb.WriteString(packQpHdrValue(s.Canonicalization))\n\t}\n\n\tif s.Length >= 0 {\n\t\tb.WriteString(\"; l=\")\n\t\tb.WriteString(strconv.FormatInt(s.Length, 10))\n\t}\n\n\tif s.Identity != nil {\n\t\tb.WriteString(\"; i=\")\n\t\tb.WriteString(packQpHdrValue(s.Identity.String()))\n\t}\n\n\tfor _, method := range s.QueryMethods {\n\t\tb.WriteString(\"; q=\")\n\t\tb.WriteString(packQpHdrValue(method))\n\t}\n\n\tif s.SignTime >= 0 {\n\t\tb.WriteString(\"; t=\")\n\t\tb.WriteString(strconv.FormatInt(s.SignTime, 10))\n\t}\n\n\tif s.ExpireTime >= 0 {\n\t\tb.WriteString(\"; x=\")\n\t\tb.WriteString(strconv.FormatInt(s.ExpireTime, 10))\n\t}\n\n\tfor _, header := range s.CopiedHeaders {\n\t\tb.WriteString(\"; z=\")\n\t\tb.WriteString(packQpHdrValue(header))\n\t}\n\n\tb.WriteString(\"\\r\\n\")\n\n\treturn b.String(), nil\n}\n```", "```go\nfunc (s *Sig) Header() (string, error) {\n\tvar buf bytes.Buffer\n\t\n\t// DKIM-Signature:\n\tbuf.WriteString(\"DKIM-Signature:\")\n\n\t// v=1\n\tbuf.WriteString(fmt.Sprintf(\" v=%d\", s.Version))\n\n\t// a=\n\tbuf.WriteString(\" a=\" + packQpHdrValue(s.Algorithm()))\n\n\t// b=\n\tbuf.WriteString(\" b=\" + base64.StdEncoding.EncodeToString(s.Signature))\n\n\t// bh=\n\tbuf.WriteString(\" bh=\" + base64.StdEncoding.EncodeToString(s.BodyHash))\n\n\t// c=\n\tif s.Canonicalization != \"\" {\n\t\tbuf.WriteString(\" c=\" + packQpHdrValue(s.Canonicalization))\n\t}\n\n\t// d=\n\tbuf.WriteString(\" d=\" + packQpHdrValue(s.Domain.ASCII))\n\n\t// h=\n\tfor _, header := range s.SignedHeaders {\n\t\tbuf.WriteString(\" h=\" + packQpHdrValue(header))\n\t}\n\n\t// i=\n\tif s.Identity != nil {\n\t\tbuf.WriteString(\" i=\" + packQpHdrValue(s.Identity.String()))\n\t}\n\n\t// l=\n\tif s.Length >= 0 {\n\t\tbuf.WriteString(fmt.Sprintf(\" l=%d\", s.Length))\n\t}\n\n\t// q=\n\tfor _, method := range s.QueryMethods {\n\t\tbuf.WriteString(\" q=\" + packQpHdrValue(method))\n\t}\n\n\t// s=\n\tbuf.WriteString(\" s=\" + packQpHdrValue(s.Selector.ASCII))\n\n\t// t=\n\tif s.SignTime >= 0 {\n\t\tbuf.WriteString(fmt.Sprintf(\" t=%d\", s.SignTime))\n\t}\n\n\t// x=\n\tif s.ExpireTime >= 0 {\n\t\tbuf.WriteString(fmt.Sprintf(\" x=%d\", s.ExpireTime))\n\t}\n\n\t// z=\n\tfor _, header := range s.CopiedHeaders {\n\t\tbuf.WriteString(\" z=\" + packQpHdrValue(header))\n\t}\n\n\t// \\r\\n\n\tbuf.WriteString(\"\\r\\n\")\n\n\treturn buf.String(), nil\n}\n```"]}, "mox-dkim/txt.go-Record": {"final_query": "# You are a professional programmer, please create a function based on the function signature and natural language annotations# Here is the related information in the same file:../repo/go_data/mox/dkim/txt.go:\n```\npackage dkim\n\nimport (\n\t\"crypto/ed25519\"\n\t\"crypto/rsa\"\n\t\"crypto/x509\"\n\t\"encoding/base64\"\n\t\"errors\"\n\t\"fmt\"\n\t\"strings\"\n)\n\n// Record is a DKIM DNS record, served on <selector>._domainkey.<domain> for a\n// given selector and domain (s= and d= in the DKIM-Signature).\n//\n// The record is a semicolon-separated list of \"=\"-separated field value pairs.\n// Strings should be compared case-insensitively, e.g. k=ed25519 is equivalent to k=ED25519.\n//\n// Example:\n//\n//\tv=DKIM1;h=sha256;k=ed25519;p=ln5zd/JEX4Jy60WAhUOv33IYm2YZMyTQAdr9stML504=\ntype Record struct {\n\tVersion  string   // Version, fixed \"DKIM1\" (case sensitive). Field \"v\".\n\tHashes   []string // Acceptable hash algorithms, e.g. \"sha1\", \"sha256\". Optional, defaults to all algorithms. Field \"h\".\n\tKey      string   // Key type, \"rsa\" or \"ed25519\". Optional, default \"rsa\". Field \"k\".\n\tNotes    string   // Debug notes. Field \"n\".\n\tPubkey   []byte   // Public key, as base64 in record. If empty, the key has been revoked. Field \"p\".\n\tServices []string // Service types. Optional, default \"*\" for all services. Other values: \"email\". Field \"s\".\n\tFlags    []string // Flags, colon-separated. Optional, default is no flags. Other values: \"y\" for testing DKIM, \"s\" for \"i=\" must have same domain as \"d\" in signatures. Field \"t\".\n\n\tPublicKey any `json:\"-\"` // Parsed form of public key, an *rsa.PublicKey or ed25519.PublicKey.\n}\n\n// ../rfc/6376:1438\n\n// ServiceAllowed returns whether service s is allowed by this key.\n//\n// The optional field \"s\" can specify purposes for which the key can be used. If\n// value was specified, both \"*\" and \"email\" are enough for use with DKIM.\nfunc (r *Record) ServiceAllowed(s string) bool {\n\tif len(r.Services) == 0 {\n\t\treturn true\n\t}\n\tfor _, ss := range r.Services {\n\t\tif ss == \"*\" || strings.EqualFold(s, ss) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// Record returns a DNS TXT record that should be served at\n// <selector>._domainkey.<domain>.\n//\n// Only values that are not the default values are included.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunc qpSection(s string) string {\n\tconst hex = \"0123456789ABCDEF\"\n\n\t// ../rfc/2045:1260\n\tvar r string\n\tfor i, b := range []byte(s) {\n\t\tif i > 0 && (b == ' ' || b == '\\t') || b > ' ' && b < 0x7f && b != '=' {\n\t\t\tr += string(rune(b))\n\t\t} else {\n\t\t\tr += \"=\" + string(hex[b>>4]) + string(hex[(b>>0)&0xf])\n\t\t}\n\t}\n\treturn r\n}\n\nvar (\n\terrRecordDuplicateTag     = errors.New(\"duplicate tag\")\n\terrRecordMissingField     = errors.New(\"missing field\")\n\terrRecordBadPublicKey     = errors.New(\"bad public key\")\n\terrRecordUnknownAlgorithm = errors.New(\"unknown algorithm\")\n\terrRecordVersionFirst     = errors.New(\"first field must be version\")\n)\n\n// ParseRecord parses a DKIM DNS TXT record.\n//\n// If the record is a dkim record, but an error occurred, isdkim will be true and\n// err will be the error. Such errors must be treated differently from parse errors\n// where the record does not appear to be DKIM, which can happen with misconfigured\n// DNS (e.g. wildcard records).\nfunc ParseRecord(s string) (record *Record, isdkim bool, err error) {\n\tdefer func() {\n\t\tx := recover()\n\t\tif x == nil {\n\t\t\treturn\n\t\t}\n\t\tif xerr, ok := x.(error); ok {\n\t\t\trecord = nil\n\t\t\terr = xerr\n\t\t\treturn\n\t\t}\n\t\tpanic(x)\n\t}()\n\n\txerrorf := func(format string, args ...any) {\n\t\tpanic(fmt.Errorf(format, args...))\n\t}\n\n\trecord = &Record{\n\t\tVersion:  \"DKIM1\",\n\t\tKey:      \"rsa\",\n\t\tServices: []string{\"*\"},\n\t}\n\n\tp := parser{s: s, drop: true}\n\tseen := map[string]struct{}{}\n\t// ../rfc/6376:655\n\t// ../rfc/6376:656 ../rfc/6376-eid5070\n\t// ../rfc/6376:658 ../rfc/6376-eid5070\n\t// ../rfc/6376:1438\n\tfor {\n\t\tp.fws()\n\t\tk := p.xtagName()\n\t\tp.fws()\n\t\tp.xtake(\"=\")\n\t\tp.fws()\n\t\t// Keys are case-sensitive: ../rfc/6376:679\n\t\tif _, ok := seen[k]; ok {\n\t\t\t// Duplicates not allowed: ../rfc/6376:683\n\t\t\txerrorf(\"%w: %q\", errRecordDuplicateTag, k)\n\t\t\tbreak\n\t\t}\n\t\tseen[k] = struct{}{}\n\t\t// Version must be the first.\n\t\tswitch k {\n\t\tcase \"v\":\n\t\t\t// ../rfc/6376:1443\n\t\t\tv := p.xtake(\"DKIM1\")\n\t\t\t// Version being set is a signal this appears to be a valid record. We must not\n\t\t\t// treat e.g. DKIM1.1 as valid, so we explicitly check there is no more data before\n\t\t\t// we decide this record is DKIM.\n\t\t\tp.fws()\n\t\t\tif !p.empty() {\n\t\t\t\tp.xtake(\";\")\n\t\t\t}\n\t\t\trecord.Version = v\n\t\t\tif len(seen) != 1 {\n\t\t\t\t// If version is present, it must be the first.\n\t\t\t\txerrorf(\"%w\", errRecordVersionFirst)\n\t\t\t}\n\t\t\tisdkim = true\n\t\t\tif p.empty() {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tcontinue\n\n\t\tcase \"h\":\n\t\t\t// ../rfc/6376:1463\n\t\t\trecord.Hashes = []string{p.xhyphenatedWord()}\n\t\t\tfor p.peekfws(\":\") {\n\t\t\t\tp.fws()\n\t\t\t\tp.xtake(\":\")\n\t\t\t\tp.fws()\n\t\t\t\trecord.Hashes = append(record.Hashes, p.xhyphenatedWord())\n\t\t\t}\n\t\tcase \"k\":\n\t\t\t// ../rfc/6376:1478\n\t\t\trecord.Key = p.xhyphenatedWord()\n\t\tcase \"n\":\n\t\t\t// ../rfc/6376:1491\n\t\t\trecord.Notes = p.xqpSection()\n\t\tcase \"p\":\n\t\t\t// ../rfc/6376:1501\n\t\t\trecord.Pubkey = p.xbase64()\n\t\tcase \"s\":\n\t\t\t// ../rfc/6376:1533\n\t\t\trecord.Services = []string{p.xhyphenatedWord()}\n\t\t\tfor p.peekfws(\":\") {\n\t\t\t\tp.fws()\n\t\t\t\tp.xtake(\":\")\n\t\t\t\tp.fws()\n\t\t\t\trecord.Services = append(record.Services, p.xhyphenatedWord())\n\t\t\t}\n\t\tcase \"t\":\n\t\t\t// ../rfc/6376:1554\n\t\t\trecord.Flags = []string{p.xhyphenatedWord()}\n\t\t\tfor p.peekfws(\":\") {\n\t\t\t\tp.fws()\n\t\t\t\tp.xtake(\":\")\n\t\t\t\tp.fws()\n\t\t\t\trecord.Flags = append(record.Flags, p.xhyphenatedWord())\n\t\t\t}\n\t\tdefault:\n\t\t\t// We must ignore unknown fields. ../rfc/6376:692 ../rfc/6376:1439\n\t\t\tfor !p.empty() && !p.hasPrefix(\";\") {\n\t\t\t\tp.xchar()\n\t\t\t}\n\t\t}\n\n\t\tisdkim = true\n\t\tp.fws()\n\t\tif p.empty() {\n\t\t\tbreak\n\t\t}\n\t\tp.xtake(\";\")\n\t\tif p.empty() {\n\t\t\tbreak\n\t\t}\n\t}\n\n\tif _, ok := seen[\"p\"]; !ok {\n\t\txerrorf(\"%w: public key\", errRecordMissingField)\n\t}\n\n\tswitch strings.ToLower(record.Key) {\n\tcase \"\", \"rsa\":\n\t\tif len(record.Pubkey) == 0 {\n\t\t\t// Revoked key, nothing to do.\n\t\t} else if pk, err := x509.ParsePKIXPublicKey(record.Pubkey); err != nil {\n\t\t\txerrorf(\"%w: %s\", errRecordBadPublicKey, err)\n\t\t} else if _, ok := pk.(*rsa.PublicKey); !ok {\n\t\t\txerrorf(\"%w: got %T, need an RSA key\", errRecordBadPublicKey, record.PublicKey)\n\t\t} else {\n\t\t\trecord.PublicKey = pk\n\t\t}\n\tcase \"ed25519\":\n\t\tif len(record.Pubkey) == 0 {\n\t\t\t// Revoked key, nothing to do.\n\t\t} else if len(record.Pubkey) != ed25519.PublicKeySize {\n\t\t\txerrorf(\"%w: got %d bytes, need %d\", errRecordBadPublicKey, len(record.Pubkey), ed25519.PublicKeySize)\n\t\t} else {\n\t\t\trecord.PublicKey = ed25519.PublicKey(record.Pubkey)\n\t\t}\n\tdefault:\n\t\txerrorf(\"%w: %q\", errRecordUnknownAlgorithm, record.Key)\n\t}\n\n\treturn record, true, nil\n}\n\n```\n# Function Signature: func (r *Record) Record() (string, error) {\n# Natural Language Annotations: // Record returns a DNS TXT record that should be served at\n// <selector>._domainkey.<domain>.\n//\n// Only values that are not the default values are included.\nPlease return the generated code surrounded by ```\n", "response": ["```go\nfunc (r *Record) Record() (string, error) {\n\tvar record string\n\n\tif r.Version != \"DKIM1\" {\n\t\trecord += fmt.Sprintf(\"v=%s;\", r.Version)\n\t}\n\n\tif len(r.Hashes) > 0 {\n\t\trecord += fmt.Sprintf(\"h=%s;\", strings.Join(r.Hashes, \":\"))\n\t}\n\n\tif r.Key != \"rsa\" {\n\t\trecord += fmt.Sprintf(\"k=%s;\", r.Key)\n\t}\n\n\tif r.Notes != \"\" {\n\t\trecord += fmt.Sprintf(\"n=%s;\", base64.StdEncoding.EncodeToString([]byte(r.Notes)))\n\t}\n\n\tif len(r.Pubkey) > 0 {\n\t\trecord += fmt.Sprintf(\"p=%s;\", base64.StdEncoding.EncodeToString(r.Pubkey))\n\t}\n\n\tif len(r.Services) > 0 {\n\t\trecord += fmt.Sprintf(\"s=%s;\", strings.Join(r.Services, \":\"))\n\t}\n\n\tif len(r.Flags) > 0 {\n\t\trecord += fmt.Sprintf(\"t=%s;\", strings.Join(r.Flags, \":\"))\n\t}\n\n\t// Remove the trailing semicolon\n\tif len(record) > 0 {\n\t\trecord = record[:len(record)-1]\n\t}\n\n\treturn record, nil\n}\n```", "```go\nfunc (r *Record) Record() (string, error) {\n\trecord := \"v=\" + r.Version\n\n\tif len(r.Hashes) > 0 {\n\t\trecord += \"; h=\" + strings.Join(r.Hashes, \":\")\n\t}\n\n\tif r.Key != \"rsa\" {\n\t\trecord += \"; k=\" + r.Key\n\t}\n\n\tif r.Notes != \"\" {\n\t\trecord += \"; n=\" + qpSection(r.Notes)\n\t}\n\n\tif len(r.Pubkey) > 0 {\n\t\trecord += \"; p=\" + base64.StdEncoding.EncodeToString(r.Pubkey)\n\t}\n\n\tif len(r.Services) > 0 {\n\t\trecord += \"; s=\" + strings.Join(r.Services, \":\")\n\t}\n\n\tif len(r.Flags) > 0 {\n\t\trecord += \"; t=\" + strings.Join(r.Flags, \":\")\n\t}\n\n\treturn record, nil\n}\n```", "```go\nfunc (r *Record) Record() (string, error) {\n\tvar record string\n\tif r.Version != \"DKIM1\" {\n\t\trecord += \"v=\" + r.Version + \";\"\n\t}\n\tif len(r.Hashes) > 0 {\n\t\trecord += \"h=\" + strings.Join(r.Hashes, \":\") + \";\"\n\t}\n\tif r.Key != \"rsa\" {\n\t\trecord += \"k=\" + r.Key + \";\"\n\t}\n\tif r.Notes != \"\" {\n\t\trecord += \"n=\" + qpSection(r.Notes) + \";\"\n\t}\n\tif len(r.Pubkey) > 0 {\n\t\trecord += \"p=\" + base64.StdEncoding.EncodeToString(r.Pubkey) + \";\"\n\t}\n\tif len(r.Services) > 1 || (len(r.Services) == 1 && r.Services[0] != \"*\") {\n\t\trecord += \"s=\" + strings.Join(r.Services, \":\") + \";\"\n\t}\n\tif len(r.Flags) > 0 {\n\t\trecord += \"t=\" + strings.Join(r.Flags, \":\") + \";\"\n\t}\n\t// Remove the trailing semicolon\n\tif len(record) > 0 {\n\t\trecord = record[:len(record)-1]\n\t}\n\treturn record, nil\n}\n```"]}}